,ID,Title,Authors,Year,Cited By,Detected_Dataset,Detected_Topic,Abstract,DOI,Journal,URL,Mentions_Accuracy,Mentions_F1,Mentions_Precision,Mentions_Recall,Mentions_Auc,Mentions_Roc,Mentions_Sensitivity,Mentions_Specificity,Mentions_Confusion_matrix,Mentions_Loss_function,Mentions_Cross-entropy,Mentions_Mean_squared_error,Mentions_Overfitting,Mentions_Underfitting,Mentions_Cross-validation,Mentions_Training_time,Mentions_Inference_time,Mentions_Statistical_significance,Mentions_P-value,Mentions_T-test,Mentions_Anova,Mentions_Correlation,Mentions_Regression,Mentions_Baseline_comparison,Mentions_Mae,Mentions_Rmse,Mentions_Bias,Full_Text
0,0,10 Automated Face Analysis for Affective Computing,"['JF Cohn', 'F De la Torre']",2015,170,Affective Faces Database,classifier,Differences in manual coding between databases may and do occur as well and can  contribute to impaired generalizability of classifiers from one database to another.,No DOI,The Oxford handbook of affective …,https://academic.oup.com/edited-volume/28057/chapter/212009519,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
1,1,3D facial expression recognition based on automatically selected features,"['H Tang', 'TS Huang']",2008,205,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",facial expression recognition from 3D facial shapes is inves distances between 83 facial  feature points in the 3D space. Using a  al. at Binghamton University. It was designed to sample,No DOI,… on computer vision and pattern recognition …,https://ieeexplore.ieee.org/document/4563052,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
2,2,3D facial expression recognition based on primitive surface feature distribution,"['J Wang', 'L Yin', 'X Wei', 'Y Sun']",2006,440,Binghamton University 3D Facial Expression,facial expression recognition,expressions using 3D facial expression range data. We propose a novel approach to extract  primitive 3D facial expression  distribution to classify the prototypic facial expressions. In,No DOI,… Vision and Pattern Recognition  …,https://ieeexplore.ieee.org/document/1640921,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
3,3,3D facial expression recognition based on properties of line segments connecting facial feature points,"['H Tang', 'TS Huang']",2008,153,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition",Binghamton University have recently constructed a 3D facial expression database for facial   This is definitely the first attempt to make a publicly available 3D facial expression database,No DOI,… on Automatic Face & Gesture Recognition,https://ieeexplore.ieee.org/document/4813304,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
4,4,3D facial expression recognition using SIFT descriptors of automatically detected keypoints,"['S Berretti', 'B Ben Amor', 'M Daoudi', 'A Del Bimbo']",2011,184,Binghamton University 3D Facial Expression,"classification, classifier, facial expression recognition","at the Binghamton University (BU-3DFE database) [34], and at the Bogaziçi University (  facial expression recognition algorithms. This is due to the fact that, differently from other",No DOI,The Visual Computer,https://www.researchgate.net/publication/220068159_3D_facial_expression_recognition_using_SIFT_descriptors_of_automatically_detected_keypoints,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
5,5,A 3D facial expression database for facial behavior research,"['L Yin', 'X Wei', 'Y Sun', 'J Wang']",2006,1631,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",3D: Critical Issues and Limitations of 2D (1) 3D surface features exhibited in facial expressions  The common theme in the current research on face expression recognition is that the face,No DOI,… and gesture recognition  …,https://ieeexplore.ieee.org/document/1613022,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
6,6,A deep learning perspective on the origin of facial expressions,"['R Breuer', 'R Kimmel']",2017,140,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Static Facial Expression in the Wild","CNN, FER, deep learning, machine learning","We verify our findings on the Extended Cohn-Kanade (CK+),  tasks and tests using transfer  learning, including cross-dataset  long-short-term-memory (LSTM) recurrent neural network (",No DOI,arXiv preprint arXiv:1705.01842,https://arxiv.org/abs/1705.01842,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
7,7,A deeper look at facial expression dataset bias,"['S Li', 'W Deng']",2020,127,"Affective Faces Database, MMI Facial Expression","FER, classification, classifier","As the skew class distribution across domains is a possible bottleneck for cross-domain FER,   Pantic, “Induced disgust, happiness and surprise: An addition to the MMI facial expression",No DOI,IEEE Transactions on Affective Computing,https://arxiv.org/abs/1904.11150,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
8,8,A high-resolution spontaneous 3d dynamic facial expression database,"['X Zhang', 'L Yin', 'JF Cohn', 'S Canavan']",2013,920,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition",3D video database of spontaneous facial expressions in a diverse group of young adults.  Well-validated emotion inductions were used to elicit expressions  units and emotion detection”,No DOI,… gesture recognition  …,https://www.sciencedirect.com/science/article/pii/S0262885614001012,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: BP4D-Spontaneous: a high-resolution spontaneous 3D
dynamic facial expression database
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. High-resolution data acquisition
7. 3\. Data annotation and meta-data creation
8. 4\. Evaluation and analysis
9. 5\. Validation & application in facial expression analysis
10. 6\. Conclusion and future work
11. Acknowledgments
12. References
Show full outline
## Cited by (523)
## Figures (11)
1. 2. 3. 4. 5. 6.
Show 5 more figures
## Tables (17)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Image and Vision Computing
Volume 32, Issue 10, October 2014, Pages 692-706
# BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression
database?
Author links open overlay panelXing Zhang a, Lijun Yin a, Jeffrey F. Cohn b,
Shaun Canavan a, Michael Reale a, Andy Horowitz a, Peng Liu a, Jeffrey M.
Girard b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.imavis.2014.06.002Get rights and content## Highlights
* ?
We present the first 3D dynamic spontaneous facial expression database.
* ?
Meta-data include FACS coding, head pose data, and 2D/3D landmarks.
* ?
We present an effective emotion elicitation protocol using eight tasks.
* ?
The database is analyzed by self-report, observers? rating, and AU annotation.
* ?
The database is validated through expression/AU recognition, and pain
analysis.
## Abstract
Facial expression is central to human experience. Its efficiency and valid
measurement are challenges that automated facial image analysis seeks to
address. Most publically available databases are limited to 2D static images
or video of posed facial behavior. Because posed and un-posed (aka
?spontaneous?) facial expressions differ along several dimensions including
complexity and timing, well-annotated video of un-posed facial behavior is
needed. Moreover, because the face is a three-dimensional deformable object,
2D video may be insufficient, and therefore 3D video archives are required. We
present a newly developed 3D video database of spontaneous facial expressions
in a diverse group of young adults. Well-validated emotion inductions were
used to elicit expressions of emotion and paralinguistic communication. Frame-
level ground-truth for facial actions was obtained using the Facial Action
Coding System. Facial features were tracked in both 2D and 3D domains. To the
best of our knowledge, this new database is the first of its kind for the
public. The work promotes the exploration of 3D spatiotemporal features in
subtle facial expression, better understanding of the relation between pose
and motion dynamics in facial action units, and deeper understanding of
naturally occurring facial action.
* Previous article in issue
* Next article in issue
## Keywords
3D facial expression
FACS
Spontaneous expression
Dynamic facial expression database
## 1\. Introduction
Research on computer-based facial expression and affect analysis has
intensified since the first FG conference in 1995. The resulting advances have
made the emerging field of affective computing possible. The continued
development of emotion-capable systems greatly depends on access to well-
annotated, representative affective corpora [13]. A number of 2D facial
expression databases have become available (e.g., [1], [2], [7], [8], [16]),
as well as some with 3D imaging data (e.g., [9], [14], [15], [24], [25],
[45]). Although some systems have been successful, performance degrades when
handling expressions with low intensity appearance, large head rotation,
subtle skin movement, and/or lighting changes with varying postures.
Due to the limitations of describing facial surface deformation when 3D
features are evaluated in 2D space, 2D images with a handful of features may
not accurately reflect the authentic facial expressions (e.g., in-depth motion
of 3D head pose, 3D wrinkles and skin extrusion in the areas of the cheek,
forehead, glabella, nasolabial, and crow's feet).Another problematic issue is that facial action units (AUs) can occur in more
than 7000 different complex combinations [17], causing bulges and various in-
and out-of-image-plane movements of permanent facial features, negative
emotions from view of the left hemiface, and mouth extrusions that are
difficult to detect in a 2D plane. Three-dimensional dynamic surface analysis
and tracking will be important for those facial expressions for which 2D
motion information is not sufficient.
Because the face is a 3D object and many communicative signals involve changes
in depth and head rotation, inclusion of 3D information is an important
addition. Another major limitation of existing databases is that most have
only posed or acted facial behavior, and thus the data are not representative
of spontaneous affective expression, which may differ in timing, complexity,
and intensity from posed expression [22]. No currently available dataset
contains _dense_ , _dynamic_ , _3D_ facial representations of _spontaneous_
facial expression with anatomically-based (FACS) _annotation_ [36].
Currently, most approaches to automatic facial expression analysis attempt to
recognize a set of prototypic emotional expressions (e.g., anger, disgust,
fear, happiness, sadness, and surprise) [3], [5], [13]. Many studies about
emotion used ?acting? or ?emotion portrayals? in a restricted sense by
recording subjects who are instructed to express single-label emotions,
sometimes using scripts [6]. However, the resulting posed and exaggerated
facial actions may occur only rarely in daily life [4].
Because posed and un-posed (aka ?spontaneous?) facial expressions differ along
several dimensions [32], including complexity (especially with respect to
segmentation), well-annotated video of un-posed facial behavior is needed.
Moreover, as noted above, because the face is a three-dimensional deformable
object, a 3D video archive would be especially important. Two-dimensional
databases, such as RU-FACS [23] or Cohn?Kanade [2], are insufficient. The CMU
Multi-PIE database [34], 3D dynamic AU database [35], Bosphorus database [9],
KDEF [33], BU 3D facial expression databases [14], [15], and ICT-3DRFE
database [24] begin to address the need for 3D (or multi-view) data but are
limited to posed facial behavior.
Recent efforts to collect, annotate, and analyze spontaneous facial expression
for community use have begun [26], [27], [28]. However, all are limited to the
2D domain or thermal imaging. To address the need for well-annotated, dynamic
3D video of spontaneous facial behavior in response to meaningful and varied
emotion inductions, we developed a _3D Dynamic Spontaneous_ facial expression
database with _annotation_ , called the _Binghamton_ ? _Pittsburgh 4D
Spontaneous Expression Database_ (_BP4D_ -_Spontaneous_), for the research
community. The contributions of the work are as follows:
* (1)
We applied a series of well-designed tasks for authentic emotion induction.
The tasks include social interviews between the previously unacquainted people
(one a naïve subject and the other a professional actor/director), planned
activities (e.g., games), film clip watching, a cold pressor test for pain
elicitation, a social challenge to elicit anger followed by reparation, and
olfactory stimulation to elicit disgust.
* (2)
Well-experienced, certified FACS coders annotated the videos. Frame-level
ground-truth for facial actions was obtained.
* (3)
The effectiveness of the eliciting methods has been verified by subject self-
report and FACS analysis.
* (4)An alternative subjective evaluation and validation were conducted by human
observer ratings.
* (5)
A set of meta-data, including AU codes, tracked 3D/2D features, and head poses
is provided.
* (6)
Additionally, the quality and usefulness of the database have also been
evaluated and validated through a number of applications in spontaneous facial
expression recognition, 3D dynamic Action Units recognition, and a case-study
for authentic pain expression analysis.
The remainder of this paper is organized as follows. In Section 2, we
introduce the data acquisition procedure, expression elicitation method, and
data processing and organization. In Section 3, we describe the creation of
meta-data, including FACS coding, 3D/2D feature tracking, and head pose
estimation. The experimental method and data quality are then evaluated
through the analysis of the self-report information, subjective ratings, and
AU statistics in Section 4. In Section 5, we further verify the usefulness of
the database through applications in 4D spontaneous facial expression
recognition and AU recognition, as well as a case study on pain expression
analysis. Finally, concluding remarks and future work are given in Section 6.
## 2\. High-resolution data acquisition
### 2.1. System setup
A Di3D dynamic face capturing system [12] captured and generated 3D facial
expression sequences. The data include both 3D model sequences and 2D texture
videos. The system consists of two stereo cameras and one texture video
camera. The three cameras are placed on a tripod with two lighting lamps, one
of each side of the cameras. A calibrating board and a blue board are used for
calibration and background segmentation. With one master machine and three
slave machines, the system captures the 3D videos at a speed of 25 fps. In
addition to the 3D imaging system to capture the head-shoulder regions, we
have also setup a regular video camera to capture the entire scene and audio
for site monitoring and as a reference for possible audio-visual editing in
the future if needed. The data is captured in the normal lighting conditions
of an indoor lab environment. Fig. 1 shows an example of the imaging system at
work.
1. Download: Download high-res image (109KB)
2. Download: Download full-size image
Fig. 1. Upper-left: general view from a regular camera; Upper-right: 2D video;
Lower-left: 3D dynamic geometric model; Lower-right: 3D dynamic geometric
model with mapped texture.
### 2.2. Data capture
Each participant was instructed to sit in front of the 3D face capturing
system at about 51 inches distance from the cameras. After view adjustment and
an initial preview capture, the capture procedure started by following an
emotion elicitation protocol as described below.
#### 2.2.1. Emotional expression elicitation
We define the ?spontaneous facial expressions? as facial actions that are not
deliberately posed, i.e., facial actions that occur in the course of social
interaction or other social or non-social stimuli. For recording spontaneous
affective behavior, a good trade-off between the acquisition of natural
emotional expressions and data quality is needed. If the recording environment
is too constrained, genuine emotion and social signaling become difficult to
elicit. If the recording environment is unconstrained, substantial error may
be introduced into the recordings. In the psychology literature, well-validated emotion techniques and guidelines have been proposed to meet this
challenge [43].
To elicit target emotional expressions and conversational behavior, we used
approaches adapted from other investigators plus techniques that proved
promising in our pilot test. Each task was administered by an experimenter who
was a professional actor/director of performing arts. The tasks include
_interview_ , _video_ -_clip viewing and discussion_ , _startle probe_ ,
_improvisation_ , _threat_ , _cold_ _pressor_ , _insult_ , and _smell_. Each
task together with its target emotion is described in Table 1. Interviews
elicit a wide range of emotion and interpersonal behavior [56], [57], [60].
Film clips and games [10], [46], [61] are well-validated approaches to elicit
emotion. Cold pressor is well studied to safely elicit pain expressions
without the risk of tissue injury [44]. Olfactory stimuli can reliably elicit
disgust [62]. These methods evoke a range of authentic emotions in a
laboratory environment [11].
Table 1. Eight tasks for emotional expression elicitation.
Task| Activity| Target emotion
---|---|---
1| _Interview_ : talk to the experimenter and listen to a joke (interview).|
Happiness or amusement
2| _Video clip_ : watch a video clip and discuss it with the experimenter.|
Sadness
3| _Startle probe_ : sudden, unexpected burst of sound.| Surprise or startle
4| _Improvisation_ : play a game in which the subjects improvise a silly
song.| Embarrassment
5| _Threat_ : anticipate and experience physical threat.| Fear or nervous
6| _Cold pressor_ : submerge a hand in ice water for as long as possible.|
Physical pain
7| _Insult_ : experience harsh insults from the experimenter.| Anger or upset
8| _Smell_ : experience an unpleasant smell.| Disgust
After participants gave informed consent to the procedures and permissible
uses of their data, the experimenter explained the general procedure (without
giving any details about the specific tasks) and began the emotion inductions.
Following usage in the psychology literature, each emotion induction is
referred to as a ?task?. The experimenter was a professional actor and
director. Each participant experienced eight tasks, as summarized in Table 1.
Those tasks were seamlessly spaced with smooth transitions between them.
Immediately after each task, participants completed self-report ratings of
their feelings unless otherwise noted.
The protocol began with a conversation, which included joke telling, between
the participant and the experimenter. The relaxed exchange and shared positive
emotion were intended to build rapport and elicit expressions of amusement.
After rating the first experience, the participant watched and listened to a
documentary about a real emergency involving a child, followed by an interview
that gave them opportunity to talk about their feelings in response to the
task. Reactions of sadness were the intended responses.
Next, the participant was asked to participate in several activities with the
experimenter. These included startle triggered by a siren, embarrassment
elicited by having to improvise a silly song, fear while playing a game that
occasioned physical danger, and physical pain elicited by submerging their
hand in ice water. Following this cold pressor task, the experimenter
intentionally berated the participant to elicit anger followed by reparation.
Finally, the participant was asked to smell an unpleasant odor to evoke strong
feelings and expressions of disgust. The tasks concluded with a debriefing bythe experimenter. Each task was recorded about 1?4 min and archived as
described in sub-Section 2.3.
The procedures elicited a range of emotions and facial expressions that
include happiness/amusement, sadness, surprise/startle, embarrassment,
fear/nervous, physical pain, anger/upset, and disgust.
#### 2.2.2. Participants
Forty-one participants (23 women, 18 men) were recruited from the departments
of psychology and computer science as well as from the school of engineering.
They were 18?29 years of age; 11 were Asian, 6 were African-American, 4 were
Hispanic, and 20 were Euro-American (Table 3).
### 2.3. Data processing and database organization
For each task, there were three synchronized videos to be captured from two
gray-scale stereo cameras and one color video camera. The stereo videos were
processed by four machines (PCs) in parallel. Each pair of stereo images is
processed using a passive stereo photogrammetry approach to produce its own
range map. The range maps are then combined to produce a sequence of high-
resolution 3D images. The geometric face model contains 30,000?50,000
vertices. The 2D texture videos are 1040 × 1392 pixels/frame. Fig. 2 shows
example expressions of eight emotions elicited from eight tasks.
1. Download: Download high-res image (467KB)
2. Download: Download full-size image
Fig. 2. 2D and 3D examples of eight emotional expressions from task 1 to task
8 (from left to right), respectively.
The database is structured by participant. Each participant is associated with
eight tasks. For each task, there are both 3D and 2D videos. Table 2
illustrates the total number of frames for each task across all 41
participants in the database. Although tasks varied in duration, to reduce
storage demands and processing time, each video consists of the segment during
which the participant was most expressive (about one minute on average). This
reduced the retention of frames in which little facial expression occurred.
The video data are about 2.6 TB in size, and the average number of vertices
for each 3D model is about 37,000.
Table 2. The frame number for each task among 41 participants.
Task| #Frames
---|---
1| Interview| 47,640
2| Video clip| 65,555
3| Startle probe| 12,863
4| Improvisation| 60,647
5| Threat| 52,323
6| Cold pressor| 44,670
7| Insult| 69,033
8| Smell| 15,305
Meta-data consists of manually annotated action units (FACS AUs),
automatically tracked head pose, and 2D/3D facial landmarks. Table 3
summarizes the 3D dynamic spontaneous facial expression database. Fig. 3 shows
the data structure of each task. Fig. 4 shows several samples of 3D dynamic
spontaneous facial expression sequences. The meta-data (e.g., AU codes,
tracked features, and head poses) will be described in detail in the next
section.
Table 3. Summary of BP4D-spontaneous database.
# of participants| # of tasks| # of 3D + 2D sequences| # of metadata sequences
(i.e., annotated AUs, facial landmarks, and poses)
---|---|---|---41| 8| 328| 328
Note: Asian (11), African-American (6), Hispanic (4), and Euro-American (20).
1. Download: Download high-res image (145KB)
2. Download: Download full-size image
Fig. 3. Organization of each task in database.
1. Download: Download high-res image (911KB)
2. Download: Download full-size image
Fig. 4. Samples of textured models, shaded models, original 2D videos, and the
annotated action units (AUs).
## 3\. Data annotation and meta-data creation
### 3.1. FACS coding
Automatic detection of FACS action units is a major thrust of the current
research in automated facial image analysis [22]. To provide necessary ground
truth in support of these efforts, we annotated facial expressions using the
Facial Action Coding System (FACS) [17], [18].
For each participant, we code action units associated with emotion and
paralinguistic communication. Because FACS coding is time intensive, we
prioritized coding to focus on 20-second segments that were most productive of
facial expression.
For each of the eight tasks, FACS coders coded a 20-second segment that had
the highest density of facial expression. Coders were free to code for longer
than 20 s if the expression continued beyond that duration. If a video was
less than 20 s, it was coded in its entirety. Descriptive statistics are
reported in Table 4.
Table 4. Descriptive statistics for FACS-coded videos (unit of measure is
seconds).
Task| Activity| _Minimum_| _Maximum_| _Mean_
---|---|---|---|---
1| _Interview_| 13.00| 29.71| 19.67
2| _Video clip_| 12.12| 25.00| 20.21
3| _Startle probe_| 8.56| 16.76| 12.25
4| _Improvisation_| 16.14| 24.12| 19.74
5| _Threat_| 18.53| 31.00| 20.74
6| _Cold pressor_| 8.00| 23.00| 18.95
7| _Insult_| 17.24| 25.01| 19.91
8| _Smell_| 3.60| 21.40| 11.49
Note. Unit of measure is seconds. Data are based on video from all 41
participants.
For each condition, two experienced FACS-certified coders independently coded
onsets and offsets of 27 action units per the 2002 edition of FACS [36] using
Observer Video-Pro Software [21]. These AUs, the corresponding amount of
frames, and the number of AU events (from onset to offset) for each are listed
in Table 5. An event is defined as a continuous series of AU from onset to
offset. The observer system makes it possible to manually code digital video
in stop-frame and at variable speed and later synchronize codes according to
the digital time stamp. For AU 12 and AU 14, intensity was coded as well on a
0?5 ordinal scale using custom software.
Table 5. Descriptive statistics for kappa reliability, events, and frames.
Action unit| Name| Kappa reliability| Events| Frames
---|---|---|---|---
1| Inner brow raiser| 0.90| 474| 31,043
2| Outer brow raiser| 0.95| 380| 25,110
4| Brow lowerer| 0.92| 408| 29,755
5| Upper lid raiser| 0.97| 182| 56936| Cheek raiser| 0.91| 540| 67,677
7| Lid tightener| 0.92| 569| 80,617
9| Nose wrinkler| 0.91| 140| 8512
10| Upper lip raiser| 0.90| 591| 87,271
11| Nasolabial deepener| 0.94| 33| 7184
12| Lip corner puller| 0.92| 448| 82,531
13| Cheek puller| n/a| 2| 138
14| Dimpler| 0.92| 571| 68,376
15| Lip corner depressor| 0.79| 657| 24,869
16| Lower lip depressor| 0.68| 219| 6593
17| Chin raiser| 0.88| 1203| 50,407
18| Lip pucker| 0.83| 33| 568
19| Tongue show| 0.84| 61| 1197
20| Lip stretcher| 0.95| 105| 3644
22| Lip funneler| 0.95| 46| 606
23| Lip tightener| 0.78| 805| 24,288
24| Lip pressor| 0.86| 457| 22,229
27| Mouth stretch| 0.95| 55| 1271
28| Lip suck| 0.97| 117| 5697
30| Jaw sideways| 0.95| 15| 506
32| Bite| 0.98| 26| 1466
38| Nostril dilator| 0.94| 1| 1319
39| Nostril compressor| 0.97| 25| 657
Overall| | 0.90| 8161| 639,224
Note: Data are based on video from all 41 participants. Overall kappa is
weighted average based on 36 double-coded videos. An event is defined as a set
of contiguous frames from onset frame to offset frame.
To quantify the inter-observer exact (25 fps) agreement, the same thirty-six
randomly selected video segments were coded by both coders. Their judgment was
quantified using coefficient kappa [37], which is the proportion of agreement
above what would be expected to occur by chance. Table 5 reports the kappa
reliability.
In summary, the expression sequences were AU-coded by two experts. For each
sequence, 27 AUs were considered for coding. For each of the target AUs, we
have various numbers of coded events, where an event is defined as the
contiguous frames from onset to offset.
### 3.2. 3D feature tracking
We defined 83 feature points around the 3D facial areas of eyes, nose, mouth,
eyebrows, and chin contour in the initial frame of a video sequence (see Fig.
6(a)). Extended from the active appearance model approach [30], we applied our
newly developed 3D geometric surface based temporal deformable shape model
[40] to track 83 points on the 3D dynamic surface directly. Our developed
method involves fitting a new multi-frame constrained 3D temporal deformable
shape model (3D-TDSM) to range data sequences. We consider this a temporal
based deformable model as we concatenate consecutive deformable shape models
into a single model driven by the appearance of facial expressions. This
allows us to simultaneously fit multiple models over a sequence of time with
one 3D-TDSM.
To construct a temporal deformable shape model, we applied a representation of
the point distribution model to describe the 3D shape, in which a
parameterized model _S_ was constructed by 83 landmark points on each model
frame. Such a set of feature points (shape vector) was aligned by the
Procrustes analysis method [30]. Principal component analysis (PCA) was then
performed on the new aligned feature vector to retain 95% of the variance ofthe model. This was done to estimate the different variations of all the
training shape data from the mean shape S¯, as shown in Eq. (1). By adjusting
the weight vector _w_ in the range, the instances of 3D-TDSM could be
constructed. When approximating a new shape _S_ , the point distribution model
was constrained by both the variations in shape and the shapes of neighbor
frames. In our experiment, two neighbor frames were considered.
(1)S=S¯+Vw(2)D=?i=1Nui?xi2+vi?yi2+wi?zi2where _N_ is 83. After creating the
instance of 3D-TDSM, the 3D range model was fully searched to find the closest
point to each landmark among the instances. Then, the weight vector _w_ can be
calculated. Since the shape variation was limited by both the deformation
rules and the shapes of neighbor frames, if _w_ was outside of the domain it
was discarded. For the remaining acceptable candidates, the Procrustes
distance, between the corresponding 3D-TDSM instance (_u_ , _v_ , _w_) and the
result from fitting (_x_ , _y_ , _z_), was computed (as shown in Eq. (2)). The
candidate with the minimum _D_ value was chosen as the tracking result. Fig. 5
illustrates the fitting process, and Fig. 7 (lower row) shows several sample
frames of the tracked 83 feature points on a 3D model sequence. The detailed
algorithm is described in [40].
1. Download: Download high-res image (110KB)
2. Download: Download full-size image
Fig. 5. Eight samples of the 3D-TDSM candidates. The distance score is listed
at the top of each candidate. The one with the minimum score (bottom right) is
the best fit.
### 3.3. 2D feature tracking
Two-dimensional facial expression sequences were automatically tracked using
the constrained local model (CLM) approach of [38], [39]. Forty-nine landmark
points were defined in the 2D face region (see Fig. 6(b)). All CLM tracking
was reviewed offline for tracking errors. Coded were: 1) ?Good tracking?; 2)
?Multiple errors?; 3) ?Jawline off?; 4) ?Occlusion?; and 5) ?Face out of
frame?. A confidence score of tracking performance was given for each frame.
If the score is lower than the error threshold, the frame is reported as lost-
track. Fig. 7 (upper row) shows several sample frames with the tracked points.
1. Download: Download high-res image (207KB)
2. Download: Download full-size image
Fig. 6. (a) 3D and (b) 2D landmarks' indices.
1. Download: Download high-res image (270KB)
2. Download: Download full-size image
Fig. 7. CLM-tracked feature points on a 2D sequence of a male subject (upper
row); a sample 3D sequence with 3D-TDSM tracked feature points of a female
subject (lower row).
Note that the 3D-TDSM tracks the 83 feature points purely based on 3D geometry
shape while the 2D-CLM tracks 49 feature points based on 2D images only. The
lost-track rates of 3D-TDSM and 2D-CLM are 0.148% and 0.194%, respectively.
The tracked 3D points offer features that are not reliably available from 2D
tracking in case of large pose variations. Since the two sets of tracking
points were obtained independently, they can be used for mutual verification
and compensation, thus allowing researchers for further study of feature
alignment between 2D and 3D, and developing algorithms for 2D/3D feature
detection and tracking with comparison to the baseline 2D/3D features that we
provide.
### 3.4. Head pose tracking
Head pose, which includes rigid head motion, is important for image
registration and is itself of communicative value, (e.g., downward head pitch
when coordinated with smiling communicates embarrassment.) Two sets of headpose data are provided based on 3D and 2D modalities. In 2D texture sequence,
head pose was measured from the 2D videos using a cylindrical head tracker
[19]. This tracker is person-independent, robust, and has concurrent validity
with a person-specific 2D + 3D AAM [20] and with a magnetic motion capture
device [19]. In 3D dynamic sequences, the 3D geometric features can derive the
pose information directly using three points (i.e., two eye corners and one
nose-base point). The head pose (pitch, yaw, and roll) was measured with
respect to the frontal pose. Table 6 shows the proportion of frames which
differs from the frontal view. In the case of extreme head movement, which
causes over 50% partial occlusion of facial regions, the pose information is
not obtainable; thus, the frame is labeled as lost-track.
Table 6. Proportion of frames in different pose relative to the frontal view.
Empty Cell| Pitch| Yaw| Roll
---|---|---|---
< 5°| 60.3%| 80.0%| 86.4%
< 10°| 94.7%| 97.5%| 98.0%
< 15°| 99.5%| 99.6%| 99.8%
< 20°| 99.9%| 99.8%| 99.9%
> 20°| 0.1%| 0.2%| 0.1%
Since the head pose information has been derived from two different modalities
with independent algorithms, the lost-track errors occur only when both pose
tracking methods fail. As a result, the lost-track error of pose is only
0.063%. The error is reduced as compared to the individual error rates of the
two tracking methods (as indicated in Section 3.3).
In short, the 3D-TDSM and the 2D-CLM provide the tracking results of
BP4D-Spontaneous in two modalities. The accuracy of head pose tracking is
increased due to the combination of the two tracking results.
## 4\. Evaluation and analysis
In order to evaluate the effectiveness of the emotion elicitation, we analyze
the data statistically based on participants' self-report, subjective ratings
from naïve observers, and AU distributions from coded videos.
### 4.1. Participants' self-report & analysis
After each task, participants used 6-point Likert-type scales (0 to 5, none to
extremely) to report their felt emotions for each task. The emotions listed
were _relaxed_ , _happiness_ /_amusement_ , _disgust_ , _anger_ /_upset_ ,
_sadness_ , _sympathy_ , _surprise_ , _fear_ /_nervous_ , _embarrassment_ ,
_physical pain_ , _and startle_. As has been found previously [53],
participants could and did experience more than one emotion for each task.
Fig. 8 shows the highest rated emotions reported for each task. Similarly,
Table 7 compares the most highly-rated emotion and the target emotion for each
task. Except for task 7, the target emotion for each task (see Table 1) was
the one most highly rated by the majority of participants. For instance, the
highest bar of task 8 shows that the majority of subjects rated the ?disgust?
emotion as the main emotion for that task. The highest bar of task 6 shows the
majority of subjects rated the ?pain? feeling as the main emotion.
Accordingly, almost all of the other tasks show this property as well. For
task 7, the most highly rated emotions could all be expected from the context
(anger at the experimenter, embarrassment from not doing better). Overall, the
tasks generally succeeded in evoking the target emotions.
1. Download: Download high-res image (235KB)
2. Download: Download full-size image
Fig. 8. Statistics of self-report emotion distribution for task 1 to task 8
(from left to right); vertical axis is the number of votes.
Table 7. Major emotions elicited from each task based on self-report.Task| Target emotion| Emotion most reported
---|---|---
1| Happiness or amusement| Happiness/amusement
2| Sadness| Sadness
3| Startle or surprise| Startle, surprise
4| Embarrassment| Embarrassment
5| Fear or nervous| Nervous/fear
6| Physical pain| Physical pain
7| Anger or upset| Anger/upset, embarrassment
8| Disgust| Disgust
To study the emotion distribution, we analyze all scales except 0 (None) of
the self-report rating of each task and illustrate the emotion scale
distribution in Fig. 9. The markers on each line show the intensity percentage
among the votes for the corresponding emotion category. Note that one to three
emotions for each task are selected for illustration. Our selection criteria
are based on the results in Fig. 8; for each task, we select a given emotion
for illustration if its vote count in Fig. 8 makes up over 20% (approximately
13 votes) of the total votes for the task. In general, Fig. 9 supports the
findings of Fig. 8 in that the major emotions of each task illustrated in Fig.
8 have high grades (scale 3 and above) in Fig. 9. In other words, the high
grades (from scale 3 to the highest scale 5) account for the majority of the
votes from the 41 subjects for the target emotions of each task.
1. Download: Download high-res image (348KB)
2. Download: Download full-size image
Fig. 9. Self-reported emotion distribution across 5 scales (very slightly, a
little, moderately, quite a bit, extremely) in each of the eight tasks (from
left to right, top to bottom, the charts correspond to the tasks 1?8 as shown
in Fig. 8).
Most of the tasks could elicit multiple emotions or mixed emotions. However,
there is still a principal emotion for each task. For the tasks such as task 2
(Documentary for Sadness), task 3 (Burst of Sound for Surprise/Startle), task
6 (Cold Pressor for Physical Pain), and task 8 (Smell for Disgust), the
majority of votes are distributed in the highest (or second highest) grades,
showing that the target emotion elicitation of those tasks was successful and
the intensity of the corresponding emotions is strong. For task 7 (Insult for
Anger/Upset), although the intensity of ?anger/upset? is not very strong, the
?anger/upset? emotion is still a major emotion of this task.
In general, the self-report information shows the target emotions were
elicited effectively.
### 4.2. Subjective rating and analysis
#### 4.2.1. Expression labeling from naïve observers
To evaluate the results of the emotion elicitation, we conducted a subjective
rating experiment by asking naïve observers to label each video with two of
the eight expressions for all the tasks.
Five naïve observers were recruited to participate in the rating experiment.
First, the purpose of the experiment was explained to the observer. Then, the
observer watched the AU coded video segments of all subjects in the database.
The videos were presented randomly in order to avoid any ordering effect, and
the videos were also muted to ensure that their rating was only based on the
visual information. For each video segment, eight expression labels were
provided to choose from. The observer was asked to choose up to two most
likely expressions in the order of their confidence. To indicate the
confidence level of each choice, the observer used a three-scale list which
represents _high confidence_ , _moderate confidence_ , and _low confidence_.The observer was allowed to replay the video segment if needed. A timer was
started when a given video segment was started, and it was stopped when the
final choices for the segment were made. In this way, the expression rating
and judgment time were also recorded. If the target emotion of a task was
recognized by an observer correctly, which means that the corresponding target
expression was included in the top two choices by the observer, we count it as
a correct recognition. The results show that the correct recognition rates of
five naïve observers are 61% (happiness), 77.1% (sadness), 94.6% (startle),
65.9% (embarrassment), 73.2% (fear), 66.8% (pain), 55.1% (anger), and 83.4%
(disgust).
#### 4.2.2. Analysis of subjective ratings
The inter-rater reliability was examined using Fleiss' kappa coefficient [37].
This has been used to assess the reliability of agreement between raters when
assigning categorical ratings to a number of items. In our case, five raters
assigned eight expression categories to all 328 video segments. The kappa
value is in the range of ? 1 to 1, corresponding to a negative range (? 1, 0)
and a non-negative range (0, 1). The non-negative value between 0 and 1 can be
divided into 0.2 sized steps. Therefore, a total of 6 levels can be generated.
Landis and Koch interpret the six levels as _poor_ , _slight_ , _fair_ ,
_moderate_ , _substantial_ , and _almost perfect_ agreement, respectively
[63].
Table 8 shows the Fleiss' kappa value for 8 expression categories. The overall
kappa indicates moderate agreement between all observers.
Table 8. Kappa coefficients from multiple raters for expression categories.
Expression| Kappa value
---|---
Happiness/amusement| 0.4325
Sadness| 0.6335
Startle| 0.8366
Embarrassment| 0.4128
Fear| 0.4933
Physical pain| 0.5250
Anger| 0.3724
Disgust| 0.7193
Overall| 0.5535
To further evaluate the performance, observers' confidence level and their
judgment time factor are also studied. We assigned to the confidence levels
_low_ , _moderate_ , and _high_ the values _0_ , _1_ , and _2_ , respectively.
The judgment time factor is a value of a given judgment time divided by the
length of the corresponding video segment of a task. If an observer spends
more time to make a decision than the length of the video segment, the factor
of the task is larger than 1. Otherwise, the factor is less than 1 if less
time is used. Table 9 lists the average values of confidence levels and the
average values of judgment time factor.
Table 9. Average value of confidence level and judgment time factor for
expression categories.
Expression| Conf. level| Judgment time factor
---|---|---
Happiness/amusement| 1.810| 1.059
Sadness| 1.780| 1.139
Startle| 1.941| 0.962
Embarrassment| 1.868| 1.039
Fear| 1.932| 0.910
Physical pain| 1.839| 1.041Anger| 1.824| 1.159
Disgust| 1.941| 1.162
To assess the performance of the human observers, a confusion matrix with
respect to the accuracy of the expression ratings from the five naïve
observers is illustrated in Table 10. The diagonal line shows the correct
recognition rates. Among them, _startle_ , _disgust_ , and _sadness_ are among
the three most distinguishable spontaneous expressions with the three highest
recognition rates by observers. _Happiness_ is sometimes confused with
_embarrassment_ as people sometimes show a soft smile in an embarrassing
situation. For the improvisation task, subjects felt embarrassed, and, in the
insult task, subjects could also feel embarrassed if they thought they had not
performed well. It is not unusual for people to restrain their genuine
negative emotions in social activities. People are likely to show a soft smile
in an awkward situation to defuse tension. Thus, the smile could be misread by
observers.
Table 10. Confusion matrix for the relationship between target expression and
observers' recognition.
Rec.| Hap.| Sad.| Star.| Emb.| Fear| Pain| Ang.| Dis.
---|---|---|---|---|---|---|---|---
Tar.
Hap.| 0.610| 0.010| 0.029| 0.166| 0.029| 0.020| 0.112| 0.024
Sad.| 0| 0.771| 0.029| 0| 0.093| 0.029| 0.044| 0.034
Star.| 0| 0.020| 0.946| 0| 0.010| 0.020| 0| 0.005
Emb.| 0.249| 0| 0.010| 0.659| 0.034| 0.005| 0.029| 0.015
Fear| 0.049| 0.005| 0.039| 0.059| 0.732| 0.044| 0.059| 0.015
Pain| 0.024| 0.059| 0| 0.044| 0.068| 0.668| 0.093| 0.044
Ang.| 0.161| 0.039| 0.015| 0.122| 0.078| 0.024| 0.551| 0.010
Dis.| 0.015| 0.015| 0| 0.015| 0.049| 0.054| 0.020| 0.834
The diversity and variety of spontaneous expressions still pose a challenge
for human observers to distinguish them. However, in general, the correct
classification rates for all the expressions in Table 10 are still dominant
when compared to the misclassification rates. The results are comparable to
the results obtained with machine recognition (which will be described in
Section 5.1). This shows that our elicitation method is effective when
eliciting distinctive facial activities associated with different tasks. The
distinctive dynamic information exhibited in spontaneous expressions benefits
expression reading by human observers.
### 4.3. Action unit analysis
#### 4.3.1. AU distribution in spontaneous expressions
Holistic expressions of emotion can be defined in terms of one or more
anatomic actions [52], [54], [55]. Following the previous work [16], [48],
[49], [50], Table 11 shows a mapping between AU(s) and the hypothetical
emotion. Table 12 shows the 27 action unit distributions across the eight
tasks. The value is a percentage _P_ that is defined as
follows:(3)P=NAUi/Ntwhere _N_ AUi is the number of frames that show the _i_ th
coded AU of a task (i = 1, 2, ?, 27), and _N_ t is the total number of AU
coded frames of the task (t = 1, 2, ?, 8). In this section, we report the
extent to which holistic expressions defined using AUs corresponded to the
target emotions.
Table 11. Emotion description in terms of facial action units.
Target emotion| Criteria
---|---
Happiness or amusement| AU 12 present
Sadness| Either AU 1 + 4 + 15 or 11 or AU 6 + 15Surprise or startle| Either AU 1 + 2 or 5 must be present for surprise
AU 7 for startle
Embarrassment| AU 12 or 24
Fear or nervous| AU 1 + 2 + 4 or AU 1 + 2 + 5
Physical pain| AU 4, 6, 7, 9, 10
Anger or upset| AU 23 and 24 must be present in the AU combination
Disgust| Either AU 9 or 10 must be present
Table 12. 27 action units percentage (%) in all tasks.
Task| T1| T2| T3| T4| T5| T6| T7| T8
---|---|---|---|---|---|---|---|---
AU
1| 13.6| 21.7| 18.1| 21.2| 38.9| 14.8| 18.1| 20.6
2| 15.7| 8.9| 18.0| 23.6| 27.9| 10.7| 17.5| 13.0
4| 7.1| 43.6| 19.3| 9.0| 11.2| 31.0| 6.3| 43.3
5| 2.4| 7.0| 7.9| 2.6| 4.5| 1.0| 4.7| 0.9
6| 61.7| 6.5| 26.2| 75.6| 60.0| 39.9| 44.7| 48.4
7| 62.2| 28.4| 34.7| 69.1| 65.1| 57.1| 52.8| 68.4
9| 1.3| 0.1| 4.4| 2.2| 7.2| 10.4| 1.5| 27.8
10| 71.1| 16.1| 28.8| 81.5| 76.1| 57.2| 67.1| 72.1
11| 9.1| 0.1| 4.1| 3.9| 4.8| 7.2| 3.7| 7.3
12| 81.9| 2.3| 32.5| 89.7| 79.3| 36.8| 67.3| 48.8
13| 0| 0| 0| 0| 0| 0.7| 0| 0
14| 50.6| 21.1| 36.5| 56.2| 54.6| 49.6| 53.2| 48.5
15| 16.4| 6.4| 8.6| 16.5| 22.9| 10.9| 22.4| 35.7
16| 4.3| 2.2| 4.3| 5.1| 6.0| 6.1| 5.0| 1.9
17| 32.9| 30.8| 21.3| 35.0| 37.5| 36.1| 32.8| 49.6
18| 0.4| 0.5| 0| 0.6| 0.7| 0.1| 0.3| 0.4
19| 0.5| 0.5| 1.1| 1.0| 1.3| 0.9| 0.7| 0.5
20| 0.8| 0.2| 2.7| 4.0| 1.1| 6.2| 1.4| 4.5
22| 0.4| 0| 0.2| 1.7| 0.1| 0.1| 0.6| 0
23| 14.7| 8.5| 9.7| 19.3| 17.5| 24.5| 19.3| 16.9
24| 11.8| 15.8| 11.5| 10.1| 15.4| 26.1| 10.3| 21.7
27| 0.9| 1.2| 0.9| 0.5| 0.3| 1.1| 1.6| 0.2
28| 2.6| 6.7| 2.5| 1.5| 5.4| 7.5| 2.2| 1.0
30| 0.1| 0.3| 0.6| 0.3| 0| 1.4| 0.1| 0.1
32| 1.8| 0| 0.3| 1.0| 2.5| 1.2| 0.5| 0
38| 0.3| 1.1| 1.5| 0| 0.2| 3.3| 0.6| 0.3
39| 0.1| 0.8| 0.1| 0.3| 0.2| 1.4| 0| 0.6
The AU distribution among different elicitation tasks illustrates the
complexity and diversity of spontaneous expressions. As we can see in Table
12, 41 subjects show genuine expressions which cover all the 27 action units.
Also, the frequencies of different AU occurrence vary dramatically across both
AUs and tasks. Unlike posed expressions, the facial action of spontaneous
expressions appears involuntarily. In Table 11, AU 12 is related to happiness,
and indeed a standard happy face contains AU 6 + 12. In Table 12, it shows
that column T1 has a major occurrence of AU 6 (61.7%) and AU 12 (81.9%).
However, it does not mean that AU 12 is unique to happiness. In fact, AU 12
occurred with varying frequencies in all tasks. This is consistent with the
hypothesis that AU 12 (smiling) is not specific to positive emotion. While AU
12 is a defining feature of expressed enjoyment, it occurs in pain,
embarrassment, and other negative emotions as well as in depression [51],
[58], [59]. In Table 11, we know that _disgust_ has either AU 9 or AU 10
present, and task 8 (experience an unpleasant smell) gives the highest
percentages of AU 9 (27.8%) and AU 10 (72.1%) among all tasks. AU 4 and AU 9are related to negative emotions. AU 4 shows relatively high percentages in
negative emotion tasks such as T2 (43.6%), T6 (31.0%), and T8 (43.3%), while
it shows low percentages in tasks for happiness (7.1%). Similar evidence can
be found for AU 9.
#### 4.3.2. Effectiveness of elicitation
Besides participants' self-report and observers' rating report, we further
study the AU distribution to verify the effectiveness of our emotion
elicitation method objectively.
Using the criteria listed in Table 11, we examine whether some AUs of the
target emotions appear most frequently in the corresponding tasks. According
to the percentage of AU sets that each task has in Table 12, the top two
ranked tasks (in order from left to right) are illustrated in Table 13. The
first column lists the AU criteria to be used for matching the corresponding
target emotions (as the second column shows). For example, ?12, 24? means that
either AU 12 or AU 24 needs to appear for the target emotion _Embarrassment_ ;
and ?1 + 2 + 4, 1 + 2 + 5? means either AU 1 + 2 + 4 or AU 1 + 2 + 5 needs to
appear for the target emotion _Fear/Nervous_. The third column shows the top
two ranked tasks based on the corresponding AU counts from the spontaneous
expressions of 41 subjects. It shows that the corresponding AUs appeared most
frequently in the top two tasks. Moreover, these two top tasks always include
the expected target emotion for all the cases (bold font). This verifies that
our elicitation method is effective in eliciting the target emotions. As an
example, in the second row, AU 4 is used as a criterion for matching the
_sadness_ (task 2). As a result, AU 4 has the most frequent occurrence in task
2 based on Table 12. Therefore, the top ranked task is the _sadness_ task,
which is exactly the expected target emotion.
Table 13. Top two tasks based on FACS emotion descriptiona.
AU criteria| Target emotion| Top two tasks indices
---|---|---
12| Happiness/amusement| Embarrassment, **happiness** /**amusement**
4| Sadness| **Sadness** , disgust
5| Surprise/startle| **Startle** , sadness
12, 24| Embarrassment| **Embarrassment** , happiness/amusement
1 + 2 + 4, 1 + 2 + 5| Fear/nervous| **Fear** /**nervous** , startle
4 + 6 + 7 + 9 + 10| Physical pain| Disgust, **physical pain**
23| Anger/upset| Physical pain, **anger** /**upset**
9 + 10| Disgust| **Disgust** , physical pain
a
In the third column, the corresponding target emotion is in bold font.
Therefore, we can see that the AU distribution of the BP4D-Spontaneous
database verifies the distinctiveness of the spontaneous expressions. Similar
to the results from the self-reported emotions, the findings for holistic
expressions suggest that the tasks were effective in eliciting the target
emotions.
## 5\. Validation & application in facial expression analysis
### 5.1. 4D spontaneous facial expression recognition
To validate the data for spontaneous facial expression recognition, we apply
an existing 3D dynamic facial expression descriptor [42] for expression
classification. A Hidden Markov Model (HMM) is used to learn the temporal
dynamics and spatial relationships of facial regions. To do so, a generic
model is adapted to each range model of a 3D model sequence. The adaptation is
controlled by a set of 83 key points based on the radial basis function (RBF).
After adaptation, the correspondence of the points across the 3D range model
sequence is established. We apply a surface labeling approach [42] to assigneach vertex one of eight primitive shape types. Thus, each range model in the
sequence is represented by a ?label map?. We use Linear Discriminative
Analysis (LDA) to transform the label map to an optimal compact space to
better separate different expressions. Given the optimized features, an HMM is
trained for each expression. Note that the HMM is applied to the optimal
features of the label map rather than the trajectories of 83 landmarks. In
recognition, the temporal/spatial dynamics of a test video is analyzed by the
trained HMMs. As a result, the probability scores of the test video to each
HMM are evaluated by the Bayesian decision rule to determine the expression
type of the test sequence.
We conducted a person-independent experiment on 41 subjects. Following a
10-fold cross-validation procedure, we used 39 subjects for training and 2
subjects for testing, and achieved an average correct recognition rate of
73.7% for distinguishing eight spontaneous emotional expressions.
In order to make a comparison to the existing work [15], [42], where _six
prototypic posed_ 3D dynamic facial expressions were used for recognition, we
have also conducted an experiment for _six prototypic spontaneous_ 3D dynamic
expression recognition using the same 41 subjects. The results show that the
correct recognition rate is 76.1%. Note that spontaneous expressions are more
difficult to classify than posed expressions. When the same approach was
applied to the 3D posed dynamic facial expression database BU-4DFE [15], a
recognition rate for classifying six posed prototypic expressions is 83%. The
performance degradation on classifying 3D spontaneous expressions is due to
the complexity, mixture, and subtlety of the spontaneous expressions in the
new database.
To further evaluate our approach, we conducted a comparison study by
implementing the 3D static model-based approach using geometric primitive
features [29] and the 2D texture-based approach using Gabor-wavelet features
[31] to classify eight expressions from the entire BP4D-Spontaneous database.
Note that we used two approaches in the case of the static image or static
model. In the first approach, we chose an apex frame of each video sequence
for the experiment. In the second approach, we chose three frames (a frame
between onset and apex, apex frame, and frame between apex and offset); we
then applied expression classification on the three frames individually. The
output confidence levels (output scores measured by the probability of Naïve
Bayesian Classifier) for the three frames were fused by averaging the output
scores for each expression. Among 8 expressions, we chose the highest fused
score as the recognized expression. The average recognition rates for the
static model and static frame based approaches were 62.4% and 63.2%,
respectively. The average recognition rates for the three-models and three-
frames-based approaches were 65.7% and 66.8%, respectively. The three-frame
fusion approach does show improved performance; however, the results are not
as good as those from the 3D dynamic model-based approach (i.e., 73.7% as
shown above for distinguishing eight spontaneous expressions from the entire
database).
### 5.2. Cross-database 4D facial expression classification
We also performed a cross-database validation on our new 4D spontaneous
database. A posed facial expression database (BU-4DFE [15]) was used for
training, and the spontaneous database was used for testing.
We extended the idea of a 3D surface primitive feature into 4D space and
developed a new feature representation: the so-called ?Nebula? features [41].
Given each vertex on the face, a local spatiotemporal volume is built from the
neighbor points across all frames in a time window (in our experiment, window
size is 15 frames). Spatial neighborhood radius sizes of 3, 5, and 7millimeters were tested, and the corresponding spatial voxel dimensions were 7
× 7, 11 × 11, and 15 × 15, respectively. The neighborhood data are voxelized
(with _x_ , _y_ , _t_ as the dimensions and depth _z_ as the values) and fit
to a cubic
polynomial:(4)fxyt=A12x2+B12y2+C12t2+Dxy+Ext+Fyt+Gx3+Hy3+It3+Jx2y+Kx2t+Lxy2+My2t+Nxt2+Pyt2+Q
x+Ry+St+U=z.
The principal curvature directions and values are computed from the
eigenvectors/eigenvalues of the Weingarten matrix:(5)ADEDBFEFC.
A label is assigned based on the principal curvature values and a threshold
_T_. Although this label compactly describes the shape of the feature, it does
not give us its orientation. This orientation information is vital, since it
can give us an indication of whether the surface changes across time or not.
For example, consider two features with cylindrical shapes. Both have the same
label; however, one may be oriented along the time axis, indicating no change
across time, while the other may be perpendicular to the time axis, indicating
a sharp change across time. Therefore, we use the label as well as the polar
angles of the direction of least curvature as the values for each feature. The
face area is then divided into regions. A 3D histogram is built for each
region of the face with the features in that region. The variables for that
histogram are the shape label and the two polar angles for each feature. The
concatenated histograms from each of the regions give us our final feature
vector. The construction of a single Nebula feature is shown in Fig. 10.
1. Download: Download high-res image (123KB)
2. Download: Download full-size image
Fig. 10. 4D Nebula feature construction: (a) sample voxel; (b) resulting
polynomial volume; principal axes shown in red, green, and blue, in order of
curvature magnitude (label = 14, cylinder); (c) _?_ (phi) angle of least
curvature direction from the time axis (shown as _Z_ axis in image); (d) _?_
(theta) angle of least curvature direction in the _XY_ plane.
We selected data (happiness, disgust, and neutral) from BU-4DFE for training.
We then tested directly on the spontaneous data using the Nebula feature
approach. Our overall average accuracy was 71%. From the experiment, we find
that Happy-Onset is often mistaken for Disgust-Onset. One possible explanation
is that the spontaneous smiles, since they are mostly genuine smiles, also
show activity around the eyes similar to some of the Disgust expressions; the
posed smiles from BU-4DFE do not always demonstrate this. To the best of our
knowledge, this is the first time that a cross-database test on 4D expression
data has been performed. Spontaneous expression data is almost invariably more
difficult to classify than posed expression data. As a consequence, we believe
our results are encouraging.
### 5.3. Action unit recognition on spontaneous 4D data
We also performed experiments in AU recognition on BP4D-Spontaneous. We
selected 16 subjects and tested on 12 AUs using a support vector machine
classifier. Please note that we only tested on the AUs listed in Table 14,
Table 15. In the database, the AUs are marked as present or not present for
each annotated frame. Several 9-frame windows were extracted around several
transition points (either near the apex frame or near the offset frame) for
each AU. Segments without the AU in question were also extracted. With three
different states per AU (?AU Onset?, ?AU Offset?, and ?No AU?), tests were
conducted on each AU individually.
Table 14. Spontaneous action unit recognition accuracy results: ?All? = all
blocks used; ?Best? = best blocks used.
AU| Nebula| LBP-TOP 2D| LBP-TOP depth
---|---|---|---All| Best| All| Best| All| Best
1| 54.1%| 58.4%| 57.9%| 49.4%| 52.4%| 48.5%
2| 63.0%| 64.8%| 59.2%| 55.4%| 55.9%| 53.1%
4| 58.7%| 63.1%| 53.3%| 48.4%| 51.1%| 48.9%
6| 67.6%| 68.8%| 64.8%| 64.8%| 61.3%| 61.7%
7| 58.9%| 58.0%| 55.4%| 51.9%| 52.4%| 53.2%
10| 66.4%| 65.9%| 62.1%| 54.3%| 56.9%| 58.6%
12| 57.3%| 57.8%| 59.1%| 54.7%| 53.3%| 54.7%
14| 54.5%| 59.1%| 52.3%| 46.4%| 52.8%| 48.1%
15| 66.0%| 69.0%| 64.5%| 61.6%| 63.1%| 62.1%
17| 61.8%| 65.6%| 60.0%| 44.6%| 53.3%| 43.5%
23| 60.6%| 61.4%| 58.5%| 53.4%| 59.3%| 55.5%
24| 67.1%| 67.6%| 63.4%| 56.3%| 62.9%| 54.0%
Avg.| 61.3%| 63.3%| 59.2%| 53.4%| 56.2%| 53.5%
Table 15. Spontaneous action unit recognition AUC score results: ?All? = all
blocks used; ?Best? = best blocks used.
AU| Nebula| LBP-TOP 2D| LBP-TOP Depth
---|---|---|---
All| Best| All| Best| All| Best
1| 0.621| 0.640| 0.631| 0.584| 0.627| 0.574
2| 0.682| 0.697| 0.642| 0.586| 0.614| 0.602
4| 0.657| 0.700| 0.637| 0.578| 0.589| 0.601
6| 0.796| 0.800| 0.774| 0.783| 0.768| 0.758
7| 0.696| 0.680| 0.678| 0.662| 0.660| 0.648
10| 0.791| 0.773| 0.739| 0.668| 0.686| 0.705
12| 0.703| 0.726| 0.732| 0.691| 0.653| 0.663
14| 0.682| 0.717| 0.648| 0.605| 0.624| 0.583
15| 0.739| 0.784| 0.688| 0.626| 0.731| 0.705
17| 0.758| 0.801| 0.713| 0.607| 0.681| 0.598
23| 0.704| 0.722| 0.675| 0.614| 0.702| 0.639
24| 0.774| 0.791| 0.713| 0.633| 0.720| 0.592
Avg.| 0.717| 0.736| 0.689| 0.636| 0.671| 0.639
Various parameter combinations and regular grid block region configurations
were tested. Specifically, block configurations with 15, 24, 35, 54, 77, and
96 blocks were tested. The number of rows and columns in each block region
configuration was chosen to make the individual blocks square, given the size
of the facial region. In addition to using all of the blocks of an entire face
for a given configuration, we also tested using the ?best? blocks for each AU.
The best set of blocks is chosen based on where the AU appears on the face.
The best blocks are in effect either dividing the face into top and bottom
halves or taking the middle third of the face. For example, since AU 15 occurs
around the mouth, the ?best? region blocks for that AU are all the blocks in
the lower half of the face; that is, the blocks on the top part of the face
are excluded. In cases where the division of blocks was uneven, blocks were
included rather than excluded (for instance, if the face was divided into 5
rows and we needed the top half of the face, rows 1, 2, and 3 were chosen).
The Nebula feature approach was again employed here. We also tested using the
LBP-TOP (Local Binary Patterns from Three Orthogonal Planes) [47] approach on
the re-rendered, pose-normalized texture images and the corresponding depth
images. The accuracy results for all three approaches are shown in Table 14,
while the AUC (Area Under Receiver Operating Characteristic Curve) score
results are shown in Table 15. The confusion tables using the Nebula feature
approach for each AU (using the parameters yielding the best accuracies) are
shown in Table 17.Table 16. _P_ score of paired _T_ -test on discrimination between pain and
other emotion tasks on AU 6, AU 9, and AU 10.
Task| T1| T2| T3| T4| T5| T7| T8
---|---|---|---|---|---|---|---
T6| 0.01| 4.50 × 10? 7| 3.52 × 10? 4| 4.15 × 10? 5| 3.08 × 10? 4| 0.03| 0.02
Table 17. Spontaneous action unit recognition: confusion tables for best
accuracies with nebula 4D approach.
The average recognition AUC score using the ideal Nebula approach for each AU
(i.e., all or ?best? blocks) was over 0.738. This validates the usefulness of
the new database as well as demonstrates the effectiveness of the test
approaches. The highest average accuracy among all approaches is 63.3% from
Nebula on the ?best? blocks. Thus, we achieve over 4% better accuracy on
average with the Nebula approach over the most accurate LBP-TOP approach (LBP-
TOP 2D on all blocks). The AUC scores follow the same pattern, with Nebula on
the ?best? blocks giving us 0.736 AUC while the best LBP-TOP score (LBP-TOP 2D
on all blocks) is 0.689. In Table 17, a common source of confusion is the
misclassification between onsets/offsets and non-existence of an AU. This may
due to the fact that spontaneous expression data is more challenging for AU
recognition than that for posed expression data.
Note that in Table 14, Table 15, if we compare the ?best? region results
between Nebula and LBP-TOP 2D for individual AUs, the former is always better
than the latter one. For the ?all? region test, this is also true except for
AU 1 and AU 12. We also note that in LBP-TOP 2D the textured image has been
pose-normalized. The results can be worse for pure 2D-based recognition
approaches. This proves that 3D face representation surpasses the 2D image
representation by isolating the facial components such as expression, head
pose and skin tone in this experiment. In our case, facial expression data can
be extracted efficiently for our experiments. Contrariwise, to analyze
expression in 2D, researchers need to carefully deal with challenges from pose
prediction and skin color normalization, etc., and the error coming with the
pre-processing stage can contaminate the training procedure. In short, we
believe that the new data and our test approaches show promise. Further
details are described in [41].
### 5.4. Case study ? pain analysis with BP4D-spontaneous database
To further validate the usefulness of the 4D spontaneous data, we choose the
spontaneous expression _physical pain_ for a case study. The goal is to
evaluate spontaneous pain appearance with automated AU recognition. In this
case, the contributed AUs may be different from the typical AUs for pain
listed in Table 11. To find the contributed AUs, we use the paired _t_ -test
on the possible subset of pain AUs (AU 4, 6, 7, 9, 10) for all tasks of the
database. This test is to assess whether pain and other spontaneous
expressions are different in terms of the AU quantity. For each possible AU,
the percentage of frames with this AU is reported for each task per subject.
Here we find that in terms of AU quantity the differences between pain and all
other expressions are significant with AU 6, AU 9, and AU 10 (_p_ < 0.05,
shown in Table 16). Therefore, the automated AU recognition system is built
based on these three AUs. The positive data (AU appearance) set is acquired
from task 6 (a total of 19,524 frames), while the negative data (No AU
appearance) is from task 2 (a total of 20,837 frames) as it has the minimum
_p_ value (4.50 × 10? 7).
To generate the facial features, face normalization and feature registration
procedures are applied through rotating the face model to a frontal view,
cropping the 3D face region, and normalizing it to a standard size of a 72 ×
72 matrix along with the landmark and pose information. Then, a Gabor waveletfilter with 3 spatial scales and 8 orientations is applied to generate the
feature vector.
Principal component analysis (PCA) is used to reduce the feature dimension to
200. Three binary support vector machines (SVMs) are then trained with one
label per frame using a radial basis function (RBF) kernel for three AUs
separately. The output margin to the SVM hyper plane is used to generate the
ROC curves. A leave-one-subject-out validation method is used for all 41
subjects in the database. Each test subject's sequence from task 2 and task 6
has been used as the negative test sequence and positive test sequence,
respectively.
The average AUC scores weighted by the positive sample numbers for AU 6, AU 9
and AU 10 are 0.865, 0.667, and 0.806 respectively. The pain appearance can be
categorized in 8 (= 23) types, depending on the occurrence of each AU (0 or
1). If none of the three AUs appears, the face shows no pain.
In Fig. 11(a?b), we illustrate the AU recognition performance, pain
categories, and example frames. In each figure, sample frames are displayed on
top. For AU 6, AU 9, and AU 10, each has been plotted with two curves, i.e., a
green curve representing the automatic recognition result, and a dashed blue
curve representing the ground truth from the binary AU coding. For each AU in
a frame, it is labeled to 1 if the AU occurs, or 0 if the AU does not appear.
Based on the eight kinds of combinations, different color indicates different
pain appearances as shown in the bar ?pain categories?. ?Red? shows intense
pain and ?Blue? shows no pain.
1. Download: Download high-res image (1014KB)
2. Download: Download full-size image
Fig. 11. a: Physical pain appearance from a non-pain activity sequence.
Examples are the 1177th, 1290th, 1369th, and 1410th frames in the sequence.
b: Physical pain appearance from a genuine pain sequence. Examples are the
14th, 99th, 203th, and 246th frames in the sequence.
Fig. 11a shows the result of pain detection along a sequence of task 2, in
which the subject was watching a documentary. The accuracy for AU 6 and AU 10
is 1, and the AUC score for AU 9 is 0.93. In such a non-pain activity, the
subject did not show any pain expression according to AU 6, AU 9 and AU 10.
The results match the ground-truth (dashed blue lines) very well in most of
the sequence except for very few frames where a single AU 9 is falsely
identified.
Fig. 11b shows the pain detection results when the subject was submerging his
hand in ice water (task 6). The AUC scores for AU 6 and AU 9 are 0.98 and
0.70, and the accuracy for AU 10 is 1. As an example, the second frame in Fig.
11b shows a less painful expression as compared to the others. Only AU 10 was
detected for that frame.
Thus, we have taken physical pain as a study case to show how the spontaneous
expression data and the corresponding meta-data (AU codes, pose, and feature
data) could be used for automated expression recognition. Based on this study,
more applications of 3D-based pain analysis and further investigation of a
complete set of pain-related AUs could be developed in the future.
## 6\. Conclusion and future work
In this paper, we reported our newly developed spontaneous 3D dynamic facial
expression database (the so-called ?BP4D-Spontaneous? ? Binghamton?Pittsburgh
4D Spontaneous Facial Expression Database). Such a database can be a valuable
resource to facilitate the research and development of human behavior analysis
in security, HCI, psychology and biomedical applications.
It is worth noting that stimulating genuine emotions with spontaneous facial
expressions in a controlled environment is still a challenging issue. OurBP4D-Spontaneous data focused on facial actions that are not deliberately
posed. Rather, they occur in the course of social interaction and other social
or non-social stimuli. The guided format using a professional actor and
director as the experimenter sought to simulate a more natural setting while
guaranteeing high quality 3D/2D dynamic facial expression data.
The AU annotation by expert coders provides valuable information about
spontaneous facial behavior. However, the diversity of spontaneous expressions
and the mixture of different expressions still pose big challenges for emotion
and expression recognition.
Expression analysis on 2D data suffers from head pose variation, illumination
change, self-occlusion, etc. These influences can be removed or reduced by 3D
representations. BP4D-Spontaneous addresses this issue by providing 3D time-
varying spontaneous facial expression data. When we use ?Nebula? and LBP
features to recognize AUs, we also noticed that the performance varies with
different AUs. This may due to the different types of features exhibited by
different AUs. For example, AU 4 and AU 9 create lines and furrows, while AU
25 reveals a new feature by showing teeth. That said, there may not be a
panacea feature for recognition of all action units. In BP4D-Spontaneous,
3D/2D imaging data and the corresponding tracking points provide important
information for feature distribution and discrimination. Combining the
features from the 3D and 2D domains may improve the expression recognition
performance.
In future work, other settings and image capturing setups might be considered.
Data quality could be improved by using a wider range imaging system which is
more robust to different illumination conditions. The database will also be
expanded to include more subjects.
Moreover, our current database includes sequential geometric model data and
texture data. In addition to the facial feature tracking algorithms, more
powerful approaches need to be investigated in order to make the data
processing and visualization fast and accurate. Automatic data annotation,
registration, and efficient data representation (or compression) for micro-
expression analysis will also be our next research direction.
## Acknowledgments
This material is based upon the work supported in part by the National Science
Foundation under grants IIS-1051103, IIS-1051169, CNS-1205664, and
CNS-1205195. We would like to thank Nicki Siverling, Dean Rosenwald, and Shaun
Zuratovic for FACS coding. We would also like to thank Dr. Peter Gerhardstein
for the help on data collection.
Special issue articlesRecommended articles"
9,9,A multimodal database for affect recognition and implicit tagging,"['M Soleymani', 'J Lichtenauer', 'T Pun']",2011,1719,Affective Faces Database,"classification, classifier","our database in Section 3. Section 4 explains the experimental setup. The first experiment  paradigm, some statistics and results of classifications  to face videos from 32 participants. The",No DOI,… on affective computing,https://ieeexplore.ieee.org/document/5975141,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
10,10,A natural visible and infrared facial expression database for expression recognition and emotion inference,"['S Wang', 'Z Liu', 'S Lv', 'Y Lv', 'G Wu', 'P Peng']",2010,458,Affective Faces Database,facial expression recognition,"database for expression recognition and emotion inference, we conduct visible facial  expression recognition  There are, however, a few thermal face databases (listed in Table II) that",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/5523955,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
11,11,A new facial expression recognition based on curvelet transform and online sequential extreme learning machine initialized with spherical clustering,"['A Uçar', 'Y Demir', 'C Güzeliş']",2016,158,Extended Cohn-Kanade,machine learning,"So, the learning machine is called as OSELM-SC. It is  Expression database and the  Cohn-Kanade database. The  of California at irvine repository of the machine learning database",No DOI,Neural Computing and Applications,https://link.springer.com/article/10.1007/s00521-014-1569-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
12,12,A new method for facial expression recognition based on sparse representation plus LBP,"['MW Huang', 'Z Wang', 'ZL Ying']",2010,101,Japanese Female Facial Expression,classification,"algorithm for face recognition, notable for  facial expression recognition based on sparse  representation of LBP features. Extensive experiments on Japanese Female Facial Expression (",No DOI,2010 3rd International …,https://ieeexplore.ieee.org/document/5647898,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
13,13,A performance comparison of eight commercially available automatic classifiers for facial affect recognition,"['D Dupré', 'EG Krumhuber', 'D Küster', 'GJ McKeown']",2020,165,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classification, classifier","classifiers, in particular when facial expressions are spontaneous rather than posed. In the  present work, we tested eight out-of-the-box automatic classifiers,  or acted facial behavior.",No DOI,Plos one,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231968,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
14,14,A review on automatic facial expression recognition systems assisted by multimodal sensor data,"['N Samadiani', 'G Huang', 'B Cai', 'W Luo', 'CH Chi', 'Y Xiang']",2019,206,"Binghamton University 3D Facial Expression, MMI Facial Expression",FER,"facial expressions (eg, sadness and anger). An efficient facial expression system (FER) can   ; this method achieves the accuracies of 98.5% (CK+ dataset) and 81.18% (MMI dataset).",No DOI,Sensors,https://www.mdpi.com/1424-8220/19/8/1863,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
15,15,A survey of 2D face recognition techniques,"['M Chihaoui', 'A Elkefi', 'W Bellil', 'C Ben Amar']",2016,131,Toronto Face Database,classification,"Figure 6 summarizes the classification of face recognition approaches presented in this   Next, we presented face databases used by researchers in this field to test their approaches and",No DOI,Computers,https://www.mdpi.com/2073-431X/5/4/21,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
16,16,A survey on facial emotion recognition techniques: A state-of-the-art literature review,"['FZ Canal', 'TR Müller', 'JC Matias', 'GG Scotton']",2022,222,Affective Faces Database,facial expression recognition,"approaches for emotion recognition from facial expressions,  area of emotion expression  recognition captured from facial  areas such as face recognition and emotion recognition color",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S0020025521010136,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: A survey on facial emotion recognition techniques: A
state-of-the-art literature review
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Methodology for literature review
6. 3\. General computational flow for facial emotion recognition
7. 4\. Preprocessing
8. 5\. Feature Extraction
9. 6\. Datasets for emotion recognition from images or videos
10. 7\. Classification algorithms
11. 8\. Emotion Recognition Methods from Facial Images or Videos
12. 9\. Conclusion and discussions
13. Declaration of Competing Interest
14. Acknowledgments
15. References
Show full outline
## Cited by (162)
## Figures (13)
1. 2. 3. 4. 5. 6.
Show 7 more figures
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Information Sciences
Volume 582, January 2022, Pages 593-617
# A survey on facial emotion recognition techniques: A state-of-the-art
literature review
Author links open overlay panelFelipe Zago Canal a, Tobias Rossi Müller a,
Jhennifer Cristine Matias a, Gustavo Gino Scotton a, Antonio Reis de Sa Junior
c, Eliane Pozzebon a b, Antonio Carlos Sobieranski a b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.ins.2021.10.005Get rights and content
## AbstractIn this survey, a systematic literature review of the state-of-the-art on
emotion expression recognition from facial images is presented. The paper has
as main objective arise the most commonly used strategies employed to
interpret and recognize facial emotion expressions, published over the past
few years. For this purpose, a total of 51 papers were analyzed over the
literature totaling 94 distinct methods, collected from well-established
scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and
Scopus), whose works were categorized according to its main construction
concept. From the analyzed works, it was possible to categorize them into two
main trends: classical and those approaches specifically designed by the use
of neural networks. The obtained statistical analysis demonstrated a
marginally better recognition precision for the classical approaches when
faced to neural networks counterpart, but with a reduced capacity of
generalization. Additionally, the present study verified the most popular
datasets for facial expression and emotion recognition showing the pros and
cons each and, thereby, demonstrating a real demand for reliable data-sources
regarding artificial and natural experimental environments.
* Previous article in issue
* Next article in issue
## Keywords
Emotion Recognition
Facial emotion recognition
Pattern recognition
Systematic literature review
## 1\. Introduction
Facial Emotion Recognition performed computationally is a very interesting and
challenging task to be explored. Besides interpreting facial emotion
expression being a task naturally performed by humans, finding computational
mechanisms to reproduce it in the same or similar way is still an unsolved
problem [8]. Designing and developing algorithmic solutions able to interpret
facial emotions from human faces opens a new window of possibilities for the
human?computer interaction context, such as in robotics, gaming, digital
marketing, intelligent tutor systems, among many others [59]. The sooner we
are able to design such recognizers, the better we can help to understand
natural areas of psychology, neuroscience, human cognition and learning [76].
Nonetheless, how human expression behavior is invariant among distinct
individuals, and how biological and social aspects may interferes in human
communication over time, are interesting questions to be studied and modeled
computationally, as well as its correlation.
In order to provide analytical models for human expression recognition from
facial images, engineers, mathematicians and computer scientists are exploring
distinct ways to reproduce approaches able to effectively implement efficient
algorithms. There are strong correlations among image processing, computer
vision, pattern recognition and artificial intelligence fields exploring this
topic, and interesting approaches can be verified over the literature. In
general, specifically due to the recent achievements in computational
processing power and new architectures for high-performance computing,
applications for the aforementioned fields, previously restricted by
computational limitations, have had their solution achieved [12]. With the
advent of the computing graphic processing units (GPU?s), many methods were
adapted for their parallel version, having execution time close to real-time
[102]. Specifically for the emotion recognition problem from facial images,
those computational improvements have also provided a new set of tools, paving
the way for the development of several approaches based on what we named hereas classical methods1.
Another important achievement was obtained over the past few years with the
advent of variants of neural networks ? the Convolutional Neural Networks
(CNN?s). CNN?s have being applied as generic problem-solvers, where some input
signal is decomposed (de-convolved) into a set of invariant-features,
providing robust mechanisms to extract relevant features (such as texture,
corners and key-points). After a training step, the classifier is ready to
interpret image with zero-bias in a very effective way. Consequently, CNN is
ascending lately in many areas, mainly the ones that involve image processing
and pattern recognition, such as those used in medical image classification
[65], [84], [7].
Over the literature it is possible to find many works fully dedicated to solve
the emotion recognition problem from facial images, using classical methods or
Neural Network based (NNB) approaches [27], [43], [50], [109]. The graph of
the Fig. 1 shows a comparison of published approaches for emotion recognition
from facial expressions, obtained from our literature review. For the selected
papers, two main trends can be verified: (i) classical approaches, being
composed of traditional image processing, pattern recognition and
miscellaneous classifiers, more specifically provided by a hand-crafted
feature design process, and (ii), NNB approaches, where classical neural
networks and its convolutional counterpart were considered, whose features are
learned from data using generic feature extractions, more specifically for the
convolutional case. One can see a considerable acceptance of NNB approaches
over the past five years, more specifically justified by the emergence of
CNN?s approaches.
1. Download: Download high-res image (80KB)
2. Download: Download full-size image
Fig. 1. Number of methods related to facial emotion recognition since 2005 in
our database. Approaches were classified into traditional image processing or
computer vision approaches (Classic) and neural networks based (NNB)
techniques. Only methods that had mentioned the classification algorithm,
dataset and results clearly were counted.
Given the large number of classical methods, which still can be combined among
them producing hybrid designs, and the emerging CNN?s approaches, a new
literature review for the area of expression and emotion recognition from
facial images is needed. In order to achieve this goal, in this paper, a
systematic literature review was conducted to select the published works whose
inclusion and exclusion criterion matched, and a total of 51 were obtained,
resulting in 94 distinct approaches. Based on the selected papers, the
objective of this work was to provide answers to the following questions: (i)
what is the state-of-the-art in emotion recognition, how many methods,
techniques or classes of methods can be identified over the literature? (ii)
what is the importance of the CNN?s as a general trend for the facial emotion
recognition problem? (iii) is there a general computational workflow for
facial emotion recognition that can be identified? (iv) how many databases for
facial emotion recognition can be identified, containing the number of
individuals and expressions? how is their acceptance in the scientific
community? (v) is it possible to predict further trends to solve the problem
of emotion recognition from facial images, and how far from a generic
effective solution are we?.
Given the importance of the aforementioned research questions, the remainder
of this work is organized as follows: Section 2 provides the systematic
literature review methodology used in this survey, describing in details the
inclusion and exclusion criterion employed. Section 3 presents a generalcomputational flow we are able to identify as commonly used approach to solve
the problem of emotion recognition from facial images or videos. Section 4
presents the most frequently used preliminary step found in the verified
works, being defined by a preprocessing step used to locate the region of
interest, as well as to perform image improvements. The process of extracting
relevant information from the images, as known as feature extraction, is
briefly discussed in Section 5. Datasets publicly available to the scientific
community used to train the computational models are described in Section 6.
The most used classification algorithms are presented in Section 7. The
computational approaches and techniques used for emotion recognition from
facial images or videos are described in Section 8. Finally, conclusions,
discussions and further directions identified in this study are presented in
Section 9.
## 2\. Methodology for literature review
Over the past few years it is possible to find interesting literature review
papers summarizing the most prominent works of the current state-of-the-art in
the FER area. In general, journal papers such as [56], [63], [39], [79]
present a very detailed description of many facial expression or emotion
recognition systems, highlighting aspects of image or video acquisition, post-
processing, feature extraction and recognition, as well as available datasets,
essential sources to properly train a FER recognition system. Not less
important, there are other papers majorly published in conferences, whose FER
technologies are also presented [80], [103], [81], [67], focusing their
discussion to traditional methods, or to the recently emerged convolutional
neural network counterpart. Also, it is possible to find interesting
literature reviews describing the combined use of FER with secondary
information, such as the use of compound facial expression emotions [103], the
combined use of augmented reality [79], gender and emotion classification
using facial expression detection [32], and a comparison of methods
specifically designed for genuine versus posed facial expressions of emotions
[44].
In the literature review presented in this paper we intend to analyze the
state-of-the-art in the area of emotion expression recognition captured from
facial images or videos, based on the methodology described by Kitchenham
[52], a well-established methodology used to provide systematic literature
reviews in a verifiable and reliable manner. It is intended to provide a
general panorama of the results achieved by computational facial emotion
recognition, employing the most different approaches, yet mainly focuses on
analyzing classical and NNB works, since those methods seems to be arising
over the past few years. Classical approaches while effective, may be very
dependent of the execution parameters and environment conditions to achieve
efficient results. On the other hand, NNB approaches can be generalized to
solve a series of problems, and have been used in the past few years to
clarify the issue of facial emotion recognition as well. We will analyze the
existing methods regarding in which emotion types they are applied; whether
they are adaptive regarding environment changes; and whether they are able to
distinguish possible faults or changes in terms of emotions; datasets and the
capability to distinguish between different kind of expressions.
The systematic literature review for emotion expression recognition aims to
postulate research questions as presented below:
_**Question**_ : What is the state-of-the-art in computerized approaches for
emotion recognition, obtained from facial images or videos, published over the
past years?
_**Population**_ : Related works describing a computational approach forexpression emotion recognition from facial images or videos, obtained from raw
data or well-established datasets publicly available.
_**Intervention**_ : Analyze the state-of-the-art in computational emotion
recognition.
_**Results**_ : Comparison among computational expression emotion recognition
approaches and datasets.
_**Context**_ : Papers found in digital libraries such as: _Science Direct_ ,
_IEEEXplore_ , _Scopus_ e _ACM Digital Library_.
The databases for literature review were chosen by taking into account their
relevance to the areas of image processing, computer vision, pattern
recognition and artificial intelligence, more specifically those that employed
the problem domain previously mentioned. Therefore, the following databases
were used: , IEEE Xplore Digital Library, Scopus and ACM Digital
Library: (i)  is a platform with a considerable amount of
scientific papers, including high impact factor journals, along with the area
of computer vision and pattern recognition. According to the website, it hosts
around 2,500 journals and more than 39,000 books until the present time; (ii)
IEEE Xplore digital library is a database of scientific and technical content,
including journals, conferences and books. It?s supported by the IEEE
(Institute of Electrical and Electronics Engineers) and it?s publishing
partners. Accordingly to the maintainer, it is composed by more than four-
million full-text documents, including 195 + journals and 1,800 + conference
proceedings; (iii) Scopus is an abstract and citation database that contains
peer-reviewer literature such as journals, books and conferences proceedings.
It contains researches from the fields of science, technology, medicine,
social sciences, and arts and humanities; (iv) ACM Digital Library is a
database of full-text papers with focus in the computational field, including
journals, conferences and books, maintained by the Association for Computing
Machinery.
The access to the databases was performed through the CAPES Portal2. To avoid
ambiguity and redundancy among the literature, the databases used were limited
exclusively to the aforementioned sources, and other sources are out of the
scope of this work.
### 2.1. Search definition
Search definitions used to support our manuscript are presented according to
the Table 1. Firstly, the four databases used to initially select relevant
papers is shown at the first row, as well as the search terms corresponding to
facial expression recognition and facial emotion recognition. Inclusion and
exclusion criterion are presented in the second and third row, respectively.
These criterion are required to provide a second filter to the literature
review methodology, describing particular desired aspects to be presented in
the articles.
Table 1. Facial emotion - Search definition for our Literature Review.
**Search Bases**| **Search Terms**
---|---
_\- ACM Digital Library;_
_\- IEEE Xplore;_
_\- Science Direct;_
_\- Scopus._| _* facial expression recognition;_
_** facial emotion recognition;_
**Inclusion criteria**
\- Papers written in English language;
\- Papers published between 2006 and 2019;
\- Papers that present a facial expression/emotion recognition approach;\- Works with face images or sequence of images approaches to detect emotion;
**Exclusion Criteria**
\- Short papers, such as abstracts or expanded abstracts;
### 2.2. Search execution
After the initial selection of articles, a total of 117 manuscripts were
filtered. These articles were elected according to their title, abstracts,
keywords and images, as described by the literature review methodology. The
second filter applied over the articles was based on analyzing their entire
structure, which means, the documents were separated according to their
contents. Articles with no significant information or lack of important data
were discarded at this stage. Besides that, while reading carefully all the
articles, we were able to classify them in terms of employed technologies,
year of publication, dataset properties, considered expressions and result?s
accuracy. After this final stage, 51 articles remained. Since some of these 51
studies propose not a single method, but multiple solutions, a total of 94
methods where analyzed (Table 3).
## 3\. General computational flow for facial emotion recognition
In our work, the literature review has revealed interesting aspects indicating
for a general computational flow, which can be applied to solve the challenge
of recognizing human facial emotion from images or videos. This flow can be
illustrated in general terms according to the Fig. 1 as follows: (i) image or
data acquisition step: input images or video sequences are used from well-
established datasets or from proper image acquisition systems and
environmental sets, using bi-dimensional signals or combined with tree-
dimensional acquisition devices, such as kinect or sterescopic cameras; (ii)
image preparation (preprocessing) step: performed in order to improve image
quality, reduce noise, reduce dimensionality in terms of spatial resolution,
or by the simplification of color information into gray scale one; (iii)
feature selection/extraction step: aims to extract relevant features from the
input signal in order to discretize the input signal and make them unique for
each individual ? this step behaves like a signature or fingerprint for each
specific facial emotion; (iv) classification step: the previously obtained
features are used in some classification model, able to provide a similarity
or dissimilarity score for facial emotions, ranked according to the number of
recognizable expressions in the dataset; (v) results and validation procedure:
this step is often required since several models are based on a training
procedure, and a validation procedure is carried in order to compute a general
precision overall for the proposed approach. This last step is usually
performed against a well-known ground-truth information, provided by the
dataset and used to train and validate the computational recognition model
proposed.
The aforementioned steps illustrated by the Fig. 2 can be identified partially
or even totally for a large number of approaches. For each step block, the
algorithms used may vary according to the particularity of the problem
approached, as well as characteristics of the dataset used, and they can be
replaced in a tailored manner by another block method independently. In the
next sections the steps cited will be explained in details, providing a
general overview of the approaches proposed over the literature and its main
constructive algorithmic steps.
1. Download: Download high-res image (19KB)
2. Download: Download full-size image
Fig. 2. General Computational Flow revealed from the literature review, often
applied as a general problem-solver for the emotion recognition problem using
images or videos as input signals.## 4\. Preprocessing
Preprocessing step can be ignored for some approaches according to the dataset
quality and whether previous preparation was done or not. For some cases,
specially when a dataset containing raw images is used, preprocessing is a
very important operation [49], [68], which may be responsible for the success
or failure of a recognition system. Preprocessing has the objective to
slightly suppress some imperfections such as noise, reduce distortions and
highlight only the most important features for the data being analyzed.
### 4.1. Grayscale conversion
Besides gray scale conversion being a simple task, it has been demonstrated
that for some areas such as face recognition and emotion recognition color
information can be suppressed into a single dimension with no significant loss
of precision [76]. Additionally, processing color inputs still provides an
increment of computation power in the order of 3 times when compared to gray
scale images at least until (iii)-feature extraction step, where information
is usually invariant from color. On the other hand, nowadays almost every
single digital camera captures images and store them into proprietary format
or Bayer-8 (which in practice is uni-dimensional), and them convert it to
color information afterwards (usually RGB) [100].
Over the literature it can be observed that reducing the input signal from
color to gray scale is very common and effective, and at the same time
preserving the relevant features and mitigate performance issues for the
emotion recognition problem. The most common operation is the simply summation
and normalization of the resulting unidimensional signal. Other approaches use
the normalized grayscale conversion formula defined by the weights
0.2989,0.5870,0.1140, for red, green and blue channels, respectively. There
are other approaches where a specific channel is taken into account as the
dominant information thus generating a single channel image from it.
### 4.2. Face detection
The most relevant stage in recognizing facial expressions and emotions is the
initial selection of the region of interest (ROI) in the input image [28].
There are some effective algorithmic approaches able to find the region of
interest (the face region) despite the problematic perturbation that may
affects their performance. Normally, these kind of algorithm has to get around
some variations in the images, such as pose and illumination [5]. The
variations between different faces (subjects), in general, are less impacting
than environment ones, so some of these problems are usually solved with use
of preprocessing techniques.
Generally, raw images provided by most datasets, have a lot of background
information that has no use for expression and emotion classification.
Therefore, the most common preprocessing technique is to detect the subject?s
face and crop the area around it, discarding useless information and even
increasing its accuracy. In the context proposed by this work, the most
important preprocessing step is to detect a face inside the image that is
being analyzed. In this section, we describe some detection algorithms and how
they work with images to achieve what they are designed for.
#### 4.2.1. Viola-Jones
The algorithm developed by Paul Viola and Michael J. Jones in 2003 had a great
impact in the face detection area, because besides its great precision, it has
low computational cost and also allows to visualize the results. Although it
is still widely used, it needs a training step that can be slow, but after
that training, face detection is fast.
According to the authors [46], this technique is based on the characteristics
of Haar-like (used for pedestrian detection), which is given by the summationof pixels that are within the area of white rectangles and then subtracted
from the area of the gray rectangles. Although there are better filters, the
advantage offered by Haar over the offered efficiency turns out to be
advantageous according to [105], as shown in the Fig. 3-(a).
1. Download: Download high-res image (43KB)
2. Download: Download full-size image
Fig. 3. The most common approaches for facial localization. Viola Jones uses
rectangular Haar-like features as show in (a). Haar classifier rectangular
blocks are illustrated in (b).
The Viola-Jones detector performs face localization and has its constructive
algorithmic based in four steps:
* 1.
Selection of rectangular Haar-Like Features as demonstrated in the Fig. 3-(a);
* 2.
Creation of the integral image that allows quick calculations of the previous
mentioned Haar characteristics;
* 3.
Training of an AdaBoost-based classifier capable of selecting relevant
characteristics;
* 4.
Using cascade classifiers that rule out regions where a face is unlikely to
exist, and focus on regions likely to contain a face.
#### 4.2.2. Haar cascade
The Haar Cascade Classifier is an object detection method that inserts Haar
resources into a series of classifiers to identify objects in an image. Taking
this into account, Haar?s features are based on Haar waves, rather than the
usual image intensities. According to [92], this method was adopted and
developed by Viola and Jones [105], despite having been proposed by [74]. The
set of alternative resources was born due to the computational cost of
calculating resources in an image with RBG pixels.
A Haar-like feature considers adjacent rectangular regions at a specific
location in the detection window, sums up the pixel intensities in each region
and calculates the difference between these sums. The difference calculated
can be used then to identify subsections in the image. Haar classifiers must
be trained by giving lots of images (positive and negative in therms of
containing the object to be detected). With that done, features can be
extracted using sliding windows of blocks, as shown in Fig. 3-(b). These
blocks are almost like convolution matrices that are applied in different
portions of the picture to find partial matches.
In sequence, even being able to fast calculate the sum intensities using
integral images, there are lots of features extracted from the previous steps
that do not add any value to the result. If a feature is detected in a region
that is not of interest, it has no real value to the classifier. For that, a
boosting technique is applied in Haar Cascade classifier, called AdaBoost
[25]. These technique reduces the number of classifiers significantly, causing
no harm to the end result.
The Haar Cascade Classifier is composed by the following stages:
* 1.
A section of the image is selected as working region;
* 2.
The first stage evaluates the presence of a feature as previously detailed;
* 3.
If the previous step?s return has a positive signal, the classifier jumps to
the next stage and that goes until the last stage. On the other hand, if theprevious step?s return was a negative signal, another section of the image is
selected and the process restarts;
* 4.
Finally, if the image was entirely evaluated, the classifiers ends its
execution.
The cascade process aforementioned is illustrated by the Fig. 4. Besides this
approach is also used for several methods for object recognition and
detection, it has been shown very effective for the problem of facial
localization with a very good response in terms of execution time.
1. Download: Download high-res image (40KB)
2. Download: Download full-size image
Fig. 4. Haar classifier stages used for facial localization.
#### 4.2.3. Convolutional neural network
A Convolutional Neural Networks (CNN) are nowadays one of the main categories
when mentioning general problem-solvers for data mapping, image recognition
and classification, object detection and, clearly, face
localization/recognition. The deriving classifications provided by CNN [1]
occur when an image is taken and classifying it the following way: the method
sees the input image as a pixel matrix; this matrix will go through a series
of convolution filters, pooling and functions for example softmax, that
classifies an object with probabilistic values between 0 and 1; this way, with
a trained network, it is possible to utilize those functions to classify
images never ?seen? by the network before (zero bias) with a considerably well
accuracy. Section 7.2 will provide further details regarding the functioning
of a convolutional neural network. One CNN architecture that became very
popular due to high performance in face detection was the Multi-Task Cascaded
Convolutional Neural Network (MTCNN) [118]. Experiments performed on
benchmarks datasets of facial detection in 2016 demonstrated a performance
superior to all other algorithms tested so far. Several implementations of
this architecture are open for use through python libraries, further
popularizing the algorithm.
#### 4.2.4. Miscellaneous
Beyond the aforementioned methods, there are still plenty of other approaches
that could be potentially used to detect faces. Some of them are based on
classic computer vision approaches (such as image segmentation, among others),
but what is remarkable is that these technologies, besides obsolete nowadays,
were essential to the development of the current technologies. Some of these
miscellaneous technologies are describe below: At [116], three methods were
used simultaneously: joint cascade detection and alignment (JDA) that has
satisfactory performance with frontal detection, whereas the Deep Convolution
Neural Network (DCNN) has good performances with non-frontal faces and,
finally, a Mixture of Trees (MoT) took place in case the other two detectors
fail. Another procedure is adopted by [43] where two combined methods for face
detection are applied, HOG and a linear classifier. To run the recognizer in
real time, the face location is initiated by the face detector with the first
frame and uses the correlation tracker to track the face through the other
frames.
### 4.3. Dimensionality reduction
Attempting to reduce the amount of data to work on, most of the researchers
opt to apply some technique to reduce images dimensions and, therefore, reduce
time consumption and lower requirement for processing power. The literature
review clearly demonstrate that one of the most common and simple approach, is
the down sampling itself. Down sampling is, in fact, a downscale of images
with geometric transformation that is done without losing significant imagefeatures for the recognition procedure. Scripts with this purpose are also
very popular and enables researchers to process their images in machines with
memory limitations.
Linear Discriminant Analysis (LDA) is a technique very cited on the articles
reviewed by this survey. The main idea of LDA is to project some set of axis
on a new and smaller set of axis in such away that the separation between the
labels of the data is more evident. To do that, LDA uses two parameters, the
mean of each label on the new set of axis, and the variation of each label on
it. The algorithm maximizes the distance between the means and minimizes the
variation for each label.
Another widely known and accepted by the scientific community approach is the
use of Principal Component Analysis (PCA). PCA is a simple, non-parametric
method that intends to isolate relevant information from a large set of input
variables, using linear algebra?s analytical solutions to isolate the
irrelevant feature vectors and eliminate them since there is no significance
to keep them into account for the recognition procedure. With low effort, PCA
is capable of provide a roadmap for reducing a complex data set into
simplified structures that contain the important piece of information needed
[87].
Performing a brief analysis of the methods presented in the articles it can be
observed that among the ones that cite dimensionality reduction algorithms,
44% used PCA, 25% LDA and 25% Down Sampling. Only 27 methods out of the 94
analyzed mentioned explicitly if they used any dimensionality reduction
algorithm. An interesting fact found was that 77.78% of the uses of down
sampling were for methods that used neural networks as classifiers and 87.5%
of the methods that used the PCA algorithm had some classic algorithm as a
classifier.
## 5\. Feature Extraction
Feature extraction is a very important stage in recognition methods, being
mainly applied when each sample has a large amount of data. It has as main
goal to extract only the most important and descriptive piece of information
from the samples, getting rid of what is not relevant for the given problem.
This is usually necessary because of the high computational complexity of
training in classification models and due to the fact that the more data there
is in each sample, the more computing is needed to achieve good results on
these models. In general, it is very important that the extracted
characteristics are simple and independent from each other, that is, there is
not such a high correlation index between the sets of characteristics. Some
classical algorithms for extracting features from images are Active Shape
Model (ASM), Local Binary Patterns (LBP) and Histogram of Oriented Gradients
(HOG). There are many variations of these algorithms being used on the Facial
Emotion Recognition task to achieve distinct results [91]. In the last few
years there can be observed some works that used Convolutional Neural Networks
to extract features as well [4], [106].
The ASM algorithm was proposed in 1995 [14] with the intent of generating
points that fit the contour of the object. Both LBP and HOG work splitting the
image in small pieces called cells and generating histograms. LBP was proposed
in 1994 by [72] and functions making comparisons between each cell?s pixels
and their eight neighbors to build a binary number. Afterwards, the algorithm
frames a histogram of this numbers for each cell and concatenates the
histograms. More information about LBP can be found in [9]. The HOG algorithm,
on the other hand, instead of making comparison between pixels, makes a
histogram from the gradients previously calculated from them [17].
There are also feature extraction attempts based on the Facial Action CodingSystem (FACS) [26]. FACS is a coding system created in 1976 by Paul Ekman and
Wallace Friesen that measures the contraction and relaxation of facial muscles
with degrees of intensity. It has undergone some changes over time making it
more robust [24]. In general, works that try to find facial emotion using FACS
use specific dataset to train neural networks that have facial images
cataloged by FACS experts, so the feature extraction process is not
computational but human.
The FACS analyzes the emotions in the face, which are classified through the
movement of the facial musculature, which is coded by the system through AU -
Action Units, AD - Action Descriptors and M - Movements. Each AU, AD or M
corresponds to a code used to describe the specific movement of one or more
muscles of the face, eye direction or head movement. Fig. 5 shows the AU
classification.3
1. Download: Download high-res image (362KB)
2. Download: Download full-size image
Fig. 5. Facial Action Coding System (FACS).
## 6\. Datasets for emotion recognition from images or videos
Datasets containing facial images or videos specifically developed for emotion
recognition are not a novelty. The first studies are related to [82], and
since then the need to develop computational approaches to inspect emotion or
behavior in face images or videos has been significantly growing. As a
consequence, many facial expression datasets have emerged, varying in
acquisition environment, number of recognizable expressions, regions, among
other features [73]. On the other hand, due to the way some of these datasets
were created, many other points were also included over time such as gender,
ethnicity, age, image quality (size, color, older datasets performed a
digitization from physical photos), and number of participants. All these
variables tend to influence the quality of the chosen dataset, as well as
impact the results that computational approaches for emotion recognition
output.
Since datasets play a fundamental role for data-driven learning recognition
based on emotion recognition [86], this section aims to present a review of
the most important datasets for emotion recognition in images or videos,
recognized and widely used by the academical society. Additionally, here we
pointed relevant implementation details used for the construction of those
datasets, as well as its particularities an possible drawbacks. Fig. 6 shows
the relationship between the performance of the reviewed methods and the
datasets in which they were tested.
1. Download: Download high-res image (38KB)
2. Download: Download full-size image
Fig. 6. This plot was generated using only classes with more than 3 members.
The x axis represents the datasets used for testing the methods and the y axis
represents the accuracy of the methods. Each colored dot represents one
benchmarked method. It is important to emphasize that some of the approaches
considered for this study may have applied the same method to multiple
datasets.
### 6.1. JAFFE
The Japanese Female Facial Expression (JAFFE) [62] is a freely available
dataset, published in 1998 by a group of Japanese researchers from the ATR
Human Information Processing Research Laboratory and the Psychology Department
of Kyushu University. The dataset features images of 10 Japanese women,
expressing the following emotions: happiness, sadness, surprise, anger,
disgust, fear and neutral. Each participant took around 3 to 4 pictures of
herself for each emotion while looking through a semi-reflective plastic sheettowards the camera, totaling 219 images. The camera was also surrounded by a
black box to prevent and mitigate light reflections. The hair was removed from
the front of the face in a manner that the expressions were more evident.
Because the photographic process was performed analogously, the photos were
digitized afterward. The final digital resolution for each image is 256x256
pixels. Images were rated for each emotion by a group of 92 Japanese female
undergraduate on a 5-point scale. Each image has a 5 or 6 component vector
representing the average ratings for each emotion. Besides not recent, this
dataset has been used for several emotion recognition approaches, such as
[64], [3], [8].
### 6.2. Cohn-Kanade
Cohn-Kanade AU-Coded Expression Database (CK) [48] is a well-established
dataset, being widely used in the literature since it?s first version [112],
[94], [10], [57], [41]. This dataset was originally created with the name CMU-
Pittsburgh AU-Coded Facial Expression Image Database, and is being managed
ever since, by the (AAG) Affect Analysis Group of the Research Lab at the
University of Pittsburgh. Back in the day, the first version of CK dataset was
developed to fill the gap of large enough facial images sets to be able to
support studies of facial feature tracking and analysis methods. Nowadays, the
database counts with 2 versions: the first one was released in 2000 and
includes 486 sequences about 97 different posers. From the subjects, there
were 69% female, 31% male, 81% Euro-American, 13% Afro-American, and 6% other
groups.
The CK is composed by sequences that varies from neutral to a peak expression,
being the peak expression image fully FACS as presented in the previous
sections, coded and given an emotion label. However, this emotion label refers
to the expression that was requested, but not necessarily the one actually
expressed by the participant. In the first version of the database, images
were taken in a controlled room in terms of lights, background and camera
position. Besides that, all expressions were collected by asking subjects to
perform them, therefore, they may differ from spontaneously taken images. The
CK dataset provides 2D images with either 640x490 or 640x480 resolution and
pixel array with 8-bit gray-scale or 24-bit color values.
The second and the current latest version of the database was named The
Extended Cohn-Kanade Dataset (CK+) [60] and released in 2010. For this
version, there were included 107 sequences of emotion transitions and 26
subjects. Besides that, non-posed images were taken this time and each image
received a nominal emotion label based on the subject?s impression of each of
the 7 basic emotion categories, that is, Anger, Contempt, Disgust, Fear,
Happy, Sadness and Surprise. The authors (The Affect Analysis Group) is still
working in what should be the third version of this dataset, whose intention
is to mainly add synchronized 30-degree images from the original frontal
video, enabling a series of analysis such as 3D facial and emotion
recognition.
### 6.3. MMI
MMI is a dataset created from 25 people of different ethnicities (European,
Asian and South American), being 44% women and 56% men, with their age ranged
from 19 to 62 years old. The expressions were recorded/photographed in a
natural way, which means participants were induced to perform expressions
based on videos used to stimulate (comedy videos and disgusting content) a
most reliable natural condition. Papers [66], [16], [36] worked with this
dataset, and one of the points analyzed by one of authors was about the use of
glasses and the difficulty that ended up being high for the recognition of
emotions.Opposite to other databases that existed at the time when only 6 basic
expressions were parsed, MMI has many non-basic expressions and not defined
expressions, since they were performed during an expression exchange. The
images in the database are all real colors but after a digitization procedure
its resolution was downgraded to 720x576 pixels, and the videos were recorded
at 24 frames per second (sequences range from 40 to 520 frames), ranging from
neutral, expression and neutral again. In total, the dataset has one hour and
thirty-two minutes of content. The backgrounds on which the photos and videos
were captured is not the same for every sample. The dataset is accessible
online and easily searchable, and is freely available to the scientific
community.
### 6.4. FER2013
The FER2013 database was originally published in the International Conference
on Machine Learning (ICML in 2013) [31]. This set of pictures is publicly open
and was created for a project by Pierre-Luc Carrier and Aaron Courville. It
was publicly shared to be used in a facial expression recognition competition
in Kaggle, shortly named as ICML.
This dataset consists of 35.887 pictures of faces with 48x48 pixels in
grayscale, all images are labeled in seven facial expressions and distributed
as follows: 4953 angry images, 547 disgust images, 5121 fear images, 8989
happy images, 6077 sad images, 4002 surprise images and 6198 neutral images.
The dataset was developed using the Google image search API in which was
searched for face images that are within a set of 184 keywords related to
emotion, such as ?happy?, ?sad?, and others.
During the Kaggle?s competition, 28709 images from this dataset were shared
between the participants to train their neural network and 3589 images were
used to the test set and validation to figure out the winning recognition
algorithm on the competition. After the competition?s ending, the dataset were
made accessible to the general public. Besides the large number of images,
this dataset has a very reduced spatial resolution since its purpose is to be
used as input to train and test computational classifiers.
### 6.5. BU-3DFE
Binghamton University 3D Facial Expression (BU-3DFE) [114] was designed to
meet the need for a dataset with emotion-classified 3D facial images, thus
allowing its use for the facial expression recognition area. In 2006, the
publication date of the article ?The 3D Facial Expression Database for Facial
Behavior Research? [114] did not detect any other dataset that fulfilled this
role. This dataset is free for academic use. Images are linked to the
emotional states anger, disgusting, fear, happiness, sadness, surprise and
neutral. In addition, there is a degree of spontaneity associated with each
image. A system with 6 cameras positioned at different angles was used to
capture the images. The stereo photogrammetry technique was used to construct
the three-dimensional face. A total of 100 participants had their expressions
scanned, being 60 percent woman and 40 percent man, totalling 2500 3D models.
In addition, all participants were part of the psychology, arts and
engineering departments. Participants were asked to demonstrate the 7 emotions
with 4 degrees of intensity, low, middle, high and highest. There are
subsequent studies to BU-3DFE, such as BU-4DFE in which the time dimension was
added, BP4D-Spontaneous and BP4D +.
### 6.6. Miscellaneous
Besides the databases presented above, there are some datasets that are worth
mentioning, such as CASIA Webface [113], FEEDB [95], RaFD [54], National
Institute of Mental Health Child Emotional Faces Picture Set (NIMH-ChEFS)
[21], Taiwanese Facial Expression Image Database [11], Acted FacialExpressions In The Wild [20].
As a public large dataset composed from images collected from the web, [113]
contains almost five hundred thousand images from more than ten thousand
subjects. [95] is a dataset of video sequences, recorded using Microsoft
Kinect sensor and it was developed to serve as a dataset to be applied in
facial recognition and facial expression/emotions studies. [54] is a
initiative from the Behavioural Science Institute of the Radboud University
Nijmegen. It contains 8 different emotions from 67 models and it is freely
available for non-commercial scientific research by researchers who work for
an officially accredited university. The [21] is a little bit different from
the datasets presented above because it is composed by images of children
only. It contains 482 high quality colored images about five emotions and of
direct and averted gaze. [11] was developed by Brain Mapping Laboratory
(National Yang-Ming University) and Integrated Brain Research Unit (Taipei
Veterans General Hospital) and has 8 different kind so emotions from 40
models. The dataset is opened for scientific researches only. The [20] dataset
proposes a face image extraction from movies, that is, a out of the scope of
lab controlled datasets. The authors compare their dataset with [60], [62].
Over the internet, there are a lot of datasets with distinct characteristics
available, most of the time for free. Even though there is such a great number
of datasets, sometimes they are not exactly what researches are looking for.
Therefore, likewise all datasets cited, a great quantity of projects on facial
expression/emotion recognition prefer to use their own dataset, not being
possible to the public to use it.
## 7\. Classification algorithms
Besides all operations being of extreme importance to accomplish good results
when classifying emotions from face expressions, usually the most valued stage
in the whole process is the classification. This step is also the one with
most variations between works over the literature. As mentioned on Section 1,
there have been some arising methods based on neural network architectures
over the past few years, mainly because of the ascension of computer
capabilities. Taken the 51 works selected for this scope, we divided them into
two distinct groups: Classic and NNB approaches.
The first group refers to classical techniques used to categorize facial
emotion expressions, that is, traditional image processing, computer vision,
pattern recognition and misc classifiers, while the second one is composed
exclusively by neural network based approaches, including the CNN arising
methods.
Both groups will be presented with more details in the next subsections, such
as the algorithms included in each one of them.
### 7.1. Classical approaches
For many years, the classical approaches for image processing in general, were
the best attempt to solve some problems with high processing cost. Over time,
with the development of computer capability and the advent of new
architectures, new methods took place for multiple reasons, such as
implementation complexity and achievement of great results. However, some
methods (that we call classical in this paper) have achieved great results and
are still widely used nowadays. From the chosen papers for this literature
review, the classical approaches usage is shown in the Fig. 7. In the graph it
is possible to see that Support Vector Machine (SVM) is the flagship when
talking about classical classification methods, corresponding to 40% of the
approaches. The second mostly applied method is the Dynamic Bayesian Network
with only 10% of the appliances. The 13,3% corresponding to ?Others? is
composed by algorithms that were applied only once over the 51 selectedarticles. These methods are: extreme learning machine [3], extreme sparse
learning [88], support vector regression [120], sparse representation
classification [124], random forest [123], random tree [123], many graph
embedding [45] and a single modified Viola-Jones [51].
1. Download: Download high-res image (54KB)
2. Download: Download full-size image
Fig. 7. Scientific share for classical approaches verified for this literature
review.
#### 7.1.1. Support Vector Machines (SVM)
As it is possible to visualize in Fig. 7 that support vector machines (SVMs)
are by far the most applied classical method to classify emotions from face
images. SVMs are supervised learning methods, that is, algorithms capable of
generating input?output mapping functions from a specific set of labeled data
with one or more feature vectors. SVMs classify elements by determining
boundaries, known as hyperplane, that separate the classes from each other.
This hyperplane can be oriented in different ways and still fulfill it?s
purpose, however, SVM goal is to orient it in such a way that it is as far as
possible from the closest vector (called support vector) from each class [38].
Initially, these machine learning techniques were developed in 1963 to perform
classification in linearly separable sets of data [104], however, with the
assistance of nonlinear kernel functions, it is possible to transform input
data to a high-dimensional feature space in which the input data become
linearly separable and, therefore, classifiable by the SVM algorithm [108].
Because of it?s simplicity in therms of processing power needed (in comparison
with NNBs), SVM is a great tool for machine learning and, with the help of
nonlinear kernels, is capable of competing with the newest and most complex
methods in therms of results. Besides the face/emotion recognition, SVMs have
been used to solve classification of all sources over the past few years,
including the medical area and many others [122].
#### 7.1.2. Dynamic Bayesian Network
The Dynamic Bayesian Networks (DBNs) are Bayesian networks (BN) for processes
with changing environments. Therefore, to understand what a DBN is, it is
necessary to understand a BN. Thus, according to [89], Bayesian networks (BNs)
are directed acyclic graphs, which represent the probabilistic relationships
between a large number of variables. It is important to note that they work
with incomplete knowledge and allow machines to make inferences, predictions
and make decisions based on the probability of variables that assume some
states, solving the problem.
A DBN or temporal Bayesian network, as well as BNs, must be acyclic and
targeted. The difference is that DBN is a way of extending the BN to
distribute the probabilities across infinite collections of random variables.
Fig. 8 shows a DBN.
1. Download: Download high-res image (66KB)
2. Download: Download full-size image
Fig. 8. Dynamic Bayesian Network.
#### 7.1.3. Fuzzy Logic
The fuzzy logic concept was born in 1965 with the proposal of the fuzzy set
theory by Lofti Zadeh [117] with the objective of natural language processing.
The goal of fuzzy logic in general, is to change the very basis of boolean
computer processing, being able to produce results based on degrees of truth
instead of 1 or 0. In other words, it is based on the idea that people make
decisions over non-numerical information and, therefore, fuzzy is able to
imitate these kind of decision making that sometimes is not absolute. For face
emotion recognition, the fundamental operation is the selection of the mostimportant features to be analyzed (usually made by an specialist or based on
FACS) and the attribution of weights for each of these features. Besides that,
the human involved has to build the fuzzy rules base, which is responsible for
the evaluation of each sample and, therefore, each feature value may be
classified into the best matching set. The overall process of a fuzzy system
is presented in Fig. 9. Firstly, the raw input need to be fuzzyfied in order
to have truth based values instead of boolean ones, as discussed before. The
fuzzification process is done for every feature that is wanted to have some
weight in the decision making. In the inference step, each feature is analyzed
based on a previously determined set of rules with antecedents and
consequences. The rules set is the very intelligence in a fuzzy set thus the
determinative factor of a fuzzy system?s accuracy. Finally, the fuzzy output
needs to be defuzzified in order to serve as an applicable action for the
computer. Fuzzy logic have been used since it?s beginning mainly for control
systems for the most different areas [101].
1. Download: Download high-res image (33KB)
2. Download: Download full-size image
Fig. 9. Schematic representation of a Fuzzy system to solve the recognition
problem.
#### 7.1.4. Miscellaneous
As showed in Fig. 7, some methods different from the ones presented above,
were applied over the articles selected for these review. Since classical
approaches, for this paper, means every single method that is not based on
artificial neural network, there are a lot of particular methods that were
applied by less than four works and will not be detailed in this document. The
methods with unique occurrences were already mentioned in Section 7.1 and
besides them, some methods with three article usages were plotted in the
graph, they were: Adaboost, Naive Bayes (NB), K-Nearest Neighbors (KNN),
Hidden Markov Model, Euclidean Distance and Decision Tree. Adaboost, as
mentioned in Section 4.2.2, is in fact a booster and not a classification
technique and therefore, was used in three articles in combination with other
methods such as Support Vector Machines and Dynamic Bayesian Network. The
Naive Bayes is a probabilistic technique and special case of Bayesian theorem
that bases itself in particular features to classify an unknown sample. KNN is
a method that projects the sample in a feature space and decides the class of
the being-analyzed sampled based on the it?s k-nearest neighbours in the
space, for that, it uses Euclidean distance for measurements. The definition
of Hidden Markov Model states that it is a doubly stochastic process with a
hidden stochastic process that can only be visualized with another set of
processes [77]. The Euclidean Distance method basically consist of comparing
the euclidean distance of the being-analyzed sample with the training set. In
the Decision Tree method, the leafs of the tree represent each class to which
the samples must be classified. The other nodes of the tree, correspond to the
splitting rules. As a sample move throw the tree, starting from the root, it
will be classified to the best fitting class.
### 7.2. Neural Network Based Approaches (NNB)
The use of convolutional neural networks to solve computer vision problems has
been successful in various areas, including emotion processing in real time
[43]. It is important to remember that what is considered in this paper as
Neural Network Based Approaches include not only Convolutional Neural Networks
but also, as presented by the authors, Multi-layer Perceptrons and Neural
Networks. However, as it is visible in Fig. 10, that the predominant method
used in the set of articles selected for this work is the CCN approach, which
achieved over 65% of the NNBs applications. As we are working with NNBs, the?Neural Network? that composes more than 21% in Fig. 10 are in fact, as
reported by the authors as Neural Network in general. Therefore, the first
step is to understand what is in fact a neural network.
1. Download: Download high-res image (27KB)
2. Download: Download full-size image
Fig. 10. Usage of NNB approaches on the chosen set of papers. The yellow
portion in the figure represents the works that did not specify the
architecture of the Artificial Neural Network used and, therefore, were
considered as a generic Neural Networks.
Artificial neural networks (ANN) are an attempt to simulate (simplified, of
course, but still) what happens in human brain where electrical signals are
forwarded through nerve cells that are only capable of communicating with each
other, the neurons [33]. In other words, an artificial neural network is a set
of processing units called nodes, that simulates the neurons of an actual
brain and are interconnected through a set of values called weights [15].
Weights are the simulation of synaptic connections that happen between neurons
in the nervous system. This elements generally have variable parameters that
affect the signal that pass throw them and, together with nodes, composes the
basis of a ANN. The configuration of the network may vary depending on which
problem it will be designated for and different configurations result in
numerous types of ANNs [90]. Therefore, MLP and CNN are specific
configurations of ANNs and yet have different results on classifying facial
emotion expressions.
#### 7.2.1. Multi Layer Perceptron (MLP)
MLP is an artificial neural network architecture that uses a structure known
as hidden layers to increase the degree of accuracy of its classifications.
Hidden layers are layers of neurons that receive the outputs of the previous
layers multiplied by their respective connection weights, add them up, and
multiply by an activation function. The sigmoid function is one of the most
used activation functions and has its definition as:F(x)=11-e-xThe use of
hidden layers was only possible thanks to the development of training
algorithms for MLP, such as the classic Backpropagation algorithm.
Backpropagation is a way to solve the problem of finding how much the weights
of the network must be changed in order to the result to be closer to the
correct one. It uses the observation that data at any point in the neural
network is generated by a composite function and the derivative chain rule to
calculate the downward gradient and then understand how the weights should be
changed. The only problem with this technique is getting stuck in local
minimums and failing to converge to a global minimum. To try to avoid the
previous problem some dynamic learning rate heuristics are used. Genetic
algorithms are also possible for training an MLP.
#### 7.2.2. Convolutional Neural Netwoks (CNNs)
Writing about CNNs in a Section on classification algorithms can be a little
inaccurate. The reason for this is that although CNNs are classifiers they do
not only have this role within the pipeline established at the beginning of
this article, but the combination of the feature extraction step with the
classification step. Convolutional neural networks have emerged as an
alternative to extracting characteristics through human knowledge on a given
topic. The main idea of the algorithm is to use the convolution operation to
extract characteristics from the image, making the position information of the
pixel with its neighborhood more atomic for the classification process.
The first operation performed in the classification step by a CNN is the
convolution between matrices called filters and a sub space of the original
image, generating several new images of smaller size than the original one.There is one output image for each filter applied to the image. The data
generated by this process is called feature maps. The second step is to apply
nonlinearity to feature maps, where one of the common nonlinear functions is
Relu. The non-linear function is applied to each pixel of the feature maps,
creating rectified feature maps. The new set of images obtained could be
passed through the same two processes explained previously and this could be
repeated many times. An optional step at this point would be to add a pooling
layer to decrease the amount of data to be passed on to the next step. One of
the possible common pooling functions in the CNN architectures used by the
reviewed articles is max-pooling, which is basically characterized by dividing
the images of the feature maps into sub spaces and taking the maximum value of
the sub space as a representative of the entire subset. In the pipeline
defined earlier by this article, this step would be within the dimensionality
reduction box. After these preprocessing, a conventional MLP is normally used
to classify the data. The architectural pattern for CNNs can be seen in Fig.
11.
1. Download: Download high-res image (93KB)
2. Download: Download full-size image
Fig. 11. Common CNN Architecture and its feature extraction convolutional
pool.
## 8\. Emotion Recognition Methods from Facial Images or Videos
Due to the increasing use of NNB methods for the emotion classification (Fig.
1), these kind of methods?s performance was expected to be superior than the
classical methods, however, when analysing the accuracy of each method
proposed by the selected papers, it was possible to observe that, on average,
the applications that applied classical methods achieved better results (Fig.
12).
1. Download: Download high-res image (105KB)
2. Download: Download full-size image
Fig. 12. The plot was generated filtering our data with two rules. The first
one was take the combination of datasets for training that had at least two
members to represents the class. The second was to delete the classes of the
plot that has members only for neural based or classical algorithms.
However, looking closer to this information in the general flow of
computational facial emotion recognition presented on Section 3, it is
possible to identify that the comparison may not be trustworthy. When
observing accuracy achieved from both method, we are ignoring all the other
steps of the general flow, such as preprocessing, feature extraction and
database selection. Taking the method all alone, despite the other steps of
the flow, the accuracy may be better in classical applications, however, every
step should be analyzed to obtain a fair comparison.
Fig. 13 presents a complement on the information by Fig. 12. The datasets
usage is unbalanced in terms of classical and NNB approaches. The CK + dataset
(Section 6.2, which is the most used by the NNB methods, is a much wider
dataset than JAFFE. That may partially explain the accuracy difference between
classical and NNB approaches, giving motive to the expansion of NNB usage,
since they achieved great results while working with large amounts of data.
1. Download: Download high-res image (82KB)
2. Download: Download full-size image
Fig. 13. Usage of distinct datasets by both NNB and Classical approaches
reflects directly in the final accuracy of the method.
Since each part of the general flow may influence the final result and a
comparison between approaches is incomplete when looking into accuracy
exclusively, this section intends to present some methods applied by 10selected works, describing the complete process, from the dataset choice to
the classification technique. For this purpose, a rank of both classical and
NNB best accuracy papers for each principal dataset was designed (Table 2).
The complete data with the 94 methods analyzed is presented in Table 3.
Table 2. Top Accuracy(Acc) papers for each one of the most used datasets by
the papers in the study.
Dataset| Classification| Average Acc (%)| Best Acc (%)| Best Acc Paper(s)|
Year
---|---|---|---|---|---
Jaffe| Classical| 92,91| 100| [3]| 2015
| NNB| 86,00| 100| [3], [76]| 2015/ 2018
CK| Classical| 92,40| 100| [91]| 2018
| NNB| 87,37| 99,75| [3]| 2015
CK+| Classical| 83,88| 94,09| [34]| 2015
| NNB| 96,76| 99,68| [58]| 2017
FER 2013| NNB| 64,24| 64,24| [27]| 2018
Bu-3dfe| NNB| 91,89| 91,89| [58]| 2017
MMI| Classical| 87,73| 100| [66]| 2014
| NNB| 78,40| 78,40| [36]| 2019
Table 3. This table contains a total of 94 methods that were extracted from
the 51 studies selected for this literature review. Some cells are left blank
because of the lack of that specific information at the source papers.
Acc (%)| Classification| Test dataset| Training dataset| Ref| Classic (C) vs
NNB (N)| Facial detection| Other preprocessing algorithms| Feature extraction|
Dimensionality reduction
---|---|---|---|---|---|---|---|---|---
78.64| src| oulu-casia nir&vis| oulu-casia nir&vis| [124]| C| | | lbp;asm| adaboost
72.09| svm| oulu-casia nir&vis| oulu-casia nir&vis| [124]| C| | | lbp;asm| adaboost
87.50| svm| jaffe| jaffe| [107]| C| | | luxand facesdk| lda
93.75| naive bayes| jaffe + tfeid + radboud| jaffe + tfeid + radboud| [2]| C| viola-jones| | hog;lbp;nn| pca
52.00| bn| proprietary| proprietary| [123]| C| | | standard deviation skewness kurtosis correlation coeficient
mean of psd standard deviation of psd| pca
70.00| decision tree| proprietary| proprietary| [123]| C| | | standard deviation skewness kurtosis correlation
coeficient mean of psd standard deviation of psd| pca
49.00| random forest| proprietary| proprietary| [123]| C| | | standard deviation skewness kurtosis correlation
coeficient mean of psd standard deviation of psd| pca
42.00| random tree| proprietary| proprietary| [123]| C| | | standard deviation skewness kurtosis correlation
coeficient mean of psd standard deviation of psd| pca
36.00| svm| proprietary| proprietary| [123]| C| | | standard deviation skewness kurtosis correlation coeficient
mean of psd standard deviation of psd| pca
93.57| euclidian distance| jaffe| jaffe| [64]| C| | log-gabor filter| | pca
90.90| hmm| ck| ck| [112]| C| nn| | | pca
99.75| elm-rbf| ck| ck| [3]| N| | radon transform| | pca;lda
100| elm-rbf| jaffe| jaffe| [3]| N| | radon transform| | pca;lda
99.61| knn| ck| ck| [3]| C| | radon transform| | pca;lda
100| knn| jaffe| jaffe| [3]| C| | radon transform| | pca;lda
99.71| svm| ck| ck| [3]| C| | radon transform| | pca;lda
100| svm| jaffe| jaffe| [3]| C| | radon transform| | pca;lda
94.09| svm| ck+| ck+| [34]| C| viola-jones| lowpass filter| haar-cascade;lbp|
pca;lda;resize
92.00| svm| jaffe| jaffe| [34]| C| viola-jones| lowpass filter| haar-
cascade;lbp| pca;lda;resize
93.43| cnn| rafd| rafd| [115]| N| | cropping thresholding| | resize
61.29| cnn| fer2013| sfew| [116]| N| jda cnn mixture of trees| gray-scale histogram equalization| | resize91.89| cnn| bu-3dfe| bu-3dfe| [58]| N| | rotation correction intensity normalization opping| | resize
**Acc (%)**| **Classification**| **Test dataset**| **Training dataset**|
**Ref**| **Classic(C) vs NNB (N)**| **Facial detection**| **Other
preprocessing algorithms**| **Feature extraction**| **Dimensionality
reduction**
96.76| cnn| ck+| ck+| [58]| N| | rotation correction intensity normalization opping| | resize
86.74| cnn| jaffe| jaffe| [58]| N| | rotation correction intensity normalization opping| | resize
68.00| cnn| rafd| fer2013| [83]| N| haar cascade| | | resize
50.50| cnn| fer2013| imagenet| [70]| N| viola-jones| | | resize
84.55| svm| ck| ck| [94]| C| | | aam|
95.00| svm| ck| ck| [10]| C| viola-jones| | asm|
99.00| svm| ck| ck| [8]| C| viola-jones| | contourlet transform|
98.00| svm| jaffe| jaffe| [8]| C| viola-jones| | contourlet transform|
93.96| fuzzy| rafd| rafd| [40]| C| | | edge detection skin color detection|
76.60| svm| otcbvs| otcbvs| [35]| C| viola-jones| | evolutionary computation|
87.43| adaboost + dbn| ck+| ck+| [57]| C| | | gabor;asm|
82.40| adaboost + dbn| mmi| ck+| [57]| C| | | gabor;asm|
47.00| euclidian distance| feedb| feedb| [96]| C| | | heuristic|
85.05| cnn| ck+;kdef| ck+;kdef| [49]| N| viola-jones| gray-scale| hog|
56.07| mlp| ck+;kdef| ck+;kdef| [49]| N| viola-jones| gray-scale| hog|
63.55| svm| ck+;kdef| ck+;kdef| [49]| C| viola-jones| gray-scale| hog|
82.87| svm| ava| ava| [106]| C| | | hog;sift;cnn|
94.36| svm| cuhk| cuhk| [106]| C| | | hog;sift;cnn|
93.77| svm| live-iq| live-iq| [106]| C| | | hog;sift;cnn|
86.63| svm| pne| pne| [106]| C| | | hog;sift;cnn|
96.42| fuzzy| jaffe| jaffe| [28]| C| | | integral projection|
94.28| svm| proprietary| proprietary| [42]| C| kinect| | kinect|
54.56| cnn| casia webface| casia webface| [55]| N| | | lbp|
100| knn| ck| ck| [91]| C| | | lbp|
97.14| knn| jaffe| jaffe| [91]| C| | | lbp|
77.67| svm| bu-3dfe| bu-3dfe| [68]| C| viola-jones| cropping;segmentation|
lbp;gmm|
91.60| svm| ck| ck| [68]| C| viola-jones| cropping;segmentation| lbp;gmm|
88.10| svm| jaffe| jaffe| [68]| C| viola-jones| cropping;segmentation|
lbp;gmm|
80.00| svm| multi-pie| multi-pie| [68]| C| viola-jones| cropping;segmentation|
lbp;gmm|
78.80| fuzzy| jaffe| jaffe| [71]| C| | thresholding| matlab|
78.80| fuzzy| ebner?s| ebner?s| [23]| C| matlab| | matlab|
75.00| nn| ck| proprietary| [119]| N| | | nn|
95.00| svm| ck| ck| [61]| C| | | rcsr;lbp;hog|
96.40| svm| jaffe| jaffe| [61]| C| | | rcsr;lbp;hog|
87.00| decision tree| jaffe| jaffe| [75]| C| | | template matching|
93.24| cnn| ck+| ck+| [41]| N| | gn| |
95.23| cnn| jaffe| jaffe| [41]| N| | gn| |
68.79| cnn + svm;| fer2013| fer2013| [109]| CN| | rotation correction histogram equalization| |
87.72| adaboost| mmi| mmi| [66]| C| viola-jones| segmentation;thresholding| |
76.60| naive bayes| mmi| mmi| [66]| C| viola-jones| segmentation;thresholding| |
84.31| svm| mmi| mmi| [66]| C| viola-jones| segmentation;thresholding| |
90.00| dbn| dtu-aam ioid-facedatabase| dtu-aam ioid-facedatabase| [53]| C| aam| | |
80.90| cnn| emotiw| emotiw| [97]| N| cnn| | |
87.52| nn| jaffe| jaffe| [111]| N| haar cascade| | |
**Acc (%)**| **Classification**| **Test dataset**| **Training dataset**|
**Ref**| **Classic (C) vs NNB (N)**| **Facial detection**| **Otherpreprocessing algorithms**| **Feature extraction**| **Dimensionality
reduction**
69.77| cnn| kaggle dataset| kaggle dataset| [43]| N| hog;linear classifier| | |
100| cnn| jaffe| jaffe| [76]| N| viola-jones| | |
89.58| cnn| kdef| kdef| [76]| N| viola-jones| | |
71.9| cnn| kdef + jaffe + sfew| kdef + jaffe + sfew| [76]| N| viola-jones| | |
90.9| svm| ck+| ck+| [16]| C| viola-jones| | |
86.4| svm| mmi| mmi| [16]| C| viola-jones| | |
90.38| fcm| ck+| ck+| [120]| C| viola-jones modified| | |
74| viola-jones modified| proprietary| proprietary| [51]| C| viola-jones modified| | |
89| adaboost| ck| ck| [22]| C| | | |
76| adaboost| ck+| ck+| [22]| C| | | |
51.2| cnn| afew| afew + proprietary| [36]| N| | | |
93.9| cnn| ck+| afew + proprietary + ck+| [36]| N| | | |
73.6| cnn| ck+| affectnet| [99]| N| | | |
78| cnn| ck+| ck+| [13]| N| | | |
64.46| cnn| fer2013| fer2013| [27]| N| | | |
62| cnn| ised| affectnet| [99]| N| | | |
46.5| cnn| jaffe| affectnet| [99]| N| | | |
78.4| cnn| mmi| afew + proprietary + mmi| [36]| N| | | |
94.48| cnn| multi-pie| multi-pie| [50]| N| | | |
41.03| cnn| tfd;google dataset| afew| [47]| N| | | |
96| knn| proprietary| proprietary| [98]| C| | | |
95.24| mge| jaffe;mug;ck+| jaffe;mug;ck+| [45]| C| | | |
90| mlp| proprietary| proprietary| [98]| N| | | |
77.8| naive bayes| ck| ck| [22]| C| | | |
72.9| naive bayes| ck+| ck+| [22]| C| | | |
87| svm| ck| ck| [22]| C| | | |
75.5| svm| ck+| ck+| [22]| C| | | |
71| svm| proprietary| proprietary| [6]| C| | | |
### 8.1. Jaffe best accuracy classical approach
A classical approach was demonstrated in [3], where the Jaffe (Section 6.1)
and CK (Section 6.2) datasets were used. For preprocessing, a image crop was
applied on the datasets based on a particular model [85]. As feature
extraction techniques, the authors have chosen a quite complex processes.
Firstly, a radon transform is applied to turn lines through images into points
in the radon domain [30]. Besides that, to overcome the subtle changes,
complexity and variability of the non-linear features, a empirical mode
decomposition (EMD) based on the work by [37] was applied to minimize the
effects of noise. As described by the authors, these method is capable of
decomposing any complicated signal into oscillating component called intrinsic
mode functions (IMF). These IMFs on the other hand, reflect to signal?s local
characteristic features. With the signal extracted from the images, the
authors applied 3 independently algorithms of dimensionality reduction: PCA \+
Linear Discriminant Analysis (LDA) [18], PCA \+ Local Fisher Discriminant
Analysis (LFDA) [93], [110], [121], [78] and Kernel LFDA (KLFDA) [124].
After the previous process, the obtained features were still subjected to a
statistical analysis named ANOVA test, with the goal of selecting only the
significant features and then proceed to the classification step, in which the
authors have decided to apply KNN previously mentioned in Section 7.1.4 and
Gaussian SVM (Section 7.1.1) classifiers. Evaluating the model with the JAFFE
dataset, both classification algorithms were able to achieve 100% accuracy. On
the other hand, while using CK + dataset for evaluation, the KNN classifier
was able to achieve 99.31% accuracy and the SVM got the classification writeon 99,43% of the time.
### 8.2. Jaffe best accuracy NNB approaches
With the highest achieved accuracy between NNB methods, a CNN approach is
presented capable off reaching 100% precision [76]. Three datasets were used
for training in this work: Jaffe (previously presented on Section 6.1) with
213 images of 10 subjects, Karolinska Directed Emotional Faces (KDEF) Database
[29] with 4900 images of 70 subjects and Static Facial Expressions In The Wild
(SFEW) Database [19] with 700 images extracted from movies. As most of the
methods, this project applied grayscale conversion and downscaling with
geometric transformations algorithms (Sections 4.1 Grayscale conversion, 4.3
Dimensionality reduction). The authors have opted to use Viola-Jones (Section
4.2.1) to detect faces in the datasets images, and than crop them to reduce
even more the data inputted to the model, making them 256x256 pixels.
In order to achieve good results with CNN models, the training set must be
very large, much more than the sum of the images of each dataset chosen.
Therefore, a Synthetic Image Generation, as named by the authors, took place
with the objective of augmenting the set for training. This process was
responsible for generating new images by rotating the images already in the
datasets and, therefore, feeding the model with much larger set of data. This
technique is capable of augmenting the dataset and helping the CNN not to get
over fitted, but in the other hand, the number of subjects does not change
with this application and, therefore, it is still difficult for the model to
get god results when introduced to an ?unknown face?. With the CNN trained,
the authors claimed to achieve 100% precision when evaluating the model with
the JAFFE dataset and over 89% when using KDEF for evaluation.
### 8.3. Jaffe/Cohn-Kanade best accuracy NNB approaches
As for Jaffe dataset applied in a NNB method, [3] makes use of an extreme
learning machine (ELM) with radial basis function (RBF) kernel. As mentioned
by the authors, ELM with RBF kernel (ELM-RBF) is an extension of ELM from
single hidden layer feedforward neural network (SLFN) with additive neurons
case to SLFNs with RBF kernel. With the same method mentioned in the previous
paragraph, this approach was able to achieve 100% precision with the Jaffe
dataset and over 95% accuracy in CK + images.
### 8.4. Cohn-Kanade best accuracy classical approach
The work from [91] is a quite different from the ones mentioned preciously.
This paper has the main goal of reviewing 22 Local Binary Pattern (LBP)-like
descriptors and provide a comparison between then in the field of facial
expression recognition. The methods proposed in this context was a simple
parameter-free Nearest Neighbor (NN) classifier, since the real goal was to
evaluate the LBP variants in terms of their performance on extracting features
from the images. For the CK dataset, the authors claimed to achieve 100% score
with eight descriptors (AELTP, BGC3, CSALTP, dLNP ?, LDN, nLNPd, STS, and
WLD). Besides the great results achieved, the paper does not describe any pre
processing step or extremely complex process, just the LBP feature extractors
associated with the NN classifier did the job quite well.
### 8.5. Extended Cohn-Kanade best accuracy classical approach
The technique used by [34] was elaborated on top of classic techniques and was
tested in the CK + (Section 6.2) and JAFFE (Section 6.1) datasets. For the
authors attempted to understand the contribution of different areas in the
face for the expression recognition and, therefore, applied the following
method:
* 1.
Pre-processing - A low pass filter with 3x3 Gaussian mask to remove noise from
the source images was used;* 2.
Face detection - Viola-Jones with Adaboost learning for face detection is
applied to locate the face inside the picture;
* 3.
Detection of eyes and nose - The noise was located using the geometric
position of the face and Haar cascade. The eyes had a specific Haar classifier
trained for each one of them;
* 4.
Eyebrows and mouth detection - The position of the mouth was obtained from the
lower region of the nose, where a Sobel horizontal edge detector was applied
to detect the upper lip position. For the detection of the eyebrows, the
authors applied the same method as for the lips, however, with the subtle
change that is the performance of an adaptive threshold operation before
applying the operator.
* 5.
Extraction of active facial patches - Fixed regions of the face (based on the
eyes, nose?) were extracted since they might contain relevant information.
* 6.
Feature extraction - For this action, LBP was chose because of its robustness
in therms of illumination invariant feature description.
After extracting the important features from the pictures, a SVM algorithm was
used to classify the image into the best matching category (anger, disgust,
fear, happiness, sadness and surprise) and was capable of achieving a average
accuracy of 94,09%.
### 8.6. Extended Cohn-Kanade/BU-3DFE best accuracy NNB approach
The article with the best performance in the CK + dataset using CNN was
written by [58], in this article data sets that have a controlled environment
were tested, these being CK+ (6.2), JAFFE(6.1), BU-3DFE(6.5). The authors
opted to use some preprocessing techniques such as rotation correction, image
cropping, down sampling and intensity normalization (4) in order to have only
the meaningful data for the next steps and yet maintain a general standard for
the images. For the classification step, a CNN was used and for its input, the
images were configured in a 32x32 matrix of pixels. After the input, the the
network was composed by two convolution and subsampling sets and a fully
connected layer. The algorithm was developed to identify anger, disgust, fear,
happiness, sadness and surprise. The proposed method was capable of achieving
a average of 96,76 for CK + and 91,89 for BU-3DFE.
### 8.7. FER 2013 best accuracy NNB method
The best accuracy paper for the FER 2013 dataset is based on the usage of the
already known CNN architectures in the facial expression recognition problem,
these being GoogleNet, ResNet, VGGNet and AlexNet. Pre-processing is not
mentioned by the authors, instead focusing on the classification methods
themselves and the process of applying them. The authors decided to train the
models for different numbers of epochs: 6, 8, 12, 20 and 25. The top average
accuracy was 64,24 achieved by the AlexNet model trained for 20 epochs.
Besides being a very small number of epochs in comparison of other methods,
the authors claim that the model could be suffering from overfitting, since
the accuracy was better when training the model for 20 epochs than at the 25
epochs attempt.
### 8.8. MMI best accuracy classic method
As used by a great partition of the papers analyzed on this work, [66] uses
the Viola-Jones algorithm to detect faces in the images and, therefore, crop
them appropriately. Additionally, a skin detector based on texture is applied
to identify the non-face parts of the pictures and, finally, segment them bythis characteristic. Done with preprocessing, a facial graph is built
containing some interest point of the face. For the classification step, the
authors used three different classifiers, all of them based on FACS (5):
Adaboost, Naive Bayes and SVM. The algorithm was developed to classify the
following emotions: Surprise, Happiness, Disgust, Fear, Anger, Sadness and
Neutral. The best accuracy achieved was 100% with the SVM algorithm for the
neutral emotion, however, the best average accuracy across all the emotions
(87,73%) was achieved by the Adaboost classifier.
### 8.9. MMI best accuracy NNB method
The last paper of the Table 2 presents a very interesting and dissimilar
approach from the other works. [36] decided to analyze data from videos
instead of images as usual. For that, facial landmarks are extracted, facial
detection, alignment and input normalization are applied in the frames of the
videos and attributed to a Long Short Term Memory (LSTM) neural network. The
videos inputted to the LSTM are composed by 16 frames of 244 x 244 sized face
images and were achieved from image sources, that being AFEW, CK + and MMI.
The average accuracy for the MMI dataset was 78.40%.In addition to the
accuracy in the MMI dataset, the algorithm also achieves good results in the
AFEW and CK + datasets, with 51.2% and 93.9% respectively.
## 9\. Conclusion and discussions
Computational Facial emotion recognition as performed by humans can be
considered a very challenging task when using input images or videos solely.
Since face-based human emotion recognition is performed in a natural way as a
kind of non-verbal communication, it is also very dependent of other inputs to
be effective, such as a context of application itself, and sometimes it is
hard to be recognized by the available analytical ways. Besides its
complexity, finding computational mechanisms that are able to effectively
recognize emotion from facial images, is very desired in the most varied
fields with significant implications.
This survey intended to present a systematic literature review of the state-
of-the-art on emotion expression recognition from facial images, considering
the works published over the past few years. We categorized a total of 51
manuscripts, analyzed according to its main constructive concept, totaling 94
distinct approaches, where two main trends were able to be identified:
classical approaches and those based on neural networks approaches, including
the emerging convolutional counterpart. For the approaches based on Digital
Image Processing, Pattern Recognition and Computer Vision techniques, we named
them classical approaches. In essence, classical approaches consider those
based on a face detection procedure followed by some recognizer able to
interpret feature vector and made some decision according to a fixed number of
output classes. These approaches usually explore some _m_ -dimensional space
problem partition, looking for some discrimination among input vectors and
output clusters, such as the well-know Support Vector Machines (SVM), Naive
Bayes, k-Nearest Neighbors, Fuzzy, among others. On the other spectrum of
problem-solvers, we have the neural network based approaches (NNB approaches),
where the input layer containing a discretization of the facial image or more
commonly feature vectors extracted using some feature descriptor method is
used. NNB approaches include the emerging convolutional neural network
counterpart, and since its first use as computer vision solver-problems, it
was also used to the facial emotion recognition as well.
The papers analyzed here used a huge variety of techniques for the
construction of the emotion recognition algorithm, during the development of
this survey. This survey highlights several aspects provided by the published
papers over the literature. One of the most important, that arises someinterest of the general public, is to answer the question if a general
effective solution has been achieved to solve the facial emotion recognition
problem, which can be translated in terms of the general precision. In this
survey we can point to some interesting solutions, but the response is that
the problem can be considered solved for some very specific situations. The
most evident problem in terms of the published approaches is that some
variability can always be introduced to decrease the performance of the
proposed works. The use of controlled datasets or a very limited subset of
those images makes in fact the proposed approaches effective, but very
restricted to this specific domain. When applied in real-world problems, the
precision overall tends to decrease significantly. This aspect can be mainly
observed for the classical approaches when compared to the NNB ones, where a
marginally better precision can be observed mainly justified due to the nature
of the domain used. In general, what can be observed for classical approaches
is a very high specificity and low generality to solve the emotion recognition
problem. Here, we can point the approaches proposed by [91], [3], [8], where a
high-precision was achieved using classical approaches. For the classical
approaches, the best ranked approach was [91] achieving a precision of 100
percent using the 1 nearest neighbor for classification and local binary
patterns for feature extraction, and we computed a mean overall precision of
83.89 percent for the classical approaches group.
On the other hand, using NNB approaches, what can be observed is a discrete
decrease of precision when compared to classical approaches, more specifically
due the generality increase and the abstraction capability. However, the cost
is to consider a large input dataset to train the network properly. In terms
of precision, the best NNB approache have achieved a accuracy of 100 percent,
and we computed a general precision of 77,52 percent. As conclusion, when
comparing classical approaches vs NNB ones, despite the fact that NNB
approaches have a lower performance for the selected articles in this survey,
NNB methods provide the possibility of abstraction, applying facial emotion
recognition with larger datasets. This fact allows it to be applied in less
restricted scenarios than the classical methods, which requires greater
restrictions for their properly operation.
Another aspect verified in this survey is the importance of the dataset used
to train the classical and the NNB approaches. Traditional datasets such as
JAFFE are very common and explored over the past few years, besides its low
resolution, restrictions in size, and low variability in terms of nationality
of the participants and gender. We can observe the lack of reliable datasets
with good resolution, number of images, and including the most different cases
and variability, made with several countries and ethnic diversity. For
instance, a very small group is observed in the MMI dataset in which only 25
people were used. In addition, some datasets are obsolete, and the
aforementioned drawbacks makes the tested algorithms often restricted and they
end up showing good results only for a limited domain. Nonetheless, there is a
remarkable difference for the datasets constructed taking into account
artificial and natural expressions, where its hard to extract from a
participant the real meaning for a facial expression, specially those
dependent from a context. This survey suggest that datasets including natural
and artificial expressions must be developed in order to increase the overall
precision of the presented approaches.
Besides the great and interesting results achieved by some of the analyzed
approaches, this study showed that the research still face difficulties with
some aspects of this type of problem, especially for reliable datasets,
controlled environments, due to the variation of luminosity, occlusion, amongothers. NNB approaches have achieved a very interesting capacity of modelling
the nature of the problem, and what can be observed is a trend for the further
works explore the capabilities of NNB approaches.
## Declaration of Competing Interest
The authors declare that they have no known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.
## Acknowledgments
This work was carried out with the support of the Conselho Nacional de
Desenvolvimento Científico e Tecnológico (CNPq).
Recommended articles"
17,17,"A systematic review on affective computing: Emotion models, databases, and recent advances","['Y Wang', 'W Song', 'W Tao', 'A Liotta', 'D Yang', 'X Li', 'S Gao']",2022,315,Affective Faces Database,"CNN, FER, classification, classifier, deep learning, machine learning, neural network","affective computing, we provide a systematical survey of important components: emotion  models, databases,  by five kinds of commonly used databases for affective computing. Next, we",No DOI,Information …,https://www.sciencedirect.com/science/article/pii/S1566253522000367,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: A systematic review on affective computing: emotion
models, databases, and recent advances
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related works
7. 3\. Emotion models
8. 4\. Databases for affective computing
9. 5\. Unimodal affect recognition
10. 6\. Multimodal affective analysis
11. 7\. Discussions
12. 8\. Conclusion and new developments
13. CRediT authorship contribution statement
14. Declaration of Competing Interest
15. Acknowledgement
16. References
Show full outline
## Cited by (223)
## Figures (6)
1. 2. 3. 4. 5. 6.
## Tables (13)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Information Fusion
Volumes 83?84, July 2022, Pages 19-52
# A systematic review on affective computing: emotion models, databases, and
recent advances
Author links open overlay panelYan Wang a, Wei Song c, Wei Tao a, Antonio
Liotta d, Dawei Yang a, Xinlei Li a, Shuyong Gao b, Yixuan Sun a, Weifeng Ge
b, Wei Zhang b, Wenqiang Zhang a b
Show more
Outline
Add to MendeleyShare
Cite
https://doi.org/10.1016/j.inffus.2022.03.009Get rights and content
## Highlights
* ?
Review on affective computing by single- and multi-modal analysis with
benchmark databases.
* ?
In the retrospect of 19 relevantly latest reviews and 350+ researches papers
by 2021.
* ?
Taxonomy of state-of-the-art methods considering ML-based or DL-based
techniques.
* ?
Comparative summary of the properties and quantitative performance of key
methods.
* ?
Discuss problems as well as some potential factors on affect recognition and
analysis.
## Abstract
Affective computing conjoins the research topics of emotion recognition and
sentiment analysis, and can be realized with unimodal or multimodal data,
consisting primarily of physical information (e.g., text, audio, and visual)
and physiological signals (e.g., EEG and ECG). Physical-based affect
recognition caters to more researchers due to the availability of multiple
public databases, but it is challenging to reveal one's inner emotion hidden
purposefully from facial expressions, audio tones, body gestures, etc.
Physiological signals can generate more precise and reliable emotional
results; yet, the difficulty in acquiring these signals hinders their
practical application. Besides, by fusing physical information and
physiological signals, useful features of emotional states can be obtained to
enhance the performance of affective computing models. While existing reviews
focus on one specific aspect of affective computing, we provide a systematical
survey of important components: emotion models, databases, and recent
advances. Firstly, we introduce two typical emotion models followed by five
kinds of commonly used databases for affective computing. Next, we survey and
taxonomize state-of-the-art unimodal affect recognition and multimodal
affective analysis in terms of their detailed architectures and performances.
Finally, we discuss some critical aspects of affective computing and its
applications and conclude this review by pointing out some of the most
promising future directions, such as the establishment of benchmark database
and fusion strategies. The overarching goal of this systematic review is to
help academic and industrial researchers understand the recent advances as
well as new developments in this fast-paced, high-impact domain.
* Previous article in issue
* Next article in issue
## Keywords
Affective computing
Machine learning
Deep learning
Feature learning
Unimodal affect recognition
Multimodal affective analysis
## 1\. IntroductionAffective computing is an umbrella term for human emotion, sentiment, and
feelings [1], emotion recognition, and sentiment analysis. Since the concept
of affective computing [2] was proposed by Prof. Picard in 1997, it has been
guiding computers to identify and express emotions and respond intelligently
to human emotions [3]. In many practical applications, it is desired to build
a cognitive, intelligent system [4] that can distinguish and understand
people's affect, and meanwhile make sensitive and friendly responses promptly
[5,6]. For example, in an intelligent vehicle system, the real-time monitoring
of the driver's emotional state and the necessary response based on the
monitored results can effectively reduce the possibility of accidents [7,8].
In social media, affective computing can avidly help to understand the
opinions being expressed on different platforms [9]. Thus, many researchers
[3,10] believe that affective computing is the key to promoting and advancing
the development of human-centric AI and human intelligence.
Affective computing involves two distinct topics: emotion recognition and
sentiment analysis [11], [12], [13], [14]. To understand and compute the
emotion or sentiment, psychologists proposed two typical theories to model
human emotion: discrete emotion model (or categorical emotion model) [15] and
dimensional emotion model [16]. The emotion recognition aims to detect the
emotional state of human beings (i.e., discrete emotions or dimensional
emotions) [17], and mostly focuses on visual emotion recognition (VER) [18],
audio/speech emotion recognition (AER/SER) [19], and physiological emotion
recognition (PER) [20]. In contrast, sentiment analysis mostly concentrates on
textual evaluations and opinion mining [21] on social events, marketing
campaigns, and product preferences. The result of sentiment analysis is
typically positive, negative, or neutral [10,22]. Considering that one person
in a happy mood typically has a positive attitude toward the surrounding
environment, emotion recognition and sentiment analysis can be overlapped. For
example, a framework based on the context-level inter-modal attention [23] was
designed to predict the sentiment (positive or negative) and recognize
expressed emotions (anger, disgust, fear, happiness, sadness, or surprise) of
an utterance.
Recent advances in affective computing have facilitated the release of public
benchmark databases, mainly consisting of unimodal databases (i.e., textual,
audio, visual, and physiological databases) and multimodal databases. These
commonly used databases have, in turn, motivated the development of machine
learning (ML)-based and deep learning (DL)-based affective computing.
A study [24] revealed that human emotions are expressed mainly through facial
expressions (55%), voice (38%), and language (7%) in daily human
communication. Throughout this review, textual, audio, and visual signals are
collectively referred to as physical data. Since people tend to express their
ideas and thoughts freely on social media platforms and websites, a large
amount of physical affect data can be easily collected. Based on these data,
many researchers pay attention to identifying subtle emotions expressed either
explicitly or implicitly [25], [26], [27]. However, physical-based affect
recognition may be ineffective since humans may involuntarily or deliberately
conceal their real emotions (so-called social masking) [20]. In contrast,
physiological signals (e.g., EEG and ECG) are not subject to these constraints
as spontaneous physiological activities associated with emotions are hardly
changed by oneself. Thus, EEG-based or ECG-based emotion recognition can
generate more objective predictions in real time and provide reliable features
of emotional states [[28], [29],].
Human affect is a complex psychological and physiological phenomenon [30]. As
human beings naturally communicate and express emotion or sentiment throughmultimodal information, more researches focus on multi-physical modality
fusion for affective analysis [31]. For example, in a conversation scenario, a
person's emotional state can be demonstrated by the words of speech, the tones
of voice, facial expressions and emotion-related body gestures [32]. Textual,
auditory and visual information together provide more information than they do
individually [33], just like the brain validating events relies on several
sensory input sources. With the rapid progress of physical-touching mechanisms
or intrusive techniques such as low-cost wearable sensors, some emotion
recognition methods are based on multimodal physiological signals (e.g., EEG,
ECG, EMG and EDA). By integrating physiological modalities with physical
modalities, physical-physiological affective computing can detect and
recognize subtle sentiments and complex emotions [34,35]. It is worth
mentioning that the suitable selection of the unimodal emotion data and
multimodal fusion strategies [36] are the two key components of multimodal
affective analysis systems, which often outperform the unimodal emotion
recognition systems [37]. Therefore, this review not only provides a summary
of the multimodal affective analysis, but also introduces an overview of
unimodal affect recognition.
Although there exist many survey papers about affective computing, most of
them focus on physical-based affect recognition. For facial expression
recognition (FER), existing studies have provided a brief review of FER [38],
DL-based FER systems [39], facial micro-expressions analysis (FMEA) [40], and
3D FER [41]. Besides, the work [42] discussed the research results of the
sentiment analysis using transfer learning algorithms, whereas authors [43]
surveyed the DL-based methods for SER. However, there are just a few review
papers related to physiological-based emotion recognition and physical-
physiological fusion for affective analysis in recent years. For example,
Jiang et al. [17] discussed multimodal databases, feature extraction based on
physical signals or EEG, and multimodal fusion strategies and recognition
methods.
Several issues have not been thoroughly addressed in previous reviews: 1)
Existing reviews take a specialist view and lack a broader perspective, for
instance when classifying the various methods and advances, some reviews do
not consider DL-based affect recognition or multimodal affective analysis; 2)
Existing reviews do not provide a clear picture about the performance of
state-of-the-art methods and the implications of their recognition ability. As
the most important contribution of our review, we aim to cover different
aspects of affective computing by introducing a series of research methods and
results as well as discussions and future works.
To sum up, the major contributions of this paper are multi-fold:
* 1)
To the best of our knowledge, this is the first review that categorizes
affective computing into two broad classes, i.e., unimodal affect recognition
and multimodal affective analysis, and further taxonomizes them based on the
data modalities.
* 2)
In the retrospect of 20 review papers released between 2017 and 2020, we
present a systematic review of more than 380 research papers published in the
past 20 years in leading conferences and journals. This leads to a vast body
of works, as taxonomized in Fig. 1, which will help the reader navigate
through this complex area.
1. Download: Download high-res image (1MB)
2. Download: Download full-size image
Fig. 1. Taxonomy of affective computing with representative examples.* 3)
We provide a comprehensive taxonomy of state-of-the-art (SOTA) affective
computing methods from the perspective of either ML-based methods or DL-based
techniques and consider how the different affective modalities are used to
analyze and recognize affect.
* 4)
Benchmark databases for affective computing are categorized by four modalities
and video-physiological modalities. The key characteristics and availability
of these databases are summarized. Based on publicly used databases, we
provide a comparative summary of the properties and quantitative performance
of some representative methods.
* 5)
Finally, we discuss the effects of unimodal, multimodal, models as well as
some potential factors on affective computing and some real-life applications
of that, and further indicate future researches on emotion recognition and
sentiment analysis.
The paper is organized as followed. Section 2 introduce the existing review
works, pinpointing useful works and better identifying the contributions of
this paper. Section 3 surveys two kinds of emotion models, the discrete and
the dimensional one. Then Section 4 discusses in detail four kinds of
databases, which are commonly used to train and test affective computing
algorithms. We then provide an extensive review of the most recent advances in
affective computing, including unimodal affect recognition in Section 5 and
multimodal affective analysis in Section 6. Finally, discussions are presented
in Section 7, while conclusions and new developments are stated in Section 8.
Table 1 lists the main acronyms used herein for the reader's reference.
Table 1. Main acronyms.
Acronym| Full Form| Acronym| Full Form
---|---|---|---
TSA| Textual Sentiment Analysis| SER| Speech Emotion Recognition
FER| Facial Expression Recognition| FMER| Facial Micro-Expression Recognition
4D/3D FER| 4D/3D Facial Expression Recognition| EBGR| Emotional Body Gesture
Recognition
EEG| Electroencephalogram| ECG| Electrocardiography
EMG| Electromyography| EDA| Electro-Dermal Activity
ML| Machine Learning| DL| Deep Learning
GMM| Gaussian Mixture Model| MLP| Multi-Layer Perceptron
NB| Naive Bayesian| LSTM| Long- Short-Term Memory
LDA| Linear Discriminant Analysis| DCNN| Deep Convolutional Neural Network
DT| Decision Tree| CNN| Convolutional Neural Network
KNN| K-Nearest Neighbors| RNN| Recurrent Neural Network
HMM| Hidden Markov Model| GRU| Gated Recurrent Unit
ANN| Artificial Neural Network| AE| Auto-encoder
PCA| Principal Component Analysis| GAN| Generative Adversarial Network
MLP| Multi-layer Perceptron| VGG| Visual Geometry Group
SVM| Support Vector Machine| DBN| Deep Belief Network
RBM| Restricted Boltzmann Machine| HAN| Hierarchical Attention Network
RBF| Radial Basis Function| ResNet| Residual Networks
FC| Full-connected| GAP| Global Average Pooling
MKL| Multiple Kernel Learning| AUs| Action Units
RF| Random Forest| AAM| Active Appearance Model
ICA| Independent Component Analysis| LFPC| Logarithmic Frequency Power
Coefficient
BoW| Bag-of-Words| ROIs| Regions of InterestLBP-TOP| Local Binary Pattern from Three Orthogonal Planes| MFCC| MEL
Frequency Cepstrum Coefficient
## 2\. Related works
In this section, we introduce the most relevant reviews related to affective
computing published between 2017 and 2020 in the perspective of affect
modalities (i.e., physical modality, physiological modality, and physical-
physiological modality). Table 2 shows an overview of these latest reviews,
compared with our survey. The comparison includes publication year (Year),
emotion model, database (DB), modality, multimodal fusion, method, and
quantitative evaluation (QE). According to Table 2, our review systematically
covers all aspects of affective computing.
Table 2. Overview of the most relevant reviews related to affective computing
published between 2017 and 2020 and our proposed review.
Author| Year| a Emotion Model| DB| b Modality| c Multimodal Fusion| Method| QE
---|---|---|---|---|---|---|---
Dis| Dim| T| A| V| P| VA| TA| VAT| M-P| P-P| ML| DL
** _Reviews on physical-based Affect Recognition_**
Ko [38]| 2018| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Patel et al. [44]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Li et al. [39]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Alexandre et al. [41]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Merghani et al. [40]| 2018| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Noroozi et al. [45]| 2018| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Liu et al. [42]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Poria et al. [46]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Yue et al. [47]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Khalil et al. [43]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Wang et al. [48]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Han et al. [49]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Erik et al. [12]| 2017| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
** _Reviews on physiological-based Emotion Recognition_**
Bota et al. [50]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
d G-M et al. [51]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
e Ala. and Fon. [29]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
** _Reviews on physical-physiological fusion for affective analysis_**
Rouast et al. [13]| 2019| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Zhang et al. [20]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Jiang et al. [17]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
Shoumy et al. [14]| 2020| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
** _Proposed review_**
Our proposed| 2021| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?| ?
a
Emotion Model: Dis=Discrete, Dim=Dimensional;
b
Modality: V=Visual (Facial expression, Body gesture), A=Audio (Speech),
T=Textual, and P= Physiological (e.g., EEG, ECG, and EMG);
c
Multimodal Fusion: VA=Visual-Audio, TA=Textual-Audio, VAT=Visual-Audio-
Textual, M-P=Multi-Physiological, P-P=Physical-Physiological.
d
G-M=Garcia-Martinez.
e
Ala. and Fon.=Alarcão and Fonseca;
### 2.1. Reviews on physical-based affect recognitionThe existing researches on physical-based affect recognition mainly used
visual, textual, and audio modalities [12]. For visual modality, most studies
surveyed FER [38,39,41,44] and FMEA [40], and 3D FER [41]. Besides, Noroozi et
al. [45] reviewed representation learning and emotion recognition from the
body gestures followed by multimodal emotion recognition on the basis of
speech or face and body gestures. For textual modality, the works on sentiment
analysis and emotion recognition [42,43,[46], [47], [48]] can be completed
according to the implicit emotions in the conversation. Han et al. [49]
presented DL-based adversarial training using three kinds of physical signals,
aiming to address various challenges associated with emotional AI systems. In
contrast, Erik et al. [12] considered multimodal fusion and provided a
critical analysis of potential performance improvements with multimodal affect
recognition under different fusion categories, compared to unimodal analysis.
All reviews on physical-based affect recognition listed in Table 2 have
reviewed DL-based methods, which indicates a clear development trend in the
application of deep learning for this domain. However, the existing works have
not fully involved the latest research advances and achievements in DL-based
affective computing, which is one of the objectives of our review.
### 2.2. Reviews on physiological-based emotion recognition
The emerging development of physiological emotion recognition has been made
possible by the utilization of embedded devices for the acquisition of
physiological signals. In 2019, Bota et al. [50] reviewed ML-based emotion
recognition using different physiological signals, along with the key
theoretical concepts and backgrounds, methods, and future developments.
Garcia-Martinez et al. [51] reviewed nonlinear EEG-based emotion recognition
and identified some nonlinear indexes in future research. Alarcão and Fonseca
[29] also reviewed works about EEG-based emotion recognition from 2009 to
2016, but focused on subjects, feature representation, classification, and
their performances.
All reviews on physiological-based emotion recognition listed in Table 2 have
reviewed the ML-based methods in discrete and dimensional emotion space, but
only one work involved a few DL-based methods. In this review, we surveyed ML-
based and DL-based works which have contributed to the advance of
physiological-based emotion recognition.
### 2.3. Reviews on physical-physiological fusion for affective analysis
There is a clear trend of using both physical and physiological signals for
affective analysis. In 2019, Rouast et al. [13] reviewed 233 DL-based human
affective recognition methods that use audio-visual and physiological signals
to learn spatial, temporal, and joint feature representations. In 2020, Zhang
et al. [20] introduced different feature extraction, feature reduction, and
ML-based classifiers in terms of the standard pipeline for multi-channel EEG
emotion recognition, and discussed the recent advances of multimodal emotion
recognition based on ML or DL techniques. Jiang et al. [17] summarized the
current development of multimodal databases, the feature extraction based on
EEG, visual, audio and text information, multimodal fusion strategies, and
recognition methods according to the pipeline of the real-time emotion health
surveillance system. Shoumy et al. [14] reviewed different frameworks and
lasted techniques using textual, audio, visual and physiological signals, and
extensive analysis of their performances. In the end, various applications of
affective analysis were discussed followed by their trends and future works.
All these recent works listed in Table 2 have reviewed general aspects related
to affective computing including emotion models, unimodal affect recognition
and multimodal fusion for affective analysis as well as ML-based and DL-based
models. However, the existing works have not fully elaborated theircomparisons in unimodal and multimodal affective analysis, which is one of the
objectives of our review.
## 3\. Emotion models
The definition of the emotion or affect is essential to establish a criterion
for affective computing. The basic concept of emotions was first introduced by
Ekman [52] in the 1970s. Although psychologists attempt to classify emotions
in different ways in the multidisciplinary fields of neuroscience, philosophy,
and computer science [53], there are no unanimously accepted emotion models.
However, there are two types of generic emotion models in affective computing,
namely discrete emotion model [52] and dimensional emotion model (or
continuous emotion model) [16,54].
### 3.1. Discrete emotion model
The discrete emotion model, also called as categorical emotion model, defines
emotions into limited categories. Two widely used discrete emotion models are
Ekman's six basic emotions [52] and Plutchik's emotional wheel model [55], as
shown in Fig. 2 (a) and Fig. 2 (b), respectively.
1. Download: Download high-res image (741KB)
2. Download: Download full-size image
Fig. 2. Two discrete emotion models for affective computing. (a) 6 basic
emotion models [15] shown in emoji types and (b) componential models
(Plutchik's emotional wheel model) [55].
Ekman's basic emotion model and its variants [56,57] are widely accepted by
the emotion recognition community [58,59]. Six basic emotions typically
include anger, disgust, fear, happy, sad, and surprise. They were derived with
the following criteria [52]: 1) Basic emotions must come from human instinct;
2) People can produce the same basic emotions when facing the same situation;
3) People express the same basic emotions under the same semantics; 4) These
basic emotions must have the same pattern of expression for all people. The
development of Ekman's basic emotion model is based on the hypothesis that
human emotions are shared across races and cultures. However, different
cultural backgrounds may have different interpretations of basic emotions, and
different basic emotions can be mixed to produce complex or compound emotions
[15].
In contrast, Plutchik's wheel model [55] involves eight basic emotions (i.e.,
joy, trust, fear, surprise, sadness, anticipation, anger, and disgust) and the
way of how these are related to one another (Fig. 2 (b)). For example, joy and
sadness are opposites, and anticipation can easily develop into vigilance.
This wheel model is also referred to as the componential model, where the
stronger emotions occupy the centre, while the weaker emotions occupy the
extremes, depending on their relative intensity levels. These discrete
emotions can be generally categorized into three kinds of polarity (positive,
negative, and neutral), which are often used for sentiment analysis. To
describe fine-grained sentiments, ambivalent sentiment handling [60] is
proposed to analyze the multi-level sentiment and improve the performance of
binary classification.
### 3.2. Dimensional emotion model
To overcome the challenges confronted by the discrete emotion models, many
researchers have adopted the concept of a continuous multi-dimensional model.
One of the most recognized models is the Pleasure-Arousal-Dominance (PAD)
[16], as shown in Fig. 3 (a).
Similar to Mehrabian´s three-dimensional space theory of emotion [61], the PAD
model has three-dimensional spaces: 1) Pleasure (Valence) dimension,
representing the magnitude of human joy from distress extreme to ecstasies; 2)
Arousal (Activation) dimension, measuring physiological activity andpsychological alertness level; 3) Dominance (Attention) dimension, expressing
the feeling of influencing the surrounding environment and other people, or of
being influenced by the surrounding environment and others.
Since two dimensions of Pleasure and Arousal in the PAD model could represent
the vast majority of different emotions [62], Russell [54] proposed a Valence-
Arousal based circumplex model to represent complex emotions. This model
defines a continuous, bi-dimensional emotion space model with the axes of
Valence (the degree of pleasantness or unpleasantness) and Arousal (the degree
of activation or deactivation), as shown in Fig. 3 (b). The circumplex model
consists of four quadrants. The first quadrant, activation arousal with
positive valence, shows the feelings associated with happy emotions; And the
third quadrant, with low arousal and negative valence, is associated with sad
emotions. The second quadrant shows angry emotions within high arousal and
negative valence; And the fourth quadrant shows calm emotion within low
arousal and positive valence [63].
## 4\. Databases for affective computing
Databases for affective computing can be classified into textual,
speech/audio, visual, physiological, and multimodal databases according to the
data modalities. The properties of these databases have a far-reaching
influence on the model design and network architecture for affective
computing.
### 4.1. Textual databases
Databases for TSA consist of the text data in different granularities (e.g.,
word, sentence, and document), labelled with emotion or sentiment tags
(positive, negative, neutral, emphatic, general, sad, happy, etc). The
earliest textual sentiment database is **Multi-domain sentiment (MDS)**
[64,65]**,** which contains more than 100,000 sentences of product reviews
acquired from Amazon.com. These sentences are labelled with both two sentiment
categories (positive and negative) and five sentiment categories (strong
positive, weak positive, neutral, weak negative, strong negative).
Another widely-used large database for binary sentiment classification is
**IMDB** [66]. It provides 25,000 highly polar movie reviews for training and
25,000 for testing. Stanford sentiment treebank (SST) [67] is the semantic
lexical database annotated by Stanford University. It includes fine-grained
emotional labels of 215,154 phrases in a parse tree of 11,855 sentences, and
it is the first corpus with fully labelled parse trees.
### 4.2. Speech/Audio databases
Speech databases can be divided into two types: non-spontaneous (simulated and
induced) and spontaneous. In the early stage, non-spontaneous speech databases
were mainly generated from professional actors? performances. Such
performance-based databases are regarded as reliable ones because they can
perform well-known emotional characteristics in professional ways. **Berlin
Database of Emotional Speech (Emo-DB)** [68] contains about 500 utterances
spoken by 10 actors (5 men and 5 women) in a happy, angry, anxious, fearful,
bored and disgusting way. However, these non-spontaneous emotions can be
exaggerated a little more than the real emotions. To narrow this gap,
spontaneous speech databases have been developed recently. **Belfast Induced
Natural Emotion (Belfast)** [69] was recorded from 40 subjects (aged between
18 and 69, 20 men and 20 women) at Queen University in Northern Ireland, UK.
Each subject took part in five tests, each of which contains short video
recordings (5 to 60 seconds in length) with stereo sound, and related to one
of the five emotional tendencies: anger, sadness, happiness, fear, and
neutrality.
### 4.3. Visual databasesVisual databases can also be divided into two categories: facial expression
databases and body gesture emotion databases.
#### 4.3.1. Facial expression databases
Table 3 provides an overview of facial expression databases, including the
main reference (access), year, samples, subject, and expression category. The
early FER databases are derived from the emotions purposely performed by
subjects in the laboratory (In-the-Lab). For example, **JAFFE** [70] released
in 1998, includes 213 images of 7 facial expressions, posed by 10 Japanese
female models. To construct the **extended Cohn-Kanade (CK+)** [71], an
extension of **CK** [72], subjects were instructed to perform 7 facial
expressions. The facial expression images were recorded and analyzed to
provide protocols and baseline results for facial feature tracking, action
units (AUs), and emotion recognition. Different from **CK+, MMI** [73]
consists of onset-apex-offset sequences. **Oulu-CASIA NIR-VIS** (**Oulu-
CASIA**) [74] released in 2011, includes 2,880 image sequences captured with
one of two kinds of imaging systems under three kinds of illumination
conditions.
Table 3. Overview of facial expression databases SBE = Seven Basic Emotions
(anger, disgust, fear, happy, sad, surprise, and neutral).
4
Database| Year| Samples| Subject| Expression Category
---|---|---|---|---
**In-the-Lab**
1JAFFE [70]| 1998| 219 images| 10| SBE
2CK [72]| 2000| 1,917 images| 210| SBE plus Contempt
2CK+ [71]| 2010| 593 sequences| 123| SBE plus Contempt
3MMI [73]| 2010| 740 images, 2,900 videos| 25| SBE
5Oulu-CASIA [74]| 2011| 6 kinds of 480 sequences| 80| SBE
6BU-3DFE [75]| 2006| 2,500 3D images| 100| SBE
6BU-4DFE [76]| 2008| 606 3D sequences| 101| SBE
6BP4D [77]| 2014| 328 3D + 2D sequences| 41| Six basic expressions plus
Embarrassed
74DFAB [78]| 2018| 1.8 million+3D images| 180| Six basic expressions
8SMIC [79]| 2013| 164 sequences| 16| Positive, Negative, Surprise
9CASME II [80]| 2014| 255 sequences| 35| Six basic expressions plus Others
SAMM [81]| 2018| 159 sequences| 32| Six basic expressions plus Others
**In-the-Wild**
10FER2013 [82]| 2013| 35,887 gray images| /| SBE
11SFEW 2.0 [83]| 2015| 1694 images| | Six basic expressions
12EmotioNet [84]| 2016| 1,000,000 images| /| Compound expressions
13ExpW [85]| 2016| 91,793 images| /| SBE
14AffectNet [86]| 2017| 450,000 images| /| SBE plus Contempt Valence and
Arousal
15RAF-DB [87]| 2017| 29,672 images| /| SBE Compound expressions
16DFEW [88]| 2020| 12059 clips| /| SBE
1
kasrl.org/jaffe;
2
jeffcohn.net/Resources/;
3
mmifacedb.eu/;
4
socsci.ru.nl:8180/RaFD2/RaFD;
5oulu.fi/cmvs/node/41316;
6
cs.binghamton.edu/?lijun/Research/3DFE/3DFE_Analysis;
7
eprints.mdx.ac.uk/24259/;
8
oulu.fi/cmvs/node/41319;
9
fu.psych.ac.cn/CASME/casme2-en.php;
10
kaggle.com/c/challenges-in-representation-learning-facial-expression-
recognition-challenge;
11
cs.anu.edu.au/few/emotiw2015;
12
cbcsl.ece.ohio-state.edu/dbform_emotionet;
13
mmlab.ie.cuhk.edu.hk/projects/socialrelation/;
14
mohammadmahoor.com/affectnet/;
15
whdeng.cn/raf/model;
16
dfew-database.github.io.
There are various 3D/4D databases designed for multi-view and multi-pose FER.
**Binghamton University 3D Facial Expression (BU-3DFE)** [75] contains 606
facial expression sequences captured from 100 people with one of six facial
expressions. **BU-4DFE** [76] is developed based on the dynamic 3D space,
which includes 606 high-resolution 3D facial expression sequences. **BP4D**
[77] released in 2014, is a well-annotated 3D video database consisting of
spontaneous facial expressions, elicited from 41 participants (23 women, 18
men), by well-validated emotion inductions. **4DFAB** [78] released in 2018,
contains at least 1,800,000 dynamic high-resolution 3D faces captured from 180
subjects in four different sessions spanning.
Similar to **MMI** [73], all micro-expression databases consist of onset-apex-
offset sequences. **Spontaneous Micro-expression (SMIC)** [79] contains 164
micro-expression video clips elicited from 16 participants. **CASME II** [80]
has videos with relatively high temporal and spatial resolution. The
participants? facial expressions have been elicited in a well-controlled
laboratory environment and proper illumination. **Spontaneous Micro-Facial
Movement (SAMM)** [81] is a currently-public database that contains sequences
with the highest resolution, and its participants are from diverse ethnicities
and the widest range of ages.
Acted facial expression databases are often constructed in a specific
environment. Another way to build the databases is collecting facial
expression images/videos from the Internet, which we refer to as In-the-Wild.
**FER2013** [82] is a firstly public large-scale and unconstrained database
that contains 35,887 grey images with 48 × 48 pixels, collected automatically
through the Google image search API. **Static Facial Expressions In-the-Wild
(SFEW 2.0)** [83] is divided into three sets, including Train (891 images) and
Val (431 images), labelled as one of six basic expressions (anger, disgust,
fear, happiness, sadness and surprise), as well as the neutral and Test (372
images) without expression labels. **EmotioNet** [84] consists of one million
images with 950,000 automatically annotated AUs and 25,000 manually annotatedAUs. **Expression in-the-Wild (ExpW)** [85] contains 91,793 facial images,
manually annotated as one of seven basic facial expressions. Non-face images
were removed in the annotation process. **AffectNet** [86] contains over
1,000,000 facial images, of which 450,000 images are manually annotated as one
of eight discrete expressions (six basic expressions plus neutral and
contempt), and the dimensional intensity of valence and arousal. **Real-world
Affective Face Database (RAF-DB)** [87] contains 29,672 highly diverse facial
images downloaded from the Internet, with manually crowd-sourced annotations
(seven basic and eleven compound emotion labels). **Dynamic Facial Expression
in the Wild (DFEW)** [88] consists of over 16,000 video clips segmented from
thousands of movies with various themes. Professional crowdsourcing is applied
to these clips, and 12,059 clips have been selected and labelled with one of 7
expressions (six basic expressions plus neutral).
#### 4.3.2. Body gesture emotion databases
Although studies in emotion recognition focused mainly on facial expressions,
a growing number of researchers in affective neuroscience demonstrates the
importance of the full body for unconscious emotion recognition. In general,
bodily expressive cues are easier to be perceived than subtle changes in the
face. To capture natural body movements, body gesture emotion databases
contain a corpus of video sequences collected from either real life or movies.
Table 4 provides an overview of body gesture emotion databases, as described
next.
Table 4. Overview of body gesture emotion databases.
Database| Year| Modality| Sources| Sub.| Category
---|---|---|---|---|---
EmoTV [89]| 2005| Face, Body Gaze| 51 images| /| 14 emotions
1FABO [90]| 2006| Face, Body| 1900 videos| 23| 10 emotions
THEATRE| 2009| Body| /| /| 8 emotions
2GEMEP [93]| 2010| Face, Body| 7000+| 10| 18 emotions
EMILYA [95]| 2014| Face, Body| /| 11| 8 emotions
1
cl.cam.ac.uk/?hg410/fabo/;
2
affective-sciences.org/gemep.
**EmoTV** [89] contains the interview video sequences from French TV channels.
It has multiple types of annotations but is not publicly available. To our
best knowledge, **FAce and BOdy database (FABO)** [90] is the first publicly
available bimodal database containing both face and body gesture. The
recordings for each subject take more than one-hour store rich information of
various affect statements. Moreover, **THEATER Corpus** [91] consists of
sections from two movie versions which are coded with eight affective states
corresponding to the eight corners of PAD space [92]. **GEneva Multimodal
Emotion Portrayals (GEMEP)** [93] is a database of body postures and gestures
collected from the perspectives of both an interlocutor and an observer. The
GEMEP database is one of the few databases that have frame-by-frame AU labels
[94]. **Emotional body expression in daily actions database (EMILYA)** [95]
collected body gestures in daily motions. Participants were trained to be
aware of using their bodies to express emotions through actions. **EMILYA**
includes not only videos of facial and bodily emotional expressions, but also
3D data of the whole-body movement.
### 4.4. Physiological databases
Physiological signals ((e.g., EEG, RESP, and ECG) are not affected by social
masking compared to textual, audio, and visual emotion signals, and thus are
more objective and reliable for emotion recognition. Table 5 provides anoverview of physiological-based databases, as described next.
Table 5. Overview of physiological-based databases.
Database| Year| Components| Subjects| Emotion Categories
---|---|---|---|---
EEG| EOG| EMG| GSR| RESP| ECG| Others
1DEAP [96]| 2012| ?| ?| ?| ?| ?| /| Plethysmograph| 32| Valence and Arousal
2SEED [97,98]| 2015| ?| /| /| /| /| /| /| 15| Positive, Neutral, and Negative
DSdRD [7]| 2005| /| /| ?| ?| /| ?| /| 24| Low, medium, and high stress
3AMIGOS [99]| 2017| ?| /| /| ?| /| ?| Frontal HD video, both RGB & depth full
body videos| 40| Valence and Arousal
WESAD [100]| 2018| /| /| ?| /| ?| ?| EDA, blood volume pulse & temperature,
AAC| 15| Neutral, Stress, Amusement
1
eecs.qmul.ac.uk/mmv/databases/deap/;
2
bcmi.sjtu.edu.cn/?seed/;
3
eecs.qmul.ac.uk/mmv/databases/amigos/.
**DEAP** [96] comprises a 32-channel EEG, a 4-channel EOG, a 4-channel EMG,
RESP, plethysmograph, Galvanic Skin Response (GSR) and body temperature,
collected from 32 subjects. Immediately after watching each video, subjects
were required to rate their truly-felt emotion from five dimensions: valence,
arousal, dominance, liking and familiarity. **SEED** [97,98] contains EGG
recordings from 15 subjects. In their study, participants were asked to
experience three EEG recording sessions, with an interval of two weeks between
two successive recording sessions. Within each session, each subject was
exposed to the same sequence of fifteen movie excerpts, each one approximately
four-minute-long, to induce three kinds of emotions: positive, neutral, and
negative.
Some databases are task-driven. For example, **Detecting Stress during Real-
World Driving Tasks (DSdRD)** [7] is used to determine the relative stress
levels of drivers. It contains various signals from 24 volunteers while having
a rest for at least 50 minutes after their driving tasks. These volunteers are
asked to fill out questionnaires which are used to map their state into low,
medium, and high-stress levels. **AMIGOS** [99] was designed to collect
participants? emotions in two social contexts: individual and group. AMIGOS
was constructed in 2 experimental settings: 1) 40 participants watch 16 short
emotional videos; 2) they watch 4 long videos, including a mix of lone and
group sessions. These emotions were annotated with self-assessment of
affective levels and external assessment of valence and arousal. Wearable
devices help to bridge the gap between lab studies and real-life emotions.
**Wearable Stress and Affect Detection (WESAD)** [100] is built for stress
detection, providing multimodal, high-quality data, including three different
affective states (neutral, stress, amusement).
### 4.5. Multimodal databases
In our daily life, people express and/or understand emotions through
multimodal signals. Multimodal databases can be mainly divided into two types:
multi-physical and physical-physiological databases. Table 6 provides an
overview of multimodal databases, as described next.
Table 6. Overview of multimodal databases. Five basic sentiments = Strongly
Positive, Weakly Positive, Neutral, Strongly Negative and Weakly Negative.
Name| Year| Components| Subjects| Type| Emotion Categories
---|---|---|---|---|---
Text| Speech| Visual| Psych.1IEMOCAP [101]| 2008| ?| ?| ?| /| 10| Acted| Happiness, Anger, Sad,
Frustration and Neutral Activation-Valence-Dominance
2CreativeIT [102,103]| 2010| ?| ?| ?| /| 16| Induced| Activation-Valence-
Dominance
3HOW [104]| 2011| ?| ?| ?| /| /| Natural| Positive, Negative and Neutral
ICT-MMMO [105]| 2013| ?| ?| ?| /| /| Natural| Five basic sentiments
4CMU-MOSEI [106]| 2018| ?| ?| ?| /| /| Natural| Six basic emotions, Five basic
sentiments
5MAHNOB-HCI [107]| 2012| /| ?| ?| ?| 27| Induced| Arousal-Valence-Dominance-
Predictability Disgust, Amusement, Joy, Fear, Sadness, Neutral
6RECOLA [108]| 2013| /| ?| ?| ?| 46| Natural| Arousal-Valence, Agreement,
Dominance, Engagement, Performance and Rapport
7DECAF [109]| 2015| /| ?| ?| ?| 30| Induced| Arousal-Valence-Dominance, Six
basic expressions Amusing, Funny and Exciting
1
sail.usc.edu/iemocap/iemocap_release.htm;
2
sail.usc.edu/CreativeIT/ImprovRelease.htm;
3
ict.usc.edu/research/;
4
github.com/A2Zadeh/CMU-MultimodalSDK;
5
mahnob-db.eu/hci-tagging/;
6
diuf.unifr.ch/diva/recola;
7
mhug.disi.unitn.it/wp-content/DECAF/DECAF.
**Interactive Emotional Dyadic Motion Capture (IEMOCAP)** [101] is constructed
by the Speech Analysis and Interpretation Laboratory. During recording, 10
actors are asked to perform selected emotional scripts and improvised
hypothetical scenarios designed to elicit 5 specific types of emotions. The
face, head, and hands of actors are marked to provide detailed information
about their facial expressions and hand movements while performing. Two famous
emotion taxonomies are employed to label the utterance level: discrete
categorical-based annotations and continuous attribute-based annotations.
Afterwards, **CreativeIT** [102,103] contains detailed full-body motion
visual-audio and text description data collected from 16 actors, during their
affective dyadic interactions ranging from 2-10 minutes each. Two kinds of
interactions (two-sentence and paraphrases exercises) are set as improvised.
According to the video frame rate, the annotator gave the values of each
actor's emotional state in the three dimensions. **Harvesting Opinions from
the Web database (HOW)** [104] contains 13 positive, 12 negative and 22
neutral videos captured from YouTube. **Institute for Creative Technologies
Multimodal Movie Opinion database (ICT-MMMO)** [105] contains 308 YouTube
videos and 78 movie review videos from ExpoTV. It has five sentiment labels:
strongly positive, weakly positive, neutral, strongly negative, and weakly
negative. As far as we know, **Multimodal Opinion Sentiment and Emotion
Intensity (CMU-MOSEI)** [106] is the largest database for sentiment analysis
and emotion recognition, consisting of 23,453 sentences and 3,228 videos
collected from more than 1,000 online YouTube speakers. Each video contains a
manual transcription that aligns audio and phoneme grades.
**MAHNOB-HCI** [107] is a video-physiological database. Using 6 video cameras,
a head-worn microphone, an eye gaze tracker, and physiological sensors, it isconstructed by monitoring and recording the emotions of 27 participants while
watching 20 films. **Remote Collaborative and Affective Interactions
(RECOLA)** [108] consists of a multimodal corpus of spontaneous interactions
from 46 participants (in French). These participants work in pairs to discuss
a disaster scenario escape plan and reach an agreement via remote video
conferencing. The recordings of the participants? activities are annotated by
6 annotators with two continuous emotional dimensions: arousal and valence, as
well as social behaviour labels on five dimensions. **DECAF** [109] is a
Magnetoencephalogram-based database for decoding affective responses of 30
subjects while watching 36 movie clips and 40 one-minute music video clips.
DECAF contains a detailed analysis of the correlations between participants?
self-assessments and their physiological responses, single-trial
classification results for valence, arousal and dominance dimensions,
performance evaluation against existing data sets, and time-continuous emotion
annotations for movie clips.
## 5\. Unimodal affect recognition
In this section, we systematically summarize the unimodal affect recognition
methods from the perspective of affect modalities: physical modalities (e.g.,
textual, audio, and visual) [12] and physiological modalities (e.g., EEG and
ECG) [29].
### 5.1. Textual sentiment analysis
With the rapid increase of online social media and e-commerce platforms, where
users freely express their ideas, a huge amount of textual data are generated
and collected. To identify subtle sentiment or emotions expressed explicitly
or implicitly from the user-generated data, textual sentiment analysis (TSA)
was introduced [110]. Traditional approaches of TSA [111,112] often rely on
the process known as ?feature engineering? to find useful features that are
related to sentiment. This is a tedious task. DL-based models can realize an
end-to-end sentiment analysis from textual data.
Table 7 shows an overview of some representative methods for TSA, where the
publication (or preprint) year, analysis granularity, feature representation,
classifier, database, and performance are presented. Next, the works of TSA
are described in detail.
Table 7. Overview of the representative methods for TSA.
Pub.| Year| Granularity| Feature Representation| Classifier| Database|
Performance (%)
---|---|---|---|---|---|---
**_ML-based SER_**
[117]| 2014| Concept-level| Multi-feature fusion| SVM-ELM| Movie Review| 2
classes: 86.21
[120]| 2018| Aspect-level| Multilingual features| Metric-based| Twitter| 2
classes: 66.00, 60.00
[125]| 2008| Aspect-level| CDM| NB| Reuters-2157| 10 classes: 85.62
[130]| 2019| Aspect-level| Features fusion| LML| Amazin; IMDB| 2 classes:
89.70; 10 classes: 85.20
** _DL-based SER_**
[133]| 2020| Document-level| CNN| HieNN-DWE| IMDB etc.| 10 classes: 47.80
[137]| 2017| Document-level| VDCNN| ReLU| Amazon etc.| 2 classes: 95.07
[139]| 2018| Aspect-level| PF-CNN| Softmax| Laptops; Restaurants| 2 classes:
86.35; 90.15
[140]| 2017| Document-level| cBLSTM| Decision Rule| IMDB| 2 classes: 92.83
[141]| 2016| Aspect-level| DT-RNN, CRF| RNCRF| Laptops; Restaurants| 2
classes: 79.44; 84.11
[146]| 2018| Document-level| HUAPA| Softmax| IMDB etc.| 10 classes: 55.00[150]| 2020| Document-level| Word2Vec| CNN-LSTM| SST etc.| 5 classes: 50.68
[151]| 2018| Aspect-level| Word Embeddings| GCAE| Laptops; Restaurants| 4
classes: 69.14; 85.92
[157]| 2018| Sentence-level| CS-GAN| Softmax| Amazon-5000| 2 classes: 86.43
[159]| 2018| Aspect-level| ADAN| Softmax| TARGET| 5 classes: 42.49; 3 classes:
54.54
#### 5.1.1. ML-based TSA
TSA based on the traditional ML methods mainly relies on knowledge-based
techniques or statistical methods [113]. The former requires thesaurus
modelling of large emotional vocabularies, and the latter assumes the
availability of large databases, labelled with polarity or emotional labels.
**Knowledge-based TSA.** Knowledge-based TSA is often based on lexicons and
linguistic rules. Different lexicons, such as WordNet, WordNet-Affect,
SenticNet, MPQA, and SentiWordNet [114], often contain a bag of words and
their semantic polarities. The lexicon-based approaches can classify a given
word into positive or negative, but perform poorly without linguistic rules.
Hence, Ding et al. [115] adopted a lexicon-based holistic approach that
combines external evidence with linguistic conventions in natural language to
evaluate the semantic orientation in reviews. Melville et al. [116] developed
a framework for domain-dependent sentiment analysis using lexical association
information.
To better understand the orientation and flow of sentiment in natural
language, Poria et al. [117] proposed a new framework combining computational
intelligence, linguistics and common-sense computing [118]. They found that
the negation played an important role in judging the polarity of the overall
sentence. Jia et al. [119] also verified the importance of negation on
emotional recognition through extensive experiments. Blekanov et al. [120]
utilized specifics of the Twitter platform in their multi-lingual knowledge-
based approach of sentiment analysis. Due to the limitations of knowledge
itself, knowledge-based models are limited to understanding only those
concepts that are typical and strictly defined.
**Statistical-based TSA.** Statistical-based TSA relies more on an annotated
dataset to train an ML-based classifier by using prior statistics or posterior
probability. Compared with lexicon-based approaches [121,122], statistical-
based TSA approaches are more suitable for sentiment analysis due to their
ability to deal with large amounts of data. Mullen and Collier [123] used the
semantic orientation of words to create a feature space that is classified by
a designed SVM. Pak et al. [124] proposed a new sub-graph-based model. It
represents a document as a collection of sub-graphs and inputs the features
from these sub-graphs into an SVM classifier. Naïve Bayes (NB) is another
powerful and widely-used classifier, which assumes that dataset features are
independent. It can be used to filter out the sentences that do not support
comparative opinions [125].
**Hybrid-based TSA**. By integrating knowledge with statistic models, hybrid-
based TSA can take full advantage of both [126]. For example, Xia et al. [127]
utilized SenticNet and a Bayesian model for contextual concept polarity
disambiguation. Due to the ambiguity and little information of neutral between
positive and negative, Valdivia et al. [128] proposed consensus vote models
and weighted aggregation to detect and filter neutrality by representing the
vague boundary between two sentiment polarities. According to experiments,
they concluded that detecting neutral can help improve the performance of
sentiment analysis. Le et al. [129] proposed a novel hybrid method in the
combination of word sentiment score calculation, text pre-processing,
sentiment feature generation and an ML-based classifier for sentimentanalysis. The hybrid method [129] performed much better than popular lexicon-
based methods in Amazon, IMDb, and Yelp, and achieved an average accuracy of
87.13%. Li et al. [130] utilized lexicons with one of the ML-based methods
including NB and SVM. This hybrid method is more effective in detecting
expressions that are difficult to be polarized into positive-negative
categories.
#### 5.1.2. DL-based TSA
DL-based techniques have a strong ability to automatically learn and discover
discriminative feature representations from data themselves. DL-based TSA has
been proved to be successful with the success of word embeddings [131] and the
increase of the training data with multi-class classification [132]. Various
DL-based approaches for TSA include deep convolutional neural network
(ConvNet) learning, deep RNN learning, deep ConvNet-RNN learning and deep
adversarial learning, as detailed next.
**Deep ConvNet learning for TSA.** CNN-based methods have been applied to
different levels of TSA including document-level [133], sentence-level [134],
and aspect-level (or word-level) [135] by using different filters to learn
local features from the input data. Yin et al. [136] proposed a framework of
sentence-level sentiment classification based on the semantic lexical-
augmented CNN (SCNN) model, which makes full use of word information. Conneau
et al. [137] applied a very deep CNN (VDCNN), which learns the hierarchical
representations of the document and long-range dependencies to text
processing. To establish long-range dependencies in documents, Johnson and
Zhang [138] proposed a word-level deep pyramid CNN (DPCNN) model, which
stacked alternately the convolutional layer and the max-pooling downsampling
layer to form a pyramid to reduce computing complexity. The DPCNN with 15
weighted layers outperformed the previous best models on six benchmark
databases for sentiment classification and topic categorization. For aspect-
level sentiment analysis, Huang and Carley also [139] proposed a novel aspect-
specific CNN by combining parameterized filters and parametrized gates.
**Deep RNN learning for TSA.** RNN-based TSA is capable of processing long
sequence data. For example, Mousa and Schuller [140] designed a novel
generative approach, contextual Bi-LSTM with a language model (cBi-LSTM LM),
which changes the structure of Bi-LSTM to learn the word's contextual
information based on its right and left contexts. Moreover, Wang et al. [141]
proposed a model of recursive neural conditional random field (RNCRF) by
integrating the RNN-based dependency tree of the sentence and conditional
random fields.
The attention mechanism contributes to prioritizing relevant parts of the
given input sequence according to a weighted representation at a low
computational cost. In the paradigm of attention-based LSTM for TSA, LSTM
helps to construct the document representation, and then attention-based deep
memory layers compute the ratings of each document. Chen et al. [142] designed
the recurrent attention memory (RAM) for aspect-level sentiment analysis.
Specifically, a multiple-attention mechanism was employed to capture sentiment
features separated by a long distance, the results of which were non-linearly
combined with RNN. Inspired by the capability of human eye-movement behavior,
Mishra et al. [143] introduced a hierarchical LSTM-based model trained by
cognition grounded eye-tracking data, and they used the model to predict the
sentiment of the overall review text. More recently, some researchers
[144,145] suggested that user preferences and product characteristics should
be taken into account.
For document-level sentiment classification, Dou [145] proposed a deep memory
network combining LSTM on account of the influence of users who express thesentiment and the products that are evaluated. Chen et al. [144] designed a
hierarchical LSTM with an attention mechanism to generate sentence and
document representations, which incorporates global user and product
information to prioritize the most contributing items. Considering the
irrationality of encoding user information and product information as one
representation, Wu et al. [146] designed an attention LSTM-based model, which
executed hierarchical user attention and product attention (HUAPA) to realize
sentiment classification.
For multi-task classification (e.g., aspect category and sentiment polarity
detection), J et al. [147] proposed convolutional stacked Bi-LSTM with a
multiplicative attention network concerning global-local information. In
contrast, to fully exploit contextual affective knowledge in aspect-level TSA,
Liang et al. [148] proposed GCN-based SenticNet to enhance graph-based
dependencies of sentences. Specifically, LSTM layers were employed to learn
contextual representations, and GCN layers were built to capture the
relationships between contextual words in specific aspects.
**Deep ConvNet-RNN learning for TSA.** Although ConvNet-based or RNN-based
models have been extensively employed to generate impressive results in TSA,
more researchers [149,150] have tried to combine both ConvNets and RNNs to
improve the performance of TSA by getting the benefits offered by each model.
As the CNN is proficient in extracting local features and the BiLSTM is
skilled in a long sequence, Li et al. [150] combined CNN and BiLSTM in a
parallel manner to extract both types of features, improving the performance
of sentiment analysis. To decrease the training time and complexity of LSTM
and attention mechanism for predicting the sentiment polarity, Xue et al.
[151] designed a gated convolutional network with aspect embedding (GCAE) for
aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis
(ATSA). The GCAE uses two parallel CNNs, which output results combined with
the gated unit and extended with the third CNN, extracting contextual
information of aspect terms.
To distinguish the importance of different features, Basiri et al. [152]
proposed an attention-based CNN-RNN deep model (ABCDM), which utilized
bidirectional LSTM and GRU layers to capture temporal contexts and apply the
attention operations on the discriminative embeddings of outputs generated by
two RNN-based networks. In addition, CNNs were employed for feature
enhancement (e.g., feature dimensionality reduction and position-invariant
feature extraction). All the above works focused on detecting sentiment or
emotion, but it is important to predict the intensity or degree of one
sentiment in the description of human intimate emotion. To address the
problem, Akhtar et al. [153] proposed a stacked ensemble method by using an
MLP to ensemble the outputs of the CNN, LSTM, GUR, and SVR.
**Deep adversarial learning for TSA.** Deep adversarial learning with the
ability to regularize supervised learning algorithms was introduced to text
classification [154]. Inspired by the domain-adversarial neural network [155],
Li et al. [156] constructed an adversarial memory network model that contains
sentiment and domain classifier modules. Both modules were trained together to
reduce the sentiment classification error and allowed the domain classifier
not to separate both domain samples. The attention mechanism is incorporated
into the deep adversarial based model to help the selection of the pivoted
words, which are useful for sentiment classification and shared between the
source and target domains. Li et al. [157] initiated the use of GANs [158] in
sentiment analysis, reinforcement learning and recurrent neural networks to
build a novel model, termed category sentence generative adversarial network
(CS-GAN). By combining GANs with RL, the CS-GAN can generate more categorysentences to improve the capability of generalization during supervised
training. Similarly, to tackle the sentiment classification in low-resource
languages without adequate annotated data, Chen et al. [159] proposed an
adversarial deep averaging network (ADAN) to realize cross-lingual sentiment
analysis by transferring the knowledge learned from labelled data on a
resource-rich source language to low-resource languages where only unlabeled
data existed. Especially, the ADAN was trained with labelled source text data
from English and unlabeled target text data from Arabic and Chinese. More
recently, Karimi et al. [160] fine-tuned the general-purpose BERT and domain-
specific post-trained BERT using adversarial training, which showed promising
results in TSA.
### 5.2. Audio emotion recognition
Audio emotion recognition (also called SER) detects the embedded emotions by
processing and understanding speech signals [161]. Various ML-based and DL-
based SER systems have been carried out on the basis of these extracted
features for better analysis [162,163]. Traditional ML-based SER concentrates
on the extraction of the acoustic features and the selection of the
classifiers. However DL-based SER constructs an end-to-end CNN architecture to
predict the final emotion without considering feature engineering and
selection [164].
Table 8 shows an overview of representative methods for SER, including the
most relevant papers, their publication (or preprint) year, feature
representation, classifier, database, and performance (from best or average
reported results). These works are next described in detail.
Table 8. Overview of the representative methods for SER.
Pub.| Year| Feature Representation| Classifier| Database| Performance (%)
---|---|---|---|---|---
**_ML-based SER_**
[174]| 2010| Acoustic-feature| SVM| LDC/ Emo-DB| 6 classes: 43.80/79.10
[176]| 2014| Feature selection and fusion| MKL| Emo-DB| 7 classes: 83.10
[181]| 2013| MFCC| SVM| Emo-DB| 7 classes: 68.00
[186]| 2020| Acoustic features| TL-FMRF| CASIA/Emo-DB| 6 classes: 81.75/77.94
** _DL-based SER_**
[163]| 2013| Semi-CNN| SVM| SAVEE/Emo-DB
DES/ MES| 7 classes: 89.70/93.70
7 classes: 90.80/90.20
[193]| 2017| CNN| FC| Emo-DB| 7 classes: 84.30
[197]| 2014| Bi-LSTM-RNN| RNN| USC-IEMOCAP| 4 classes: 51.86
[198]| 2017| DNN/RNN| Pooling| IEMOCAP| 7 classes: 58.80
[199]| 2018| Attention-3D-RNN| FC| IEMOCAP
Emo-DB| 4 classes: 64.74
7 classes: 82.82
[200]| 2016| CNNs-LSTM| LSTM| RECOLA| A/V: 74.10/ 32.50
[203]| 2018| CNN/Attention-BLSTM| DNN| IEMOCAP| 7 classes: 68.00
[204]| 2018| Adversarial AE| SVM| IEMOCAP| 7 classes: 58.38
[205]| 2018| Conditional adversarial| CNN| RECOLA| A/V: 73.70/44.40
#### 5.2.1. ML-based SER
The ML-based SER systems include two key steps: the strong features
representation learning for emotional speech and an appropriate classification
for final emotion prediction [19]. Different kinds of acoustic features can be
fused to get the mixed features for a robust SER. Although prosodic features
and spectral features are more frequently used in SER systems [165], in some
cases, voice-quality features and other features are sometimes more important
[166]. OpenSMILE [167] is a popular audio feature extraction toolkit thatextracts all key features of the speech. The commonly used classifiers for SER
systems encompass HMM, GMM, SVM, RF and ANN. In addition to these classifiers,
the improved conventional interpretable classifiers and ensemble classifiers
are also adopted for SER. In this sub-section, we divided the ML-based SER
systems into acoustic-feature based SER and interpretable-classifier based SER
[168]**,** [169]. Note that different combinations of models and features
result in obvious differences in the performance of SER [170].
**Acoustic-feature based SER.** Prosodic features (e.g. intonation and rhythm)
have been discovered to convey the most distinctive properties of emotional
content for SER. The prosodic features consist of fundamental frequency
(rhythmical and tonal characteristics) [171], energy (volume or the
intensity), and duration (the total of time to build vowels, words and similar
constructs). Voice quality is determined by the physical properties of the
vocal tract such as jitter, shimmer, and harmonics to noise ratio. Lugger and
Yang [172] investigated the effect of prosodic features, voice quality
parameters, and different combinations of both types on emotion
classification. Spectral features are often obtained by transforming the time-
domain speech signal into the frequency-domain speech signal using the Fourier
transform [173]. Bitouk et al. [174] introduced a new set of fine-grained
spectral features which are statistics of Mel Frequency cepstrum coefficients
(MFCC) over three phoneme type classes of interest in the utterance. Compared
to prosodic features or utterance level spectral features, the fine-grained
spectral features can yield results with higher accuracy. In addition, the
combination of these features and prosodic features also improves accuracy.
Shen et al. [175] utilized SVM to evaluate the performance of using energy and
pitch, linear prediction cepstrum coefficients (LPCC), MFCC, and their
combination. The experiment demonstrated the superior performance based on the
combination of various acoustic features.
In order to improve the recognition performance of speaker-independent SER,
Jin et al. [176] designed feature selection with L1-normalization constraint
of Multiple Kernel Learning (MKL) and feature fusion with Gaussian kernels.
This work [176] achieved an accuracy of 83.10% on the Berlin Database, which
is 2.10% higher than the model that used the sequential floating forward
selection algorithm, and a GMM [177]. Wang et al. [178] proposed a new Fourier
parameter-based model using the perceptual content of voice quality, and the
first-order and second-order differences for speaker-independent SER.
Experimental results revealed that the combination of Fourier parameters and
MFCC could significantly increase the recognition rate of SER.
**ML-based classifier for SER.** The HMM classifier is extensively adopted for
SER due to the production mechanism of the speech signals. In 2003, Nwe et al.
[179] designed an HMM with log frequency power coefficients (LFPC) to detect
human stress and emotion. It achieved the best recognition accuracy of 89%,
which was higher than 65.8% of human recognition. The GMM and its variants can
be considered as a special continuous HMM and are appropriate for global-
feature based SER. For example, Navas et al. [180] proposed a GMM-based
baseline method by using prosodic, voice quality, and MFCC features.
Different from HMM and GMM, SVM maps the emotion vector to a higher
dimensional space by using a kernel function and establishes the maximum
interval hyperplane in the high-dimensional space for optimal classification.
Milton et al. [181] designed a three-stage hierarchical SVM with linear and
RBF kernels to classify seven emotions using MFCC features on the Berlin
EmoDB. There are some works [182,183] using different SVM classifiers with
various acoustic features and their combinations. Ensemble learning has been
proven to give superior performance compared to a single classifier. Yüncü etal. [184] designed the SVM with Binary DT by integrating the tree architecture
with SVM. Bhavan et al. [185] proposed a bagged ensemble comprising of SVM
with different Gaussian kernels as a viable algorithm. Considering differences
among various categories of human beings, Chen et al. [186] proposed the two-
layer fuzzy multiple RF by integrating the decision trees with Bootstraps to
recognize six emotional states.
#### 5.2.2. DL-based SER
DL-based SER systems can understand and detect contexts and features of
emotional speech without designing a tailored feature extractor. The CNNs with
auto-encoder [187] are regarded as commonly used techniques for DL-based SER
[187]. The RNNs [188] and their variants (e.g., Bi-LSTM) [189] are widely
introduced to capture temporal information. The hybrid deep learning for SER
includes ConvNets and RNNs, as well as attention mechanisms [190,191]. For the
issues of limited data amount and low quality of the databases, adversarial
learning can be used for DL-based SER [192] by augmenting trained data and
eliminating perturbations.
**ConvNet learning for SER**. Huang et al. [163] utilized the semi-CNN to
extract features of spectrogram images, which are fed into SVM with different
parameters for SER. Badshah et al. [193] proposed an end-to-end deep CNN with
three convolutional layers and three fully connected layers to extract
discriminative features from spectrogram images and predict seven emotions.
Zhang et al. [194] designed AlexNet DCNN pre-trained on ImageNet with
discriminant temporal pyramid matching (DTPM) strategy to form a global
utterance-level feature representation. The deep ConvNet-based models are also
used to extract emotion features from raw speech input for multi-task SER
[195].
**RNN learning for SER.** RNNs can process a sequence of speech inputs and
retain its state while processing the next sequence of inputs. They learn the
short-time frame-level acoustic features and aggregate appropriately these
features over time into an utterance-level representation. Considering the
different emotion states that may exist in a long utterance, RNN and its
variants (e.g. LSTM and Bi-LSTM) can tackle the uncertainty of emotional
labels. Extreme learning machine (ELM), a single-hidden layer neural network,
was regarded as the utterance-level classifier [196]. Lee and Tashev [188]
proposed the Bi-LSTM with ELM to capture a high-level representation of
temporal dynamic characteristics. Ghosh et al. [197] pre-trained a stacked
denoising autoencoder to extract low-dimensional distributed feature
representation and trained Bi-LSTM-RNN for final emotion classification of
speech sequence.
The attention model is designed to ignore irrelevantly emotional frames and
other parts of the utterance. Mirsamadi et al. [198] introduced an attention
model with a weighted time-pooling strategy into the RNN to more emotionally
salient regions. Because there exists some irrelevantly emotional silence in
the speech, the silence removal needs to be employed before BLSTMs,
incorporated with the attention model used for feature extraction [190]. Chen
et al. [199] proposed the attention-based 3D-RNNs to learn discriminative
features for SER. Experimental results show that the Attention-3D-RNNs can
exceed the performance of SOTA SER on IEMOCAP and Emo-DB in terms of
unweighted average recall.
The attention model is designed to ignore irrelevant emotional frames in the
utterance. Mirsamadi et al. [198] introduced an attention model with a
weighted time-pooling strategy into the RNN to highlight more emotionally
salient regions. Since there exists some irrelevant emotional silence in the
speech, it is necessary to combine the attention model and silence removal forfeature extraction [190]. Chen et al. [199] proposed the attention-based
3D-RNNs to learn discriminative features for SER, which achieved outstanding
performances on IEMOCAP and Emo-DB in terms of unweighted average recall.
**ConvNet-RNN learning for SER.** As both ConvNets and RNNs have their
advantages and limitations, the combination of CNNs and RNNs enables the SER
system to obtain both frequency and temporal dependency [191]. For example,
Trigeorgis et al. [200] proposed an end-to-end SER system consisting of CNNs
and LSTMs to automatically learn the best representation of the speech signal
directly from the raw time representation. In contrast, Tzirakis et al. [201]
proposed an end-to-end model comprising of CNNs and LSTM networks, which show
that a deeper network is consistently more accurate than one shallow
structure, with an average improvement of 10.1% and 17.9% of Arousal and
Valence (A/V) on RECOLA. Wu et al. [202] designed the capsule networks
(CapsNets) based SER system by integrating with the recurrent connection. The
experiments employed on IEMOCAP demonstrated that CapsNets achieved 72.73% and
59.71% of weighted accuracy (WA) and unweighted accuracy (UA), respectively.
Zhao et al. [203] designed a spatial CNN and an attention-based BLSTM for deep
spectrum feature extraction. These features were concatenated and fed into a
DNN to predict the final emotion.
**Adversarial learning for SER.** In SER systems, the classifiers are often
exposed to training data that have a different distribution from the test
data. The difference in data distributions between the training and testing
data results in severe misclassification. Abdelwahab and Busso [192] proposed
the domain adversarial neural network to train gradients coming from the
domain classifier, which makes the source and target domain representations
closer. Sahu et al. [204] proposed an adversarial AE for the domain of emotion
recognition while maintaining the discriminability between emotion classes.
Similarly, Han et al. [205] proposed a conditional adversarial training
framework to predict dimensional representations of emotion while
distinguishing the difference between generated predictions and the ground-
truth labels.
GAN and its variants have been demonstrated that the synthetic data generated
by generative models can enhance the classification performance of SER [206].
To augment the emotional speech information, Bao et al. [207] utilized Cycle
consistent adversarial networks (CycleGAN) [208] to generate synthetic
features representing the target emotions, by learning feature vectors
extracted from unlabeled source data.
### 5.3. Visual emotion recognition
Visual emotion recognition [209,12] can be primarily categorized into facial
expression recognition (FER) and body gesture emotion recognition (also known
as emotional body gesture recognition, or EBGR). The following section looks
at FER and EBGR in a great deal of detail, capturing a vast body of research
(well-over 100 references).
#### 5.3.1. Facial expression recognition
FER is implemented using images or videos containing facial emotional cues
[210,211]. According to whether static images or dynamic videos are to be used
for facial expression representation, FER systems can be divided into static-
based FER [212] and dynamic-based FER [213]. When it comes to the duration and
intensity of facial expression [214,215], FER can be further divided into
macro-FER and micro-FER (or FMER) [216], [217], [218]. Based on the dimensions
of the facial images, macro-FER can be further grouped into 2D FER [219],
[220], [221], and 3D/4D FER [222], [223], [224]. Note that since facial images
or videos suffer from a varied range of backgrounds, illuminations, and head
poses, it is essential to employ pre-processing techniques (e.g., facealignment [225], face normalization [226], and pose normalization [227]) to
align and normalize semantic information of face region.
In this sub-section, we distinguish FER methods (shown in Fig. 4) via the
point of whether the features are hand-crafted features based ML models [228]
or high-level features based on DL-based models [229]. Table 9 provides an
overview of representative FER methods. The first column gives the publication
reference (abbreviated to Pub.), followed by the publication (or preprint)
year in the second column. The feature representations and classifiers of the
referenced method are given in the third and fourth columns, respectively. The
last six columns show the best or average results (%) with the given
databases. These works are next described in detail.
1. Download: Download high-res image (337KB)
2. Download: Download full-size image
Fig. 4. Taxonomy of representative FER methods based on ML techniques or DL
models. (a) Geometry-based FER adopted from [230]; (b) Appearance-based FER
adopted from [235]; (c) Feature fusion for 3D FER adopted from [236]; (d)
Feature selection for FMER adopted from [237]; (e) ConvNet learning for FMER
adopted from [218]; (f) ConvNet-RNN learning for FER adopted from [238]; (g)
Adversarial learning for 3D FER adopted from [239].
1. Download: Download high-res image (2MB)
2. Download: Download full-size image
Fig. 3. Dimensional emotion models. (a) Pleasure-Arousal-Dominance (PAD) model
reproduced based on [61] and (b) Valence-Arousal (V-A) model reproduced based
on [54].
Table 9. Overview of the representative FER methods (publicly used databases).
Pub.| Year| Feature Representation| Classifier| CK+| Oulu-CASIA| SFEW|
BU-4DFE| CASME II| SMIC
---|---|---|---|---|---|---|---|---|---
** _ML-based FER_**
[230]| 2013| FLP| SVM| 97.35| /| /| /| /| /
[234]| 2019| LPDP| SVM| 94.50| /| /| 73.40| /| /
[243]| 2012| MSGF| KNN| 91.51| /| /| /| /| /
[235]| 2020| IFSL| SVM| 98.70| /| 46.50| /| /| /
[247]| 2018| OSF+LBP-TOP| SVM| /| /| /| /| 241.74/161.82| 250.79/162.74
[245]| 2015| LBP-MOP| SVM| /| /| /| /| 245.75/166.80| 250.61/160.98
[245]| 2015| LBP-SIP| SVM| /| /| /| /| 244.53/166.40| 250.00/164.02
[236]| 2018| 2D+3D maps| MKL| /| /| /| 90.12| /| /
[249]| 2019| DSF| HMM| /| /| /| 95.13| /| /
[253]| 2015| DE| SVM| /| /| /| 85.81/384.00| /| /
[216]| 2016| MDMO| SVM| /| /| /| /| 267.37| 280.00
[237]| 2018| HSDS| KGSL| /| /| /| /| 265.18| 266.46
** _DL-based FER_**
[272]| 2018| FLM-CNN| FC| /| /| /| 483.32| /| /
[280]| 2020| GCN-CNN| FC| /| /| /| /| 242.70| /
[263]| 2017| IACNN| FC| 95.37| /| 54.30| /| /| /
[270]| 2021| SCAN| FC| 97.31| 89.11| 58.93| /| /| /
[238]| 2017| CNN-RNN| Fusion| 98.50| 86.25| /| /| /| /
[286]| 2020| CNN-LSTM| Pooling| 99.54| 88.33| 54.56| /| /| /
[282]| 2021| CNN-LSTM| 6Col-Cla| /| /| /| 99.69| /| /
[284]| 2020| STRCN| Pooling| /| /| /| /| 184.10| 175.80
[290]| 2019| DE-GAN| SVM/MLP| 97.28| 89.17| /| /| /| /
[289]| 2020| SNA| FC| 98.58| 87.60| /| /| /| /
[291]| 2020| ICE-GAN| CED| /| /| /| /| 586.80| 579.10
1Leave-one-subject-out (LOSO);
2
Leave-one-video-out (LOVO);
3
BU-3DFE/ Bosphorus;
4
The average accuracy of BU-3DFE under both the protocols, P1 and P2;
5
Unweighted Average Recall (UAR);
6
Col-Cla=Collaborative Classification.
##### 5.3.1.1. ML-based FER
The ML-based FER methods mainly rely on hand-crafted feature extraction and
feature post-processing. Generally, hand-crafted facial features can be
categorized into geometry-based features explaining the shape of the face and
its components, and appearance-based features defining facial texture. The
feature post-processing can be divided into two types: feature fusion and
feature selection [18].
**Geometry-based FER.** Ghimire and Lee [230] (Fig. 4 (a)) utilized 52 facial
landmark points (FLP), which represent features of geometric positions and
angles, for automatic FER in facial sequences. Sujono and Gunawan [231] used
the Kinect motion sensor to detect the face region based on depth information
and active shape model (AAM) [232]. The change of key features in AAM and a
fuzzy logic model is utilized to recognize facial expression based on prior
knowledge derived from the facial action coding system (FACS) [233]. As the
geometry of different local structures with distortions has unstable shape
representations, the local prominent directional pattern descriptor (LPDP)
[234] was proposed by using statistical information of a pixel neighbourhood
to encode more meaningful and reliable information.
**Appearance-based FER.** Appearance-based approaches often extract and
analyze spatial information or spatial-temporal information from the whole or
specific facial regions [240]. Tian et al. [210] designed an automatic face
analysis system that captured fine-grained changes of facial expression into
AUs [241] of FACS by using the permanent and transient facial features
extracted by the Gabor wavelet, SIFT and local binary pattern (LBP) [242]. Gu
et al. [243] divided one facial image into several local regions by grids,
then applied multi-scale Gabor-filter operations on local blocks, and finally
encoded the mean intensity of each feature map. Yan et al. [235] (Fig. 4 (b))
proposed a framework of low-resolution FER based on image filter-based
subspace learning (IFSL), including deriving discriminative image filters
(DIFs), their combination, and an expression-aware transformation matrix.
The local binary pattern from three orthogonal planes (LBP-TOP) [244] is an
extension of LBP [242] computes over three orthogonal planes at each bin of a
3D volume formed by stacking the frames. The LBP-TOP and its variants have
shown a promising performance on dynamic FER. For example, Wang et al. [245]
designed two kinds of feature extractors (LBP-six intersection points (LBP-
SIP) and super-compact LBP-three mean orthogonal planes (LBP-MOP)) based on
the improved LBP-TOP to preserve the essential patterns while reducing the
redundancy. Davison et al. [246] employed LBP-TOP features and Gaussian
derivatives features for FEMR. Similarly, Liong et al. [247] designed a
framework of FMER based on two kinds of feature extractors, consisting of
optical strain flow (OSF) and block-based LBP-TOP.
**Feature fusion for FER.** It has been proved to fuse different types of
geometry-based features and appearance-based features to enhance therobustness of FER [236]. For example, Majumder et al. [219] designed an
automatic FER system based on the deep fusion of geometric features and LBP
features using autoencoders. For the FMER task, Zhang et al. [248] proposed
the aggregating local spatiotemporal patterns (ALSTP), which adopts cascaded
fusion of local LBP-TOP and LOF extracted from 9 representative local regions
of the face. For 3D/4D facial expression images, Zhen et al. [249] computed
spatial facial deformations using a Riemannian based on dense scalar fields
(DSF) and magnified them by a temporal filtering technique. For 2D/3D facial
expression images, Yao et al. [236] (Fig. 4 (c)) used the MKL fusion strategy
to combine 2D texture features, 3D shape features, and their corresponding
Fourier transform maps of different face regions.
**Feature selection for FER.** Although more 3D/4D features [250] or dynamic
features [251] have different effects on FER [252], excessive features may
break the predictive model. The feature selection is to choose a relevant and
useful subset of the given set of features while identifying and removing
redundant attributes. To overcome the high-dimensionality problem of 3D facial
features, Azazi et al. [253] firstly transformed the 3D faces into 2D planes
using conformal mapping, and then proposed the differential evolution (DE) to
select the optimal facial feature set and SVM classifier parameters,
simultaneously. Savran and Sankur [254] investigated the model-free 3D FER
based on the non-rigid registration by selecting the most discriminative
feature points from 3D facial images. As different facial regions contributed
different to micro-expressions, Chen et al. [255] utilized weighted 3DHOG
features and weighted fuzzy classification for FMER. Different from the
spatial division with fixed grid, a hierarchical spatial division scheme
(HSDS) [237] (Fig. 4 (d)) was proposed to generate multiple types of gradually
denser grids and designed kernelized group sparse learning (KGSL) to learn a
set of importance weights.
##### 5.3.1.2. DL-based FER
The backbone networks of DL-based FER are mostly derived from well-known pre-
trained ConvNets such as VGG [256], VGG-face [257], ResNet [258], and
GoogLeNet [259]. Thus, we divide DL-based FER into ConvNet learning for FER,
ConvNet-RNN learning for FER, and adversarial learning for FER considering the
difference of network architectures.
**ConvNet learning for FER**. ConvNet-based FER often design transform
learning or loss function [221] to overcome overfitting when using relatively
small facial expression databases. For example, Yang et al. [260] proposed a
de-expression residue learning (DeRL) to recognize facial expressions by
extracting expressive information from one facial expression image. For FMER,
the transform-learning based model is pre-trained on the ImageNet and several
popular macro-expression databases with the original residual network [261].
Su et al. [218] (Fig. 4 (e)) proposed a novel knowledge transfer technique,
which comprised a pre-trained deep teacher neural network and a shallow
student neural network. Specifically, the AU-based model is trained on the
residual network, which is then distilled and transferred for FMER. Liu et al.
[262] developed an identity-disentangled FER by integrating the hard negative
generation with the radial metric learning (RML). Specifically, the RML module
combined inception-structured convolutional groups with an expression
classification branch for the final emotion recognition by minimizing both the
cross-entropy loss and RML loss. To alleviate variations introduced by
personal attributes, Meng et al. [263] proposed an identity-aware
convolutional neural network (IACNN) consisting of two identical sub-CNNs with
shared weights. Besides, they designed two losses for identity-invariant FER:
expression-sensitive loss and identity-sensitive loss.To highlight the most helpful information of facial images, various attention
mechanisms [264,265] are proposed to discriminate distinctive features. For
example, Fernandez et al. [266] proposed an attention network and embedded it
into an encoder-decoder architecture for the facial expression representation.
Similarly, Xie et al [267] proposed an attention-based salient expressional
region descriptor (SERD) to locate the most expression-related regions that
are beneficial to FER. To reduce the uncertainties of FER, Wang et al. [268]
proposed the self-cure network consisting of the self-attention importance
weighting module, the ranking regularization module, and the relabeling
module. Zhu et al. [269] proposed a novel discriminative attention-based CNN,
where the attention module was used to emphasize the unequal contributions of
features for different expressions, and a dimensional distribution loss was
designed to model the inter-expression relationship. To provide local-global
attention across the channel and spatial location for feature maps, Gera and
Balasubramanian [270] proposed the spatial-channel attention net (SCAN).
Besides, the complementary context information (CCI) branch is proposed to
enhance the discriminating ability by integrating with another channel-wise
attention.
Except for the efficient network architectures with attention networks or loss
functions [271], there is a fast and light manifold convolutional neural
network (FLM-CNN) based on the multi-scale encoding strategy [272] or the deep
fusion convolutional neural network [273]. Due to the facial expression
database biases, conditional probability distributions between source and
target databases are often different. To implement a cross-database FER, Li
and Deng [274] proposed a novel deep emotion-conditional adaption network to
learn domain-invariant and discriminative feature representations. Another
challenge for FER is the class imbalance of in-the-wild databases. Li et al.
[275] proposed an adaptive regular loss function named AdaReg loss, which can
re-weight category importance coefficients, to learn class-imbalanced
expression representations.
The 3D ConvNet (or C3D) [276] has been universally used for dynamic-based FER
by learning and representing the spatiotemporal features of videos or
sequences. For example, the C3D was first used to learn local spatiotemporal
features [277] and then these features were cascaded with the multimodal deep-
belief networks (DBNs) [278]. Besides, the C3D can be combined with the global
attention module to represent Eulerian motion feature maps generated based on
the Eulerian video magnification (EVM) [279]. Lo et al. [280] utilized a 3D
CNN to extract AU features and applied a graph convolutional network (GCN) to
discover the dependency of AU nodes.
**ConvNet-RNN learning for FER.** For dynamic facial sequences or videos, the
temporal correlations of consecutive frames should be considered as important
cues. RNNs and their variants (LSTMs) can robustly derive temporal
characteristics of the spatial feature representation. In contrast, spatial
characteristics of the representative expression-state frames can be learned
with CNNs [281]. Based on the architecture of ConvNet-RNN networks, many
studies have proposed cascaded fusion [224,282] or the ensemble strategy [238]
to capture both spatial and temporal information for FER.
The standard pipeline of ConvNet-RNN based FER using the cascaded fusion is to
cascade outputs of ConvNets into RNNs to extract temporal dynamics. For
example, Kim et al. [283] proposed an end-to-end FMER framework by cascading
the spatial features extracted by the CNN into the LSTM to encode the temporal
characteristics. Xia et al. [284] proposed a deep spatiotemporal recurrent
convolutional network (STRCN) with a balanced loss that can capture the
spatiotemporal deformations of the micro-expression sequence. For dimensionalemotion recognition, Kollias and Zafeiriou [285] proposed RNN subnets to
explore the temporal dynamics of low-, mid- and high-level features extracted
from the trained CNNs. Liu et al. [286] proposed a framework of dynamic FER
based on the siamese action-units attention network (SAANet). Specifically,
the SAANet is a pairwise sampling strategy, consisting of CNNs with global-AU
attention modules, a BiLSTM module and an attentive pooling module. For 4D
FER, Behzad et al. [224] utilized CNNs to extract deep features of multi-view
and augmented images, and then fed these features into the Bi-LSTM to predict
4D facial expression. On the basis of the work [224], sparsity-aware deep
learning [282] was further proposed to compute the sparse representations of
multi-view CNN features.
The standard pipeline of ConvNet-RNN based FER using the ensemble strategy is
to fuse outputs of two streams. For example, Zhang et al. [238] (Fig. 4 (f))
proposed a deep evolutional spatial-temporal network, which consists of a
part-based hierarchical bidirectional recurrent neural network (PHRNN) and a
multi-signal convolutional neural network (MSCNN), for analyzing temporal
facial expression information and still appearance information, respectively.
**Adversarial learning for FER.** As GANs can generate synthetic facial
expression images under different poses and views, GAN-based models are used
for pose/view-invariant FER [287] or identity-invariant FER [288]. For
pose/view-invariant FER, Zhang et al. [239] proposed the GAN using AE
structure to generate more facial images with different expressions under
arbitrary poses (Fig. 4 (g)) and [287] further took the shape geometry into
consideration. Since the slight semantic perturbations of the inputs often
affected prediction accuracy, Fu et al. [289] proposed a semantic
neighbourhood aware (SNA) network, which formulated the semantic perturbation
based on the asymmetric AE with additive noise. For identity-invariant FER,
Ali and Hughes [290] proposed a novel disentangled expression learning GAN
(DE-GAN) by untangling the facial expression representation from identity
information. Yu et al. [291] investigated a framework of facial micro-
expression recognition and synthesis based on the identity-aware and capsule-
enhanced GAN (ICE-GAN), which consisted of an AE-based generator for identity-
aware expression synthesis, and a capsule-enhanced discriminator (CED) for
discriminating the real/fake images and recognizing micro-expressions.
#### 5.3.2. Body gesture emotion recognition
Most studies of visual emotion recognition focus on FER due to the prominent
advantages of distinguishing human emotions. However, FER will be unsuitable
when the dedicated sensors fail to capture facial images or just capture low-
resolution facial images in some environments. EBGR [45] aims to expose one's
hidden emotional state from full-body visual information (e.g., body postures)
and body skeleton movements or upper-body visual information (e.g., hand
gestures, head positioning and eye movements) [292,293]. The general pipeline
of EBGR includes human detection [294,258,295] (regarded as pre-processing),
feature representation, and emotion recognition. In the view of whether the
process of feature extraction and emotion recognition is performed in an end-
to-end manner, EBGR systems are categorized into ML-based EBGR and DL-based
EBGR.
##### 5.3.2.1. ML-based EBGR
In existing ML-based EBGR systems, the input is an abstraction of the human
body gestures (or their dynamics) through an ensemble body parts or a
kinematic model [296]; and the output emotion is distinguished through ML-
based methods or statistical measures to map the input into the emotional
feature space, where the emotional state can be recognized by an ML-based
classifier [297].**Statistic-based or movement-based EBGR**. The ways of feature extraction can
be grouped into statistic-based analysis [298], [299], [300] and movement-
based analysis [301]. For example, Castellano et al. [298] proposed an
emotional behavior recognition method based on the analysis of body movement
and gesture expressivity. Different statistic qualities of dynamic body
gestures are used to infer emotions with designed indicators describing the
dynamics of expressive motion cues. Similarly, Saha et al. [299] and Maret et
al. [300] utilized low-level features for EBGR, which were calculated based on
the statistical analysis of the 3-D human skeleton generated by a Kinect
sensor. Besides, Senecal et al. [301] proposed a framework of continuous
emotional recognition based on the gestures and full-body dynamical motions
using the laban movement analysis-based feature descriptors.
**Feature fusion for EBGR**. Fusing multiple body posture features can enhance
the generalization capability and the robustness of EBGR [296]. By analyzing
postural and dynamic expressive gesture features, Glowinski et al. [302]
proposed a framework of upper-body based EBGR, using the minimal
representation of emotional displays with a reduced amount of visual
information related to human upper-body movements. Razzaq et al. [303]
utilized skeletal joint features from a Kinect v2 sensor, to build mesh
distance features and mesh angular features for upper-body emotion
representation. Santhoshkumar and Geetha successively proposed two EBGR
methods, by using histogram of orientation gradient (HOG) and HOG-KLT features
from the sequences [304], or by calculating and extracting four kinds of
geometric body expressive features [305]. They achieved the best recognition
accuracies of 95.9% and 93.1% on GEMEP, respectively.
**Classifier-based EBGR**. The common classifiers for ML-based EBGR include
decision tree, ensemble decision tree, KNN, SVM, etc. For example, Kapur et
al. [306] proposed gesture-based affective computing by using five different
classifiers to analyze full-body skeletal movements captured by the Vicon
system. Different from full-body based EBGR, Saha et al. [299] focused on the
upper-body based EBGR by employing some comparisons, using five classic ML-
based classifiers in terms of the average classification accuracy and
computation time. The experimental results showed that the ensemble decision
tree achieved the highest recognition rate of 90.83%, under an acceptable
execution efficiency. Maret et al. [300] also utilized five commonly used
classifiers, whereby the genetic algorithm was invoked to search the optimal
parameters of the recognition process.
**Non-acted EBGR**. While the above works show good results on acted data,
they fail to address the more difficult non-acted scenario due to the
exaggerated displays of emotional body motions. Kleinsmith et al. [307] used
low-level posture descriptions and feature analysis using non-acted body
gestures to implement MLP-based emotion and dimension recognition. Volkova et
al. [308] further investigated whether emotional body expressions could be
recognized when they were recorded during natural scenarios. To explore the
emotion recognition from daily actions, Fourati et al. [309] recorded
varieties of emotional body expressions in daily actions and constructed a new
database.
##### 5.3.2.2. DL-based EBGR
Although DL-based EBGR systems do not require to design of a tailored-feature
extractor, they often pre-processed the input data based on the commonly-used
pose estimation models or low-level feature extractors [310]. High-level
features can be learned in spatial, temporal or spatial-temporal dimensions
through the CNN-based network [311], the LSTM-based network [310] or the CNN-
LSTM based network [312]. Recently, many studies have demonstrated theadvantages of effectively combining different DL-based models and the
attention mechanism [313] to improve the performance of EBGR.
**ConvNet-RNN learning for EBGR.** Ly et al. [312] first utilized the hashing
model to detect keyframes of upper-body videos, and then applied a CNN-LSTM
network to extract sequence information. The model employed on FABO achieved
recognition accuracy of 72.5%. Avola et al. [314] investigated a framework of
non-acted EBGR based on 3D skeleton and DNNs, which consist of MLP and
N-stacked LSTMs. Shen et al. [310] proposed full-body based EBGR by fusing RGB
features of optical flow extracted by the temporal segment network [315], and
skeleton features extracted by spatial-temporal graph convolutional networks
[316].
**Zero-shot based EBGR.** Due to the complexity and diversity of human emotion
through body gestures, it is difficult to enumerate all emotional body
gestures and collect enough samples for each category [317]. Therefore, the
existing methods fail to determine which emotional state a new body gesture
belongs to. In order to recognize unknown emotions from seen body gestures or
to know emotions from unseen body gestures, Banerjee et al. [318] and Wu et
al. [313] introduced the generalized zero-shot learning framework, including
CNN-based feature extraction, autoencoder-based representation learning, and
emotion classifier.
### 5.4. Physiological-based emotion recognition
Facial expressions, text, voice, and body gesture from a human being can be
easily collected. As the reliability of physical information largely depends
on the social environment and cultural background, and personalities of
testers, their emotions are easy to be forged [20]. However, the changes in
physiological signals directly reflect the changes in human emotions, which
can help humans recognize, interpret and simulate emotional states [30,319].
Therefore, it is highly objective to learn human emotions through
physiological signals [320].
The diagram of physiological-based emotion recognition, as shown in Fig. 5,
typically includes the following five aspects: 1) Stimulating subjects?
emotions with images, music and videos; 2) Recording physiological signals
that mainly include EEG, skin conductance, RESP, heart rate, EMG, and ECG; 3)
Extracting features through physiological signals pre-processing, feature
analysis, feature selection and reduction; 4) Training the classification
model such as SVM, KNN, LDA, RF, NB and NN, etc.; and 5) Emotion recognition
based on a discrete emotion model or a dimensional emotion model.
1. Download: Download high-res image (607KB)
2. Download: Download full-size image
Fig. 5. The diagram of emotion recognition via physiological signals.
Among the above physiological emotion signals, EEG or ECG can provide simple,
objective, and reliable data for identifying emotions [321], and is most
frequently used for sentiment analysis and emotion recognition. Afterwards, we
review EEG-based and ECG-based emotion recognition in this sub-section. Table
10 shows an overview of representative methods for physiological-based emotion
recognition, as detailed next.
Table 10. Overview of the representative methods for physiological-based
emotion recognition.
Publication| Year| Feature Representation| Classifier| Database| Performance
(%)
---|---|---|---|---|---
_**EEG-based Emotion Recognition**_
[328]| 2016| mRMR-based| SVM| DEAP| A/V: 73.06/73.14
[326]| 2019| ADMM-based| Multi-class SGL| MAHNOB| 3 classes: 74.00[330]| 2018| BiDANN| LSTM+Softmax| SEED| 3 classes: 92.38
[334]| 2020| Spatial-temporal| Pooling+Softmax| SEED
DEAP| 3 classes: 90.63 A/V: 92.92/92.24
** _ECG-based Emotion Recognition_**
[342]| 2017| Feature Fusion+LLN| KNN| Mahnob-HCI| A/V: 66.10/64.10
[339]| 2017| Feature Fusion| SVM| BioVid EmoDB| 2 classes: 79.15
[343]| 2019| EmotionalGAN| SVM| DECAF| A/V: 58.60/59.40
[344]| 2020| Self-supervised CNN| FC+Sigmoid| SWELL; AMIGOS| A/V: 96.00/96.30;
A/V: 85.80/83.70
#### 5.4.1. EEG-based emotion recognition
Compared with other peripheral neuro-physiological signals, EEG can directly
measure the changes of brain activities, which provides internal features of
emotional states [20]. Besides, EEG with a high temporal resolution makes it
possible to monitor a real-time emotional state. Therefore, various EEG-based
emotion recognition techniques [29,322] have been developed recently.
##### 5.4.1.1. ML-based EEG emotion recognition
The performance of ML-based EEG-based emotion recognition [20] depends on how
to properly design feature extraction, feature dimensionality reduction (or
feature selection), and classification methods.
The core objective of feature extraction is to extract important EEG features,
which contain time-domain, frequency-domain, and time?frequency-domain
features. The fast Fourier transform (FFT) analysis is often used to transform
the EEG into the power spectrum [323]. Feature dimensionality reduction is an
important step in EEG-based emotion recognition due to the redundancy of EEG.
Yoon and Chung [323] used the FFT analysis for feature extraction and the
Pearson correlation coefficient (PCC) for feature selection. Yin et al.
successively designed a novel transfer recursive feature elimination [324] and
the dynamical recursive feature elimination [325] for EEG feature selection,
which determined sets of robust EEG features. Puk et al. [326] investigated an
alternating direction method of multipliers ADMM-based sparse group lasso
(SGL), with hierarchical splitting for recognizing the discrete states of
three emotions. He et al. [327] designed a firefly integrated optimization
algorithm (FIOA) to realize the optimal selection of the features subset and
the classifier, without stagnating in the local optimum for the automatic
emotion recognition. The FIOA evaluated on DEAP can gradually regulate the
balance between ACC and feature number in the whole optimization process,
which achieves an accuracy of 95.00%.
The commonly used ML-based classifier for EEG-based emotion recognition is SVM
or its variations. For example, Atkinson and Campos [328] combined the mutual
information-based EEG feature selection approach and SVM to improve the
recognition accuracy. Yin et al. successively designed the linear least square
SVM [324] and the selected least square SVM [325] for EEG-based emotion
recognition.
##### 5.4.1.2. DL-based EEG emotion recognition
Different from the pipeline of ML-based EEG-based emotion recognition, Gao et
al. [329] utilized CNNs and restricted Boltzmann machine (RBM) with three
layers, to simultaneously learn the features and classify EEG-based emotions.
The CNN-based emotion recognition with subject-tied protocol achieves an
accuracy of 68.4%. The affective computing team directed by Prof. Zheng [330],
[331], [332] from Southeast University, China, has proposed various EEG-based
emotion recognition networks such as bi-hemispheres domain adversarial neural
network (BiDANN) [330], instance-adaptive graph network [331] and variational
pathway reasoning [332].
As the biological topology among different brain regions can capture bothlocal and global relations among different EEG channels, Zhong et al. [333]
designed a regularized graph neural network (RGNN), which consists of both
regularization operators of node-wise domain adversarial training and emotion-
aware distribution learning. Gao et al. [334] proposed a channel-fused dense
convolutional network for EEG-based emotion recognition. Considering the
spatial information from adjacent channels and symmetric channels, Cui et al.
[335] proposed the regional-asymmetric CNN (RACNN), including temporal,
regional and asymmetric feature extractors. An asymmetric differential layer
is introduced into three feature extractors to capture the discriminative
information, by considering the asymmetry property of emotion responses. The
RACNN achieves prominent results with average accuracies of 96.88% and 96.28%
on DEAP [96] and DREAMER [321], respectively.
#### 5.4.2. ECG-based emotion recognition
ECG records the physiological changes of the human heart in different
situations through the autonomous nervous system activity. With the change of
human emotion or sentiment state, ECG will detect the corresponding waveform
transformation [336], which can provide enough information in emotion
recognition. Next, we introduce ECG-based emotion recognition using ML models
or DL models.
##### 5.4.2.1. ML-based ECG emotion recognition
Following the diagram of physiological-based emotion recognition, Hsu et al.
[336] first constructed a music-induced ECG emotion database, and then
developed a nine-stage framework for automatic ECG-based emotion recognition,
including 1) Signal preprocessing; 2) R-wave detection; 3) Windowing ECG
recording; 4) Noisy epoch rejection; 5) Feature extraction based on the time-,
and frequency-domain and nonlinear analysis; 6) Feature normalization; 7)
Feature selection using sequential forward floating selection-kernel-based
class separability; 8) Feature reduction based on the generalized discriminant
analysis, and 9) Classifier construction with LS-SVM. Note that not all ML-
based ECG emotion recognition methods follow the abovementioned steps, but the
steps of feature extraction, feature selection and classifier are
indispensable.
The ECG features can be directly extracted in the time domain. Bong et al.
[337] extracted three time-domain features: heart rate, mean R peak amplitude,
and mean R-R intervals to detect human emotional stress detection. Another
common way is to transform time-domain ECG features into those in other
domains. For example, Jerritta et al. [338] applied FFT, Discrete Wavelet
Transform (DWT), and Hilbert Huang Transform (HHT) to transform ECG signals
into frequency-domain features, and then utilized PCA and Tabu search to
select key features in low-, high- and total (low and high together) frequency
range. Note that both [338] and [337] used the SVM to implement the final
emotion classification.
It may be beneficial to combine different types of ECG features. Cheng et al.
[339] computed linear-derived features, nonlinear-derived features, time-
domain features, and time-frequency domain features from ECG and its derived
heart rate variability, and then fused them for SVM-based negative emotion
detection. The experiments implemented on BioVid Emo DB [340] show that [339]
achieves an accuracy of 79.51%, with a minor time cost of 0.13ms in the
classification of positive and negative emotion states.
Statistic ECG features are also useful. Selvaraj et al. [341] proposed a non-
linear Hurst feature extraction method by combining the rescaled range
statistics and the finite variance scaling with higher-order statistics. They
further investigated the performances of four conventional classifiers of NB,
RT, KNN and fuzzy KNN for emotion classification. A novel Hurst feature andfuzzy KNN achieved recognition accuracy of 92.87%. Ferdinando et al. [342]
investigated the effect of feature dimensionality reduction in ECG-based
emotion recognition. A bivariate empirical mode decomposition was employed to
compute features for KNN based on the statistical distribution of dominant
frequencies.
##### 5.4.2.2. DL-based FER ECG emotion recognition
Chen et al. [343] proposed a novel EmotionalGAN-based framework to enhance the
generalization ability of emotion recognition by incorporating the augmented
ECG samples generated by EmotionalGAN. Compared with that using only original
data, around 5% improvement of average accuracy shows the significance of GAN-
based models on the emotion recognition task. Sarkar and Etemad [344]
introduced one self-supervised approach to training the signal transformation
recognition network (STRN) to learn spatiotemporal features and abstract
representations of the ECG. The weights of convolutional layers in the STRN
are frozen and then train two dense layers to classify arousal and valence.
## 6\. Multimodal affective analysis
We have reviewed the relevant studies of unimodal feature extraction and
emotion classification, in this section we then describe how to integrate
multiple unimodal signals to develop a framework of multimodal affective
analysis [345], which can be regarded as the fusion of different modalities
[346], aiming to achieve a more accurate result and more comprehensive
understanding than unimodal affect recognition [347,348].
Nowadays, most reviews of multimodal affective analysis [12,14,17] focus on
multimodal fusion strategies and classify them into feature-level fusion (or
early fusion), decision-level fusion (or late fusion), model-level fusion, and
hybrid-level fusion. However, the multimodal affective analysis can be also
varied with combinations of different modalities. Therefore, we categorize
multimodal affective analysis into multi-physical modality fusion for
affective analysis, multi-physiological modality fusion for affective
analysis, and physical-physiological modality fusion for affective analysis,
and further classify them based on four kinds of fusion strategies. Fig. 6
illustrates prominent examples of using different fusion strategies:
* 1
Feature-level fusion combines features extracted from the multimodal inputs to
form one general feature vector, which is then sent into a classifier. Fig. 6
(a), (b) and (c) show examples based on feature-level fusion for visual-audio
modalities, text-audio modalities, and visual-audio-text modalities,
respectively.
* 2
Decision-level fusion connects all decision vectors independently generated
from each modality into one feature vector. Fig. 6 (d) shows one example based
on decision-level fusion for multi-physiological modalities of EGG, ECG and
EDA.
* 3
Model-level fusion discovers the correlation properties between features
extracted from different modalities and uses or designs a fusion model with
relaxed and smooth types such as HMM and two-stage ELM [349]. Fig. 6 (e) and
(f) are two examples based on model-level fusion for physical-physiological
modalities and visual-audio-text modalities, respectively.
* 4
Hybrid fusion combines feature-level fusion and decision-level fusion. Fig. 6
(g) shows one example based on hybrid fusion for visual-audio-text modalities.
1. Download: Download high-res image (2MB)
2. Download: Download full-size imageFig. 6. Taxonomy of multimodal affective analysis. (a) Feature-level fusion
for visual-audio emotion recognition adopted from [360]; (b) Feature-level
fusion for text-audio emotion recognition adopted from [361]; (c) Feature-
level fusion for visual-audio-text emotion recognition adopted from [362]; (d)
Decision-level fusion for multi-physiological affective analysis adopted from
[363]; (e) Model-level fusion for physical-physiological affective analysis
adopted from [35]; (f) Model-level fusion for visual-audio-text emotion
recognition adopted from [23]; (g) Hybrid-level fusion for visual-audio-text
emotion recognition adopted from [105].
### 6.1. Multi-physical modality fusion for affective analysis
In light of common manners of modality combinations, we categorize multi-
physical modalities fusion for affective analysis into visual-audio emotion
recognition [350,31], text-audio emotion recognition [351,352], and visual-
audio-text emotion recognition [353,354]. Table 11 shows an overview of
representative methods for multi-physical affective analysis, as detailed
next.
Table 11. Overview of representative methods for multi-physical affective
analysis.
Publication| Year| Feature Representation| Classifier| Fusion Strategy|
Database| Performance (%)
---|---|---|---|---|---|---
**_Visual-audio Emotion Recognition_**
[360]| 2018| Acoustic, Geometric and HOG-TOP| Multiple kernel SVM| Feature-
level| CK; AFEW| 17 classes: 95.7 17 classes: 45.20
[31]| 2017| CNN, Resnet| LSTM| Feature-level| RECOLA| A/V: 78.80/73.20
[364]| 2020| 2D ResNet+Attention 3D ResNet+Attention| FC| Feature-level|
VideoEmotion-8
Ekman-6| 8 classes: 54.50 6 classes: 55.30
[367]| 2020| Multitask CNN| Meta-Classifier| Decision-level| eNTERFACCE| 6
classes: 81.36
[278]| 2017| C3D + DBN| Score-level Fusion| Model-based| eNTERFACE| 6 classes:
89.39
[349]| 2019| 2D CNN, 3D CNN| ELM-based fusion, SVM| Model-based| Big Data
eNTERFACE| 3 classes: 91.30
6 classes: 78.42
** _Text-audio Emotion Recognition_**
[361]| 2020| A-DCNN, T-DNN Self-attention| FC| Feature-level| IEMOCAP| 24
classes: 80.51
34 classes: 79.22
[374]| 2011| Acoustic-prosodic Semantic labels| Base classifiers, MDT, MaxEnt|
Decision-level| 2033 utterances| 4 classes: 83.55
44 classes: 85.79
[376]| 2020| Acoustic features Word embeddings| Pooling Scalar weight fusion|
Feature-level Decision-level| IEMOCAP; MSP-PODCAST| 565.10/558.20
563.90/558.00
** _Visual-audio-text Emotion Recognition Emotion Recognition_**
[362]| 2020| Proxy and Attention
Multiplicative fusion| FC| Feature-level| IEMOCAP CMU-MOSEI| 4 classes1:82.70
6 classes1:89.00
[382]| 2015| CNN, handcrafted, CFS, PCA| MKL| Feature-level Decision-level|
HOW| 3 classes: 88.60
3 classes: 86.27
[23]| 2019| Three Bi-GRU| CIM-attention| Model-based| CMU-MOSEI| 2Multi-label:
62.8022-class: 80.50
[105]| 2013| Facial movement, MFCC| SVM, BiLSMT| Hybrid-level| SEMAINE| 665.20
1
LOSO;
2
WA;
3
UA;
4
The recognition accuracy considering the individual personality trait for
personalized application;
5
The median values evaluated on IEMOCAP/MSP-PODCAST databases;
6
Mean WA of Arousal, Expectation, Power and Valence.
#### 6.1.1. Visual-audio emotion recognition
Visual and audio signals are the most natural and affective cues to express
emotions when people communicate in daily life [355]. Many research works
[[356], [357], [358], [359],350] show that visual-audio emotion recognition
outperforms visual or audio emotion recognition.
**Feature-level fusion**. Chen et al. [360] (Fig. 6 (a)) proposed ML-based
visual-audio emotion recognition by fusing dynamic HOG-TOP texture features
and acoustic/ geometric features. These two kinds of features are then sent
into a multiple kernel SVM for FER both under the wild and lab-controlled
environments. Tzirakis et al. [31] adopted a CNN and a deep residual network
to extract audio and visual features, respectively; and then concatenated
these visual-audio features to feed into a 2-layer LSTM to predict Arousal-
Valence values.
Various attention mechanisms have been successfully applied for visual-audio
emotion recognition [364,365]. For example, Zhang et al. [365] introduced an
embedded attention mechanism to obtain the emotion-related regions from their
respective modalities. To deeply fuse video-audio features, the factorized
bilinear pooling (FBP) fusion strategy was proposed in consideration of
feature differences in expressions of video frames. The FBP achieves a
recognition accuracy of 62.48% on AFEW of the audio-video sub-challenge in
EmotiW2018. Zhao et al [364] proposed a novel deep visual-audio attention
network (VAANet) with specific attention modules and polarity-consistent
cross-entropy loss. Specifically, spatial, channel-wise, and temporal
attentions are integrated with a 3D CNN [366] for video frame segments, and
spatial attention is integrated with a 2D CNN (ResNet-18) for audio MFCC
segments.
**Decision-level fusion**. Hao et al. [367] proposed an ensemble visual-audio
emotion recognition framework based on multi-task and blending learning with
multiple features. Specifically, SVM classifiers and CNNs for handcraft-based
and DL-based visual-audio features generated four sub-models, which are then
fused to predict the final emotion based on the blending ensemble algorithm.
The work [367] achieved the average accuracies of 81.36% (speaker-independent)
and 78.42% (speaker-dependent) on eNTERFACE [368].
Model-level fusion. It requires an ML-based model (e.g., HMM, Kalman filters
and DBN) to construct the relationships of different modalities to make
decisions. Lin et al. [33] proposed a semi-coupled HMM (SC-HMM) to align the
temporal relations of audio-visual signals, followed by a Bayesian classifier
with an error weighted scheme. The SC-HMM with Bayesian achieves prominent
average accuracies of 90.59% (four-class emotions) and 78.13% (four quadrants)on the multimedia human-machine communication posed database and public
SEMAINE database, respectively. Glodek et al. [369] designed Kalman filters
based on a Markov model to combine temporally ordered classifier decisions
with the reject option to recognize the affective states. For audio-visual
data, the unimodal feature extractors and base classifiers are fused based on
a Kalman filter and confidence measures.
Zhang et al. [37] utilized CNNs to extract audio-visual features and developed
a deep fusion method (DBNs) for feature fusion. A linear SVM classifier
achieved average accuracies of 80.36%, 54.57% and 85.97% on RML, eNTERFACE05
and BAUM-1s, respectively. Similarly, Nguyen et al. [278] proposed a score-
level fusion approach to compute all likelihoods of DBNs trained on
spatiotemporal information of the audio-video streams. Hossain and Muhammad
[349] used a 2D CNN and a 3D CNN to extract high-level representations of the
pre-processed audio-video signals. A two-stage ELM based fusion model with an
SVM classifier was designed to estimate different emotional states.
#### 6.1.2. Text-audio emotion recognition
Although SER [26,163,178] and TSA [156,370] have achieved significant
progress, performing the two tasks separately makes it hard to achieve
compelling results [361]. Text-audio emotion recognition approaches use
linguistic content and speech clues to enhance the performance of the unimodal
emotion recognition system [352,371].
**Feature-level fusion**. Yoon et al. [372] proposed a deep dual recurrent
neural network for encoding audio-text sequences and then concatenated their
outputs to predict the final emotion. It achieves an accuracy of 71.8% (four
classes) on IEMOCAP. Afterwards, Cai et al. [373] designed an improved CNN and
Bi-LSTM to extract spatial features and capture their temporal dynamics.
Considering the phonemes effect on emotion recognition, Zhang et al. [371]
utilized a temporal CNN to investigate the acoustic and lexical properties of
phonetic information based on unimodal, multimodal single-stage fusion, and
multimodal multi-stage fusion systems. To deeply exploit and fuse text-
acoustic features for emotion classification, Priyasad et al. [361] (Fig. 6
(b)) designed T-DNN (DCNNs and Bi-RNN followed by other DCNNs) and A-DCNN
(SincNet with band-pass filters followed by a DCNN) to extract textual
features and learning acoustic features, respectively. Textual-acoustic
features are fused with different attention strategies (self-attention or no
attention) to predict four emotions on IEMOCAP.
**Decision-level fusion.** Wu et al. [374] fused acoustic-prosodic information
based on a meta decision tree (MDT) with multiple base classifiers (e.g. GMM,
SVM, and MLP), and employed a maximum entropy model (MaxEnt) to establish the
relationship between emotional states and emotion association rules in
semantic labels for speech emotion recognition and text emotion recognition,
respectively. Using the weighted product fusion strategy, AP-based and SL-
based emotion confidences are fused to predict final emotions.
**Feature-level fusion versus decision-level fusion.** To verify which
feature-level fusion or decision-level fusion is more effective in text-audio
emotion recognition, Jin et al. [375] firstly generated new lexical features
and different acoustic features and then utilized two fusion strategies to
achieve four-class recognition accuracies of 55.4% and 69.2% on IEMOCAP,
respectively. Considering the emotional dialogue composed of sound and spoken
content, Pepino et al. [376] exploited multiple dual RNNs to encode audio-text
sequences. The feature-level fusion and decision-level fusion approaches in 3
different ways are explored to compare their performances.
#### 6.1.3. Visual-audio-text emotion recognition
The vocal modulation, facial expression, and context-based text provideimportant cues to better identify the true affective states of the opinion
holder [377,378]. For example, when a lady is proposed and then she says ?I
do? in tears, none of textual-based, audio-based, or visual-based emotion
recognition models can predict confident results. While visual-audio-text
emotion recognition leads to a better solution.
**Feature-level fusion.** Veronica et al. [379] used BoW, OpenEAR [380], and
vision software to extract linguistic, audio and visual features,
respectively. Compared with three unimodal and three bimodal models, the
multimodal (text-audio-visual) model can significantly improve the model
performance over the individual use of one modality or fusion of any two
modalities. In addition, Poria et al. [354] utilized a standard RNN, a deep
CNN, and openSMILE to capture temporal dependence of visual data, spatial
information of textual data and low-level descriptors of audio data,
respectively. The MKL is further designed for feature selection of different
modalities to improve the recognition results. Mittal et al. [362] (Fig. 6
(c)) proposed the multiplicative multimodal emotion recognition (M3ER):
firstly, feature vectors are extracted from the raw three modalities; then,
these features are transferred into modality check step to retain the
effective features and discard the ineffectual ones which are used to
regenerate proxy feature vectors; finally, selected features are fused to
predict six emotions based on the multiplicative feature-level fusion combined
with attention module. The M3ER achieved better recognition accuracies of
82.7% and 89.0% on IEMOCAP and CMU-MOSEI, respectively. Different from
language-independent approaches for English or German sentiment analysis,
Chinese sentiment analysis not only understands symbols with explicit meaning,
but also captures phonemic orthography (tonal language) with implicit meaning.
Based on this assumption, Peng et al. [381] proposed reinforcement learning
based disambiguate intonation for sentiment analysis (DISA) which consists of
policy network, embedding lookup, loss computation, and feature-level fusion
of three modalities. By integrating phonetic features with textual and visual
representations, multimodal Chinese sentiment analysis significantly
outperforms than unimodal models.
**Feature-level fusion versus decision-level fusion.** Poria et al. [382]
utilized a CNN to extract textual features, calculated handcrafted features
from visual data, and generated audio features by the openSMILE. These three
kinds of feature vectors were fused and then used to train a classifier based
on the MKL. Feature-level fusion and decision-level fusion with feature
selection are conducted to implement unimodal, bimodal and multimodal emotion
recognition employed on HOW [104]. Experimental results show that textual
emotion recognition achieves the best performance among three unimodal emotion
recognition methods, and in feature-level fusion, visual-audio-text emotion
recognition outperforms other unimodal and bimodal emotion recognition.
Besides, the accuracy of feature-level fusion is significantly higher than
that of decision-level fusion at the expense of computational speed.
**Model-level fusion.** Considering the interdependencies and relations among
the utterances of a video [378], Poria et al. [383] introduced some variants
of the contextual LSTM into the hierarchical architecture to extract context-
dependent multimodal utterance features. The visual-audio-text features are
concatenated and then fed into a contextual LSTM to predict emotions, reaching
80.3%, 68.1% and 76.1% on MOSI, MOUD and IEMOCAP, respectively. Akhtar et al.
[23] (Fig. 6 (f)) proposed an end-to-end DL-based multi-task learning for
multimodal emotion recognition and sentiment analysis. Due to the unequal
importance of three modalities, a context-level inter-modal (CIM) attention
module is designed to learn the joint association between textual-audio-visualfeatures of utterances captured by three bi-directional gated recurrent unit
(biGRU) networks. These features of three CIMs and three individual modalities
are concatenated to generate high-level feature representation to predict
sentiments and multi-label emotions.
**Hybrid-level fusion.** To take full advantage of feature-based fusion and
decision-based fusion, and overcome the disadvantages of both, Wöllmer et al.
[105] (Fig. 6 (g)) used BoW or Bag-of-N-Gram (BoNG) features with SVM for
linguistic sentiment classification, and fused audio-video features with Bi-
LSTM for audio-visual emotion recognition. And then these two prediction
results were fused to obtain the final emotion under a strategy of weighted
fusion.
### 6.2. Multi-physiological modality fusion for affective analysis
With the enhancement and refinement of wearable technologies, automatic
affective analysis based on multi-physiological modalities has attracted more
attention [384]. However, due to the complexity of emotion and significant
individual differences in physiological responses [348], it is difficult to
achieve satisfactory prediction performance with EEG-based or ECG-based
emotion recognition. In this sub-section, we review multi-physiological
modality fusion for affective analysis. Table 12 provides an overview of
representative methods for multi-physiological affective analysis, as detailed
next.
Table 12. Overview of some representative methods for multi-physiological
affective analysis.
Publication| Year| Signal| Architecture| Fusion Strategy| Database|
Performance (%)
---|---|---|---|---|---|---
[347]| 2014| EEG, GSR, ST, BVP, RESP, EMG, EOG| DWT, Multiple kernel SVM|
Feature-level| DEAP| 13 classes: 85.00
[387]| 2019| EDA, BVP, zEMG| DBN, FGSVM| Feature-level| DEAP| 5 classes: 89.53
A/V: 65.10/61.80
[363]| 2019| EEG, ECG, EDA| AILE-VAE, SVM| Decision-level| AMIGOS| A/V: 68.80/
67.00
[390]| 2020| EEG, ECG, GSR| Attention-based LSTM-RNNs, DNN| Decision-level|
AMIGOS| A/V: 83.3/79.40
[392]| 2017| EEG, EOG, EMG, ST, GSR, BVP, RESP| Deep stacked AE, Bayesian
model| Model-based| DEAP| A/V: 77.19/76.17
**Feature-level fusion.** Li et al. [385] collected multi-physiological
emotion signals induced via music, picture or video, and extracted low-level
descriptors and statistical features from the signal waveforms. Then, they
fused these features for emotion prediction using ML-based classifiers with a
Group-based IRS (Individual Response Specificity) model. The combination of RF
with Group-based IRS achieved a higher accuracy of about 90% in the imbalance
of the emotion database. Similarly, Nakisa et al. [386] pre-processed,
extracted and fused time-frequency features of EEG and BVP, and then fed them
into an LSTM network, whose hyperparameters were optimized by differential
evolution (DE). On their self-collected dataset, its overall accuracy was
77.68% for the four-quadrant dimensional emotions.
Using ECG, GSR, ST, BVP, RESP, EMG, and EOG selected from DEAP [96], Verma and
Tiwary [347] extracted multi-resolution features based on DWT and used them to
estimate the valence-arousal-dominance emotions. Hossain et al. [387]
extracted 9 statistical features, 9 power spectral density features, and 46
DBN features of EDA, Photo plethysmogram, and zEMG. These features were fused
to train a fine gaussian SVM to recognize 5 basic emotions. Ma et al. [388]
designed a multimodal residual LSTM network for learning the dependency ofhigh-level temporal-feature EEG, EOG, and EMG to predict emotions. It achieves
the classification accuracies of 92.87% and 92.30% for arousal and valence,
respectively.
**Decision-level fusion.** Wei et al. [389] selected EEG, ECG, RESP and GSR
from MAHNOB-HCI [107], and designed a linear fusing weight matrix to fuse the
outputs from multiple SVM classifiers to predict 5 basic emotions. Its highest
average accuracy of recognition was 84.6%. To explore the emotional
physiological-based temporal features, Li et al. [390] transformed EEG, ECG
and GSR selected from AMIGOS [99] into spectrogram images to represent their
time-frequency information. The attention-based Bi-LSTM-RNNs was designed to
automatically learn the best temporal features, which were further fed into a
DNN to predict the probability of unimodal emotion. The final emotion state
was computed based on either an equal weights scheme or a variable weights
scheme. As personality-specific human beings often show different
physiological reactions after being stimulated by emotional elements, Yang and
Lee [363] (Fig. 6 (d)) proposed an attribute-invariance loss embedded
variational autoencoder to learn personality-invariant representations. With
EEG, ECG and EDA selected from AMIGOS, different features were extracted and
then fed into different SVM classifiers to predict unimodal classification
results. Dar et al. [391] designed a 2D-CNN for EEG and combined LSTM and
1D-CNN for ECG and GSR. By using majority voting based on decisions made by
multiple classifiers, the framework achieved the overall highest accuracy of
99.0% and 90.8% for AMIGOS [99] and DREAMER [321], respectively.
**Model-level fusion.** Yin et al. [392] first pre-processed and extracted 425
salient physiological features of 7 signals selected from DEAP. Separate deep
hidden neurons in stacked-AEs were then investigated to extract higher-level
abstractions of these salient features. An ensemble of deep classifiers with
an adjacent graph-based hierarchical feature fusion network was designed for
recognizing emotions based on a Bayesian model.
### 6.3. Physical-physiological modality fusion for affective analysis
Since the change of human emotions is a complicated psycho-physiological
activity, the research on affective analysis is related to many cues (e.g.,
behavioral, physical, and psychological signals) [393,394]. Researchers have
focused on the physical-physiological modality fusion for affective analysis
via mining the strong external expression of physical data and undisguisable
internal changes of physiological signals [395], [396], [397]. Table 13
provides an overview of representative methods for physical-physiological
affective analysis, as detailed next.
Table 13. Overview of the representative methods for physical-physiological
affective analysis.
Publication| Year| Signal| Architecture| Fusion Strategy| Database|
Performance (%)
---|---|---|---|---|---|---
[397]| 2020| EDA, Music| RTCAN-1D| Feature-level| PMEmo| A/V: 82.51/ 77.30
[400]| 2020| EEG, Video| LSTM, Self-attention| Model-level| DEAP| 2AAVA: 85.90
[395]| 2015| EEG, Eye Movements| Feature extraction Fuzzy integral fusion|
Feature-level Decision-level| SEED| 3 classes: 83.70
13 classes: 87.59
[35]| 2020| ECG, SCL, tEMG, Video| Inception-ResNet-v2, BDBN, SVM| Model-
level| BioVid Emo DB| 5 classes: 80.89
3AAVAL: 84.29
1
The fuzzy integral fusion strategy;
2AAVA = Average accuracy of Valence-Arousal;
3
AAVAL = Average accuracy of Valence-Arousal-Liking.
**Feature-level fusion.** To fully leverage the advantages of the
complementary property between EEG and eye movement, Liu et al. [398] proposed
a bimodal deep AE based on an RBM to recognize emotions on SEED and DEAP.
Soleymani et al. [399] designed a framework of video-EEG based emotion
detection using an LSTM-RNN and continuous conditional random fields. Wu et
al. [400] proposed a hierarchical LSTM with a self-attention mechanism to fuse
the facial features and EEG features to calculate the final emotion. Yin et
al. [397] proposed an efficient end-to-end framework of EDA-music fused
emotion recognition, denominating it as a 1-D residual temporal and channel
attention network (RTCAN-1D). Specially, the RTCAN consists of shallow feature
extraction, residual feature extraction, the attention module stacked by a
signal channel attention module, and a residual non-local temporal attention
module. It achieved outstanding performances in AMIGOS, DEAP and PMEmo [401].
**Feature-level fusion versus decision-level fusion.** Huang et al. [34]
proposed video-EEG based multimodal affective analysis by fusing external
facial expression of spatiotemporal local monogenic binary pattern and
discriminative spectral power of internal EEG on feature-level and decision-
level aspects. According to their experiments, the multimodal affective
analysis achieves a better performance than the unimodal emotion recognition;
and when it comes to fusion strategies, decision-level fusion outperforms
feature-level fusion in the recognition of valence and arousal on MAHNOB-HCI.
**Model-level fusion.** Wang et al. [35] (Fig. 6 (e)) designed a multimodal
deep belief network for fusing and optimizing multiple psycho-physiological
features, a bimodal DBN (BDBN) for representing discriminative video-based
features, and another BDBN to extract high multimodal features of both video
and psycho-physiological modalities. The SVM was employed for emotion
recognition after the features of all modalities were integrated into a
unified dimension.
## 7\. Discussions
In this review, we have involved emotion models and databases commonly used
for affective computing, as well as unimodal affect recognition and multimodal
affective analysis. In this section, we mainly discuss the following aspects:
* 1)
Effects of different signals (textual, audio, visual, or physiological) on
unimodal affect recognition [152,194,244,289,327,331];
* 2)
Effects of modality combinations and fusion strategies on multimodal affective
analysis [37,371,375,383,388,400];
* 3)
Effects of ML-based techniques [127,184,249,305,326] or DL-based methods
[146,203,273,283,316,335] on affective computing;
* 4)
Effects of some potential factors (e.g., released databases and performance
metrics) on affective computing;
* 5)
Applications of affective computing in real-life scenarios.
### 7.1. Effects of different signals on unimodal affect recognition
According to unimodal affect recognition based on the text [126,402], audio
[178,194,199], visual [253,281,403], EEG [404], or ECG [336], we can find that
the most widely used modality is the visual signal, mainly consisting of
facial expressions and body gestures. The number of visual-based emotionrecognition systems is comparable to the sum of that of systems based on other
modalities since the visual signals are easier to capture than other signals
and emotional information in visual signals is more helpful than other signals
in recognizing the emotion state of human beings. Visual-based emotion
recognition is more effective than audio-based emotion recognition because
audio signals are susceptible to noise [405]. However, a study [23] reveals
that textual-based affective analysis achieves the highest accuracy in emotion
recognition and sentiment analysis. Although the physiological signals
collected by wearable sensors are more difficult to obtain than physical
signals, numerous EEG-based [334] or ECG-based [336] emotion recognition
methods have been investigated and proposed due to their objective and
reliable outcomes.
### 7.2. Effects of modality combinations and fusion strategies on multimodal
affective analysis
The combination of different modalities and the fusion strategy are two key
aspects of the multimodal affective analysis. Multimodal combinations are
divided into multi-physical modalities, multi-physiological modalities, and
physical-physiological modalities. The fusion strategies consist of feature-
level fusion [361,387], decision-level fusion [375], hybrid-level fusion
[105], and model-level fusion.
In the multi-physical modalities [406], there are three kinds of combinations
of different modalities, consisting of visual-audio, text-audio and visual-
audio-text. Integrating visual and audio information can enhance performance
over unimodal affect recognition [407]. There are similar results in other
combinations of multi-physical modalities [375], in which the text modality
plays the most vital role in multimodal sentiment analysis [382,362]. In
studies of multi-physiological modality fusion for affective analysis, in
addition to EEG and ECG, other types of physiological signals (e.g., ECG, EOG,
BVP, GSR, and EMG) are jointly combined to interpret emotional states
[388,390]. The visual modality (facial expression, voice, gesture, posture,
etc.) may also be integrated with multimodal physiological signals for visual-
physiological affective analysis [397,400].
Two basic fusion strategies for multimodal affective analysis are feature-
level fusion [361,387] and decision-level fusion [375]. The concatenation
[362] or factorized bilinear pooling [365] of feature vectors is commonly used
for feature-level fusion. The majority/average voting is often used for
decision-level fusion. Linear weighted computing [374] can be utilized for
both feature-level and decision-level fusion, by employing sum or product
operators to fuse features or classification decisions of different
modalities. According to the multimodal affective analysis, we find that
feature-level fusion [388] is strikingly more common than decision-level
fusion. The performance of an affect classifier based on feature-level fusion
is significantly influenced by the time scales and metric levels of features
coming from different modalities. On the other hand, in decision-level fusion,
the input coming from each modality is modelled independently, and these
results of unimodal affect recognition are combined in the end. Compared with
feature-level fusion, decision-level fusion [362] is performed easier, but
ignores the relevance among features of different modalities. Hybrid-level
fusion [105] aims to make full use of the advantages of feature-based fusion
and decision-based fusion strategies as well as overcome the disadvantages of
either one. Unlike the above three fusion strategies, model-level fusion uses
HMM [33] or Bayesian networks [392] to establish the correlation between
features of different modalities and one relaxed fusion mode. The selection
and establishment of HMM or Bayesian have a fatal effect on the results of themodel-level fusion, which is often designed for one specific task.
### 7.3. Effects of ML-based and DL-based models on affective computing
The majority of the early works on affective computing have employed ML-based
techniques [19,10,18]. The ML-based pipeline [124,181,249,309,326] consists of
pre-processing of raw signals, hand-crafted feature extractor (feature
selection if possible), and well-designed classifiers. Although various types
of hand-crafted features have been designed for different modalities, ML-based
techniques for affective analysis are hard to be reused across similar
problems on account of their task-specific and domain-specific feature
descriptors. The commonly used ML-based classifiers are SVM [182,253], HMM
[179], GMM [180], RF [305], KNN [337] and ANN [374], of which the SVM
classifier is the most effective one, and is indeed used in most tasks of ML-
based affective computing. These ML-based classifiers are also used for final
classification when the DL-based model is only designed for unimodal feature
extraction [290] or multimodal feature analysis [35,387].
Nowadays, DL-based models have become hot spots and outperformed ML-based
models in most areas of affective computing [13,135,43,17,39] due to their
strong ability of feature representation learning. For static information
(e.g., facial and spectrogram images), CNNs and their variants are designed to
extract important and discriminative features [137,194,289]. For sequence
information (e.g., physiological signals and videos), RNNs and their variants
are designed for capturing temporal dynamics [143,201,310]. The CNN-LSTM
models can perform the deep spatial-temporal feature extraction. Adversarial
learning is widely used to improve the robustness of models by augmenting data
[206,291] and cross-domain learning [156,192]. Besides, different attention
mechanisms [142,199,203,279] and autoencoders [363,289] are integrated with
DL-based techniques to improve the overall performance. It seems that DL-based
methods have an advantage in automatically learning the most discriminative
features. However, DL-based approaches have not yet had a huge impact on
physiological emotion recognition, if compared with ML-based models [20].
### 7.4. Effects of some potential factors on affective computing
Throughout this review, we have consistently found that advances in affective
computing are driven by various database benchmarks. For example, there are
few video-physiological emotion recognition methods due to the limitations of
physical-physiological emotion databases. In contrast, the rapid development
of FER is inseparable from various baseline databases, which are publicly
available and can be freely downloaded. Besides, some large-scale visual
databases such as BU-4DFE and BP4D can be employed to pre-train the target
model to recognize facial expressions [260] or micro-expressions [408].
However, there are significant discrepancies in size, quality, and collection
conditions [274] across different databases. For example, most body gesture
emotion databases contain only several hundred samples with limited gesture
categories. What is worse, samples are typically collected in a laboratory
environment, which is often far away from real-world conditions. Furthermore,
the size and quality of databases have a more obvious effect on DL-based
emotion recognition than on ML-based emotion recognition. Many studies have
concluded that the reduced size of the available databases is a key factor in
the quest for high-performance affective analysis [409,207]. To tackle this
problem, the pre-trained DL-based models [410,411] may be transferred into
task-based models specialized for affective analysis.
Although the representation of the natural affective states has no consensus,
most affective analyses are trained and evaluated based on two types of
emotion models: discrete models and dimensional models. In building the
affective databases, either discrete or dimensional labels are typicallychosen alternatively to fit the raw signals. For example, emotional images or
sequences are typically matched with a discrete affective state (basic
emotions or polarity). Affective recognition can be divided into
classification (emotions, dimensional or polarity) and continuous dimensional
regression (Pleasure, Arousal, Dominance Expectation, or Intensity) [412,413].
The metrics of accuracy, precision and recall are generally adopted for
categorical or componential emotion classification. When the databases are
imbalanced, the F-Measure (or F1-Score) seems to be the best choice out of the
existing evaluation metrics of the emotional classification, across 10-fold
cross validation and LOSO. The weighted average recall/F1-score (WAR/WF1) and
the unweighted average recall/F1-score (UAR/UF1) are best suited for the
classification performance of visual, audio or multimodal affective analysis
[291]. On the other hand, MSE and RMSE are commonly used for the evaluation of
continuous dimensional emotion prediction [414]. In order to describe the
degree of coincidence, by integrating PCC and MSR, the coefficient concordance
correlation coefficient is advised to assess the baseline performance
assessment [415,416].
### 7.5. Applications of affective computing in real-life scenarios
In recent years, more and more research teams have shifted their focus to
applications of affective computing in real-life scenarios [417,418]. In order
to detect emotions and sentiments from the textual information, the SenticNet
directed by Erik Cambria of NTU applied the research outputs of affective
computing [106,150,419] and sentiment analysis [22,[420], [421], [422], [423],
[424]] into many aspects of daily life, including HCI [425], finance [426] and
social media monitoring and forecasting [132,427]. TSA is often used for
recommender systems, by integrating diverse feedback information [428] or
microblog texts [429]. The applications of visual emotion recognition include
course teaching [430], smarter decision aid [431], HCI [432], dynamic quality
adaption to the players in games [433], [434], [435], depression recognition
[436] and for helping medical rehabilitation children affected by the autism
spectrum condition [437]. In particular, audio and physiological signals are
often used for detecting clinical depression and stress [100,166,438] due to
the reliability and stability of audio/speech emotion signals and the
accessibility of physiological signals from wearable devices [439]. As
multimodal affective analysis can enhance the robustness and performance of
unimodal affect recognition, more researches have begun to transform them into
various real-life applications [440,441], making it a promising research
avenue.
## 8\. Conclusion and new developments
This review has comprehensively surveyed more than 400 papers, including an
overview of the recent reviews on affective computing in Section 2, and built
the taxonomy of affective computing with representative examples in Section 1.
In Section 3, we categorize current emotion models based on psychological
theories into discrete models and dimensional models, which determine the
category of the output of the affective analysis. These recognition results
via either classification or regression are evaluated by a range of
corresponding metrics. More importantly, the development of affective
computing requires benchmark databases for training and computational models
for either DL-based or ML-based affective understanding. In Section 4, we
survey five kinds of the commonly adopted baseline databases for affective
computing, which are classified into textual, audio, visual, physiological,
and multimodal databases. Most methods for affective analysis benefit from
these released databases.
In Section 5 and Section 6, we introduced recent advances of affectivecomputing, which are mainly grouped into unimodal affect recognition and
multimodal affective analysis, and further divide them into ML-based
techniques and DL-based models. The unimodal affect recognition systems are
further divided into textual sentiment analysis, speech emotion recognition,
visual emotion recognition (FER and EBGR) and physiological emotion
recognition (EEG-based and ECG-based). Traditional ML-based unimodal affect
recognition mostly investigates hand-crafted feature extractors or pre-defined
rules and interpretable classifiers. In contrast, DL-based unimodal affect
recognition further improves the ability of the feature representation and
classification by designing deeper network architectures or task-specific
network modules and learning objectives. Generally, in addition to employing
different strategies (feature-level, decision-level, model-level, or hybrid
fusion strategies), the multimodal affective analysis is divided into multi-
physical approaches (visual-audio, text-audio and visual-audio-text
modalities), multi-physiological approaches, and physical-physiological
approaches. The performance of multimodal affective analysis is mainly
affected by both modality combination and fusion strategy.
In Section 7, we discuss some important issues related to affective computing
including effects of textual, audio, visual, or physiological signals on
unimodal affect recognition, effects of modality combinations and fusion
strategies on multimodal affective analysis, the effects of ML-based and DL-
based models on affective computing, effects of some potential factors on
affective computing, and applications of affective computing in the real-life
scenarios.
Although affective computing systems using either unimodal or multimodal data
have made significant breakthroughs, there are only a few robust and effective
algorithms to predict emotion and recognize sentiment under diverse and
challenging scenes. Hence, we would like to conclude this review with many
important recommendations for future research in affective computing:
* 1)
It will be instrumental to develop new and more extended baseline databases,
particularly multimodal affect databases, consisting of different modalities
(textual, audio, visual and physiological). Conditions should include both
spontaneous and non-spontaneous scenarios, with the provision of annotating
data in both discrete and dimensional emotion models.
* 2)
There are some challenging tasks of affective analysis to be solved including
FER under partial occlusion or fake emotion expression, physiological emotion
recognition based on various complex signals, and a baseline model
specifically for both discrete emotion recognition and dimensional emotion
prediction.
* 3)
There is significant space for improving fusion strategies, particularly with
rule-based or statistic-based knowledge, to implement a mutual fusion of
different modalities that can consider the role and importance of each
modality in affect recognition.
* 4)
Zero/few-shot learning or unsupervised learning methods (e.g., self-supervised
learning) need to be further explored, particularly thanks to their potential
to enhance the robustness and stability of affective analysis under limited or
biased databases.
* 5)
A prominent application of affective analysis is robotics. Advances presented
in this review make it possible to conceive robots equipped with emotionalintelligence, which can appropriately imitate and promptly respond to mankind
affect and the surrounding environment.
## CRediT authorship contribution statement
**Yan Wang:** Conceptualization, Investigation, Visualization, Writing ?
original draft, Writing ? review & editing. **Wei Song:** Writing ? review &
editing, Supervision, Funding acquisition. **Wei Tao:** Writing ? original
draft, Writing ? review & editing. Antonio Liotta: Writing ? review & editing.
**Dawei Yang:** Writing ? original draft, Writing ? review & editing. **Xinlei
Li:** Writing ? review & editing. **Shuyong Gao:** Writing ? review & editing.
**Yixuan Sun:** Writing ? review & editing. **Weifeng Ge:** Writing ? review &
editing. **Wei Zhang:** Writing ? review & editing. **Wenqiang Zhang:**
Supervision, Writing ? review & editing, Funding acquisition.
## Declaration of Competing Interest
We declare that we have no financial and personal relationships with other
people or organizations that can inappropriately influence our work, there is
no professional or other personal interest of any nature or kind in any
product, service and/or company that could be construed as influencing the
position presented in, or the review of, the manuscript entitled.
## Acknowledgement
This work was supported by National Key R&D Program of China (2020AAA0108301,
2019YFC1711800), National Natural Science Foundation of China (No. 62072112),
Fudan University-CIOMP Joint Fund (FC2019-005), and partly supported by the
National Natural Science Foundation of China under Grant No. 61972240.
Recommended articles"
18,18,Accurate periocular recognition under less constrained environment using semantics-assisted convolutional neural network,"['Z Zhao', 'A Kumar']",2016,136,Toronto Face Database,neural network,"v4 is the first publicly available long-range iris and face database acquired under NIR  illumination, which is released by the Center for Biometrics and Security Research (CBSR) from",No DOI,IEEE Transactions on Information Forensics …,http://ieeexplore.ieee.org/document/7775081,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
19,19,Acted facial expressions in the wild database,"['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2011,140,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","classification, facial expression recognition, machine learning, neural network","The Cohn-Kanade database [17] is widely used facial expression database. This database  formed a standard for facial expression recognition, it contains both static and dynamic data",No DOI,"… , Australia, Technical Report …",https://paperswithcode.com/dataset/acted-facial-expressions-in-the-wild-afew,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
20,20,Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild,"['AP Fard', 'MH Mahoor']",2022,113,"Affective Faces Database, Expression in-the-Wild","FER, classification, classifier, facial expression recognition","FER and then discuss the use of DML for general image classification tasks. Afterward, we  review the work used DML in FER expressional features from an input face image, as well as",No DOI,IEEE Access,https://ieeexplore.ieee.org/document/9727163,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
21,21,Adaptive deep metric learning for identity-aware facial expression recognition,"['X Liu', 'BVK Vijaya Kumar', 'J You']",2017,236,"CMU Multi-PIE, MMI Facial Expression",FER,"features which are not useful for FER. As shown in Fig. 1,  For FER, we desire that two  face images with the same  approaches in posed facial expression dataset (eg, CK+, MMI),",No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/document/8014813,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
22,22,Adaptive feature mapping for customizing deep learning based facial expression recognition model,"['BF Wu', 'CH Lin']",2018,106,"Extended Cohn-Kanade, Radboud Faces Database","deep learning, machine learning","progress in this artificial intelligence era. Many deep learning approaches have been applied  in  Compared to the competing deep learning architectures with the same training data, our",No DOI,IEEE access,https://ieeexplore.ieee.org/document/8291717,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
23,23,Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems,"['A Howard', 'C Zhang', 'E Horvitz']",2017,115,"Affective Faces Database, Radboud Faces Database",machine learning,"on 50% of the feature vectors classified as Fear or Surprise by the deep learning algorithm  and extracted from the Radboud Faces Database, the Dartmouth Database of Children’s",No DOI,2017 IEEE Workshop on …,https://ieeexplore.ieee.org/abstract/document/8025197,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
24,24,Aff-wild2: Extending the aff-wild database for affect recognition,"['D Kollias', 'S Zafeiriou']",2018,167,Affective Faces Database,"CNN, deep learning, neural network","the paper is that by training deep neural networks on the Aff- -art emotion recognition  performance on the other database,  of Aff-Wild2 database starts with the detection of faces [1] in",No DOI,arXiv preprint arXiv:1811.07770,https://arxiv.org/abs/1811.07770,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
25,25,Aff-wild: valence and arousal'In-the-Wild'challenge,"['S Zafeiriou', 'D Kollias', 'MA Nicolaou']",2017,373,Expression in-the-Wild,"deep learning, machine learning, neural network",”inthe-wild”. For a recent survey on facial behaviour analysis in-the-wild with an emphasis on  deep learning  come from movies and the annotation is limited to the universal expressions.,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014982/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
26,26,"Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework","['D Kollias', 'S Zafeiriou']",2021,206,Expression in-the-Wild,"classification, deep learning","databases and ii) design and training of novel deep neural architectures  in-the-wild databases,  ie, Aff-Wild and Aff-Wild2 and presents the design of two classes of deep neural networks",No DOI,arXiv preprint arXiv:2103.15792,https://arxiv.org/abs/2103.15792,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
27,27,Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected,"['D McDuff', 'R Kaliouby', 'T Senechal', 'M Amr']",2013,314,"Acted Facial Expressions In The Wild, Affective Faces Database",classifier,"Computer classification of facial expressions requires large amounts of data and this data  needs to  If consent is granted, the commercial is played in the browser whilst simultaneously",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/6595975,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
28,28,Affective image classification using features inspired by psychology and art theory,"['J Machajdik', 'A Hanbury']",2010,1010,Affective Faces Database,classification,"as features, and the expected affective state in which the user is  that are specific to the task  of affective image classification.  face detection algorithm by Viola and Jones [28] to find faces",No DOI,Proceedings of the 18th ACM international …,https://dl.acm.org/doi/10.1145/1873951.1873965,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
29,29,"Affectnet: A database for facial expression, valence, and arousal computing in the wild","['A Mollahosseini', 'B Hasani']",2017,1965,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",our deep neural network baselines can perform better than conventional machine learning   [18] released Acted Facial Expressions in the Wild (AFEW) from 54 movies by a recom,No DOI,IEEE Transactions on …,https://arxiv.org/abs/1708.03985,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
30,32,Appearance-based gender classification with Gaussian processes,"['HC Kim', 'D Kim', 'Z Ghahramani', 'SY Bang']",2006,102,Toronto Face Database,classification,"In Section 3, we introduce Gaussian process classification. In  classification. In Section 5,  we show experimental results on the PF01 database and compared with other classification",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865505002801,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Appearance-based gender classification with Gaussian
processes
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Appearance-based gender classification
6. 3\. Gaussian process classifiers
7. 4\. The EM?EP algorithm for GPCs
8. 5\. Experimental results
9. 6\. Conclusion
10. Acknowledgments
11. References
Show full outline
## Cited by (56)
## Figures (7)
1. 2. 3. 4. 5. 6.
Show 1 more figure
## Pattern Recognition Letters
Volume 27, Issue 6, 15 April 2006, Pages 618-626
# Appearance-based gender classification with Gaussian processes
Author links open overlay panelHyun-Chul Kim a, Daijin Kim b, Zoubin
Ghahramani c, Sung Yang Bang b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2005.09.027Get rights and content
## Abstract
This paper concerns the gender classification task of discriminating between
images of faces of men and women from face images. In appearance-based
approaches, the initial images are preprocessed (e.g. normalized) and input
into classifiers. Recently, support vector machines (SVMs) which are popular
kernel classifiers have been applied to gender classification and have shown
excellent performance. SVMs have difficulty in determining the hyperparameters
in kernels (using cross-validation). We propose to use Gaussian process
classifiers (GPCs) which are Bayesian kernel classifiers. The main advantage
of GPCs over SVMs is that they determine the hyperparameters of the kernel
based on Bayesian model selection criterion. The experimental results showthat our methods outperformed SVMs with cross-validation in most of data sets.
Moreover, the kernel hyperparameters found by GPCs using Bayesian methods can
be used to improve SVM performance.
* Previous article in issue
* Next article in issue
## Keywords
Gender classification
Appearance-based gender classification
Kernel machines
Gaussian process classifiers
Support vector machines
## 1\. Introduction
The face is a characteristic feature of human beings which contains identity
and emotion. It is possible to identify a person and her/his characteristics
such as emotion (or expression) and gender from her/his face. Recognizing
human gender is important since lots of social interactions and services
depend on the gender. People respond differently according to gender. Human
computer interaction system can be more user-friendly and more human-like when
it considers the user?s gender.
There are two main approaches for gender classification. The first approach is
the appearance-based approach which uses a whole face image. Cottrell and
Metcalfe (1991) reduced the dimension of whole face images by autoencoder
network and classified gender based on the reduced input features. Golomb et
al. (1991) used a two-layer neural network (called SexNet) without
dimensionality reduction. Tamura et al. (1996) used a neural network and
showed that even very low resolution image such as 8 × 8 can be used for
gender classification. Gutta et al. (2000) used the mixture of experts with
ensembles of radial basis functions (RBF) networks and a decision tree as a
gating network. Moghaddam and Yang (2002) showed that support vector machines
(SVMs) worked better than other classifiers such as ensemble of RBF networks,
classical RBF networks, Fisher linear discriminant, nearest neighbor etc. Jain
and Huang (2004) extracted wholistic features by independent component
analysis (ICA) and classified it with linear discriminant analysis (LDA).
Costen et al. (2004) used the exploratory basis pursuit classification which
is a sparse kernel classifier.
The second approach is the geometrical feature based approach. Burton et al.
(1993) extracted point-to-point distances from 73 points on face images and
used discriminant analysis as a classifier. Brunelli and Poggio (1992)
extracted 16 geometric features such as eyebrow thickness and pupil-to-eyebrow
distance and used HyperBF networks as a classifier.
As mentioned above, the appearance-based approach with SVM showed excellent
performance (Moghaddam and Yang, 2002). In their experiments the Gaussian
kernel worked better than linear or polynomial kernels. They did not mention
how to set the hyperparameters1 for Gaussian kernel which have an influence on
performance, but just showed the test results with several different
hyperparameters. Learning the hyperparameters should be included in the
training process. A standard way to determine the hyperparameters is by cross-
validation. Alternatively we could use kernel classifiers such as Gaussian
process classifiers which automatically incorporate method to determine the
hyperparameters. In this paper we propose to use Gaussian process classifiers
(GPCs) for appearance-based gender classification.
GPCs are a Bayesian kernel classifier derived from Gaussian process priors
over functions which were developed originally for regression (O?Hagan, 1978,
Neal, 1997, Williams and Barber, 1998, Gibbs and MacKay, 2000). Inclassification, the target values are discrete class labels. To use Gaussian
processes for binary classification, the Gaussian process regression model can
be modified so that the sign of the continuous latent function it outputs
determines the class label. Observing the class label at some data point
constrains the function value to be positive or negative at that point, but
leaves it otherwise unknown. To compute predictive quantities of interest we
therefore need to integrate over the possible unknown values of this function
at the data points.
Exact evaluation of this integral is computationally intractable. However,
several successful methods have been proposed for approximately integrating
over the latent function values, such as the Laplace approximation (Williams
and Barber, 1998), Markov Chain Monte Carlo (Neal, 1997), and variational
approximations (Gibbs and MacKay, 2000). Opper and Winther (2000) used the
TAP2 approach originally proposed in statistical physics of disordered systems
to integrate over the latent values. The TAP approach for this model is
equivalent to the more general expectation propagation (EP) algorithm for
approximate inference (Minka, 2001). The expectation maximization?expectation
propagation (EM?EP) algorithm has been proposed to learn the hyperparameters
based on EP (Kim and Ghahramani, 2003). GPCs with the hyperparameters obtained
by the EM?EP algorithm have shown better performance than SVMs which had the
hyperparameters set by cross-validation, on most of data sets tested. In many
cases the hyperparameters determined by the EM?EP algorithm were more suitable
for SVMs than the ones determined by cross-validation technique. In this paper
we use the EM?EP algorithm to learn Gaussian process classifiers for gender
classification. We expect that GPCs with the EM?EP algorithm work better than
SVMs with the cross-validation and provide better hyperparameters for the
kernels of SVMs.
The paper is organized as follows. Section 2 introduces appearance-based
gender classification. In Section 3, we introduce Gaussian process
classification. In Section 4, we describe the EP method and the EM?EP
algorithm for Gaussian process classification. In Section 5, we show
experimental results on the PF01 database and compared with other
classification methods including SVMs. In Section 6, we draw conclusions and
remark on future work.
## 2\. Appearance-based gender classification
The appearance-based approach to gender classification discriminates between
male and female classes from face images without first explicitly extracting
any geometrical features. A typical way to do this is to train a classifier
with training images and to classify new images by the trained classifier.
Face images should be well-aligned so that facial features are in the same
positions. Since gender classification is a two-class classification problem,
any kind of binary classifier can be deployed.
Fig. 1 shows the process of appearance-based gender classification. Assume
that a classifier has been already trained with some images in advance. The
whole process of gender classification can be explained by the following.
First, images are captured. Then, the captured images are preprocessed by face
detection and facial feature extraction algorithms and cropped by an
appropriate cropping technique. The preprocessed face images can include a
whole outline of faces with hair or can include only inner face parts with
only facial features. Then, the preprocessed image (pixel-level features) is
applied to the classifier and the classifier determines the gender of the
input image.
1. Download: Download full-size image
Fig. 1. The process of appearance-based gender classification.The appearance-based approach has two main advantages. First, it preserves
appearance of face images which can be considered to be naive features. It is
difficult to determine what kind of geometrical features we should use and to
tell the meaning of those features. In contrast to this, appearance-based
approach is more natural since it uses face images themselves. The benefit in
being natural is that it could make it easier to do what the natural being
does. Second, it does not need to extract facial features or points very
accurately. To get good geometrical features, we need to know quite accurate
facial feature or point locations which requires accurate facial feature
extraction. In contrast to this, we need to know relatively small number of
facial features for alignment in the appearance-based approach. The
disadvantages of the appearance-based approach is that it has more features
than the geometrical feature based approach and that it does not provide a
good explanation why a facial image is classified as a male or female.
We follow the above process for appearance-based gender classification and use
Gaussian process classifiers.
## 3\. Gaussian process classifiers
Let us assume that we have a data set _D_ of data points **_x_** _i_ with
binary class labels _y_ _i_ ? {?1,1}: _D_ = {(**_x_** _i_ , _y_ _i_)? _i_ =
1, 2, ? , _n_}, _X_ = {**_x_** _i_ ? _i_ = 1, 2, ? , _n_}, _Y_ = {_y_ _i_ ?
_i_ = 1, 2, ? , _n_}. Given this data set, we wish to find the correct class
label for a new data point x?. We do this by computing the class probability
p(y?|x?,D).
We assume that the class label is obtained by transforming some real valued
latent variable f?, which is the value of some latent function _f_ (·)
evaluated at x?. We put a Gaussian process prior on this function, meaning
that any number of points evaluated from the function have a multivariate
Gaussian density (see Williams and Rasmussen (1995) for a review of GPs).
Assume that this GP prior is parameterized by _?_ which we will call the
hyperparameters. We can write the probability of interest given _?_
as(1)p(y?|x?,D,?)=?p(y?|f?,?)p(f?|D,x?,?),df?.This is the probability of the
class label y? at a new data point x? given data _D_ and hyperparameters _?_.
The second part of Eq. (1) is obtained by further integration over **_f_** =
[_f_ 1, _f_ 2, ? , _f_ _n_], the values of the latent function at the data
points.(2)p(f?|D,x?,?)=?p(f,f?|D,x?,?)df=?p(f?|x?,f,?)p(f|D,?)df,where
p(f?|x?,f,?)=p(f?,f|x?,X,?)/p(f|X,?)
and(3)p(f|D,?)?p(Y|f,X,?)p(f|X,?)=?i=1np(yi|fi,?)p(f|X,?).The first term in
Eq. (3) is the likelihood: the probability for each observed class given the
latent function value, while the second term is the GP prior over functions
evaluated at the data. Writing the dependence of **_f_** on **_x_**
implicitly, the GP prior over functions can be
written(4)p(f|X,?)=1(2?)N/2|C?|1/2exp-12(f-?)?C?-1(f-?),where the mean **_?_**
is usually assumed to be the zero vector **_0_** and each term of a covariance
matrix _C_ _ij_ is a function of **_x_** _i_ and **_x_** _j_ , i.e.
_c_(**_x_** _i_ , ** _x_** _j_).
One form for the likelihood term _p_(_y_ _i_ ? _f_ _i_ ,_?_), which relates
_f_(**_x_** _i_) monotonically to probability of _y_ _i_ = +1,
is(5)p(yi|fi,?)=12??-?yif(xi)exp-z22dz=erf(yif(xi)).Other possible forms for
the likelihood are a sigmoid function 1/(1 + exp(? _y_ _i_ _f_(**_x_** _i_))),
a step function _H_(_y_ _i_ _f_(**_x_** _i_)), and a step function with a
labelling error _?_ \+ (1 ? 2 _?_)_H_(_y_ _i_ _f_(**_x_** _i_)).
Since _p_(**_f_** ? _D_ ,_?_) in Eq. (3) is intractable due to the non-
linearity in the likelihood terms, we use an approximate method. Laplace
approximation, variational methods and Markov Chain Monte Carlo method wereused in (Williams and Barber, 1998, Gibbs and MacKay, 2000, Neal, 1997),
respectively. Expectation propagation, which is described in the next section,
was used in (Opper and Winther, 2000, Minka, 2001).
## 4\. The EM?EP algorithm for GPCs
### 4.1. Expectation propagation for GPCs
The expectation-propagation (EP) algorithm is an approximate Bayesian
inference method (Minka, 2001). We review EP in its general form before
describing its application to GPCs.
Consider a Bayesian inference problem where the posterior over some parameter
_?_ is proportional to the prior times likelihood terms for an i.i.d. data
set(6)p(?|y1,?,yn)?p(?)?i=1np(yi|?).We approximate this
by(7)q(?)?t?0(?)?i=1nt?i(?),where each term (and therefore _q_) is assumed to
be in the exponential family. EP successively solves the following
optimization problem for each
_i_(8)t?inew(?)=argmint?i(?)KLq(?)t?iold(?)p(yi|?)q(?)t?iold(?)t?i(?),where KL
is the Kullback?Leibler divergence
and(9)KL(p(x)?q(x))=?p(x)logp(x)q(x)dx.Since _q_ is in the exponential family,
this minimization is solved by matching moments of the approximated
distribution. EP iterates over _i_ until convergence. The algorithm is not
guaranteed to converge although it did in practice in all our examples and has
worked well for many other authors. Assumed density filtering (ADF) is a
special online form of EP where only one pass through the data is performed
(_i_ = 1, ? , _n_).
We describe EP for GPC referring to (Minka, 2001, Opper and Winther, 2000).
The latent function **_f_** plays the role of the parameter _?_ above. The
form of the likelihood we use in the GPC is(10)p(yi|fi)=?+(1-2?)H(yifi),where
_H_(_x_) = 1 if _x_ > 0, and otherwise 0. The hyperparameter, _?_ in Eq. (10)
models labeling error outliers. The EP algorithm approximates the posterior
_p_(**_f_** ? _D_) = _p_(**_f_**)_p_(_D_ ?** _f_**)/_p_(_D_) as a Gaussian
having the form q(f)?N(mf,Vf), where the GP prior p(f)?N(0,C) has covariance
matrix **_C_** with elements _C_ _ij_ defined by the covariance
function(11)Cij=c(xi,xj)=v0exp-12?m=1dlmdm(xim,xjm)+v1+v2?(i,j),where xim is
the _m_ th element of **_x_** _i_ , and dm(xim,xjm)=(xim-xjm)2 if _x_ _m_ is
continuous; 1-?(xim,xjm) if _x_ is discrete, where ?(xim,xjm) is 1 if xim=xjm
and 0 if xim?xjm. The hyperparameter _v_ 0 specifies the overall vertical
scale of variation of the latent values, _v_ 1 the overall bias of the latent
values from zero mean, _v_ 2 the latent noise variance, and _l_ _m_ the
(inverse) lengthscale for feature dimension _m_. The erf likelihood term in
Eq. (5) is equivalent to using the threshold function in Eq. (10) with _?_ = 0
and non-zero latent noise _v_ 2.
EP tries to approximate p(f|D)=p(f)/p(D)?i=1np(yi|f), where p(f)?N(0,**_C_**).
_p_(_y_ _i_ ?** _f_**) = _t_ _i_(**_f_**) is approximated by
t?i(f)=siexp(-12vi(fi-mi)2). From this initial setting, we can derive EP for
GPC by applying the general idea described above. The resulting EP procedure
is virtually identical to the one derived in (Minka, 2001). We define the
following notation3: ???=diag(v1,?,vn);hi=E[fi];hi?i=E[fi?i], where ?hi?i and
?fi?i are quantities obtained from a whole set except for **_x_** _i_. The EP
algorithm is as follows which we repeat for completeness?please refer to Minka
(2001) for the details of the derivation. After the initialization _v_ _i_ =
?, _m_ _i_ = 0, _s_ _i_ = 1, _h_ _i_ = 0, _?_ _i_ = _C_ _ii_ , the following
process is performed until all (_m_ _i_ , _v_ _i_ , _s_ _i_) converge.
Loop _i_ = 1,2, ? , _n_ :
* (1)
Remove the approximate density t?i (for _i_ th data point) from the posteriorto get an ?old? posterior: ?hi?i=hi+?ivi-1(hi-mi).
* (2)
Recompute part of the new posterior: ?z=yihi?i?i;Zi=?+(1-2?)erf(z)
??i=1?i(1-2?)N(z;0,1)?+(1-2?)erf(z);hi=hi?i+?i?i, where erf(_z_) is a
cumulative normal density function.
* (3)
Get a new t?i:vi=?i(1?ihi-1);mi=hi+vi?i;si=Zi1+vi-1?iexp(?i?i2hi).
* (4)
Now that _v_ _i_ is updated, finish recomputing the new posterior: **_A_** =
(**_C_** ?1 \+ ** _?_** ?1)?1; For all _i_ , hi=?jAijmjvj;?i=(1Aii-1vi)-1.
Our approximated posterior over the latent values is:(12)q(f)?N(C??,A),where
C?ij=yjc(xi,xj) (or C?=Cdiag(y)). Classification of a new data point x? can be
done according to argmaxy?p(y?|x?)=sgn(E[f?])=sgn(?i=1n?iyic(xi,x?)).
The approximate evidence can be obtained
as(13)p(Y|X,?)?|?|1/2|C+?|1/2exp(B/2)?i=1nsi,where B=?ijAijmimjvivj-?imi2vi.
The approximate evidence in Eq. (13) can be used to evaluate the feasibility
of kernels or their hyperparameters to the data. But, it is tricky to get a
hyperparameter updating rule from Eq. (13). In the following section, we
derive the algorithm to find the hyperparameters automatically based not in
Eq. (13) but a variational lower bound of the evidence.
### 4.2. The EM?EP algorithm
EP for GPCs propose a method to estimate latent values but not
hyperparameters. We put _?_ = _?_ cov ? {_?_}, and _?_ cov = {_v_ 0, _v_ 1,
_v_ 2} ? {_l_ _p_ ? _p_ = 1, 2, ? , _d_} for the hyperparameters. Here we
present the EM?EP algorithm based on EP to estimate both latent values and
hyerparameters (Kim and Ghahramani, 2003). We tackle the problem of learning
the classifier hyperparameters as one of optimizing hyperparameters for
Gaussian process regression with hidden target values. This idea makes it
possible to apply an approximate EM (expectation maximization) algorithm. In
the E-step, we infer the approximate (Gaussian) density for latent function
values _q_(**_f_**) using EP. In the M-step, using _q_(**_f_**) obtained in
the E-step, we maximize the variational lower bound of _p_(_Y_ ? _X_ , _?_).
The E-step and M-step are alternated until convergence.
_E-step:_ EP iterations are performed given the hyperparameters. _p_(**_f_** ?
_D_) is approximated as a Gaussian density _q_(**_f_**) given by Eq. (12).
_M-step:_ Given _q_(**_f_**) obtained from the E-step, find the
hyperparameters which maximize the variational lower bound of _p_(_Y_ ? _X_
,_?_) = ? _p_(_Y_ ?** _f_** , _X_ , _?_)_p_(**_f_** ? _X_ , _?_ cov)d**
_f_**. Since the above integral is intractable, we take a variational lower
bound _F_ as
follows:(14)logp(Y|X,?)=log?p(Y|f,X,?)p(f|X,?cov)df??q(f)logp(Y|f,X,?)p(f|X,?cov)q(f)df=F.
Using the E-step result Eq. (12) and the definition of C?, we obtain the
following gradient update rule with respect to the covariance
hyperparameters(15)?F??cov=12??diag(y)?C??covdiag(y)?-12trC-1?C??cov+12trC-1?C??covC-1A.
(See Kim and Ghahramani, 2003 for the derivation of the M-step.)
We found that in practice EM?EP always converged and the local maxima were
good solutions. EM?EP has a complexity of _O_(_n_ 3) due to the matrix
inversion in EP.
## 5\. Experimental results
We performed experiments on appearance-based gender classification with
Gaussian processes using the database PF01 (Postech Faces 2001) (Kim et al.,
2001) and Aleix database (Martinez and Benavente, 1998). The database PF01 has
color face images of 103 Asian people, 53 men and 50 women, where for each
person there are 17 images under various conditions (one normal, fourillumination-varying ones, eight pose-varying ones, four expression-varying
ones). The Aleix database has over 4000 color images of 126 people?s faces (70
men and 56 women), where images are frontal view faces with different facial
expressions, illumination conditions, and occlusions (sun glasses and scarf).
We performed gender classification on four partial data sets which are only
normal face images (103 images, Faceset PF-I) in PF01, normal and expression-
varying face images (5 × 103 = 515 images, Faceset PF-II) in PF01, only normal
face images (126 images, Faceset AL-I) in PF01, and normal and expression-
varying face images (4 × 126 = 504 images, Faceset AL-II) in PF01. Fig. 2,
Fig. 3 show the normal and expression-varying images of three men and three
women in the database PF01 and the Aleix database, respectively. For each
partial data set, we preprocessed face images in two ways. The first from of
preprocessing downsampled and cropped face images including hairs and contour
of faces and the second form of preprocessing further cropped the face images
to exclude hair and background. Fig. 4 shows the example of a normalized image
(256 × 256) and a cropped face image (28 × 23, cropped type A) and a more
cropped face image (20 × 16, cropped type B). All images are aligned so that
eyes are placed in the same positions, which can be done with eye detection
algorithms in practice. If images are not aligned well, the appearance-based
method would not work well.
1. Download: Download full-size image
Fig. 2. Some images in the database PF01.
1. Download: Download full-size image
Fig. 3. Some images in the Aleix database.
1. Download: Download full-size image
Fig. 4. Preprocessed images: (a) Normalized image, (b) downsampled and cropped
image (28 × 23), (c) downsampled and more cropped image (20 × 16).
We have eight different data sets: data set P-I (Faceset PF-I, cropped type
A), data set P-II (Faceset PF-I, cropped type B), data set P-III (Faceset PF-
II, cropped type A), and data set P-IV (Faceset PF-II, cropped type B), data
set A-I (Faceset AL-I, cropped type A), data set A-II (Faceset AL-I, cropped
type B), data set A-III (Faceset AL-II, cropped type A), and data set A-IV
(Faceset AL-II, cropped type B). Data set P-I, P-II, P-III and P-IV are the
data sets which include normal faces, more cropped normal faces, expression-
varying faces, and more cropped expression-varying faces from the database
PF01, respectively. Data set A-I, A-II, A-III and A-IV are the data sets which
include normal faces, more cropped normal faces, expression-varying faces, and
more cropped expression-varying faces from the Aleix database, respectively.
On these data sets, we applied many different classifiers including one
nearest neighbor (1-NN), linear discriminant analysis (LDA), SVM with cross-
validation (SVM-CV), SVM with EM?EP hyperparameters (SVM-EP), and GPC with the
EM?EP algorithm (GPC-EP). Fig. 5, Fig. 6 show the classification error rates
of these methods over four different data sets in the database PF01 and Aleix
database. Each data set was divided into 10 folds. Each fold was subsequently
used as a test set, while the other nine folds were used as a training set.
Before GPC or SVM are applied, all feature values are normalized based on the
training set so that their means are zero and their variances are one. The
points ?×? in Fig. 5, Fig. 6 are means of 10 trials and error bars are from
standard deviations of the mean estimators. In Fig. 7, we show misclassified
images. In Fig. 7(a), we can know that long hair is not always a key feature
of women. The classification rates of data sets of cropped type A are not
always better than ones of cropped type B.
1. Download: Download full-size image
Fig. 5. Classification error rates of various methods for four kinds of genderclassification data sets (PF?01 DB): (a) data set P-I, (b) data set P-II, (c)
data set P-III, (d) data set P-IV.
1. Download: Download full-size image
Fig. 6. Classification error rates of various methods for four kinds of gender
classification data sets (Aleix DB): (a) data set A-I, (b) data set A-II, (c)
data set A-III, (d) data set A-IV.
1. Download: Download full-size image
Fig. 7. Misclassified facial images: (a) data set P-I, (b) data set P-II.
GPC-EP used a single lengthscale hyperparameter (i.e. _l_ _m_ = _l_) for all
feature dimensions4. In all GPC models the hyperparameter _?_ was not updated
but fixed to zero. In SVM-EP the kernel (i.e. covariance function) had the
same hyperparameters as the corresponding GPC-EP that were trained using EM?EP
except for the latent noise variance _v_ 2 which was omitted because it caused
degradation in SVM performance5. Instead, the penalty parameter _C_ allowing
training errors (i.e. penalizing the SVM slack variables) was selected by
5-fold cross-validation.6 In SVM-CV we applied SVMs with a Gaussian kernel
with a single lengthscale hyperparameter (without _v_ 0, _v_ 1 and _v_ 2)
selected by five-fold cross-validation.7 We also had to determine the penalty
parameter _C_ , so we performed a 2-level grid search over a 2-dimensional
parameter space (_C_ , _l_)8.
In the data set P-I, P-II, P-IV, and A-IV, GPC-EP is the best, in the data set
P-III, A-I, and A-II, SVM-EP is the best, and in the data set A-III SVM-CV is
the best. In all the data sets except for one, GPC-EP or SVM-EP is the best.
Also, in all the data sets except for one, SVM-EP is better than SVM-CV.
Therefore, for the data sets tested the hyperparameters found by the EM?EP
algorithm seem to be also more suitable hyperparameters for SVMs than the ones
obtained by cross-validation. This shows that the EM?EP algorithm finds
suitable hyperparameters successfully and those hyperparameters are also
suitable for SVMs. This result is consistent with the result on the benchmark
data sets in (Kim and Ghahramani, 2003).
## 6\. Conclusion
We have proposed the appearance-based gender classification method with
Gaussian processes. GPCs incorporate the Bayesian model selection framework to
determine the kernel hyperparameters, which is an important advantage over
SVMs. In the experiments the hyperparameters obtained by GPC with the EM?EP
algorithm were even more suitable for SVMs than the ones obtained by cross-
validation. In most of the data sets, EM?EP algorithms worked better than SVMs
with cross-validation and provided kernel hyperparameters to make SVMs work
better.
We used Gaussian kernels in this paper. Gaussian kernels do not seem to be
ideal for image data since they do not capture correlations between pixels. If
we invent more proper kernels for face images, we might improve the
performance. It would also be interesting to perform experiments on a larger
face data set.
## Acknowledgments
Hyun-Chul Kim, Daijin Kim and Sung-Yang Bang would like to thank the Ministry
of Education of Korea for its financial support toward the Division of
Mechanical and Industrial Engineering, and the Division of Electrical and
Computer Engineering at POSTECH through BK21 program.
Recommended articles"
31,33,Attention mechanism-based CNN for facial expression recognition,"['J Li', 'K Jin', 'D Zhou', 'N Kubota', 'Z Ju']",2020,284,Japanese Female Facial Expression,CNN,"The JAFFE dataset was collected from 10 Japanese females in a laboratory condition and  includes 213 images of posed expressions. For each subject, there are three or four images",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220309838,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Attention mechanism-based CNN for facial expression
recognition
Search 
## Outline
1. Abstract
2. **Beta****Powered by GenAI** Questions answered in this article
3. Keywords
4. 1\. Introduction
5. 2\. Related works
6. 3\. The proposed method
7. 4\. Experimental results
8. 5\. Conclusions and future work
9. CRediT authorship contribution statement
10. Declaration of Competing Interest
11. Acknowledgements
12. References
13. Vitae
Show full outline
## Cited by (245)
## Figures (8)
1. 2. 3. 4. 5. 6.
Show 2 more figures
## Tables (8)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Neurocomputing
Volume 411, 21 October 2020, Pages 340-350
# Attention mechanism-based CNN for facial expression recognition
Author links open overlay panelJing Li a, Kan Jin a, Dalin Zhou b, Naoyuki
Kubota c, Zhaojie Ju b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2020.06.014Get rights and content## Abstract
Facial expression recognition is a hot research topic and can be applied in
many computer vision fields, such as human?computer interaction, affective
computing and so on. In this paper, we propose a novel end-to-end network with
attention mechanism for automatic facial expression recognition. The new
network architecture consists of four parts, i.e., the feature extraction
module, the attention module, the reconstruction module and the classification
module. The LBP features extract image texture information and then catch the
small movements of the faces, which can improve the network performance.
Attention mechanism can make the neural network pay more attention to useful
features. We combine LBP features and attention mechanism to enhance the
attention model to obtain better results. In addition, we collected and
labelled a new facial expression dataset of seven expressions from 35 subjects
aged from 20 to 25. For each subject, we captured both RGB images and depth
images with a Microsoft Kinect sensor. For each image type, there are 245
image sequences, each of which contains 110 images, resulting in 26,950 images
in total. We apply the newly proposed method to our own dataset and four
representative expression datasets, i.e., JAFFE, CK+, FER2013 and Oulu-CASIA.
The experimental results demonstrate the feasibility and effectiveness of the
proposed method.
## Questions answered in this article
**Beta****Powered by GenAI**
_This is generative AI content and the quality may vary. Learn more._
1. What happens to the recognition rates when LBP or attention is not used?
2. What deep network architectures were most works inspired by for facial expression recognition?
3. How are LBP features and convolution features combined in the attention module?
4. Why is data augmentation important for facial expression recognition?
5. What are the advantages of using LBP features in facial expression recognition?
* Previous article in issue
* Next article in issue
## Keywords
Facial Expression Recognition
Convolutional Neural Network
Attention Mechanism
Local Binary Patten
Image Classification
## 1\. Introduction
Facial expression is one of the most direct signals to express inner feelings
in people's daily communication. The physical or mental state of a person at
one time can be obtained by analyzing facial expressions. Therefore, facial
expression recognition is of great significance in autopilot, human?computer
interaction, medical treatment and other fields related to facial expression,
and has gradually become a more and more important research direction. In
machine learning, a variety of facial expression recognition algorithms have
been proposed. Due to the complexity, diversity, occlusion, lighting and other
challenges in facial expression recognition, the recognition accuracy in
practical applications is still unsatisfactory.
In this paper, our goal is to design a recognition model that can
automatically and accurately recognize different expressions in various types
of images. Generally, the process of facial expression recognition consists of
the following steps: i) pre-processing of the facial expression data; ii)
feature extraction of facial expressions; and iii) classification of facial
expressions. The process is depicted in Fig. 1. We usually consider two kinds
of features, namely, facial features and face model features. The facialfeatures are specific points on the face, like eyes, mouth, and eyebrows; the
face model features are the features used to model the face. Therefore, there
are several ways for facial representation, like using the whole face to get
the holistic representation, using specific points for local representation,
and combining different points to get a hybrid approach. The final step is to
define some set of categories to which the expression belongs.
1. Download: Download high-res image (108KB)
2. Download: Download full-size image
Fig. 1. The main steps of facial expression recognition.
When dealing with expression recognition as a classification problem,
traditional methods often use hand-crafted features such as Local Binary
Patterns (LBP) and traditional machine learning algorithms such as Support
Vector Machine (SVM) to classify. These methods may work well on datasets
collected under laboratory conditions, but with the introduction of more
challenging expression datasets in uncontrollable environments (e.g.,
FER2013), they cannot effectively achieve this task. Fortunately, deep
learning has made a breakthrough in convenience and effectiveness since it has
been used to deal with the image classification problem.
The attention mechanism has been widely used in various computer vision tasks
such as saliency detection [15], crowd counting [16] and facial expression
recognition [38]. The operation can select the most useful features for
classification by learning an intermediate attention map and then applying
element-wise product on attention maps and source feature maps to weight the
importance of different features. For the task of facial expression
recognition, the features that are useful for recognition are mainly in some
key parts such as eyes, nose and mouth. The attention mechanism increases the
weights of these key features and helps improve the expression recognition
results.
In this paper, we design a novel Convolutional Neural Network with an
attention model for recognizing facial expressions. In [43], it showed that
using LBP features is better than using HOG and Gabor features because LBP can
achieve rotation invariance and grey-scale invariance and thus is suitable for
extracting texture features at different scales and can solve the imbalance of
displacement, rotation angles and illumination conditions in facial images. In
addition, LBP features can reflect fine facial changes in skin textures like
wrinkles and furrows, which shows the changes of expressions. In [38], an end-
to-end network with an attention model was presented for facial expression
recognition. The attention module makes the network focus more on useful
features which are vital for expression recognition by increasing the weights
of these features. This makes the network recognize expressions more
efficiently. Inspired by [38], [43], we combine LBP features with an attention
model for facial expression recognition. Embedding the attention model into
the network allows the network to pay different attention and weight to
different parts of the input data. This can make the neural network pay more
attention to useful features, which is vital to expression recognition.
Furthermore, we combine LBP features with convolution features to improve our
recognition results. The proposed method has been tested on five facial
expression datasets, which are CK+ [11], JAFFE [13], FER2013 [39], Oulu-CASIA
[25], and our self-collected Nanchang University Facial Expression (NCUFE).
To verify the effectiveness of our algorithm, we collected a new facial
expression dataset called NCUFE. The dataset consists of seven expressions
(i.e., anger, disgust, fear, happiness, sadness, surprise and neutral). We
collected these facial expression images from 35 graduate students (6 females
and 29 males) by a Microsoft Kinect sensor for acquiring both RGB images anddepth images. The sample images are shown in Fig. 2. For each student, the
size of these two types of images is 1280*1024 and 512*424, respectively. For
each image type, there are 245 image sequences, each of which contains 110
images, resulting in 26,950 images in total. During the process of image
capture, the students sat on a chair in front of the Kinect and faced the
camera. The distance between the face and the Kinect was about 100 cm. We
asked each student to look the expression examples printed on some pieces of
paper and then make seven expressions.
1. Download: Download high-res image (59KB)
2. Download: Download full-size image
Fig. 2. Sample images in the NCUFE dataset. The left one is an RGB image, and
the right one is its corresponding depth image.
The main contributions of this paper are as follows:
* 1)
We introduce a novel facial expression recognition method with attention
mechanism. Not only raw images, but also LBP features are added to the
attention layers of the network. LBP features contain texture information and
can reflect fine facial changes in skin textures, which can help distinguish
expressions with subtle difference.
* 2)
We collected and labelled a new dataset named Nanchang University Facial
Expression (NCUFE) for facial expression recognition. The dataset includes 490
image sequence collected from 35 subjects labeled with seven facial
expressions (i.e., anger, disgust, fear, happiness, sadness, surprise and
neutral). For each subject, we captured both RGB images and depth images.
* 3)
We implement substantial experiments on five different datasets, as shown in
Fig. 3. There are not only datasets collected under laboratory conditions,
such as CK+, JAFFE, Oulu-CASIA and NCUFE, but also those collected in real
world like FER2013. We also compare the model performance with some state-of-
the-art expression recognition algorithms, and the results show that our model
is superior.
1. Download: Download high-res image (331KB)
2. Download: Download full-size image
Fig. 3. Examples of the datasets used in our experiments, from top to bottom
is from CK+ [11], JAFFE [13], Oulu-CASIA [25], FER2013 [39], and our dataset
NCUFE.
The remainder of this paper is as follows. In Section 2, we introduce the
related works in expression recognition and the existing algorithms. In
Section 3, we describe the proposed method in detail. We describe our
experimental process and results on different datasets and compare them with
the results of the state-of-the-art algorithms in Section 4. We give a
conclusion of the whole paper in Section 5.
## 2\. Related works
Traditional facial feature extraction algorithms can be separated into two
categories: 1) geometric-based methods, such as Active Appearance Models (AAM)
[17]; and 2) appearance-based methods, such as LBP [9] and Gabor Wavelet
Representation [21]. After feature description, the features are fed into a
classifier, such as SVM [22] and K-nearest Neighbors (KNN) [24], for
recognizing different facial expressions. Therefore, the performance of the
classifier depends to a large extent on the quality of the extracted features.
In [43], various feature extraction techniques combined with different
classification algorithms were presented to find the best combination that can
be used for emotion intensity recognition. The results with LBP features arebetter than those using HOG and Gabor features.
The CK+ dataset [11] contains emotion annotations as well as action unit
annotations. In the classification stage, the dataset was evaluated by AAM and
SVM. AAM tracks the face and extracts facial features, and then SVM classifies
the facial expressions. For each expression, the architecture achieves more
than 65% accuracy, and the best recognition accuracy is 100% for the happy
emotion.
In recent years, deep neural network has become popular in recognizing facial
expressions and other computer vision tasks. In [36], LeNet-5, which is the
earliest convolutional neural network, is presented to recognize handwriting.
There are only seven layers in the network, including three convolutional
layers, two sub-sampling layers and two fully connected layers. Then, many
variants of networks based on this basic design are prevalent in deep learning
tasks. VGG Net [32] used very small convolution filters (3*3) to increase the
architecture depth, where the small-size filter can make the decision function
more discriminative and decrease the number of parameters. The network often
stacked several convolutional layers and then followed one pooling layers.
When there are 16 or 19 wt layers in the network, the architecture can achieve
significant improvement on the prior-art configurations. GoogLeNet [3] is a
22-layer deep network. Not only the width, but also the depth of the network
is increased compared with previous networks. The main structure of the
network is ?Inception? layers, which contain several parallel convolution
branches. ?Inception? layers have difference sizes of convolution filters, and
the input images can convolve at different scales of feature maps. In ResNet
[4], skip connections are added between the input layer and output layer in
the network. This structure not only increases the training speed and improves
the training effect of the model, but also avoids gradient disappearance and
network degradation.
For the facial expression recognition task, most works were inspired by the
above-mentioned deep network architectures. When classifying static images, a
facial expression recognition network [2] inspired by GoogLeNet was proposed.
The network includes two convolutional layers, each of which is followed by a
max-pooling layer, and then four inception layers are followed. Sun et al.
[12] proposed a facial expression recognition network with visual attention.
In this network, the deep convolution features are extracted from the face,
and thus the regions of interest are detected and used to classify
expressions. In [5], binarized Auto-encoders and Stacked Binarized Auto-
encoders were used to learn a type of domain knowledge from unlabeled facial
expression datasets. Fernandez et al. [38] proposed an end-to-end network with
an attention model for facial expression recognition. The results showed that
the attention module improves the classification performance. In [1], a GAN-
based face frontalization method was presented. The input face images are
frontalized by the generator and the identity and expression characteristics
are preserved at the same time. Then, the discriminator makes a distinction
between the real images and the generated face images.
While classifying video images, in [7], a DNN-based architecture combined with
conditional Random Field was proposed to solve the expression recognition
problem. Here, the Inception-ResNet networks [6] is used in the networks for
facial expression recognition, the experimental results show good performance
on CK+, MMI and FER2013. In [33], VGG-based convolutional neural network was
first used to learn facial features, which are then linked to a Long Short
Term Memory (LSTM) to exploit the temporal relation between video frames. An
accuracy of 97.2% was obtained on CK+. A network termed Spatio-Temporal
Convolutional features with Nested LSTM (STC-NLSTM) [8] was proposed to learnmuti-level facial expression features and temporal dynamics of facial
expressions in a joint way.
In this paper, a novel convolutional neural network with attention mechanism
is presented to classify different facial expressions. We combine LBP features
and convolution features in an attention module. With the help of LBP features
which provide texture information and can reflect fine changes on the face,
the ability of the attention module can be improved so as to improve the
recognition accuracy of the network. In addition, in order to avoid
overfitting, data augmentation is applied in the datasets used in our
experiments. We also use the batch normalization after each layer to speed up
the convergence of the network.
## 3\. The proposed method
### 3.1. Network architecture
In this section, we introduce our newly proposed convolutional neural network
with attention mechanism for automatically recognizing facial expressions. The
new network consists of four parts, i.e., the feature extraction module, the
attention module, the reconstruction module and the classification module. The
architecture starts from the feature extraction module composed of two
separate CNN processing streams: one is for raw images and the other is for
LBP feature maps. Our model uses pure convolutional layers as the backbone to
extract features. To prevent the network from being too complex, small-size
convolution filters (3 × 3) are used in all layers. Because VGG-16 Net has
strong ability of transfer learning and a flexible architecture, it can easily
concatenate the backend to extract deeper features for classification. To this
end, the first 13 layers of VGG-16 are used as the front-end of our model to
extract initial features of the raw images. For LBP feature images, we also
use the first 13 layers from VGG-16 to extract deeper features and then reduce
the dimensionality to the same as that of the raw images.
As shown in Fig. 5, our dimensionality reduction-based CNN and feature
extraction-based CNN have the same architecture. In order to reduce the
computational complexity, we add three 1 × 1 × 64 convolutions into the
original VGG-16 net to reduce the channels. Unlike the traditional two-stream
CNN networks such as Light-CNN [30], [44] which extract two different features
and then simply fuse them to classify, our feature extraction module is to
obtain initial features for future processing in the following modules.
Afterwards, we fuse the features F1 extracted from the raw images with the
features F2 extracted from the LBP feature images, and then add the fused
features F3 to an attention module.
1. Download: Download high-res image (215KB)
2. Download: Download full-size image
Fig. 4. The architecture of our network.
1. Download: Download high-res image (168KB)
2. Download: Download full-size image
Fig. 5. The architecture of dimensionality reduction-based CNN and feature
extraction-based CNN.
The attention module works by increasing the weights of useful features and
makes the network focus more on these features that are vital for expression
recognition. In this way, the network can recognize different expressions more
efficiently.
Followed the attention module, we use a dense connection convolution layers as
the reconstruction module to adjust the attention map in order to create an
enhanced feature map for the classification module. Many works like DenseNet
[14] and ResNet [4] have shown that convolution networks with skip connections
between different layers can be substantially deeper, more accurate, and moreefficient to train. Motivated by DenseNet [14] and ResNet [4], in this work,
we use dense atrous convolution for reconstruction. Atrous kernel can be
dilated in varied rates by inserting zeros into appropriate positions in the
kernel mask. A large dilation rate means a large receptive field, vice versa.
Our dense atrous convolution module consists of four 3 × 3 atrous convolution,
the dilation rates from lower layers to higher layers are 2, 3, 4 and 5,
respectively. Compared to the traditional convolution operator, atrous
convolution is able to achieve a larger receptive field size without
increasing the number of kernel parameters. Feature maps from the attention
module contain important information for expression recognition. Atrous
convolution can increase the receptive field while keeping the resolution of
feature maps unchanged. We can further extract features with atrous
convolution without information loss and have larger a receptive field. For
each layer, the feature-maps of all preceding layers and features F1 are
concatenated as inputs, and its own feature-maps are used as inputs into all
subsequent layers. The fused feature maps F3 are made element-wise sum
operation with the output of the attention module. This architecture not only
can extract deeper features but also can help alleviate the problem of
vanishing-gradient and reuse useful features. At last, fully connected layers
with softmax are used for classification. We use the batch normalization after
each layer to speed up the convergence of the network and avoid overfitting.
The detail of each module in the schema can be seen in Fig. 4.
### 3.2. Attention mechanism
For a classification task, we extract image features and classify the images
into different categories by the differences among these features. However,
only useful features are helpful for classification and different features
contribute different significance. The attention mechanism has been proved to
be useful in pixel-wise computer vision tasks and is used to measure how much
attention to pay to the features in different regions. In this paper, our
attention module contains two branches, one is the trunk branch to obtain
feature Fp, and the other is the mask branch which integrates LBP features to
obtain attention maps Fm. Then, the element-wise product is applied on
attention mapsFm and the feature maps Fp to generate refined feature maps Fm
as:(1)Frefine=FpFm
Suppose the input of the last layer in the mask branch as fm, the attention
maps Fm are generated as:(2)©Fm=SigmoidW©fm+bwhere w and b are the weights and
bias of the convolution layer, respectively; ©© denotes the convolution
operation and Sigmoid denotes the sigmoid function. The Sigmoid activation
function gives out (0, 1) probability scores to make network discriminate the
importance of different features. Fig. 6 provides two example images in
FER2013 and their corresponding attention heatmaps generated by our method. As
we can see, the heatmaps show the attention areas clearly. For the upper side
?angry? face, the attention of our network is mainly on the eyes; for the
lower side ?happy? face, the attention of our network is mainly on the mouth.
This indicates that the eyes area contains the most useful features for
recognizing the angry expression; while the mouth area is the most suitable
for recognizing the happy expression.
1. Download: Download high-res image (90KB)
2. Download: Download full-size image
Fig. 6. Results of attention heatmap.
### 3.3. Local binary patterns
Local binary patterns (LBPs) reflect the basic information that is helpful to
recognize facial expressions. More specifically, subtle changes can be
reflected by the features extracted with LBPs. In addition, LBPs can achieverotation invariance and grey-scale invariance and thus are suitable for
extracting texture features at different scales and can solve the displacement
imbalance, rotation angles and illumination conditions in facial images. In
Fig. 7, we provide some examples of LBP images extracted from five datasets
used in this paper.
1. Download: Download high-res image (517KB)
2. Download: Download full-size image
Fig. 7. Examples of LBP images extracted from CK+, JAFFE, NCUFE, Oulu-CASIA
and FER2013. From top to bottom is the original image, circle LBP image,
original LBP image, and uniform LBP image.
In [9], the original LBP defined in a 3 × 3 window was first introduced. The
center pixel of the window is taken as the threshold and then is compared with
the gray values of the adjacent 8 pixels. If the value of the surrounding
pixel is greater than the value of the center pixel, the position of the pixel
is marked as 1; otherwise, it is 0. In this way, the comparison of the 8
points in the 3 × 3 neighborhood can generate 8-bit binary numbers, which are
usually converted to decimal numbers which is namely LBP code and have a total
of 256 kinds. Through the above steps, the LBP value of the pixel in the
center of the window is obtained, and the texture information of the region is
reflected by the value. It can be defined as follows:(1)LBPxc,yc=?p=07Sip-
ic2pwhere p is the number of pixels, (xc,yc) is the coordinate of the center
pixel, ic is the pixel value of the center pixel and ip is the pixel value of
the neighborhood pixel, s is the sign function and can be defined as
follows:(2)Sx=1if x?00if x?0
The biggest drawback of the original LBP is that only a small region is
covered by it within a fixed radius, and this is not suitable when the sizes
and frequency of textures are different. Circle LBP was proposed to make it
suitable for different size and frequency textures features and meet the needs
of gray scale and rotation invariance. It is not only 3 × 3 neighborhood but
can be any neighborhood, and the circular neighborhood is also replaced with a
square neighborhood. Circle LBP allows arbitrary multiple pixel points in the
circular neighborhood with radius R. The LBP operator with P sampling points
in the circular region with radius R is obtained. It can be defined as
follows:(3)LBPP,Rxc,yc=?P=0P-1Sip-ic2p#where p is the number of sampling
points, R is the radius of the circle neighborhood, (xc,yc) is the coordinate
of center pixel, ic is the pixel value of the center pixel and ip is the pixel
value of P sampling points, and s is the sign function which is same as in Eq.
(2).
Further extension of LBP such as uniform patterns [10] was used to solve the
problems of too many binary eigenvalue encoding modes and improve statistical
performance. When the bitwise pattern is circularity, uniform LBP includes at
most two bitwise transitions from 0 to 1 or 1 to 0. It can be defined as
follows:(4)LBPP,Rrius2=?P=0P-1s(ip-ic)ifU(LBPP,R)?2P+1otherwisewhere U is the
uniformity measure and the ?rius2? represents rotation-invariant uniform
pattern.
### 3.4. Data augmentation
In order to ensure that the network can achieve a good generalization ability,
enough training data are required. However, most publicly-available datasets
such as JAFFE do not have enough number of images for training. The amount of
data is small, which can also lead to the overfitting problem. Therefore, data
augmentation is important for facial expression recognition.
Label-preserving transformation is one of the most common methods to enlarge
the number of images in datasets. In this paper, four methods are used to
enlarge the image number of the original data and the example images afteraugmentation are shown in Fig. 8. We take random rotation, flipping, shifting
and scaling on the images. The rotation range is 0?20° and the shifting ranges
of width and height are set to 0?15%. Both the shear range and the zoom range
are 0?0.15. Before data augmentation, the number of images in FER2013, CK+,
JAFFE, Oulu-CASIA and NCUFE are 35,795, 981, 213, 1440 and 735, respectively.
After data augmentation, the number of images in FER2013, CK+, Oulu-CASIA and
NCUFE are increased to 45,017, 12,000, 11,843, 13,800 and 13,827,
respectively.
1. Download: Download high-res image (65KB)
2. Download: Download full-size image
Fig. 8. Examples for data augmentation in the NCUFE dataset, (a) is the image
from the original dataset, (b), (c), (d) and (e) are the images after
augmentation.
## 4\. Experimental results
In this work, we design a novel deep Convolutional Neural Network with an
attention model to automatically recognizing facial expressions. Except for
the famous facial expression datasets such as CK+ [11], JAFFE [13], Oulu-CASIA
[25] and FER2013 [39], we also evaluate our proposed method on our self-
collected dataset NCUFE. Because CK+, JAFFE, Oulu-CASIA and NCUFE do not
provide specified training and testing sets, we employ 5-fold cross-validation
protocol in these four datasets. The proposed facial expression recognition
framework is implemented with Keras. The framework is trained with an
initialized learning rate 0.000001, and the batch size is 40. The size of
input data is 256,256× because using large images as input to the network can
make the network as deep as possible and help extract more useful features. We
add batch normalization after each convolutional layer. Our network is trained
on one TITAN RTX GPU. In this section, we first conduct an ablation study on
FER2013 [39] to analyze the configuration of our framework (as shown in Table
1). Then, we evaluate our proposed method in all these five datasets and
compare the result to some state-of-the-art methods.
Table 1. Comparison of different LBPs on FER2013 dataset.
LBP| Recognition Rates (%)
---|---
Original LBP| 72.56
Uniform LBP| 70.32
**Circle LBP**| **75.82**
Without LBP| 67.73
### 4.1. Ablations on FER2013
We perform an ablation study to analyze the effects of three kinds of LBPs
(original LBP, circle LBP and uniform LBP) in our method on the FER2013
dataset [39], which is a large-scale and unconstrained dataset collected from
Internet. It is challenging to classify these images because of varied
perspectives of face, wrong labels and some other noises. The recognition
results are shown in Table 1, where circle LBP achieves the best accuracy.
Therefore, we use circle LBP in our model for the following experiments.
Our model includes four parts, which are the feature extraction module, the
attention module, the reconstruction module and the classification module. In
order to figure out how each module affects the performance of our network we
perform an ablation study on the networks. Because the classification module
is necessary to classify in our networks, we retain the classification module
and remove the feature extraction module, the attention module, and the
reconstruction module separately and then conduct different experiments. The
results are given in Table 2. As we can see, when one of these modules is
removed, the recognition rate has a certain degree of decline compared withthe complete network which achieves the recognition rate of 75.82%.
Especially, when there is no feature extraction module, the recognition rate
drops to 57.86%. In the feature extraction module, initial features are
extracted from raw images and then sent to later modules for future
processing. Generally, the obtained initial features are too coarse, but we
need more refined features to send to the later attention module in order to
make it work directly. Therefore, the feature extraction module is necessary
in the network for extracting refined features. The attention module can make
our network focus more on useful features and improve the recognition rate.
When the attention module is removed, the recognition rate reduces to 73.96%,
which proves the effectiveness of the attention module. The reconstruction
module can improve the recognition rate by adjusting the attention map to
create an enhanced feature map. When there is no reconstruction module in the
network, the recognition rate drops to 74.25%. Based on the experimental
results, we conclude that each module has a certain degree of improvement on
the final results.
Table 2. Comparison of different architectures on the FER2013 dataset.
Architecture| Recognition Rates (%)
---|---
Without the feature extraction module| 57.86
Without the attention module| 73.96
Without the reconstruction module| 74.25
Complete network| 75.82
When designing the reconstruction module, we try several atrous kernels of
different dilation rates. We perform experiments on five kinds of
configurations and use A, B, C, D and E to refer to them separately. There are
four atrous convolutions in the reconstruction module. In A, B, C and D, we
use invariable dilation rates, the dilation rates are 1, 2, 3, and 4
separately. We use four atrous convolutions with four different dilation rates
in E, which are 2, 3, 4 and 5 separately. The results are provided in Table 3.
We can see that the reconstruction module E achieves the highest recognition
rates. Therefore, we use variable dilation rates which are 2, 3, 4, 5 in the
proposed method in this paper.
Table 3. Comparison of different dilation rates of the reconstruction module
on the FER2013 dataset.
Dilation Rate| Recognition Rates (%)
---|---
A (1)| 74.83
B (2)| 73.91
C (3)| 75.52
D (4)| 74.31
E (2, 3, 4, 5)| 75.82
### 4.2. Evaluation and comparison
#### 4.2.1. FER2013 dataset
FER2013 [39] is a dataset collected in real world, which includes 28,709
training images and 3589 test images. Unlike CK+, Oulu-CASIA and JAFFE,
FER2013 contains pictures of different postures, unbalanced illumination, and
occlusion. We evaluate and compare our method with other five recent works
[18], [30], [40], [42], [44]. The recognition results are given in Table 4,
which show that our method is superior to all of the five advanced algorithms.
The recognition rates are reduced in this dataset when there is no LBP or
attention module. The result is reduced to 67.73% without LBP and 73.96
without attention module.
Table 4. Comparison of different methods on the FER2013 dataset.Method| Recognition Rates (%)
---|---
Wang [18]| 71.1
Pramerdorfer et al. [42]| 75.2
Kim et al. [41]| 73.73
Guo et al. [40]| 71.33
Shao et al. [30]| 71.14
**Ours**| **75.82**
Ours (without LBP)| 67.73
Ours (without attention module)| 73.96
#### 4.2.2. CK+ dataset
The CK+ dataset [11] includes 593 image sequences collected from 123 subjects,
and 327 of these sequences are labeled with six facial expressions (i.e.,
anger, disgust, fear, happiness, sadness, and surprise). Each image sequence
gradually reaches a peak expression from neutral face. In this paper, for each
kind of these six expressions, we select the last three frames with peak
information as our new dataset. The comparison results of our method and some
representative methods [20], [30], [33], [35], [37] on this dataset are listed
in Table 5, which indicate that our method performs better than most of the
methods except Zhang et al. [37]. However, the multitask network in [37]
learns from some auxiliary attributes like gender, age and head pose, except
for facial expression images. Our method can achieve competitive result by
only using the facial expression data. When removing the LBP or the attention
module, the recognition results are reduced from 98.68% to 97.53% and 97.10%,
respectively. This shows that LBP and the attention module can improve the
recognition accuracy.
Table 5. Comparison of different methods on the CK+ dataset.
Method| Recognition Rates (%)
---|---
VGG Net + LSTM [33]| 97.2
Yang et al. [35]| 97.3
**Zhang et al.** [37]| **98.9**
Turan [20]| 96.10
Shao et al. [30]| 95.29
Ours| 98.68
Ours (Without LBP)| 97.53
Ours (Without the attention module)| 97.10
#### 4.2.3. Oulu-CASIA dataset
The Oulu-CASIA dataset [25] contains 2880 image sequences which are collected
from 80 subjects and each sequence is labeled with six facial expression
labels (i.e., anger, disgust, fear, happiness, sadness, and surprise). Each
image sequence gradually reaches a peak expression from neutral face. The last
three frames from each expression sequence are selected to be new data used in
our experiment. We compare our approach with previous state-of-the-art methods
and give the results in Table 6. As we can see, our method is superior to all
the other methods and achieves the state-of-the-art performance which is
94.63%. This also demonstrates that LBP and the attention module can improve
the accuracy in different degrees. When there is no LBP or attention module,
the results are reduced to 93.52% and 91.17 respectively.
Table 6. Comparison of different methods on the Oulu-CASIA dataset.
Method| Recognition Rates (%)
---|---
Zhong et al. [28]| 93.06
Yang et al. [26]| 88.00Zhang et al. [23]| 86.95
Kuo et al. [19]| 88.75
**Ours**| **94.63**
Ours (Without LBP)| 93.52
Ours (without the attention module)| 91.17
#### 4.2.4. JAFFE dataset
The JAFFE dataset was collected from 10 Japanese females in a laboratory
condition and includes 213 images of posed expressions. For each subject,
there are three or four images belonging to one of the six expressions which
are anger, disgust, fear, happiness, sadness, and surprise, and one image
belongs to neutral face. We compare our method with five existing methods
[18], [20], [27], [31] and the comparison results are provided in Table 7,
which shows that the result of our method achieves 98.52% and outperforms the
others on the JAFFE dataset. When LBP or attention is not used, the
recognition rates are reduced to 96.53% and 95.12%, respectively.
Table 7. Comparison of different methods on the JAFFE dataset.
Method| Recognition Rates (%)
---|---
Hamester et al. [31]| 95.8
Liu et al. [27]| 91.8
Turan [20]| 91.8
Wang [18]| 95.7
**Ours**| **98.52**
Ours (Without LBP)| 96.53
Ours (Without the attention module)| 95.12
#### 4.2.5. NCUFE dataset
We collected a new facial expression dataset called Nanchang University Facial
Expression (NCUFE) from 35 graduate students (6 females and 29 males) by the
Microsoft Kinect sensor. For each student, we captured both RGB images and
depth images, and the sizes of these two kinds of images are 1280*1024 and
512*424, respectively. The NCUFE dataset was annotated with seven expressions
(i.e., anger, disgust, fear, happiness, sadness, surprise, and neural). For
each image type, there are 245 image sequences, each of which contains 110
images, resulting in 26,950 images in total. Because none of the other four
datasets has depth images, we only use RGB images as our datasets in this
work. To better distinguish different kinds of expressions, for every
expression sequence, we select three images that are close to the peak
expression as our new dataset. We evaluate our method on this dataset and
achieve 94.33% accuracy. When we remove LBP or the attention module, the
recognition rates are reduced to 93.91% and 92.07%, respectively. In addition,
when we replace LBP with depth images, the recognition result is lower than
that using only raw images or LBP images. It indicates that depth images are
not helpful in our networks. The reason is that in our network we fuse these
two kinds of features by element-wise sum, as a kind of auxiliary feature,
depth features have a negative effect on the raw image features when fusing
these two kinds of features. In our future work, we will apply depth images to
other methods, for example, we can concatenate the feature vectors of these
two kinds of features. We also compare our method with some other works. From
Table 8, we can find that our method outperforms the others on our dataset.
The recognition results of [29], [30], [34] on NCUFE are 91.58%, 81.3% and
82.5%, respectively.
Table 8. Comparison of different methods on NCUFE dataset.
Method| Recognition Rates (%)
---|---Li et al. [34]| 91.58
Arriaga et al. [29]| 81.3
Light-CNN. [30]| 82.5
**Ours**| **94.33**
Ours (Without LBP)| 93.91
Ours (Without attention module)| 92.07
Ours (raw + depth)| 90.35
## 5\. Conclusions and future work
This paper presents a novel convolutional neural network with attention
mechanism for facial expression recognition. The method fuses LBP features and
convolution features, and then is combined with attention mechanism to improve
the performance of the network. In order to prevent overfitting and ensure the
generalization ability of the network, we apply data augmentation in the
datasets we used in the experiments. In addition, we collected a new dataset
called Nanchang University Facial Expression (NCUFE) and the NCUFE dataset was
annotated with seven expressions, which are anger, disgust, fear, happy, sad,
surprise, and neutral. For each image type, there are 245 image sequences,
each of which contains 110 images, resulting in 26,950 images in total. The
presented method is evaluated on NCUFE and four famous facial expression
datasets, i.e., Oulu-CASIA, JAFFE, CK+ and FER2013. The experimental results
show that our method is superior to many existing methods on these datasets.
However, our method is only suitable for 2D images. In the future, we will
improve our architecture to make it suitable for video data, 3D face datasets
and our depth images data, and explore better machine learning methods to
enhance the network.
## CRediT authorship contribution statement
**Jing Li:** Conceptualization, Methodology, Writing - original draft. **Kan
Jin:** . **Dalin Zhou:** . **Naoyuki Kubota:** Writing - review & editing.
**Zhaojie Ju:** Conceptualization, Methodology, Writing - original draft.
## Declaration of Competing Interest
The authors declare that they have no known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.
## Acknowledgements
This work is supported by National Natural Science Foundation of China under
Grant 61963027, 61703198, and 51575412, Natural Science Foundation for
Distinguished Young Scholars of Jiangxi Province under Grant 2018ACB21014.
Recommended articles"
32,34,Au-aware deep networks for facial expression recognition,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2013,318,MMI Facial Expression,facial expression recognition,"controlled expression database CK+ [21], MMI [22], and a wild  facial expression recognition.  All comparisons are performed on three datasets: CK+ [21], MMI [22], and a wild expression",No DOI,… face and gesture recognition (FG),https://ieeexplore.ieee.org/document/6553734,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
33,35,Au-inspired deep networks for facial expression feature learning,"['M Liu', 'S Li', 'S Shan', 'X Chen']",2015,265,"Acted Facial Expressions In The Wild, MMI Facial Expression","deep learning, neural network","Most existing technologies for facial expression recognition utilize off-the-shelf feature  extraction methods for classification. In this paper, aiming at learning better features specific for",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231215001605,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: AU-inspired Deep Networks for Facial Expression Feature
Learning
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. AU-inspired Deep Networks
6. 3\. Databases and evaluation protocols
7. 4\. Experiments
8. 5\. Conclusion
9. Acknowledgment
10. References
11. Vitae
Show full outline
## Cited by (205)
## Figures (11)
1. 2. 3. 4. 5. 6.
Show 5 more figures
## Tables (7)
1. Table 1
2. Table
3. Table 2
4. Table 3
5. Table 4
6. Table 5
Show all tables
## Neurocomputing
Volume 159, 2 July 2015, Pages 126-136
# AU-inspired Deep Networks for Facial Expression Feature Learning
Author links open overlay panelMengyi Liu, Shaoxin Li, Shiguang Shan, Xilin
Chen
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2015.02.011Get rights and content
## Abstract
Most existing technologies for facial expression recognition utilize off-the-
shelf feature extraction methods for classification. In this paper, aiming atlearning better features specific for expression representation, we propose to
construct a deep architecture, AU-inspired Deep Networks (AUDN), inspired by
the psychological theory that expressions can be decomposed into multiple
facial Action Units (AUs). To fully exploit this inspiration but avoid
detecting AUs, we propose to automatically learn: (1) informative local
appearance variation; (2) optimal way to combining local variation and (3)
high level representation for final expression recognition. Accordingly, the
proposed AUDN is composed of three sequential modules. Firstly, we build a
convolutional layer and a max-pooling layer to learn the Micro-Action-Pattern
(MAP) representation, which can explicitly depict local appearance variations
caused by facial expressions. Secondly, feature grouping is applied to
simulate larger receptive fields by combining correlated MAPs adaptively,
aiming to generate more abstract mid-level semantics. Finally, a multi-layer
learning process is employed in each receptive field respectively to construct
group-wise sub-networks for higher-level representations. Experiments on three
expression databases CK+, MMI and SFEW demonstrate that, by simply applying
linear classifiers on the learned features, our method can achieve state-of-
the-art results on all the databases, which validates the effectiveness of
AUDN in both lab-controlled and wild environments.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
AU-inspired Deep Networks (AUDN)
Micro-Action-Pattern
Receptive field
Group-wise sub-network learning
## 1\. Introduction
In the recent years, automatic facial expression recognition has attracted
much attention due to its potential applications, such as human?computer
interaction, multimedia, and surveillance. In the literature, many works have
been done, especially to classify six basic expressions [1], [2], [3]. However
it remains a great challenge to achieve accurate expression recognition in
real-world applications due to the large inter-personal differences in facial
expression appearance, as well as those caused by other imaging conditions.
The key to cope with the challenge is to develop more robust feature
representations for facial expression. According to the representation
exploited, previous expression recognition methods can be roughly categorized
into two groups: Action Units (AU) based methods [4], [5] and appearance-based
methods [2], [6], which will be briefly reviewed respectively in the
following. For a more comprehensive survey of facial expression recognition,
readers are referred to [7].
The _AU-based methods_ are designed based on strong support from physiology
and psychology, as facial expression is the result of the motions of facial
muscles. In the Facial Action Coding System (FACS) [8], an pioneering work in
expression recognition, each expression is decomposed into several facial
Action Units (AUs) with correspondence to the muscle motions. Following the
FACS, many methods recognize expressions by first detecting these pre-defined
AUs and then decoding specific expression from them. For instance, [4]
utilized the positions of facial landmarks and geometrical modeling for AU
recognition and then expression analysis, and [5] used a dynamic Bayesian
network to model the relationships among various AUs to achieve the
recognition of single AU or AU combinations. However, since the AUs are
essentially defined based on invisible muscle motions, they are difficult todetect from the skin appearance in the face image.
On the contrary, given the input images, _appearance-based methods_ recognize
facial expressions via classifying with some off-the-shelf features extracted
directly from the image, without explicitly taking muscle motion into account.
Typical features include Local Binary Pattern (LBP) as in [2], Gabor as in
[9], Local Gabor Binary Patterns (LGBP) as in [10], Scale Invariant Feature
Transform (SIFT) as in [11], and Histogram of Oriented Gradient (HOG) as in
[12]. As the variations caused by expression are mainly located around certain
facial parts, extracting the features from the whole face might undermine the
contribution of the most expressive regions. To avoid undesirable influence
caused by irrelevant appearance variations, [3], [13] applied feature
selection to automatically search for descriptive features extracted from
local patches. Then the selected local features are directly concatenated into
a vector for expression classification.
To briefly summarize, the appearance-based methods attempt to infer high-level
expression category directly from low-level features, which is difficult and
also known as the semantic gap. Thus we need something in the middle to bridge
them. For facial expression, AU is a good choice to play the role.
Unfortunately, AU seems not computationally feasible due to its rigid
definition and hard encoding manner. To handle the above problems, in this
paper, we construct a deep network to learn such mid-level representation
automatically. Inspired by the principle of AUs, we propose a computational
concept, Micro-Action-Pattern (MAP), which is learned from data for capturing
local appearance variations caused by motion of facial muscles, such as frown,
grin, and glare. Furthermore, similar to the decomposition of expression into
AUs, we also designed mechanism to combine MAP into groups to simulate the
abstract of higher-level concepts. As MAPs can be treated as units in a
certain layer of our deep networks, each group of MAPs constitutes a receptive
field of a certain unit in the next layer to specify the network connections
between adjacent layers. For each receptive field, i.e. a subset of MAPs, a
multi-layer learning process is further employed, which constructs a group-
wise sub-network for learning incrementally higher-level features. An overview
of the proposed method is presented in Fig. 1. There are three sequential
modules, i.e. Micro-Action-Pattern (MAP) representation learning, receptive
field construction, and group-wise sub-network learning. As the proposed deep
architecture performs a hierarchical feature learning process inspired by the
interpretation of facial AUs, we call it AU-inspired Deep Networks (AUDN).
1. Download: Download full-size image
Fig. 1. The pipeline of the proposed method. There are three sequential
modules, i.e. Micro-Action-Pattern (MAP) representation learning, receptive
field construction, and group-wise sub-network learning.
This paper is an extended version of our previous conference paper [14]. There
are three major differences in this extended version: (1) different strategies
for receptive field construction are extensively evaluated for comprehensive
understanding about optimal feature combination in deep networks; (2) For
group-wise feature learning in each receptive field, two methods, i.e. Multi-
Layer Perceptron (MLP) [15] and Deep Belief Networks (DBN) [16], are compared
to clarify the necessity of additional unsupervised pertaining in DBN and (3)
cross-database validation experiments are added to validate the generalization
ability of proposed AUDN method for expression recognition.
The main contributions of this paper are summarized as follows: (1) We propose
a novel AU-inspired feature learning framework to extract features
specifically for expression recognition, which have strong descriptive power
and are more physiologically and psychologically appealing. (2) Differentreceptive field construction and subnetwork learning schemes are investigated
to explore what kind of compositional local features is good for mid-level
expression description. (3) With only linear classifier, the learned features
achieve state-of-the-art performance on all three databases under strict
person-independent protocol, i.e. lab-controlled databases CK+ [17], MMI [18],
and a wild scenario database SFEW [19].
The remainder of this paper is structured as follows. We present the details
of our proposed AUDN in Section 2. Section 3 gives a brief introduction of the
databases and settings in our experiments. Section 4 provides comprehensive
evaluations on our whole framework and three sequential modules. Section 5
concludes the paper.
## 2\. AU-inspired Deep Networks
As shown in Fig. 1, the proposed AUDN is constituted of three functional
modules, i.e. MAP representation learning, receptive field construction and
group-wise subnetwork learning. In this section, we present how these three
well-designed modules in AUDN can help learn expression specific features
automatically.
### 2.1. Micro-Action-Pattern Representation
One of the key ingredients of well known FACS [8] theory is that an observed
expression can be decomposed into a set of local appearance variations
produced by specific AUs. Inspired by this observation, in order to learn high
level expression specific features, we should first encode these local
variations for subsequent usage. Considering the locality of AU, we densely
(with step of 1 pixel) sampled a large number of small patches from all the
training expression images. And by employing a clustering algorithm, we can
obtain a bank of typical patterns, the so-called Micro-Action-Pattern (MAP)
prototypes in our framework, to jointly represent all kinds of local
variations caused by facial expressions. Specifically, suppose the patch size
is _u_ -by-_u_ pixels, to obtain an over-complete representation, we set K>u2
in K-means clustering and learn _K_ centroids c(k)(k=1,2,?,K) from all patches
after normalizing and whitening, which are considered as the MAP prototypes
mentioned above. Then each MAP prototype is used as a filter to convolve with
the patches in a whole facial images, for calculating ?responses? to this MAP
(filter). For an _l_ -by-_l_ -pixel input image with _t_ -by-_t_ patches
(where t=l+1?u), each 2D grid of _t_ -by-_t_ responses for a single filter is
generally called a ?feature map?. In the end we will get a _t_ -by-_t_ -by-_K_
dimensions representation after the convolutional layer. To achieve
translation invariance, we further apply max-pooling over adjacent, disjoint
_p_ -by-_p_ patches on each _t_ -by-_t_ map to obtain the final MAP
representation for each expression image (see Fig. 2).
1. Download: Download full-size image
Fig. 2. Implementation details: (a) MAP prototypes (i.e. convolutional
filters) learned by K-means. (b) Process of convolving and pooling.
### 2.2. Receptive field construction
In this module, we focus on constructing receptive fields from the outputs of
max-pooling layer (i.e. MAP representation), each of which corresponds to a
complex combinations of local appearance variations depicted by MAPs.
#### 2.2.1. Discussion
Due to the preconceived biological notions of receptive field, in the
traditional deep networks [20], [21], [22], [23], the receptive fields are
often manually defined as local spatial regions. However, in various
recognition scenarios, the size and shape of receptive fields may be task-
dependent which makes it difficult to pre-define without prior knowledge. As
expression often involves a set of synergetic movements of disconnected facialregions, such task-dependent receptive field architecture may be especially
crucial for deep networks aiming at facial expression modeling. To address
this problem, some previous works did have been proposed. For example, [24]
proposed to select receptive fields with arbitrary shapes from a pre-generated
over-complete set, which is constituted of all possible rectangle candidates
of receptive field in spatial locality. Coates and Ng [25] proposed to
construct receptive fields automatically by grouping sets of most similar
features in the current deep network layer. In this work, we propose to
exploit more adaptive receptive fields in network layers. Two major issues
have been considered on grouping MAPs into receptive field: feature redundancy
within each receptive field and feature relevance to the expression
categories. First, if features are highly redundant, single receptive field
may not be informative enough for subsequent feature learning. Second, the
relevance between features and expression categories should be considered to
improve the description and discrimination of each receptive field.
#### 2.2.2. Formulation and algorithm
In this paper, a uniform formulation is proposed to discuss the effect of two
factors: usage of supervised information (Supervision (S)/No Supervision
(NS)), i.e. the relevance between features and expression labels, and within-
receptive-field feature redundancy (Redundancy (R)/No Redundancy (NR)).
Before presenting the uniform formulation of receptive field construction, we
first introduce the measurement of supervision and redundancy. In our
approach, the supervision is measured by the relevance between the category
label and specific MAP features, and the redundancy is measured by the
relevance among MAP features within the same receptive field. Here mutual
information is utilized to measure relevance of two variables. Formally, given
two random variables _x_ and _y_ , their mutual information is defined in
accordance with their probabilistic density functions _p_(_x_), _p_(_y_), and
p(x,y):(1)I(x;y)=?p(x,y)logp(x,y)p(x)p(y)dxdy.Suppose a MAP subset _S_ (that
forms the receptive field) has _m_ features S={xi|i=1,2,?,m}. Given expression
label _c_ , the supervised information can be represented by measuring the
overall label-relevance:(2)D(S,c)=1|S|?xi?SI(xi;c).If there is no supervision,
the self-information entropy can be used
instead:(3)H(S)=1|S|?xi?SI(xi;xi).Intuitively, features of higher label-
relevance or larger entropy are more descriptive for classification, as the
former implies more discriminability while the latter means larger variance.
So, we attempt to maximize D(S,c) in case of supervision or _H_(_S_) in
unsupervised scenario. Note that, in implementation, the distribution of
continuous _x_ _i_ is approximated via discretizing it to discrete variables.
Similarly, the overall redundancy between every pair of MAP features within a
receptive field is defined as follows:(4)R(S)=1|S|2?xi,xj?SI(xi;xj).In
previous deep networks, the receptive fields are often manually designed as
local spatial regions, in which the features are highly redundant. We argue
that this kind of receptive field may not be informative enough for subsequent
feature learning. To explore whether the features within each receptive field
should be more redundant or not, we evaluate two conflicting criteria:
maximize _R_(_S_) and minimize _R_(_S_). By combining it with the above
information theory terms, four criteria are designed respectively for
comparison: max(H+R), max(H?R), max(D+R), and max(D?R). The four schemes and
formulations are listed in Table 1.
Table 1. The correspondence of our schemes for MAP grouping and their
formulations.
Schemes| NS + R| NS + NR| S + R| S + NR
---|---|---|---|---Formulations| max(H+R)| max(H?R)| max(D+R)| max(D?R)
Inspired by [26], we apply a greedy search to find the local optimal features
defined by our objective. Suppose we already have a set Sk?1 containing k?1
features. Our goal is to find the _k_ -th feature from the rest P=X?Sk?1. The
incremental algorithm optimizes the following
condition:(5)maxxj?P[OBJ(xj)±1k?1?xi?Sk?1I(xj;xi)].where OBJ(xj) represents
I(xj;c) or I(xj;xj). The algorithm is shown in Algorithm 1.
**Algorithm 1**
Receptive Field Construction.
**Input**
---
MAP representations: X={xi|i=1,2,?,M};
Expression labels: _c_ ;
The number of MAPs in each receptive field: _m_ ;
The number of receptive fields: _N_ ;
**Output**
Receptive fields RF(n)={xi(n)|i=1,2,?,m};
**Algorithm**
1: Initialize S=?, P=X?S, RF(n)=?;
2: **for** k=1,?,m **do**
3: **for** n=1,?,N **do**
4: Search for feature _x_ _s_ from _P_ based on Eq. (5):
(6)xs={argmaxxs?POBJ(xs),k=1argmaxxs?P[OBJ(xs)±1k?1?xi(n)?RF(n)I(xs;xi(n))],k>1
5: Update RF(n)=RF(n)?{xs}, S=S?{xs}, P=X?S ;
6: **end for**
7: **end for**
To show the difference of the selected features in each receptive field under
different schemes, some examples of local patches corresponding to MAPs are
visualized in Fig. 3. We can clearly find that the ?R? schemes tend to group
the MAPs in local spatial region, while ?NR? schemes can group some
disconnected patches together for co-occurrence consideration. ?S? seems to be
prone to selecting features around eyes or mouth, which are more informative
for characterizing expressions.
1. Download: Download full-size image
Fig. 3. Examples of patches corresponding to grouped features under different
schemes: (a) NS + R, (b) NS + NR, (c) S + R, (d) S + NR.
### 2.3. Group-wise Sub-network Learning (GSL)
After the groups of MAPs in each Receptive Field (RF) are determined, we then
learn for each RF a sub-network taking the MAPs in this RF as the input. In
this module, we apply two addition layers for higher-level features learning
in each sub-network. After training of each layer, the features in all
previous layers can be concatenated to a long vector for the sub-classifier.
Simply, ?GSL 1? denotes the usage of only first layer, i.e. groups of MAPs
learning by receptive field construction module; ?GSL 2? denotes the usage of
first layer concatenated with the first hidden layer; and ?GSL 3? denotes the
usage of all three layers in the sub-network.
For the multi-layer group-wise sub-network learning, we investigate two
mainstream algorithm: Multi-Layer Perceptron (MLP) [15], which is trained via
fully supervised gradient descent, and Deep Belief Networks (DBN) [16], which
involves an unsupervised pre-training step and a supervised fine-tuning step.
#### 2.3.1. Multi-Layer Perceptron (MLP)
A Multi-Layer Perceptron (MLP) is a feedforward artificial neural network
model consists of multiple layers of nodes in a directed graph, with each
layer fully connected to the next one. Except for the input nodes, each nodeis a neuron (or processing element) with a nonlinear activation function. MLP
utilizes a supervised learning technique called back-propagation for training
the network [15], [27]. MLP is a modification of the standard linear
perceptron and can distinguish data that are not linearly separable [28].
Formally, a one-hidden layer MLP constitutes a function _f_ : RD?RL,
(7)f(x)=G(b(2)+W(2)(s(b(1)+W(1)x))),where _D_ is the size of input vector _x_
and _L_ is the size of the output vector _f_(_x_), with bias vectors
b(1),b(2); weight matrices W(1),W(2) and activation functions _G_ and _s_. The
vector h(x)=s(b(1)+W(1)x) is the output of the hidden layer. W(1)?RD×Dh is the
weight matrix connecting the input vector to the hidden layer. Each column
W·i(1) represents the weights from the input units to the _i_ -th hidden unit.
Typical choices for _s_ include _tanh_ or the logistic _sigmoid_ function.
More generally, one can build a deep MLP by stacking more such hidden layers,
each of which may have a different dimension.
To train an MLP, we learn all parameters of the model using Stochastic
Gradient Descent with minibatches. The gradients can be calculated using the
back-propagation algorithm.
#### 2.3.2. Deep Belief Networks (DBN)
Deep Belief Networks (DBN) are graphical models which learn to extract a deep
hierarchical representation of the training data. They model the joint
distribution between observed vector _x_ and the ? hidden layers hk(k=1,2,?,l)
as follows:(8)P(x,h1,?,h?)=(?k=0??2P(hk|hk+1))P(h??1,h?),where x=h0,P(hk?1|hk)
is a conditional distribution for the visible units conditioned on the hidden
units at level _k_ , and P(h??1,h?) is the visible-hidden joint distribution
in the top-level.
It has been shown that Restricted Boltzmann Machines (RBM) can be stacked and
trained in a greedy manner to build a DBN [16], [29]. The single RBM is a two-
layer (i.e. visible and hidden layer) undirected graphic model without lateral
connections. The nodes in visible layer and hidden layer are represented as
_v_ _i_ , _h_ _i_ respectively. If the visible units are real values, the
configurations of _v_ _i_ , _h_ _i_ are characterized by an energy function as
follows:(9)E(v,h)=12?ivi2??i,jviWijhj??jbjhj??icivi,where _W_ _ij_
characterizes the association between visible and hidden nodes and _c_ _i_ ,
_b_ _j_ are the biases of visible layer and hidden layer respectively.
Probabilistically, this is interpreted
as(10)P(v,h)=exp(?E(v,h))Z,Z=?v,hexp(?E(v,h)).The hidden nodes are
conditionally independent given the visible layer nodes, and vice versa. The
parameters of RBM can be optimized by performing stochastic gradient descent
on maximizing the log-likelihood of training data. As computing the exact
gradient of log-likelihood is intractable, Contrastive Divergence (CD)
approximation is used [30] which works fairly well in practice.
As RBM is usually served as an unsupervised ?pre-training? tool, we also
performed supervised ?fine-tuning? after stacked RBMs to refine the
parameters. This procedure is equivalent to initializing the parameters of a
MLP with the weights and hidden layer biases obtained with the stacked RBMs.
## 3\. Databases and evaluation protocols
In this section, we introduce the three databases and the corresponding
evaluation protocols in our experiments. Two of them, CK+ [17] and MMI [18],
are lab-controlled data. Another one, SFEW [19], is wild environment data.
### 3.1. CK+ database
The CK+ database [17] consists of 593 sequences from 123 subjects, which is an
extended version of Cohn?Kanade (CK) [31] database (some examples are shown in
Fig. 4). The validating emotion labels are only assigned to 327 sequences
which were found to meet criteria for one of the 7 discrete emotion (anger,contempt, disgust, fear, happiness, sadness, and surprise) based on FACS. In
our experiments, to compare with other methods [3], [2] those focus on 6 basic
expression classes, we make use of 309 of these sequences (remove 18
?contempt? sequences out). For each sequence, the first image (neutral face)
and three peak frames are used for prototypic expression recognition which are
the same as the settings in [2], [3]. Based on the subject ID given in the
dataset, we construct 10 person-independent subsets by sampling in ID
ascending order with step size equals 10 and adopt 10-fold cross-validation as
in [3]. What is more, to avoid parameter sensitivity, only linear SVM
classifier [32] is used in all of our experiments.
1. Download: Download full-size image
Fig. 4. The sample facial expression images from CK+ database.
### 3.2. MMI database
The MMI database [18] includes 30 subjects of both sexes and ages from 19 to
62. In the datasets, 213 sequences have been labeled with six basic
expressions, in which 205 sequences are captured in frontal view. We use the
data from all these 205 sequences as in [3]. Same as the settings on CK+, the
neutral face and three peak frames in each sequence have been used and 10-fold
cross-validation is conducted in the same way (construct 10 person-independent
subsets by sampling in ID ascending order with step size equals 10). Compared
to CK+, MMI have more challenging conditions: The subjects posed expressions
non-uniformly, and many of them wear accessories (e.g. glasses, moustache).
### 3.3. SFEW database
For further validation, we also evaluate our method in a much more difficult
scenario: facial expressions in the wild. The Static Facial Expression in the
Wild (SFEW) database [19], which has been extracted from movies (see examples
in Fig. 5), is different from the available facial expression datasets
generated in highly controlled lab environments. It is the first database that
depicts real-world or simulated real-world conditions for expression
recognition. The database is divided into two sets. Each set contains seven
subfolders corresponding to seven expression categories (anger, disgust, fear,
neutral, happy, sad, and surprise). The sets were created in strict person
independent manner that there is no overlap between training and testing set.
In total, there are 700 images (346 in Set1, 354 in Set2) and 95 subjects.
According to Strictly Person Independent (SPI) Protocol for SFEW, two-fold
experiments need to be conducted, i.e. train on Set1/Set2 and test on
Set2/Set1. The average accuracy of two-fold experiments is used as final
performance measurement.
1. Download: Download full-size image
Fig. 5. The sample facial expression images from SFEW database.
## 4\. Experiments
In this section, we provide a comprehensive evaluation of the building modules
and the whole framework of our method. All comparisons are performed on the
data introduced in Section 3. We discuss all the important parameters and
alternative network structures in our experiments. Before feeding to our AUDN,
all the faces are detected automatically by Viola?Jones face detector [33] and
then normalized to 32×32 based on the locations of eyes detected by method in
[34].
### 4.1. Evaluation on MAP representation
In MAP representation learning module, there are two important parameters,
i.e. the size of densely sampled patches _u_ , and the number of MAP
prototypes (filters) _K_. We evaluate the effect of two parameters by ranging
u=3,6,9 and K=21,22,?,28. Here max-pooling is applied over 3-by-3 regions on
each convolutional feature map for the final MAP representation. Using alinear SVM classifier, we can obtain the recognition results as shown in Fig.
6.
1. Download: Download full-size image
Fig. 6. Comparison of different MAP parameters. (a) CK+ database. (b) MMI
database.
As shown, for all different sizes of patches, the recognition accuracy is
improved as the number of MAP prototypes _K_ increases. The ?6×6 patches?
worked best when there are enough MAP prototypes (e.g. K>100). Considering the
computational time and space, we choose _u_ =6 and _K_ =100 in our method.
Accordingly, the dimension of final MAP features is 9×9×100=8100.
### 4.2. Evaluations on receptive field construction
Recalling Algorithm 1, two parameters are very important for the receptive
field construction: i.e. the number of MAPs in each receptive field (i.e. the
size of receptive field) _m_ , and the number of receptive field _N_. We
comprehensively evaluate the effect of the two parameters by ranging
m=100,200,?,900 and N=1,2,?,9. To find the best way for constructing receptive
fields, we also evaluate the four criteria for feature grouping as described
in Section 2.2: NS + R, NS + NR, S + R, and S + NR. Under different parameters
in Algorithm 1, the results are demonstrated in Fig. 7 for CK+ and Fig. 8 for
MMI respectively.
1. Download: Download full-size image
Fig. 7. Performance comparison on CK+ with different receptive field
construction schemes: (a) No Supervision + Redundancy (NS + R). (b) No
Supervision + No Redundancy (NS + NR). (c) Supervision + Redundancy (S + R).
(d) Supervision + No Redundancy (S + NR).
1. Download: Download full-size image
Fig. 8. Performance comparison on MMI with different receptive field
construction schemes: (a) No Supervision + Redundancy (NS + R). (b) No
Supervision + No Redundancy (NS + NR). (c) Supervision + Redundancy (S + R).
(d) Supervision + No Redundancy (S + NR).
As can be seen from the figures, we can get consistent observation on both of
the databases: the sorting of the methods in terms of mean accuracy can be
given as NS+R<NS+NR, S+R<S+NR. And the sorting of the methods in terms of
sensitivity to parameters can be given as NS+R>NS+NR>S+R>S+NR. The
observations imply that (i) redundancy controlling is good for constructing a
batch of descriptive receptive fields, and further improves the fusion
performance. (ii) With supervision, the algorithm can achieve comparable
accuracy even using low dimensional features.
We also conduct comparisons with two general schemes, ?random? (which means
constructing each receptive field by randomly selecting MAP features) and
?spatial? (which means constructing receptive fields according to the spatial
locations of features). The same parameters are evaluated for ?random? and
only _N_ =9 is performed for ?spatial?. To illustrate the overall trends of
each scheme, we average the results obtained under different numbers of
receptive fields. Including ?random? and ?spatial? schemes, we show the
comparisons in Fig. 9. We can see that ?spatial? scheme cannot provide good
performance due to the large redundancy within each local receptive field.
?random? scheme gets the middle place for its ability to reduce redundancy by
scattering the features.
1. Download: Download full-size image
Fig. 9. Performance comparison of different strategies for receptive field
construction. (a) CK+ database. (b) MMI database.
### 4.3. Evaluations on group-wise sub-network learning
In each sub-network, The number of units in visible layer equals to thefeature dimensions in receptive field, i.e. m=100,200,?,900. The number of
units in hidden layers is 100 and 50 respectively. Two kinds of multi-layer
feature learning methods, i.e. Multi-Layer Perceptron (MLP) and Deep Belief
Networks (DBN), are evaluated in this paper. After multi-layer learning, for
each receptive field, the features in all the layers are concatenated to
construct final hierarchical features for sub-classifier. In the end, the sub-
classifiers from each receptive field are averaged for fusion result as
demonstrated in Fig. 10.
1. Download: Download full-size image
Fig. 10. Performance comparison of different group-wise sub-networks learning
schemes. (a) CK+ database. (b) MMI database.
As redundancy controlling is proved to be effective, in this module, we
conduct our experiment only based on NS+NR and S+NR. We demonstrate the
overall trends of NS+NR and S+NR before and after group-wise sub-network
learning in Fig. 11. As shown, DBN achieves better performance compared to MLP
due to unsupervised pre-training, which initializes the model to a point in
parameter space that renders the optimization process more effective, in the
sense of achieving a lower minimum of the empirical cost function.
1. Download: Download full-size image
Fig. 11. AU detection performance comparison on CK+ database. (a) Different
strategies for receptive field construction. (b) Different group-wise sub-
networks learning schemes.
### 4.4. Overall results
Under the optimal combinations of the three modules, the best performance we
achieve are listed in Table 2 (where the ?GSL x? represents different
evaluation manners in Group-wise Sub-network Learning, as mentioned in Section
2.3). We compared our ?learned feature? with the hand-crafted features as
listed in Table 2. All the experimental results are based on the features
achieved by running the descriptor code released by the authors. The
experiment settings are Image size is 32×32 pixels as the same as before. LBP
(944 dimensions): 16 patches with the size of 8×8 pixels and 59 dimensions
uniformed feature on each patch; SIFT (1152 dimensions): 9 lattice points with
the step of 8 and 128 dimensions feature for each point; HOG (1568
dimensions): 7×7 overlapped blocks with the size of 8×8 pixels, 2×2 cells and
8 histogram bins for each block; Gabor (10 240 dimensions): convolutional
images with 5 spatial scales and 8 orientations with 2×2 downsampling. We
performed both linear SVM and RBF SVM on all these hand-crafted features for
the best performance. The parameters of RBF are tuned empirically by grid
searching over c=2k,k=0,1,?,9;g=2l,l=?5,?4,?,0.
Table 2. Expression recognition performance on CK+, MMI, and SFEW databases
(compared with hand-crafted features).
Methods| Accuracy (%)
---|---
Empty Cell| CK+| MMI| SFEW
LBP| _83.87_ 81.89(RBF)| _52.93_ 50.37(RBF)| 21.29 _23.71_ (RBF)
SIFT| 86.39 _87.31_ (RBF)| 57.80 _61.46_ (RBF)| 20.45 _21.14_ (RBF)
HOG| _89.53_ 88.61(RBF)| 63.17 _65.24_ (RBF)| 19.52 _22.71_ (RBF)
Gabor| _88.61_ 85.09(RBF)| 56.10 _57.56_ (RBF)| _19.29_ 19.14(RBF)
| | |
**AUDN**| | |
**MAP**| **92.19**| **68.66**| **23.14**
**GSL 1**| **93.06**| **71.59**| **26.43**
**GSL 2**| _**93.70**_| **73.05**| **28.86**
**GSL 3**| **93.62**| _**75.85**_| _**30.14**_The bold results are obtained using our method. The underlined results are the
better ones in their table cells or columns.
Table 3. Expression recognition performance comparison with state-of-the-art
methods.
Methods| Shan09 [2]| Zhong12 [3]| Dhall11 [19]| Liu12 [35]| Liu13 [14]|
**AUDN**
---|---|---|---|---|---|---
CK+| 86.94| 89.89| ?| ?| 92.22| **93.70**
MMI| 47.74| 73.53| ?| ?| 74.76| **75.85**
SFEW| ?| ?| 19.00| 29.30| 26.14| **30.14**
We also conduct comparisons with the state-of-the-art methods (see Table 3).
For CK+ and MMI databases, we cite two results AFL [2] and CSPL [3] which were
reported in [3]. These two results are achieved under the same 10-fold person-
independent protocols as ours. However, our experiments are performed on seven
expression categories including neutral, which is more challenging than the
six categories problem in [3]. For SFEW database, we cite the baseline results
reported in [19] and the best known result in [35].
Table 4. Cross-database performance on CK+, MMI, and SFEW databases.
Train/Test| CK+/MMI| CK+/SFEW| MMI/CK+| MMI/SFEW
---|---|---|---|---
Accuracy (%)| 72.20| 29.43| 93.46| 25.00
Compared to the results on CK+ and MMI, the recognition performance degrade
significantly on SFEW due to its challenging image conditions, e.g. large
variations of pose and illumination. The faces normalized by automatically
detected eyes location suffer from severe misalignment, so that none of the
algorithms can work well as on CK+ and MMI. However, it shows that gradually
better results have been achieved during our learning process, and finally
significant improvements are achieved on all the three databases.
### 4.5. Cross-database evaluation
As a learning-based methods, there is pervasive worry about its generalization
ability. To evaluate this point, we also perform cross-database experiments,
that is, training feature models on one database and testing on the other two
data. The results are reported in Table 4, which shows that our method can
also achieve very promising results even under cross-database testing.
Specifically, the models trained on lab data CK+ can get similar performance
on the wild data SFEW compared with training on itself. These results
convincingly prove the strength of the proposed method.
### 4.6. Extended discussion on AU detection
AUDN is an AU-inspired framework for expression recognition without the
expensive computation of AU, however in this section, we will show that our
method can simultaneously work well on AU detection. The evaluation is
performed on CK+ database, which full FACS coding of peak frames is provided
for all 593 sequences. Following [17], we use all neutral and peak frames for
training and linear one-vs-all two-class SVM for each AU detector. The Area
Underneath the Receiver-Operator Characteristic (ROC) Curve (AUC) is served as
the evaluation measurement. In Fig. 11, we report the average AUC results of
all AUs, which is the same as in [17]. And the performance is also compared
with the hand-crafted features in Table 5 and state-of-the-art method in Table
6. From these results, we can see that the proposed method is also valid for
action unit detection.
Table 5. AU detection performance on CK+ database (compared with hand-crafted
features).
Method| LBP| SIFT| HOG| Gabor| **AUDN-MAP**| **AUDN-GSL**
---|---|---|---|---|---|---AUC (%)| 92.67| 93.86| 94.66| 92.34| **95.27**| _**95.78**_
The bold results are obtained using our method. The underlined results are the
better ones in their table cells or columns.
Table 6. AU detection performance comparison with state-of-the-art methods.
Method| SPTS [17]| CAPP [17]| SPTS+CAPP [17]| **AUDN-MAP**| **AUDN-GSL**
---|---|---|---|---|---
AUC (%)| 90.0| 91.4| 94.5| **95.27**| _**95.78**_
The bold results are obtained using our method. The underlined results are the
better ones in their table cells or columns.
## 5\. Conclusion
In this paper, we propose to construct a deep architecture to learn features
especially for facial expression recognition, which is called ?AUDN?. Inspired
by the interpretation of AU, we propose a computational representation MAP to
capture the local appearance variations caused by facial expression, and
construct adaptive receptive fields to simulate the grouping of different
MAPs. Additional multi-layer receptive field specific sub-network learning
process can further generate high-level features which benefit expression
recognition specifically. The proposed AUDN achieves the best published
performance on three facial expression databases including both lab control
and wild scenarios.
In the future, we will try to explore more descriptive mid-level
representation rather than simple non-overlapping receptive fields and extend
this method to other facial image analysis tasks related to local facial
actions, such as expression synthesis, facial animation, and facial feature
tracking.
## Acknowledgment
This work is partially supported by National Basic Research Program of China
973 Program under Contract no. 2015CB351802, and Natural Science Foundation of
China under Contract nos. 61222211, 61272319, and 61390510.
Recommended articles"
34,36,Automatic coding of facial expressions displayed during posed and genuine pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2009,292,Toronto Face Database,machine learning,"a machine learning approach in a two-stage system. In the first stage, a set of 20 detectors  for facial actions from the Facial Action  Toronto. Mean accuracy of naïve human subjects for",No DOI,Image and Vision Computing,https://www.sciencedirect.com/science/article/pii/S0262885609000055,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Automatic coding of facial expressions displayed during
posed and genuine pain
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Human subject methods
6. 3\. Automated facial action detection
7. 4\. Automated measurement of real and fake pain expressions
8. 5\. Discussion
9. Acknowledgments
10. References
Show full outline
## Cited by (172)
## Figures (4)
1. 2. 3. 4.
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Image and Vision Computing
Volume 27, Issue 12, November 2009, Pages 1797-1803
# Automatic coding of facial expressions displayed during posed and genuine
pain
Author links open overlay panelGwen C. Littlewort a, Marian Stewart Bartlett
a, Kang Lee b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.imavis.2008.12.010Get rights and content
## Abstract
We present initial results from the application of an automated facial
expression recognition system to spontaneous facial expressions of pain. In
this study, 26 participants were videotaped under three experimental
conditions: baseline, posed pain, and real pain. The real pain condition
consisted of cold pressor pain induced by submerging the arm in ice water. Our
goal was to (1) assess whether the automated measurements were consistent with
expression measurements obtained by human experts, and (2) develop aclassifier to automatically differentiate real from faked pain in a subject-
independent manner from the automated measurements. We employed a machine
learning approach in a two-stage system. In the first stage, a set of 20
detectors for facial actions from the Facial Action Coding System operated on
the continuous video stream. These data were then passed to a second machine
learning stage, in which a classifier was trained to detect the difference
between expressions of real pain and fake pain. Naïve human subjects tested on
the same videos were at chance for differentiating faked from real pain,
obtaining only 49% accuracy. The automated system was successfully able to
differentiate faked from real pain. In an analysis of 26 subjects with faked
pain before real pain, the system obtained 88% correct for subject independent
discrimination of real versus fake pain on a 2-alternative forced choice.
Moreover, the most discriminative facial actions in the automated system were
consistent with findings using human expert FACS codes.
* Previous article in issue
* Next article in issue
## Keywords
Machine learning
Computer vision
Malingering
Facial expression
Spontaneous behavior
Automated FACS
## 1\. Introduction
The computer vision field has advanced to the point that we are now able to
begin to apply automatic facial expression recognition systems to important
research questions in behavioral science. This paper is among the first
applications of fully automated facial expression measurement to such research
questions. It explores the application of a machine learning system for
automatic facial expression measurement to the task of differentiating fake
from real expressions of pain. This application involves measurement of
spontaneous expressions, in which the data conditions are much less
constrained than posed expressions of basic emotions, on which most automated
systems are developed and tested. Ability to process spontaneous expressions
shows that automated expression measurement systems can perform effectively in
real applications.
The ability to distinguish real pain from faked pain, (malingering) is an
important issue in medicine [10]. Naïve human subjects are near chance for
differentiating real from fake pain from observing facial expression (e.g.
[13]). In the absence of direct training in facial expressions, clinicians are
also poor at assessing pain from the face (e.g. [24], [25], [12]). However, a
number of studies using the Facial Action Coding System (FACS) [6] have shown
that information exists in the face for differentiating real from posed pain
(e.g. [14], [3], [22]). In fact, if subjects receive corrective feedback,
their performance improves substantially [15]. Thus it appears that a signal
is present, but that most people do not know what to look for.
Recent advances in automated facial expression measurement open up the
possibility of automatically differentiating posed from real pain using
computer vision systems (e.g. [1], [18], [2], [20]). This paper explores the
application of a system for automatically detecting facial actions to this
problem. The goal of the paper is to (1) assess whether the automated
measurements were consistent with expression measurements obtained by human
experts, and (2) develop a classifier to automatically differentiate real from
faked pain in a subject-independent manner from the automated measurements.In this study, 26 participants were videotaped under three experimental
conditions: baseline, posed pain, and real pain. The real pain condition
consisted of cold pressor pain induced by submerging the arm in ice water. We
employed a machine learning approach in a two-stage system. In the first
stage, the video was passed through a system for detecting facial actions from
the Facial Action Coding System [1]. These data were then passed to a second
machine learning stage, in which a classifier was trained to detect the
difference between expressions of real pain and fake pain. Naïve human
subjects were tested on the same videos to compare their ability to
differentiate faked from real pain.
Section 2 describes the human subject methods and experiments. Section 3
describes the computer vision system employed for the first stage of the
2-stage approach, in which automated detectors were developed for 20 facial
actions. Section 4.1 analyzes the output of the automated facial action
detectors and assesses the degree to which they match previous studies based
on human coding. Section 4.2 describes the second machine learning stage, in
which a classifier for real versus faked pain is trained on the output of the
20 facial action detectors.
The ultimate goal of this work is not the detection of malingering per se, but
rather to demonstrate the ability of the automated systems to detect facial
behavior that the untrained eye might fail to interpret, and to differentiate
types of neural control of the face. It holds out the prospect of illuminating
basic questions pertaining to the behavioral fingerprint of neural control
systems, and thus opens many future lines of inquiry.
The contribution of this paper is to show that this system for automated FACS
can drive further analysis of behavior by the addition of extra computational
layers. We show that the automated system is consistent with earlier research,
and brings added value by providing measurement of the face when expert human
coding is impractical and enabling measurement of expression dynamics than was
infeasible with human coding due to the time required. We hope that the
availability of such tools will lead to new experimental designs.
### 1.1. The facial action coding system
FACS [6] is arguably the most widely used method for coding facial expressions
in the behavioral sciences. The system describes facial expressions in terms
of 46 component movements, which roughly correspond to the individual facial
muscle movements. An example is shown in Fig. 1. FACS provides an objective
and comprehensive way to analyze expressions into elementary components,
analogous to decomposition of speech into phonemes. Because it is
comprehensive, FACS has proven useful for discovering facial movements that
are indicative of cognitive and affective states. See [8] for a review of
facial expression studies using FACS. The primary limitation to the widespread
use of FACS is the time required to code. FACS was developed for coding by
hand, using human experts. It takes over a hundred hours of training to become
proficient in FACS, and it takes about two hours for human experts to code
each minute of video. The authors have been developing methods for fully
automating the facial action coding system (e.g. [5], [1]). In this paper, we
apply a computer vision system trained to automatically detect FACS to the
problem of differentiating posed from real expressions of pain.
1. Download: Download high-res image (84KB)
2. Download: Download full-size image
Fig. 1. Example facial action decomposition from the facial action coding
system. A prototypical expression of fear is decomposed into seven component
movements. Letters indicate intensity. A fear brow (1 + 2 + 4) is illustrated
here.In previous studies using manual FACS coding by human experts, at least 12
facial actions showed significant relationships with pain across multiple
studies and pain modalities. Of these, the ones specifically associated with
cold pressor pain were 4, 6, 7, 9, 10, 12, 25, 26 [4], [22]. See Table 1 and
Fig. 2 for names and examples of these AU?s. A previous study compared faked
to real pain, but in a different pain modality (lower back pain). This study
found that when faking, subjects tended to display the following AU?s: 4, 6,
7, 10, 12, 25. When faked pain expressions were compared to real pain
expressions, the faked pain expressions contained significantly more brow
lower (AU 4), cheek raise (AU 6), and lip corner pull (AU 12) [3]. These
studies also reported substantial individual differences in the expressions of
both real pain and faked pain, making automated detection of faked pain a
challenging problem.
Table 1. AU detection performance on posed and spontaneous facial actions.
Values are area under the roc (A?) for generalization to novel subjects.
AU| Name| Posed| Spont
---|---|---|---
1| Inner brow raise| .90| .88
2| Outer brow raise| .94| .81
4| Brow lower| .98| .73
5| Upper lid raise| .98| .80
6| Cheek raise| .85| .89
7| Lids tight| .96| .77
9| Nose wrinkle| .99| .88
10| Upper lip raise| .98| .78
12| Lip corner pull| .97| .92
14| Dimpler| .90| .77
15| Lip corner depress| .80| .83
17| Chin raise| .92| .80
18| Lip pucker| .87| .70
20| Lip stretch| .98| .60
23| Lip tighten| .89| .63
24| Lip press| .84| .80
25| Lips part| .98| .71
26| Jaw drop| .98| .71
1, 1 + 4| Distress brow| .94| .70
1 + 2 + 4| Fear brow| .95| .63
Mean| | .93| .77
1. Download: Download high-res image (40KB)
2. Download: Download full-size image
Fig. 2. Sample behavior: (a) faked pain and (b) real pain.
### 1.2. Spontaneous expressions
The machine learning system presented here was trained on spontaneous facial
expressions. The importance of using spontaneous behavior for developing and
testing computer vision systems becomes apparent when we examine the
neurological substrate for facial expression. There are two distinct neural
pathways that mediate facial expressions, each one originating in a different
area of the brain. Volitional facial movements originate in the cortical motor
strip, whereas it has been suggested that spontaneous facial expressions
originate in the subcortical areas of the brain (see [26], for a review).
These two pathways have different patterns of innervation on the face, with
the cortical system tending to give stronger innervation to certain muscles
primarily in the lower face, while the subcortical system tends to more
strongly innervate certain muscles primarily in the upper face (e.g. [19]).The facial expressions mediated by these two pathways have differences both in
which facial muscles are moved and in their dynamics [7], [8]. Subcortically
initiated facial expressions (the spontaneous group) are characterized by
synchronized, smooth, symmetrical, consistent, and reflex-like facial muscle
movements whereas cortically initiated facial expressions (posed expressions)
are subject to volitional real-time control and tend to be less smooth, with
more variable dynamics [26], [11], [27], [2]. Given the two different neural
pathways for facial expressions, it is reasonable to expect to find
differences between genuine and posed expressions of pain. Moreover, it is
crucial that the computer vision model for detecting genuine pain is based on
machine learning of spontaneous examples of real pain expressions.
## 2\. Human subject methods
Video data were collected of 26 human subjects during real pain, faked pain,
and baseline conditions. Human subjects were university students consisting of
6 men and 20 women. The pain condition consisted of cold pressor pain induced
by immersing the arm in cold water at 3 °C. For the baseline and faked pain
conditions, the water was 20 °C. Subjects were instructed to immerse their
forearm into the water up to the elbow, and hold it there for 60 s in each of
the three conditions. The order of the conditions was baseline, faked pain,
and then real pain. For the faked pain condition, subjects were asked to
manipulate their facial expressions so that an ?expert would be convinced they
were in actual pain.? Participants facial expressions were recorded using a
digital video camera during each condition.
A second subject group underwent the conditions in the counterbalanced order,
with real pain followed by faked pain. This ordering involves immediate memory
of the pain just felt, which is a fundamentally different task from imagining
unknown pain. The present paper therefore analyzes only the first subject
group. The second group will be analyzed separately in a future paper, and
compared to the first group.
After the videos were collected, a set of 170 naïve observers were shown the
videos and asked to guess whether each video contained faked or real pain.
Subjects were undergraduates with no explicit training in facial expression
measurement. They were primarily Psychology majors at U. Toronto. Mean
accuracy of naïve human subjects for discriminating fake from real pain in
these videos was near chance at 49.1% (SD = 13.7). These observers had no
specific training in facial expression and were not clinicians. One might
suppose that clinicians would be more accurate. However, previous studies
suggest that clinicians judgments of pain from the face are similarly
unreliable (e.g. [25], [12]). Facial signals to differentiate real from faked
pain do appear to exist however [15], [3], [22], [23]. A system based on
machine learning from examples of real and faked pain expressions could pick
up these signals. This paper investigated whether an automated system could
outperform the naïve human subjects on the same set of videos.
## 3\. Automated facial action detection
The first stage of the computer vision analysis was to employ a system for
fully automated facial action coding developed previously by the authors [1],
[18]. In this paper, we evaluate this system on the task of measuring facial
expressions of pain, and then extend it by developing a second stage
classifier for real versus faked pain that operates on the facial action
output channels. Here we describe the facial action detection system employed
in Stage One. It is a user independent fully automatic system for real time
recognition of facial actions from the FACS. The system automatically detects
frontal faces in the video stream and codes each frame with respect to 20
action units. It is a machine learning system on image-based features. Inprevious work, we conducted empirical comparisons of image features, including
Gabors, independent components, and flow-based features (e.g. [5]),
classifiers such as AdaBoost, support vector machines, and linear discriminant
analysis, as well as feature selection techniques [18]. An overview of the
system is shown in Fig. 3.
1. Download: Download high-res image (89KB)
2. Download: Download full-size image
Fig. 3. Overview of the automated facial action recognition system.
### 3.1. Real time face and feature detection
The system employs a real-time face detection system that uses boosting
techniques in a generative framework [9] and extends work by Viola and Jones
[29]. Enhancements to Viola and Jones include employing Gentleboost instead of
AdaBoost, smart feature search, and a novel cascade training procedure,
combined in a generative framework. The source code for the face detector is
freely available at http://kolmogorov.sourceforge.net. Accuracy on the CMU-MIT
dataset, a standard public data set for benchmarking frontal face detection
systems [28], is 90% detections and 1/million false alarms, which is state-of-
the-art accuracy. The CMU test set has unconstrained lighting and background.
With controlled lighting and background, such as the facial expression data
employed here, detection accuracy is much higher. All faces in the training
datasets, for example, were successfully detected. The system presently
operates at 24 frames/s on a 3 GHz Pentium IV for 320 × 240 images. The
automatically located faces were rescaled to 96 × 96 pixels. The typical
distance between the centers of the eyes was roughly 48 pixels. Previously,
automatic eye detection [9] was employed to align the eyes in each image. The
current system aligns the face based on four points ? the centers of the eyes,
nose and mouth. For this study only, we applied temporal smoothing to these
feature positions and linear compensation for motion-related alignment
anomalies. There is a choice of Procrustes alignment or unconstrained least
square alignment. We used procrustes for this experiment. The images were then
passed through a bank of Gabor filters eight orientations and 9 spatial
frequencies (2?32 pixels per cycle at 1/2 octave steps). Output magnitudes
were then passed to the action unit classifiers.
### 3.2. Facial action classifiers
The training data for the facial action classifiers came from four data sets,
not including the pain data ? three posed datasets and one dataset of
spontaneous expressions. The facial expressions in each dataset were FACS
coded by certified FACS coders. The first posed datasets was the Cohn?Kanade
DFAT-504 dataset [16]. This dataset consists of 100 university students who
were instructed by an experimenter to perform a series of 23 facial displays,
including expressions of seven basic emotions. The second posed dataset
consisted of directed facial actions from 24 subjects collected by Ekman and
Hager. Subjects were instructed by a FACS expert on the display of individual
facial actions and action combinations, and they practiced with a mirror. The
resulting video was verified for AU content by two certified FACS coders. The
third posed dataset consisted of a subset of 50 videos from 20 subjects from
the MMI database [21]. The spontaneous expression dataset consisted of the
FACS-101 dataset collected by Mark Frank [1]. Thirty-three subjects underwent
an interview about political opinions on which they felt strongly. Two minutes
of each subject were FACS coded. The total training set consisted of 5500
examples, 2500 from posed databases and 3000 from the spontaneous set.
Linear Support Vector Machines were trained for each of 20 facial actions.
Separate binary classifiers, one for each action, were trained to detect the
presence of the action in a ?one versus all? manner. Positive examplesconsisted of the apex frame for the target AU. Negative examples consisted of
all apex frames that did not contain the target AU plus neutral images
obtained from the first frame of each sequence. Eighteen of the detectors were
for individual action units, and two of the detectors were for specific brow
region combinations: fear brow (1 + 2 + 4) and distress brow (1 alone or 1 +
4). All other detectors were trained to detect the presence of the target
action regardless of co-occurring actions. A list is shown in Table 1.
The output of the system was a real valued number indicating the distance to
the separating hyperplane for each classifier. Previous work showed that the
distance to the separating hyperplane (the margin) contained information about
action unit intensity (e.g. [1]).
In this paper, area under the ROC (A?) is used to assess performance rather
than overall percent correct, since percent correct can be an unreliable
measure of performance, as it depends on the proportion of targets to non-
targets, and also on the decision threshold. Similarly, other statistics such
as true positive and false positive rates depend on decision threshold, which
can complicate comparisons across systems. A? is a measure derived from signal
detection theory and characterizes the discriminative capacity of the signal,
independent of decision threshold. The ROC curve is obtained by plotting true
positives against false positives as the decision threshold shifts from 0% to
100% detections. The area under the ROC (A?) ranges from 0.5 (chance) to 1
(perfect discrimination). A? can also be interpreted in terms of percent
correct. A? is equivalent to the theoretical maximum percent correct
achievable with the information provided by the system when using a
2-Alternative Forced Choice testing paradigm.
Table 1 shows performance for detecting facial actions in posed and
spontaneous facial actions. Generalization to novel subjects was tested using
threefold cross-validation on the images in the FACS training set (not the
pain video). Performance was separated into the posed set, which was 2500
images, and a spontaneous set, which was 1100 images from the FACS-101
database which includes speech. The performance on the spontaneous data is
lower than posed data because of less restrictive conditions on head position,
lighting, movement and the lower intensity of spontaneous expressions. Despite
this performance loss, particularly in the very important brow lowering AU,
task labels based on one minute worth of this system output could be learned
successfully. In addition to FACS accuracy, the decision is implicitly
influenced by combinations and temporal effects.
## 4\. Automated measurement of real and fake pain expressions
Applying this system to the pain video data produced a 20 channel output
stream, consisting of one real value for each learned AU, for each frame of
the video. We first examine the output of the automated facial action
detectors. We assess which AU outputs contained information about genuine pain
expressions, faked pain expressions, and show differences between genuine
versus faked pain. The results are compared to studies that employed expert
human coding. Section 4.2 goes on to develop a second-stage classifier to
automatically discriminate real from faked pain from the 20-channel output
stream.
### 4.1. Characterizing the difference between real and fake pain expressions
We first examined which facial action detectors were elevated in real pain
compared to the baseline condition. _Z_ -scores for each subject and each AU
detector were computed as _Z_ = (_x_ ? _?_)/_?_ , where (_?_ , _?_) are the
mean and variance for the output of frames 100?1100 in the baseline condition
(warm water, no faked expressions). The mean difference in _Z_ -score between
the baseline and pain conditions was computed across the 26 subjects. Table 2shows the action detectors with the largest difference in _Z_ -scores. We
observed that the actions with the largest _Z_ -scores for genuine pain were
Mouth opening and jaw drop (25 and 26), lip corner puller by zygomatic (12),
nose wrinkle (9), and to a lesser extent, lip raise (10) and cheek raise (6).
These facial actions have been previously associated with cold pressor pain
(e.g. [22], [4]).
Table 2. _Z_ -score differences of the three pain conditions, averaged across
subjects. FB, fear brow 1 + 2 + 4; DB, distress brow (1, 1 + 4).
Action unit| 25| 12| 9| 26| 10| 6
---|---|---|---|---|---|---
_(A) Real pain vs baseline_
_Z_ -score| 1.4| 1.4| 1.3| 1.2| 0.9| 0.9
Action unit| 4| DB| 1| 25| 12| 6| 26| 10| FB| 9| 20| 7
---|---|---|---|---|---|---|---|---|---|---|---|---
_(B) Faked pain vs baseline_
_Z_ -score| | 2.7| 2.1| 1.7| 1.5| 1.4| 1.4| 1.3| 1.3| 1.2| 1.1| 1.0
0.9| | | | | | | | | | | |
Action unit| 4| DB| 1
---|---|---|---
_(C) Real pain vs faked pain_
_Z_ -score difference| 1.8| 1.7| 1.0
The _Z_ -score analysis was next repeated for faked versus baseline. We
observed that in faked pain there was relatively more facial activity than in
real pain. The facial action outputs with the highest _Z_ -scores for faked
pain relative to baseline were brow lower (4), distress brow (1 or 1 + 4),
inner brow raise (1), mouth open and jaw drop (25 and 26), cheek raise (6),
lip raise (10), fear brow (1 + 2 + 4), nose wrinkle (9), mouth stretch (20),
and lower lid raise (7).
Differences between real and faked pain were examined by computing the
difference of the two _Z_ -scores. Differences were observed primarily in the
outputs of action unit 4 (brow lower), as well as distress brow (1 or 1 + 4)
and inner brow raise (1 in any combination).
Individual subject differences between faked and real pain are shown in Table
3. Difference-of-_Z_ -scores between the genuine and faked pain conditions
were computed for each subject and each AU. There was considerable variation
among subjects in the difference between their faked and real pain
expressions. However, the most consistent finding is that 9 of the 26 subjects
showed significantly more brow lowering activity (AU 4) during the faked pain
condition, whereas none of the subjects showed significantly more AU 4
activity during the real pain condition. Also seven subjects showed more cheek
raise (AU 6), and six subjects showed more inner brow raise (AU 1), and the
fear brow combination (1 + 2 + 4). The next most common differences were to
show more 12, 15, 17, and distress brow (1 alone or 1 + 4) during faked pain.
Table 3. Individual subject differences between faked and genuine pain.
Differences greater than 2 standard deviations are shown. _F_ > _P_ : number
of subjects in which the output for the given AU was greater in faked than
genuine pain. _P_ > _F_ : number of subjects for which the output was greater
in genuine than faked pain. FB: fear brow 1 + 2 + 4. DB: distress brow (1, 1 +
4).
AU| 1| 2| 4| 5| 6| 7| 9| 10| 12| 14| 15| 17| 18| 20| 23| 24| 25| 26| FB| DB
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---
_F_ > _P_| 6| 4| 9| 1| 7| 4| 3| 6| 5| 3| 5| 5| 1| 4| 3| 4| 4| 4| 6| 5
_P_ > _F_| 3| 3| 0| 0| 4| 0| 4| 4| 4| 2| 3| 1| 3| 1| 1| 1| 2| 4| 2| 0
Paired _t_ -tests were conducted for each AU to assess whether it was areliable indicator of genuine versus faked pain in a within-subjects design.
Of the 20 actions tested, the difference was statistically significant for
three actions. It was highly significant for AU 4 (_p_ < .001), and marginally
significant for AU 7 and distress brow (_p_ < .05).
In order to characterize action unit combinations that relate to the
difference between fake and real pain expressions, principal component
analysis was conducted on the difference-of-_Z_ -scores. The first eigenvector
had the largest loading on distress brow and inner brow raise (AU 1). The
second eigenvector had the largest loading on lip corner puller (12) and cheek
raise (6) and was _lower_ for fake pain expressions. The third eigenvector had
the largest loading on brow lower (AU 4). Thus when analyzed singly, the
action unit channel with the most information for discriminating fake from
real pain was brow lower (AU 4). However, when correlations were assessed
through PCA, the largest variance was attributed to two combinations, and AU 4
accounted for the third most variance.
#### 4.1.1. Comparison with previous studies that employed human expert coding
Overall, the outputs of the automated system showed similar patterns to
previous studies of real and faked pain using manual FACS coding by human
experts. Exaggerated activity of the brow lower (AU 4) during faked pain is
consistent with previous studies in which the real pain condition was
exacerbated lower back pain [3], [14]. Another study performed a FACS analysis
of fake and real pain expressions with cold pressor pain, but with children
ages 8?12 [17]. This study observed significant elevation in the following
AU?s for fake pain relative to baseline: 1 4 6 7 10 12 20 23 25 26. This
closely matches the AU?s with the highest _Z_ -scores in the automated system
output of the present study (Table 2B). LaRochette et al. did not measure AU 9
or the brow combinations. When faked pain expressions were compared with real
cold pressor pain in children, LaRochette et al found significant differences
in AU?s 1 4 7 10. Again the findings of the present study using the automated
system are similar, as the AU channels with the highest _Z_ -scores were 1, 4,
and 1 + 4 (Table 2C), and the _t_ -tests were significant for 4, 1 + 4 and 7.
#### 4.1.2. Comparison with human expert coding of a subset of the video data
In order to further assess the validity of the automated system findings, we
obtained FACS codes for a portion of the video data employed in this study.
FACS codes were obtained by an expert coder certified in the Facial Action
Coding System. For each subject, the last 500 frames of the fake pain and real
pain conditions were FACS coded (about fifteen seconds each). It took 60 man
hours to collect the human codes, over the course of more than 3 months, since
human coders can only code up to two hours per day before having negative
repercussions in accuracy and coder burn-out.
The sum of the frames containing each action unit were collected for each
subject condition, as well as a weighted sum, multiplied by the intensity of
the action on a 1?5 scale. To investigate whether any action units
successfully differentiated real from faked pain, paired _t_ -tests were
computed on each individual action unit. (Tests on specific brow region
combinations 1 + 2 + 4 and 1, 1 + 4 have not yet been conducted.) The one
action unit that significantly differentiated the two conditions was AU 4,
brow lower, (_p_ < .01) for both the sum and weighted sum measures. This
finding is consistent with the analysis of the automated system, which also
found action unit 4 most discriminative.
The above analysis examined the outputs of the automated facial action
detectors and compared them to studies that employed expert human coding. We
next turned to the problem of automatically discriminating genuine from faked
pain expressions from the facial action output stream.### 4.2. Automated classification of real vs faked pain
This section describes the second machine learning stage, in which a
classifier was trained to discriminate genuine from faked pain from the output
of the facial action detectors. The task was to perform subject-independent
classification. If the task were to simply detect the presence of a red-flag
set of facial actions, then differentiating fake from real pain expressions
would be relatively simple. However, it should be noted that subjects display
actions such as AU 4, for example, in both real and fake pain, and the
distinction is in the magnitude and duration of AU 4. Also, there is inter-
subject variation in expressions of both real and fake pain, there may be
combinatorial differences in the sets of actions displayed during real and
fake pain, and the subjects may cluster. We therefore applied machine learning
to the task of discriminating real from faked pain expressions.
A second-layer classifier was trained to discriminate genuine pain from faked
pain based on the 20-channel output stream. The system was trained using
cross-validation on the 26 subject videos described in Section 2. The input to
this second stage consisted of the 20 facial action detector outputs from the
full minute of video in each condition. In the cross-validation approach, the
system was trained and tested 26 times, each time using data from 25 subjects
for parameter estimation and reserving a different subject for testing. This
provided an estimate of subject-independent detection performance.
Prior to learning, the system performed an automatic reliability estimate of
the face alignment based on the smoothness of the eye positions. Abrupt shifts
of two to five pixels tend to occur during eyeblinks. (A more recent version
of the eye detector corrects this issue.) Those frames with abrupt shifts of 2
or more pixels in the returned eye positions were automatically detected and
the feature positions were recomputed by interpolating from neighboring
frames. This eye position filter had a relatively small effect on performance.
The analysis of Table 2 was repeated under this criterion, and the _Z_ -scores
improved by about 0.1.
The first approach we examined was to employ input features that consisted of
window-based statistics. The sixty second from each condition was broken up
into six overlapping segments of 500 frames, the windows. For each segment,
the following five statistics were measured for each of the 20 AU?s: median,
maximum, range, first to third quartile difference and ninetieth to one
hundredth percentile difference. Thus the input to the classifier for each
segment contained 100 dimensions. Each cross-validation trial contained 300
training samples (25 subjects × 2 conditions × 6 segments).
For this second-layer classification step, we first explored SVM?s, Adaboost,
and linear discriminant analysis. Nonlinear SVM?s with radial basis function
kernels gave the best performance. Linear classifiers may be inadequate to
deal with the combinations from different behavioral clusters for the same
task.
A nonlinear SVM trained to discriminate posed from real facial expressions of
pain obtained an area under the ROC curve of 0.72 for generalization to novel
subjects. This was significantly higher than performance of naïve human
subjects, who obtained a mean accuracy of 49% correct for discriminating faked
from real pain on the same set of videos.
Our later approach was based on statistics of integrated ?events?. We applied
temporal filters at eight different fundamental frequencies to the AU output
stream. Whenever these filter outputs were continuously greater than zero for
a particular AU, we integrated the intensity into one ?event?. The histograms
of these integrated intensities were used as a representation to train a
Gaussian SVM. The percent correct 2-alternative forced choice of fake versusreal pain on new subjects was 88%.
This Integrated Event representation, shown in Fig. 4, contains useful dynamic
information allowing more accurate behavioral analysis. This suggests that
this decision task depends not only on which subset of AU?s are present at
which intensity, but also on the duration and number of AU events. More
detailed analysis of the dynamics of the action units is currently underway.
1. Download: Download high-res image (66KB)
2. Download: Download full-size image
Fig. 4. Example of integrated event representation, for one subject, showing
AU 4 for two of the eight frequencies used.
## 5\. Discussion
The field of automatic facial expression analysis has advanced to the point
that we can begin to apply it to address research questions in behavioral
science. Here we describe a pioneering effort to apply fully automated facial
action coding to the problem of differentiating fake from real expressions of
pain. While naïve human subjects were only at 49% accuracy for distinguishing
fake from real pain, the automated system obtained 88% correct on a
2-alternative forced choice. Moreover, the pattern of results in terms of
which facial actions may be involved in real pain, fake pain, and
differentiating real from fake pain is similar to previous findings in the
psychology literature using manual FACS coding.
Here we applied machine learning on a 20-channel output stream of facial
action detectors. The machine learning was applied to samples of spontaneous
expressions during the subject state in question. Here the state in question
was fake versus real pain. The same approach can be applied to learn about
other subject states, given a set of spontaneous expression samples. For
example, we recently developed a related system to detect driver drowsiness
from facial expression [30].
While the accuracy of individual facial action detectors is still below that
of human experts, automated systems can be applied to large quantities of
video data. Statistical pattern recognition on this large quantity of data can
reveal emergent behavioral patterns that previously would have required
hundreds of coding hours by human experts, and would be unattainable by the
non-expert. Moreover, automated facial expression analysis will enable
investigations into facial expression dynamics that were previously
intractable by human coding because of the time required to code intensity
changes. Future work in automatic discrimination of fake and real pain will
include investigations into facial expression dynamics.
## Acknowledgments
Portions of the research in this paper use the MMI Facial Expression Database
collected by M. Pantic and M.F. Valstar. Support for this work was provided by
NSF CNS-0454233 and NSF ADVANCE award 0340851. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the
author(s) and do not necessarily reflect the views of the National Science
Foundation.
Special issue articlesRecommended articles"
35,37,Automatic facial expression recognition based on a deep convolutional-neural-network structure,"['K Shan', 'J Guo', 'W You', 'D Lu']",2017,159,Japanese Female Facial Expression,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","in speech recognition, collaborative filtering, handwriting  We employ two standard facial  expression databases for the  10 Japanese women, while CK+ covers the expression images",No DOI,2017 IEEE 15th …,https://ieeexplore.ieee.org/abstract/document/7965717/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
36,38,Automatic facial expression recognition system using deep network-based data fusion,"['A Majumder', 'L Behera']",2016,192,MMI Facial Expression,"classification, classifier, deep learning, facial expression recognition, neural network","High level emotion recognition has been recently reported in the literature, such as  We  observed an average recognition accuracy of 89.3% for MMI DB and 92.54% for CK+ DB. The",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7747479,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
37,39,Automatic facial expression recognition using features of salient facial patches,"['SL Happy', 'A Routray']",2014,686,Affective Faces Database,facial expression recognition,of the salient patches on face images. This paper proposes a novel framework for expression  recognition by using appearance features of selected facial patches. A few prominent,No DOI,IEEE transactions on Affective …,https://ieeexplore.ieee.org/document/6998925/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
38,40,Automaticity and the amygdala: Nonconscious responses to emotional faces,['A Öhman'],2002,540,Karolinska Directed Emotional Faces,neural network,"when exposed to emotionally expressive faces. Attention is  for responding to negative  emotional faces, and particularly to  emotional faces, but may instead respond to faces because",No DOI,Current directions in psychological science,https://journals.sagepub.com/doi/abs/10.1111/1467-8721.00169,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
39,41,BAUM-1: A spontaneous audio-visual face database of affective and mental states,"['S Zhalehpour', 'O Onder', 'Z Akhtar']",2016,224,Affective Faces Database,"FER, classification, classifier",Most databases available today are acted or do not contain audio data. We present a  -visual  affective face database of affective and mental states. The video clips in the database are,No DOI,… on Affective Computing,https://ieeexplore.ieee.org/document/7451244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
40,42,Beyond emotion archetypes: Databases for emotion modelling using neural networks,"['R Cowie', 'E Douglas-Cowie', 'C Cox']",2005,179,Affective Faces Database,neural network,"such a database ideally cover quality, emotional content, emotion-related  database of  emotional faces remains for many people the example of what they expect an emotion database",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608005000353,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Beyond emotion archetypes: Databases for emotion
modelling using neural networks
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Emotion and archetypes
5. 2\. The scope of databases
6. 3\. Descriptive issues in databases
7. 4\. Databases
8. 5\. Conclusion
9. Acknowledgements
10. Appendix.
11. References
Show full outline
## Cited by (95)
## Tables (2)
1. Table 1
2. Table A1
## Neural Networks
Volume 18, Issue 4, May 2005, Pages 371-388
#
2005 Special Issue
Beyond emotion archetypes: Databases for emotion modelling using neural
networks
Author links open overlay panelRoddy Cowie, Ellen Douglas-Cowie, Cate Cox
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neunet.2005.03.002Get rights and content
## Abstract
There has been rapid development in conceptions of the kind of database that
is needed for emotion research. Familiar archetypes are still influential, but
the state of the art has moved beyond them. There is concern to capture
emotion as it occurs in action and interaction (?pervasive emotion?) as well
as in short episodes dominated by emotion, and therefore in a range of
contexts, which shape the way it is expressed. Context links to modality-
different contexts favour different modalities. The strategy of using acted
data is not suited to those aims, and has been supplemented by work on bothfully natural emotion and emotion induced by various technique that allow more
controlled records. Applications for that kind of work go far beyond the
?trouble shooting? that has been the focus for application: ?really natural
language processing? is a key goal. The descriptions included in such a
database ideally cover quality, emotional content, emotion-related signals and
signs, and context. Several schemes are emerging as candidates for describing
pervasive emotion. The major contemporary databases are listed, emphasising
those which are naturalistic or induced, multimodal, and influential.
* Previous article in issue
* Next article in issue
## Keywords
Database
Emotion
Multimodal
Elicitation
Naturalistic
It is a truism that a neural network can only achieve what its training data
allows it to. In the area of research on emotion, the quality and quantity of
data available is a major issue. This paper aims to inform the research
community about the collections of data that are available, and to inform it
about ongoing developments in the area.
One of the key developments is growing recognition that the task of assembling
adequate databases is a major challenge, intellectual as well as practical,
with links to fundamental issues in the theory of emotion. In effect, a
database expresses a tacit theory of the domain it deals with, and learning
extracts it (selectively). To choose a database, and still more to construct
one, research teams need some sense of the theoretical issues in the area.
That includes knowing when ideas that intuitively seem appealing?or even
compelling?are old science or ?folk psychology? (Place, 1996) rather than
credible contemporary science. The paper aims to draw out the scientific
issues that are relevant to decisions about databases as well as describing
the actual material available.
Reviewing the issues leads to a broad sense of the ground that database
research might ideally aim to cover. The best convenient summary is ?emotion
in action and interaction??indicating that the challenge is not to find the
emotional equivalent of pure elements, but to represent emotion as it appears
woven through everyday activities.
From that we move to list the significant data sources reported in the
literature. For each source, we give the information most likely to be
relevant to research teams aiming to identify useful sources or to evaluate
work that has been done with them.
## 1\. Emotion and archetypes
?Emotion? and related words are notoriously elusive (Russell & Barrett-
Feldman, 1999). One of the key issues relates to a well known characteristic
of words in general. When they are used in isolation, they tend to call up
images that people regard as archetypal examples of the category in question
(Mervis, Catlin, & Rosch, 1976). That is a useful device in its place, but it
needs to be treated with proper caution. Certainly research should not slip
into assuming that the empirical data needed to understand a topic equate to a
collection of cases that reflect the archetypal images we associate with it.
In the domain of emotion, there are several kinds of pressure to make exactly
that assumption. Understanding recent work on databases depends on recognising
that it has been concerned to break their grip.
The first type of pressure to focus on archetypes comes from well-knowntheoretical concepts. The concept of primary emotions is particularly
influential. The underlying idea, which goes back at least to Descartes
(1911?1912), is that emotion is like colour. Certain emotions, like certain
colours, are ?pure? examples of the phenomenon: others are ?secondary?, made
by combining them in different proportions. If that theory were correct, then
the agenda for emotion research would be clear: investigate the primaries, and
the secondary forms will fall into place.
The primary/secondary distinction has enduring appeal, but Scherer et al.
(2004, p. 10) sum up the consensus within the emotion research community:
?This distinction is especially problematic ? and should therefore be used
only with caution.?. The same warning applies to the idea that the ideal basis
for training is a database stocked with examples of ?primary emotions?.
More recent is the concept of ?basic emotions?. It expresses the idea that
emotion consists of qualitatively distinct types of state, each related to a
biologically important scenario; and (in the best known version of the idea)
each with a direct biological link to particular expressive behaviours, vocal
and more particularly facial. A famous list due to Ekman's group (Ekman et
al., 1987), is sometimes called the ?big six? (Cornelius, 1966) However, even
Ekman no longer advocates that kind of attractively short list (Ekman, 1999).
Longer alternatives are summarised by Cowie and Cornelius (2003).
Correspondingly, databases composed of a few hypothetically basic states need
to be treated with caution.
A second pressure to focus on archetypes is precisely that many databases do
deal with a few archetypal states. The outstanding example is Ekman and
Friesen, 1975a, Ekman and Friesen, 1975b collection of still photographs,
which is grounded in an early version of his ?basic emotion? theory. Various
other collections, still or video, have been modelled more or less explicitly
on it (e.g. http://www.media.mit.edu/).
These databases are intuitively appealing, but, as noted above, the theory
behind them is dated. And even in its strongest form, Ekman's account included
qualifications that set limits to the likely uses of his database and others
like it. It specified that in everday life, the expressive gestures rooted in
biology were overlaid, modified, and mimicked according to socially defined
?display rules? (Ekman & Friesen, 1969). Hence, the ?pure? examples of Ekman's
database were explicitly not the patterns that we should expect to see when
humans actually interact, with each other or with machines; and training
systems to deal with those interactions would need data that showed the
effects of display rules.
A third type of pressure favours a different kind of archetype. There is a
tradition of believing that there is a peculiarly direct correspondence
between emotion terms and physiological measurements. One version of the idea
was introduced by William James (1884), who suggested that the basis of
emotion was a set of bodily responses to a significant type of stimulus?change
in heart rate, breathing, skin state, and so on. More recently there has been
intense interest in relationships between emotion and brain states, such as
activation of the amygdala (LeDoux, 1987) and change in the balance of
electrical activity between the two cerebral hemispheres (Davidson, 1999). It
is a natural step from those ideas to infer that some set of physical
measurements represents the ?ground truth? behind the confusing surface of
emotional behaviour, and it is the business of a database to ensure that those
measures are available.
The Jamesian idea seems to contain some truth (Cornelius, 1966), but it is far
from the whole truth. There is a large literature on the extent to which
emotions can be discriminated by bodily signs (e.g. Cacioppo, Klein, Berntson,& Hatfield, 1993). Some discriminations certainly can be made, but they are
quite coarse, and they almost always depend on very carefully controlled
experimental conditions. The reason is that most of the variables which are
affected by emotion are also affected, as much or more, by commonplace
activities such as moving, speaking, or even thinking. As a result, their
value as discriminators diminishes rapidly as one moves towards free
situations where people are likely to be moving, speaking, or thinking.
On one level, the jury is still out on the measurement of brain states. The
reason is that many of the key measurement techniques require activity to be
severely restricted, making it impossible to study whether normal activity
would interfere with the detection of emotion or change the patterns
associated with it. However, there are good reasons to doubt that reductionist
approaches based on brain states will fare better than any others.
Saying that a person is in a particular emotional state carries implications
about the way he or she attends to and perceives things, people, and events,
which may be present, past, anticipated, or imagined; about his or her
feelings; about his or her capacity to weigh options dispassionately; about
the actions he or she is disposed or likely to take; about the moral status of
his or her thoughts and actions; and at least in some cases about the reality
of his or her situation (e.g. Sabini and Silver, 2005, Cowie, 2005).
Such a statement strongly suggests that ascriptions of emotionality implicate
many systems, at least some of them of rather a high order, and not all of
them contained within the person to whom the emotion is attributed; and that
they refer to subtle features of the ways in which systems operate rather than
simply whether they are active or not. It is instructive that systems such as
attention, which are part of the emotional pattern, are themselves based on
highly complex and distributed neurophysiological substructures (Schneider,
1995). In such a situation, research cannot realistically expect brain imaging
to identify tractable correlates of emotion terms as they are used in natural
language. It may find correlates of something simpler that plays a role in
emotionality, but that is another matter.
A common feature of the ideas covered in this section is that they suggest a
two-stage approach to understanding emotion. In the first stage, a set of
relatively simple core correspondences is established. It is assumed?perhaps
implicitly?that analysing the core correspondences will pave the way to deal
with the complexities of emotion as it appears in everyday action and
interaction, and the attribution of everyday emotion words. It is a short step
to the assumption that the business of databases is to supply data that allow
these core correspondences to be recovered.
It would be misleading to say that two-stage approaches are discredited.
Rather, it is no longer obvious that attempting to bypass the apparent
complexity of the emotion domain is the best way to proceed. Emotion-related
phenomena have refused to fall into place around a succession of plausible
simplifying assumptions. It may be that current assumptions will pay off with
additional research, or that assumptions yet to be articulated will pay off.
But the alternative has to be taken seriously?that the phenomena are
essentially complex, and will resist simplification indefinitely.
The idea that the phenomena are essentially complex suggests two conclusions.
First, it is the responsibility of research on databases to supply data that
fully reflect the complexity of the phenomena. Second, techniques like neural
nets, which are inherently suited to addressing complex relationships, are
likely to have a key place in describing the patterns that exist. The two
define complementary parts of an emerging strategy.
## 2\. The scope of databasesIt is not self-evident what range of material emotion-related databases should
contain, particularly if the decision is not pre-empted by archetypes. This
section outlines major themes in recent thinking about the subject.
### 2.1. Types of emotionality
At least two broad types of phenomenon are described as emotional in common
parlance. We have described them as ?episodic? and ?pervasive? emotion (Cowie
& Schroeder, 2005).
The term ?episodic emotions? describes states where emotion dominates people's
awareness, usually for a relatively short time, by affecting their
perceptions, feelings, and dispositions to act. The emotion may not determine
the action that the person takes, but it requires effort to prevent it from
doing so. Clear-cut episodic emotional states are generally brief, and
relatively rare.
In contrast, ?pervasive emotion? refers to something that is an integral part
of most mental states, including states where rationality seems subjectively
to be in control. It is integral in the sense that emotional colouring is part
and parcel of the way people experience situations that they are in, or
remembering, or anticipating. It may be associated with the situation as a
whole, or with individual elements of it, or with possible courses of action.
However, the colouring is part of the background unless something triggers a
kind of phase shift and propels people into a state of episodic emotion.
It is a major issue whether the business of databases is to assemble instances
of episodic emotion, or to take a broader remit and represent pervasive
emotion. Database users have the right to look for sources that consist of
emotion in the strong sense formulated by Scherer. But if they do, they should
be aware of the reasons why recent collections have moved away from that
pattern. These are bound up with prevalence and application.
The key question about prevalence is whether episodic emotion is intermittent,
but relatively common, or distinctly rare. Naturalistic studies indicate that
there are strikingly few clear instances of episodic emotion even in material
that would usually be considered emotional in a global sense. For our Belfast
naturalistic database, we identified TV talk shows which previews suggested
were rich in genuine emotion (Douglas-Cowie, Campbell, & Roach, 2003).
Analysis showed that clear-cut emotional episodes were unexpectedly rare even
in that material (Cowie & Cornelius, 2003). Instead, the sense of emotionality
derived mainly from emotional colouring?fluctuating, often apparently
conflicting, concerned with situations that were remembered or anticipated as
much as with present surroundings, and with rare exceptions thoroughly
subordinated to the demands of civil discourse. Other analyses of naturalistic
data present similar pictures, though in different language (Ang et al., 2002,
Batliner et al., 2004, Batliner et al., 2004; Campbell, 2002a, Campbell,
2002b, Campbell, 2004; Craggs and Wood, 2004, Greasley et al., 2000, Roach,
2000, Tsukahara and Ward, 2001, Ward, 2003).
Application is discussed later, but it can be said at this stage that one
might expect to find much more use for systems that respond to a very
prevalent feature of human communication than for systems that respond to one
that is much rarer than people think.
A purely verbal point sometimes complicates the issue. Some theorists prefer
to reserve the term ?emotion? for episodic phenomena. For instance, Scherer
proposes to define emotions as ?episodes of massive, synchronized recruitment
of mental and somatic resources to adapt or cope with a stimulus event
subjectively appraised as being highly pertinent to the needs, goals and
values of the individual? (Scherer et al., 2004, p. 10). The definition
captures very precisely what we have called episodic emotion, but it impliesthat another name should be found for what we have called pervasive emotion.
Ekman (1999) takes a related position.
We prefer the episodic/pervasive terminology because it allows experts and
non-experts to be roughly agreed about how much of everyday life might be
called emotional, whereas the Scherer/Ekman usage implies that experts will
focus on a much smaller part of life (and may actively encourage them to focus
on a smaller part). Which usage wins depends on the politics of language
rather than science.
What is a matter for science and technology is that very large parts of
everyday life are emotionally coloured, and yet are not what we call episodic
emotion and Scherer calls emotion. It is important to have databases that
represent them, whatever they are called. We will continue to use the term
pervasive emotion.
### 2.2. Contexts and modalities of emotion
The context of emotion becomes a major issue as soon as one considers
pervasive emotion. It is plausible to assume that clear-cut bursts of episodic
emotion will look and sound somewhat similar in most contexts?whatever the
person was doing will be interrupted and replaced by emotion-specific
behaviour. Thinking of pervasive emotion raises different issues. It will be
integrated into the way ongoing actions and interactions are executed. The
results will depend on the nature of those ongoing actions and interactions,
and will be very different in different contexts?watching a football match,
driving a car, buying groceries, standing waiting for a friend, chairing a
meeting, writing a paper.
Database research has not generally paid systematic attention to the questions
about context, but the issue is gaining recognition. In particular, the
ongoing HUMAINE project has set out a programme involving two types of
context-sensitivity?labelling for context in naturalistic data, and carrying
out induction studies chosen to represent a range of strategically different
contexts.
Context is inescapably linked to modality. Emotion is strongly multimodal in
the sense that signs may appear in various different channels. However, not
all types of sign tend to be available together, because context will affect
the signs that are relevant or accessible. One of the most challenging tasks
for database work is to capture records that clarify when and how which
modalities come into play.
Facial signs are potentially available in a wide range of contexts, but they
are also subject to a wide range of influences. These include culture-specific
display rules and interactions with speech, involving both the lips and the
eyebrows. They are reduced in the absence of social context, and can be
feigned or dramatised, consciously or unconsciously. Databases provide
systematic coverage of a small part of this range.
Gesture-related signs are not generally available when the hands are occupied
by a task, and they are constrained when the arms are constricted (e.g. by a
narrow space) or being used (e.g. to lean on or to lift something). They are
also socially constrained (as every good child knows, it is rude to point),
and the violation of social constraints may be a sign of emotion in itself.
Databases showing emotion-related gestures reflect a very small part of that
situation.
Similar comments apply to global posture (?body language?). The signs that can
occur in mobile, standing, seated, and reclining contexts are very different,
as are postures shaped by engagement with an object and/or other people. The
emotional significance of a posture may also depend on its orientation to
another person or congruence with his or her posture.The classical vocal signs of emotion involve prosody, voice quality, and
timing. Their context is subtle. They are clearly linked to interactive
contexts, but there seems to be antagonism between overt emotionality and
well-formed speech communication, as reflected in phrases like ?speechless
with emotion? (Lee & Narayanan, 2005). That suggests the natural setting for
vocal signs of emotion is in a transition zone between controlled speech and
inarticulate vocalisation. Intrusion of non-speech sounds (Schroeder, 2003)
makes sense as part of a transitional state like that.
Capturing that kind of setting is an intriguing challenge in itself. Like the
face, the voice can easily be dramatised. That presumably involves mimicking
some of the signs that occur in the transition zone. In any case, dramatised
speech is an integral part of the way people communicate their emotional
states, and well worth representing in a database.
Traditional research on speech and emotion focused on vocal signs and
deliberately excluded verbal variation by requiring subjects to express
various emotions using the same word of phrase. In fact, though, verbal
content naturally covaries with vocal signs, in several respects. Acted
databases almost never capture that aspect of emotionality, and relatively
little has been done to draw it out of the naturalistic databases that do
reflect it.
Some attention has been paid to cues at the level of discourse (broadly
speaking). Lee and Narayanan (2003) classified utterances into rejections,
repeats, rephrases, ask-start overs, and others. The discourse measures were
less informative than acoustic measures, but nevertheless they did relate
systematically to emotional state.
Beyond that, emotion affects the classical linguistic variables of syntax and
vocabulary. A simple demonstration comes from the paper by Athanaselis et al.
in this special issue, which shows that the verbal content of emotional speech
can be recognised much better if the language model is based on a corpus
biased towards utterances with some signs of emotional content. A large
literature dating back several decades provides a rich source of ideas about
the way emotion might influence choice of vocabulary?immediacy of expression,
concreteness of terms, use of expletives, and so on (Berger & Bradac, 1982).
Intuitively, it seems obvious that emotion will usually be intimately linked
to the semantic content of people's speech, and that the most powerful cues to
emotionality will usually come from content. Rich datatabases are the key to
pursuing that idea systematically, and they have only begun to reach the point
where that is possible.
Effects involving attention are similarly under-represented in mainstream
databases, with the partial exception of extreme inattention in boredom
(Cowie, McGuiggan, McMahon, & Douglas-Cowie, 2003) and drowsiness (Hadfield &
Marks, 2000). That is curious in the light of contemporary theory, which
understands emotion in terms of appraisal processes concerned with forming
selective, valenced representations of the environment (Scherer, 1999). Eye
movement, head movement and posture all offer potential sources of information
about the focus and manner of a person's attention, and hence of the
appraisals governing his or her emotional state. Again, rich datatabases are
the key to pursuing the idea systematically.
Action ranks with attention as a variable that theory suggests should be
pivotal. Aristotle understood emotion in terms of action tendencies (such as
the impulse to revenge in anger), and the idea has remained an integral part
of modern theory since Frijda (1986) re-established it. Very few databases are
designed to record action patterns that might be symptomatic of emotion. An
exception is work on driving (McMahon, Cowie, Kasderidis, Taylor, & Kollias,2003), which offers a context for measuring behaviours associated with risk-
taking and aggression.
Sensitivity to context is particularly crucial in the case of physiological
measurements, peripheral and central. The optimal setting for discriminating
emotional states on the basis of physiological measurements is a sedentary
individual who is inducing pure and intense emotional states by acts of
imagination (Picard, Vyzas, & Healer, 2001) or being exposed passively to
powerful emotion-inducing images. Such experiments clarify points of theory by
showing that connections exist between emotion and certain somatic and neural
systems. It is a different issue to establish how they feature and may be used
in the more complex contexts that are relevant to most applications. Failure
to engage with that issue underlay the running scandal of ?lie detection?
using the polygraph (Lykken, 1984), and it is a mistake that should not be
repeated.
There are contexts where it is reasonable to think that physiological
measurement could be useful. Driving is a prime example, because simple
physical effort is limited, and relevant environmental variables (temperature,
humidity, etc.) can be monitored. There is some data on physiological
correlates of emotion-related states in that context, and more work is under
way (McMahon et al., 2003).
Forming an overview of contexts, and the ways in which emotion might be
expressed within them, is a first step towards developing an analysis of
pervasive emotion. The next step is to assemble records that establish what
actually does happen in contexts that have been identified as significant.
That is considered in the next section.
### 2.3. Obtaining samples of emotion
Most technological research on emotion continues to be based on recordings of
actors, skilled or unskilled. The fundamental reason is linked to the previous
sections. It is assumed that the proper target is episodic emotion; episodic
emotion is much rarer than people tend to assume, and difficult to elicit;
therefore, research teams turn to actors.
Blanket rejection of acted data is not a sensible option, for various reasons.
However, there are good reasons to move away from uncritical reliance on acted
data, and that has been a major theme in recent database work.
Several kinds of standard can be used to evaluate acted data. The one which is
most often observed is that people can recognise the emotion being portrayed
(Banse & Scherer, 1996; Ekman and Friesen, 1975a, Ekman and Friesen, 1975b;
Laukka, 2004). It is not a criterion that should be over-emphasised. On one
hand, it would tend to rule out a good deal of genuine emotional material,
because inter-observer agreement on it is often not particularly high. On the
other, it does not rule out material which is identifiable, but not at all
like natural emotion?in short, ?ham? acting.
We have explored a technique which addresses the second problem. As a
recording is played (audio, visual or audio?visual), observers use a sliding
scale to indicate whether they think the material is spontaneously expressed
emotion or acting, and how confident they are in their judgment. We compared
ten extracts from the Belfast naturalistic database with renditions of the
same material by actors in the Belfast structured database (Douglas-Cowie et
al., 2003). On the basis of pilot studies, we chose the most convincing
rendition of each extract, and one which was about average. Two measures were
of interest. The first was the percentage of trials which eventually favoured
the right choice (acted or not). 68% concluded that the less convincing
renditions were acted, and 67% that more convincing renditions were. In
contrast 79% concluded that the ones which were actually spontaneous were notacted. The second measure was how quickly the difference between responses to
the corresponding real and acted extracts reached statistical significance.
For both convincing and unconvincing renditions, the difference reached
significance within the first three seconds. Both measures indicate that even
well acted material differs very noticeably from spontaneous emotionality.
To the best of our knowledge, no databases of acted material currently include
measures of naturalness. However, it is easy to carry out the kind of study we
have described above, and it would make sense to carry out checks of that sort
before committing to using an acted database.
Even if acted material is far from natural, it could conceivably be well
suited for training, if it presented the features that are relevant to
everyday behaviour in sharper focus. There seem to be no direct tests of the
idea, but the evidence that exists is not encouraging. Batliner, Fischer,
Huber, Spilker, & Nöth (2003) have shown that the vocal signs of emotionality
used by an actor simulating a particular human?machine interaction were
different from, and simpler than, the signs given by people genuinely engaged
in it. Kollias (2005) have used the Ekman and Medialab databases of acted
facial expressions as a basis for generating less extreme facial profiles, and
are applying the approach to recognition in induced emotion data. Results to
date suggest that the approach has some success with individuals who express
themselves in a dramatic way, but not with individuals behaving more
naturally.
Note that for some applications, acted material is actually the optimal
source. For example, an artificial newsreader should probably simulate the
conventionalised emotional signs offered by human newsreaders rather than
showing true grief at disasters or euphoria at triumphs.
Naturalistic data seems an ideal alternative to acted sources, but the reality
is not straightforward. Problems of copyright and privacy are well recognised
(Douglas-Cowie et al., 2003). More fundamental, but less often noted, is the
fact that key tools needed to deal with naturalistic data are underdeveloped.
First, recordings of the kind seen in ?reality? TV could be obtained in
reasonable quantity, but technology lacks the robust low-level analysis needed
to handle them. Humans can recognise expressions of emotion from faces that
are at an angle or at a distance, but the 3D modelling techniques needed to do
that automatically have barely been explored (Kollias et al., 2004).
Similarly, speech analysis lacks the tools to deal with varying distances from
microphones, reverberation, and noise. The traditional strategy is to expect
recordings to cater to the engineering limitations, but the usual methods of
doing that conflict with free emotional expression, and the long term solution
is clearly to address the engineering limitations.
Second, we lack the conceptual tools to deal with the diversity of natural
emotional behaviours. The traditional strategy is to look for elements common
to all or most instances of an emotion (Juslin & Laukka, 2002). From the
perspective of the previous sections, that filters out a huge amount of
evidence, because so much evidence lies in the way emotion modulates
activities that carry their own constraints. If the expression of emotion has
common elements in the contexts considered above, they are at a deeper level,
involving attention, planning, risk-taking, and so on. We do not have the
theory at that level that would allow us to draw out the common threads in a
genuinely naturalistic collection. Sophisticated learning techniques may well
be relevant to developing it.
Recordings from real human?machine interactions avoid both of these problems
to some extent, because they involve built-in constraints and a kind of
uniformity. If short-term applications are the target, then directly relevantdatabases are clearly the appropriate resource?not least because of the
context-dependence that has been emphasised. However, focussing on short term
applications may be counterproductive in the long term, because it postpones
confronting the major issues, and finding relationships which are dependent on
the structure of a particular task may not clarify the general picture. To
illustrate, we studied bored subjects carrying out a naming task (Cowie et
al., 2003), and found reduced pausing (because speakers ran descriptions of
successive items together). However, we have since found that a small change
in the structure of the task results in exactly the opposite trend, increased
pausing (Cowie & Schroeder, 2005). It seems that the underlying effect is not
on pausing, but on attention; and the overt signs of the attentional effect
depends on task details. If the field were driven by data from many very
specific tasks, it would risk being overwhelmed by that kind of highly
context-specific contingency. Allied to that is the risk demonstrated by
Yacoub, Simske, Lin, & Burns (2003). A recogniser trained on a data containing
a narrow range of emotions is likely to respond quite inappropriately if it
encounters an emotion outside the training range. That raises ethical as well
as practical problems (Goldie et al., 2004).
In spite of the problems, major efforts have been invested in collecting
naturalistic data (Batliner et al., 2004, Batliner et al., 2004; Douglas-Cowie
et al., 2003 DeVilliers et al. this issue). Key sources are listed in the
Appendix to this paper.
Between acting and pure naturalism lie various emotion induction techniques.
It is not difficult to create situations which induce a specific emotion such
as irritation (Bachorowski, 1999) or boredom (Cowie et al., 2003). It is more
challenging to induce a range of emotions in comparable conditions, but there
are various established methods?verbal methods such as the Velten technique
(Velten, 1968); listening to emotive music (Clark, 1983, Kenealy, 1988);
looking at emotive pictures (Center for the Study of Emotion and Attention
(CSEA-NIMH), 1999, Lang et al., 1999) or films (Gross & Levenson, 1995); and
playing specially designed games (Scherer, Johnstone, & Bänziger, 1998). Cowie
and Cornelius (2003) give a more detailed review of classical techniques and
their status.
The traditional test of induction techniques is whether they evoke a state
that genuinely qualifies as emotion. That is a reasonable criterion if one is
looking for an inner core of phenomena that (hypothetically) accompany all
emotions. However, the perspective developed here points to a different test,
which is ability to induce emotion that carries through into action and
interaction. Work on that problem is much more recent.
Scherer et al. (1998) carried out pioneering work with a computer game
containing incidents designed to induce specific appraisals. Their effect was
shown not only in transient facial responses, but also in a reading task which
was part of the game. That taps strictly vocal signs of emotion in speech,
though not effects at the level of phrasing, word selection, etc.
A technique developed by Auberge, Audibert, and Rilliard (2003) evokes freer
expression. Students with an interest in language used a voice-driven language
teaching program, and were given feedback indicating that their performance
was outstandingly good or disturbingly bad. Their ongoing reactions were
reflected in facial expression and vocal behaviour which included specific
items to be pronounced and unscripted comments.
Our SAL technique (Cox 2004) is modelled on ?chatbot? systems. The user talks
to characters whose conversation is defined by ?scripts? consisting of
emotionally loaded stock phrases. The system contains different characters,
each of which has a distinctive emotional outlook (in the current version,angry, happy, sad, and pragmatic), and each tries to draw the user towards its
own outlook. Users do not automatically engage with SAL, but when they do, it
allows them to express a range of emotions through a relatively wide range of
channels, facial, vocal, discourse, and content.
One of the key difficulties for database research is that induction remains an
uncertain art, seriously complicated by ethical issues. It is a telling sign
that we are currently exploring ways to induce a range of genuinely emotional
states that are persistent enough to affect driving behaviour (in a
simulator). Transient emotion can certainly be induced by events of no
enduring significance in subjects' lives, but it seems to dissipate very
quickly when they are presented with a task or a challenge. The same does not
hold when the inducing events or situations are part of the fabric of people's
lives, but not many laboratories would manipulate those events to gather data.
### 2.4. Emotion and applications
Application is clearly an issue in the development of databases, but it is
best addressed after the scope of the field has been indicated.
The obvious short-term application of neural nets in the domain of emotion is
?trouble shooting??detecting emotional states that may be troublesome in
callers using automatic exchanges or websites, pilots, drivers, and so on
(Cowie & Schroeder, 2005). Another possibility is ?affective selection??using
emotion-related signs as evidence that a person might want to hear certain
music, preserve certain experiences, or be shown certain types of option
(Cowie & Schroeder, 2005). Neither the databases nor the computing techniques
needed for these applications is particularly exciting, at least at first
sight.
The longer term goal, though, is ?really natural language
processing??communication between humans and machines that follows human norms
in emotional terms as well as others. That means both registering and
providing the kind of emotional colouring that is the norm in human
interaction. In applications such as artificial companions and tutors, there
is a clear incentive to do these things better than many humans do. It is to
achieve targets like these that research needs databases of great scope and
subtlety.
Correspondingly sophisticated computing is needed to exploit the data. The
requirement in these contexts is not simply to label isolated patterns. It
seems likely that sequence and timing, often crossing different modalities,
are critical in the detection of emotion-related effects. Taylor's ANNA (this
issue) illustrates the reasonable supposition that mechanisms modelled on the
human brain will prove the best way to handle that kind of problem. The same
point applies to the inverse problem, which is to generate appropriately
sequenced and timed arrays of signals.
## 3\. Descriptive issues in databases
Emotion databases include not only recordings in various modalities, but also
descriptions of the emotions involved and the signs relevant to them. This
section summarises the main options in these areas.
### 3.1. Describing quality
Several preliminary issues should clearly be documented in a satisfying
database, though they rarely are. The key examples are
* ?
Is any emotion evident? If, for instance, one is exploring a large database
that involves very little emotion, one of the most useful elements a database
could have is a trace showing where there is emotionality of any sort is
among.
* ?Is emotion being masked? This is linked to the Ekman concept of display rules
(see Section 1).
* ?
Is the emotionality that is displayed spontaneous or contrived? This is not a
simple matter of ?good? and ?bad? samples. For instance, an element of
dramatisation is part of everyday communication, and sometimes the target is
to achieve acceptable formal representations of emotion (see Section 2.3).
### 3.2. Describing emotional content
Three main types of scheme could reasonably be used to describe the emotional
content of a database: categorical; continuous; and appraisal-based.
Categorical schemes are rooted in everyday language. They involve assigning
terms like angry, ashamed, jealous, and so on. There is no real doubt that
terms like these have an enduring place in emotion-oriented technology,
because they are the means of describing emotion that people find most
natural. The difficulty is how to deal with the range of descriptors that
everyday language offers and the kind of emotion that appears in relatively
natural settings.
Numerous teams have tried to label relatively naturalistic material using
schemes based on established psychological lists of emotions. Two examples
illustrate the outcomes. The Leeds-Reading database (Douglas-Cowie et al.,
2003, Roach, 2000) produced fine labellings in which a single utterance might
be described as moving from hate to vengeful anger. The result is a very low
incidence rate of similarly labelled utterances, leading to intractable
statistical problems. Craggs and Wood (2004) used a list derived from Ortony
and Turner (1990)?courage, dejection, sadness, disgust, aversion, shame,
anger, surprise, guilt, wonder, hate, love, happiness, desire, contempt, fear.
The result was considered unacceptable because of very low inter-rater
agreement.
Part of the problem is that traditional psychological lists are oriented to
archetypal emotions, and those are not the states that appears in most
naturalistic data. Two main types of response have developed within a
categorical framework. One is to group the original categories into ?cover
classes?. For example, Batliner et al used positive, pronounced, weak
negative, and strong negative. Another is to develop lists oriented towards
describing everyday emotionality rather than states which are considered
important theoretically, but may rarely occur in a clear-cut form.
An early list of that kind (Cowie et al., 1999) was used to label the Belfast
naturalistic database. More recently it has become clear that there are
problems with lists that are derived without reference to context. As a
result, Sweeney (2005) asked participants to rate how useful 75 emotion-
related words would be to describe a recent day in their lives. For one group,
the day was to be typical; for another, a good day; and for the third, a bad
day. Table 1 shows the words that were most highly rated by each group (in
order). The list for a typical day is similar to our older list, and to the
list for a good day; but neither includes the terms needed to describe a
negative day. The strategy of preselecting word lists that suit the context is
a useful extension of categorical approaches, and is being applied, for
instance, to labelling emotion in meetings.
Table 1. The emotion-related words rated most useful for describing three
types of day (from most to least useful)
Typical day| Good day| Bad day
---|---|---
Amusement| Pleasure| Stress
Friendliness| Amusement| SadnessPleasure| Delight| Anxiety
Affection| Excitement| Frustration
Love| Satisfaction| Hurt
Politeness| Affection| Annoyance
Contentment| Content| Despair
Interest| Friendliness| Tension
Trust| Happiness| Worry
Excitement| Love| Disappointment
Calm| Interest| Powerlessness
Satisfaction| Hopefulness| Shock
A more radical alternative is to use the long-established psychological
concept of emotion dimensions (Cowie & Cornelius, 2003). Statistically, most
of the information contained in verbal labellings can be expressed in terms of
two dimensions, activation (whether the person's emotional state makes him or
her more or less likely than average to take action), and evaluation (how
positive or negative he or she feels). The FEELtrace tool allows raters to
track a target person's perceived emotional state in real time as they watch
and/or listen to a recording of the person in question. It gives a
continuously valued trace. Craggs and Wood (2004) have described a categorical
rating technique based on the same underlying theory, though they substitute
strength of emotion for activation. The result is actually quite close to
Batliner et al's cover categories.
Dimensional description appears to be increasingly accepted as an established
tool. It has to be emphasised that it is by definition an approximate tool.
Two-dimensional versions, for instance, do not distinguish between fear and
anger (that requires a third dimension involving power or control); and
concepts like politeness and interest which figure in Table 1 lie quite
outside the scheme. Its coarseness both makes it useful and limits its use.
A finer theoretical tool comes from appraisal theory, which is probably the
most influential approach to emotion within psychology at present (Scherer,
1999). Appraisal theory offers a descriptive framework for emotion based on
the way the person involved experiences the events, things, or people at the
focus of the emotional state. This is described involving five broad headings:
* ?
Novelty (involving suddenness, familiarity and predictability as distinct
subcategories)
* ?
Intrinsic pleasantness
* ?
Goal significance (including the aspect of life to which the events are
relevant, from the body to relationships to social order; the likelihood that
there will be a significant outcome; how the events relate to expectation, and
to the agent's goals; and how urgent they are)
* ?
Coping potential (covering how much control anyone could exercise over the
event, how much power the agent in particular has to control them, and how
easily the agent can adapt by adjusting his or her own goals)
* ?
Compatibility with standards (covering how far the events conform to or
violate moral or ethical standards, held either by the agent personally or by
society as a whole)
* Work is under way on translating this scheme into a practical tool. One of the main motivations is that the
individual items may be linked to component gestures in the expression of emotion (such as eyebrow raising,
mouth movement, etc.). That opens the way to deal both with the sequencing of gestures, and with situationswhere some, but not all of the appraisal elements associated with full-blown emotion are present.
* These descriptive schemes are far removed from the pattern that someone entering the field might expect
to find in a database. Understanding how the descriptions involved relate to signs of emotion is also a much
more interesting task.
### 3.3. Analyses of emotion-related signals and signs
Not many teams working on neural nets will be able to use data in the form of
direct records in the audio, visual, or physiological domain. Most will need
material to be analysed on at least one of two other levels, feature
extraction and sign description. Feature extraction refers to the output of
signal processing operations (perhaps corrected by hand). For example,
existing routines can extract positions of key joints from video records. Sign
description refers to the ways in which humans spontaneously interpret
incoming evidence, usually involving intentionality?for instance, pointing at
a car. Automatic transition from extracted features to signs is very
difficult.
There are various levels at which neural network research could engage with
sequences of signals, features, and signs, given appropriate data. There is
still a need for general, robust solutions to many of the basic problems in
feature extraction. The mapping of features to signs is an extremely
challenging problem, as is the mapping of signs to descriptions of emotion.
Neural nets may well also be well suited to some aspects of the inverse
problem, synthesis of sign sequences that are appropriate to express a
sequence of emotional states.
There is currently rapid evolution in the provision of data relevant to these
tasks. The Erlangen group is in the process of making available the AIBO
database, with hand corrected transcriptions of verbal content and pitch. SAL
data collected under the ERMIS project includes FAPS (Balomenos et al., 2004),
basic speech features (pitch and intensity contours and pause boundaries) and
higher order features calculated by the ASSESS system. ASSESS features are
also available for the Belfast naturalistic database. The EmoTV Corpus
contains rich descriptions of signs. Programs under way use motion capture to
provide machine-readable descriptions of body movements relevant to emotion
(see e.g. Pollick, Hill, Calder, & Paterson 2003).
Records are often not clear about the level of analysis available, and the
situation is often evolving. Hence the Appendix does not try to detail the
level of analysis associated with data sources. Some additional information is
available via the listing of databases on the HUMAINE website (http://emotion-
research.net).
### 3.4. Describing context
Despite the importance of context, there are no generally used schemes for
describing it. A recent HUMAINE report included a proposal that at least the
following issues should be specified
* ?
Agent characteristics (age, gender, race)
* ?
Recording context (intrusiveness, formality, etc.)
* ?
Intended audience (kin, colleagues, public)
* ?
Overall communicative goal (to claim, to sway, to share a feeling, etc.)
* ?
Social setting (none, passive other, interactant, group)
* ?
Spatial focus (physical focus, imagined focus, none)* ?
Physical constraint (unrestricted, posture constrained, hands constrained)
* ?
Social constraint (pressure to expressiveness, neutral, pressure to formality)
It is proposed to refine this scheme through work with the HUMAINE databases
as they develop. Parts are already implemented in the EmoTV Corpus described
in the Appendix.
## 4\. Databases
The Appendix lists the databases that seem most likely to be of interest to
the neural net community (and to research on machine learning in general).
Items are included for three main reasons.
First and foremost is the use of data that is naturalistic or induced in
credible ways. These are covered as fully as possible.
The second is multimodality. Multimodal databases are still rare, but the
picture has changed considerably since a relatively recent review (Douglas-
Cowie et al., 2003).
The third is influence. Databases which are neither multimodal nor
naturalistic have nevertheless been influential for a variety of reasons, one
of which may be that they are convenient to access and use. It would be remiss
not to include influential sources in a systematic review.
## 5\. Conclusion
Ekman and Friesen's database of emotional faces remains for many people the
example of what they expect an emotion database to be. The databases listed in
the first part of the Appendix are very different. It seems likely that the
future trend will be towards databases more like the latter than the former.
One of the main aims of this paper has been to convey the background against
which that move has occurred, so that people who are inclined to expect
something like the Ekman and Friesen resource understand why they are being
pointed to something so different.
It is also the case that assembling databases has not traditionally been
considered a high-profile or intellectually challenging area. Good quality
recording and large balanced samples tend to be thought of as the basic
requirements, with the human side assumed to be relatively straightforward.
A little thought shows that in the domain of emotion, that cannot be the case.
The human race expends a huge proportion of its resources trying (with mixed
success) to direct people out of some emotional states and into others. If it
were easy to achieve the shifts, there would be no need for whole industries
and cities to exist.
As a result, capturing a faithful, detailed record of human emotion as it
appears in real action and interaction is a massively challenging task.
Nevertheless, the payoff is also large. At root, it is enlisting computers to
co-operate in the old task of directing people away from some emotional states
and into others. The lure of technologies capable of doing that is enough to
keep the enterprise going in spite of the difficulties.
## Acknowledgements
Preparation of this paper was supported by EC projects ERMIS (IST-2000-29319)
and HUMAINE (IST-2003-507422).
## Appendix.
List of key databases for computational research on emotion. The abbreviations
in the ?modality? column are as follows: A=audio; V=video; G=gesture;
P=physiology. Column 1 cites published sources where they are available: the
corresponding references are in the general list at the end of the paper.
(Table A1)
Table A1.Name| Modality| Elicitation method| Size| Language| Type of emotional
description| Emotional content| Electronic contact
---|---|---|---|---|---|---|---
Belfast naturalistic database (Douglas-Cowie et al., 2003)| AV| Natural: 10?60
s long ?clips? taken from television chat shows, current affairs programmes
and interviews conducted by research team| 239 clips; 209 from TV recordings,
30 interview recordings. 125 subjects; 31 M, 94 F| English| Dimensional
labelling/Categorical labelling| Wide range|
Geneva airport lost luggage study (Scherer and Ceschi, 1997, Scherer and
Ceschi, 2000)| AV| Natural: unobtrusive videotaping of passengers at Geneva
airport lost luggage counter followed up by interviews with passengers| 112
subjects| not known| Rating on emotion category scales| Anger, good humour,
indifference, stress, sadness|
EmoTV corpus| AV| Natural: 4?43 s long ?clips? from French television
channels| 51 clips from French TV channels; 48 different individuals;| French|
Categorical labelling| Wide range| http://emotion-
research.net/ws/wp5/databasesLQ.ppt
SAL (Sensitive artificial listener) database| AV| Induced: subjects talk to
artificial listener and emotional states are changed by interaction with
different personalities of the listener| Approx. 10 h; study 1, 20 subjects;
study 2, 4 subjects x2 20 min sessions each| English| Dimensional labelling|
Wide range of emotions/emotion related states but not very intense|
http://emotion-research.net/ws/summerschool1/SALAS.ppt, http://emotion-
research.net/ws/wp5/ellsal.ppt
SMARTKOM| AVG| Human machine in WOZ scenario: solving tasks with system| 224
speakers; 4/5 minute sessions| English, German| Categorical labelling holistic
and for each channel in turn Intensity also labelled| Joy, anger,
gratification, irritation, helplessness, pondering, reflecting, surprise,
neutral| http://www.phonetik.uni-
muenchen.de/Bas/BasMultiModaleng.html#SmartKom; http://www.phonetik.uni-
muenchen.de/Publications/Steininger-02-LREC.pdf
DaFEx Database of Kinetic Facial Expressions (Batocchi & Pianesi, 2004)| AV|
Acted: Instructions given to actors. Use of ?scenarios?, one for each emotion|
1008 clips (4?27 s each) coming from recordings with 8 professional actors (4
male, 4 female) Total recording time: 2 h 50 min 13 s| Italian| Categorical
emotional labelling| Ekman 6 emotions plus neutral expression, 3 intensity
levels, 2 acting conditions: ?Utterance? and ?Non?Utterance?.|
{battocchi,pianesi@itc.it
eWiz database (Auberge et al. 2003)| AVP| Induced using a WoZ scenario
simulating a language teaching exercise| 17 subjects (10 general, 7 actors)
3400 single words plus comments 5hrs speech, 15 hours of non-verbal| French|
Labels chosen by subjects (category and dimensional)| Concentration, positive,
negative, stressed, satisfaction, boredom, surprise, disappointment, anxious,
confused| http://www.icp.inpg.fr/EMOTION
Speech motion face database| AV| Acted| Three different expression variations
for each emotion and 1326 utterances: frustrated 59, happy 85, neutral 86, sad
100, angry 112| English| Pre-defined| frustrated, happy, neutral, sad, angry|
http://www.cs.ucla.edu/~abingcao/Documents/Cao_TOG_small.pdf
Polzin and Waibel (Polzin & Waibel, 2000)| AV| Acted: sentence length segments
taken from acted movies| Unspecified no of speakers. Segment numbers 1586
angry, 1076 sad, 2991 neutral| English| Choice from a given set of emotion
tags| Anger, sadness, neutrality (other emotions, but in insufficient numbers
to be used)|
Banse and Scherer (Banse & Scherer, 1996)| AV| Acted: actors were givenscripted eliciting scenarios for each emotion, then asked to act out the
scenario. (visual info used to verify listener judgements of emotion)| 12
subjects (6M, 6F)| German| Expert (peer) rating of authenticity; rating from
14 emotional categories| Anger (hot), anger (cold), anxiety, boredom,
contempt, disgust, elation, fear (panic), happiness, interest, pride, sadness,
shame|
AIBO ROBOT DATABASE(Erlangen database) (Batliner et al., 2004a)| AV/A| Task
directions to robot| 51 German children, 51393 words| German| Categorical
labelling of emotion by several labellers| Joyful, surprised, emphatic,
helpless, touchy (irritated), angry, motherese, bored, reprimanding, neutral|
http://pfstar.itc.it/public/publications/lrec04.pdf
Amir et al. (Amir, Ron & Laor, 2000)| AP| Induced: subjects asked to recall
personal experiences involving each of the emotional states| 140 subjects; 60
Hebrew speakers, 1 Russian speaker| Hebrew, Russian| Categorical labelling|
Anger, disgust, fear, joy, neutrality, sadness|
http://www.qub.ac.uk/en/isca/proceedings/pdfs/amir.pdf
Chung (Chung, 2000)| AV| Natural: television interviews in which speakers talk
on a range of topics including sad and joyful moments in their lives| 77
subjects; 61 Korean speakers, 6 Americans| English, Korean| Dimensional
labelling/ Categorical labelling| Joy, neutrality, sadness (distress)|
ORESTEIA database Balomenos, Cowie, Malinescos, McMahon & Kasderidis (2003)|
P| Induced: subjects encounter various problems while driving (deliberately
positioned obstructions, dangers, annoyances ?on the road?| 29 subjects, 90
min sessions per subject| English| Performance in tasks used as index of
subject state| Stress, irritation, shock|
Kim, Bang and Kim| P| Induced?multimodal approach to evoke specific targeted
emotional statuses| Databse 1=125 subjects (children 5?8 yrs old), Database
2=50 subjects (children 5-6yrs old)| n/a| Self report by subjects using verbal
rating| sadness, anger, stress, surprise|
http://www.iee.org/Publish/Journals/ProfJourn/MBEC/20043886.pdf
The HUT Facial Expression Database| AV| Acted: static pictures and short video
sequences| 2 actors| n/a| Pre-defined| Anger, disgust, fear, surprise
happiness, sadness| http://www.lce.hut.fi/research/cogntech/emo.shtml
Reading-Leeds database (Greasley et al., 1995; Greasley et al., 2000; Roach et
al., 1998; Stibbard, 2001)| A| Natural: unscripted interviews on
radio/television in which speakers are asked by interviewers to relive
emotionally intense experiences| 5 hours| English| emotion labels| Range of
full blown emotions|
Campbell CREST database, (ongoing) (Douglas-Cowie et al., 2003)| A| Natural:
volunteers record their domestic and social spoken interactions for extended
periods throughout the day| Target?1000 hrs over 5 years at 2004 500+ hours|
English, Japanese, Chinese| 3 levels of label: speaker state, speaking style,
voice quality| Wide range of emotional states and emotion-related attitudes|
http://feast.his.atr.jp/data/, http://www.mpi.nl/lrec/papers/lrec-pap-06-nick-
speech.pdf
Capital Bank Service and Stock Exchange Customer Service (used by Devillers &
Vasilescu, 2004)| A| Natural: call center human-human interactions| Stock
Exchange: 100 dialogues, 5000 speaker turns; Capital Bank: 250 dialogues, 5000
turns of word extracts| French| 2 annotators using categorical labels| Mainly
negative - fear, anger, stress| http://www.isca-
speech.org/archive/sp2004/sp04_205.html
France et al. (France, Shiava, Silverman, Silverman & Wilkes, 2000)| A|
Natural: therapy sessions and phone conversations. Post therapy evaluation
sessions were also used to elicit speech for control subjects.| 115 subjects48 females, 67 males, controls (therapists), 38, patients 77| English| DSM-IV
diagnosis of depression| Dysthemic disorder; major depression (single episode
and recurrent, no psychotic features)|
SYMPAFLY (as used by Batliner et al., 2004b) (Batliner et al., 2004, Batliner
et al., 2004)| A| Human machine dialogue system| 110 dialogues, 29200 words
(i.e. tokens, not vocabulary)| German| Word-based emotional user states|
Joyful, neutral, emphatic, surprised, ironic, helpless, touchy, angry, panic|
Belfast Boredom database (Cowie, et al 2003)| A| Induced: naming objects on
computer screen| 12 subjects| English| Self rating of boredom/interest, rate
at which trials completed, task error rate| Boredom| Belfast Boredom database
(Cowie, et al. 2003)
DARPA Communicator corpus (as used by Ang et al., 2002) (Ang, et al, 2002)| A|
Human machine dialogue system (Extracts from recordings of simulated
interactions with a call centre)| 13187 utterances, average length about 2.75
words, 1750 considered emotional| English| Categorical emotional labeling by 5
students from 1 of 7 possible labels| Frustration, annoyance, neutral, tired,
amused, other, not applicable| http://www.speech.cs.cmu.edu/Communicator/
Fernandez and Picard (Fernandez & Picard, 2003)| A| Induced: subjects give
verbal responses to maths problems in simulated driving context| Data reported
from 4 subjects| English| Performance in tasks used as indicator of subject
state| Stress|
Lee et al. (Lee & Narayanan, 2003)| A| Dialogue with automated system| 1187
calls, Total 7200 utterances| English| Tagged: negative and non negative|
Negative, non-negative| http://sail.usc.edu/publications/euro_fuzzy.pdf
Yacoub et al| A| Acted| 2433 utterances from 8 actors| English| Emotional
categorical labelling| hot anger, cold anger, happy, sadness, disgust, panic,
anxiety, despair, interest, shame, pride, boredom, contempt elation, neutral|
http://www.isca-speech.org/archive/eurospeech_2003/e03_0729.html
Tolkmitt and Scherer (Tolkmitt & Scherer, 1986)| A| Induced: Cognitive stress
induced through slides containing logical problems; emotional stress induced
through slides of human bodies showing skin disease/accident injuries| 60
subjects (33 male, 27 female)| German| Low versus high stress| Stress (both
cognitive and emotional)|
Iriondo et al. (Iriondo et al., 2000)| A| Contextualised acting: subjects
asked to read passages written with appropriate emotional content| 8 subjects
reading paragraph length passages (20?40 mm s each)| Spanish| Emotional
categorical labelling| Desire, disgust, fury, fear, joy, surprise, sadness|
Mozziconacci (Mozziconacci 1998)| A| Contextualised acting: actors asked to
read semantically neutral sentences in range of emotions, but practised on
emotionally loaded sentences beforehand to get in the right mood| 3 subjects
reading 8 semantically neutral sentences (each repeated 3 times)| Dutch|
Emotional categorical labelling| Anger, boredom, fear, disgust, guilt,
happiness, haughtiness, indignation, joy, rage, sadness, worry, neutrality|
McGilloway Database (McGilloway, 1997)| A| Contextualised acting: subjects
asked to read passages written in appropriate emotional tone and content for
each emotional state| 40 subjects reading 5 passages each| English| Emotional
categorical labelling| Anger, fear, happiness, sadness, neutrality|
Belfast structured Database (McGilloway et al., 2000)| A| Contextualised
acting: subjects read 10 McGilloway?style passages AND 10 other
passages?scripted versions of naturally occurring emotion in the Belfast
Naturalistic Database| 50 subjects reading 20 passages| English| Emotional
categorical labelling| Anger, fear, happiness, sadness, neutrality|
Danish Emotional Speech Database| A| Scripted (material not emotionally
coloured)| 4 subjects read 2 words, 9 sentences and 2 passages in range ofemotions| Danish| Emotional categorical labelling| Anger, happiness sadness,
surprise, neutrality| http://www.isca-
speech.org/archive/eurospeech_1997/e97_1695.html
Groningen ELRA corpus number S0020| A| Acted| 238 subjects reading 2 short
texts| Dutch| Unclear| Database only partially oriented to emotion|
http://www.elda.org/catalogue/en/speech/S0020.html
Berlin database (Kienast & Sendlmeier, 2000)| A| Acted: Scripted (material
selected to be semantically neutral)| 10 subjects (5 M, 5 F) reading 10
sentences each| German| Emotional categorical labelling| Anger- hot, boredom,
disgust, fear- panic, happiness, sadness-sorrow, neutrality|
http://www.expressive-speech.net/
Leinonen and Hiltunen (Leinonen & Hiltunen, 1997)| A| Acted| 483 samples
selected down to 120 (one word utterances)| Finnish| Emotional categorical
labelling| Angry, frightened, commanding, pleading content, (naming),
astonished, scornful, admiring, sad|
Nakatsu et al. (Nakatsu, Tosa & Nicholson 1999)| A| Utterances acted in
various emotions then imitated by speakers| 100 words, uttered (by 100
speakers) once for each emotion, totalling 800 words| Japanese| Emotional
categorical labelling| Neutrality, anger, sadness, fear, disgust, playfulness,
surprise, happiness|
Nogueiras et al 2001| A| Acted| 6 sentences, paragraphs, words, numbers by
each of 2 Spanish, French Slovenian, English speakers| Spanish, French,
Slovenian, English| Emotional categorical labelling| Anger, disgust, fear,
joy, sadness, surprise, neutrality| http://www.isca-
speech.org/archive/eurospeech_2001/e01_2679.html
Media Team corpus in Finnish| A| Acted| 14 actors (8m, 6f) Finnish passage of
aprox 100 words read in 6 emotions plus neutral| Finnish| Emotional
categorical labelling| Happiness/joy, sadness, fear, anger, boredom, disgust,
neutral| http://www.mediateam.oulu.fi/publications/pdf/438.pdf,
http://www.mediateam.oulu.fi/projects/mpappce/?lang=en
ORATOR| A| Acted (actors asked to interpret same 8 sentence monologue in
different social situations)| 117 samples produced by professional actors, 14
non-actors produced 28 samples| German| Not specified| Not specified|
http://mplab.ucsd.edu/databases/databases.html
RUFACS| V| Naturalistic/Induced| 100 subjects, 2.5 minutes each. Aiming for
400-800 mins| n/a| unclear| not exactly specified, but spontaneous facial
expressions| http:mplab.ucsd.edu/databases/databases.html
The AR face database| V| Posed| 154 subjects (82 male, 74 female) 26 pictures
per person| n/a| Pre-defined| Smile, anger, scream neutral|
http://rvl1.ecn.purdue.edu/~aleix/aleix_face_DB.html
CVL face database| V| Posed| 114 subjects (108 male, 6 female) 7 pictures per
person| n/a| Pre-defined| Smile| http://lrv.fri.uni-lj.si/facedb.html
The Japanese Female Facial Expression (JAFFE) Database| V| Posed| 10 subjects|
n/a| Rated by 60 Japanese raters using 6 emotion adjectives| Sadness,
happiness, surprise, anger, disgust, fear, neutral|
http://www.mis.atr.co.jp/~mlyons/jaffe.html
CMU PIE Database (CMU Pose, Illumination, and Expression (PIE) database)| V|
Posed| 68 subjects| n/a| Pre-defined| Neutral, smile, blinking and talking|
http://www.ri.cmu.edu/projects/project_418.html
The Yale Face Database A| V| Posed| 15 subjects| n/a| Pre-defined| Sad,
sleepy, surprised| http://cvc.yale.edu/projects/yalefaces/yalefaces.html
CMU Facial Expression Database (Cohn-Kanade)| V| Posed| 210 subjects| n/a|
Pre-defined| Six of the displays were based on descriptions of prototypic
emotions (i.e., joy, surprise, anger, fear, disgust, and sadness)|http://vasc.ri.cmu.edu/idb/html/face/facial_expression/index.html
ATandT The Database of Faces (formerly called ORL database)| V| Posed| 40
subjects, 10 pictures per person| n/a| Pre-defined| Smiling|
http://www.uk.research.att.com/facedatabase.html
The Psychological Image Collection at Stirling| V| Posed| Aberdeen: 116
subjects, Nottingham scans: 100, Nott-faces-original: 100, Stirling faces:36|
n/a| Pre-defined| Smile, surprise, disgust| http://pics.psych.stir.ac.uk/
Special issue articlesRecommended articles"
41,43,Bilinear models for 3-D face and facial expression recognition,"['I Mpiperis', 'S Malassiotis']",2008,230,Binghamton University 3D Facial Expression,"facial expression recognition, machine learning, neural network",of bilinear models on the Binghamton University 3-D Facial Expression (BU-3DFE)  ’s work  [1] on facial expression recognition and our previous work [2] on face recognition. BU-3DFE is,No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/abstract/document/4539275/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
42,44,Blended emotion in-the-wild: Multi-label facial expression recognition using crowdsourced annotations and deep locality feature learning,"['S Li', 'W Deng']",2019,127,"Expression in-the-Wild, Static Facial Expression in the Wild","CNN, classification, classifier, deep learning, facial expression recognition, machine learning","multi-label facial expression database, RAF-ML, along with a new deep learning algorithm,  to  To address this limitation, we propose a novel deep learning model, called DBM-CNN, to",No DOI,International Journal of Computer Vision,https://link.springer.com/article/10.1007/s11263-018-1131-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
43,45,Categorization and evaluation of emotional faces in psychopathic women,"['H Eisenbarth', 'GW Alpers', 'D Segrè', 'A Calogero']",2008,117,Karolinska Directed Emotional Faces,classification,"detailed picture of the emotion decoding deficit through the dimensional rating. Furthermore,  the paradigm combines a quite obvious question on emotion in the valence dimension and",No DOI,Psychiatry …,https://www.sciencedirect.com/science/article/pii/S0165178107003265,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Categorization and evaluation of emotional faces in
psychopathic women
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Method
6. 3\. Results
7. 4\. Discussion
8. Acknowledgements
9. References
Show full outline
## Cited by (63)
## Figures (2)
1. 2.
## Tables (2)
1. Table 1
2. Table 2
## Psychiatry Research
Volume 159, Issues 1?2, 30 May 2008, Pages 189-195
# Categorization and evaluation of emotional faces in psychopathic women
Author links open overlay panelHedwig Eisenbarth a, Georg W. Alpers a, Dalia
Segrè b, Antonino Calogero c, Alessandro Angrilli b d
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.psychres.2007.09.001Get rights and content
## Abstract
Psychopathic individuals have been shown to respond less strongly than normal
controls to emotional stimuli. Data about their ability to judge emotional
facial expressions are inconsistent and limited to males. To measure
categorical and dimensional evaluations of emotional facial expressions in
psychopathic and non-psychopathic women, 13 female psychopathic forensic
inpatients, 15 female non-psychopathic forensic inmates and 16 female healthy
participants were tested in an emotion-categorizing task. Emotional facial
expressions were presented briefly (33 ms) or until buttonpress. Participants
were to classify emotional expressions, and to rate their valence and arousal.
Group differences in categorization were observed at both presentation times.Psychopathic patients performed worst with briefly presented sad expressions.
Moreover, their dimensional evaluation resulted in less positive ratings for
happy expressions and less arousal for angry expressions compared with the
responses of non-psychopathic and normal subjects. Results shed light on the
mechanism possibly underlying the emotional deficits in psychopathic women.
* Previous article in issue
* Next article in issue
## Keywords
Psychopathy
Female inmates
Emotion detection
Facial expression recognition
## 1\. Introduction
Psychopathy has been characterized as a clinical construct that comprises,
amongst other characteristics, antisocial deviance and deficient affect (Hare
et al., 1990) that are reflected in shallow affect, lack of empathy, remorse
or guilt, and failure to accept responsibility.
According to the Violence Inhibition Model (VIM; Blair, 1995), distress cues
do not inhibit aggressive behavior in psychopathic people as they do in
healthy people. According to this model, the absent effect of distressing
stimuli is due to a failure of the psychopathic individual to decode emotional
stimuli. As part of the deficient affect, this decoding deficit can be found
in reduced physiological or cortical activations in response to emotional
stimuli (i.e., Kiehl et al., 2001, Benning et al., 2005) as well as in reduced
behavioral responses. In particular, reactions to stimuli with sad or fearful
content evoke weaker reactions in psychopathic persons than in controls (e.g.,
Levenston et al., 2000, Blair et al., 2001). Nevertheless, reported results
vary from no group differences (e.g., Campanella et al., 2005, Glass and
Newman, 2005, Kreklewetz and Roesch, 2005) and even better emotion decoding
performance of psychopathic participants (Habel et al., 2002) to group
differences for specific facial expressions (e.g., Blair et al., 2004,
Montagne et al., 2005). The latter studies found impaired emotion
categorization in psychopathic participants for negative emotional
expressions, but past research included only male participants. To our
knowledge, there are so far no data on emotion detection or emotion
categorization in psychopathic women; therefore, this study investigates
facial affect recognition and evaluations in female psychopaths.
In order to consider most of the aspects of facial picture presentation, we
included facial expressions of all seven basic emotions and presented the
facial expressions for two different presentation durations. In one block,
facial expressions were shown briefly and masked, so as to limit the
processing time for decoding the corresponding emotion category and thereby
revealing deficits in psychopaths' spontaneous emotion categorization compared
with deficits in cognitively elaborated emotion decoding. A condition that did
not limit the time to process the stimuli was included in a second block of
presentations with ad libitum duration, until participants decided which
button they wanted to press.
In addition to this classical categorizing task, based on Ekman's idea of
distinct emotions, we also included a dimensional evaluation task based on the
model of Russell (Lang, 1979, Posner et al., 2005) by asking subjects to rate
their self-perceived valence and arousal levels. The use of these ratings
could provide a more detailed picture of the emotion decoding deficit through
the dimensional rating. Furthermore, the paradigm combines a quite obvious
question on emotion in the valence dimension and a rather indirect question onemotion decoding in the arousal dimension.
In sum, this study examines differences in categorization and evaluation of
emotional facial expressions between psychopathic and non-psychopathic women
and a female control group.
## 2\. Method
### 2.1. Design
This study was conducted in a 3 (group: female psychopathic patients, female
non-psychopathic patients, and female healthy control participants) × 2
(duration: short picture presentation and ad libitum picture presentation) × 7
(emotion category: afraid, angry, disgust, happy, neutral, sad, surprise)
design. The dependent variables were the hit rates and the response latencies
as well as the ratings of the stimuli for valence and arousal levels. Age,
years of education and age-related intelligence, measured by Raven's Standard
Progressive Matrices set A (Raven, 1938), were taken into account as
covariates.
### 2.2. Participants
In this study three groups of participants took part (_n_ = 44). Two groups of
female forensic patients were recruited at a forensic hospital in Northern
Italy (Ospedale Psichiatrico Giudiziario di Castiglione delle Stiviere).
Patients were inmates in the high security psychiatric facility and were
convicted for physical assault or homicide. One more group of healthy control
participants was recruited at the University of Padova, and consisted of
female employees working at the administration offices of the Faculty of
Psychology (_n_ = 16). For the forensic patients the PCL-R scores (Hare et
al., 1990) were assessed and two groups were formed, one with scores greater
than or equal to 30 (_n_ = 13, _M_ = 31.77, _SD_ = 1.17, range: 30?34),
according to the guidelines of Hare et al. (1990), and one with scores below
30 (_n_ = 15, _M_ = 17.40, _SD_ = 6.21, range: 7?28). Patients with psychotic
symptoms were excluded. In both patients groups the primary diagnoses of the
patients were personality disorders. The distribution of the individual
personality disorder diagnoses (histrionic PD, borderline PD, paranoid PD,
schizotypical PD and antisocial PD) was not equal due to the priority given to
the psychopathy score. Since distributions of Psychopathy Checklist-Revised
scores in the normal population are very low, the group of employees was
considered to be low on psychopathy.
The three groups differed significantly in age, due to the _a priori_
categorization criterion based on the psychopathy score (_F_(2,41) = 7.08; _P_
= 0.01). The psychopathic patients (_M_ = 33.00; _SD_ = 7.66) were relatively
younger and differed significantly from non-psychopathic patients (_M_ =
46.67; _SD_ = 14.88) and from the employees (_M_ = 44.19; _SD_ = 5.19). There
were no age differences between non-psychopathic patients and employees.
For the level of education, there was an overall difference between all groups
(_F_(2,41) = 8.24; _P_ = 0.01), which revealed the lowest level of education
in psychopathic patients (_M_ = 8.15; _SD_ = 3.08) and a higher level in non-
psychopathic patients (_M_ = 11.60; _SD_ = 3.70) and employees (_M_ = 13.94;
_SD_ = 4.17). Concerning intelligence, the forensic groups did not differ in
intelligence measured by age- and education-corrected Raven's scores (_T_(26)
= 8.24; _P_ = 0.15) (see Table 1).
Table 1. Numbers of participants, numbers of right-handed/ambidextrous
participants, means and standard deviations for age, education and Raven's
Standard Progressive Matrices
Empty Cell| Psychopathic patients| Non-psychopathic patients| Employees| Sum
---|---|---|---|---
_n_| 13| 15| 16| 44Right-hander/ambidextrous| 10/3| 14/1| 16/0| 40/4
Age| 33.00 (7.66)| 46.67 (14.88)| 44.19 (5.19)| 41.73 (11.47)
Education| 8.15 (3.08)| 11.60 (3.70)| 13.94 (4.17)| 11.43 (4.42)
Intelligence (Raven's)| 83.81 (29.55)| 99.36 (25.99)| ?| 92.14 (28.30)
Written informed consent was obtained from all participants. Each participant
understood that participation was voluntary and would not result in financial
or other gain nor in advantages concerning their imprisonment, and that
consent could be withdrawn at any stage of the study.
### 2.3. Measures
#### 2.3.1. Psychopathy Checklist Revised
The Psychopathy Checklist Revised (PCL-R) contains 20 items on behavioral and
personality features. An expert who draws on information from file review and
from a semi-structured interview rates all items. The validity and reliability
of the PCL-R have been demonstrated repeatedly (Hare, 2003). Harpur and
colleagues (1989) proposed a two-factor model, which is the prevailing idea of
psychopathy and of the PCL-R. The two factors, psychopathic personality and
antisocial behavior, differentiate between personality-related features that
are difficult to measure via self-report and behavioral features that
correlate highly with antisocial personality disorder symptoms and diagnoses.
Another model (Cooke and Michie, 2001) supports three factors derived from
factor analyses, namely arrogant and deceitful interpersonal style, deficient
affective experience and impulsive and irresponsible behavioral style. Thus,
the PCL-R takes into account the pathological behavior as well as the specific
personality style of individuals scoring high on psychopathy.
#### 2.3.2. Raven Standard Progressive Matrices
Raven's Standard Progressive Matrices (SPM; Raven, 1938) are a standard
measure for intelligence. The SPM comprises two main components of general
intelligence in a directly measurable way using robust and directly
interpretable procedures. The SPM tests are made up of a series of diagrams or
designs with a missing figure that should be selected among others for logical
match. Those taking the tests are expected to select the correct part to
complete the designs from a number of options printed underneath. By means of
this task, both components of intelligence, educative and reproductive
ability, can be measured.
### 2.4. Procedure
Before the experimental session, participants gave written consent for their
participation in the study and filled in a questionnaire for demographic
variables. The experimental session was conducted on a Laptop via Presentation
9.70 (Neurobehavioral Systems, 2005) and an external keyboard. Pictures of six
women and six men from the Karolinska Directed Emotional Faces set (KDEF,
Lundqvist et al., 1998) were chosen, each depicting anxious, angry, disgusted,
happy, neutral, sad and surprised expressions in size of 20.78° × 7.98° angle
of vision. In the first block of trials half of the pictures were presented
once in random order for 33 ms and masked by blurred versions of each actor's
neutral expression (Adobe Photoshop 6.0, San Jose). In the second block of
trials, the remaining pictures were shown until a button press of the
participant (ad libitum) was registered. For both blocks the participants had
to choose one of seven buttons, labeled with the nouns of all basic emotions
(anxiety, anger, disgust, happiness, neutral, sadness and surprise) and
neutral that best matched the presented facial expression. The third task was
to rate all pictures for valence (How positive or negative was this picture?)
and arousal (How arousing was this picture?) on scales ranging from ? 4 to 4
and 1 to 9, respectively. After the experimental task, patients were tested
for intelligence with the SPM (Raven, 1938) and received a small gift fortheir participation.
## 3\. Results
### 3.1. Categorization
An analysis of variance (ANOVA) for repeated measures revealed main effects in
categorizing accuracy for duration; the accuracy was higher for ad libitum
than for briefly presented stimuli (_F_(1,41) = 109.31, _P_ < 0.001) (Fig. 1).
1. Download: Download full-size image
Fig. 1. Percent of correct responses for ad libitum (A) and briefly (B)
presented facial expressions (?? _P_ < 0.050).
Another main effect was found for the emotion category; the most accurate
performance was found for happy facial expressions and the least accurate
performance for afraid facial expressions (_F_(6,246) = 51.22, _P_ < 0.001).
The significant main effect for group showed that the employees were the most
accurate, while patients with high psychopathy-scores performed worst
(_F_(2,41) = 4.16, _P_ = 0.02) in the categorizing tasks. The trend for an
interaction of emotion and group further indicates that these main group
differences exist for all emotions except for happy and afraid facial
expressions (_F_(12,246) = 1.75, _P_ = 0.08).
Post hoc tests for the interaction between emotion and group revealed
significant effects between groups for sad, disgusted, neutral and surprised
expressions, but not for happy, afraid or angry ones. Bonferroni-adjusted post
hoc analyses for these group effects showed significant differences in
categorization of sad expressions between psychopathic patients and employees,
of disgusted expressions between non-psychopathic patients and employees, of
neutral expressions between psychopathic patients and employees and of
surprised expressions between psychopathic patients and employees (see Table
2).
Table 2. Mean percent of correct responses and standard deviations for all
conditions and all participant groups; ANOVA results
Empty Cell| Psychopathic patients| Non-psychopathic patients| Employees|
Significance _F_ (_P_)
---|---|---|---|---
Short presentation| Afraid| 23.08 (16.01)| 22.22 (16.27)| 22.92 (15.96)| 0.01
(0.99)
Angry| 41.03 (26.01)| 40.00 (24.23)| 41.67 (29.81)| 0.02 (0.99)
Disgust| 47.44 (39.00)| 35.56 (35.00)| 51.04 (25.44)| 0.91 (0.41)
Happy| 76.92 (16.01)| 81.11 (30.12)| 91.67 (10.54)| 2.01 (0.15)
Neutral| 53.85 (40.34)| 61.11 (39.67)| 78.13 (33.18)| 1.62 (0.21)
Sad| 19.23 (17.80)| 46.67 (31.62)| 48.96 (31.90)| 4.68 (0.02)
Surprise| 47.44 (31.80)| 64.44 (32.65)| 81.25 (25.73)| 4.56 (0.02)
Ad libitum presentation| Afraid| 32.05 (19.79)| 32.22 (22.24)| 39.58 (32.13)|
0.43 (0.66)
Angry| 76.92 (27.67)| 85.56 (26.63)| 91.67 (14.91)| 1.43 (0.25)
Disgust| 52.56 (28.74)| 52.22 (29.46)| 81.25 (14.75)| 6.87 (0.01)
Happy| 93.59 (8.44)| 90.00 (23.40)| 96.88 (6.72)| 0.82 (0.45)
Neutral| 69.23 (32.52)| 82.22 (23.96)| 96.88 (6.72)| 5.30 (0.01)
Sad| 57.69 (24.17)| 66.67 (27.46)| 72.92 (22.67)| 1.35 (0.27)
Surprise| 64.10 (31.80)| 75.56 (32.65)| 88.54 (26.33)| 2.37 (0.11)
Planned contrasts between psychopathic and non-psychopathic patients in the
short presentation condition showed significant differences only for sad
facial expressions (_T_(26) = 2.77, _P_ = 0.04).
Taking into account the group differences in age and education as well as
handedness as covariates, the main effect of group remained significant
(_F_(2,38) = 4.00, _P_ = 0.03), whereas the main effects of duration and ofemotion were no longer significant. The relevance of age for the effects
concerning the categorization task was reflected in the significant main
effect of age (_F_(1,38) = 10.50, _P_ = 0.01) as well as in an interaction of
duration and age (_F_(1,38) = 4.56, _P_ = 0.04).
#### 3.1.1. False responses
To examine the false responses in the categorization task, we calculated
ratios of false positive evaluations and false negative evaluations for each
emotion (_?_ (false positive)/_?_ (false negatives)) as a measure of emotion-
specific response bias. We again found significant differences between short
and ad libitum presentation (_F_(1,41) = 29.96, _P_ < 0.001) as well as a
trend toward a difference between groups (_F_(2,41) = 2.52, _P_ = 0.09),
resulting in an interaction of duration, emotion and group (_F_(12,246) =
2.74, _P =_ 0.01). Thus, psychopathic patients more often erroneously
categorized briefly presented facial expressions as happy with respect to
other emotions (mainly surprise, angry, sad and neutral) and with respect to
the other groups (_F_(2,41) = 5.46, _P_ = 0.01). The ad libitum presented
facial expressions (mainly the afraid ones) were more often categorized as
angry by psychopathic patients than by employees (_F_(2,41) = 2.791 _P_ =
0.07). Non-psychopathic patients more often erroneously categorized ad libitum
presented facial expressions (mainly afraid and sad) as surprise compared with
employees (_F_(2,41) = 3.32, _P_ = 0.05).
### 3.2. Dimensional evaluation
Subjective evaluations of valence and arousal dimensions in pictures were
analyzed for effects of emotion category and group differences as well as for
the kind of dimension (valence vs. arousal).
#### 3.2.1. Valence ratings
For emotional valence evaluations we found a main effect of emotion category
(_F_(6,246) = 77.57, _P_ < 0.001), but no main effect of group nor interaction
of emotion category and group. The subjective evaluation of valence showed
more negative evaluations for afraid, angry, disgusted and sad facial
expressions, more positive evaluations for happy expressions and neutral
evaluations for neutral and surprised expressions (see Fig. 2).
1. Download: Download full-size image
Fig. 2. Means and standard deviations of valence (A) and arousal (B) ratings.
#### 3.2.2. Arousal ratings
The differences between the groups were significant in the arousal ratings
(see Fig. 2). Besides a main effect of emotion category (_F_(6246) = 15.47,
_P_ < 0.001), in terms of more arousal for happy facial expressions and less
arousal for neutral facial expressions compared with all other emotional
facial expressions, we also found a main effect of group (_F_(2,41) = 3.15,
_P_ = 0.05), indicating that the psychopathic group perceived all facial
expressions less arousing than the three remaining groups.
Bonferroni-corrected post hoc tests showed significant differences in arousal
ratings between psychopathic and non-psychopathic patients.
## 4\. Discussion
This study tested the hypothesis that there are differences in emotional
facial expression recognition and evaluation between psychopathic and non-
psychopathic women as well as a control group of employees. Results showed a
decline of categorization accuracy starting from employees, to non-
psychopathic patients and psychopathic patients who performed worse in
categorizing all emotions, except happy. Nevertheless there were no group
differences for categorizing pictures with happy facial expressions, which
were recognized quite well, and pictures with fearful expressions, which were
categorized very poorly by all participants, independent of presentationduration.
In the categorizing task, psychopathic patients differed in their accuracy
from non-psychopathic patients only in one condition: the short presentation
of sad facial expressions. This is in accordance with previous results for men
(Stevens et al., 2001, Fullam and Dolan, 2006). But this also points to the
influence of the presentation duration, which can reduce the time for
cognitively evaluating the stimulus. Thus, the impairment in categorizing
facial expressions could be reduced to conditions of reduced cognitive
elaboration.
More relevant, to our knowledge, this study is the first to report a response
bias, showing that the psychopathic group categorized briefly presented angry,
sad, surprised and neutral facial expressions as happy; instead they
interpreted ad libitum presented afraid expressions as angry. This
misclassification directly contributes to the hypothesis that false
interpretations of emotional facial expressions could be a relevant factor for
antisocial behavior, which is often observed in psychopathic individuals.
Moreover, these results could offer an explanation for the Violence Inhibition
Model (VIM; Blair, 1995) as follows: if sad or afraid facial expressions are
misinterpreted by psychopathic persons as happy or angry stimuli, this
information cannot inhibit aggressive behavior.
Results concerning the dimensional evaluation of emotional facial expressions
are new and provide more evidence for a reduced subjective activation of
psychopathic women in response to emotional expressions. Valence ratings
showed no between-groups differences; indeed psychopathic patients rated
nearly all emotional facial expressions, especially angry, disgusted, neutral
and surprised expressions, as less arousing than students and employees did.
These results, together with data from the literature, suggest that there are
differences between psychopathic and non-psychopathic inmates also at a
subjective level concerning negative emotional contents, not only at a
physiological level. The observed impairment in the perception of emotional
facial expressions might contribute to advance our knowledge about the altered
mechanisms of social interaction, in which psychopathic patients often tend to
display clearly antisocial behavior. Consequently, this impairment raises the
question whether it could be possible to improve psychopaths' ability to
recognize the emotional contents in others' faces via focused training, but
additional studies with larger samples are needed.
A further limitation of these results is that we only tested a female sample.
Even if men and women with psychopathic attributes are both emotionally
impaired (Hamburger et al., 1996, Warren et al., 2003) and, in general,
psychopathic women show reduced physiological responses to emotional stimuli
(Sutton et al., 2002), the conclusions cannot be directly generalized to
psychopathic men. Future research should directly compare male and female
psychopaths. Moreover, the group differences in age and intelligence suggest
that age- and intelligence-matched samples should be included in future
studies, preferably using a correlational approach with self-report measures
(see Eisenbarth and Alpers, 2007).
The question remains, why the fearful facial expressions were categorized
erroneously by all groups independently of the presentation duration, and at
the same time have been rated (as expected) as rather negative and highly
arousing. Reasons for this could be that the facial stimuli did not adequately
represent an effective prototype of a fearful facial expression or,
alternatively, the increased difficulty of attributing the correct emotion
because of the high number of categories among which subjects had to choose.
However, results of this study confirm the hypothesis that, similarly topsychopathic males, psychopathic women are impaired in the perception of
emotional facial expressions.
## Acknowledgements
This study was supported by a dissertation grant of the Konrad-Adenauer-
Stiftung and by a grant from University of Padova project (N. CPDA047438) to
A.A. We thank the Ospedale Psichiatrico Guidiziario di Castiglione delle
Stiviere including patients and staff for collaboration, Prof. Dr. P. Pauli
for useful comments, and the employees of the Università degli Studi di Padova
for uncompensated participation in this study.
Recommended articles"
44,46,Classifying affective states using thermal infrared imaging of the human face,"['BR Nhan', 'T Chau']",2009,221,Affective Faces Database,classifier,"For similar reasons, we also used adjusted accuracy to estimate the performance of the  linear classifier on the unseen test data in the external cross validation. This measure is",No DOI,IEEE Transactions on Biomedical …,https://pubmed.ncbi.nlm.nih.gov/19923040/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
45,47,Classifying emotions and engagement in online learning based on a single facial expression recognition neural network,"['AV Savchenko', 'LV Savchenko']",2022,201,"Acted Facial Expressions In The Wild, Affective Faces Database, Static Facial Expression in the Wild","classifier, deep learning, machine learning, neural network","our models on EngageWild [8], AFEW (Acted Facial Expression In The Wild) [21] and VGAF  (Video- One of the first techniques that applied machine learning and FER to predict student’s",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9815154,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,
46,48,Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature,"['MM Khan', 'RD Ward', 'M Ingleby']",2009,116,Affective Faces Database,"classification, classifier","Similarly, our evoked expression classifier (Table VI) could correctly classify 90% of the  sad faces and 70% faces of disgust. The facial expression of anger could not be",No DOI,ACM Transactions on Applied …,https://dl.acm.org/doi/10.1145/1462055.1462061,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
47,49,"Collecting large, richly annotated facial-expression databases from movies","['A Dhall', 'R Goecke', 'S Lucey', 'T Gedeon']",2012,690,Acted Facial Expressions In The Wild,classification,"facial expression database Acted Facial Expressions in the Wild (AFEW) and its static subset  Static Facial Expressions  and classification techniques. However, in the case of human",No DOI,IEEE multimedia,https://ieeexplore.ieee.org/document/6200254,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
48,50,Color face recognition for degraded face images,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2009,140,Toronto Face Database,classification,", ON M5B 2K3, Canada (e-mail: kostas@comm.toronto.edu). Color versions of one or more   classification tasks [22], [25], [26]? To our knowledge, however, the color effect on face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/4804691,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
49,51,Color local texture features for color face recognition,"['JY Choi', 'YM Ro', 'KN Plataniotis']",2011,206,Toronto Face Database,classification,be used to enhance classification/recognition performance.  color texture methods for  classification tasks on texture images  information can improve classification performance obtained,No DOI,IEEE transactions on image …,https://ieeexplore.ieee.org/iel5/83/4358840/06020798.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
50,52,Combining modality specific deep neural networks for emotion recognition in video,"['SE Kahou', 'C Pal', 'X Bouthillier', 'P Froumenty']",2013,438,Toronto Face Database,"deep learning, neural network","neural network trained to predict emotions from static frames using two large data sets, the  Toronto Face Database and our own set of faces  profile results with deep learning, here we",No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531745,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
51,53,Covariance pooling for facial expression recognition,"['D Acharya', 'Z Huang', 'D Pani Paudel']",2018,216,"Affective Faces Database, Static Facial Expression in the Wild",facial expression recognition,methods on riemannian manifold for emotion recognition in the wild. In Proceedings of the   Image based static facial expression recognition with multiple deep network learning. In Pro,No DOI,… pattern recognition …,https://arxiv.org/abs/1805.04855,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
52,54,Decline or improvement?: Age-related differences in facial expression recognition,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2007,173,Japanese Female Facial Expression,facial expression recognition,"emotion recognition. Currently, there is a growing emphasis on the inseparability between  emotional experiences and emotion recognition, particularly facial expression  the Japanese",No DOI,Biological psychology,https://www.sciencedirect.com/science/article/pii/S0301051106001669,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Decline or improvement?
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Methods
5. 2\. Results
6. 3\. Discussion
7. Acknowledgments
8. Appendix A. Frequencies of erroneous identification of facial expressions of a given emotion as other
emotions
9. References
Show full outline
## Cited by (76)
## Figures (1)
1.
## Tables (5)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table
## Biological Psychology
Volume 74, Issue 1, January 2007, Pages 75-84
# Decline or improvement?: Age-related differences in facial expression
recognition
Author links open overlay panelAtsunobu Suzuki a, Takahiro Hoshino a, Kazuo
Shigemasu a, Mitsuru Kawamura b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.biopsycho.2006.07.003Get rights and content
## Abstract
We examined age-related differences in facial expression recognition in
association with potentially interfering variables such as general cognitive
ability (verbal and visuospatial abilities), face recognition ability, and the
experiences of positive and negative emotions. Participants comprised 34 older
(aged 62?81 years) and 34 younger (aged 18?25 years) healthy Japanese adults.
The results showed not only age-related decline in sadness recognition but
also age-related improvement in disgust recognition. Among other variables,visuospatial ability was moderately related to facial expression recognition
in general, and the experience of negative emotions was related to sadness
recognition. Consequently, age-related decline in sadness recognition was
statistically explained by age-related decrease in the experience of negative
emotions. On the other hand, age-related improvement in disgust recognition
was not explained by the interfering variables, and it reflected a higher
tendency in the younger participants to mistake disgust for anger. Possible
mechanisms are discussed in terms of neurobiological and socio-environmental
factors.
* Previous article in issue
* Next article in issue
## Keywords
Aging
Facial expression recognition
Disgust
Anger
Affective aging has attracted attention among researchers because of its
unique life-span trajectory that is contrasted with cognitive aging. Despite a
predominant view of age-related declines in cognitive functions (Hedden and
Gabrieli, 2004), the socio-emotional selectivity theory proposed by Carstensen
et al. (1999) argues that at least some aspects of emotional functions
_improve_ with advancing age. Specifically, the theory asserts that emotional
experiences in older adults are optimally regulated in terms of the relative
increase in positive emotions against negative emotions (Mather and
Carstensen, 2005). Consistently with the theory, earlier studies examined age-
related differences in daily emotional experiences using self-report
questionnaires and found a decrease in negative emotions as well as
maintenance of or increase in positive emotions (Carstensen and Charles, 1998,
Mroczek, 2001). In addition, older adults reported better emotional control
than their younger counterparts (Gross et al., 1997). Recent experimental
studies also support this view: they show that older adults? attentional
(Mather and Carstensen, 2003, Rosler et al., 2005) and memory (Charles et al.,
2003) biases mask negative emotion elicitors (but see also Comblain et al.,
2004, Kensinger et al., 2002 for contradictory findings on the memory bias).
However, the optimistic view on affective aging has recently been challenged
by the research focusing on the other fundamental aspect of emotional
functions, that is, emotion recognition. Human competency to infer and
recognize others? emotional states ? mostly from non-verbal cues ? underlies
success in interpersonal communication, and it is suggested that this
competency involves age-related decline. Among others, facial expression
recognition is the most thoroughly studied area in emotion recognition, of
which neural and cognitive mechanisms are well documented (Adolphs, 2002,
Calder et al., 2001); therefore, age-related differences in facial expression
recognition have been of great interest.
In particular, the recognition of facial expressions of _basic emotions_
(happiness, surprise, fear, anger, disgust, and sadness; Ekman, 1994, Russell,
1994) has been examined in detail (Calder et al., 2003, MacPherson et al.,
2002, McDowell et al., 1994, Moreno et al., 1993, Phillips and Allen, 2004,
Phillips et al., 2002, Sullivan and Ruffman, 2004). There are cross-nationally
standardized photograph sets for such facial expressions (e.g., Ekman and
Friesen, 1976, Matsumoto and Ekman, 1988), ensuring the comparability of
different experiments by different researchers. Overall, the earlier studies
indicated age-related declines in recognizing facial expressions of specific
basic emotions; age-related declines in anger and/or sadness recognition arethe most prevailing observations (Calder et al., 2003, MacPherson et al.,
2002, McDowell et al., 1994, Moreno et al., 1993, Phillips and Allen, 2004,
Phillips et al., 2002, Sullivan and Ruffman, 2004). This is followed by age-
related decline in fear recognition (Calder et al., 2003, McDowell et al.,
1994, Sullivan and Ruffman, 2004). Interestingly, not only age-related decline
(Sullivan and Ruffman, 2004) but also age-related improvement (Calder et al.,
2003) was observed in the recognition of facial expressions of disgust,
although the authors interpreted the results as preservation instead of
improvement.
The above-mentioned emotion-specific effects of aging have often been
attributed to age-related structural and functional changes in the neural
substrates that are hypothesized to play an important role in the recognition
of specific basic emotions. At present, the involvement of dissociable neural
substrates in the recognition of facial expressions of fear and disgust is
particularly emphasized (Calder et al., 2001). Since Adolphs et al. (1994)
demonstrated a disproportionate impairment of the recognition of fear in a
patient with selective amygdala damage, a number of neurological (Broks et
al., 1998, Calder et al., 1996, Sato et al., 2002) and functional imaging
studies (Morris et al., 1996, Whalen et al., 1998, Yoshimura et al., 2005)
have replicated the link between the amygdala and fear recognition. Damage to
the amygdala also compromises the recognition of other emotions such as anger
and sadness to a certain extent (Adolphs and Tranel, 2004, Fine and Blair,
2000); however, impairment of fear recognition is the most consistent and
disproportionately severe one (Adolphs et al., 1999).
A disproportionate impairment of disgust recognition was first reported in
patients with Huntington's disease (Sprengelmeyer et al., 1996, Sprengelmeyer
et al., 1997), a hereditary neurodegenerative disorder associated with
pathological changes in the basal ganglia and possibly in the insula
(Hennenlotter et al., 2004, Thieben et al., 2002). The proposed contributions
of the two neural substrates to disgust recognition are also confirmed in
functional imaging research (Phillips et al., 1997, Sprengelmeyer et al.,
1998) and in a single-case report (Calder et al., 2000).
Currently, less evidence is available with respect to the recognition of the
other emotions, but a meta-analysis of functional imaging research (Murphy et
al., 2003) highlights the activation of the orbitofrontal cortex in response
to facial expressions of anger.
Considering the involvement of the amygdala in fear recognition, and to a
certain extent in anger and sadness recognition, the aging of the amygdala can
explain age-related decline in recognizing facial expressions of fear (Calder
et al., 2003), anger (Sullivan and Ruffman, 2004), and sadness (MacPherson et
al., 2002). Indeed, a part of the medial temporal lobe structures, including
the amygdala, is suspected to be mildly affected with advancing age (for a
review, Raz, 2000). In line with the structural changes, age-related decrease
in the amygdala activation in response to emotional stimuli (Gunning-Dixon et
al., 2003, Iidaka et al., 2002), particularly to negative ones (Mather et al.,
2004), is demonstrated.
A limitation of the amygdala aging hypothesis is that it may predict the most
severe age-related decline in fear recognition (Adolphs et al., 1999, Calder
et al., 2003), which does not appear to be the case; the age-related decline
is reported most consistently in the recognition of anger and sadness. Thus,
Sullivan and Ruffman (2004) suggest that the aging of the orbitofrontal cortex
may underlie age-related decline in the recognition of anger. The effects of
aging on the orbitofrontal cortex are indeed demonstrated by both structural
(Convit et al., 2001, Raz et al., 1997, Tisserand et al., 2002) and functional(Lamar et al., 2004) examination. On the other hand, for sadness recognition,
no dedicated neural substrates are currently indicated (Murphy et al., 2003)
except the amygdala (Fine and Blair, 2000).
With regard to the preserved recognition of facial expressions of disgust,
Calder et al. (2003) speculates that it may reflect the relative insensitivity
of the globus pallidus to aging. Among other nuclei constituting the basal
ganglia, it is suggested that the globus pallidus is activated most
consistently in response to facial expressions of disgust (Murphy et al.,
2003). However, a recent longitudinal study by Raz et al. (2003) showed that
age-related shrinkage was indeed evident in the whole basal ganglia, although
the shrinkage was milder in the globus pallidus than in the other nuclei
(caudate and putamen). In addition, Good et al. (2001) reported age-related
reduction in the gray matter volume of the insula. Thus, it appears that the
neural substrates involved in disgust recognition may not be very insensitive
to aging and that some other factors may underlie the preserved recognition of
disgust in older adults.
As such, it has been explained that age-related differences in recognizing
facial expressions of specific emotions may reflect some specific
neurobiological factors. However, before identifying the emotion-specific
factors, it is necessary to carefully examine the possibility that the
observed emotion-specificity may stem in part from non-emotional factors. It
is well known that the difficulty levels involved in recognizing facial
expressions substantially differ across emotions. For example, fear is the
most difficult emotion to recognize, and negative emotions as a whole are more
difficult to recognize as compared with happiness and surprise (Biehl et al.,
1997, Russell, 1994). Owing to the differential difficulty levels across
emotions, general cognitive and visual disturbances can disproportionately
impair the recognition of facial expressions of fear (Rapcsak et al., 2000)
and negative emotions (Johnston et al., 2003). Since age-related decline in
facial expression recognition is reported mainly in negative emotions, it is
likely that the decline may be underlain at least in part by age-related
cognitive and visual disturbances.
First, the effects of aging on general cognitive ability are well documented,
and specifically, age-related decline in visuospatial or fluid ability rather
than in verbal or crystallized ability is highlighted (Howieson et al., 1993,
Kaufman et al., 1989). Some aging studies (Phillips and Allen, 2004, Phillips
et al., 2002) actually reported significant relationships between general
cognitive ability and facial expression recognition, although the covariation
alone may not fully account for the age-related differences in facial
expression recognition (McDowell et al., 1994, Sullivan and Ruffman, 2004).
Second, all types of face recognition, including facial expression
recognition, primarily entail visuospatial processing, which was termed as
_structural encoding_ by Bruce and Young (1986). Because aging is known to
affect both the visuospatial processing of faces (Benton and Van Allen, 1968,
Owsley et al., 1981) and neural responses to faces (Grady et al., 1994, Grady
et al., 2000; for a review, Grady, 2000), it is possible that age-related
decline in facial expression recognition may be a part of age-related decline
in face recognition in general. To date, only one study (Sullivan and Ruffman,
2004) examined this issue carefully and found that age-related decline in
facial expression recognition was at least partially independent from the age-
related decline in another type of face recognition (i.e., facial gender
recognition). Given that in clinical research, general deficits in face
recognition (typically, facial identity recognition) contribute to the
impaired recognition of facial expressions (Beatty et al., 1989, Milders etal., 2003), replication concerning this matter will be beneficial.
In addition to these non-emotional confounders, a theoretically interesting
issue is concerned with a link between emotional experiences and emotion
recognition. Currently, there is a growing emphasis on the inseparability
between emotional experiences and emotion recognition, particularly facial
expression recognition (Adolphs, 2002, Goldman and Sripada, 2005). The
emphasized view referred to as the _simulation theory_ proposes that a person
recognizes the emotional state of another person by attempting to generate and
experience (i.e., simulate) the analogous emotional state in himself/herself.
It then follows that a decline in the recognition of certain emotions is
accompanied by a decrease in the experience of those emotions, which is indeed
observed in some neuropathological cases (for reviews, Goldman and Sripada,
2005, Lawrence and Calder, 2003). Similarly, it is possible that age-related
decrease in experiencing negative emotions in general may lead to age-related
decline in recognizing facial expressions of negative emotions. Indeed,
Phillips and Allen (2004) reported that most of the age-related differences in
facial expression recognition could be explained by lower negative emotions
(depression and anxiety) in the older adults.
Thus, aging is associated with a range of general factors that can affect
facial expression recognition, and due to the differential difficulty levels
across emotions, these factors can be responsible for at least part of the
age-related differences in the recognition of specific emotions. To the best
of our knowledge, there have been rare attempts to individually examine the
joint effects of the general factors, and this fact substantially limits our
inference with regard to which age-related differences in facial expression
recognition may truly involve emotion-specific factors. Therefore, the current
study was designed to examine age-related differences in facial expression
recognition in association with general cognitive ability, face recognition
ability in general, and emotional experiences.
## 1\. Methods
### 1.1. Participants
A total of 68 participants were paid to participate in this study. Of these,
34 were older adults (17 men; aged 62?81 years, _M_ = 69.7, S.D. = 4.8) and 34
were younger adults (17 men; aged 18?25 years, _M_ = 20.6, S.D. = 1.8). The
two groups were matched in terms of years of education (older, _M_ = 13.2,
S.D. = 2.6; younger, _M_ = 13.6, S.D. = 1.7; _t_(66) = ?0.836, _p_ = 0.406).
The older participants were recruited from a public human resource center for
the elderly in a special ward of Tokyo. The younger participants were graduate
or postgraduate students from several universities in Tokyo; they were
recruited by means of posted and aural announcements and word of mouth. None
of the participants reported any history of neurological or psychiatric
disorders. For the older participants, the scores of the Mini-Mental State
Examination (MMSE; Folstein et al., 1975) ranged from 26 to 30 (_M_ = 29.0,
S.D. = 1.1), indicating no signs of clinical dementia.
### 1.2. Procedure and tasks
The participants were tested individually in a quiet room and were engaged in
the following five tasks: Facial Expression Identification, two WAIS-R
subtests of Information and Picture Completion, Facial Identity Matching, and
General Affect Scales.1 At the beginning of the experiment, the nature of the
current study was explained to each participant, and written informed consent
was obtained.
#### 1.2.1. Facial Expression Identification
As a test of facial expression recognition, we used a forced-choice
identification (labeling) task that was commonly used in earlier studies(Calder et al., 2003, MacPherson et al., 2002, McDowell et al., 1994, Moreno
et al., 1993, Phillips et al., 2002). The participants viewed the photographs
of prototypical facial expressions enacting the six basic emotions, and they
were asked to identify (label) each photograph as one basic emotion that best
described it.
The photographs used in this task were selected from the Japanese and
Caucasian Facial Expressions of Emotion (JACFEE; Matsumoto and Ekman, 1988).
For each of the six basic emotions, there were eight facial expression
photographs enacted by different persons, of which gender (female or male) and
race (Caucasian or Japanese) were counterbalanced, yielding a total of 48
photographs.
Each photograph was printed in gray scale on a letter size gloss photo paper.
The photographs were presented to the participants in either of two quasi-
random orders, and the orders assigned were counterbalanced within each age
group.
#### 1.2.2. WAIS-R subtests
We used the WAIS-R subtests of Information and Picture Completion as the tests
of verbal (crystallized) ability and visuospatial (fluid) ability,
respectively.
#### 1.2.3. Facial Identity Matching
In order to test face recognition ability in general, we developed a task
named Facial Identity Matching, which is designed as a Japanese version of the
Benton test of face recognition (Benton and Van Allen, 1968). In each trial,
the participants were asked to match a facial photograph (target) with a set
of six facial photographs (references) of different persons of the same
gender.
The photographs used in this task were selected from the Facial Information
Norm Database (FIND; Yoshida et al., 2004) distributed by Nihon University.
The selected photographs were of 60 Japanese people, half of whom were female,
yielding 10 sets of six facial photographs (neutral faces) of different men or
women. There were photographs of the front-view, side-view, and upward face of
each male and female. The Facial Identity Matching task included three
conditions: front-view target with front-view references, front-view target
with side-view references, and upward target with front-view references. Thus,
the task consisted of 30 trials (10 sets × 3 conditions).
Each photograph was printed in gray scale either on a letter size (target) or
an A4-size (references) gloss photo paper. The photographs were presented to
the participants in either of two quasi-random orders, and the orders assigned
were counterbalanced within each age group.
#### 1.2.4. General Affect Scales
A self-report questionnaire of General Affect Scales (Ogawa et al., 2000) was
used as a measure of emotional experiences. The questionnaire included
subscales of Positive Affect and Negative Affect, each of which contained
eight descriptors (adjectives) related to positive and negative emotions,
respectively (e.g., ?Excited? for Positive Affect and ?Afraid? for Negative
Affect). The participants were asked to rate the extent to which they had
experienced each described emotion in the previous month on a 4-point scale,
and summed scores (max score = 32) were calculated for the respective
subscales. The questionnaire was developed on the basis of the Positive and
Negative Affect Schedule (PANAS; Watson et al., 1988) and other existing
instruments, and their reliability and validity were confirmed in a Japanese
population (Ogawa et al., 2000).
## 2\. Results
Fig. 1 provides the mean number of correct identifications by age and emotionin the task of Facial Expression Identification. An analysis of variance was
conducted on the number of correct identifications of the six basic emotions
with two factors of age and emotion. The results showed a marginally
significant main effect of age (_F_(1, 66) = 11.669, _p_ = 0.078) and a
significant main effect of emotion (_F_(5, 330) = 66.802, _p_ < 0.001).
Importantly, the main effects were qualified by a significant interaction
between age and emotion (_F_(5, 330) = 5.000, _p_ < 0.001). Therefore, the
simple main effects of age were examined for respective emotions, indicating a
significant age-related improvement for disgust (_F_(1, 396) = 8.768, _p_ =
0.003) and a significant age-related decline for sadness (_F_(1, 396) =
10.207, _p_ = 0.002). Age-related declines for surprise and anger (for both,
_F_(1, 396) = 2.939, _p_ = 0.087) were only marginal, and age-related
differences were insignificant for happiness and fear (_p_ > 0.1).
1. Download: Download full-size image
Fig. 1. Mean number (±S.E.) of correct identifications in the task of Facial
Expression Identification by age and emotion. HA: happiness, SU: surprise, FE:
fear, AN: anger, DI: disgust, SA: sadness.
The performances of the older and younger participants in the other four tasks
are summarized in Table 1. The scores the two WAIS-R subtests were
consistently lower in the older participants than in the younger participants,
and statistically, the age-related decline was marginal in Information
(_t_(66) = ?1.764, _p_ = 0.082) and was highly significant in Picture
Completion (_t_(66) = ?4.563, _p_ < 0.001). The results probably reflect
steeper age-related decline in visuospatial ability than in verbal ability
(Howieson et al., 1993, Kaufman et al., 1989). The age-related difference was
also significant in Facial Identity Matching (_t_(66) = ?3.245, _p_ = 0.002),
indicating age-related decline in face recognition ability in general. With
regard to General Affect Scales, age-related decrease in Negative Affect
(_t_(66) = ?2.713, _p_ = 0.008) as well as the maintenance of Positive Affect
(_t_(66) = 0.264, _p_ = 0.793) was observed, which is consistent with the
earlier literature (Carstensen and Charles, 1998, Mroczek, 2001).
Table 1. Means (±S.E.) for the scores of two WAIS-R subtests, Facial Identity
Matching, and General Affect Scales
Empty Cell| Older| Younger
---|---|---
WAIS-R subtests
Informationa| 17.85 ± 0.75| 19.65 ± 0.69
Picture Completiona| 11.53 ± 0.56| 14.53 ± 0.34
Facial Identity Matching| 23.35 ± 0.60| 25.76 ± 0.44
General Affect Scales
Positive Affect| 26.03 ± 0.71| 25.76 ± 0.70
Negative Affect| 16.00 ± 0.77| 19.12 ± 0.85
a
Raw scores.
Table 2 shows correlations between the number of correct identifications of
respective emotions in Facial Expression Identification on one hand and the
remaining five measures on the other hand for each age group. Happiness and
surprise were excluded from the correlation analyses because ceiling effects
and poor variances are indicated in Fig. 1. The score of Picture Completion
had the most consistent relationship with the performances in Facial
Expression Identification across age and emotion. Specifically, in the younger
participants, the positive correlations between the two tasks were significant
or marginally significant for any emotion, whereas in the older participants,
all the correlations were positive but reached the significance level only fordisgust. As compared with Picture Completion, Information appeared to have a
less marked relationship with Facial Expression Identification. Likewise, the
correlations between the score of Facial Identity Matching and performances in
Facial Expression Identification were not significant for any emotion. With
respect to emotional experiences, the positive correlation between Negative
Affect and Facial Expression Identification was noted only for sadness, and it
reached the significance level in the younger participants.
Table 2. Correlations between performances in Facial Expression Identification
and the other five measures
Empty Cell| FE| AN| DI| SA
---|---|---|---|---
Older
Information| 0.160| ?0.217| 0.284| 0.223
Picture Completion| 0.137| 0.212| 0.378*| 0.050
Facial Identity Matching| 0.226| 0.134| 0.095| 0.167
Positive Affect| ?0.129| 0.186| 0.119| ?0.165
Negative Affect| ?0.050| 0.004| ?0.081| 0.242
Younger
Information| 0.199| ?0.087| 0.159| 0.047
Picture Completion| 0.411*| 0.289?| 0.381*| 0.543**
Facial Identity Matching| ?0.168| ?0.009| 0\. 141| 0.101
Positive Affect| 0.069| 0\. 058| ?0.124| ?0.003
Negative Affect| ?0.009| 0\. 150| ?0.018| 0.340*
_Note_ : FE: fear, AN: anger, DI: disgust, SA: sadness.
? _p_ < 0.10. *_p_ < 0.05. **_p_ < 0.01.
In order to clarify the unique contribution of age to disgust and sadness
identification, regression analyses were conducted on the number of correct
identifications of disgust or sadness as the dependent variable; the scores of
Information, Picture Completion, Facial Identity Matching, Positive Affect,
Negative Affect, and age (dummy variable; older = 1, younger = 0) were entered
as the independent variables. Table 3 summarizes the results for the
regression analyses. With regard to disgust, the addition of age resulted in a
significant increment in _R_ 2 (? _R_ 2 = 0.144, _F_(1, 61) = 11.110, _p_ =
0.001), indicating the unique contribution of age. In line with Table 2, the
score of Picture Completion was another significant predictor of disgust
identification. On the other hand, the addition of age did not result in a
significant increment in _R_ 2 for sadness (? _R_ 2 = 0.004, _F_(1, 61) =
0.361, _p_ = 0.550). Table 3 indicated that the score of Negative Affect was a
significant predictor of sadness identification, as was expected from Table 2.
Table 3. Summary of regression analyses on the number of correct
identifications of disgust or sadness
Empty Cell| Disgust| Sadness
---|---|---
Empty Cell| _B_| S.E. _B_| _?_| _B_| S.E. _B_| _?_
Information| 0.080| 0.064| 0.154| 0.029| 0.052| 0.068
Picture Completion| 0.242| 0.098| 0.337*| 0.119| 0.079| 0.199
Facial Identity Matching| 0.089| 0.085| 0.131| 0.055| 0.069| 0.098
Positive Affect| 0.004| 0.064| 0.008| ?0.065| 0.052| ?0.144
Negative Affect| ?0.038| 0.056| ?0.085| 0.111| 0.046| 0.298*
Age| 2.081| 0.624| 0.473**| ?0.302| 0.503| ?0.083
| _R_ 2 = 0.210 (_p_ = 0.021)| _R_ 2 = 0.254 (_p_ = 0.005)
_Note_ : All the independent variables were forcibly entered into the
regression model.
*_p_ < 0.05. **_p_ < 0.01.We then analyzed whether some age-related differences in erroneous
identification underlay age-related improvement in disgust identification. A
trivial possibility was that the older participants? preferential use of the
disgust label might lead to an apparent improvement in disgust identification
(Calder et al., 2003). Indeed, the number of older participants misusing the
disgust label was marginally significantly larger than that of the younger
participants (older, _M_ = 3.88, S.E. = 0.38; younger, _M_ = 2.91, S.E. =
0.38; _t_(66) = 1.811, _p_ = 0.075). However, the correlation between the
number of correct identifications of disgust and the number of such misuse was
neither significant nor positive (_r_ = ?0.139, _t_(66) = ?1.140, _p_ =
0.259).
Another possibility was that the younger participants might tend to make some
specific errors in identifying facial expressions of disgust. Table 4
summarizes the frequencies of erroneous identifications mistaking facial
expressions of disgust to be other emotions.2 Table 4 reveals that the younger
participants misused the anger label almost twice as often as the older
participants did, and statistically, the mean of such errors was significantly
larger in the younger participants (older, _M_ = 1.53, S.E. = 0.25; younger,
_M_ = 2.79, S.E. = 0.42; _t_(66) = ?2.568, _p_ = 0.012). As a natural
consequence, the number of correct identifications of disgust and the number
of erroneous identifications as anger were negatively correlated almost
perfectly (_r_ = ?0.980, _t_(66) = ?40.008, _p_ < 0.001).
Table 4. Frequencies of erroneous identification of facial expressions of
disgust as other emotions
Empty Cell| HA| SU| FE| AN| SA
---|---|---|---|---|---
Older| 0| 1| 5| 52| 2
Younger| 0| 0| 1| 95| 2
_Note_ : HA: happiness, SU: surprise, FE: fear, AN: anger, SA: sadness.
## 3\. Discussion
In the current study, we found not only age-related decline in the recognition
of facial expressions of sadness but also age-related improvement in the
recognition of facial expressions of disgust. The results of the current study
are consistent with those of earlier studies (Calder et al., 2003, MacPherson
et al., 2002, McDowell et al., 1994, Moreno et al., 1993, Phillips and Allen,
2004, Phillips et al., 2002, Sullivan and Ruffman, 2004). Although age-related
decline in anger recognition has been reported as frequently as that in
sadness recognition, it was only marginally significant in the current study.
However, we believe that age-related improvement in disgust recognition and
age-related decline in anger recognition may be closely related; which will be
discussed in detail later. We also failed to replicate age-related decline in
fear recognition (Calder et al., 2003, McDowell et al., 1994, Sullivan and
Ruffman, 2004), which may reflect less robust effects of aging on fear than on
anger and sadness (Sullivan and Ruffman, 2004). As seen in earlier studies,
performances in the recognition of happiness and surprise indicated ceiling
effects, and thus it may be inappropriate to discuss age-related differences
in them based on the present data.
We then explored whether facial expression recognition had any relationship
with other mental functions that were sensitive to aging, such as general
cognitive ability, the visuospatial processing of faces, and emotional
experiences. First, we found that visuospatial ability, and not verbal
ability, was consistently related to facial expression recognition. The
results are plausible considering that facial expression recognition is
visuospatial in nature (Buitelaar et al., 1999, Haxby et al., 2000) and maypreferentially involve the right hemisphere (Damasio et al., 2000), although
the differential involvements of visuospatial and verbal abilities have not
been documented in the aging research (Phillips and Allen, 2004, Phillips et
al., 2002). Intriguingly, the results also indicated that the relationship
between facial expression recognition and visuospatial ability was
consistently reliable in the younger participants, but not in the older
participants. Such age-related difference is not readily explicable, and thus
replication is necessary in order to determine its robustness. Despite the
reservations, we can safely recommend that an investigation into the effects
of aging on facial expression recognition should include a measure of
visuospatial ability, given its marked age-related decline (Howieson et al.,
1993, Kaufman et al., 1989).
Second, we found no evidence for the relationship between facial expression
recognition and the visuospatial processing of faces measured by Facial
Identity Matching. The findings are consistent with the work of Sullivan and
Ruffman (2004), demonstrating that age-related decline in facial expression
discrimination was independent from that in facial gender discrimination.
Cumulatively, facial expression recognition may be related more to high-level
visuospatial cognition, which requires manipulation of and reasoning about
visual stimuli (e.g., Picture Completion; Cummings and Huber, 1992), than to
elementary visuospatial perception that is tapped by unfamiliar face matching
such as in the Benton test and in this study (see also Gagliardi et al.,
2003).
Third, from the simulation theory, we expected that the less experiences of
negative emotions might lower the recognition of facial expressions of
negative emotions, but the expectation was only partially supported. In other
words, we found that the score of Negative Affect was positively correlated
only with the performance in sadness recognition. The findings indicate that
reduction in negative emotions in general may not have remarkable effects on
the recognition of facial expressions of negative emotions, which supports the
view that the relationship between emotion recognition and emotion experience
is more emotion-specific: the relationship based on a discrete emotion (e.g.,
fear, anger, sadness) as a unit (Goldman and Sripada, 2005, Lawrence and
Calder, 2003).
We further analyzed whether or not the observed age-related differences in
disgust and sadness recognition were statistically accounted for by these
other age-sensitive functions. The results showed that age-related improvement
in disgust recognition was not explained by the other functions; rather, a
positive association between disgust recognition and visuospatial ability
suggests that age-related decline in the latter might have obscured age-
related improvement in the former in earlier studies with the exception of the
study by Calder et al. (2003). In contrast, age-related decline in sadness
recognition was statistically explained by age-related decrease in the score
of Negative Affect. Phillips and Allen (2004) reported similar results in that
age-related decline in emotional intensity rating for facial expressions of
sadness was attributable to age-related decrease in anxiety and depression.
Moreover, MacPherson et al. (2002) demonstrated that age-related decline in
the recognition of sadness disappeared when memory performance was partialed
out. These findings suggest that age-related decline in sadness recognition
may not reflect specific factors intimately linked with the emotion of sadness
but may reflect general factors influencing a range of mental functions. A
definitive conclusion on this matter should await further research. At
present, we can safely say that age-related improvement in disgust recognition
is far more likely to require some specific factors.Calder et al. (2003) also observed the improved levels of disgust recognition
in older adults (but see also Sullivan and Ruffman, 2004), which are
consistent with our findings, but they interpreted their results as the
evidence of preservation of disgust recognition rather than as the evidence of
improvement. However, with our clear replication for the Japanese population,
it may be more plausible to believe that older adults are able to better
recognize facial expressions of disgust, at least in a forced-choice
identification task. Then, what types of mechanisms underlie the observed age-
related improvement in disgust recognition? It may appear slightly difficult
to speculate that some factors directly enhance the sensitivity to the emotion
of disgust in older adults. An ongoing study using an intensity rating task
(Suzuki et al., 2006, Suzuki et al., 2005) indicates that in reality, older
adults are not more sensitive to disgust than younger adults, by showing that
the intensity rating of disgust for facial expressions containing disgust is
not higher in older adults. In this regard, we believe that another aspect of
our current findings may serve as a clue: the higher tendency in younger
participants to mistake facial expressions of disgust to be anger.
It has thus far been argued that younger adults are better able to recognize
facial expressions of anger (Calder et al., 2003, McDowell et al., 1994,
Phillips et al., 2002, Sullivan and Ruffman, 2004). However, at the same time,
we can speculate that the younger adults? high sensitivity to the emotion of
anger may lead to frequent false alarms. Phillips and Allen (2004) empirically
support this view by showing that younger adults rated ?neutral? faces as
those expressing a higher intensity of anger than the older adults did.
As mentioned in the introduction, age-related decline in anger recognition has
been linked with the effects of aging on specific neural substrates such as
the amygdala and the orbitofrontal cortex (Calder et al., 2003, Sullivan and
Ruffman, 2004). Because the aging of the amygdala will primarily affect the
recognition of fear (Adolphs et al., 1999, Calder et al., 2003), the aging of
the orbitofrontal cortex appears to be more attractive in explaining age-
related decline in anger recognition (Sullivan and Ruffman, 2004), given its
specific contribution to the recognition of anger (Murphy et al., 2003). It is
well-documented that, among other brain regions, the prefrontal cortex is the
most sensitive to the effects of aging (Raz, 2000). A number of studies
investigating the sub-regions of the prefrontal cortex further reveal a
significant negative correlation between age and the volumes of the
orbitofrontal cortex (Convit et al., 2001, Raz et al., 1997, Tisserand et al.,
2002; but see also Salat et al., 2001). The structural changes are supported
by a functional imaging study (Lamar et al., 2004) demonstrating that the
older adults failed to activate the orbitofrontal cortex during delayed match
and non-match to sample tasks, which were previously shown to engage the
orbitofrontal cortex in the younger adults (Elliott and Dolan, 1999). Thus, it
is likely that age-related decline in anger recognition may be underlain by
such structural and functional changes in the orbitofrontal cortex with
advancing age.
In addition to the neurobiological factors, socio-environmental factors may
also contribute to age-related decline in the recognition of anger. Carstensen
and Charles (1998) and Carstensen et al. (1999) argue that advancing age or an
appraisal process in which people perceive their lifetime to be limited, is
associated with motivational shifts that prioritize present-oriented emotional
comfort over future-oriented knowledge acquisition. The motives are
accomplished by a proactive pruning process of social networks that
selectively emphasizes familiar social partners (Carstensen et al., 1999),
which results in decreasing experiences of negative emotions. In particular,there are surveys indicating a negative association between age and anger
experiences (Birditt and Fingerman, 2003, Mirowsky and Ross, 1995, Schieman,
1999). Schieman (1999) showed that the less frequent experiences of anger
among older adults were attributable to age-related differences in the
psychosocial and structural environment such as freedom from work and family
roles. In addition, it is argued that older adults are motivated to control
anger experiences specifically because anger is the only negative emotion that
involves blaming others, which is potentially harmful for social relationships
(Birditt and Fingerman, 2003).
The simulation theory will explain that age-related decline in anger
recognition may be partly due to the negative association between age and
anger experiences. We examined the effects of age-related differences in
emotional experiences on facial expression recognition by using broad measures
of Positive Affect and Negative Affect. However, the descriptors constituting
the Negative Affect subscale were primarily related to anxiety and tension,
and thus possibly failed to capture anger experiences. Earlier studies suggest
that the experiences of negative emotions such as fear, anger, and disgust can
be impaired separately after focal brain damage (Calder et al., 2000, Calder
et al., 2004, Sprengelmeyer et al., 1999). Therefore, the use of a measure
specific to anger experiences will be beneficial in future research.
Thus far, the explanation based on the neurobiological factors has been
dominant, and age-related decline in anger recognition is regarded as a type
of ?deficit? caused by the aging brain. However, it is also likely that the
older adults? lower performance in anger recognition may result from some
adaptive processes in response to their ?anger-less? environment, rather than
from passive, deteriorative processes. Thus, older adults may be more cautious
about the recognition of anger than younger adults as suggested by Phillips
and Allen (2004) and the current study. Eventually, the neurobiological and
socio-environmental factors are not mutually exclusive. Further, the most
challenging topic will be to capture age-related changes in anger recognition
in the realm of brain?environment interactions.
Finally, we would like to note some limitations of this study. First, due to
its nature as an extreme group design, the current findings are not free from
cohort effects and are insensitive to the non-linear effects of aging
(Salthouse, 2000). The latter aspect is particularly important because Calder
et al. (2003) contrasted the non-linearity of age-related improvement in
disgust recognition with the linearity of age-related decline in fear
recognition. Second, the universality/culture specificity of the current
findings may merit further investigation giving the well-known differences in
facial expression recognition between the Japanese and Caucasian populations
(Matsumoto, 1992, Russell, 1994, Shioiri et al., 1999). We believe that our
findings on the link between age-related differences in disgust and anger
recognition may be rather universal phenomena because Calder et al. (2003)
also observed age-related improvement in disgust recognition and because the
confusion between facial expressions of disgust and anger is also very common
in the Caucasians (Ekman and Friesen, 1976). Third, the relationships between
age-related differences in facial expression recognition and those in other
emotional functions such as the recognition of emotional prosody (Brosgole and
Weisman, 1995) and lexical stimuli (Grunwald et al., 1999, Phillips and Allen,
2004), expressive behaviors (Borod et al., 2004, Nakamura and Masutani, 2001),
and emotional control (Gross et al., 1997) are unclear. Both the
neurobiological and socio-environmental factors will predict coupled changes
in affective processing as a whole (Calder et al., 2001, Carstensen et al.,
1999), which will be worth empirical examination in the future.## Acknowledgments
We gratefully acknowledge Arvid Kappas, Andy Calder, and the three anonymous
reviewers for their valuable comments on the earlier version of the
manuscript. We would like to thank H. Yamada and N. Watanabe for allowing us
to use the FIND. We would also like to thank N. Suzuki and T. Ogawa for
granting us permission to use the General Affect Scales. Portions of this work
were presented at the 6 _th Tsukuba International Conference on Memory_ ?
_Memory and Emotion_ , Tsukuba, Japan, and at the 13th meeting of the _Japan
Society for Research on Emotions_ , Nagoya, Japan. The first author was
supported by the 2004 research grant from Meiji Yasuda Mental Health
Foundation. The last author was supported by the Showa University Grant-in-Aid
for Innovative Collaborative Research Projects, Core Research for Evolutional
Science and Technology (CREST) and a Grant-in-Aid for Scientific Research on
Priority Areas (17022035) from the Japanese Ministry of Education, Culture,
Sports, Science and Technology.
## Appendix A. Frequencies of erroneous identification of facial expressions
of a given emotion as other emotions
Group| Stimulus| Response
---|---|---
Empty Cell| Empty Cell| HA| SU| FE| AN| DI| SA
Older| HA| ?| 1| 0| 0| 0| 1
| SU| 12| ?| 18| 3| 0| 1
| FE| 0| 119| ?| 15| 7| 10
| AN| 0| 0| 5| ?| 73| 21
| DI| 0| 1| 5| 52| ?| 2
| SA| 0| 1| 8| 21| 52| ?
Younger| HA| ?| 0| 0| 0| 0| 0
| SU| 4| ?| 6| 1| 0| 1
| FE| 0| 103| ?| 9| 8| 11
| AN| 0| 0| 1| ?| 61| 15
| DI| 0| 0| 1| 95| ?| 2
| SA| 0| 1| 2| 8| 30| ?
_Note_ : HA: happiness, SU: surprise, FE: fear, AN: anger, DI: disgust, SA:
sadness.
Recommended articles"
53,55,"Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond","['D Kollias', 'P Tzirakis', 'MA Nicolaou']",2019,440,"Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, machine learning",", we show that our network can be also used for other emotion  -Face network, pre-trained  for face recognition on the VGG- We note that all deep learning architectures have been",No DOI,International Journal of …,https://arxiv.org/abs/1804.10938,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
54,56,Deep convolution network based emotion analysis towards mental health care,"['Z Fei', 'E Yang', 'DDU Li', 'S Butler', 'W Ijomah', 'X Li', 'H Zhou']",2020,150,Karolinska Directed Emotional Faces,CNN,expression databases including the Karolinska Directed Emotional Faces (KDEF) Database  [25] CNN works as the deep feature extractor. The advantages of using a pre-trained CNN to,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231220300783,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Deep convolution network based emotion analysis towards
mental health care
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Materials and methods
6. 3\. Results
7. 4\. Discussion
8. 5\. Conclusion
9. CRediT authorship contribution statement
10. Declaration of Competing Interest
11. Acknowledgements
12. Ethical approval
13. References
14. Vitae
Show full outline
## Cited by (111)
## Figures (9)
1. 2. 3. 4. 5. 6.
Show 3 more figures
## Tables (14)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Neurocomputing
Volume 388, 7 May 2020, Pages 212-227
# Deep convolution network based emotion analysis towards mental health care
Author links open overlay panelZixiang Fei a, Erfu Yang a, David Day-Uei Li b,
Stephen Butler c, Winifred Ijomah a, Xia Li d, Huiyu Zhou e
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2020.01.034Get rights and content## Abstract
Facial expressions play an important role during communications, allowing
information regarding the emotional state of an individual to be conveyed and
inferred. Research suggests that automatic facial expression recognition is a
promising avenue of enquiry in mental healthcare, as facial expressions can
also reflect an individual's mental state. In order to develop user-friendly,
low-cost and effective facial expression analysis systems for mental health
care, this paper presents a novel deep convolution network based emotion
analysis framework to support mental state detection and diagnosis. The
proposed system is able to process facial images and interpret the temporal
evolution of emotions through a new solution in which deep features are
extracted from the Fully Connected Layer 6 of the AlexNet, with a standard
Linear Discriminant Analysis Classifier exploited to obtain the final
classification outcome. It is tested against 5 benchmarking databases,
including JAFFE, KDEF,CK+, and databases with the images obtained ?in the
wild? such as FER2013 and AffectNet. Compared with the other state-of-the-art
methods, we observe that our method has overall higher accuracy of facial
expression recognition. Additionally, when compared to the state-of-the-art
deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the
proposed method demonstrated better efficiency and has less device
requirements. The experiments presented in this paper demonstrate that the
proposed method outperforms the other methods in terms of accuracy and
efficiency which suggests it could act as a smart, low-cost, user-friendly
cognitive aid to detect, monitor, and diagnose the mental health of a patient
through automatic facial expression analysis.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
Deep convolution network
Mental health care
Emotion analysis
## 1\. Introduction
Understanding people's emotions plays an important role in daily human
communications. For advanced human-computer interaction in many emerging
applications, recognizing users? emotions is also vital. Currently, there are
many approaches for automated emotional recognition, including the recognition
of facial expression and analysis of voice tone [1], which exist alongside
more conventional physiological measures such as measurements of blood
pressure, pulse rate or skin conductivity.
Automated facial expression recognition has found many practical applications
such as in e-learning and health care systems [2,3]. For example, facial
expressions are considered as an important feedback mechanism for teachers in
terms of monitoring levels of students? understanding [3]. Within this domain,
Mau-Tsuen et al. proposed an automatic system to identify how well students
were learning, by means of the analysis of videos taken from learners [3], and
demonstrated that such a system could help improving teaching effectiveness
and efficiency. Additionally, the user interface developed in such a system
could work as a cognitive tool [4] to support and understand the users? mental
state better when and where it was appropriate without external actuation.
Further applications of facial expression analysis include its use in the
fields of human-computer interaction and interface optimization. Within these
domains, Bahr et al. proposed a novel method to analyze postural and facial
expressions to guide interface actuation and actions [4], whilst Pedro et al.employed electromyogram (EMG) sensors to investigate the relationship between
users? facial expressions and adverse-event occurrences [5]. Moreover, health
and medical applications of automated facial expression analysis are also
being investigated, for example in the area of diagnostics related to
developmental disorders such as autism. Here, promising work has been
conducted by means of a system of facial expression analysis during social
interactions, where many individuals with such disorders find challenging [6].
Modern techniques in facial expression recognition systems play important
roles in these practical applications. These techniques typically involve
multiple components, including face localization and facial component
alignment, facial feature extraction and facial feature classification. As a
simple dichotomy, facial expression analysis systems can be divided into two
groups: those using static images and those using continuous video frames.
Static algorithms enjoy the advantage of facial expression recognition from a
single image or a video frame. In this paper, we describe an algorithm which
has been tested for facial expression recognition accuracy from single images
mainly acquired through webcams and mobile phone cameras. We would argue that
static approaches show promising, but require rigorous and various testing
conditions for robust outcomes. Deepak et al., for example, proposed a
convolution neural network based algorithm to recognize facial expression with
high accuracy [7]. However, their algorithm was tested on only two small
facial expression datasets. Conversely, Jie et al. also proposed three novel
convolution neural network (CNN) models with different architectures [8], and
tested their algorithms on several datasets such as CK+ and FER2013 datasets.
Within the three architectures they presented, the data suggested that the
pertained CNN with 5 convolution layers had the best performance. It should be
noted however that not all researchers restrict their work to 2D images. For
instance, Chenlei et al. very recently proposed a new 3D facial expression
modeling method based on facial landmarks [9]. On the other hand, other
researchers have moved beyond the analysis of static images and proposed
facial expression recognition systems using continuous video frames. For
instance, Zia, Lee and other researchers worked on facial expression
recognition using temporal dynamics [10,11]. Their proposed system used Fisher
Independent Component Analysis as a feature extractor and Hidden Markov Models
to learn the features of six different expressions. The experiment results
showed that the recognition rate was about 92.85%.
Despite advances in the recognition accuracy, there remain significant issues
to be addressed in the field of facial expression recognition systems. The
first issue is related to the datasets employed in testing such systems. Some
facial expression recognition systems may have good performance in some image
datasets, but perform poorly in the others. For instance, deep CNN approaches
often need to determine large amount of weights in the training phase.
Consequently it is observed that such approaches suffer from performance
decrements when being trained on a small image dataset [8]. A second important
issue is ecological validity, in that most of the existing systems use lab-
posed facial expressions images. This is potentially problematic, as it
ignores real-world problems such as lighting conditions, image quality and
background complexity. It is fatal to address such issues for real-world
applications. Facial expression recognition in the wild is a challenging topic
due to such issue as well as others such as variance in poses [8]. Thirdly,
both traditional approaches and deep learning based approaches have inherent
weaknesses. Traditional approaches such as Local Binary Pattern (LBP), Scale
Invariant Feature Transform (SIFT) and Support Vector Machine (SVM) employ
manually designed features, which can result in poor performance with unseenimages. Deniz et al., for instance, observed that traditional approaches
performed weakly when being presented with variations in pose, and instead
employed a deep CNN based approach to recognize facial expressions, which they
argued resulted in improved performance [12]. Approaches such as AlexNet, Vgg,
and GoogleNet have long training and testing time as well as an enormous
amount of memory resource [13] and require hardware incorporating Graphics
Processing Units (GPU's), whilst it is also essential in these approaches.
Extensive literature provides a comprehensive review of the relative
advantages and weaknesses of the existing facial expression systems, we would
recommend those seeking further information to consult one of the several good
reviews of the area (see [14], [15], [16], [17], [18]). Indeed, even more
recently Byoung, reviewed approaches to facial emotion recognition including
conventional facial expression recognition, deep learning based facial
expression recognition, well-known facial expression datasets and has also
examined some common performance evaluation methods for automated facial
expression recognition [19].
Whilst there are many notable developments within the field of automatic face
recognition systems, there remain urgent priorities to be addressed to allow
these technologies to flourish within the field of mental health care. We
would argue that there is vast untapped potential for the field in this area
of healthcare. Globally, there are rising numbers of people suffering from
cognitive impairment, and consequently there is an urgent need to develop and
deploy user-friendly, low-cost and effective facial expression analysis
systems for mental health care. Within mental health care, there is enormous
potential for automatic facial expression recognition systems to assist
clinicians working within mental health settings. Whilst it is uncontroversial
to suggest that facial expressions can reflect people's mental states [20], it
has also been reported that the patients with cognitive impairments may
express abnormal facial expressions [21] that are quantitatively different
from those of healthy elderly people. Burton and Kaszniak, for example,
reported abnormal corrugator activity in individuals with cognitive impairment
when compared to a control group, when these participants were exposed to
image or video stimuli [21]. The forehead muscle is integral to the emitted
emotions related to frowning [22]. Work in line with this was conducted by
Henry et al., who invited 20 cognitive impaired people and 20 healthy people
to watch videos in order to compare their facial reactions when viewing such
stimuli, reporting that the group of participants with cognitive impairment
demonstrated difficulty both in facial muscle control and in the amplification
of expressed emotion [23]. Similarly, Smithfound cognitive impaired people
demonstrated more negative emotions when they were exposed to negative image
stimuli, which they argued was indicative of reduced emotional control [24].
Building upon such initial findings, this paper presents a proposed system
that has the potential to be applied to the detection and monitoring of
cognitive impairments such as dementia through the application of machine
learning techniques to the automatic analysis of people's facial expressions.
Our proposed smart system is able to label and quantify the users? emotions
automatically, through continuous focus upon the evolution of facial
expressions over a period of time.
In order to develop an efficient and effective framework for recognizing
facial expression towards mental health care, this paper presents a facial
expression recognition framework which effectively exploits the features
extracted from the Fully Connected Layer 6 of AlexNet and the Linear
Discriminant Analysis Classifier (LDA). We present the findings from a series
of experiments where the performance of our proposed method are compared tothat of traditional approaches such as SVM, LDA, the K-nearest Neighbor
Classifier (KNN) and made further comparisons to deep learning based
approaches such as AlexNet, Vgg, GoogleNet and ResNet. Within our comparisons,
we have employed a broad range of well-known facial expression databases
including the Karolinska Directed Emotional Faces (KDEF) Database [25], the
Japanese Female Facial Expression (JAFFE) Database [26], the extended Cohn-
Kanade dataset (CK+) [27,28], the Facial Expression Recognition Challenge
(FER2013) [29] and the AffectNet Database [30]. From our experiments it would
suggest that exploiting the features extracted from the Fully Connected Layer
6 of AlexNet with the LDA classifier can achieve the best performance in all
the five testing databases.
The major contributions of this paper are as follows:
* 1.
First, our novel framework for facial expression analysis to support mental
health care is presented. Our proposed system extracts deep features from the
Fully Connected Layer 6 of the AlexNet, and uses a standard Linear
Discriminant Analysis Classifier to train these deep features. As it is known
that patients with cognitive impairments may express abnormal facial
expressions when exposed to emotional visual stimuli, we would argue that our
proposed facial analysis system has excellent potential to detect cognitive
impairment at the early stage.
* 2.
We present the findings from the tests of our proposed framework against both
across databases with small number of images such as the JAFFE, KDEF and CK+
databases, and databases with images obtained ?in the wild? such as the
FER2013 and the AffectNet databases.
* 3.
We present the findings from the comparisons of our proposed method with both
traditional methods and state-of-the-art methods proposed by other
researchers. It is observed that our method has better accuracy facial
expression recognition. More importantly, we have also observed that our
proposed method has much less computing time and lower device requirements
than the other state-of-the-art deep learning algorithms such as Vgg16,
GoogleNet, and ResNet. We would argue that these characteristics support our
view that the proposed method is both competent and suitable for a facial
analysis system that can be employed within mental health care settings.
* 4.
A system which can analyze facial expressions from video stimuli, and
subsequently produce an accurate evaluation of the facial expressions detected
in an automated system is presented.
This paper is organized as follows. Following our general overview in Section
1, we present our proposed deep learning-based framework in Section 2. Section
3 then introduces the experimental set-up for the evaluation of the proposed
system to obtain the results. Our findings are then discussed within Section
4. Finally, Section 5 provides our conclusions.
## 2\. Materials and methods
### 2.1. Overview of the whole system structure
In the current research, the inputs to the system are videos of people's
frontal faces. Image pre-processing is then applied to the video streams. The
analysis of the acquired videos is carried out using the deep convolution
network AlexNet combined with traditional classifiers such as SVM, LDA and
KNN. Finally, the system we present analyzes the emotions acquired in the
videos and reports the evolutions of the emotions detected over a period of
time. The general structure of the system is shown in Fig. 1.1. Download: Download high-res image (960KB)
2. Download: Download full-size image
Fig. 1. Structure of the whole system [34].
In the system, the first stage is the system input. Here we take video frames
of facial expressions from 120 s of video with a rate of 30 frames per second.
As a result, a 120-second video is thus converted to 3600 images. The image
pre-processing technique can then be applied to such input image frames. This
stage has two main operations: removal of unnecessary image parts such as
background environmental aspects of the image, and removal of hair; and
resizing of the images. Within our approach, firstly a face detector is used
to locate the position of the face in the images using the standard Viola-
Jones algorithm [31,32]. Next, the appropriate face part is cropped from the
images. Finally, the cropped face part is resized to the required size, which
is decided by the input of the deep learning network. In our proposed system,
we utilize well-established and reliable AlexNet to extract the deep features
from the images which have been applied with the aforementioned image pre-
processing techniques.
Finally, at this stage the image will be also converted into RGB format by
concatenating the arrays [33] to meet the requirement of the deep learning
network if the video frames are grayscale.
In the third stage, facial expression analysis techniques are applied to the
pre-processed images. These techniques use a combination of deep learning
networks i.e. AlexNet, and traditional classifiers such as SVM, LDA and KNN,
which will be introduced in detail in the next section. Facial expressions in
the video stimuli are classified into five facial expression groups, namely
neutral, happy, sad, angry and surprised. In addition, the probability of each
facial expression category for every frame is established. By combining all
the facial expression recognition results in the image frames, we obtain the
evolution of the probability of each type of facial expression over a period
of time.
### 2.2. Convolution neural networks and proposed framework
#### 2.2.1. Overview of convolution neural networks
Convolution Neural Networks (CNNs) are a type of deep learning network that
needs less image pre-processing compared to other traditional image
classification algorithms [35]. CNNs have an advantage in that they do not
need prior knowledge and manually design the features. They have many
applications in various domains including natural language processing and
computer vision [36]. A typical CNN has an input layer, an output layer, and
hidden layers such as convolution layers, pooling layers and fully connected
layers.
#### 2.2.2. AlexNet
AlexNet is a type of CNN that was designed by Alex Krizhevsky in the
SuperVision group [37]. The AlexNet competed in the ImageNet Large Scale
Visual Recognition Challenge in 2012, and achieved a top-five error of 15.3%.
It has eight major layers in total including five convolution layers and three
fully connected layers. The detailed network structure is shown in Fig. 2. The
original AlexNet was trained on a subset of the ImageNet database which
contains more than one million images, and was able to classify images into
1000 object categories [34]. In the AlexNet, the input receives images and the
output includes the label of the images and the probabilities for each of the
object categories. In our experiments, the AlexNet is run in Matlab. Moreover,
the transfer learning strategy is used to reduce the training time of the
network [34]. By using the transfer learning, a much smaller number of
training images are needed. There are several steps needed for transferlearning in a pre-trained AlexNet.
1. Download: Download high-res image (317KB)
2. Download: Download full-size image
Fig. 2. Structure of the AlexNet [34,61].
There are 25 layers in the AlexNet, which begins with an image input layer.
Next, there are five convolution layers to extract facial features. The output
of the activation function forms the neurons of the current layer. As a
result, it will form the feature map of the current convolution layer. The
calculation can be described in the following function [37], [38],
[39]:(1)Xjl=F(?i?MjXil?1*wijl+wb)Where, Xjl?1 is the feature map of the output
of the L-1 layer, * depicts the convolution operation, wijl and _w
b_represents weight and the bias, respectively. Each convolution layer is
followed by the ReLU (Rectified Linear Units) layer in order to increase the
nonlinear properties of the network. The ReLU is a half-wave rectifier
function with the advantage of reducing the training time whilst preventing
overfitting [40]. In addition, ReLU layers can prevent the gradient vanishing
problem and are much faster than other logistic function [41]. The ReLU layers
for input _x_ can be described as [37], [38], [39]:(2)F(x)=max(0,x)
In addition, convolution layers are also followed by the max pooling layers.
The max pooling layers are used for down-sampling. The number of feature maps
will not' be changed by the down-sampling process. On the other hand, the
down-sampling process removes unnecessary information and reduces the number
of the parameters of the feature map. The down-sampling layer can be described
by Krizhevsky et al. [37], [38], [39]:(3)Xjl=F(down(Xjl?1)+wb)where Xjl?1
represents the _j_ feature map of the pooling layer _l_ , and _w b_ depicts
the offset term of the down-sampling layer.
Finally, in the AlexNet, after the five convolution layers, there are three
fully connected layers: FC6, FC7 and FC8. The fully connected layers can be
considered as a convolution layer. Also, its convolution kernel size and the
input data size should be consistent with those used in the convolution layer.
The Fully Connected Layer can be described by Krizhevsky et al. [37], [38],
[39]:(4)Xjl=F(?iwijlXjl?1+wbl)
#### 2.2.3. Deep feature extraction
In this context, the AlexNet works as a deep feature extractor to extract
image features. We then use these features to train traditional classifiers
such as SVM, LDA and KNN. In spite of the possible improvement in recognition
accuracy, feature extraction remains fast and relatively simple, using the
representational power of the pre-trained deep networks [42]. In addition, as
feature extraction only requires a single pass through the data, a CPU is also
able to do this work. In this work, a pre-trained CNN works as the deep
feature extractor. The advantages of using a pre-trained CNN to extract deep
features, compared to the training of a new CNN, are: (1) Less computational
power is needed and (2) less data is needed to achieve high recognition
accuracy [43]. The work we present used a pre-trained AlexNet, which was
trained with more than a million images so that the network model has learned
rich feature representations.
To demonstrate what features can be learned by the AlexNet, we need to
visualize the deep features extracted by the network. In our work, we employed
the T-SNE (Stochastic Neighbor Embedding) approach to visualize the features
extracted by the AlexNet. The T-SNE - (TSNE) is an algorithm for
dimensionality reduction [44,45]. This algorithm allows us to visualize the
high-dimensional data of the facial images. The T-SNE function will convert
high-dimensional data into low dimensional data. Generally, distant points in
high-dimensional space will be converted into distant embedded low-dimensionalpoints and nearby points in the high-dimensional space will be converted into
nearby embedded low-dimensional points. As a result, we can visualize the low-
dimensional points to find the clusters in the original high-dimensional data.
The features extracted by the AlexNet which have transfer-learned facial
images from the KDEF dataset are shown in Fig. 3.
1. Download: Download high-res image (238KB)
2. Download: Download full-size image
Fig. 3. Using the T-SNE to visualize the features extracted by the AlexNet.
#### 2.2.4. Layer selection
In this study, we aimed to achieve the best performance by selecting the most
appropriate layer to extract the features including the Fully Connected Layer
6 (FC6), Fully Connected Layer 7 (FC7) and Fully Connected Layer 8 (FC8) from
the AlexNet. While extracting the features from deep CNNs, deeper layers
contain higher level features and earlier layers produce lower-level features
[42]. In order to obtain the feature representations of the training and
testing images, we can use the activations on FC7 or we can obtain lower-level
representations of the images from FC6. Meanwhile, the earlier layers may
learn features like colors and edges [46]. However, in the deeper layers, the
network may learn complicated features such as eyes. We will do experiments to
explore, evaluate and compare which layer in the AlexNet is most appropriate
to extract the deep features in our study.
#### 2.2.5. Traditional classifiers
The deep features or ?activations? can provide the input of traditional
classifiers such as SVM, LDA and KNN in order to further improve the
recognition accuracy. Therefore, we also investigated which traditional
classifier combined with the AlexNet can achieve the best recognition
accuracy. Each of the traditional classifiers has its own characteristics. For
example, SVM can use kernels to transform many feature representations into a
higher dimensional space in order to classify multiple classes [43]. In
addition, SVM has good performance in object classification and face detection
applications.
On the other hand, the LDA approach is able to find the optimal transformation
which can better separate different classes [47]. The LDA has wide
applications such as face recognition and image retrieval [48]. In the LDA,
when _W_ represents an optimal set of discriminant projection vectors [48],
the LDA can be represented as a function of _W_ :(5)J(W)=|WTSBW||WTSWW|
In (5), _S B_ and _S W_ are between class scatter matrix and within class
scatter matrix, respectively. They are given
by(6)Sb=?i=1NMi(mi?m)(mi?m)T(7)SW=?i=1NMiwhere(8)Mi=?X?xi(x??i)(x??i)T
The comparison of these traditional classifiers are presented in the
experimental results section.
## 3\. Results
In order to evaluate and identify the approach with the best recognition
accuracy, different methods were tested in our experiments. To this end, we
tested the pre-trained deep CNN AlexNet with the initial learning rate 0.0003,
minimum batch size 5 and maximum epochs 10 [34]. The test of the traditional
classifiers included multiclass model for SVM, the LDA with linear
Discriminant-Type and the KNN with Euclidean Distance and 1 neighbor number.
We also conducted some experiments to test the influence of the
hyperparameters in the training stage. For the proposed method (AlexNet + FC6
+ LDA), we use the LDA classifier to train and classify the deep features
extracted from the AlexNet. We have tried to use the ?OptimizeHyperparameters?
function in Matlab to find the optimized hyperparameters. We found that the
time cost outweighs the small performance improvement. In the LDA, thefunction will try to optimize the performance by changing hyperparameters
Delta and Gamma automatically. By using the OptimizeHyperparameters? function,
we found the operating time is increased by 100 times and there is limited
improvement in recognition accuracy in the JAFFE dataset for the LDA
classifier. We found the ?OptimizeHyperparameters? function has the drawback
of greatly increasing the operating time, which is not appropriate in our
research. As a result, we use the default hyperparameters for the LDA
classifier.
We also tested the combination of the AlexNet with traditional classifiers
[49]. As the layer selected to extract the features affects the recognition
result, we conducted the experiments using different layers to extract the
deep features. The explored layers are the FC6, FC7, and FC8 of the AlexNet.
We first tested the combination of the AlexNet and the LDA using FC6, FC7 and
FC8 to investigate which layer may have the best recognition accuracy. We
subsequently discovered that FC6 demonstrated the highest recognition
accuracy. We then used FC6 to extract the deep features and experimentally
tested which classifier demonstrated the highest recognition accuracy among
SVM, LDA and KNN. We found that the LDA showed the best recognition
performance. In summary, we tested the recognition accuracy of facial
expressions using the following methods: deep convolution neural networks
AlexNet, traditional classifiers SVM, LDA and KNN and the combination of the
AlexNet and traditional classifiers using FC8 and LDA, FC7 and LDA, FC6 and
LDA, FC6 and SVM, and FC6 and KNN, respectively.
Additionally, in order to compare the recognition accuracy among the nine
methods, we conducted our experiments on different facial expression databases
in this research. Specifically, we used JAFFE, KDEF, CK+, FER2013 and
AffectNet Datasets. Moreover, we used the proposed system to quantify the
evolution of facial expressions from a video of facial expressions emitted by
the first author as the ground truth is known in this case. The detailed
experimental outcomes of these comparisons are reported in the following
sections.
### 3.1. Experiment using the online datasets
#### 3.1.1. JAFFE dataset experiments
To begin with, the JAFFE is a facial expression database with only 213 static
images. By using the JAFFE Dataset, we aim to test the influence of small
number of images in training the system using different methods. From the
JAFFE Dataset, we selected 202 images that were all processed using the image
preprocessing techniques mentioned above (it was observed that the JAFFE
dataset included some incorrectly labeled facial expressions, which were
removed [26]). There are 7 different facial expressions in this dataset:
angry, happy, neutral, surprised, sad, afraid, and disgusted. In each test,
70% of the images were randomly selected as the training images, with the rest
of the images serving as testing images. Table 1 compares the 5-fold Cross
Validation Error Rate for the facial expressions using the JAFFE Dataset with
each different method. The first column in the table shows the method used for
facial expressions recognition. The second column shows the 5-fold Cross
Validation Error Rate for each method.
Table 1. Comparison of cross-validation error rate using different methods for
the JAFFE dataset.
Method| Error Rate (%)
---|---
AlexNet| 24.8
LDA| 26.7
SVM| 24.8KNN| 39.6
AlexNet + FC8 + LDA| 18.8
AlexNet + FC7 + LDA| 9.4
AlexNet + FC6 + LDA| 5.9
AlexNet + FC6 + SVM| 10.9
AlexNet + FC6 + KNN| 15.4
As the table shows, our proposed method (AlexNet + FC6 + LDA) reaches the
lowest error rate of 5.9%. As the number of images in the JAFFE Database is
quite small, the deep CNN AlexNet does not demonstrate satisfactory
performance. Also, the performance of the AlexNet appears to be quite near to
that of the traditional classifiers such as LDA and SVM.
Furthermore, Table 2 also shows the recognition accuracy for each emotion and
the overall recognition accuracy rates obtained when using each different
method for the JAFFE Dataset. The first column in the table shows the method
used for facial expressions recognition whilst the second to the eighth column
shows the recognition accuracy of each method for angry, disgusted, fearful,
happy, neutral, sad and surprised faces, respectively. The ninth column shows
the overall recognition accuracy of each method. Also, the last row shows the
average recognition accuracy using all 9 methods for each emotion. The method
using AlexNet, FC6 and LDA has the highest overall recognition accuracy, but
does not demonstrate good performance in recognizing ?surprise?. In general,
?happy? is the most easily recognized emotion by this method, while
?surprised? is the most difficult emotion category to be recognized.
Table 2. Comparison of recognition accuracy of each emotion and overall
recognition accuracy using different methods for the JAFFE dataset.
Method| Angry (%)| Disgust (%)| Fear (%)| Happy (%)| Neutral (%)| Sad (%)|
Surprise (%)| Overall Accuracy for Each Method (%)
---|---|---|---|---|---|---|---|---
AlexNet| 88.9| 37.5| 33.3| 100| 77.8| 87.5| 87.5| 73.3
LDA| 66.7| 62.5| 55.6| 88.9| 55.6| 37.5| 25.0| 56.7
SVM| 88.9| 62.5| 66.7| 88.9| 100| 62.5| 25.0| 71.7
KNN| 77.8| 62.5| 33.3| 77.8| 100| 75.0| 37.5| 66.7
AlexNet + FC8 + LDA| 100| 87.5| 88.9| 88.9| 88.9| 75.0| 50.0| 83.3
AlexNet + FC7 + LDA| 100| 87.5| 88.9| 100| 100| 87.5| 25.0| 85.0
AlexNet + FC6 + LDA| 100| 87.5| 100| 100| 100| 100| 75.0| 95.0
AlexNet + FC6 + SVM| 88.9| 87.5| 88.9| 100| 100| 87.5| 37.5| 85.0
AlexNet + FC6 + KNN| 77.8| 87.5| 44.4| 88.9| 100| 87.5| 62.5| 78.3
Average Accuracy for Each Emotion| 87.7| 73.6| 66.7| 92.6| 91.4| 77.8| 47.2|
77.2
#### 3.1.2. KDEF dataset experiments
The KDEF Dataset contains 4900 facial expression images. As our system aims to
quantify the evolution of emotion from front-view videos of facial
expressions, we have only used front-view images. From the KDEF Dataset, we
selected all 980 front-view images of facial expressions that were all
processed using the images preprocessing techniques outlined above [25]. There
are seven different facial expressions in this dataset: angry, happy, neutral,
surprised, sad, fearful, and disgusted. In the experiment, 70% of the images
were again randomly selected as the training images, with the rest of the
images used as the testing images. Table 3 compares the 5-fold Cross
Validation Error Rate for the facial expressions using the KDEF Dataset with
each different method. The first column in the table shows the method used for
facial expressions recognition, whilst the second column shows the 5-fold
Cross Validation Error Rate for each method.
Table 3. Comparison cross-validation error rate using different method for theKDEF dataset.
Method| Error Rate (%)
---|---
AlexNet| 15.1
LDA| 36.3
SVM| 32.1
KNN| 56.0
AlexNet + FC8 + LDA| 17.2
AlexNet + FC7 + LDA| 12.0
AlexNet + FC6 + LDA| 11.6
AlexNet + FC6 + SVM| 12.6
AlexNet + FC6 + KNN| 32.6
As the table shows, our proposed method has the lowest error rate at 11.6%.
However, as the number of images in the KDEF was greatly increased compared to
the JAFFE Database, the performance of the AlexNet can be seen to be
significantly improved, with an error rate now 4% higher than the proposed
method. On the other hand, the traditional classifiers such as LDA and SVM did
not show such a good performance when there were tested with such a large
image database.
In addition, Table 4 shows the recognition accuracy for each emotion category,
and the overall recognition accuracy when using each different method for the
KDEF Dataset. The first column in the table shows the method used for facial
expressions recognition whilst the second to eighth columns show the
recognition accuracy of each method for angry, disgusted, fearful, happy,
neutral, sad and surprised faces respectively. The ninth column shows the
overall recognition accuracy of each method. Also, the last row shows the
average recognition accuracy using all 9 methods for each emotion. The
proposed method of using AlexNet, FC6 and LDA clearly demonstrates the highest
recognition accuracy. Moreover, we can observe that ?happy? is still the
easiest emotion to be recognized, while ?sad? and ?angry? are the two most
difficult emotions to be recognized.
Table 4. Comparison of recognition accuracy of each emotion and overall
recognition accuracy using different methods for the KDEF dataset.
Method| Angry (%)| Disgust (%)| Fear (%)| Happy (%)| Neutral (%)| Sad (%)|
Surprise (%)| Overall Accuracy for Each Method (%)
---|---|---|---|---|---|---|---|---
AlexNet| 76.2| 81.0| 92.9| 97.6| 88.1| 81.0| 76.2| 84.7
LDA| 47.6| 69.0| 50.0| 83.3| 71.4| 50.0| 71.4| 63.3
SVM| 57.1| 66.7| 66.7| 81.0| 54.8| 52.4| 76.2| 65.0
KNN| 38.1| 35.7| 31.0| 47.6| 50.0| 35.7| 57.1| 42.2
AlexNet + FC8 + LDA| 83.3| 69.0| 83.3| 97.6| 88.1| 73.8| 90.5| 83.7
AlexNet + FC7 + LDA| 83.3| 81.0| 85.7| 100| 85.7| 78.6| 85.7| 85.7
AlexNet + FC6 + LDA| 78.6| 85.7| 83.3| 100| 92.9| 83.3| 90.5| 87.8
AlexNet + FC6 + SVM| 78.6| 83.3| 90.5| 97.6| 88.1| 78.6| 88.1| 86.4
AlexNet + FC6 + KNN| 40.5| 64.3| 45.2| 88.1| 78.6| 54.8| 76.2| 64.0
Average Accuracy for Each Emotion| 64.8| 70.6| 69.8| 88.1| 77.5| 65.4| 79.1|
73.6
#### 3.1.3. CK+ dataset experiments
The CK+ Dataset consists of 593 sequences of facial expressions [27,28]. Each
video sequences can be regarded as a few continuous video frames. As a result,
this is a large database of around 10,000 images of facial expressions taken
from 123 models. As these image sequences are continuous, there are many
similar images. In our experiment, after removing similar images, 693 images
were selected and processed using the image preprocessing techniquesdescribed. We selected images with seven different facial expressions in the
dataset: angry, happy, neutral, surprised, sad, fearful, and disgusted. We
again randomly selected 70% of the images as the training images, with the
remainder employed as the testing images. Table 5 shows the 5-fold Cross
Validation Error Rate for the facial expressions from the CK+ Dataset with
each different method. The first column in the table shows the method used for
facial expressions recognition, whilst the second column shows the 5-fold
Cross Validation Error Rate for each method.
Table 5. Comparison cross-validation error rate using different methods for
the CK dataset.
Method| Error Rate (%)
---|---
AlexNet| 10.9
LDA| 10.7
SVM| 8.4
KNN| 24.2
AlexNet + FC8 + LDA| 8.0
AlexNet + FC7 + LDA| 5.2
AlexNet + FC6 + LDA| 3.6
AlexNet + FC6 + SVM| 6.7
AlexNet + FC6 + KNN| 21.6
As the table shows, our proposed method still obtains the lowest error rate at
3.6%. Generally, in the CK+ Dataset, two images are selected for each emotion
for each subject, with one image being the frame when the emotion begins to be
expressed whilst the other image is the frame in the image sequence when the
emotion reaches the peak of its expression. All the methods appear to have
good performance in this dataset.
Additionally, Table 6 shows the recognition accuracy for each emotion, and the
overall recognition accuracy rate when employing each different method for the
CK+ Dataset. The first column in the table shows the method used for facial
expressions recognition, with the next seven columns showing the recognition
accuracy for each method for angry, disgusted, fearful, happy, neutral, sad
and surprised facial images respectively. The ninth column shows the overall
recognition accuracy of each method tested. Also, the last row shows the
average recognition accuracy using all the 9 methods for each emotion. Within
this dataset, ?sad? seems to be the most difficult emotion to be recognized,
with several methods wrongly classifying the sad emotion as ?neutral?. The
difficulty of recognition of sad emotion will be discussed in the discussion
part.
Table 6. Comparison of recognition accuracy of each emotion and overall
recognition accuracy using different method for the CK dataset.
Method| Angry (%)| Disgust (%)| Fear (%)| Happy (%)| Neutral (%)| Sad (%)|
Surprise (%)| Overall Accuracy for Each Method (%)
---|---|---|---|---|---|---|---|---
AlexNet| 95.7| 85.7| 81.8| 97.4| 76.2| 33.3| 97.8| 86.4
LDA| 78.3| 88.6| 72.7| 97.4| 88.1| 33.3| 91.1| 85.4
SVM| 73.9| 91.4| 81.8| 92.1| 95.2| 66.7| 88.9| 87.9
KNN| 65.2| 77.1| 63.6| 65.8| 59.5| 33.3| 64.4| 64.1
AlexNet + FC8 + LDA| 87.0| 77.1| 63.6| 97.4| 95.2| 33.3| 91.1| 85.4
AlexNet + FC7 + LDA| 91.3| 82.9| 72.7| 97.4| 97.6| 33.3| 93.3| 88.3
AlexNet + FC6 + LDA| 100| 91.4| 100| 100| 97.6| 58.3| 95.6| 94.7
AlexNet + FC6 + SVM| 91.3| 80.0| 81.8| 97.4| 97.6| 41.7| 97.8| 89.8
AlexNet + FC6 + KNN| 73.9| 85.7| 63.6| 68.4| 66.7| 33.3| 71.1| 69.9
Average Accuracy for Each Emotion| 84.1| 84.4| 75.7| 90.4| 86.0| 40.7| 87.9|83.5
#### 3.1.4. FER2013 dataset experiments
The FER2013 is an online open dataset for facial expressions [29]. This
dataset consists of more than 30,000 48×48 pixel grayscale images of faces.
There are also seven types of emotions in this dataset: angry, disgusted,
fearful, happy, sad, surprised and neutral. However, unlike the JAFFE and KDEF
datasets that are lab-posed facial expressions, the FER2013 dataset consists
of facial images taken from the internet. In the JAFFE and KDEF databases,
facial expression of the same emotional category are similar to each other.
However, in the FER2013 dataset, very large variations in facial expression
can be observed, within the same category of emotion. In the current
experiment, about 30,000 images were selected. Image preprocessing techniques
were not applied to this dataset, as the original 48×48 pixel grayscale images
are already quite small. We tested with all seven categories of facial emotion
available in the database, again randomly selecting 70% of the images as the
training set and using the remainder of the dataset as the testing images.
Table 7 compares the 5-fold Cross Validation Error Rate for the facial
expressions using the FER2013 Dataset with each different method. The first
column in the table shows the method used for facial expressions recognition,
whilst the second column shows the 5-fold Cross Validation Error Rate for each
method tested. As the table shows, the proposed method still obtains the
lowest error rate at 43.5%. However, as the FER2013 dataset is primarily drawn
from the internet, with a broad range of individual variation in terms of
expression within the same category of emotion, we can see from the data that
all approaches demonstrated difficulty in accurate classification, with no
method demonstrating good levels of performance.
Table 7. Comparison cross-validation error rate using different methods for
the FER2013 dataset.
Method| Error Rate (%)
---|---
AlexNet| 44.6
LDA| 68.6
SVM| 73.4
KNN| 61.7
AlexNet + FC8 + LDA| 50.2
AlexNet + FC7 + LDA| 46.5
AlexNet + FC6 + LDA| 43.5
AlexNet + FC6 + SVM| 48.2
AlexNet + FC6 + KNN| 49.6
Table 8 shows the recognition accuracy for each emotion category, and the
overall recognition accuracy using each particular method with the FER2013
Dataset. The first column in the table shows the method used for facial
expressions recognition, whilst the next seven columns show the recognition
accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and
surprised faces respectively. The ninth column shows the overall recognition
accuracy of each method. Also, the last row shows the average recognition
accuracy using all 9 methods for each emotion. In this dataset, happy and
surprised facial categories seem to be the simplest emotions to be recognized.
However, even the method using the AlexNet, FC6 and LDA fails to show good
performance in recognizing fear emotion.
Table 8. Comparison of recognition accuracy of each emotion and overall
recognition accuracy using different methods for the FER2013 dataset.
Method| Angry (%)| Disgust (%)| Fear (%)| Happy (%)| Neutral (%)| Sad (%)|
Surprise (%)| Overall Accuracy for Each Method (%)---|---|---|---|---|---|---|---|---
AlexNet| 42.0| 43.8| 34.2| 81.3| 58.2| 43.0| 65.6| 56.3
LDA| 18.7| 14.6| 18.4| 47.6| 28.9| 21.1| 43.9| 30.8
SVM| 14.7| 27.0| 13.0| 38.0| 20.5| 25.6| 40.1| 26.2
KNN| 28.0| 54.0| 31.7| 38.0| 38.8| 30.9| 52.4| 36.5
AlexNet + FC8 + LDA| 37.5| 25.5| 24.6| 70.9| 50.1| 41.5| 65.3| 49.8
AlexNet + FC7 + LDA| 42.3| 32.1| 28.5| 72.3| 55.8| 46.4| 67.2| 53.5
AlexNet + FC6 + LDA| 43.8| 36.5| 30.7| 74.9| 59.9| 52.1| 67.5| 56.4
AlexNet + FC6 + SVM| 42.2| 41.6| 36.9| 70.0| 45.6| 40.3| 69.5| 51.7
AlexNet + FC6 + KNN| 40.1| 59.9| 42.1| 61.6| 43.0| 39.5| 66.9| 49.5
Average Accuracy for Each Emotion| 34.5| 35.8| 28.9| 62.1| 44.3| 37.5| 60.9|
45.8
#### 3.1.5. AffectNet dataset experiments
The AffectNet is also an online dataset which consists of about one million
images of facial expressions which were again collected from the Internet,
through three major search engines and 1250 emotion related keywords [30].
This dataset provides eleven emotion and non-emotion labels including:
Neutral, Surprised, Happy, Sad, Fearful, Disgusted, and Angry, whilst
additionally providing the categories of Contemptuous, None, Uncertain and No-
Face. In line with the FER2013 Dataset, the AffectNet contains the images of
facial expressions which are naturally occurring rather than containing the
images posed within a lab. In the current study, about 16,000 images were
selected. Image preprocessing techniques were not applied to this dataset. We
selected the seven emotional expressions that were in line with the previous
four datasets tested and again we randomly selected 70% of the images to serve
as the training images, while the remaining images were again employed as
testing images. Table 9 compares the 5-fold Cross Validation Error Rate for
the facial expressions using the AffectNet Dataset with each different method.
The first column in the table shows the method used for facial expressions
recognition, whilst the second column shows the 5-fold Cross Validation Error
Rate for each method.
Table 9. Comparison cross-validation error rate using different methods for
the AffectNet dataset.
Method| Error Rate (%)
---|---
AlexNet| 40.81
LDA| 59.97
SVM| 60.92
KNN| 68.54
AlexNet + FC8 + LDA| 48.02
AlexNet + FC7 + LDA| 43.55
AlexNet + FC6 + LDA| 39.43
AlexNet + FC6 + SVM| 45.28
AlexNet + FC6 + KNN| 59.81
As the table shows, the proposed method has the lowest error rate at 39.43%.
As the AffectNet dataset is similar to the FER2013 dataset in employing images
of facial expressions taken from the Internet, the huge variance within the
same class of emotion can again be seen to result in low recognition accuracy
rates.
Table 10 shows the recognition accuracy of each emotion category, and the
overall recognition accuracy using each different method with the AffectNet
Dataset. The first column in the table shows the method used for facial
expressions recognition, whilst the second to the eighth columns show the
recognition accuracy for each method tested for angry, disgusted, fearful,happy, neutral, sad and surprised emotional expressions respectively. The
ninth column shows the overall recognition accuracy for each method. Also, the
last row shows the average recognition accuracy using all 9 methods for each
emotion. In general, it can be seen that these methods show their best
performance in recognizing happy emotions, which is similar to the performance
observed with the FER2013 dataset. On the other hand, each method demonstrates
its worst performance in recognizing the emotion of surprise.
Table 10. Comparison of recognition accuracy of each emotion and overall
recognition accuracy using different methods for the AffectNet dataset.
Method| Angry (%)| Disgust (%)| Fear (%)| Happy (%)| Neutral (%)| Sad (%)|
Surprise (%)| Overall Accuracy for Each Method (%)
---|---|---|---|---|---|---|---|---
AlexNet| 58.2| 22.0| 14.3| 85.6| 50.5| 17.0| 36.3| 58.6
LDA| 17.8| 4.3| 8.9| 60.8| 38.5| 14.3| 14.4| 38.9
SVM| 20.8| 9.1| 12.5| 61.8| 37.3| 19.4| 16.1| 40.3
KNN| 14.5| 8.1| 15.6| 43.1| 34.0| 19.4| 7.6| 30.7
AlexNet + FC8 + LDA| 25.1| 5.4| 19.6| 78.8| 53.1| 17.0| 26.9| 52.1
AlexNet + FC7 + LDA| 33.1| 11.8| 22.8| 78.6| 59.2| 25.1| 34.0| 56.0
AlexNet + FC6 + LDA| 36.2| 12.4| 25.4| 83.2| 64.1| 32.7| 32.0| 60.1
AlexNet + FC6 + SVM| 38.0| 15.6| 31.3| 77.2| 49.1| 32.3| 32.0| 54.6
AlexNet + FC6 + KNN| 18.8| 8.6| 19.2| 59.4| 35.6| 21.4| 17.6| 39.2
Average Accuracy for Each Emotion| 29.2| 10.8| 18.8| 69.8| 46.8| 22.1| 24.1|
47.8
### 3.2. Experiment using the author's facial expressions
In this experiment, our proposed framework was used to recognize facial
expression taken from a video of facial expressions emitted by the first
author as the ground truth of emotions are perfectly known in this case.
Within the video stimulus there were 898 continuous frames, which were
preprocessed by the techniques previously outlined. The framework mainly aimed
to recognize 5 kinds of emotion in the video stimulus: angry, happy, neutral,
surprised, and sad. The training images used 20% of the frames from the video,
combined with another dataset emitted by the author containing about 2600
images, whilst the remaining 80% images from the video acted as the testing
images in this case. In the subsequent testing, the proposed system
successfully classified the images into five facial expression categories:
angry, happy, neutral, sad and surprised. In this experiment, as Fig. 4
illustrates, the accuracy was about 96.0%.
1. Download: Download high-res image (195KB)
2. Download: Download full-size image
Fig. 4. Recognition results using the proposed framework.
Fig. 5 shows the probability of each type of emotion, which is made up of
predicted results for each frame in the video. The graph displays the
evolution of facial expressions from the video and changes in facial
expressions over time. In the graph, the probability of the five facial
expression categories for every frame was predicted by the proposed framework.
Each line represents the evolution of one emotion. In Fig. 5, the five lines
show the evolution of five emotions over a period of time. In addition, the
plot was smoothed by calculating the average of the recent frames.
1. Download: Download high-res image (611KB)
2. Download: Download full-size image
Fig. 5. Facial expression analysis for the video stimulus of the author own
emotional expressions.
As an electrocardiogram can reflect the electrical activity of the heart, Fig.
5 reflects the mental state of the patient/ user over a period of time. Asshown in Fig. 5, around the 300th frame, there are three emotions: happy,
neutral and sad. The evolutions of the three emotions at that time are also
shown. In addition, this figure illustrates when the user was said to be
happy, the duration of the emotion, and how quickly it reached the peak of the
emotion. By data analysis, the plot also shows the relative percentage of time
for each emotion over the period. As a result, in the area of mental health
care, the proposed system clearly has the potential to identify the emotional
state of the user. As patients with severe cognitive impairments may express
abnormal facial expressions [21], the proposed system may have the potential
to be used diagnostically in the future.
## 4\. Discussion
Facial expressions are an important component of human social interaction,
which reflect people's emotions, attitudes, social relations and physiological
state. The automated facial expression analysis can play an important role in
human-computer interaction in many important applications. On the other hand,
deep learning is a dynamic and vibrant topic, encompassing diverse and useful
applications, such as the use of A DSAE-based deep learning framework for
facial expression recognition and the use of a deep-belief-network-based
particle Filter for Analysis of Gold Immunochromatographic Strips [50], [51],
[52]. In this work, we have proposed a deep learning based facial expression
recognition framework towards mental health care.
In the experiments presented here we have tested the facial expression
recognition performance of nine methods, including deep convolution neural
network AlexNet, the traditional classifiers SVM, LDA and KNN and the
combination of the AlexNet and traditional classifiers against five datasets.
The overall performance can be seen in Table 11, which shows the 5-fold cross
validation error rate of each method for the five datasets employed in our
testing. The first column in the table shows the method used for facial
expressions recognition, while the second column to the sixth columns show the
error rate of each method for JAFFE, KDEF, CK+, FER2013 and AffectNet
respectively.
Table 11. Comparison cross-validation error rate using different methods over
5 databases.
Method| JAFFE| KDEF| CK+| FER2013| AffectNet
---|---|---|---|---|---
AlexNet| 24.8| 15.1| 10.9| 44.6| 40.8
LDA| 26.7| 36.3| 10.7| 68.6| 60.0
SVM| 24.8| 32.1| 8.4| 73.4| 60.9
KNN| 39.6| 56.0| 24.2| 61.7| 68.5
AlexNet + FC8 + LDA| 18.8| 17.2| 8.0| 50.2| 48.0
AlexNet + FC7 + LDA| 9.4| 12.0| 5.2| 46.5| 43.6
AlexNet + FC6 + LDA| 5.9| 11.6| 3.6| 43.5| 39.4
AlexNet + FC6 + SVM| 10.9| 12.6| 6.7| 48.2| 45.3
AlexNet + FC6 + KNN| 15.4| 32.6| 21.6| 49.6| 59.8
Our experiments demonstrate that in a small dataset like the JAFFE and CK+
datasets, the deep convolution neural network AlexNet and some traditional
classifiers like SVM and LDA have similar facial expression recognition
accuracy. However, by combining the AlexNet with traditional classifiers like
SVM and LDA, the recognition accuracy increases, especially when we extract
the deep features from FC6. The experiments show that the method that extracts
features from FC6 has better performance than FC7 and FC8 in the five online
datasets. We use the AlexNet which is pre-trained with one million natural
images from ImageNet to extract features. The features extracted from FC7 are
more in line with the classification attribute of the training set of naturalimages but less in accordance with the dataset of facial expressions [53]. As
a result, the methods that extract the features from FC6 have better
performance.
Additionally, we have observed that using the LDA to classify the deep
features results in better performance. When the number of the images in the
training dataset increased, such as in the case of the KDEF dataset (which
contains about 1000 images), the AlexNet seemed to have better recognition
accuracy relatively to its performance when tested with a small image
database. On the other hand, we have observed that classifying facial
expressions with traditional classifiers did not show good overall
performance. As a result, in the KDEF dataset, the recognition accuracy of the
combination of the AlexNet and traditional classifiers only demonstrated
slightly better performance than that of the AlexNet. In addition, it is
noticed that the overall recognition accuracy for the FER2013 and AffectNet
databases are lower than that of the JAFFE, KDEF and CK+ databases. We would
argue that this is mainly as a result of the FER2013 and AffectNet databases
containing the more challenging facial expressions sourced from the Internet,
which have huge variance within a given class of emotion for example the
difference in image sizes and lighting situations. These facial expressions
are more natural and diverse, resulting in stimuli that are more difficult to
recognize than the lab-based, controlled and actor-generated facial
expressions. Although the recognition performance for the AffectNet is worse
than the performance in the JAFFE, KDEF and CK+ databases, the proposed
framework has a relatively good performance for the AffectNet database
compared to the other state-of-art facial expression recognition algorithms
[54,55]. Fig. 6 shows the accumulative recognition error rate for the nine
methods over the five databases.
1. Download: Download high-res image (160KB)
2. Download: Download full-size image
Fig. 6. Comparison of accumulative cross-validation error rates using
different methods over 5 databases.
In general, the method that extracts the deep features using FC6 from the
AlexNet and classifies with LDA showed the best overall performance in the
five databases tested using these nine methods. Our experiment result showed a
facial expression recognition accuracy of 94.1% on the JAFFE database and
88.4% on the KDEF database, which is a relatively good performance compared to
other facial expression recognition algorithms tested on the same databases,
as shown in Table 12. The recognition accuracy is calculated from the error
rate.
Table 12. Recognition accuracy from published papers on KDEF and JAFFE
datasets.
System| Accuracy for KDEF| Accuracy for JAFFE
---|---|---
DeepPCA [56]| 83.0%| /
AAM+SVM [57]| 74.6%| /
Feature+SVM [58]| 82.4%| /
C+CNN [59]| /| 91.6%
HF [60]| /| 87.1%
Proposed Method (5-fold cross validation recognition accuracy)| 88.4%| 94.1%
In order to further estimate the performance of the proposed method, we
compared the proposed method (AlexNet + FC6 + LDA) with some state-of-the-art
deep CNN including pure AlexNet, VGG16, GoogleNet and ResNet. We estimated the
performance mainly with regard to the operating time of training the network,
recognizing the facial expressions and in terms of the recognition accuracy ofthe facial expression categories. Three facial expression datasets including
the JAFFE, KDEF and CK+ Datasets were used in this estimation. Fig. 7 shows
both the recognition accuracy and the operating time for each method with the
JAFFE, KDEF and CK+ Datasets. In Fig. 7, the recognition accuracy is shown as
clustered columns, with the operating time superimposed as a line chart. We
can observe that the proposed method has high recognition accuracy compared to
the other deep learning algorithms, but slightly lower than that of the
ResNet. However, this should be considered in light of the clear reduction in
operating time, as the operating time of the proposed method is around 100
times shorter than that of the ResNet. In addition, it is important to note
that deep learning algorithms have high device requirements relating to GPU
resources and local dynamic random-access memory requirements. Indeed in the
current assessment, the Vgg16 failed to produce a recognition result in the
KDEF and CK+ datasets due to insufficiency in memory resource to complete the
task. In general, the proposed method can be seen to have relatively good
recognition accuracy, much shorter operating time and low device requirements
compared to the state-of-art deep learning algorithms.
1. Download: Download high-res image (521KB)
2. Download: Download full-size image
Fig. 7. Comparison of recognition accuracy and operating time for different
methods in the JAFFE, KDEF and CK+ datasets.
Another important aspect is the ratio of training images to testing the images
employed in assessments. This can enable us to determine the influence of
training image ratios for different algorithms on the selected dataset. In the
experiments presented, we selected 70% of the images randomly from the whole
dataset as the training dataset and used the remaining images as the testing
dataset. Here we choose to test the JAFFE Database. Fig. 8 shows the different
ratios of the training images with the different methods for the JAFFE
Database and the resulting recognition accuracy. We observe that when the
training images ratios increases, the recognition accuracy for all the methods
is increased. The result also suggests that the proposed method using the
AlexNet, FC6 and LDA demonstrates the best performance, regardless of the
training images ratio examined. Indeed, when we select 90% of the images
randomly from the JAFFE Database to act as the training images, and use the
remaining images as the testing dataset, the recognition accuracy of the
proposed method reaches 97.0%. We can assume that when the training images
ratios increase, the recognition performance for other datasets will also be
improved.
1. Download: Download high-res image (301KB)
2. Download: Download full-size image
Fig. 8. The influence of the training images ratios on recognition accuracy
with the different algorithms for the JAFFE database.
In the experiment, the first phase of the proposed method involves image pre-
processing, including aspects such as removing unnecessary environment aspects
in the image, identification of the facial region in the image, and some
additional essential pre-processing steps. To test the efficiency of this
phase we also conducted experiments to compare the 5-fold cross validation
error rate using each particular method, for both processed and unprocessed
images from the CK+ dataset. The results of these experiments are shown in
Table 13 below. The first column in the table shows the method used for facial
expressions recognition, with the second and third columns showing the 5-fold
Cross-Validation Error Rate of each method for both processed and unprocessed
images, respectively.
Table 13. Comparison cross-validation error rate using different methods forprocessed and unprocessed images from the CK+ dataset.
Method| Error Rate (%) for Processed Data| Error Rate (%) for Unprocessed Data
---|---|---
AlexNet| 10.90| 12.72
LDA| 10.74| 26.42
SVM| 8.42| 23.80
KNN| 24.24| 20.46
AlexNet + FC8 + LDA| 7.98| 11.76
AlexNet + FC7 + LDA| 5.22| 8.56
AlexNet + FC6 + LDA| 3.63| 3.92
AlexNet + FC6 + SVM| 6.68| 8.85
AlexNet + FC6 + KNN| 21.63| 19.74
As Table 13 shows, the error rate is decreased in all the algorithms for the
processed dataset, which proves the efficiency of the image-preprocessing
phase. Additionally, we can observe that the image pre-processing phase
improves the error rate performance of the traditional classifier to a greater
extent, but the combination of the AlexNet and the traditional classifiers
seems less dependent on this phase.
Finally, we report on the performance relating to the recognition of each kind
of facial expression category. Confusion matrices (a), (b), (c), (d) and (e)
in Fig. 9 show the data relating to the AlexNet + FC6 LDA for the Datasets of
JAFFE, KDEF, CK+, FER2013 and AffectNet respectively. Here, it can be observed
that the emotional category ?happy? appears to be the easiest emotion to be
recognized. On the other hand, some emotional categories like ?sad? and
?disgust? seems to be much more difficult to be recognized. We would argue
that the sad emotion is currently relatively hard to be recognized for the
following two reasons. The first reason is due to the quantity of images
depicting the sad emotion within the training dataset. We have observed that
in most of the datasets, there are notably fewer images of sad emotions, which
consequently increases the recognition difficulty. For example, within the CK+
dataset there are 693 images in total, whilst only 40 images depict sad
expressions. Additionally, there are relatively small visual differences
between sad emotions and neutral emotions, and consequently sad expressions
may be recognized as neutral by the system without using sufficient training
data.
1. Download: Download high-res image (2MB)
2. Download: Download full-size image
Fig. 9. The confusion matrix using the AlexNet + FC6 LDA for the datasets of
JAFFE, KDEF, CK+, FER2013 and AffectNet are shown in (a), (b), (c), (d) and
(e).
Furthermore, we notice that in the datasets using the images from the
Internet, such as the FER2013 and AffectNet datasets, the number of images for
each emotion category is uneven. The predominant emotion type contained within
these two datasets are happy images, which consequently reduces the difficulty
of recognition of the happy emotion. However, in the case of the other
databases, which contain a more balanced range of emotions, the emotional
category with the lowest error rate varies.
As the developed facial expression recognition method aims to benefit the
mental health care, we further test the proposed method with the datasets that
contain the images of facial expressions from the patients with cognitive
impairment. We tested the proposed method in the following three datasets: a
dataset with facial expressions from the patient with cognitive impairment, a
dataset that combines the images from the JAFFE dataset and the images from
patients and a dataset that combines the images from the KDEF dataset and theimages form patients. We used 70% of images as the training dataset and 30% of
images as the testing dataset. Table 14 shows the recognition accuracy using
the proposed method in the three datasets. The experimental result shows that
the proposed method also has a good performance for the facial expressions
from the patients with cognitive impairment.
Table 14. The recognition accuracy using the proposed algorithm (AlexNet + FC6
+ LDA) in the datasets containing facial expressions from patient with
cognitive impairment.
Dataset| Recognition Accuracy (%)
---|---
Patient with Cognitive Impairment Dataset| 85.13
JAFFE + Patient Dataset| 89.90
KDEF + Patient Dataset| 89.33
In our examination of the data relating to recognition when employing the
first author's facial expressions, drawn from a series of continuous video
frames, the accuracy rate was approximately 96.0%.We would argue that the
system achieved strong performance in recognizing facial expressions taken
from videos in this context. It should be noted that one factor that
contributed to the high accuracy rate may be that some of the training images
and the testing images were selected from a video that had the similar
lighting condition and viewpoint. In addition, there was only one participant
and one viewpoint. However, the analysis of facial expressions from the videos
in our testing appears to be able to evaluate the evolution of the facial
expression over a period of time, with changes of the expression emitted, by
recognizing facial expressions for each frame in the video. Additionally, the
system was also able to quantify the extent of the facial expression. However,
there remain some practical issues to be considered. First, it is noticed that
the differences in facial expression for happy and neutral, neutral and sad,
sad and angry are small. Moreover, for the video of the facial expressions
emitted by the first author, the facial expression is currently labelled
manually, and there may be potential problems in labelling some frames during
a change of expressions. In addition, as stated previously, it was noted that
in the JAFFE dataset, some facial expressions appeared to be labelled
incorrectly and were removed prior to testing. In the experiment, 202 images
of facial expressions were used from the JAFFE, while the original JAFFE
dataset has about 213 images. Finally, there are some practical problems for
the proposed facial expression recognition system that remain to be addressed
in future, including a need for enhanced stability with regard to how the
system crops the head area in the images.
From the perspective of clinical practice, in order to use the system within
elderly people to detect mental health issues like cognitive impairment, the
framework needs to be trained with large samples of natural facial expressions
taken from elderly people, in order to achieve optimal performance. In recent
work, we have collected circa 100, 000 images of facial expressions from
elderly people. The recognition of naturally occurring facial expression is
more challenging. We should also note that work to complete the processing of
these images is ongoing, and we will report in a future work regarding
satisfactory verification of these images as a suitable training set for our
network.
## 5\. Conclusion
This paper has presented a novel emotion analysis framework that is able to
understand and automatically recognize users? emotions by analyzing the users?
facial expression from their facial images. The system consists of three
parts: input of the videos of facial expressions, the image pre-processingtechnique, and automatic facial expression analysis. The system is able to
successfully conduct image pre-processing and facial expression analysis.
Additionally, after facial expression analysis has been undertaken, it is able
to understand the evolution of facial expression over a period of time and can
quantify the extent of the emotions detected from a facial video. As facial
expressions reflect people's mental health state, the proposed framework has
great potential to be employed within mental health care.
For the facial expression analysis, this paper has also proposed a new
solution that extracts the deep features from the FC6 of the AlexNet whilst
the standard LDA is exploited to train these deep features. The proposed
solution shows promising and has stable performance on all the five tested
datasets for the nine methods studied. Additionally, we would argue that the
proposed method has relatively good recognition accuracy, much less operating
time and lower device requirements compared to the other current state-of-the-
art deep learning algorithms. Furthermore, the analysis of facial expressions
when taken from the videos demonstrated that our proposed approach is able to
report the evolution of facial expression over a period of time, and reliably
detect the changes of expression by means of the recognition of facial
expression within each frame in the video.
## CRediT authorship contribution statement
**Zixiang Fei:** Conceptualization, Methodology, Software, Validation, Formal
analysis, Investigation, Resources, Data curation, Writing - original draft,
Writing - review & editing, Visualization. **Erfu Yang:** Conceptualization,
Methodology, Investigation, Resources, Writing - review & editing,
Supervision, Funding acquisition. **David Day-Uei Li:** Writing - review &
editing, Supervision. **Stephen Butler:** Writing - review & editing,
Supervision. **Winifred Ijomah:** Writing - review & editing, Supervision.
**Xia Li:** Investigation, Resources, Supervision. **Huiyu Zhou:** Writing -
review & editing, Supervision.
## Declaration of Competing Interest
The authors declare no conflict of interest.
## Acknowledgements
The research work is funded by Strathclyde's Strategic Technology Partnership
(STP) Programme with CAPITA (2016?2019). We thank Dr. Neil Mackin and Miss
Angela Anderson for their support. The contents in the paper are those of the
authors alone and don't stand for the views of CAPITA plc. Huiyu Zhou was
partly funded by UK EPSRC under Grant EP/N011074/1, and Royal Society in
Newton Advanced Fellowship under Grant NA160342. Winifred Ijomah is supported
under EPSRC (Grant no. EP/N018427/1). The authors thank Shanghai Mental Health
Center for their help and support. We also thank Dr. Fei Gao from Beihang
University, China for his kind support and comment.
## Ethical approval
This article does not contain any studies with human participants or animals
performed by any of the authors without the proper ethical approval.
Recommended articles"
55,57,Deep facial expression recognition: A survey,"['S Li', 'W Deng']",2020,1828,"Acted Facial Expressions In The Wild, Affective Faces Database, Binghamton University 3D Facial Expression, Japanese Female Facial Expression, MMI Facial Expression, Radboud Faces Database, Static Facial Expression in the Wild, Toronto Face Database","FER, deep learning, facial expression recognition, neural network","definition of facial expressions. In this survey, we limit our discussion on FER based on the  cat Pantic, “Induced disgust, happiness and surprise: an addition to the mmi facial expression",No DOI,IEEE transactions on affective computing,https://arxiv.org/abs/1804.08348,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
56,58,Deep imbalanced learning for face recognition and attribute prediction,"['C Huang', 'Y Li', 'CC Loy', 'X Tang']",2019,390,Toronto Face Database,"deep learning, machine learning","To mitigate this issue, contemporary deep learning methods typically follow classic strategies  such as class re- Given an imagery dataset with imbalanced class distribution, our goal is to",No DOI,… pattern analysis and machine …,https://arxiv.org/abs/1806.00194,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
57,59,Deep joint spatiotemporal network (DJSTN) for efficient facial expression recognition,"['D Jeong', 'BG Kim', 'SY Dong']",2020,105,MMI Facial Expression,"deep learning, facial expression recognition, neural network",facial emotion recognition systems with various signals. Most of them employ facial expression  recognition  : An addition to the mmi facial expression database. In Proceedings of the 3rd,No DOI,Sensors,https://www.mdpi.com/1424-8220/20/7/1936,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
58,60,Deep learning approaches for facial emotion recognition: A case study on FER-2013,"['P Giannopoulos', 'I Perikos', 'I Hatzilygeroudis']",2018,206,Affective Faces Database,FER,"the emotional behavior and the affective state of the human.  , for images of the Jaffe face  database with little noise and with  /non smiling expressions in the ORL database. In [32], a",No DOI,Advances in hybridization of …,https://link.springer.com/chapter/10.1007/978-3-319-66790-4_1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
59,61,Deep learning based facs action unit occurrence and intensity estimation,"['A Gudi', 'HE Tasli', 'TM Den Uyl']",2015,221,Toronto Face Database,deep learning,method on the SEMAINE dataset was much lower than on the BP4D dataset. One of the main  contributing factors to this observation is the low number of individual faces included in the,No DOI,… on automatic face and …,https://ieeexplore.ieee.org/document/7284873,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
60,62,Deep learning for biometrics: A survey,"['K Sundararajan', 'DL Woodard']",2018,314,Toronto Face Database,deep learning,"with the SVS2004 dataset, it can be observed that both deep learning and other approaches  seem to perform comparably. This could be attributed to smaller dataset size. However, for",No DOI,ACM Computing Surveys (CSUR),https://dl.acm.org/doi/10.1145/3190618,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
61,63,Deep learning for emotion recognition on small datasets using transfer learning,"['HW Ng', 'VD Nguyen', 'V Vonikakis']",2015,802,Affective Faces Database,deep learning,"dataset, when a sufficiently large face dataset such as FER-2013 is available, whether by  adding it to the auxiliary dataset  exploit deep neural networks such as CNN for face expression",No DOI,Proceedings of the 2015 …,https://dl.acm.org/doi/10.1145/2818346.2830593,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
62,64,Deep learning for human affect recognition: Insights and new developments,"['PV Rouast', 'MTP Adam', 'R Chiong']",2019,613,"Affective Faces Database, Toronto Face Database",deep learning,"deep neural networks, we comprehensively quantify their applications in this field. We find  that deep learning is used for learning  number of examples per dataset does not see a clear",No DOI,IEEE Transactions on …,https://arxiv.org/abs/1901.02884,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
63,65,Deep neural network augmentation: Generating faces for affect analysis,"['D Kollias', 'S Cheng', 'E Ververas', 'I Kotsia']",2020,135,"Affective Faces Database, Radboud Faces Database","CNN, classification, deep learning, neural network",train Deep Neural Networks over eight databases face database is annotated in terms of  valence and arousal and is then used for affect synthesis. The fact that this a temporal database,No DOI,International Journal of …,https://arxiv.org/abs/1811.05027,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
64,66,Deep neural networks with relativity learning for facial expression recognition,"['Y Guo', 'D Tao', 'J Yu', 'H Xiong', 'Y Li']",2016,127,Acted Facial Expressions In The Wild,"deep learning, neural network",Here we present a deep learning method termed Deep Neural Networks with Relativity   The SFEW 2.0 dataset was created from Acted Facial Expressions in the Wild (AFEW) [3] using,No DOI,2016 IEEE International …,https://ieeexplore.ieee.org/document/7574736,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
65,67,Deep spatial-temporal feature fusion for facial expression recognition in static images,"['N Sun', 'Q Li', 'R Huan', 'J Liu', 'G Han']",2019,116,MMI Facial Expression,"deep learning, facial expression recognition, neural network",MMI database holds 2885 videos and over 500 images of 88 subjects displaying various  facial expressions  output of the channel to recognize the facial expression. For MDSTFN with,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517303902,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Deep spatial-temporal feature fusion for facial expression
recognition in static images
Search 
## Outline
1. Highlights
2. ABSTRACT
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Proposed facial expression recognition method
8. 4\. Experiments and discussion
9. 5\. Conclusion
10. Acknowledgments
11. Appendix
12. References
Show full outline
## Cited by (92)
## Figures (13)
1. 2. 3. 4. 5. 6.
Show 7 more figures
## Tables (7)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Pattern Recognition Letters
Volume 119, 1 March 2019, Pages 49-61
# Deep spatial-temporal feature fusion for facial expression recognition in
static images
Author links open overlay panelNing Sun, Li Qi, Ruizhi Huan, Jixin Liu, Guang
Han
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2017.10.022Get rights and content
## Highlights* ?
Using optical flow to represent the temporal features of FER in static image.
* ?
A MDSTFN is present to extract and fuse the temporal and spatial features for
FER.
* ?
Using Average-face as a substitution for neutral-face.
* ?
Achieving recognition accuracy 98.38% on CK+, 99.17% on RafD, and 99.59% on
MMI.
## ABSTRACT
Traditional methods of performing facial expression recognition commonly use
hand-crafted spatial features. This paper proposes a multi-channel deep neural
network that learns and fuses the spatial-temporal features for recognizing
facial expressions in static images. The essential idea of this method is to
extract optical flow from the changes between the peak expression face image
(emotional-face) and the neutral face image (neutral-face) as the temporal
information of a certain facial expression, and use the gray-level image of
emotional-face as the spatial information. A Multi-channel Deep Spatial-
Temporal feature Fusion neural Network (MDSTFN) is presented to perform the
deep spatial-temporal feature extraction and fusion from static images. Each
channel of the proposed method is fine-tuned from a pre-trained deep
convolutional neural networks (CNN) instead of training a new CNN from
scratch. In addition, average-face is used as a substitute for neutral-face in
real-world applications. Extensive experiments are conducted to evaluate the
proposed method on benchmarks databases including CK+, MMI, and RaFD. The
results show that the optical flow information from emotional-face and
neutral-face is a useful complement to spatial feature and can effectively
improve the performance of facial expression recognition from static images.
Compared with state-of-the-art methods, the proposed method can achieve better
recognition accuracy, with rates of 98.38% on the CK+ database, 99.17% on the
RaFD database, and 99.59% on the MMI database, respectively.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
Deep neural network
Optical flow
Spatial-temporal feature fusion
Transfer learning
## 1\. Introduction
Facial expression recognition (FER) is an important aspect of face image
analysis and has been a popular research topic in computer vision for decades.
Recently, FER has been successfully implemented in the applications of smart
video surveillance, personal robot and interactive games [1]. Facial
expressions are the facial changes in response to a person's internal
emotional states, intentions, or social communications. In 1978, Ekman et al.
[2] reviewed psychological research on facial expressions and emotions and
found six emotions to be universal. The six universal emotions are disgust,
sadness, happiness, fear, anger, and surprise.
A FER system generally consists of three steps: image pre-processing, feature
extraction, and expression recognition. The main purpose of image pre-
processing is to crop the detected face from the image and align it to a fixed
size and position based on facial landmark points. Features are then extractedfrom the aligned face to represent the intrinsic property of facial
expressions. In the third step, a classifier is trained to recognize the
facial expression.
Due to the complexities of the human face and emotional expressions, together
with the effects of the illumination, perspective, and pose, effective feature
extraction is vital to FER. Feature extraction methods for FER can be divided
into geometry-based and appearance-based methods. Geometry-based methods
recognizes facial expressions by measuring the geometric characteristics of
the face, such as distance and curvature of facial fiducial points or salient
regions. The early work in this area was the Facial Action Coding System
(FACS), designed by Ekman[2]. FACS encoded a facial expression in 44 facial
Action Units (AUs), which defined the contraction of one or more facial
muscles. A face registration algorithm such as Active Shape Model (ASM) [3]
and Supervised Descent Method (SDM) [4] was then proposed for locating the
facial fiducial points. According to the deformation of these fiducial points,
Pantic's method [5] can determine the category of the facial expression.
Geometry-based methods are intuitive and convenient, but they are sensitive to
noise and disturbance and have difficulties describing some subtle changes in
the face. Appearance-based methods use appearance features to represent facial
expressions. These appearance features include image density, edge, texture,
and more discriminative features that are obtained through a local descriptor
or global space transformation such as LBP descriptor [6], PHOG descriptor
[7], Gabor filter [8], and Non negative Matrix Factorization (NMF) [9].
Compared with geometry-based methods, appearance-based methods have an obvious
advantage in robustness to noise and in retaining detailed information of the
facial expression.
In recent years, research on deep learning has had tremendous success in many
competitions and computer vision applications [10], [11], [12], [13], [14].
Deep learning methods stack a number of intermediate layers from input data to
a classification layer, and can automatically learn high-level semantic
features from a large amount of training data. FER methods based on CNN
(Convolutional Neural Network) [15], which extract a hierarchy of nonlinear
facial features by means of multi-layers of convolution and pooling, can
achieve higher rates of accuracy on several facial expression benchmarks.
Others deep neural network models such as Deep Belief Network (DBN) [16] and
Deep Boltzmann Machine (DBM) [17] have also been successfully applied to FER.
FER methods are commonly designed to deal with two types of input: a static
image and a dynamic image sequence (video). Image-oriented methods determine
the class of facial expression from a single still image. This image indicates
the momentary appearance of a facial expression (usually, the apex of the
expression). Compared with a static image and momentary expression, video
shows the temporal changes to facial appearance when an expression occurs.
Besides the spatial features extracted from a single face image, video-
oriented methods can represent the temporal features of expressions from
dynamic transitions of between different stages of an expression rather than
their corresponding static key frames. Generally, a video-oriented method can
achieve a better recognition accuracy, because it can extract spatial and
temporal features from dynamic image sequences. However, video-oriented
methods recognize expressions from large-scale image sequences, which
inevitably lead to higher computational complexity and can also introduce
noise and disturbance.
In this paper, we present a FER method to simultaneously extract and fuse the
temporal and spatial features from static face images. This method can not
only improve the recognition performance of FER from static images, but alsoavoid the processing cost of large-scale image sequences. As an initial step,
we assume that the face image with expression (emotional-face) implicitly
contains the temporal information of the changes in facial appearance, which
can be obtained by measuring the difference between emotional-face and the
face image without expression (neutral-face). This kind of difference is
represented by means of the optical flow between emotional-face and neutral-
face. A Multi-channel Deep Spatial-Temporal feature Fusion neural Network
(MDSTFN) is proposed to extract and fuse the spatial-temporal features from
the static face image. There are several deep neural network channels in
MDSTFN. One channel is used to extract spatial features from gray-level image
of emotional-face, and the others are utilized to extract temporal features
from optical flow images. These temporal and spatial features are fused by
several fusion schemes to generate the final output of recognition results.
This multi-channel deep neural network architecture has been successfully
applied in the field of video-oriented FER [18] and action recognition [19],
[20].
The proposed method uses pre-trained deep neural network models, which are
fine-tuned on a facial expression database to build MDSTFN instead of
designing a new network model trained from scratch. These pre-trained deep
models were trained on large-scale image databases (such as ImageNet) and have
achieved the top performance in the recent years in object recognition tasks
set by the ImageNet Large-Scale Visual Recognition Competition (ILSVRC). The
pre-trained models are employed at an initialization stage and fine-tuned to
fit the FER task on the benchmark facial expression databases. Transfer
learning from pre-trained models can offer a strong initial feature extractor
and make the large-scale CNN model easier to fine-tune on a limited facial
expression database. It is also convenient for research as the feature
extraction channels of MDSTFN can be substituted with more powerful deep
models in the future. However, it is usually difficult to find the neutral-
face corresponding to a certain emotional-face in real-world applications. To
solve this problem, we use average-face, which is the average of a large
number of facial images, to replace neutral-face for computing optical flow.
The distinctive features of this research can be summarized as follows:
* (1)
We use the optical flow extracted from the changes between emotional-face and
neutral-face to represent the temporal features of a facial expression in a
static image;
* (2)
An MDSTFN is present to extract and fuse the temporal and spatial features of
the facial expression in a static image. This architecture not only improves
the of recognition performance but also makes the proposed method easy to
update;
* (3)
Using Average-face as a substitute for neutral-face makes the proposed method
suitable for implementation in real-world applications;
* (4)
We conduct extensive experiments to comprehensively evaluate the proposed FER
method on benchmarks including CK+, MMI, and RaFD. Results show that the
optical flow is able to represent the temporal changes to facial expression in
a static image. It can effectively improve the performance of FER by fusing
these spatial-temporal features extracted by the proposed MDSTFN-based method.
## 2\. Related work
In this section, we review recent related work in FER which uses methods based
on deep learning architecture. Firstly, as regards the image-oriented FERmethod, Ranzato et al [21] presented a DBN-based deep generative model
composed of three layers to learn expression features from a face image; this
worked well when a face image was disrupted by heavy occlusion. Liu et al.
[16] developed a Boosted Deep Belief Network (BDBN) for FER. This method
attempted to perform feature learning, feature selection, and classifier
construction in a loopy process. He et al. [17] proposed a Deep Boltzmann
Machine model composed of a two-layers Boltzmann machine for FER from infrared
images. The advantage of the DBN model is that it can be pre-trained in an
unsupervised way, but the DBN model has difficulties to dealing with a large
number of high-resolution images due to its fully-connected structure.
The essential characteristics of a CNN model are local receptive field and
weight sharing, which give it a significant advantage in dealing with high-
resolution data such as images and video. Khorrami et al. [22] proposed a CNN
that ignored the biases of the convolutional layers; this zero-bias CNN was
trained on facial expression data and achieved good performance on two
expression recognition benchmarks. Burkert et al [23] proposed a novel CNN-
based FER method called DeXpression. The core of their proposed CNN model
comprised two Parallel Feature Extraction (FeatEx) blocks; FeatEx blocks
create two parallel paths of features with different scales. Liu et al. [24]
constructed an AU-inspired Deep Network (AUDN) architecture, which was
inspired by Ekman's psychological theory that expressions can be decomposed
into multiple facial Action Units (AUs). AUDN consisted of three sequential
modules; Micro-Action-Pattern (MAP) representation learning, receptive field
construction and group-wise subnetwork learning. Lopes et al. [25] presented a
FER system that used a combination of CNN and specific image pre-processing
steps. These pre-processing steps were used to augment the presentation order
of the samples for training the deep neural network. Hamester et al. [26]
proposed a two-channel deep learning architecture for a FER system. One
channel was a standard CNN; the second had the same topology but its first
layer weights were trained as a Convolutional AutoEncoder (CAE). The two
channels were connected with a fully connected (FC) layer that generates the
output for recognition.
For video or sequential image data, the correlation between consecutive frames
provides discriminative information for FER. Jung et al. [27] proposed a two-
channel deep neural network to recognize expressions from video data. The
first channel extracted temporal appearance features from image sequences,
while the second extracted temporal geometric features from temporal facial
landmark points. These two channels were combined using joint fine-tuning to
boost the performance of FER. Byeon et al. [28] developed a 3D-CNN to learn
spatial-temporal expression information from five successive frames. The
authors claimed that the 3D-CNN model can handle some degrees of shift and
deformation invariance.
Compared with the aforementioned FER methods, this paper presents a MDSTFN
model that extracts and fuses the spatial-temporal features for FER from a
static image. We use optical flow extracted from the changes between
emotional-face and neutral-face to represent the temporal features of facial
expression. Based on transfer learning, the channels in MDSTFN are fine-tuned
from pre-trained CNN models. This not only can solve the problem of
insufficient facial expression training for large-scale deep neural networks,
but also facilitates future improvement.
## 3\. Proposed facial expression recognition method
The proposed MDSTFN-based FER method involves the following three steps: image
pre-processing, deep spatial-temporal feature learning, and feature fusion and
recognition. A schematic diagram of the proposed method is shown in Fig. 1.1. Download: Download high-res image (539KB)
2. Download: Download full-size image
Fig. 1. The architecture of the MDSTFN-based FER method.
### 3.1. Image pre-processing
The image pre-processing step includes two parts: face alignment and optical
flow extraction. Face alignment locates the key points of face and deforms the
face image to a fixed size and position. The optical flow features are
extracted from the differences between emotional-face and neutral-face.
Whether the positions of face parts of two images are aligned or not can
greatly affect the performance of optical flow extraction. In our work, the
input face images are aligned by the ASM algorithm.
The key pre-processing of the proposed method is the extraction of optical
flow. In comparison with extracting multiple optical flow from consecutive
frames in video data, we use only one optical flow image computed from
difference between emotional-face and neutral-face to represent the temporal
changes in a static expression. This strategy can effectively capture the
facial changes when a certain expression occurs, and reduce the computational
cost of a frame-by-frame optical flow extraction. The optical method proposed
by Brox [29] is chosen as the optical flow extractor in this paper. This
optical method integrates a coarse-to-fine warping strategy and implements the
non-linear optical flow constraint to yield excellent results even under a
considerable amount of noise. Images of optical flow extracted by this method
are shown in Fig. 6.
### 3.2. Multi-channel deep spatial-temporal feature fusion neural network
(MDSTFN)
After the aforementioned pre-processing, we have three channels of input data.
One channel is the gray-level image of emotional-face; the others two the X
and Y components of optical flow extracted from emotional-face and neutral-
face. In order to learn and fuse spatial-temporal features from three channels
of input data, a Multi-channel Deep Spatial-Temporal feature Fusion neural
Network (MDSTFN) is constructed. The architecture of MDSTFN is shown in Fig.
2. The pre-trained CNN model is used to form the feature extraction channel of
MDSTFN. To fit the FER task, transfer learning is performed to fine-tune the
pre-trained CNN model on facial expression databases. There are two benefits
of using a pre-trained CNN model instead of training a new CNN model from
scratch. Firstly, these publicly available pre-trained CNN models have strong
generalized capabilities in image feature representation. Transfer-learning
from a pre-trained model is an effective way to improve the performance of
training since a facial expression image database is usually small. Secondly,
the pre-trained CNN model can be replaced easily by a new better deep neural
network model in the future.
1. Download: Download high-res image (197KB)
2. Download: Download full-size image
Fig. 2. The architecture of Multi-channel Deep Spatial-Temporal feature Fusion
neural Network.
According to recent developments in deep learning, the most straightforward
way of improving the performance of deep neural networks is by increasing
their size. The size of representative CNN architectures has dramatically
increased from AlexNet [11] with 8 layers in 2012, to VGG [12] with 19 layers,
and GoogLeNet [13] with 22 layers in 2014, then to ResNet [15] with 152 layers
in 2015. Deeper neural networks achieve better performance in large scale
image recognition. However, a bigger size typically means a larger number of
parameters, which makes the network more prone to overfitting. Especially with
regard to the task of FER, the number of labeled examples in the training setis limited. It is always difficult to obtain a satisfactory performance on a
large size neural network with fewer training samples. In this paper, we use a
simplified version of GoogLeNet (GoogLeNetv2), which comprises the layers
below the second softmax output of GoogLeNet, as the feature extraction
channel in the MDSTFN. The size of the receptive field in GoogLeNetv2 is 224 ×
224. This is followed by six Inception modules and several convolution and
pooling layers. The detail of GoogLeNetv2 is shown in Fig. 3; details of the
Inception module are shown in Fig. 4. The bottom layers of convolution and
pooling reduce significantly the dimension of input data and extract low-level
features from the face image. The six Inception modules basically act as
multiple convolution filters that process the same input and also do pooling
at the same time. This allows the model to take advantage of multi-level
feature extraction from each input.
1. Download: Download high-res image (169KB)
2. Download: Download full-size image
Fig. 3. The architecture of GoogLeNetv2.
1. Download: Download high-res image (167KB)
2. Download: Download full-size image
Fig. 4. The detail of Inception module.
The parameters of GoogLeNetv2 (as trained on the ImageNet database) are
retained to initialize the training of the MDSTFN-based method. We fine-tune
GoogLeNetv2 on facial expression benchmark databases to fit the FER task.
Based on transfer learning like this, we can obtain a better CNN classifier on
a small number of image samples while reducing the effect of overfitting.
### 3.3. Spatial-temporal feature fusion
We investigate three approaches to fusing temporal and spatial information
from three feature extraction channels, as shown in Fig. 5. They are: score
averaging fusion, Support Vector Machine (SVM) based fusion and neural network
based fusion. A detailed comparison of the three fusion methods follows a in
the discussion of experiments (4.4 below).
1. Download: Download high-res image (324KB)
2. Download: Download full-size image
Fig. 5. Three different fusion strategies. Red, blue, yellow, and black boxes
indicate convolutional, pooling, fully-connected and softmax layers
respectively. The larger green dashed line boxes are Inception module. (For
interpretation of the references to color in this figure legend, the reader is
referred to the web version of this article.)
_Score Averaging Fusion (SAF):_ We average the softmax output of three
channels of MDSTFN to fuse the spatial-temporal feature. The expression
corresponding to the max value of fusion is the recognition result. SAF is
easy to compute but the improvement in performance is limited.
_SVM_ _based Fusion (SVMF):_ After being independently fine-tuned on a facial
expression image database, the output of the top FC layer of GoogLeNetv2 is
concatenated in order to train a SVM-based classifier. The SVM-based
classifier performs feature fusion and FER simultaneously.
_Neural Network based Fusion (NNF):_ At first, three channels of MDSTFN are
fine-tuned independently. We then adjust the model of the fine-tuned MDSTFN.
The softmax layers of the three channels are removed, and a new FC layer is
added which is now connected to the top FC layers of the three channels; the
new FC layer is followed by a softmax layer. We then freeze the rest layers of
MDSTFN and just train the top two FC layers of MDSTFN. When a new face image
is input, the MDSTFN can generate directly the final recognition result.
### 3.4. Replacing neutral-face with average-face
In this method, the key step of the proposed method is to compute the opticalflow between emotional-face and neutral-face. Generally, neutral-face images
exist in most facial expression databases, but it is not easy to find the
neutral-face corresponding to a certain emotional-face in real-world
application. To address this problem, average-face can be used to replace
neutral-face when our MDSTFN-based FER method is applied in a real-world
application. Average-face is the average of a large number of faces, which
effectively smooths the appearance differences in face images caused by
changes in expression and illumination. Although there is still a difference
between average-face and neutral-face, it has very little influence on FER, as
shown by the results of subsequent experiments (4.5 below).
## 4\. Experiments and discussion
In this section, we conduct extensive experiments to comprehensively evaluate
the proposed FER method. The experiments are performed on three publicly
facial expression benchmarks: the extended Cohn?Kanade (CK+) database [30];
the Radboud faces database (RaFD) [31]; and the MMI facial expression database
[32]. These three benchmarks contain facial expression images obtained from a
wide variety of subjects (i.e., as regards age, gender, and ethnicity).
### 4.1. Database and experimental protocols
**CK+ database** consists of 593 sequences from 123 subjects, which is an
extended version of Cohn-Kanade (CK) database. The subjects in the database
are 81%, Euro-American, 13% Afro-American, and 6% other groups with 69%
female, from 18 to 50 years of age. The validating emotion labels were only
assigned to 327 sequences which were found to meet criteria for one of 7
discrete emotions (anger, contempt, disgust, fear, happiness, sadness, and
surprise). Each of the sequences contains images from onset (neutral frame) to
peak expression (last frame). In our experiments, we firstly select seven
successive the most peak images in each sequence, and the first, fourth, and
seventh images are collected to be as the emotional-face. The reason why we do
not use three consecutive face images is to reduce the sample correlation
caused by too-similar face images. And, the first image in each sequence is
used to be as the neutral-face. So we have 1083 emotional-face images and 123
neutral-face images.
**MMI database** holds 2885 videos and over 500 images of 88 subjects
displaying various facial expressions on command, where 236 videos have basic
emotion annotation (anger, disgust, fear, happiness, sadness, surprise,
scream, bored, and sleepy). There is not fixed pattern of expression change in
each video. Some videos begin with neutral state, and others begin with peak
expression. So we manually select 5042 emotional-face images and 88 neutral-
face images from these videos for the experiments.
**RaFD database** is a set of pictures of 67 models (including Caucasian males
and females, Caucasian children, both boys and girls, and Moroccan Dutch
males) displaying 8 emotional expressions, which are anger, disgust, fear,
happiness, sadness, surprise, contempt, and neutral. Each emotion was shown
with three different gaze directions and all pictures were taken from five
camera angles simultaneously. Only the front view images are collected for
training the MDSTFN-based FER method. There are 1407 emotional-face images and
201 neutral-face images. Compared with other two database, the image quality
and acquisition environment of RaFD database is the best.
We train the proposed method using two kinds of experimental protocol. The
first is subject-independent. Using this protocol, we construct ten person-
independent subsets by sampling in ID ascending order with a step size equal
to 10, which means that if image of subject X is in one group, no image of X
will be in any other group). The second protocol is random-division. Using
this protocol, samples are randomly partitioned into ten groups of equal sizewithout constraint of person independence between groups. With both training
protocols, ten-fold cross-validation is used to test the recognition accuracy
of the proposed method. The experimental protocol are specified in the
following experiments.
The steps of facial alignment and optical flow extraction are developed in the
OpenCV and C++environment, and the MDSTFN is trained using Caffe Python API.
All experiments run on an image processing workstation with an Intel Xeon 2.4
GHz 8 core CPU, 128GB memory, and two NVIDIA Titan X GPUs.
### 4.2. Evaluation of different channel combinations
The primary idea of the proposed method is to extract optical flow between
emotional-face and neutral-face as the information of temporal change, and to
fuse this with complementary information from the gray-level emotional-face to
improve the performance of FER. In order to verify the effectiveness of this
idea, samples of six universal emotional-face and the corresponding
directional components of optical flow are extracted from the CK+, RaFD, and
MMI databases, as shown in Fig. 6. It can clearly be seen that there is a
unique pattern for each expression although these face images are captured
from subjects of different age, gender, and ethnicity, and even though some
images have facial accessories such as glass and scarves. For example, in the
expression of surprise, the significant change of face is opening one's eyes
and mouth widely. The corresponding Y-component of optical flow has a strong
response in the areas of the eyes and mouth. There are similar results in
other expressions. Compared with the X-component of optical flow, the
Y-component of optical flow looks more discriminative for FER. The main result
is that the change in appearance caused by facial expression change in the Y
direction is stronger than the one in the X direction.
1. Download: Download high-res image (573KB)
2. Download: Download full-size image
Fig. 6. Optical flow extracted from three databases cross different
expressions.
We showed above that optical flow extracted between emotional-face and
neutral-face can represent the temporal features of a facial expression. In
the next step, we conduct experiments to assess the performance of the MDSTFN-
based FER method with different channel combinations on the CK+, RaFD, and MMI
databases. For the MDSTFN-based FER method with a single channel, we use the
softmax output of the channel to recognize the facial expression. For MDSTFN
with two or three channels, the NNF method is used to fuse the spatial-
temporal features and generate the final recognition result. These experiments
are conducted based on the random-division protocol. The recognition accuracy
is shown in Table 1.
Table 1. The recognition accuracy of the MDSTFN-based FER method with
different channel combinations on three benchmark databases. The letter G, Y,
and X in the second row means gray-level emotional-face, Y component of
optical flow, and X component of optical flow, respectively.
Database| Channel
---|---
Empty Cell| G| X| Y| G + X| G + Y| X + Y| G + X + Y
CK+| 89.60%| 66.52%| 65.06%| 96.65%| 96.89%| 85.27%| **98.38** %
RaFD| 93.19%| 45.27%| 75.88%| 98.33%| **99.17%**| 82.37%| 98.75%
MMI| 85.09%| 82.10%| 94.62%| 99.40%| 99.50%| 99.08%| **99.59%**
The proposed method is able to achieve an accuracy of 66.52% and 65.09% using
only the Y or X component of optical flow respectively on the CK+ databases.
Obviously, the recognition accuracy improves greatly when fusing the spatial-
temporal feature: 98.38% for G+X+Y up from 89.6% for G on the CK+ database;99.17% for G + Y up from 93.19% for G on the RaFD database; and 99.59% for G +
X + Y up from 85.09% for G on the MMI database. The recognition accuracy shows
once again that the optical flow extracted between emotional-face and neutral-
face plays an effective complementary role to gray-level emotional-face.
Fusing the spatial-temporal features extracted in this way can significantly
improve the performance of FER. From Table 1, it can be seen that the results
achieved by Y or Y + G are generally better than the results achieved by X or
X + G. This confirms that the Y-component of optical flow is more
discriminative for FER than the X-component of optical flow, as also
demonstrated in Fig. 6.
We show the confusion matrixes of the proposed method using channel G or using
channel G + X + Y on three benchmark facial expression database in Fig. 7. It
can be clearly seen that those expressions hardly distinguished by the
proposed method only using gray-level emotion-face are effectively classified
by the proposed method combining gray-level emotion-face and optical flow
information, such as anger vs. fear in the CK+ and MMI database. The rest 15
confusion matrixes of our method using various channel combinations are shown
in Appendix.
1. Download: Download high-res image (446KB)
2. Download: Download full-size image
Fig. 7. Confusion matrixes of the proposed method using various channel
combination on CK+, RaFD, and MMI database, which is shown in the first,
second and third row, respectively. The first column is the results by the
proposed method using one feature channel of gray-level emotional-face. The
second column is the results by the proposed method using three feature
channel including gray-level emotional-face, Y-component, and X-component of
optical flow.
### 4.3. Evaluation of different pre-trained CNN models
We train the MDSTFN-based FER method using four publicly available CNN models;
namely, AlexNet and three versions of GoogLeNet. The best model in this
comparison is chosen as the model for feature extraction in the proposed
method. The three versions of GoogLeNet are GoogLeNetv1 (the layers below the
first softmax output of GoogLeNet); GoogLeNetv2 (the layers below the second
softmax output of GoogLeNet); and the full GoogLeNet. Other CNN models like
VGG and ResNet are not tested in the experiment because of the lack of pre-
trained models. The experiments are conducted using the random-division
protocol.
Table 2 shows the accuracy of the proposed FER method with the four CNN
models. Overall, the results of the three GoogLeNet versions are better than
that of AlexNet under almost all kinds of feature extraction combinations. It
shows that the deeper architecture and Inception module provide more powerful
capabilities in non-linear feature representation. GoogLeNetv2 (the layers
below the second softmax output of GoogLeNet), achieves the best recognition
accuracy in comparison with GoogLeNetv1 and GoogLeNet. This shows that the
model complexity of GoogLeNetv2 is most suited for the sample size of the
three facial expression databases (about 1 k to 5 k). Larger scale model like
GoogLeNet always results in overfitting while smaller scale model like
GoogLeNetv1 leads to under-fitting.
Table 2. The recognition accuracy of the MDSTFN-based FER method with
different models on three benchmark databases.
Database| Model| Channel
---|---|---
Empty Cell| Empty Cell| G + X| G + Y| X + Y| G + X + Y
CK+| AlexNet| 79.56%| 87.56%| 88.89%| 88.89%| GoogLeNet| 93.99%| 95.08%| 74.5%| 93.44%
| GoogLeNetv1| 91.26%| 92.90%| 70.33%| 91.80%
| GoogLeNetv2| 96.65%| 96.89%| 85.27%| **98.38%**
RaFD| AlexNet| 84.36%| 96.42%| 90.11%| 96.9%
| GoogLeNet| 98.75%| 97.08%| 78.75%| 97.92%
| GoogLeNetv1| 94.58%| 97.92%| 77.80%| 97.08%
| GoogLeNetv2| 98.33%| **99.17%**| 82.37%| 98.75%
MMI| AlexNet| 93.23%| 92.90%| 92.71%| 93.82%
| GoogLeNet| 96.56%| 94.32%| 89.77%| 96.43%
| GoogLeNetv1| 92.33%| 93.91%| 90.8%| 93.0%
| GoogLeNetv2| 99.41%| 99.50%| 98.97%| **99.59%**
### 4.4. Evaluation of different fusion strategies
We test the proposed method with different fusion strategies: SAF, SVMF, and
NNF (see Section 3.3 above). The experiments are conducted using the random-
division protocol. Table 3 shows the recognition results on three benchmark
databases. NNF is the best fusion method compared with SVMF and SAF. The
accuracy achieved by the FER method with NNF under different channel
combinations is about 0.5% to 6% higher than the one of method with SVMF, and
is about 10% higher than the one of method with SAF. SAF is a decision-level
fusion that simply uses the average of the softmax output of the feature
extraction channel as the fusion result. It is computationally efficient, but
the improvement in recognition performance by SAF fusion is quite limited. The
input of SVMF comes from the concatenation of the outputs of multi-channels.
The SVM-based classifier can learn further from the relationships among the
concatenated features, but the concatenation is a kind of rough fusion
strategy and hard to achieve optimal performance. NNF on the other hand is
treated as a multi-input fully-connected neural network. Instead of simply
averaging or concatenation, the output of MDSTFN is learning from the three-
layers of the neural network. Hence the feature fusion ability of NNF is more
powerful than that of SAF and SVMF.
Table 3. The recognition accuracy of the MDSTFN-based FER method with fusion
strategies on three benchmark databases.
Database| Fusion strategy| Channel
---|---|---
Empty Cell| Empty Cell| G + X| G + Y| X + Y| G + X + Y
CK+| SAF| 91.07%| 92.55%| 80.76%| 93.28%
| SVMF| 95.17%| 95.08%| 80.83%| 95.58%
| NNF| 96.65%| 96.89%| 85.27%| **98.38%**
RaFD| SAF| 92.43%| 95.66%| 77.6%| 95.63%
| SVMF| 97.05%| 97.54%| 78.54%| 97.15%
| NNF| 98.33%| **99.17%**| 82.37%| 98.75%
MMI| SAF| 93.5%| 97.63%| 97.04%| 98.96%
| SVMF| 98.74%| 98.08%| 95.35%| 99.11%
| NNF| 99.41%| 99.50%| 98.97%| **99.59** %
### 4.5. Evaluation of the proposed method using average-face
Average-face is the alternative to neutral-face when the proposed method is
used in real-world applications. We evaluate the impact of using average-face
on the proposed method. Two kinds of average-face images, which are average-
face of man and average-face of woman, are generated by averaging all male or
female face images in the three benchmark databases, respectively. As Fig. 8
shows, average-face of man and average-face of woman are well aligned and
shows a neutral state.
1. Download: Download high-res image (92KB)
2. Download: Download full-size imageFig. 8. Average-face of man and average-face of woman.
The results of the proposed method using average-face compared to neutral-face
are shown in Table 4. The experiments are conducted using the random-division
protocol. Table 4 shows that the proposed method can still extract the
temporal feature effectively from the optical flow between emotional-face and
average-face. The proposed method using average-face can also achieve
satisfactory recognition results, such as 97.08% on the CK+ database, 98.34%
on the RaFD database, and 99.38% on the MMI database. Additionally, the
proposed method using average-face is able to achieve a result close to the
proposed method using neutral-face. There is only a slight decline in accuracy
when we use average-face instead of neutral-face, such as 1.3% (from 98.38% to
97.08%) on the CK+ database, 0.83%(from 99.38% to 99.17%) on the RaFD
database, and 0.21% (from 99.59% to 99.38%) on the MMI database. The main
cause of performance degradation in the proposed method using average-face is
that average-face cannot replicate the neutral-face corresponding to a certain
image of emotional-face. We believe that the recognition accuracy of the
proposed method using average-face can be further improved when more kinds of
average-face with a wider range of age and ethnicity are used.
Table 4. The recognition accuracy of the MDSTFN-based FER method using
average-face and neutral-face on three benchmark databases. The letter A and N
in the second column means average-face and neutral-face, respectively.
Database| Empty Cell| Channel
---|---|---
Empty Cell| Empty Cell| G| X| Y| G + X| G + Y| X + Y| G + X + Y
CK+| A| 89.60%| 51.18%| 59.19%| 96.54%| 96.90%| 83.97%| **97.08%**
| N| 89.60%| 66.52%| 65.06%| 96.65%| 96.89%| 85.27%| **98.38%**
RaFD| A| 93.19%| 42.17%| 73.33%| 97.01%| **98.34%**| 80.63%| 97.64%
| N| 93.19%| 45.27%| 75.88%| 98.33%| **99.17%**| 82.37%| 98.75%
MMI| A| 85.09%| 82.12%| 86.42%| 99.41%| **99.38%**| 99.42%| 99.27%
| N| 85.09%| 82.03%| 94.57%| 99.41%| 99.50%| 98.97%| **99.59%**
### 4.6. Evaluation of cross-database performance
The cross-database experiment is a good evaluation of generalization
performance of the proposed method in real environments. In these experiments,
one database is used for evaluation and the other databases are used to train
the network. The experiments are conducted using the random-division protocol.
Table 5 shows the results of the cross-database experiments. As can be seen,
the recognition accuracy has decreased considerably. It shows that the
differences between training database and test database have a negative impact
on the accuracy of the proposed method. Also, the recognition accuracies of
the model trained by samples without the RaFD database is higher than those
trained by samples with the RaFD database. This shows that the generalization
of the model trained by a database with limited constraints is better than
that of the model trained by a database captured under ideal environments.
Table 5. Recognition results of the proposed method in cross-database
experiments.
Training database| Test database| Channel
---|---|---
Empty Cell| Empty Cell| G + X| C + Y| G + X + Y
CK+,RaFD| MMI| 59.27%| 61.67%| 67.18%
CK+,MMI| RaFD| 77.53%| 82.21%| 86.80%
MMI, RaFD| CK+| 66.22%| 68.57%| 75.13%
### 4.7. Comparison with state-of-the-art
In this section, we compare the performance of the proposed method with state-
of-the-art FER methods on the CK+, RaFD, and MMI benchmark databases. Theserecently reported FER methods are all based on the architecture of deep neural
networks. FER methods based on shallow learning are not tested here, because
the superiority of the deep learning method has been proven in a lot of object
recognition tasks.
Although they use the same databases, these ten state-of-the-art methods are
tested using two kinds of experimental protocol. The results reported in the
literature [33], [23], [36] are obtained using the random-division protocol,
while the results reported in the literature [24], [15], [18], [35], [25],
[22], [34] are achieved using the subject-independent protocol. To perform a
valid comparison, we test the proposed method using both protocols. The
comparison results using the subject-independent protocol are listed in Table
6, and the results based on the random-division protocol are listed in Table
7.
Table 6. Recognition accuracy comparison with state-of-the-art methods based
on the subject-independent protocol.
Method| Database
---|---
Empty Cell| CK+| RaFD| MMI
Liu et al. [24]| 93.70%| | 73.85%
Mollahosseini et al. [15]| 93.20%| | 77.60%
Jung et al. [18]| 97.25%| | 70.24%
Liu et al. [36].| 92.05%| | 74.76%
Lopes et al. [25]| 96.76%| |
Khorrami et al. [22]| **98.30%**| |
Zhou et al. [34]| 97.50%| 97.75%|
Our method (G + Y + A)| 95.93%| 97.02%| 89.79%
Our method (G + X + Y + A)| 96.48%| 97.40%| 90.21%
Our method (G + Y + N)| 96.26%| 98.13%| 91.03%
Our method (G + X + Y + N)| 97.28%| **98.17%**| **91.46%**
Table 7. Recognition accuracy comparison with state-of-the-art methods based
on the random-division protocol.
Method| Database
---|---
Empty Cell| CK+| RaFD| MMI
Majumder et al. [33].| 98.95%| | 97.55%
Burkert et al. [23]| **99.60** %| | 98.63%
Ali et al. [35]| | 93.75%|
Our method (G + Y + A)| 96.90%| 98.34%| 99.38%
Our method (G + X + Y + A)| 97.08%| 97.64%| 99.27%
Our method (G + Y + N)| 96.89%| **99.17%**| 99.50%
Our method (G + X + Y + N)| 98.38%| 98.75%| **99.59%**
As the results in Tables 6 and 7 show, the accuracy of almost all versions of
the proposed method (using average-face or neutral-face, random-division or
subject-independent protocol, and including various models and parameters such
as different feature channels) is better than the results of other deep
learning based methods on three benchmark facial expression databases.
It is worth noting that the experimental protocol of the results reported by
the literatures listed in Tables 6 and 7 is not exactly the same as that of
our method as there is no an accepted experimental standard for FER. Besides,
most researchers do not describe the experimental parameters in detail.
Especially in the experiments on the CK+ and MMI databases, researchers often
collect the training images from videos according to their own principles.
Although the proposed method does not achieve the highest recognition accuracy
on CK+ database, we can safely draw the conclusion that our MDSTFN-basedmethod is a successful deep spatial-temporal feature extraction architecture
for FER and can achieve a better recognition performance compared with state-
of-the-art.
## 5\. Conclusion
This paper presented a deep neural network architecture with multi-channels to
extract and fuse the spatial-temporal features of static image for FER. The
optical flow computed from the changes between emotional-face and neutral-face
is used to represent the temporal changes of expression, while the gray-level
image of emotional-face is used to provide the spatial information of
expression. The feature extraction channels of the MDSTFN (Multi-channel Deep
Spatial-Temporal feature Fusion neural Network) are fine-tuned from a pre-
trained CNN model. This transfer learning scheme can not only obtain a better
feature extractor for FER on a small number of image samples but can also
reduce the risk of overfitting. Three kinds of strategies were investigated to
fuse the temporal and spatial features obtained by multiple feature channels.
On three benchmark databases (CK+, RaFD, and MMI), extensive experiments were
conducted to evaluate the proposed method under various parameters such as
channel combination, fusion strategy, cross-database, and pre-trained CNN
models. The results show that the MDSTFN-based method is a feasible deep
spatial-temporal feature extraction architecture for facial expression
recognition. Replacing neutral-face with average-face improves the
practicality of the proposed method, while the results of a comparison show
that the MDSTFN-based method can achieve better accuracy than state-of-the-art
methods.
## Acknowledgments
This work was supported by the National Nature Science Foundation of China
(61471206 and 61401220), Natural Science Foundation of Jiangsu province
(BK20141428 and BK20140884) and Science Foundation of Ministry of Education-
China Mobile Communications Corporation(MCM20150504).
## Appendix
Figs. A1?A5
1. Download: Download high-res image (275KB)
2. Download: Download full-size image
Fig. A1. Confusion matrix of the proposed method using X on three database.
1. Download: Download high-res image (264KB)
2. Download: Download full-size image
Fig. A2. Confusion matrix of the proposed method using Y on three database.
1. Download: Download high-res image (243KB)
2. Download: Download full-size image
Fig. A3. Confusion matrix of the proposed method using G + X on three
database.
1. Download: Download high-res image (249KB)
2. Download: Download full-size image
Fig. A4. Confusion matrix of the proposed method using G + Y on three
database.
1. Download: Download high-res image (263KB)
2. Download: Download full-size image
Fig. A5. Confusion matrix of the proposed method using X + Y on three
database.
Special issue articlesRecommended articles"
66,68,Deep-emotion: Facial expression recognition using attentional convolutional network,"['S Minaee', 'M Minaei', 'A Abdolrashidi']",2021,664,"Affective Faces Database, Extended Cohn-Kanade, Japanese Female Facial Expression, Toronto Face Database","FER, deep learning, facial expression recognition","FER dataset, and the images in the second and fourth rows belong to the extended Cohn-Kanade   images in the second and fourth rows belong to the extended Cohn-Kanade dataset.",No DOI,Sensors,https://www.mdpi.com/1424-8220/21/9/3046,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
67,69,Deeply learning deformable facial action parts model for dynamic expression analysis,"['M Liu', 'S Li', 'S Shan', 'R Wang', 'X Chen']",2015,389,MMI Facial Expression,CNN,"evaluated on two posed expression datasets, CK+, MMI, and a  is how to represent different  facial expressions. In the past  is applying CNN or 3D CNN directly to expression analysis,",No DOI,Computer Vision--ACCV 2014: 12th …,https://link.springer.com/chapter/10.1007/978-3-319-16817-3_10,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
68,70,Development of a real-time emotion recognition system using facial expressions and EEG based on machine learning and deep neural network methods,"['A Hassouneh', 'AM Mutawa', 'M Murugappan']",2020,239,Affective Faces Database,machine learning,A convolutional neural network was used in our system to obtain improved facial emotion  detection as it is applied to other computer fields such as face recognition [25] and object,No DOI,Informatics in Medicine …,https://www.sciencedirect.com/science/article/pii/S235291482030201X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Development of a Real-Time Emotion Recognition System
Using Facial Expressions and EEG based on machine learning and
deep neural network methods
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Methods and materials
7. 3\. Data validation
8. 4\. Experimental results and discussion
9. 5\. Conclusion and future work
10. Ethical statement
11. Declaration of competing interest
12. Acknowledgment
13. Appendix A. Supplementary data
14. Research Data
15. References
Show full outline
## Cited by (174)
## Figures (11)
1. 2. 3. 4. 5. 6.
Show 5 more figures
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Extras (1)
1. Multimedia component 1
## Informatics in Medicine Unlocked
Volume 20, 2020, 100372
# Development of a Real-Time Emotion Recognition System Using Facial
Expressions and EEG based on machine learning and deep neural network methods
Author links open overlay panelAya Hassouneh a, A.M. Mutawa a, M. Murugappan b
Show more
Outline
Add to Mendeley
Share
Citehttps://doi.org/10.1016/j.imu.2020.100372Get rights and content
Under a Creative Commons license
open access
## Highlights
* ?
Classify emotional expressions based on facial landmarks and EEG signals.
* ?
The system allows real-time monitoring of physically disabled patients.
* ?
The system works effectively in uneven lighting and various skin tones.
## Abstract
Real-time emotion recognition has been an active field of research over the
past several decades. This work aims to classify physically disabled people
(deaf, dumb, and bedridden) and Autism children's emotional expressions based
on facial landmarks and electroencephalograph (EEG) signals using a
convolutional neural network (CNN) and long short-term memory (LSTM)
classifiers by developing an algorithm for real-time emotion recognition using
virtual markers through an optical flow algorithm that works effectively in
uneven lightning and subject head rotation (up to 25°), different backgrounds,
and various skin tones. Six facial emotions (happiness, sadness, anger, fear,
disgust, and surprise) are collected using ten virtual markers. Fifty-five
undergraduate students (35 male and 25 female) with a mean age of 22.9 years
voluntarily participated in the experiment for facial emotion recognition.
Nineteen undergraduate students volunteered to collect EEG signals. Initially,
Haar-like features are used for facial and eye detection. Later, virtual
markers are placed on defined locations on the subject's face based on a
facial action coding system using the mathematical model approach, and the
markers are tracked using the Lucas-Kande optical flow algorithm. The distance
between the center of the subject's face and each marker position is used as a
feature for facial expression classification. This distance feature is
statistically validated using a one-way analysis of variance with a
significance level of _p_ < 0.01. Additionally, the fourteen signals collected
from the EEG signal reader (EPOC+) channels are used as features for emotional
classification using EEG signals. Finally, the features are cross-validated
using fivefold cross-validation and given to the LSTM and CNN classifiers. We
achieved a maximum recognition rate of 99.81% using CNN for emotion detection
using facial landmarks. However, the maximum recognition rate achieved using
the LSTM classifier is 87.25% for emotion detection using EEG signals.
* Previous article in issue
* Next article in issue
## Keywords
Face emotion recognition
Virtual markers
LSTM
EEG emotion Detection
## 1\. Introduction
One of the important ways humans display emotions is through facial
expressions. Facial expression recognition is one of the most powerful,
natural and immediate means for human beings to communicate their emotions and
intensions. Humans can be in some circumstances restricted from showing their
emotions, such as hospitalized patients, or due to deficiencies; hence, better
recognition of other human emotions will lead to effective communication.
Automatic human emotion recognition has received much attention recently with
the introduction of IOT and smart environments at hospitals, smart homes andsmart cities. Intelligent personal assistants (IPAs), such as Siri, Alexia,
Cortana and others, use natural language processing to communicate with
humans, but when augmented with emotions, it increases the level of effective
communication and human-level intelligence.
Due to the fast advancement of artificial intelligence (AI) and machine
learning, its application is actively being used in many domains including
spam detection, in which a spam classifier is utilized to rearrange email
according to some specific standards and to move unwanted and unsolicited
email to spam folder [1]. As well as significantly being used in data mining
it is used for market analysis to support the large amount of data being
produced every day and to detect fraud probability through the customer's
fraud insurance [2]. For example, enhanced Fraud Miner uses the clustering-
based data mining method ?Lingo? in order to identify frequent patterns [3].
In addition, it is used for machine learning driven advances in medical space,
such as revenue cycle management (i.e. payments) and understanding patient
health through focusing on clinical data-rich environment [4,5].
Moreover, machine learning algorithms have played a significant role in
pattern recognition and pattern classification problems, especially in facial
expression recognition and electroencephalography (EEG), over the past several
decades [[6], [7], [8]]. It has come of age and has revolutionized several
fields in computing and beyond, including human computer interaction (HCI)
[9,10]. Human computer interaction has become a part of our daily life.
Additionally, emotion-related declaration is an important aspect in human
interaction and communication due to its effective cost, reliable recognition
and shorter computational time, among other advantages [10]. In other words,
it may be a potential nonverbal communication media for creating diverse
situations that can illustrate superior interaction with humans by close
collaboration with human-human communication [[10], [11], [12], [13]]. Facial
expression analysis is an interesting and challenging problem and impacts
important applications in many areas, such as human?computer interactions and
medical applications. Several works are based on facial landmarks to extract
some features to help in emotion detection [16]. presents a potential approach
that uses 68 facial landmarks to detect three kinds of emotions in real time;
negative, blank, and positive using one camera. Their proposed system can
detect emotions using both 79 new features and 26 geometrical features (10
eccentricity features, 3 linear features and 13 differential features) from
Ref. [17] with the average accuracy of 70.65%. Palestra et al. [18] computes
32 geometric facial (linear, eccentricity, polygonal and slope) features based
on 20 facial landmarks for automatic facial expression recognition. Although
considerable progress has been made, recognizing facial expressions with high
accuracy has been performed in real-time systems for several applications,
such as behavioral analysis, machine vision and video gaming. Thus, the
expressions of humans can easily be ?understood? by recent HMI systems
[10,[12], [13], [14]]. Among the many methods studied in the literature,
studies that made use of still images and emotion were perceived by measuring
the dimensions of lips and eyes [10,11,15]. Biosensors, such as
electromyograms (EMGs) and electroencephalograms (EEGs), have been used to
perceive facial muscle changes and to conceive brain activities.
There have been efforts to develop multimodal information using facial
expressions [11]. Facial recognition suffers limitations such as light
intensity, face position, and background changes. EEG also suffers limitations
such as noise, artifacts, placement of wired/wireless sensors, speech-noise
and interferences, gesture-light intensity, and background changes. Therefore,
combining both signals adds a new layer of the complexity of the environment,which meets the requirements of both modalities. Indeed, the computational
complexity has been increasing exponentially in the case of multimodal
systems. The multimodal strategy has a better performance in terms of the
emotion recognition rate compared to the single modal strategy [19]. However,
combining facial expressions with other modalities, such as speech signals,
gestures and biosignals, will not efficiently detect emotions in dumb, deaf
and paralyzed patients when developing intelligent human machine interface
(HMI) assistive systems due to the above-mentioned issues. The existence of
such HMI can be convenient for those who are totally disabled physically as
well as patients with special needs, when seeking help. To date, most of the
existing methods work offline, and will not be useful for real-time
applications [10,11]. Hence, the present work is mainly focused on developing
a multimodal intelligent HMI system that works in a real-time environment. It
will recognize emotions using facial expressions and EEG based on machine
learning and deep neural network methods. It aims to be a more reliable real-
time emotion recognition system compared to systems in earlier works. To
identify emotional facial expressions in real-time, the changes in facial
reactions are measured. The facial action coding system (FACS) is a human
observer-based system designed by Ekman and Friesen [15], which is used to
detect subtle changes in facial features and controls facial models by
manipulating single actions, which are called action units (AUs). These AUs
are considered a reference for identifying ten virtual markers for detecting
six basic emotional facial expressions: sadness, anger, happiness, fear,
disgust and surprise. Accordingly, if these facial muscle responses are
studied and used as an instruction to pinpoint the expressions, the emotional
state of humans can be perceived in real time [11]. Hence, the main objective
of this study is to recognize six basic emotional expressions of the subjects
using facial virtual markers and EEG signals. Therefore, the proposed system
has less computational complexity (execution time, memory) and works in real-
time applications. As it is using the markers based approach in which ten
virtual markers are placed on the face rather than using the image pixels
based approach that requires a longer computational time to detect the face
[20], this will simplify the system design as well as reduce the computational
time and system memory. As in real-time systems, face recognition is
implemented on either by using the pixels of the image, or Haar-like features
that efficiently use the AdaBoost cascade classifier, to detect any object in
a given image or a video sequence involving human faces [21,22]. These Haar-
like features based face detection can process 384 × 288 pixels of a face
image in approximately 0.067 s [23]. They have used an AdaBoost cascaded
classifier for a lesser computational time [20]. Most of the works in the
literature focus on developing an offline emotion recognition system. In
addition, this study aims to develop an algorithm for real-time emotion
recognition using virtual markers through optical flow algorithm that works
effectively in uneven lighting and subject head rotation (up to 25°),
different backgrounds, and various skin tones. The distance feature computed
from face virtual markers and the fourteen signals recorded from an EPOC +
device are used to classify the emotional expressions using a convolutional
neural network (CNN) and long short-term memory (LSTM) classifier. This will
lead to achieve the aim of the system to help physically disabled people
(deaf, dumb, and bedridden), in addition to its benefit for Autism children to
recognize the feelings of others. Moreover, it can drive business outcomes and
judge the emotional responses of the audience. Rather than helping to maximize
learning, it has a good benefit in personalized e-learning. Three performance
measures, namely, the mean emotional recognition rate, specificity, andsensitivity, are used to evaluate the performance of the classifiers.
## 2\. Methods and materials
Fig. 1 shows the structure of the proposed system in this study. As
illustrated in Fig. 1, we used two approaches to detect the subject's emotion:
emotion detection using facial landmarks and emotion detection using EEG
signals.
1. Download: Download high-res image (445KB)
2. Download: Download full-size image
Fig. 1. System structure.
### 2.1. Emotion detection using facial landmarks
#### 2.1.1. Facial landmarks database
Regarding emotion detection using facial landmarks, data collection occurred
on the same day after informed consent was obtained from each subject
volunteered in this study. Thus, two facial expression databases were
developed: one with a total of 30 subjects (15 male, 15 female) for automated
marker placement, and another with a total of 55 subjects (25 male, 30 female)
for testing and validating the proposed system. All subjects were chosen from
a mean age domain of 22.9 years. They were mixed males and females and healthy
undergraduate university students with a history free from any cognitive,
muscular, facial or emotional disorders. The subjects were requested to sit in
front of a computer that had a built-in camera, and express six different
emotional expressions (happiness, anger, fear, sadness, surprise, and disgust)
in a video sequence for the purpose of data collection. They were requested to
express particular emotion in a controlled environment (room temperature: 26°,
lighting intensity: 50 lux; the distance between the camera and subject: 0.95
m).
Facial expression for each emotion lasted 60 s and was performed once by each
subject. Therefore, each subject required 6 min to express the emotions, as
illustrated in Fig. 2. However, each subject took approximately 20 min as a
total time, including instructions and baseline state. The total input virtual
distances collected from each subject on all emotions is 2284 on average,
saved in a format of CSV file, and the total virtual distance collected from
all subjects on all emotions is 125,620.
1. Download: Download high-res image (335KB)
2. Download: Download full-size image
Fig. 2. Data acquisition protocol for facial emotion detection.
#### 2.1.2. Face feature extraction
In this study, an HD camera was used to capture the subjects' faces and create
a grayscale image. This simplified the facial image process in facial
expression recognition. Then, by using a grayscale image, the subject's eyes
are detected, and ten virtual markers (action units) are placed on the
subject's face at defined locations using a mathematical model, as shown in
Fig. 3.
1. Download: Download high-res image (757KB)
2. Download: Download full-size image
Fig. 3. Ten virtual locations using a mathematical model.
The Lucas-Kande optical flow algorithm is used to transfer each virtual marker
position to track its position during the subjects? emotional expression. The
ten features are derived as the distance between each marker and the point as
depicted in Fig. 4. In the current study, all the distance data were
calculated using the Pythagorean theorem [24]. Then, they are stored in CSV
format during the data acquisition process for further processing.
1. Download: Download high-res image (397KB)
2. Download: Download full-size imageFig. 4. Ten distances between each marker point and the center point.
In Fig. 5, in the right mouth column, line m1 is the hypotenuse of a right
triangle, wherein the line parallel to the x-axis is dx [the difference
between coordinates of p_m1 (xp_m1) and the point (xc)], and the line parallel
to the axis is dy [the difference between y-coordinates of p_m1 (yp_m1) and
the point (yc)]. Thus, the formula for the computation of distance is given in
Equation (1):(1)Distance(m1)=(Xc?Xp?m1)2+(Yc?Yp?m1)2
1. Download: Download high-res image (211KB)
2. Download: Download full-size image
Fig. 5. The center point (C) and p_m1 coordinates at the distance of m1.
#### 2.1.3. Facial landmarks classification
A convolutional neural network was used in our system to obtain improved
facial emotion detection as it is applied to other computer fields such as
face recognition [25] and object detection [26]. In addition, predictions are
based on information given at a particular time [27].
Fig. 6 shows the network structure that is used for emotion detection using
facial landmarks. This network takes an input image and attempts to predict
the output emotion. It has eight stages, including convolutions, pooling and
fully connected layers with rectified linear unit (ReLU) operations, which
preserve good quality while making convergence much faster [28]. The number of
filters was 32, 64, and 128 with a filter size of 5 × 5 for the convolutional
layers, and the number of output nodes in the fully connected layer was 6 with
the ?Adam? optimizer and a dropout rate of 0.3.
1. Download: Download high-res image (247KB)
2. Download: Download full-size image
Fig. 6. Facial landmarks system structure.
### 2.2. Emotion detection using EEG signals
#### 2.2.1. EEG database
Regarding emotion detection using EEG signals, data collection took place on
the same day after written consent was obtained from each subject volunteered
in this study.
For the purpose of EEG data collection, a video was created from six main
clips to obtain the best reaction of the brain in terms of electrical
activities. In the beginning, video clips were created using emotional images
from the International Affective Picture System [29] and music [30]. Those
clips were supposed to elicit the six emotions to be predicted. After that,
the video clips were tested on a small test sample to confirm the elicited
emotions by prompting the volunteers to vote for the emotion they felt while
watching the clips. Accordingly, other video clips were selected based on
certain criteria, one of which must have a minimum of one million views on
YouTube. Each clip was 2 min long and referred to one emotion. Therefore, the
total length of the video, including the gaps between the clips, was 13 min
and 10-s. After the video clips were determined, all the subjects were asked
to watch the video and classify the emotional categories following their
proto-think and explain the effect that they consciously believe they caused.
In this way, EEG raw data of the 14 channels were collected at a sampling rate
of 128 Hz using the Emotiv EPOC device [31]. Thus, an EEG signal database was
developed with a sum of 19 subjects (7 males and 12 females) for EEG signal
investigation with a total of 1,700,000 records of signal data. All subjects
were chosen from a mean age domain of 22.9 years. Collecting data was
conducted in a laboratory environment and required approximately 1 h to
perform. All subjects were healthy undergraduate university students with a
history free from any cognitive, muscular, facial, or emotional disorders.
EEG recording was performed using the EPOC + interface. It is connectedwirelessly to the Emotiv EPOC device and records EEG signals that are coming
from the brain while the subject is watching the video. As each emotion's
video clip lasted 2 min with a 10 s gap between the videos and was performed
once by each subject, each subject required 13 min and 10 s to record the EEG
signals, as illustrated in Fig. 7. However, each subject took approximately 1
h as a total time, including instructions and a baseline state. Then, the same
interface was used to export the recorded signals into CSV files and feed them
to a Python system to filter using an infinite impulse response (IIR) filter
before classification and training [32].
1. Download: Download high-res image (267KB)
2. Download: Download full-size image
Fig. 7. Data acquisition protocol for EEG emotion detection.
#### 2.2.2. EEG signal preprocessing
Due to the artifacts that come from different sources during EEG signal
recording, such as eye blinks, eye movements, muscle movements, respiration,
and sweat, a process is needed to remove this unwanted information from the
EEG signals that can cause significant distortion. First, to reduce the
artifact effects, the EEG signal voltage was taken in an amplitude range of
±85 ?V. Second, a sixth-order Butterworth infinite impulse response (IIR)
filter utilized in Ref. [32] with a cut-off frequency of [1?49] Hz was used to
remove noise from the EEG signals.
#### 2.2.3. EEG signal classification
A long short-term memory (LSTM) network model was used for training the
affective model and obtaining improved EEG signal emotion detection. LSTM is a
special type of artificial recurrent neural network (RNN) architecture. It is
used in the field of deep learning and well-suited to time series data to
classify, process, and make predictions. It can process entire sequences as
analog data in this study.
Nineteen participants were asked to watch the same video that contained six
different parts, to recognize the emotion elicited from these videos. Then,
the collected data (EEG raw data) were fed to the proposed model, as shown in
Fig. 8. The proposed model achieved the highest accuracy among three models
(conventional, fully connected layers, grid search). Each part of the video
was segmented into 20 segments with a 6-s window. Thus, each participant had
an array of EEG data records with their labels. The array of labels represents
the six emotions ?happy, fear, anger, sad, surprise and disgust? of each video
performed by each participant. Each participant produced approximately 118,000
records of EEG raw data from 14 EEG channels for the six videos.
1. Download: Download high-res image (224KB)
2. Download: Download full-size image
Fig. 8. NN model to handle and train EEG signals.
Fig. 8 shows the proposed deep learning neural network model that achieves the
highest accuracy. It consists of a fully connected embedding layer and three
LSTM layers with 128, 64, and 32 neurons, two dropout layers, and a dense
layer. The LSTM and dropout layers are used to learn features from raw EEG
signals. However, the dropout layer is used to reduce the overfitting by
preventing too many units from ?coadapting?. Finally, the dense layer that
uses the Softmax activation function is used for classification.
The model is trained on 70% of the EEG records using 3-fold cross-validation
and tested on 30% of them. One hundred epochs are used for each cross-
validation iteration. The Adam optimizer is used in the training process with
a 0.001 learning rate.
Additionally, the grid search method and fivefold cross-validation strategies
are used to tune the dimension of features, the threshold value, and thelearning rate.
## 3\. Data validation
System validation aims to evaluate the accuracy after development. It was
tested by collecting data especially for the testing.
### 3.1. Facial landmarks database validation
For emotion detection using facial landmarks, two facial expression databases
were developed, one with a total of 30 subjects (15 male, 15 female) for
automated marker placement, and another one with a total of 55 subjects (25
male, 30 female) for testing and validating of the proposed system. They were
requested to sit in front of a computer that has a built in Camera and express
six different emotional expressions (happiness, anger, fear, sadness,
surprise, and disgust) in a video sequence for the purpose of data collection.
They were requested to express particular emotion in a controlled environment
(room temperature: 26°, lighting intensity: 50 lux; the distance between the
camera and subject: 0.95 m). Thus, the proposed method achieves 99.81% as a
highest accuracy.
### 3.2. EEG signals database validation
The facial landmarks data for the 19 subjects are given into the three-fold
cross-validation method to split the set of features into training and testing
sets. Thus, the proposed method achieves the higher emotional detection rate
of 87.25%.
## 4\. Experimental results and discussion
For emotion detection using facial landmarks, data were collected from 55
subjects for testing; their ages were between 20 and 25. They are
undergraduate students (25 males, 30 females). Then, the accuracy was found at
100 epochs for the collected data in both cases of normalized and not
normalized data, as illustrated in Table (1).
Table 1. Facial landmark accuracies.
Data Sets| Avg. Accuracy
---|---
Training Data from 30 Subjects (Not Normalized)| 89.04%
Training Data from 30 Subjects (Normalized)| 96.93%
Testing Data from 55 Subjects (Normalized)| 93.02%
The collected facial landmark data were normalized using Equation (2) to make
all data values between [0, 1].(2)Zi=(Xi?Min(X))/(Max(X)?Min(x))where Zi is
the ith normalized data.
Both models were run till 300 epochs to check the shape of the curve and
obtain the highest accuracy for emotion detection. As shown in Fig. 9, emotion
detection using facial landmarks has a direct relationship till 300 epochs.
Therefore, the system can recognize emotions in 99.81% of facial landmarks
using the proposed model at 300 Epochs. Whereas, the proposed model of EEG
signals has a direct relationship between accuracy and number of epochs till
100, and the curve stops increasing after 100, as shown in Fig. 10. As a
result, the highest emotion recognition accuracy of EEG signals is 87.25% at
100 Epochs.
1. Download: Download high-res image (116KB)
2. Download: Download full-size image
Fig. 9. Accuracy vs. #Epochs (Facial Landmarks).
1. Download: Download high-res image (156KB)
2. Download: Download full-size image
Fig. 10. Accuracy vs. #Epochs (EEG).
Additionally, the sensitivity, specificity, F-score, and accuracy are
calculated for each class by using the following calculated confusion matrix
(Fig. 11) for the six classes for emotion detection using facial landmarks.Each class is used against all classes in order to find those performance
factors related to it.
1. Download: Download high-res image (157KB)
2. Download: Download full-size image
Fig. 11. Confusion matrix.
The results in Table (2) are calculated using the following
equations:precision = TP / (TP + FP)sensitivity = TP / (TP + FN)specificity =
TN / (FP + TN)F-score = 2*TP /(2*TP + FP + FN)Where; TP: True Positive, FP:
False Positive, TN: True Negative, FN: False Negative.
Table (2) shows the performance factors for each emotion; 0:Anger, 1:Disgust,
2:Fear, 3:Sad, 4:Smile and 5:Surprise. It shows the perfect precision and the
proportion of actual positives and negatives of emotions that are accurately
identified. Thus, the collected data can be used as a benchmark for other
researchers. Our system is based on just 10 virtual distances for facial
landmarks and 14 EEG raw data for the EEG signals. In other words, it
increased the performance of the system as the number of training facial
landmark features is fewer than in previous work, as shown in Table (2). Table
(3) shows that [[33], [34], [35], [36], [37]] authors use more than 10 virtual
markers and those markers are manually placed on the face to detect emotions.
They used the CK + database [38], and the maximum accuracy they achieved was
93.2 [37]. Whereas, our system places exactly 10 virtual markers automatically
to detect emotions. It is trained using our own created database, and the
maximum accuracy was 99.81%. This means that the system detects emotions using
facial landmarks with high accuracy and without too many features.
Table 2. Facial landmarks performance factors.
Empty Cell| 0| 1| 2| 3| 4| 5
---|---|---|---|---|---|---
Precision (%)| 98.9| 100.0| 99.9| 99.9| 99.1| 99.8
Sensitivity (%)| 100| 99.8| 99.9| 99.9| 98.8| 99.0
Specificity (%)| 99.8| 100| 99.9| 99.9| 99.8| 99.9
F-score (%)| 99.5| 99.9| 99.9| 99.9| 98.9| 99.4
Acc (%)| 99.8| 100| 99.9| 99.9| 99.1| 99.8
Table 3. Accuracy of earlier and present work on emotion detection using
facial landmarks.
\Authors| [33]| [34]| [35]| [36]| [37]|
---|---|---|---|---|---|---
| Related Work| Our Work
#Virtual Markers| 22| 77| 68| 22| 68| 10
Marker Placement| Manual| Auto
Database| CK+[38]| 6 different ready DBs| Own DB| CK+[38]| Own Database
Classifier| SVM| DNN| Neural Network| FURIA| CNN
Acc. (%)| 86.0| 85.8| 93.2| 84.27| 83.2| 99.8
Some of the major aspects that can affect the results found by the system are
the user's accuracy in his/her feelings during the watched video, especially
in the detection of EEG signals. However, in the detection using facial
landmarks, it was more accurate since it depends on external physical
features. Additionally, the external environment had an effect on the
accuracy.
Many exhaustive tests were conducted to minimize the errors in obtaining the
desired accuracy. Tests were performed to obtain a reliable classifier,
normalization method, and an optimal number of markers with the best-proposed
features to reduce the errors. The highest recognition rate obtained in this
research was 99.81% for emotion detection using facial landmarks with the 10
virtual markers. However, the highest accuracy for emotion detection using EEGsignals was 87.25%, which can be improved by collecting more data from more
subjects and finding techniques to extract more features from the EEG signals.
## 5\. Conclusion and future work
An algorithm for real-time emotion recognition using virtual markers through
an optical flow algorithm has been developed to create a real-time emotion
recognition system with less computational complexity (execution time, memory)
using facial expressions and EEG signals. This algorithm works effectively in
uneven lightning and subject head rotation (up to 25°), different backgrounds,
and various skin tones. The system aims to help physically disabled people
(deaf, dumb, and bedridden), in addition to its benefit for Autism children to
recognize the feelings of others. Moreover, it can drive business outcomes and
judge the emotional responses of the audience. Rather than helping to maximize
learning, it has a good benefit in personalized e-learning. The system can
recognize six emotions in real time for facial landmarks and in offline
settings for EEG raw data. Users of the system need to wear an EPOC+ headset
and are faced in front of a camera to record the EEG raw data wirelessly and
collect the ten virtual landmarks placed on the subject's face.
The results show that the system can recognize emotion in 99.81% of facial
landmarks and 87.25% of EEG signals. The cases that were used to collect the
data were performed at Kuwait University. It was difficult to collect data
from many subjects due to the student's schedules and timings. Thus, only a
few subjects were available for collecting the data.
For future work, the system's precision and accuracy can be improved by
collecting more data from more subjects. Additionally, techniques can be used
to extract more features from EEG signals [14]. In addition to improving the
system techniques, putting the subjects in real situations to express the
exact feelings can help to improve the system's accuracy for the EEG.
## Ethical statement
Each named author assures that there are no financial or any personal
relationships (potential competing interests, consultancies, stock ownership,
honoraria, paid expert testimony, patent applications/registrations, and
grants or other funding) with other people or organizations that could
inappropriately influence their work.
## Declaration of competing interest
The authors declare that they have no known competing financial interests or
personal relationships that could have appeared to influence the work reported
in this paper.
## Acknowledgment
The first two authors would like to thank the continuous support of the
graduate school of Kuwait University.
## Appendix A. Supplementary data
The following are the supplementary data to this article:Download: Download
XML file (253B)
Multimedia component 1.
Recommended articles
## Research data for this article
Data not available / Data will be made available on request
Further information on research data"
69,71,Dexpression: Deep convolutional neural network for expression recognition,"['P Burkert', 'F Trier', 'MZ Afzal', 'A Dengel']",2015,192,"Extended Cohn-Kanade, MMI Facial Expression","CNN, classification, deep learning, facial expression recognition, neural network",of different image classification approaches submitted by  evaluated on the Extended  Cohn-Kanade Dataset (Section 4.2 [12] have created the Extended CohnKanade dataset. This,No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1509.05371,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
70,72,Dfew: A large-scale database for recognizing dynamic facial expressions in the wild,"['X Jiang', 'Y Zong', 'W Zheng', 'C Tang', 'W Xia']",2020,126,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Recently, facial expression recognition (FER) in the wild has gained a lot of researchers’   a dynamic facial expression in the wild database, ie, acted facial expressions in the wild (AFEW)",No DOI,Proceedings of the 28th …,https://arxiv.org/abs/2008.05924,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
71,73,Diagnostic features of emotional expressions are processed preferentially,"['E Scheller', 'C Büchel', 'M Gamer']",2012,135,Karolinska Directed Emotional Faces,classifier,control group in an emotion classification task revealed a  faces that were shown during  the experiment were selected from several picture sets (The Karolinska directed emotional faces,No DOI,PloS one,https://pubmed.ncbi.nlm.nih.gov/22848607/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
72,74,Discovering hidden factors of variation in deep networks,"['B Cheung', 'JA Livezey', 'AK Bansal']",2014,225,Toronto Face Database,"classification, deep learning, machine learning, neural network","Deep learning has enjoyed a great deal of success because of its ability to  handwritten  digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
73,75,Disentangling factors of variation for facial expression recognition,"['S Rifai', 'Y Bengio', 'A Courville', 'P Vincent']",2012,264,Toronto Face Database,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network","This system beats the state-of-the-art on a recently proposed dataset for facial expression  recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%",No DOI,Computer Vision–ECCV …,https://link.springer.com/chapter/10.1007/978-3-642-33783-3_58,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
74,76,Disgust-specific impairment of facial expression recognition in Parkinson's disease,"['A Suzuki', 'T Hoshino', 'K Shigemasu', 'M Kawamura']",2006,259,Japanese Female Facial Expression,facial expression recognition,impair the recognition of facial expressions of disgust; this provides concrete evidence for  emotion- (eight photographs for each emotion; Japanese and Caucasian facial expressions of,No DOI,Brain,https://pubmed.ncbi.nlm.nih.gov/16415306/,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
75,77,Do deep neural networks learn facial action units when doing expression recognition?,"['P Khorrami', 'T Paine', 'T Huang']",2015,360,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Toronto Face Database","CNN, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Cohn-Kanade (CK+) dataset [18], and the Multi-PIE dataset [10]. However, the recent success  of deep neural networks  in our experiments: the extended Cohn-Kanade database (CK+) [",No DOI,Proceedings of the IEEE …,http://arxiv.org/abs/1510.02969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
76,78,Driver's facial expression recognition in real-time for safe driving,"['M Jeong', 'BC Ko']",2018,158,MMI Facial Expression,"FER, classification, classifier, deep learning, facial expression recognition, machine learning",": An addition to the mmi facial expression database. In Proceedings of the 3rd International  Conference on Language Resources and Evaluation Workshop on EMOTION, Valletta, Malta",No DOI,Sensors,https://www.mdpi.com/1424-8220/18/12/4270,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
77,79,Dynamic facial expression recognition using longitudinal facial expression atlases,"['Y Guo', 'G Zhao', 'M Pietikäinen']",2012,116,MMI Facial Expression,facial expression recognition,"facial expression atlases of each expression, we are able to recognize expression of a new  facial expression  In this section, we evaluate the proposed method on the MMI database [24]",No DOI,"… Computer Vision, Florence, Italy, October 7 …",https://link.springer.com/chapter/10.1007/978-3-642-33709-3_45,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
78,80,EMPATH: A neural network that categorizes facial expressions,"['MN Dailey', 'GW Cottrell', 'C Padgett']",2002,349,Affective Faces Database,neural network,to which each face portrayed each basic emotion on a 1–5  rating’’ vectors for each face pair  as a measure of dissimilarity.  networks respond with emotion i when the intended emotion,No DOI,Journal of cognitive …,https://pubmed.ncbi.nlm.nih.gov/12495523/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
79,81,Early cortical processing of natural and artificial emotional faces differs between lower and higher socially anxious persons,"['A Mühlberger', 'MJ Wieser', 'MJ Herrmann']",2009,276,Karolinska Directed Emotional Faces,classification,", emotional faces should  emotional faces regarding their processing as reflected by ERP  components. On the one hand, natural faces contain a lot more information than the emotion of",No DOI,Journal of neural …,https://pubmed.ncbi.nlm.nih.gov/18784899/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
80,82,Emonets: Multimodal deep learning approaches for emotion recognition in video,"['SE Kahou', 'X Bouthillier', 'P Lamblin', 'C Gulcehre']",2016,510,Toronto Face Database,deep learning,"learning several specialist models using deep learning techniques, each focusing on one  modality. Among these are a convolutional neural network,  is the Toronto Face Dataset (TFD) [",No DOI,Journal on Multimodal …,https://arxiv.org/abs/1503.01800,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
81,83,Emotion distribution recognition from facial expressions,"['Y Zhou', 'H Xue', 'X Geng']",2015,161,Toronto Face Database,"FER, facial expression recognition",facial expression recognition methods assume the availability of a single emotion for each  expression in  The s-JAFFE database contains 213 facial expression images. Each image was,No DOI,Proceedings of the 23rd ACM international …,https://dl.acm.org/doi/10.1145/2733373.2806328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
82,84,Emotion recognition from facial expression using deep convolutional neural network,['DY Liliana'],2019,116,Extended Cohn-Kanade,"CNN, classification, deep learning, neural network",This paper extends the deep Convolutional Neural Network ( This research uses the  extended Cohn Kanade (CK+) dataset  In this paper we contribute a Deep Learning approaches,No DOI,Journal of physics: conference series,https://iopscience.iop.org/article/10.1088/1742-6596/1193/1/012004,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
83,85,Emotion recognition in the wild challenge 2013,"['A Dhall', 'R Goecke', 'J Joshi', 'M Wagner']",2013,241,Acted Facial Expressions In The Wild,"classification, classifier, facial expression recognition",emotion recognition methods in real-world conditions. The database in the 2013 challenge is  the Acted Facial Expression in the Wild  ] and Static Facial Expressions In The Wild (SFEW) [,No DOI,Proceedings of the 15th …,https://dl.acm.org/doi/10.1145/2522848.2531739,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
84,86,"Emotion recognition in the wild challenge 2014: Baseline, data and protocol","['A Dhall', 'R Goecke', 'J Joshi', 'K Sikka']",2014,269,"Acted Facial Expressions In The Wild, Expression in-the-Wild","classification, classifier, facial expression recognition, machine learning",The Second Emotion Recognition In The Wild Challenge 2014 provides a platform for   with their emotion recognition method on the Acted Facial Expressions In The Wild database.,No DOI,Proceedings of the 16th …,https://dl.acm.org/doi/10.1145/2663204.2666275,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
85,87,Emotion recognition in the wild via convolutional neural networks and mapped binary patterns,"['G Levi', 'T Hassner']",2015,413,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild",neural network,"version 2.0 of the Static Facial Expression in the Wild benchmark [9]. It was assembled by  selecting frames from different videos of the Acted Facial Expressions in the Wild (AFEW), and",No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830587,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
86,88,Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels,"['CH Wu', 'WB Liang']",2010,351,Affective Faces Database,classifier,to emotion recognition of affective speech using multiple classifiers with AP and SLs. The   using multiple classifiers and the MDT was used to select an appropriate classifier to output the,No DOI,IEEE Transactions on Affective Computing,https://ieeexplore.ieee.org/document/5674019,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
87,89,Emotion recognition using deep learning approach from audio–visual emotional big data,"['MS Hossain', 'G Muhammad']",2019,442,Affective Faces Database,deep learning,"The proposed system is evaluated using two audio–visual emotional databases, one of   The histograms are obtained from the cropped face images. If there was no face detected in a",No DOI,Information Fusion,https://www.sciencedirect.com/science/article/pii/S1566253517307066,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
88,90,"Emotional facial expressions evoke faster orienting responses, but weaker emotional responses at neural and behavioural levels compared to scenes: A …","['A Mavratzakis', 'C Herbert', 'P Walla']",2016,118,Karolinska Directed Emotional Faces,neural network,"how affective neural activity during emotional face and scene perceptions translates into   of the analysis was to investigate differences in emotional face and scene processing, only the",No DOI,Neuroimage,https://www.sciencedirect.com/science/article/pii/S1053811915008873,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Emotional facial expressions evoke faster orienting
responses, but weaker emotional responses at neural and
behavioural levels compared to scenes: A simultaneous EEG and
facial EMG study
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. Introduction
6. Methods
7. Results
8. Discussion
9. Acknowledgments
10. References
Show full outline
## Cited by (67)
## Figures (7)
1. 2. 3. 4. 5. 6.
Show 1 more figure
## Tables (4)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
## NeuroImage
Volume 124, Part A, 1 January 2016, Pages 931-946
# Emotional facial expressions evoke faster orienting responses, but weaker
emotional responses at neural and behavioural levels compared to scenes: A
simultaneous EEG and facial EMG study
Author links open overlay panelAimee Mavratzakis a b, Cornelia Herbert c d,
Peter Walla a b e f
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neuroimage.2015.09.065Get rights and content
Under a Creative Commons licenseopen access
## Highlights
* ?
Faces and scenes elicit different emotion-related brain activities.
* ?
Faces and scenes elicit different facial expression-related muscle activities.
* ?
Faces and scenes are shown to be different emotion elicitors.
## Abstract
In the current study, electroencephalography (EEG) was recorded simultaneously
with facial electromyography (fEMG) to determine whether emotional faces and
emotional scenes are processed differently at the neural level. In addition,
it was investigated whether these differences can be observed at the
behavioural level via spontaneous facial muscle activity. Emotional content of
the stimuli did not affect early P1 activity. Emotional faces elicited
enhanced amplitudes of the face-sensitive N170 component, while its
counterpart, the scene-related N100, was not sensitive to emotional content of
scenes. At 220?280 ms, the early posterior negativity (EPN) was enhanced only
slightly for fearful as compared to neutral or happy faces. However, its
amplitudes were significantly enhanced during processing of scenes with
positive content, particularly over the right hemisphere. Scenes of positive
content also elicited enhanced spontaneous zygomatic activity from 500?750 ms
onwards, while happy faces elicited no such changes. Contrastingly, both
fearful faces and negative scenes elicited enhanced spontaneous corrugator
activity at 500?750 ms after stimulus onset. However, relative to baseline EMG
changes occurred earlier for faces (250 ms) than for scenes (500 ms) whereas
for scenes activity changes were more pronounced over the whole viewing
period. Taking into account all effects, the data suggests that emotional
facial expressions evoke faster attentional orienting, but weaker affective
neural activity and emotional behavioural responses compared to emotional
scenes.
* Previous article in issue
* Next article in issue
## Keywords
Emotion
Affective processing
Faces and scenes
Electroencephalography
Spontaneous facial EMG
N170
N100
Early posterior negativity
## Introduction
In emotion research two kinds of stimuli are frequently used: facial
expressions (e.g. a smiling or sad face) and emotionally evocative scenes
(e.g. snakes, erotic pictures). But, do these forms of emotional stimuli
undergo the same neural processing? Despite each being intrinsically
emotionally evocative, only a few studies exist that have compared affective
processing of faces and scenes in the same experiment and context.
A recent meta-analysis comparing 157 functional Magnetic Resonance Imaging
(fMRI) studies that used either emotional faces or emotional scenes
(Sabatinelli et al., 2011) revealed multiple clusters of brain activations
unique to these different forms of stimuli even after the subtraction of
neural activity related to basic visual processing. Although this suggeststhat both types of stimuli might be processed differently in the brain, direct
comparisons of the time course of affective processing for faces and scenes
are lacking and little is known about whether both stimulus classes elicit
similar expressive behavioural reactions.
In other contexts it is obvious that faces and scenes are indeed quite
different. For example, facial expressions elicit mimicry and facial feedback
mechanisms might modify emotion-related processing (see Niedenthal et al.,
2001). Facial expressions can be understood as interpersonal, facilitating
social transactions, and require complex neural processing to translate these
emotional cues into social meaning. Emotional scenes on the other hand are
more intrapersonal and directly elicit motivational behaviours without needing
to translate their meaning beyond knowing whether to approach or avoid.
Different facial expressions are more similar to each other than different
scene pictures are. Various processing differences between face and scene
stimuli have been described. Hariri et al. (2002) found varying amygdala
activity depending on whether a fearful face or a threatening scene was
presented to their participants. In their fMRI study, Keightley et al. (2011)
reported about their conclusion that the contextual information in emotional
scenes may facilitate memory via additional visual processing, whereas memory
for emotional faces may rely more on cognitive control mediated by
rostrolateral prefrontal regions. Epstein et al. (2006) investigated
differences between face and scene inversion. Their results demonstrate that
both face and scene inversion cause a shift from specialised processing
streams towards generic object-processing mechanisms, but this shift only
leads to a reliable behavioural deficit in the case of face inversion.
The temporal characteristics of neural affective processing have been
relatively well documented for emotional faces (Vuilleumier and Pourtois,
2007, Wieser and Brosch, 2012) and emotional scenes (Olofsson et al., 2008,
Schupp et al., 2006a). Anatomically, visual information passes through the
extrastriate visual cortex where low-lying physical stimulus properties such
as luminance and spatial complexity determine which aspects of visual
information receive rapid attentional capture and further processing (Clark
and Hillyard, 1996, Givre et al., 1994, Hillyard and Anllo-Vento, 1998, Mangun
et al., 1993, Rugg et al., 1987). This rapid-attentional capture is seen in
scalp-recorded potentials as a prominent positively charged deflection in
amplitude over lateral posterior occipital sites at approximately 100 ms post-
stimulus (termed the P100 component, or P1 to represent the first positive
peak in neural activity), where the size of the amplitude deflection indexes
the degree of attentional capture of the related stimulus. Attended-to
information then undergoes object recognition processing in neural circuits
proceeding through bilateral ventral?lateral streams from the visual cortex
into the temporal cortices (Allison et al., 1999). Here, the fusiform gyrus, a
well-studied structure located in the inferior temporal lobes, facilitates
face recognition via a highly specialised process of collating local facial
features into a holistic global face representation (Rossion et al., 2003).
This activity is observed in scalp-recorded potentials as a strong negatively
charged deflection in amplitude over lateral temporal?occipital areas
approximately 170 ms post-stimulus onset (Bentin et al., 1996, Deffke et al.,
2007), hence the name N170. Other stimuli such as complex scenes also undergo
category-specific processing across more widely distributed hierarchically
organised circuits in the ventral?lateral streams, with this activity being
observed as a more modest negative deflection in amplitude at around 150?200
ms after stimulus onset over lateral temporal-occipital scalp locations
(termed the N100). From here, it has been posited that affective informationof faces and scenes begins to influence neural activity, seen at
lateral?occipital scalp recordings as a more stable negative shift in polarity
when viewing emotionally-evocative relative to neutral stimuli. This posterior
negativity (i.e. the early posterior negativity or EPN) typically emerges at
the offset of the N100/N170, around 200 to 250 ms post-stimulus and has been
found to be modulated as a function of increased attentional allocation and
greater motivational relevance of the emotionally evocative stimuli (Bublatzky
and Schupp, 2011, Foti et al., 2009, Schupp et al., 2006a, Weinberg and
Hajcak, 2010).
There is however evidence that affective information can influence activity at
earlier stages of processing relative to the EPN. Several studies have
reported larger N170 amplitudes when viewing negatively-valanced facial
expressions such as fear and anger (e.g. Batty and Taylor, 2003, Leppänen et
al., 2008, Pourtois et al., 2005, Stekelenburg and de Gelder, 2004), which has
been interpreted as an innate attentional ?negativity bias? (Carretie et al.,
2009, Holmes et al., 2005). The same controversy exists for emotional scenes,
with some studies reporting a negativity bias for highly unpleasant
threatening or fearful scenes in the time window of the N100. Affective
modulation has even been reported as early as 100 ms post-stimulus (Batty and
Taylor, 2003, Eger et al., 2003, Eimer and Holmes, 2002, Holmes et al., 2003,
Pizzagalli et al., 1999, Pourtois et al., 2005, Recio et al., 2014, Smith et
al., 2013, Streit et al., 2003). Differences as well as similarities in
affective stimulus processing may be better understood by directly comparing
when these processes occur for emotional faces and scenes in a single
experimental framework. This would also allow a direct comparison of
behavioural reactions elicited by faces and scenes.
In the current study, we were interested in investigating how affective neural
activity during emotional face and scene perceptions translates into emotional
behaviour, building on the idea that emotional behaviour should be understood
as a consequence of subcortical affective neural activity (Walla and Panksepp,
2013). Spontaneous facial muscle activity is an emotion-related behavioural
phenomenon that is thought to play a crucial role in social emotion
recognition, whereby perceiving an emotional facial expression elicits a rapid
or spontaneous micro-simulation of the perceived facial expression by the
perceiver less than 1000 ms post onset (Achaibou et al., 2008, Dimberg et al.,
2000, Grèzes et al., 2013, Korb et al., 2010, Moody et al., 2007). By
utilising the excellent temporal resolution offered by electromyography to
measure facial muscle activity (fEMG), these studies have shown that
zygomaticus major ?cheek? muscles rapidly and spontaneously contract in
response to smiling faces while corrugator supercilii ?eyebrow? muscles
rapidly and spontaneously contract in response to angry or fearful faces. The
phenomenon is thought to facilitate emotion recognition by triggering the
reactivation of specific neural regions that are involved in producing that
same emotion in the perceiver, leading to a realisation of the other person's
emotional state (e.g. Barsalou, 2003, Barsalou et al., 2003, Clark et al.,
2008, Niedenthal, 2007). Moreover, empirical evidence suggests that
spontaneous facial reactions play a causal role in emotion recognition whereby
selectively preventing movement in facial muscle/s required to simulate an
expression leads to poor recognition ability for that facial expression in
another person (Foroni and Semin, 2011, Niedenthal et al., 2001, Oberman et
al., 2007, Ponari et al., 2012).
However, emotional scenes have also been shown to evoke spontaneous facial
reactions (Dimberg et al., 1998). In contrast to faces emotional scenes often
do not contain any third-party emotion to recognise. This raises the questionof whether and in what ways spontaneous facial muscle activity may differ when
elicited by emotional faces compared to scenes, such as in latency or strength
of the response. To this extent, the objective of the current study was to
investigate differences in emotional responses evoked by happy, fearful and
neutral faces versus positive, neutral and negative scenes: (1) during early
visually-evoked stages of neural activity including the P1, N100/N170, and
EPN; and (2) in spontaneous zygomatic and corrugator facial reactions; and (3)
to examine correlations between affective neural activity and emotional
behaviour.
When considering motivationally-relevant emotion processing, arousal must be
taken into consideration, because stimuli that evoke heightened arousal have
been shown to modulate both neural and facial muscle activity independent of
emotional valence or stimulus type (Cacioppo et al., 1986, Cuthbert et al.,
2000, Feng et al., 2014, Lang et al., 1993). For this reason the face and
scene stimuli chosen for the current study were relatively low-arousing (see
Fig. 1 bottom right graph). However, it was still possible that face and scene
stimuli could evoke different degrees of arousal. Therefore, arousal responses
to pictures were also recorded via the skin conductance response (SCR), a
neurophysiological measure of sweat gland activity which is controlled by the
sympathetic part of the autonomic nervous system. SCRs could therefore be used
to differentiate neural and behavioural effects associated with enhanced
levels of arousal from those associated with emotional valence or stimulus
type.
1. Download: Download high-res image (193KB)
2. Download: Download full-size image
Fig. 1. Mean luminance and spatial frequency values (top) and pre-evaluated
pleasantness (bottom left) and arousal (bottom right) ratings for the final
collection of stimuli. Pleasantness was rated on a scale of 1 (very
unpleasant) to 9 (very pleasant). Arousal was rated on a scale of 1 (very
calm) to 9 (very arousing). Error bars represent one standard error of the
mean.* = The differences are significant at .05 alpha level.
A secondary aim of this study was to examine whether or not early emotion
processing is influenced by the depth of conceptual emotion processing, and,
if so, whether such effects might suppress or enhance spontaneous facial
reactions. Traditionally, the delayed match-to-sample task involves the
?passive? presentation of a first stimulus (e.g. an emotional facial
expression) followed by an ?active? presentation of a second stimulus, at
which point some judgement must be made regarding the second stimulus as a
function of the first, usually whether or not they express the same type of
emotion. In the current study, we varied the semantic format of emotion
recognition between three ?delayed match-to-sample? emotion-matching tasks. A
consistent presentation format was always used for the first emotional
stimulus (i.e. always an emotional picture) while for the second emotional
stimulus, the presentation format was varied across tasks to be either another
emotional picture, an emotional word or to freely label the depicted emotion
(see Fig. 2 for examples of each task). Hence, one version of the task was to
compare an emotional picture with another emotional picture; the second
version was to compare an emotional picture with an emotional word; and the
third task had no second emotional stimulus to compare the first emotional
picture with, instead participants had to freely label the depicted emotion.
1. Download: Download high-res image (389KB)
2. Download: Download full-size image
Fig. 2. An example of the trial structure for each emotion recognition task.
The top figure illustrates examples for the versions using emotional faces,and the bottom figure illustrates the same trial examples, but using emotional
scenes. From the left to right of each figure is an example of the
Picture?Picture matching task; the Picture?Word matching; and the Picture-
Labelling task. Notice that each task begins exactly the same, with a fixation
cross followed by a passively viewed picture (Stimulus 1; a happy, neutral or
fearful picture (face or scene, depending on the task version)). After this,
the trial structure changed according to type of task being completed. Neural
and facial muscle activity during the 3000 ms time window corresponding to
Stimulus 1 was analysed in this study to determine whether processing of
emotional information is differently influenced by the way that information
needs to be used.
Most research using the match-to-sample paradigm focuses on neural or
behavioural activity associated with the second ?active? stimulus (e.g. Hirai
et al., 2008, Narumoto et al., 2001). However the focus of the current study
was neural and behavioural activity associated with the first passively viewed
stimulus. This design specifically allowed us to examine whether, when
emotional pictures are viewed under exactly the same presentation conditions,
does passive emotion processing and responding vary as a function of the
semantic level of emotion recognition? Due to unresolved muscle-related
artefact issues in the picture-labelling task, we here focus on EEG and fEMG
effects associated with the ?picture?picture matching? and ?picture?word
matching? tasks.
## Methods
### Participants
Participants were 27 undergraduate students enrolled at the University of
Newcastle. Data of four participants were excluded from the analysis due to
technical issues with the EMG and skin conductance recording equipment (two
females and one male) and too few remaining EEG trials after artefact removal
(one female). The mean age of the remaining 23 participants is 21 years (SD =
1.72) (17 females). Participants were native speakers of English, right-
handed, non-smokers, had no known history of neuropathology and were not
taking central nervous system targeted medication such as antidepressants or
stimulants at the time of testing. Participants provided written informed
consent and the project was approved by the University of Newcastle Human
Research Ethics Committee [H-2012-0229].
### Stimuli
The 270 happy, fearful and neutral face stimuli were taken from the Radboud
Faces Database (RAFD; Langner et al., 2010) and Set A of the Karolinska
Directed Emotional Faces Database (KDEF; Lundqvist et al., 1998). For face
stimuli, each face was cropped to remove hair, ears, clothing etc. from the
image, leaving only the necessary elements of the face for distinguishing an
emotional expression. The 270 positive, negative and neutral scene stimuli
were taken from the Geneva Affective Picture Database (GAPED; Dan-Glauser and
Scherer, 2011) and the International Affective Picture System (IAPS; Lang et
al., 2005). Positive scenes included nature scenes, baby animals, appetising
food and erotic scenes depicting a male and female embrace. Negative scenes
were specifically selected based on a study by Mikels et al. (2005), which
categorised IAPS stimuli into discrete emotion categories including fear.
Obvious thematic characteristics of the discrete IAPS fear collection, such as
spiders and snakes, were then used as a basis for selecting negative (mainly
fearful) scenes from the GAPED database, for which no discrete emotional
categorisation exists. For neutral scenes, we specifically chose stimuli that
visually represented neutrality (e.g. a stair case, computer, light bulb),
because valence ratings are not accurate predictors of emotionalcategorisation (Blairy et al., 1999). Scene stimuli featuring a forward-facing
face were excluded from the scenes collection.
The stimuli collections were rated on levels of valence and arousal in a pilot
study of a larger pool of images using an independent group of 42 participants
(23 females) with a mean age of 25 years (_SD_ = 4.61). Participants rated
equal samples of face and scene stimuli using the Self-assessment manikin
(SAM; Bradley and Lang, 1994). Pictures with ratings that best balanced
valence and arousal levels across stimulus categories were then chosen to be
included in the study. Mean valence and arousal ratings in addition to
luminance and spatial frequency values for the final collection are displayed
in Fig. 1.
There is existing empirical evidence showing that low level physical features
of visual stimuli such as luminance and spatial frequency modify early brain
activities (e.g. Alorda et al., 2007, De Cesarei and Codispoti, 2012,
Delplanque et al., 2007), a phenomenon also known as exogenous brain activity
effects (Donchin, 1978). Since our motivation was focused on early brain
activity effects related to emotion-specific content of faces versus scenes it
is important to look at physical features of all visual stimuli, in particular
luminance and spatial frequency and to test whether or not potential
differences in luminance and spatial frequency across stimuli could
theoretically explain any early brain activity differences that are described
in the frame of this paper.
For this purpose we ran a spatial frequency analysis of all our images and
calculated analytic statistics to test whether or not spatial frequencies
differed between stimulus categories (faces and scenes) and also between
emotion categories. An ANOVA including all spatial frequency values was run
and revealed a highly significant main stimulus category effect (p < .001) and
a significant main emotion category effect (p = .009). However, the
interaction of both factors was not significant (p = .097) (all
Greenhouse?Geisser corrected). The pattern of these results is understood as
demonstrating that mean spatial frequencies of our images differ between faces
and scenes. In addition, spatial frequencies of our images differ as a
function of emotion category, but the way they differ between emotion
categories does not depend on stimulus category. Descriptive statistics shows
that scenes had overall higher spatial frequencies than faces (reflected in
the main stimulus category effect), which we interpret as a result of higher
complexity of scenes compared to faces. In both stimulus categories it can be
seen that neutral images are associated with lower spatial frequency values
compared to both positive and negative emotion categories (reflected in the
main emotion category effect and the not significant interaction of both
factors).
Further, t-tests revealed significant differences between spatial frequencies
of neutral and negative faces (p = .010; T = 2.618), also neutral and positive
faces (p = .047; T = ? 2.009), but not negative and positive faces (p = .557;
T = .590). Spatial frequencies of negative scenes don't differ from those of
neutral scenes (p = .312; T = 1.017), but they do differ between neutral and
positive scenes (p = .009; T = ? 2.679). No differences are found between
positive and negative scenes (p = .107; T = ? 1.626).
In summary, positive and negative stimuli are associated with higher spatial
frequencies than neutral stimuli, which is true for both stimulus categories.
In general, scenes are associated with higher spatial frequencies compared to
faces (see Fig. 1).
It also turned out that luminance differences exist. An ANOVA revealed a
significant main stimulus category effect on luminance values (F = 10.106; p =.002). There is also a highly significant emotion category effect (F = 15.911;
p < .001), but a not significant interaction of those two factors (F = 2.931;
p = .058) (all Greenhouse?Geisser corrected). Similar to spatial frequency
data luminance data also demonstrate that differences exist between emotion
categories, but that these differences do not depend on stimulus category.
Paired-sample T-Tests revealed that image luminance differs significantly
between fearful faces and negative scenes (t = 4.177; p < .001). No other
differences were found to be significant (see Fig. 1).
Overall, we can summarise that both physical features show similar patterns in
terms of how they differ across stimulus and emotion categories. There are
emotion-specific differences in physical features, but those differences are
independent from stimulus category. In other words, any stimulus category
effect on brain activities can theoretically be explained by differences in
physical image features, but stimulus-specific emotion category effects
cannot.
For instance, taking a closer look at the present early EEG effects we notice
that the P1 stimulus category effects could theoretically be explained by
spatial frequency and/or luminance differences of our stimuli, but this is not
the case for the task-dependent effects and also not for all of the emotion-
specific brain activities that differ between faces and scenes as they were
found in later time windows.
### Tasks and procedure
During individual testing sessions, participants sat in a reclining chair
under dim lighting and positioned in front of a display monitor to allow 9.9°
× 8.5° of visual angle (300 × 399 pixels). After being connected to the
recording equipment, participants completed three delayed match-to-sample
emotion recognition tasks in random order. Each task was completed once with
face stimuli and once with scene stimuli (blocked sessions randomised within
tasks; Figs. 2a and b). At the beginning of the session participants were
informed of what each task involved so as to minimise potential practice
effects of task order bias. Then for each task, instructions for that task
were repeated and six practice trials were completed. Practice trials were not
included in the analyses.
Trials in each task always began with a fixation cross followed by a happy,
neutral or fearful picture (Stimulus 1; ?S1?) for 3000 ms, which required no
overt response from the participant other than to simply view the picture. S1
was proceeded by a second stimulus (Stimulus 2; ?S2?) which required an active
response. For one of the tasks (Picture?Picture matching; ?Pic?Pic?), the S2
was another happy, neutral or fearful picture, while for another one
(Picture?Word matching; ?Pic?Word?), the S2 was one of the three emotion
category labels ?fear?, ?neutral? or ?happy? presented in block white letters
against a black background. The active response for the Pic?Pic and Pic?Word
tasks was a forced choice (match/mismatch) judgement of whether S1 and S2
represented the same emotion (happy, fear or neutral) by pressing one of two
buttons (?Z? or ?/?) with the corresponding index finger as quickly as
possible without forgoing accuracy. For a third task (Picture labelling; ?Pic-
Label?), the S2 was the symbol ??? which cued the participant to say out loud
any one word that best described the emotion depicted in S1. For the Pic-Label
task, participants continued to the next trial by pressing the space bar. Key
responses at the end of each trial cued a 1000 ms inter-trial interval before
the next trial began. It should be emphasised that the S1 presentation
conditions were identical across the three tasks, i.e., the S1 was always a
passively viewed picture. Hence, the S2 event served as the experimental
manipulation to examine whether emotional information (S1) is processeddifferently depending on the emotion context of S2 and the task. Accordingly,
the event of interest for the analysis was the 3000 ms time window
corresponding to the S1 presentation.
There were 90 trials for each face and scene version of the tasks (30 fearful
(negative), 30 neutral and 30 happy (positive) S1 presentations). All S1
pictures were novel (i.e., no picture was presented more than once) and were
presented in colour (as were the S2 Pic?Pic pictures). S1 stimuli were
randomly presented within tasks, and for the Pic?Pic and Pic?Word tasks, S2
items were also randomised. Hence, each S1?S2 pairing was randomly generated,
however the frequency of match/mismatch and S1?S2 emotion category
combinations was balanced. Participants were given a short break midway and at
the end of each 90-trial task. The experiment took approximately one hour to
complete.
### Measures and data reduction
Due to the different nature and dynamics of biosignals recorded in the frame
of this study we chose different epoch lengths for the different measures.
Since we focus on early brain activity changes we set maximum epoch length to
1 s, but actually display ERPs only until 400 ms post stimulus, because during
this period early changes occur. Facial EMG epochs were set to 1.5 s to
potentially capture later effects and skin conductance epochs were set to 4 s,
because of their less dynamic nature and their delay (see more details below).
#### EEG recordings
Scalp EEG, measured in micro Volts (?V), was recorded using a 64 channel
Biosemi cap and amplifier (http://www.Biosemi.com) sampled continuously at
2048 Hz using an electrode layout corresponding to the 10-10-electrode
placement standard, and referenced to a common-mode signal. The data was down-
sampled offline to 256 Hz and filtered from 0.1 to 30 Hz using EEG Display
software. Eye blink artefacts were corrected using a set of linear regression
weights at each EEG electrode derived from an averaged eye blink (Semlitsch et
al., 1986). Segments of the EEG record containing gross artefact were detected
by an automated procedure that applied amplitude thresholds within a number of
frequency bands. For each S1 stimulus, EEG epochs were extracted from 100 ms
pre-stimulus to 1000 ms post-stimulus, and baseline corrected across the pre-
stimulus interval. Trials containing artefact exceeding ± 100 ?V were
excluded. Finally, trials were averaged to produce the ERP for each image
type.
Noisy channels were interpolated prior to data reduction for five
participants, involving no more than one electrode within each analysed
cluster. The first two trials completed during each face and scene recognition
task were removed. On average, 14% of trials were removed (4/30 trials per
condition for the 12 analysed conditions, SD = 2.92). Trials were then group-
averaged to create a single waveform per condition at each electrode location
and re-referenced to an average of all electrodes excluding the mastoid and
ocular sites.
The event-related components of interest were identified by visual inspection
of the electrode montage to identify clusters with prominent activity, and
then by software-facilitated comparisons to verify the exact electrode
locations of peak amplitude deflection. The high number of factors being
analysed increased the likelihood of generating false positive effects. To
reduce this risk, we employed procedures similar to those used by Schupp et
al. (2003) for calculating the grand mean of activity evoked during
experimental conditions for each component analysed. These included averaging
the activity of two electrode locations showing the greatest peak deflection,
and then averaging over a time interval of at least 10 data samples (40 ms)centred over the peak. The P1 component peaked over posterior?occipital
electrodes PO7/O1 and PO8/O2at 130 ms on average for face stimuli,
approximately 20 ms earlier than for scene stimuli, which had an average
latency of 150 ms. For each participant a single mean amplitude for each of
the 12 experimental conditions was calculated for the 120?160 ms time
interval. At temporal?occipital regions face stimuli elicited a prominent N170
component, while scene stimuli elicited only a small N100. These components
were immediately followed by a slower progressive negative shift, the so
called early posterior negativity (EPN). The N100/N170 and EPN were most
prominent over temporal?occipital electrode locations P7/P9 and P8/P10. For
each experimental condition, a single mean amplitude was again calculated for
each participant for the 150?190 ms time interval corresponding to the
N100/N170 and for the 220?280 ms time interval corresponding to the EPN.
#### fEMG recordings
The corrugator supercilii (CS) muscles, which furrow the eyebrows, were used
to reference muscle potential changes corresponding to face and scene stimuli
of negative content (see Ekman and Friesen (1978)). The zygomaticus major (ZM)
muscles, which lift the cheeks and lips were used to reference muscle
potential changes corresponding to face and scene of positive content. fEMG of
the CS and ZM, measured in micro Volts (?V), was recorded using a NeXus-10
wireless amplifier (http://www.Mindmedia.com) connected via Bluetooth to a PC
laptop, and output measurements were recorded using the NeXus-customised
Biotrace + Software. A NeXus Trigger Interface was used to synchronise the
onset of trial events between the continuous EEG and EMG recordings to within
less than 1 ms accuracy (http://www.Mindmedia.com).
Bipolar electromyography (EMG) was used to record muscle potential changes of
both muscles on both sides of the face. We used dual channel electrode cables
with carbon coating and active shielding technology for low noise and an
additional ground electrode cable attached to the back of the neck (see Reaz
et al., 2006, Wand, 2015). The EMG sampling rate was 2048 Hz. A band pass
filter from 20 Hz to 500 Hz was applied during online recording. Raw EMG data
were then recalculated by using the root mean square (RMS) method (epoch-size
= 1/16 s) to transform EMG signals into amplitudes.
The resulting amplitudes were then subject to statistical analysis. Using a
Matlab based program (www.mathworks.com), a single 1750 ms epoch time-locked
to 250 ms preceding the onset of each S1 stimulus presentation was then
extracted and divided into seven 250 ms time intervals by averaging across
data points. The first time window (? 250?0 ms) served as a baseline
correction for the six following intervals (0?250, 250?500, 500?750, 750?1000,
1000?1250, 1250?1500 ms) which were the subject of the analysis. After the
removal of gross artefacts, the time windows were baseline corrected and the
first two trials for each face and scene recognition task were removed from
analysis. An inspection of within-trial and across-trial variance was carried
out for each data set using an outlier criterion of 3.5 SD or greater. On
average, 18% of trials were removed from the ZM data (5/30 trials per
condition for the 12 analysed conditions, SD = 3.56) and 13% of trials were
removed from the CS data (4/30 trials per condition, SD = 3.35).
#### Skin conductance recordings
Skin conductance was recorded at a rate of 32 Hz with a Nexus-10-SC/GSR sensor
(Two finger sensor) connected to the Nexus-10 recording system with a 24 bit
resolution which is able to register changes of less than 0.0001 ?S. Because
the galvanic skin response is slow-changing, a 4250 ms epoch was extracted,
time-locked to 250 ms preceding the onset of S1, with ? 250 to 0 ms serving as
the baseline correction interval. The residual was divided into four 1000 mstime intervals for further analysis (0?1000, 1000?2000, 2000?3000, 3000?4000
ms). Like with EMG data, after the removal of gross artefacts, the time
windows were baseline corrected and the first two trials for each face and
scene recognition task were removed from analysis. An inspection of within-
trial and across-trial variance was carried out for each data set using an
outlier criterion of 3.5 SD or greater. On average, 22% of trials were removed
(7/30 trials per condition, SD = 3.56).
### Statistical analyses
The analysis was a fully within-subjects design with three factors: Stimulus
type (Faces, Scenes) × Emotion (Fear (negative), Neutral, Happy (positive)) ×
Task (Pic?Pic, Pic?Word). Note again that the Pic-Label task was not included
in the current analysis. For each event-related potential (ERP) component of
interest, condition grand means for the extracted time intervals were subject
to a 4-way Stimulus type × Emotion × Task × Hemisphere (Left, Right) repeated
measures analysis of variance (RM ANOVA). For the ZM, CS and SCR analyses,
condition grand means for each time interval (six time intervals for ZM and
CS, and four for SCR) were subject to 3-way Stimulus type × Emotion × Task RM
ANOVAs. Significant interactions between Stimulus type and Emotion (_p_ < .05)
were further investigated where appropriate with secondary RM ANOVAs,
conducted separately for each stimulus type. All other significant
interactions involving the factor Stimulus type (_p_ < .05) were further
investigated with paired-samples _t_ -tests with bonferroni alpha corrections.
For Sphericity violations (_p_ < .05), Greenhouse?Geisser epsilon adjustments
were applied if ? < .75, otherwise Hyundt?Feldt. All main effects are
reported, however because the primary objective of the analysis was to
investigate differences in emotional face and scene processing, only the
interactions involving the factor Stimulus type are reported.
## Results
### EEG data
#### P1 component
Emotional content did not affect P1 amplitudes, however the type of stimulus
and task did. In addition to the different latencies at which the P1 emerged
for faces and scenes, significant main effects also emerged for the factors
Stimulus type (_F_ (1, 22) = 15.64, _p_ = .001, _?2_ = .42) and Task (_F_ (1,
22) = 24.89, _p_ < .001, _?2_ = .53). As can be seen in the top left and right
waveforms in Fig. 3, scenes evoked a larger mean P1 deflection compared to
faces, while pictures viewed during the Pic?Pic task produced larger mean P1
deflections compared to the Pic?Word task. The factors Stimulus type and Task
also interacted significantly (_F_ (1, 22) = 5.43, _p_ = .029, _?2_ = .20),
indicating that the effect of the Pic?Pic task on P1 amplitudes was greater
for scenes than for faces (see also topography maps in Fig. 3, far right).
1. Download: Download high-res image (292KB)
2. Download: Download full-size image
Fig. 3. Effects of the Pic?Pic and Pic?Word tasks on early visual processing
of emotional faces and scenes. For quick reference, examples of the tasks are
shown in the top right corner. Waveforms show grand-averaged ERPs collapsed
across emotion categories and time-locked to the onset of the passively viewed
Stimulus 1 (onset = 0 ms). Waveforms in the top panel represent brain activity
recorded at left and right posterior?occipital electrode regions. At the far
right are corresponding topographic maps of P1-related brain activity.
Waveforms in the bottom panel represent brain activity recorded at left and
right lateral occipital electrode regions. L = Left hemisphere. R = Right
hemisphere.
#### N170 component (faces)/N100 component (scenes)The factor Stimulus type produced a strongly significant main effect (_F_ (1,
22) = 324.22, _p_ < .001, _?2_ = .94) indicating that, in line with past
research, faces evoked a much larger negative deflection in the N100 time
window, the so called N170 component, compared to scenes. A significant main
effect of Task was also observed (_F_ (1, 22) = 33.60, _p_ < .001, _?2_ =
.60), indicating that pictures viewed during the Pic?Word task produced larger
mean N100/N170 deflections compared to the Pic?Pic task (Fig. 3 bottom left
and right waveforms).
Stimulus type also interacted separately with the factors Hemisphere (_F_ (1,
22) = 17.64, _p_ = < .001, _?2_ = .45) and Emotion (_F_ (2, 44) = 7.63, _p_ =
.001, _?2_ = .26). Effects of emotion content are shown in Fig. 4 top left and
right waveforms, and bar graphs depicting the mean activity across the
categories are also displayed at the bottom left. The Stimulus type ×
Hemisphere interaction indicated that regardless of task or emotional
expression, face stimuli produced greater activity over the right hemisphere
relative to left compared to scenes, which produced no observable lateralised
effects. However, note that when we followed up this effect by comparing the
total mean activity produced by faces over left and right hemispheres (i.e.
collapsing the means of emotion and task conditions), right hemispheric
activity was only marginally greater than left (_p_ = .059). More critically,
the Stimulus type × Emotion interaction was further investigated using
separate secondary ANOVAs for each stimulus type. These ANOVAs showed that the
N100 was not sensitive to the emotional content of scenes (_F_ (2, 44) = 1.64,
_p_ = .206, _?2_ = .07), but that the N170 was differently modulated depending
on the emotional facial expression (_F_ (2, 44) = 6.27, _p_ = .004, _?2_ =
.22). Contrasts confirmed that fearful faces evoked significantly (_p_ = .004;
_p_ = .043) more negative N170 amplitudes compared to neutral and happy faces,
respectively, while amplitudes for happy and neutral facial expressions were
not different (_p_ = .131).1
1. Download: Download high-res image (269KB)
2. Download: Download full-size image
Fig. 4. Effects of emotion category on early visual processing of emotional
faces and scenes. In the top panel, waveforms represent brain activity
recorded at left and right lateral occipital regions and show grand-averaged
ERPs collapsed across task categories and time-locked to the onset of the
passively viewed Stimulus 1 (onset = 0 ms). The bar graph at the bottom left
illustrates the mean N170-related activity averaged across left and right
hemispheres. The bar graph at the bottom right illustrates EPN-related
activity recorded over left and right hemispheres. Error bars represent one
standard error of the mean. * = The differences are significant after
Bonferroni corrections. L = Left hemisphere. R = Right hemisphere.
#### EPN time window
Waveforms corresponding to the EPN are also presented in Fig. 4 (top left and
right waveforms), and bar graphs depicting the mean activity across the
categories are displayed at the bottom right. Main effects of Stimulus type
(_F_ (1, 22) = 412.62, _p_ < .001, _?2_ = .95), Task (_F_ (1, 22) = 19.08, _p_
< .001, _?2_ = .46), and Emotion (_F_ (1.70, 37.38) = 11.60, _p_ < .001, _?_ 2
= .35, with sphericity corrections ?2 = 6.16, ? = .85, _p_ = .046) re-emerged,
as did the interaction between Stimulus type and Emotion (_F_ (2, 44) = 43.11,
_p_ < .001, _?2_ = .66). Additionally, a significant main effect of Hemisphere
emerged (_F_ (1, 22) = 5.70, _p_ = .026, _?2_ = .21), which led to a three-way
interaction between Stimulus type, Emotion, and Hemisphere (_F_ (2, 44) =
5.54, _p_ = .007, _?2_ = .20). Accordingly, secondary ANOVAs of each stimulus
type were performed with Emotion and Hemisphere as within-subjects factors.For faces, Emotion (_F_ (2, 44) = 11.63, _p_ < .001, _?2_ = .35) and
Hemisphere (_F_ (1, 22) = 4.53, _p_ = .045, _?2_ = .17) produced significant
main effects but did not interact (_p_ = .171). Contrasts confirmed that the
negative-going shift in activity during fearful face presentations was
significantly greater than during happy and (_p_ < .001) and neutral (_p_ =
.013) face presentations. The effect of Hemisphere further indicated greater
negative-going activity over the left hemisphere, but see below for a more
thorough interpretation. For scene stimuli, Emotion (_F_ (2, 44) = 41.96, _p_
< .001, _?2_ = .66) and Hemisphere (_F_ (1, 22) = 5.95, _p_ = .023, _?2_ =
.21) produced significant main effects and did interact significantly (_F_ (2,
44) = 3.74, _p_ = .032, _?2_ = .15). The EPN was differently sensitive to
emotional content in scenes than in faces however, with positive scenes
eliciting significantly greater negative-going activity compared to negative
and neutral scenes (both _p_ 's < .001). Critically, the interaction of
Emotion and Hemisphere evoked by scenes revealed that although scenes (and
faces) generated more negativity over the left hemisphere, there was greater
discrimination of positive from negative scenes over the right hemisphere (_p_
= .005), hence the EPN was more robust over the right hemisphere. In short,
fearful faces and positive scenes elicited a stronger EPN overall compared to
other emotional stimuli, but positive scenes also elicited a lateral
difference in EPN magnitude whereas fearful faces did not. See Table 1
summarising all EEG-related statistical significancies.
Table 1. Summary of significant factor main effects and/or significant factor
interactions related to EEG data.
EEG| P1| N170/N100| EPN
---|---|---|---
_Task_| | P < .001| P < .001
_Emotion_| | | P < .001
_Stimulus_| P < .001| P < .001| P < .001
_Hemisphere_| | | P = .026
_stimulus * task_| P = 0.29| |
_stimulus * hemisphere_| | P < .001|
_stimulus * emotion_| | P = .001| P < .001
_stimulus* emotion * hemisphere_| | | P = .007
### EMG data
#### Zygomatic recordings
No main effects emerged from the six ANOVAs, however a significant three-way
interaction between Stimulus type, Emotion and Task emerged at the third time
interval corresponding to the time between 500 and 750 ms post-stimulus (_F_
(2, 44) = 3.22, _p_ = .049, _?2_ = .13) followed by a sustained interaction
between Stimulus type and Emotion over the next three time intervals (750?1500
ms; _F_ (2, 44) = 4.54, 6.45, 4.85, _p_ = .016, .003, .012, _?2_ = .17, .23,
.18, respectively). As seen in Fig. 5, these interactions collectively
indicated that positive scenes evoked spontaneous ZM activity, while happy
faces did not. The initial three-way interaction between Stimulus type,
Emotion and Task also suggested that positive scenes elicited spontaneous
activity earlier during the Pic?Word task compared to the Pic?Pic task.
However, when the corresponding data for scene stimuli was submitted to a
secondary ANOVA with Task and Emotion as the within-subjects factors, the
modulatory effect of Task disappeared (_p_ = .202), and, consistent with the
effects at ensuing time intervals, was replaced with a significant Stimulus
type × Emotion interaction (_F_ (2, 44) = 3.62, _p_ = .035, _?2_ = .14). As
expected, the secondary ANOVA for face stimuli revealed no effects of task or
emotion (all p-values > .2)1. Download: Download high-res image (128KB)
2. Download: Download full-size image
Fig. 5. Mean zygomatic muscle EMG amplitudes (?V) and error bars for 1
standard error, time-locked to the onset of the passively viewed Stimulus 1
(onset = 0 ms). * = The differences are significant after Bonferroni
corrections.
Paired samples _t_ -tests were used to determine significant fluctuations in
ZM activity between emotional scene categories at each time interval for
intervals three to six (corrected significance threshold = .017). From
approximately 500?750 ms, positive scenes evoked significant differences in ZM
activity relative to negative scenes (_p_ = .034, .005, .019, .088 (trend),
respectively for intervals 3?6). Significant differences between positive and
neutral scenes did not emerge until 750?1000 ms, but were reliably strong
across the epoch (_p_ = .228, .004, .001, < .001, respectively for intervals
3?6). See Table 2 summarising all statistically significant zygomaticus
effects.
Table 2. Summary of significant factor main effects and/or significant factor
interactions related to _zygomaticus major_ EMG data.
EMG| Zygomaticus
---|---
Empty Cell| _0?250_| 250?500| 500?750| 750?1000| 1000?1250| 1250?1500
_Stimulus * Emotion_| | | | P = .016| P = .003| P = .012
_Stimulus* Emotion * Task_| | | P = .049| | |
#### Corrugator recordings
As shown in the top panel in Fig. 6, CS activity was characterised by a rapid
reduction in muscle activity from stimulus onset to 750 ms during all face and
scene S1 presentations, however the rate of this decline was faster when faces
were viewed compared to scenes, which led to a significant main effect of
Stimulus type in the 250?500 ms time window (_F_ (1, 22) = 5.58, _p_ = .027,
_?_ 2 = .20).2 This apparent relaxation of corrugator muscles at the point of
stimulus onset has been demonstrated by others (e.g. Achaibou et al., 2008,
Dimberg and Petterson, 2000, Dimberg et al., 2000, Dimberg et al., 2002), and
is thought to be the result of increased tension in corrugator muscles at
baseline due to anticipatory focus and attention towards an imminent visual
stimulus presentation (van Boxtel and Jessurun, 1993, Van Boxtel et al.,
1996).
1. Download: Download high-res image (193KB)
2. Download: Download full-size image
Fig. 6. Mean corrugator muscle EMG amplitudes (?V) and error bars for 1
standard error, time-locked to the onset of the passively viewed Stimulus 1
(onset = 0 ms). The top graph illustrates the mean amplitudes evoked by
emotional faces compared with scenes after collapsing across emotion and task
categories in order to highlight early latency differences involving
corrugator muscle relaxation between the stimuli, presumably reflecting
differences in the speed of early attentional orienting. The bottom graphs
illustrate the different patterns of spontaneous corrugator activity elicited
by emotional faces and scenes. Note that amplitude values differ across the
scales in the top and bottom panel graphs. * = The differences are significant
after Bonferroni corrections.
Then, from 500 to 750 ms, spontaneous muscle activity emerged as a function of
emotion category for both face and scene stimuli, seen via a significant main
effect of emotion (_F_ (2, 44) = 9.39, _p_ < .001, _?_ 2 = .30) that remained
reliably significant across the next three time windows.3 As seen in the
bottom panel in Fig. 6, spontaneous activity tended to be greater for fearfuland neutral faces and negative and neutral scenes, while happy faces and
positive scenes led to greater relaxation of the CS muscles. Paired samples
_t_ -tests were again used to determine significant fluctuations in CS
activity between emotional categories, done separately for faces and scenes at
each time interval of interest i.e. intervals three to six (with a corrected
significance threshold of .017). For faces, spontaneous emotion-related
activity emerged only briefly at 500?750 ms as a trend (comparisons at all
other intervals, _p_ > .07). Here, fearful and neutral expressions evoked
significantly (_p_ = .017; _p_ = .061 (only trend)) greater CS activity
compared to happy expressions, respectively. Contrastingly, negative scenes
evoked a stronger, more enduring spontaneous effect from 500?750 ms onwards,
producing significantly greater activity relative to positive scenes across
most of the epoch (_p_ = .017, < .001, .091, .005, respectively for intervals
3-6). The generally stronger activity evoked by neutral compared to positive
scenes reached significance only at the fourth interval between 750 and 1000
ms (_p_ = .638, .014, .078, .186, respectively for intervals 3?6), and similar
to the effects observed in ZM activity, negative and neutral scenes evoked
very little difference in CS activity (_p_ = .043, .161, .801, .048,
respectively for intervals 3?6). See Table 3 summarising all statistically
significant corrugator effects.
Table 3. Summary of significant factor main effects and/or significant factor
interactions related to _corrugator supercilii_ EMG data.
EMG| Corrugator
---|---
Empty Cell| _0?250_| 250?500| 500?750| 750?1000| 1000?1250| 1250?1500
_Stimulus_| | P = .027| | | |
_Emotion_| | | P < .001| P = .004| P = .006| P = .020
_Stimulus* Emotion * Task_| | | | | |
### Skin conductance recordings
The ANOVA corresponding to the first 1000 ms post S1-onset showed a trend
towards a significant main effect of Stimulus type (_F_ (1, 22) = 3.97, _p_ =
.059, _?_ 2 = .15), followed by a significant effect between 1000 and 2000 ms
(_F_ (1, 22) = 5.22, _p_ = .032, _?_ 2 = .19), which then diminished from 2000
ms onwards (_p_ > .3). As seen in Fig. 7, skin conductance levels were greater
overall at an early post-stimulus stage and tended to decrease over the three
second presentation, with face stimuli evoking slightly greater activity
during the early stage. That the effect occurred at such an early stage
relative to a typical skin conductance response which emerges more slowly at
around 2 s post stimulus, suggests that differences between faces and scenes
were a residual effect related to the S2 active response stage of preceding
trials. Nevertheless, the significant effect indicates that faces and scenes
evoked different arousal-related activity which was independent of emotional
content. See Table 4 summarising all statistically significant skin
conductance effects.
1. Download: Download high-res image (133KB)
2. Download: Download full-size image
Fig. 7. Mean skin conductance amplitudes (?S) and error bars for 1 standard
error evoked by emotional faces and scenes time-locked to the onset of the
passively viewed Stimulus 1 (onset = 0 ms). Means were calculated by
collapsing across emotion and task categories. * = The differences are
significant after Bonferroni corrections.
Table 4. Summary of significant factor main effects and/or significant factor
interactions related to SC data.
SC---
Empty Cell| _0?1000_| 1000?2000| 2000?3000| 3000?4000
_Stimulus_| P = .059| P = .032| |
## Discussion
The aim of the current study was to determine the differences in emotional
face and scene processing at neural and behavioural (i.e. spontaneous facial
activity) levels. Using EEG to measure neural activity we found that the early
visually-evoked P1 component peaked earlier for faces than for scenes, and
that the type of task differently modulated the depth of this visual-related
processing for faces and scenes. For faces the N170 was sensitive to the
emotional content of the stimuli whereas the N100 for scenes was not. The EPN
was sensitive to the emotional content of both faces and scenes, but
differently so. For faces, the EPN was enhanced by fearful expressions as was
the N170, while for scenes, positive content elicited enhanced EPN amplitudes,
more prominent over the right hemisphere. Using fEMG we found that positive
scenes but not happy faces elicited enhanced spontaneous zygomatic activity,
whereas both fearful faces and negative scenes elicited enhanced spontaneous
corrugator activity, but again this emotion effect was more enduring for
scenes. Furthermore, prior to the influence of emotion, corrugator activity
was marked by a rapid orienting response that occurred faster for faces than
for scenes, which was akin to early P1 effects. Finally, skin conductance
responses revealed slightly greater arousal levels when viewing faces than
when viewing scenes. That the effect occurred at an early stage relative to a
typical skin conductance response which emerges more slowly at around 2 s post
stimulus, suggests that differences between faces and scenes were a residual
effect related to the S2 active response stage of preceding trials.
Nevertheless, the significant effect indicates that faces and scenes evoked
different arousal-related activity which was independent of emotional content.
### Early neural processing of emotional faces and scenes
#### P1 component
Neural activity at the early visually evoked P1 component showed that
stimulus-specific features of faces and scenes evoked different degrees of
rapid attentional processing irrespective of emotional content. Scenes
generated greater visually-evoked cortical activity compared to faces, most
likely because of their greater degree of complexity (Bradley et al., 2007).
Bradley et al. also found that picture complexity influences the magnitude of
evoked potentials proceeding the P1, which would explain the large differences
in visually evoked potentials between stimulus groups in the current study.
The Pic?Pic task also differently influenced the depth of P1-related visual
processing of scenes and faces in a manner that was proportional to the
complexity of the stimuli. In other words, processing of scene stimuli, which
were more complex, was considerably enhanced, whereas processing of face
stimuli, which were less complex, was only slightly enhanced.
It is important to mention that even though we can rule out that both
luminance and spatial frequency (see method section) explain later task- and
emotion-specific effects it is theoretically possible that those physical
features explain category-specific effects like the ones described above.
Beyond this stage, other early processes involved in face and object
perception, i.e. the N100/N170 and EPN components, were not differently
modulated by faces and scenes as a function of the recognition tasks.
#### N100/N170 component
A critical finding at the neural level was that affective information in faces
influenced neural activity earlier than affective information in scenes, as
reflected by the enhanced activity of the N100/N170 for fearful faces, but notfor emotional compared to neutral scenes. Recently, Thom et al. (2014) also
compared neural activity generated by emotional faces and scenes in a single
experimental paradigm, and found that neural processes underlying the
N100/N170 component were sensitive to affective information for scenes. These
differences between Thom et al.'s and our findings may be explained by
methodological differences including that, in their study, some emotional
scene stimuli contained faces (fear, joy and angry stimuli, but not neutral,
which were all inanimate objects), while we specifically did not include
scenes with forward-facing faces. Also, their participants were all males
whereas in the current study participants were a mixture of males and females.
The latter distinction is important because males have been shown to generate
enhanced N100/N170 amplitudes compared to females (Proverbio et al., 2009),
and it has been shown that natural scenes with and without human faces can
evoke significantly different activity in the N100/N170 time window (Ferri et
al., 2012, Proverbio et al., 2009). It is possible that these factors,
particularly when combined, led to selective enhancement of amplitudes in the
N100/N170 time window during their emotional scene presentations.
The negativity bias of fearful facial expressions relative to neutral and
other emotional expressions has received a variety of interpretations in the
literature. Some have suggested that the early discrimination of fearful from
neutral faces is due to crude threat or signs of danger which rapidly activate
neural circuits specialised for detecting danger (e.g. Esteves et al., 1994,
Hansen and Hansen, 1988, LeDoux, 2003, Öhman, 2005, Öhman and Mineka, 2001).
However, this theory likely does not explain our results, as we would have
expected negative scenes, which included fearful components such as snakes and
spiders to evoke such activity as well, particularly considering that
detecting negative scenes is highly survival-relevant, and that negative
scenes in this study were rated as more unpleasant and more arousing than
fearful faces.
Vuilleumier and Pourtois (2007) instead reason that the anatomical regions
involved in facial expression recognition may be spatially organised according
to emotion categories, in that sub-regions associated with encoding facial
features more unique to one expression are spatially segregated from sub-
regions associated with encoding those that are more unique to another
expression. Hence, emotion category-specific modulation of the N170 component
may be reflecting these regional variations rather than motivational emotional
significance of the stimuli (Vuilleumier and Pourtois, 2007). In support of
this theory, they argue that activity in the N170 time window has been found
to be differently sensitive to a range of facial expressions in addition to
fear such as surprise and disgust. Along this line, faces with more similar
expressions such as anger and fear (both negative and with overlapping facial
muscle contraction) evoke more similar modulatory activity than when compared
with happy faces (Thom et al., 2014). On this basis, and when considering that
emotional content is identified faster for faces compared to scenes (Britton
et al., 2006, Dimberg et al., 2002, Eisenbarth et al., 2011), the current data
could be reflecting faster extraction of low level semantic affective
information from faces than from scenes, but not necessarily faster
identification of motivational emotional significance.
#### EPN time window
An EPN emerged at a similar latency for emotional faces and scenes, occurring
immediately following the offset of the N100/N170 components. For faces, the
EPN was pronounced only for fearful expressions, while for scenes, the EPN was
pronounced only for positive content. When considered separately, these
selective modulations of the EPN are in line with past research showingenhanced negativity for fearful faces in the EPN time window compared to
neutral (Leppänen et al., 2008, Stekelenburg and de Gelder, 2004) and happy
expressions (Herbert et al., 2013b, Mühlberger et al., 2009, Schupp et al.,
2004b), and enhanced EPN negativity for positive scenes compared to negative
and neutral scenes (Bublatzky and Schupp, 2011, Franken et al., 2008, Schupp
et al., 2004a, Schupp et al., 2006b, Schupp et al., 2007, Schupp et al.,
2013b, Weinberg and Hajcak, 2010). Further research also supports our
observation that neural activity during the EPN time-frame was generally more
negative over the left hemisphere compared with the right (Schupp et al.,
2004b, Smith et al., 2013), while stronger emotion-specific modulation
occurred over the right hemisphere, and seemed to be an exclusive effect of
scene stimuli (Junghöfer et al., 2001, Schupp et al., 2007). These findings
suggest that EPN activity includes a commonality between processing of fearful
facial expressions and positive scenes.
Still, there seems to be no clear explanation in the literature addressing why
emotional faces and scenes evoke such different category-specific responses in
the EPN time window. Speculation has centred on the motivational significance
of affective cues, particularly in that erotica are highly arousing stimuli.
In the present study, stimuli with low or moderate arousal were chosen which
could have facilitated an arousal-driven processing bias for positive scenes
due to the erotica content. However skin conductance recordings during these
presentations do not support this interpretation, and instead, suggest that
all face stimuli evoked enhanced arousal levels compared to scenes. Findings
from Thom et al.'s study of faces and scenes (2014) also showed that despite
positive scenes (including erotica) being rated as more arousing than all
other emotional scene categories, these stimuli did not produce enhanced EPN
activity. Thus it seems that other factors are more likely.
Experiments involving EPN analyses have inevitably become more elaborate, and
there is now some evidence linking EPN activity to modulations of self-
reference or task relevance (e.g. Herbert et al., 2013a, Herbert et al.,
2013b, Stekelenburg and de Gelder, 2004), which could also be extended to
explain the differential effects of face and scene stimuli seen in the current
study. Evidence from several studies suggest that as stimuli become more
salient with respect to the complexity of required processing, typically as a
result of task demands, so too does the degree of EPN modulation during the
associated stimulus presentations, suggesting that EPN activity could be a
precursor to more conscious levels of stimulus evaluation (Herbert et al.,
2013a, Stekelenburg and de Gelder, 2004).
For example, in a study requiring participants to categorise fearful and
neutral faces as either upright or inverted, upright fearful faces predictably
evoked increased EPN activity compared to upright neutral faces. When faces
were inverted however, the EPN was enhanced for both fearful and neutral faces
compared to when the same faces were shown in the upright position
(Stekelenburg and de Gelder, 2004). This suggests that the EPN is sensitive to
task-induced changes in stimulus complexity.
These effects extend even to self-referential emotion processing whereby
emotional faces preceded by matched self-relevant word cues such as ?my fear?
or ?my happiness? evoked enhanced EPN activity compared to when the preceding
cues were meaningless letter strings (Herbert et al., 2013b), supporting the
view that the self-reference of affective stimuli facilitates motivated
attention capture to emotional stimuli as reflected by the EPN component. In
another study, participants were asked to use specific cue words (e.g. cues
like ?no fear?, ?no panic? etc.) to intentionally regulate their feelings
evoked by fearful and happy faces. Using these cue words that attenuated theemotion described in the picture (e.g. no fear paired with a fearful face)
attenuated ERP amplitudes to fearful faces as early as in the EPN time window
(Herbert et al., 2013a).
In the frame of active versus passive tasks, similar effects have also been
documented during the EPN time interval. For example, Schupp et al. (2007b)
compared the effect of passively viewing emotional scenes versus the effect of
actively counting the number of times a specifically valanced scene was
presented. They found typical EPN modulation during the passive viewing task,
whereby erotica (pleasant stimuli) elicited enhanced EPN activity compared to
mutilation (unpleasant) and neutral stimuli, but when participants were
required to count the number of presentations occurring for each emotion
category, EPN activity was significantly modulated relative to when the same
category of emotional stimuli were passively viewed (Schupp et al., 2007b).
More recently, a study by Schupp et al. (2013a) comparing passive viewing to
active semantic categorisation also suggested a link between EPN activity and
higher cognitive evaluations. Here, emotional scenes were overlayed with
simple pictures of either animals or other non-animal scenes and objects. In
the active categorisation task, participants were required to judge whether
the foreground picture was either an animal or not, while no response was
required during passive viewing. Again, in the passive viewing task, stimuli
with pleasant scenes as the background image evoked stronger EPN activity than
did stimuli with unpleasant or neutral background scenes. However, active
categorisation led to the diminishment of emotion-specific modulation, and
instead, EPN activity was enhanced overall relative to the passive viewing
task regardless of the emotional background. Moreover, EPN activity was
significantly enhanced for foreground pictures of animals compared to non-
animals, suggesting that task difficulty and stimulus salience can override
emotional significance in the EPN time window.
In the current experiment, where the task was to passively view the S1
stimuli, but also to implicitly identify its emotional content, stimulus
salience, and thus EPN activity, should have been driven, at least partially,
by emotion recognition difficulty. Fearful facial expressions and positive
scenes, which evoked enhanced EPN activity, may have been more difficult to
recognise relative to other sub-categories, resulting in a call for more
sophisticated cognitive processing to accurately identify and categorise these
stimuli. This is exactly what past research predicts. Recio et al. (2014)
twice demonstrated that recognising fearful faces as expressing fear was more
difficult than recognising happiness from smiling faces and neutrality from
neutral faces, and found that negative expressions (i.e., anger, disgust,
fear, sadness and surprise) were most often confused, whereas happy faces
enjoy a recognition advantage in the EPN time window (Calvo and Beltran,
2013). As would be predicted by these findings, other research further shows
that happy faces are identified and responded to faster than angry faces
(Sonnby-Borgström, 2002; see also Leppänen and Hietanen, 2004, Calvo and
Lundqvist, 2008) supporting the view that happy faces are attention grabbing
due to their salience (i.e. the biologically determined social relevance of a
happy face for both interaction partners including the perceiver and the
receiver (Becker and Srinivasan, 2014)).
Speculatively, this could also explain why positive scenes evoked enhanced EPN
activity relative to other emotional scene categories. The negative, mainly
fearful (e.g. spiders and snakes) and neutral (e.g. a computer, a chair) scene
stimuli used in the current study were generally rather obvious and intuitive
to categorise. However, the content of positive scene stimuli was more varied
(e.g. extreme sports, nature scenes, appetising foods and erotica), therebyproviding less-intuitive cues directly linked to ?happiness?. Hence, emotion-
category specific modulation of the EPN may have been related to differences
in recognition difficulty, resulting in a call for more sophisticated
cognitive processing to accurately categorise emotional stimuli which lacked
intuitive cues, thereby causing fearful facial expressions and positive scenes
to become more salient with respect to EPN-related brain activity.
### Spontaneous facial reactions to emotional faces and scenes
Emotion-related spontaneous activity of both the zygomatic and corrugator
muscles emerged between 500 and 1000 ms post stimulus, which is consistent
with other fEMG studies of emotional faces (Dimberg, 1982, Dimberg, 1997,
Moody et al., 2007) and scenes (Dimberg et al., 1998). It suggests that faces
and scenes trigger the same neural affective processes, or processes with
similar latencies leading to spontaneous facial reactions. This is also
consistent with the observation that emotional significance was detected in
neural activity at a similar latency for face and scene stimuli.
One difference between faces and scenes however, was that spontaneous
zygomatic activity was evoked by positive scenes but not by happy faces.
Künecke et al. (2014) also found no effect of happy faces on spontaneous
zygomatic activity, while fearful facial expressions evoked reliably enhanced
corrugator activity. Moreover, their experiment was also conducted in the
frame of an emotion recognition task which, similar to the current study,
required a delayed rather than immediate recognition judgement. As has been
discussed above, one possible explanation for why positive scenes but not
happy faces evoked spontaneous reactions is that, in the frame of an emotion
recognition task, smiling faces may simply be easy to recognise and
semantically categorise, whereas semantically categorising positive scenes
requires more effortful mental processing. If this is the case, we want to
speculate on these findings and point to two possible ideas: firstly, emotion-
related spontaneous facial activity is linked to emotion recognition
processes. Secondly, the mechanisms are triggered only when emotional
information is ambiguous, as has been suggested by others (Winkielman et al.,
2009). Moreover, basic motor-mimicry is not necessary for social-emotion
recognition, as has been demonstrated by others (Grèzes et al., 2013, Magnée
et al., 2007, Moody et al., 2007). The latter interpretation is particularly
supported by the observation that happy faces did not seem to evoke any
substantial change in zygomatic activity relative to baseline.
In addition to effects related to the content of the images, motivational
factors also seemed to contribute to the differential spontaneous reactions to
faces and scenes. Emotional scenes not only elicited spontaneous activity in
the emotion-appropriate muscles, but did so quite strongly compared to faces.
The disparity is particularly evident in the observed corrugator activity in
that fearful (and neutral4) faces evoked only a momentarily enhanced response,
whereas negative scenes evoked a strong stable response which seemed to become
even stronger at the same time that the effect of fearful faces diminished.
Alpers et al. (2011) also reported stronger orbicularis oculi (ring muscle
around the eyes) activity (an index of the Duchene smile) to pleasant scenes
than to smiling faces, and more broadly, demonstrated greater activation of
other behavioural indexes of motivation by emotional scenes compared to faces
including decreased heart rate acceleration, greater startle reflex modulation
and increased skin conductance levels, findings that were also replicated by
Wangelin et al. (2012).5 Indeed, stimulus arousal ratings in both our study
and Alpers' study also point to motivational influences in that emotional
scenes were rated as more arousing than emotional faces. It should also be
considered that in such a task-primed context with little ?real-world? social-motivational relevance, the veracity of motivational influences of facial
expressions on behaviour diminish, or perhaps strong stable
behavioural?emotional responses are not necessary during social interactions,
and instead could hinder one's ability to keep up with the naturally dynamic
exchange of affective signals in social?emotional interactions. Contrastingly,
emotionally evocative scenes more often involve an immediate
approach/avoidance overt physical response, and thus even at such an automated
stage of behaviour, it is logical to expect these stimuli will evoke more
stable and enduring motivational reactions.
### Consistencies between neural activity and spontaneous facial reactions
#### Stronger emotional responses to scenes
Consistent with the observation that emotional scenes evoked stronger and more
stable spontaneous behavioural activations compared to faces, neural processes
related to the EPN were also more strongly and stably activated by scenes than
by faces. These effects also emerged in Thom et al.'s comparison of faces and
scenes (2014), where they discussed this effect in terms of scenes activating
stronger motivationally relevant activity, particularly with respect to
theories pointing to increased amygdala activations as a contributing source
of EPN activity. Thom and colleagues interpretation is further supported by
functional MRI data showing that biologically relevant stimuli do activate
stronger functional connectivity between visual areas and the amygdala than do
socially relevant stimuli (Sakaki et al., 2012). Hence motivational factors
may influence how long evoked affective responses persist, rather than or in
addition to the strength of the response. Such a distinction may have gone
unnoticed in past facial EMG research, given that many studies focus on an
averaged amplitude for a block of time (some up to six seconds), rather than
shorter sequential averages.
The findings therefore draw on two similarities between EPN activity and
spontaneous facial reactions including stimulus salience, which is largely a
product of task demands, and the persistence of motivational responses, which
begs the question of whether EPN activity may be a precursor to motivated
behaviour. To test this theory, responses to positive scenes were analysed
(chosen because this stimuli evoked the strongest EPN activity as well as
reliably strong zygomatic activity, and thus would be of greatest interest and
least implicated by floor/ceiling effects) via a post hoc correlation of right
hemispheric EPN activity, and spontaneous zygomatic activity averaged over the
time frame of strongest emotional responding (750?1500 ms). If stronger EPN
activity is related to stronger spontaneous zygomatic responses, then we would
at least expect to see a negative linear correlation between these
physiological outputs indicating that the stronger the negative shift in EPN-
evoked potentials, the greater the amplitude of zygomatic muscle contractions.
Even without attention to potential outliers, the analysis of the 23 data sets
revealed a modest correlation in the predicted direction (_r_ = ? .29) which
was trending towards significance (_p_ = .091), while after removing one
outlier, the correlation strengthened considerably (_r_ = ? .47, _p_ = .014).
Of course, this correlation may represent a consistency in the absolute levels
of neural and behavioural activity within individuals. Therefore, further
research is necessary to determine the meaning of this relationship between
EPN activity and behavioural responses. In addition, it would also be
interesting to investigate the modulation of late ERP potentials such as the
LPP. The LPP has been shown to be influenced by emotional and cognitive
factors. In contrast to early ERP potentials such as the EPN, however, the LPP
is thought to reflect top-down controlled affective processing (Hajcak and
Nieuwenhuis, 2006; for review see Olofsson et al., 2008, Korb et al., 2012).#### Faster attentional orienting to faces
Of course it would also be interesting to explore the relationship between
fEMG activity and LPP modulation. However, as argued above such a correlation
would not reveal if cortical or facial changes occurred first. In the present
study we show that EPN modulation is correlated with changes in fEMG activity,
which suggests that facilitated early cortical processing of emotional scenes
is a prerequisite for changes in facial muscle activity.
Early neural activity and behavioural responses to faces and scenes suggests
that viewing emotional faces engages attention processes faster than when
viewing emotionally evocative scenes. This was reflected in the earlier onset
latency of the P1 component and the faster release of tension in the
corrugator eyebrow muscles when viewing faces. Such differences can also be
seen in evoked potentials in other studies of emotional faces and scenes
(Kujawa et al., 2012, Thom et al., 2014). These findings could assist in
explaining why the valence of emotional faces is identified faster than the
valence of emotional scenes when measured via button press (Britton et al.,
2006) and voluntary mimicry latencies (Dimberg et al., 2002, Eisenbarth et
al., 2011), and may be a factor underlying why attention disengagement from
socially-relevant stimuli to a secondary target is faster than from
biologically-relevant stimuli (Sakaki et al., 2012).
### Conclusions
The findings in this study point to two key differences between emotional face
and scene stimuli during early visual processing, including rapid attentional
capture mechanisms and motivated response mechanisms. Broadly, there was a
logical chronology of stages of perception that could be seen in both neural
activity and behavioural output in that selective attention determined what
was attended to, which manifested into affectively-driven selective emotional
responses. However, not all stimulus percepts affecting neural activity were
apparent in behavioural activity. The picture- versus word-primed recognition
tasks elicited different neural activity at the P1 and N100/N170 components,
which broadly, reflects variations in basic object recognition processes.
However, the recognition tasks did not influence the nature of spontaneous
behavioural responses. This suggests that fast motivated emotional behaviour,
including spontaneous facial reactions, is relatively robust to variations in
basic object and face recognition pathways such as variations in semantic-
priming, and logically corroborates with the idea that behavioural responses
to emotional stimuli are grounded in motivational emotional significance.
Further research investigating consistencies between neural activity and
behavioural responses is needed to replicate and extend the current research.
Critically though, this will require careful sequential temporal analyses, as
opposed to amplitude-only analyses, particularly at very early time frames of
less than one second, which is quite rare in the facial EMG literature.
However, the fact that EMG techniques offer excellent temporal resolution
matching that of EEG means that such studies are viable and will be very
valuable.
## Acknowledgments
We thank Ross Fulham, Tony Kemp, Melinda Vardanega, Natalie Townsend, Jennifer
Gilchrist, Samantha Allen and Jacob Duffy for their assistance performing the
study.
Recommended articles"
89,91,Emotionet challenge: Recognition of facial expressions of emotion in the wild,"['CF Benitez-Quiroz', 'R Srinivasan', 'Q Feng']",2017,109,Expression in-the-Wild,machine learning,"recognition of facial expressions of emotion in the wild. Key  facial expressions, the  heterogeneity of expression in the wild in the wild by current computer vision and machine",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1703.01210,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
90,92,"Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild","['C Fabian Benitez-Quiroz', 'R Srinivasan']",2016,685,"Affective Faces Database, Expression in-the-Wild","classification, classifier","in our set of a million face images in the wild, we  databases to successfully recognize AUs  and AU intensities on an independent database of images not used to train our classifiers. 2.",No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/7780969,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
91,93,Enhanced amygdala reactivity to emotional faces in adults reporting childhood emotional maltreatment,"['AL van Harmelen', 'MJ van Tol']",2013,302,Karolinska Directed Emotional Faces,"classification, neural network","to negative emotional faces are related to psychopathology, we investigated whether  abnormal amygdala (and/or mPFC functioning) was more apparent in emotionally maltreated",No DOI,Social cognitive and …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3624946/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
92,94,Ensemble-based discriminant learning with boosting for face recognition,"['J Lu', 'KN Plataniotis']",2006,244,Toronto Face Database,"classifier, machine learning",ensemble-based methods as they are known in the machine learning literature [18]). Globally  nonlinear methods are not without problems. Approaches such as those based on kernel,No DOI,IEEE transactions on …,https://pubmed.ncbi.nlm.nih.gov/16526485/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
93,95,Estimation of continuous valence and arousal levels from faces in naturalistic conditions,"['A Toisoul', 'J Kossaifi', 'A Bulat', 'G Tzimiropoulos']",2021,160,Affective Faces Database,"machine learning, neural network","For example, the ability to accurately extract emotional information from the face of the  person one is communicating with plays a major role in prosociality 24 and this capacity is often",No DOI,… Machine Intelligence,https://www.nature.com/articles/s42256-020-00280-0,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
94,96,Evidence and a computational explanation of cultural differences in facial expression recognition.,"['MN Dailey', 'C Joyce', 'MJ Lyons', 'M Kamachi', 'H Ishi']",2010,249,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning, neural network",our interpretation of facial expressions of emotion is universal  EMPATH models to recognize  facial expressions in a variety  of facial expression images with different mixes of Japanese,No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/21171759/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,
95,97,Excavating AI: The politics of images in machine learning training sets,"['K Crawford', 'T Paglen']",2021,442,Toronto Face Database,machine learning,"By looking at the politics of classification within machine learning systems, this article  demonstrates why the automated interpretation of images is an inherently social and political",No DOI,Ai & Society,https://link.springer.com/article/10.1007/s00146-021-01162-8,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
96,98,Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset,"['D Kollias', 'S Zafeiriou']",2020,182,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"The Aff-Wild database served as benchmark for the Aff-Wild Challenge, organized in   the temporal dependencies of facial expressions in each utterance, we designed standard",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1910.01417,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
97,99,"Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface","['D Kollias', 'S Zafeiriou']",2019,345,Expression in-the-Wild,CNN,"expression classification). To address these, we substantially extend the largest available  in-the-wild  We conduct extensive experiments with CNN and CNN-RNN architectures that use",No DOI,arXiv preprint arXiv:1910.04855,https://arxiv.org/abs/1910.04855,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
98,100,Extended deep neural network for facial emotion recognition,"['DK Jain', 'P Shamsolmoali', 'P Sehdev']",2019,416,"Extended Cohn-Kanade, Japanese Female Facial Expression","classification, deep learning, facial expression recognition, neural network","This paper proposed a new deep learning model for the  size and variety of datasets, deep  neural network is the most  datasets are publicly available such as Cohn–Kanade (CK+) [7]",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S016786551930008X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Extended deep neural network for facial emotion recognition
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 3\. Datasets and pre-processing
7. 4\. Proposed model
8. 5\. Experiment and evaluation
9. 6\. Conclusion
10. References
Show full outline
## Cited by (291)
## Figures (8)
1. 2. 3. 4. 5. 6.
Show 2 more figures
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Pattern Recognition Letters
Volume 120, 1 April 2019, Pages 69-74
# Extended deep neural network for facial emotion recognition
Author links open overlay panelDeepak Kumar Jain a, Pourya Shamsolmoali b,
Paramjit Sehdev c
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2019.01.008Get rights and content
## Highlights
* ?
A new Deep Fully Connected model for facial emotion recognition.
* ?
The model evaluated based on multiple hyperparameters and real world datasets.
* ?
The application of the model on the real-time events.
## Abstract
Humans use facial expressions to show their emotional states. However, facial
expression recognition has remained a challenging and interesting problem incomputer vision. In this paper we present our approach which is the extension
of our previous work for facial emotion recognition [1]. The aim of this work
is to classify each image into one of six facial emotion classes. The proposed
model is based on single Deep Convolutional Neural Networks (DNNs), which
contain convolution layers and deep residual blocks. In the proposed model,
firstly the image label to all faces has been set for the training. Secondly,
the images go through proposed DNN model. This model trained on two datasets
Extended Cohn?Kanade (CK+) and Japanese Female Facial Expression (JAFFE)
Dataset. The overall results show that, the proposed DNN model can outperform
the recent state-of-the-art approaches for emotion recognition. Even the
proposed model has accuracy improvement in comparison with our previous model.
* Previous article in issue
* Next article in issue
## Keywords
Facial emotion recognition
Deep neural network
Fully convolution network
## 1\. Introduction
Facial expression is one of the most important features of human emotion
recognition. As persons we can assume the impression of someone's emotions by
observing their face. Facial emotion recognition has several applications in
computer vision, non-verbal human behavior and human-computer interaction. It
is a challenging task due to several issues for example similarity of actions,
large head poses and etc. Recently, Facial emotion recognition in images has
attracted growing attention [5], which is for instances more complicated due
to the backgrounds and low-resolution faces. Emotion refers to the feeling,
energy, and dispositional effects. The goal of this work is to classify the
facial emotion as sad, angry, happy, surprise, fear, disgust, and neutral.
Facial expression is one of the most powerful, natural and common signals for
human beings to convey their emotional states and intentions.
It is computationally complex and challenging to achieve high recognition
rates using conventional feature extraction and classification schemes. This
paper proposed a new deep learning model for the emotional recognition, to
classify facial emotion from the images. Several techniques have been
established in this regards, but, most current works are appeared focusing on
hand-engineered features [2], [3], [10]. Currently, due to the size and
variety of datasets, deep neural network is the most appropriate techniques in
all the image processing and computer visions tasks [4], [5], [8]. Deep
conventional networks have the ability to simply handle spatial image [6],
[16], [17]. The main commitment of this work is to propose a deep neural
network model which contains several convolution layers and deep residual
blocks for facial emotion recognition [14], [15]. The proposed model is able
to learn the subtle features that discriminate the six different facial
expressions [9], [11], [13]. The rest of the paper is organized as follow:
next section delivers the related works. Section 3 presents the proposed
network. The results and experiments are presented in Section 4. At the end,
we have concluded our observation in Section 5.
### 2\. Related work
The researches in the area of facial emotion detection focused on recognizing
human emotion based on image or video records. Some recent work pursue to
recognize faces in images or video records [20], however, these approaches did
not use neural network strategy to extract features from the images.
Based on the features extracted for the recognition, we can differentiate two
fundamental methodologies: geometric based approaches and appearanceapproaches. For the first scenario, the model focus on limiting and tracking
specific facial standards, in order to train the model to classify based on
relevant positions of these standards. In [26] the authors proposed a model to
track a set of points to classify emotions from sequences. In the similar way
[24] the authors applied a transformation on a reduced subset of 117 landmarks
for emotion recognition. In form based emotion recognition, a set of features
is extracted from pixel images to train the classification model. Ko [27]
presented a review article which focuses on recent hybrid deep-learning
models. Krestinskaya and James [28], proposed an emotion recognition model
based on min-max similarity which reduces the problem of interclass pixel
mismatching while classification. This paper analyzes the strengths and the
limitations of systems based only on facial expressions or acoustic
information. It also discusses two approaches used to fuse these two
modalities: decision level and feature level integration.
Busso et al. [29] evaluate the strengths and the boundaries of models based on
facial expressions or acoustic material. In addition, the authors discussed
two models that used to fuse decision level and feature level integration.
Lopes et al. [31] proposed a simple way for facial expression recognition by
combining Convolutional Neural Network and some image pre-processing methods.
Regardless of the current progresses, still emotion recognition remains an
open problem for the computer vision community. EmotionNet is the largest
Challenge in terms of data available, with more than 1 million images (2000
labeled emotions and 950 K unlabeled samples). The first version of the
challenge determined that non-frontal faces still pose main problem for the
classification algorithms. So, the recognition rates decrease as a function of
pitch and yaw rotations [5].
For the last couple of years, Convolutional Neural Networks (CNNs) is one of
the most popular approaches in the computer vison. For example, C. Szegedy et
al. [20] proposed a model called as GoogLeNet which has numerous convolution
layers and this model had novelty due to usage of inception blocks. In every
inception block there are several convolution layers which are individually
connected to each other and at the last stage the results of convolution
layers concatenated and pass to the next convolution layer out of inception
block.
## 3\. Datasets and pre-processing
### 3.1. Dataset details
To train a neural network, a huge amount of labeled data is required to handle
the curse of dimensionality [5]. There are several facial expressions datasets
are publicly available such as Cohn?Kanade (CK+) [7] and Japanese Female
Facial Expression (JAFFE) datasets and were used in this paper. A big part of
datasets with emotion labels have facial pose expressions that were recorded
in a precise environment, with head pose and constant lighting. The models
that trained on this kind of dataset mostly have low performance on the data
which have different conditions, particularly in the live and outdoor
environment. We performed training on non-similar datasets to have better
generalization on the performance.
The CK dataset involves of 486 video sequences from 97 posers, in CK+ the
number of sequences has been extended by 22%. The data has been scaled to 640
× 480 or 640 × 490 pixels and all recorded in grayscale. Entire images were
captured with the equal lighting and poses. The dataset is quite appropriate
to use for all the models of feature extraction. The JAFFE dataset holds 213
images in total of seven facial expressions (six basic facial expressions and
one neutral) posed by ten Japanese female models. The images are in size of
256 × 256 pixels and the expresser have 2?4 samples for every expression. Thesamples presented in Fig. 1.
1. Download: Download high-res image (137KB)
2. Download: Download full-size image
Fig. 1. Sample of dataset for four type of emotion (Sur, Ang, Hap, Neu).
The experimental dataset contains in total 8363 images. 8200 images are used
for training, and the rest of images for testing and validation. The dataset
details are in Table 1.
Table 1. Dataset description.
Base| Number of images
---|---
Training| CK+| 8000
| JAFFE| 200
Testing| CK+| 150
| JAFFE| 13
### 3.2. Data pre-processing
Image normalization is important pre-processing technique to decrease the
inner-class feature mismatch which could be observed as intensity offsets. As
the intensity offsets are constant in the local region, Gaussian normalization
and standard deviation has been used. The input image is denoted as x(p, q),
and y(p, q) is the normalized output image, while p and q are the row and
column number of the processed image. The normalized resulted image is
computed by Eq. (1) [19], where µ is a local mean and ? is a local standard
deviation calculated over a window of M × M size
[12].(1)?(?,?)=?(?,?)??(?,?)6?(?,?)
The proposed model has the ability to handle different image sizes without
human intermediation. The cropping section is delimited by a vertical factor
of 4.7 (considering 1.4 for the above the eyes area and 3.3 for the area
below) useful to the gap between the eyes middle point and the center of right
eye. The horizontal cropping section is delimited by a factor of 2.5
functional to the similar distance. These factor values were determined
empirically. A sample of this process is presented in Fig. 2.
1. Download: Download high-res image (129KB)
2. Download: Download full-size image
Fig. 2. Image cropping example. This action aims to remove all non-expression
features, such as background and hair.
The image intensity and contrast can differ even in the same person images and
in the same expression, hence, increasing the variation in the feature vector
which could increase the complexity of the problem that the classifier has to
solve for each expression. To reduce these problems intensity normalization
been used. This technique adapted from [30], which named contrastive
equalization. In fact, the normalization process is a two-step scheme: firstly
subtract local contrast; next, uses divisive local contrast normalization.
Initially, the value of all pixels is subtracted from a Gaussian-weighted
average of its neighbors. Later, each pixel is divided by the standard
deviation of its own neighbor pixels. An example of this procedure is
presented in Fig. 3.
1. Download: Download high-res image (106KB)
2. Download: Download full-size image
Fig. 3. The intensity normalization. The original intensity image (left) and
its intensity normalized form (right).
Eqs. (2) and (3) are used to calculate the factors of µ and ? while a = (M ?
1)/2.(2)?(?,?)=1M2??=?????=????(?+?,?+?)(3)?(?,?)=1N2??=?????=???[?(?+?,?+?)??(?,?)]2
### 3.3. Feature detection
The feature parts that valuable for the facial emotion recognition areforehead, eyebrows, eyes, cheeks and mouth areas. Here, we show the feature
detection by computing the local deviation of already normalized image by a
window of N × N size. Eq. (4) used for the feature detection with b = (N ?
1)/2.(4)?(?,?)=1N2?K=????n=???[?(K+?,n+?)??3(?+?)]2
Parameter µ? In Eq. (4) denotes the mean of the normalized image y(p, q) and
can be computed by Eq. (5).(5)?(?,?)=1N2??=?????=???[?(?+?,?+?)??(?+?)]2
In the proposed model the convolution layers extracts features hierarchically,
and the fully-connected layer and the softmax layer used for indicating 6
expression classes. The input of the network is 128 × 96 × k for all patches,
where k = 3 for color patches and k = 1 for gray patches. The final output is
one of the 6 expression classes.
## 4\. Proposed model
In the following subsections the details of proposed model presented.
### 4.1. Convolution neural network architecture
Firstly, we introduce the use of face detector.
For the face detection we proposed a deep convolution neural network as shown
in Fig. 4 which contains six convolution layers (the green blocks) and two
blocks of deep residual learning (purple blocks) the details presented in the
next section. Right after each convolution layer there is a max pooling layer
(orange blocks). The deep residual blocks implemented after 2nd and 4th
convolution layer. There are also 2 Fully Connected layers (FC), each with a
ReLU activation function, and dropout for training [18], [22], [23].
1. Download: Download high-res image (221KB)
2. Download: Download full-size image
Fig. 4. Architecture of the proposed DNN model.
Finally we used Softmax for the classification [21]. The details of proposed
network presented in Table 2.
Table 2. Details of proposed network.
Type| Filter size / stride| Output size
---|---|---
Conv1| 5 × 5 / 2| 64 × 48 × 32
Maxpool1| 2 × 2 / 2| 32 × 24 × 32
Conv2| 3 × 3 / 1| 32 × 24 × 64
Res1| 4 Conv| ?
Conv3| 3 × 3 / 1| 32 × 24 × 128
Maxpool2| 2 × 2 / 2| 16 × 12 × 128
Conv4| 3 × 3 / 1| 16 × 12 × 128
Res2| 4 Conv| ?
Conv5| 3 × 3 / 1| 16 × 12 × 256
Maxpool3| 2 × 2 / 2| 8 × 6 × 256
Conv6| 3 × 3 / 1| 8 × 6 × 512
FC1| 1024| 1024
FC2| 512| 512
Furthermore, we performed regularization for each weight matrix _W_ that
limits the size of the weights at individual layer by adding a term to the
loss equal to a fixed hyper parameter. We explain these in Eq. (1), where _x_
be the output of a particular neuron in the network and _p_ the dropout
possibility.
The DNN is used for feature extraction and we just used the extra dataset for
the training.
### 4.2. Residual network
In the proposed model we implemented 2 residual blocks. Each residual block
has four convolution layers two short connections and one skip connection. The
first convolution layer in residual block has size of 1 × 1 × 64. The secondone has size of 3 × 3 × 64. The third one has size of 3 × 3 × 128 and the last
one has size of 1 × 1 × 256.
### 4.3. Regression DNN
Firstly we used a single DNN model to train the datasets. At each time trained
a single image, the corresponding image passed through the DNN model, the
details of the model shown in Fig. 4.
Two fully-connected layers with 250 hidden units for the approximation of the
valence label have been used. For the cost function the mean squared error has
been used. For the network training stochastic gradient descent while the
batch size sets to 64 and the weight decay sets to 1E-5. Moreover, the
learning rate at the beginning sets to 4e-2 which decrees by 0.005 every 10
epochs.
## 5\. Experiment and evaluation
For all the pre-training settings described in this paper, global average
pooling at the last layer has been applied to decrease spatial dimensionality
of data before passing to the fully connected layers. In addition, batch
normalization is used to improve generalization and optimization.
For the data preprocessing, we initially identify the face in every outline
utilizing face and point of interest finder. Then map the distinguished
landmark points to characterized pixel areas in a request to guarantee
correspondence concerning outlines. After the normalization the forehead,
eyes, nose, mouth while processing each face image through the subtraction and
contrast normalization. For tested the proposed DNN model we used a
workstation with Intel(R) Core(TM) i7-8700 K and 32GB memory.
Fig. 5 shows the prediction accuracy of the proposed DNN model for training
vas validation on the CK+ dataset. These charts clearly show the smooth
performance of the proposed model.
1. Download: Download high-res image (246KB)
2. Download: Download full-size image
Fig. 5. Loss and the prediction accuracy for the proposed model.
Fig. 6 presents the Roc curve and the Precision-Recall curve of the proposed
model. As it is visible the proposed model has the ability to with the least
number of errors and high performance for the face emotion recognition.
1. Download: Download high-res image (175KB)
2. Download: Download full-size image
Fig. 6. Roc and the precision-recall curve.
The confusion matrices of Proposed DNN model on the JAFFE dataset presented in
Fig. 7 and its performance on CK+ dataset presented in Fig. 8. The proposed
model emotion recognition can reach to 95%. As it is visible in the Figs. 7
and 8 the best recognition are for the Happy, Angry, Neutral, and Surprise
emotions.
1. Download: Download high-res image (266KB)
2. Download: Download full-size image
Fig. 7. Confusion matrices on JAFFE Datasets.
1. Download: Download high-res image (261KB)
2. Download: Download full-size image
Fig. 8. Confusion matrices on CK+ Datasets.
Table 3 illustrated the overall performance of proposed DNN model as compared
to other deep learning recent models on the CK+ and JAFFE datasets. The
proposed DNN model gain higher results in comparison with five other models
[[1], [24], [25], and [26]].
Table 3. Proposed model versus other models Performance Comparison on JAFFE
and MMI dataset.
Method| Accuracy JAFFE| Accuracy CK+---|---|---
Zhang et al.[24]| 94.89%| 92.35%
Khorrami et al. [25]| 82.43%| 81.48%
Chernykh et al. [26]| 73%| 70.12%
Jain et al. [1]| 94.91%| 92.71%
Krestinskaya and James [28]| 94.89%| 92.74%
Lopes et al. [31]| 94.86%| 92.73%
Proposed model| **95.23%**| **93.24%**
Our recent approach has better performance in emotion recognition in
comparison with our previous model; in Table 3 we present the performance of
recent approaches on the whole JAFFE and CK+ dataset.
As the results shows the proposed model on the JAFFE dataset has 0.32%, 0.34%
better performance as compared to Jain et al. [1] and Zhang et al. [24]
respectively. On the CK+ dataset the proposed model has 0.53% better
performance in comparison with Jain et al. [1] and 0.89% better performance
while comparing with Zhang et al. [24] model.
## 6\. Conclusion
In this paper, we present a fully deep neural network model for facial emotion
recognition and the model has been tested on two public datasets to assess the
performance of the proposed model. Particularly, it has been found that the
combination of FCN and residual block cloud considerably improve the overall
result, which verified the efficiency of the proposed model.
Special issue articlesRecommended articles"
99,101,"FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation","['NC Ebner', 'M Riediger', 'U Lindenberger']",2010,1382,Karolinska Directed Emotional Faces,FER,"to neutral faces (Isaacowitz et al., 2006). These studies have almost exclusively used  emotional faces of young individuals and have not systematically varied the age of the face, even",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.42.1.351,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
100,102,"Face behavior a la carte: Expressions, affect and action units in a single network","['D Kollias', 'V Sharmanska', 'S Zafeiriou']",2019,206,Acted Facial Expressions In The Wild,neural network,manifestation of complex facial expressions. The dataset  We train a multi-task neural  network model to jointly perform ( It served as benchmark for the ABAW Competition organized,No DOI,arXiv preprint arXiv:1910.11111,https://arxiv.org/abs/1910.11111,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
101,103,Face detection using mixtures of linear subspaces,"['MH Yang', 'N Abuja', 'D Kriegman']",2000,121,Toronto Face Database,classification,"in face recognition [2]. The reason for this is that FLD provides a better projection than PCA  for pattern classification. In the second proposed method, we decompose the training face",No DOI,… on Automatic Face and …,https://ieeexplore.ieee.org/document/840614,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
102,104,Face expression recognition with a 2-channel convolutional neural network,"['D Hamester', 'P Barros', 'S Wermter']",2015,164,Toronto Face Database,"facial expression recognition, neural network","CNN and train it with the Acted Facial Expression in the Wild (AFEW) dataset [17]. In the sec   with the Toronto Face Dataset. After that, a softmax layer is trained with the AFEW dataset.",No DOI,2015 international joint …,https://ieeexplore.ieee.org/document/7280539,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
103,105,Face recognition using kernel direct discriminant analysis algorithms,"['J Lu', 'KN Plataniotis']",2003,836,Toronto Face Database,"classification, classifier",") [20] have in pattern regression and classification tasks, we propose a new kernel discriminant  analysis algorithm for face recognition. The algorithm generalizes the strengths of the",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/1176132,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,
104,106,Face recognition with radial basis function (RBF) neural networks,"['MJ Er', 'S Wu', 'J Lu', 'HL Toh']",2002,975,Toronto Face Database,"classification, classifier, neural network",excellent performance both in terms of error rates of classification and learning efficiency.   Database Our experiments were performed on the face database which contains a set of face,No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1000134,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
105,107,Face recognition: A convolutional neural-network approach,"['S Lawrence', 'CL Giles', 'AC Tsoi']",1997,4525,Toronto Face Database,"classification, neural network",We are interested in rapid classification and hence we do not assume that time is  available for extensive preprocessing and normalization. Good algorithms for locating,No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/554195,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
106,108,Facenet2expnet: Regularizing a deep face recognition net for expression recognition,"['H Ding', 'SK Zhou', 'R Chellappa']",2017,489,Toronto Face Database,"CNN, classification, deep learning, facial expression recognition, machine learning, neural network","adapt to the new domain task (facial expression recognition), we attach the fully- databases:  CK+ [27], Oulu-CASIA [28], Toronto Face Database (TFD) [29] and Static Facial Expression",No DOI,… face & gesture recognition (FG …,https://arxiv.org/abs/1609.06591,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
107,109,Faces of pain: automated measurement of spontaneousallfacial expressions of genuine and posed pain,"['GC Littlewort', 'MS Bartlett', 'K Lee']",2007,232,Toronto Face Database,machine learning,"machine learning approach, previously used successfully to categorize basic emotional facial   Here we applied machine learning on a 20-channel output stream of facial action detectors",No DOI,… of the 9th international conference on …,https://www.researchgate.net/publication/221052396_Faces_of_pain_automated_measurement_of_spontaneousallfacial_expressions_of_genuine_and_posed_pain,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
108,110,Facial Affect``In-The-Wild,"['S Zafeiriou', 'A Papaioannou', 'I Kotsia']",2016,122,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","of compound expressions [20]). A recent survey on facial expression recognition can be   In particular, the network exploits the fact that facial expressions can be decomposed to FAU.",No DOI,… Pattern Recognition …,https://openaccess.thecvf.com/content_cvpr_2016_workshops/w28/papers/Zafeiriou_Facial_Affect_In-The-Wild_CVPR_2016_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
109,111,Facial emotion detection using deep learning,"['A Jaiswal', 'AK Raju', 'S Deb']",2020,150,Japanese Female Facial Expression,deep learning,"basically three main steps: face detection, features extraction,  a convolutional neural networks  (CNN) based deep learning  the Japanese Female Face Expression (JAFFE) [15], Facial",No DOI,2020 international conference for …,https://ieeexplore.ieee.org/document/9154121,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
110,112,Facial emotion processing in borderline personality disorder: a systematic review and meta-analysis,"['AE Mitchell', 'GL Dickens', 'MM Picchioni']",2014,220,Karolinska Directed Emotional Faces,FER,A body of work has developed over the last 20 years that explores facial emotion perception  in Borderline Personality Disorder (BPD). We identified 25 behavioural and functional,No DOI,Neuropsychology review,https://pubmed.ncbi.nlm.nih.gov/24574071/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
111,113,Facial emotion recognition using convolutional neural networks (FERC),['N Mehendale'],2020,328,Extended Cohn-Kanade,"FER, neural network","FERC was extensively tested with more than 750K images using extended Cohn–Kanade  expression, Caltech faces, CMU and NIST datasets. We expect the FERC emotion detection",No DOI,SN Applied Sciences,https://link.springer.com/article/10.1007/s42452-020-2234-1,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
112,114,Facial expression analysis under partial occlusion: A survey,"['L Zhang', 'B Verma', 'D Tjondronegoro']",2018,121,Toronto Face Database,facial expression recognition,"While face and facial expression recognition systems  database includes four typical complex  facial expressions: smile while hands obscure the face, anger while hand obscure the face",No DOI,ACM Computing Surveys …,https://arxiv.org/abs/1802.08784,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
113,115,Facial expression analysis with AFFDEX and FACET: A validation study,"['S Stöckli', 'M Schulte-Mecklenbeck', 'S Borer']",2018,309,"Affective Faces Database, Radboud Faces Database","classification, deep learning, facial expression recognition, machine learning, neural network",Picture Database (GAPED) and the Radboud Faces Database (RaFD) facial expression  analysis assumes that there is a direct link between emotion production and emotion recognition,No DOI,Behavior research …,https://pubmed.ncbi.nlm.nih.gov/29218587/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
114,116,"Facial expression classification based on SVM, KNN and MLP classifiers","['HI Dino', 'MB Abdulrazzaq']",2019,160,"Extended Cohn-Kanade, MMI Facial Expression","classification, classifier",The Extended Cohn-Kanade (CK+) dataset used as good data recourse to exam the  classification of human Facial Expression. Principal component analysis (PCA) used to reduce the,No DOI,2019 International Conference on …,http://ieeexplore.ieee.org/document/8723728/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
115,117,Facial expression recognition,"['Y Tian', 'T Kanade', 'JF Cohn']",2011,255,"Affective Faces Database, Binghamton University 3D Facial Expression, Extended Cohn-Kanade, Japanese Female Facial Expression",facial expression recognition,"This chapter introduces recent advances in facial expression analysis and recognition. The first part discusses general structure of AFEA systems. The second part describes the problem space for facial expression analysis. This space includes multiple dimensions: level of description, individual differences in subjects, transitions among expressions, intensity of facial expression, deliberate versus spontaneous expression, head orientation and scene complexity, image acquisition and resolution, reliability of ground truth, databases, and the",No DOI,Handbook of face recognition,https://paperswithcode.com/task/facial-expression-recognition,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
116,118,Facial expression recognition ability among women with borderline personality disorder: implications for emotion regulation?,"['AW Wagner', 'MM Linehan']",1999,459,Japanese Female Facial Expression,facial expression recognition,"examined recognition of facial expressions of emo tion among women  21), compared to a  group of women with histories of  Japanese males, 2 Japanese females) for each of 7 different",No DOI,Journal of personality disorders,https://pubmed.ncbi.nlm.nih.gov/10633314/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
117,119,Facial expression recognition and histograms of oriented gradients: a comprehensive study,"['P Carcagnì', 'M Del Coco', 'M Leo', 'C Distante']",2015,214,Radboud Faces Database,FER,"Automatic facial expression recognition (FER) is a topic of  (HOG) descriptor in the FER  problem, highlighting as this  with most commonly used FER frameworks was carried out. In the",No DOI,SpringerPlus,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4628009/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
118,120,Facial expression recognition based on 3D dynamic range model sequences,"['Y Sun', 'L Yin']",2008,172,Binghamton University 3D Facial Expression,"FER, facial expression recognition","3D facial expression database [21], we extend the facial expression analysis to a dynamic  3D  In this paper, we propose a spatio-temporal 3D facial expression analysis approach for",No DOI,Computer Vision–ECCV 2008: 10th European …,https://link.springer.com/chapter/10.1007/978-3-540-88688-4_5,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
119,121,Facial expression recognition based on facial components detection and hog features,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2014,133,Extended Cohn-Kanade,"classification, classifier",perform the facial expression classification. We evaluate our proposed method on the JAFFE  dataset and an extended Cohn-Kanade dataset. The average classification rate on the two,No DOI,International workshops on …,http://www.cedus.it/documents/SicurezzaUrbana/Videosorveglianza_e_altre_tecnologie_video_di_controllo/3ZChi_ACV-1.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
120,122,Facial expression recognition by de-expression residue learning,"['H Yang', 'U Ciftci', 'L Yin']",2018,512,Oulu-CASIA,classification,that we target to exploit for expression classification.  The Oulu-CASIA database [33]  contains data captured under three  The Oulu-CASIA VIS has 480 video sequences taken from,No DOI,… of the IEEE conference on computer …,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Facial_Expression_Recognition_CVPR_2018_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
121,123,Facial expression recognition from world wild web,"['A Mollahosseini', 'B Hasani', 'MJ Salvador']",2016,103,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",networks can recognize wild facial expressions with an  of inthe-wild facial expressions by  querying different search en search engines for facial expression recognition. We trained two,No DOI,… pattern recognition …,https://arxiv.org/abs/1605.03639,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
122,124,Facial expression recognition in JAFFE dataset based on Gaussian process classification,"['F Cheng', 'J Yu', 'H Xiong']",2010,134,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","Here we propose a GP model and investigate it for the facial expression recognition in the  Japanese female facial expression dataset. By the strategy of leave-one-out cross validation,",No DOI,IEEE Transactions on Neural …,http://ieeexplore.ieee.org/document/5551215/,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
123,125,Facial expression recognition in image sequences using geometric deformation features and support vector machines,"['I Kotsia', 'I Pitas']",2006,900,Toronto Face Database,facial expression recognition,tion problems used for FAUs detection in the grid and for facial expression recognition.   The Cohn–Kanade database [2] was used for the facial expression recognition in six basic,No DOI,IEEE transactions on image processing,https://ieeexplore.ieee.org/document/4032815,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
124,126,Facial expression recognition in the wild via deep attentive center loss,"['AH Farzaneh', 'X Qi']",2021,289,Expression in-the-Wild,"CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","Accordingly, for the task of FER in the wild, where  Face Database (RAFDB) [14]. Then, we  conduct extensive experiments on these two widely used wild Facial Expression Recognition (",No DOI,Proceedings of the IEEE/CVF winter …,https://openaccess.thecvf.com/content/WACV2021/papers/Farzaneh_Facial_Expression_Recognition_in_the_Wild_via_Deep_Attentive_Center_WACV_2021_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
125,127,Facial expression recognition in video with multiple feature fusion,"['J Chen', 'Z Chen', 'Z Chi', 'H Fu']",2016,246,Acted Facial Expressions In The Wild,"classification, classifier, facial expression recognition",Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in  dealing with video-based facial expression recognition  streams of facial expressions analysis,No DOI,IEEE Transactions on Affective …,https://ieeexplore.ieee.org/document/7518582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
126,128,Facial expression recognition using computer vision: A systematic review,"['D Canedo', 'AJR Neves']",2019,164,"Affective Faces Database, Binghamton University 3D Facial Expression, Radboud Faces Database","CNN, FER, facial expression recognition","Facial expressions are the main focus of this systematic review. Generally, an FER system  consists  Binghamton University 3D Facial Expression database (BU-3DFE) [52]: contains 606",No DOI,Applied Sciences,https://www.mdpi.com/2076-3417/9/21/4678,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
127,129,Facial expression recognition using deep convolutional neural networks,"['DV Sang', 'N Van Dat']",2017,102,Toronto Face Database,"facial expression recognition, neural network","different effective CNNs to tackle the problem of facial expression recognition. Since  FERC-2013 dataset is much smaller than ImageNet dataset, we propose some changes to avoid",No DOI,2017 9th International Conference on …,https://ieeexplore.ieee.org/document/8119447,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
128,130,Facial expression recognition using enhanced deep 3D convolutional neural networks,"['B Hasani', 'MH Mahoor']",2017,337,MMI Facial Expression,neural network,"facial expressions in a sequence (Figure 1). We evaluate our proposed method using four  well-known facial expression databases (CK+, MMI in recognition of facial expressions in cross",No DOI,… of the IEEE conference on computer …,https://arxiv.org/abs/1705.07871,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
129,131,Facial expression recognition using facial movement features,"['L Zhang', 'D Tjondronegoro']",2011,314,"Affective Faces Database, Japanese Female Facial Expression","FER, facial expression recognition","FER that considers facial movement features. In this paper, we aim for improving the  performance of FER by automatically capturing facial  on the Japanese female facial expression (",No DOI,IEEE transactions on affective …,https://ieeexplore.ieee.org/document/5871583,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
130,132,Facial expression recognition using hierarchical features with deep comprehensive multipatches aggregation convolutional neural networks,"['S Xie', 'H Hu']",2018,213,Japanese Female Facial Expression,"FER, neural network",on Facial Expression Recognition (FER) develop quickly as well. Applications based on FER   213 images of 7 facial expressions posed by 10 Japanese females. 183 images with six,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/8371638,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
131,133,Facial expression recognition using kernel canonical correlation analysis (KCCA),"['W Zheng', 'X Zhou', 'C Zou', 'L Zhao']",2006,358,Japanese Female Facial Expression,"FER, classification","We will use the Japanese female facial expression (JAFFE) database [6]–[8] and Ekman’s  “Pictures of Facial Affect” database [22], respectively, to conduct the FER based on KCCA. In",No DOI,IEEE transactions on neural …,https://ieeexplore.ieee.org/document/1593706,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
132,134,Facial expression recognition using local gravitational force descriptor-based deep convolution neural networks,"['K Mohan', 'A Seal', 'O Krejcar']",2020,145,Japanese Female Facial Expression,neural network,"facial expression of a person. The proposed method consists of two parts. The former  one finds out local features from face  Budynek, “The Japanese female facial expression (JAFFE)",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/9226437,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
133,135,Facial expression recognition via deep learning,"['X Zhao', 'X Shi', 'S Zhang']",2015,270,"Acted Facial Expressions In The Wild, Extended Cohn-Kanade, Japanese Female Facial Expression, Radboud Faces Database","CNN, deep learning, facial expression recognition","Experimental results on two benchmarking facial expression databases, ie, the JAFFE  database and the Cohn-Kanade database, demonstrate the promising performance of the",No DOI,IETE technical review,https://ieeexplore.ieee.org/document/8308363,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
134,136,Facial expression recognition with FRR‐CNN,"['S Xie', 'H Hu']",2017,108,Japanese Female Facial Expression,CNN,"Japanese Female Facial Expression (JAFFE) database. CK+ database is a public expression   As for JAFFE, it consists of 213 expressional images of ten Japanese female subjects. We",No DOI,Electronics Letters,https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2016.4328,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
135,137,Facial expression recognition with convolutional neural networks: coping with few data and the training sample order,"['AT Lopes', 'E De Aguiar', 'AF De Souza']",2017,923,Binghamton University 3D Facial Expression,"CNN, neural network","Hence, facial expression recognition is still a challenging problem in computer vision. In this   for facial expression recognition that uses a combination of Convolutional Neural Network",No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320316301753,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Facial expression recognition with Convolutional Neural
Networks: Coping with few data and the training sample order
Search 
## Outline
1. 2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Facial expression recognition system
8. 4\. Experiments and discussions
9. 5\. Conclusion
10. Conflict of interest
11. Acknowledgment
12. Appendix A.
13. References
14. Vitae
Show full outline
## Cited by (674)
## Figures (12)
1. 2. 3. 4. 5. 6.
Show 6 more figures
## Tables (20)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Pattern Recognition
Volume 61, January 2017, Pages 610-628
# Facial expression recognition with Convolutional Neural Networks: Coping
with few data and the training sample order
Author links open overlay panelAndré Teixeira Lopes a, Edilson de Aguiar b,
Alberto F. De Souza a, Thiago Oliveira-Santos a
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2016.07.026Get rights and content### Highlights
* ?
A CNN based approach for facial expression recognition.
* ?
A set of pre-processing steps allowing for a simpler CNN architecture.
* ?
A study of the impact of each pre-processing step in the accuracy.
* ?
A study for lowering the impact of the sample presentation order during
training.
* ?
High facial expression recognition accuracy (96.76%) with real time
evaluation.
## Abstract
Facial expression recognition has been an active research area in the past 10
years, with growing application areas including avatar animation,
neuromarketing and sociable robots. The recognition of facial expressions is
not an easy problem for machine learning methods, since people can vary
significantly in the way they show their expressions. Even images of the same
person in the same facial expression can vary in brightness, background and
pose, and these variations are emphasized if considering different subjects
(because of variations in shape, ethnicity among others). Although facial
expression recognition is very studied in the literature, few works perform
fair evaluation avoiding mixing subjects while training and testing the
proposed algorithms. Hence, facial expression recognition is still a
challenging problem in computer vision. In this work, we propose a simple
solution for facial expression recognition that uses a combination of
Convolutional Neural Network and specific image pre-processing steps.
Convolutional Neural Networks achieve better accuracy with big data. However,
there are no publicly available datasets with sufficient data for facial
expression recognition with deep architectures. Therefore, to tackle the
problem, we apply some pre-processing techniques to extract only expression
specific features from a face image and explore the presentation order of the
samples during training. The experiments employed to evaluate our technique
were carried out using three largely used public databases (CK+, JAFFE and
BU-3DFE). A study of the impact of each image pre-processing operation in the
accuracy rate is presented. The proposed method: achieves competitive results
when compared with other facial expression recognition methods ? 96.76% of
accuracy in the CK+ database ? it is fast to train, and it allows for real
time facial expression recognition with standard computers.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
Convolutional Neural Networks
Computer vision
Machine learning
Expression specific features
## 1\. Introduction
Facial expression is one of the most important features of human emotion
recognition [1]. It was introduced as a research field by Darwin in his book
?The Expression of the Emotions in Man and Animals' [2]. According to Li and
Jain [3], it can be defined as the facial changes in response to a person's
internal emotional state, intentions, or social communication. Nowadays,automated facial expression recognition has a large variety of applications,
such as data-driven animation, neuromarketing, interactive games, sociable
robotics and many other human?computer interaction systems.
Expression recognition is a task that humans perform daily and effortlessly
[3], but it is not yet easily performed by computers, despite recent methods
have presented with accuracies larger than 95% in some conditions (frontal
face, controlled environments, and high-resolution images). Many works in the
literature do not perform a consistent evaluation methodology (e.g. without
subject overlap in training and testing) and therefore present a misleading
high-accuracy, but do not represent most of the face expression recognition
problems real scenarios. On the other hand, low accuracy has been reported on
databases with uncontrolled environments and in cross-database evaluations.
Trying to cope with these limitations, several research works have tried to
make computers reach the same accuracy of humans, and some examples of these
works are highlighted below. This problem is still a challenge for computers
because it is very hard to separate the expressions? feature space, i.e.
facial features from one subject in two different expressions may be very
close in the feature space, while facial features from two subjects with the
same expression may be very far from each other. In addition, some expressions
like ?sad? and ?fear?, for example, are, in some cases, very similar.
Fig. 1 shows three subjects with a happy expression. As it can be seen in the
figure, the images vary a lot from each other not only in the way that the
subjects show their expression, but also in lighting, brightness, pose and
background. This figure also exemplifies another challenge related to the
facial expression recognition that is the uncontrolled training?testing
scenarios (training images can be very different in terms of environmental
conditions and subject ethnicity from the testing images). One approach to
evaluate the facial expression recognition under these scenarios is to train
the method with one database and to test it with another (possibly from
different ethnic groups). We present results following this approach.
1. Download: Download high-res image (347KB)
2. Download: Download full-size image
Fig. 1. Three different subjects with the happy expression. As it can be seen,
the images vary a lot from each other not only in the way that the subjects
show their expression but also in light, brightness, position and background.
The images are from the following databases: CK+ database [4], JAFFE database
[5] and BU-3DFE database [6], in this order.
Facial expression recognition systems can be divided into two main categories:
those that work with static images [7], [8], [9], [10], [11], [12], [13] and
those that work with dynamic image sequences [14], [15], [16], [17]. Static-
based methods do not use temporal information, i.e. the feature vector
comprises information about the current input image only. Sequence based
methods, in the other hand, use temporal information of images to recognize
the expression captured from one or more frames. Automated systems for facial
expression recognition receive the expected input (static image or image
sequence) and typically give as output one of six basic expressions (anger,
sad, surprise, happy, disgust and fear, for example); some systems also
recognize the neutral expression. This work will focus on methods based on
static images and it will consider the six and seven expressions sets (six
basic plus neutral), for controlled and uncontrolled scenarios.
As described by Li and Jain [3], automatic facial expression analysis
comprises three steps: face acquisition, facial data extraction and
representation, and facial expression recognition. Face acquisition can be
split in two major steps: face detection [18], [19], [20], [21] and head poseestimation [22], [23], [24]. After the face acquisition, the facial changes
caused by facial expressions need to be extracted. These changes are usually
extracted using geometric feature-based methods [25], [26], [27], [21] or
appearance-based methods [8], [9], [10], [11], [13], [25], [28]. The extracted
features are often represented in vectors, referred as feature vectors.
Geometric feature-based methods work with shape and location of facial
components like mouth, eyes, nose and eyebrows. The feature vector that
represents the face geometry is composed of facial components or facial
feature points. Appearance-based methods work with feature vectors extracted
from the whole face, or from specific regions; these feature vectors are
acquired using image filters applied to the whole face image [3].
Once feature vectors related to the facial expression are available,
expression recognition can be performed. According to Liu et al. [7],
expression recognition systems basically use a three-stage training procedure:
feature learning, feature selection and classifier construction, in this
order. The feature learning stage is responsible for the extraction of all
features related to the facial expression. The feature selection selects the
best features to represent the facial expression. They should minimize the
intra-class variation of expressions while maximizing the inter-class
variation [8]. Minimizing the intra-class variation of expressions is a
problem because images of different individuals with the same expression are
far from each other in the pixel's space. Maximizing the inter-class variation
is also difficult because images of the same person in different expressions
may be very close to one another in the pixel's space [29]. At the end of the
whole process, a classifier (or a set of classifiers, with one for each
expression) is used to infer the facial expression, given the selected
features.
One of the techniques that has been successfully applied to the facial
expression recognition problem was the deep multi-layer neural network [30],
[31], [32], [14], [10], [11], [13]. This technique comprises the three steps
of facial expression recognition (learning and selection of features and
classification) in one single step. In the last decade, neural network
researches were motivated to find a way to train deep multi-layer neural
networks (i.e. networks with more than one or two hidden layers) in order to
increase their accuracy [33], [34]. According to Bengio [35], until 2006, many
new attempts have shown little success. Although somewhat old, the
Convolutional Neural Networks (CNNs) proposed in 1998 by Lecun et al. [36] has
shown to be very effective in learning features with a high level of
abstraction when using deeper architectures (i.e. with a lot of layers) and
new training techniques. In general, this type of hierarchical network has
alternating types of layers, including convolutional layers, sub-sampling
layers and fully connected layers. Convolutional layers are characterized by
the kernel's size and the number of generated maps. The kernel is shifted over
the valid region of the input image generating one map. Sub-sampling layers
are used to increase the position invariance of the kernels by reducing the
map size [37]. The main types of sub-sampling layers are maximum-pooling and
average pooling [37]. Fully connected layers on CNN's are similar to the ones
in general neural networks, its neurons are fully connected with the previous
layer (generally: convolution layer, sub-sampling layer or even a fully
connected layer). The learning procedure of CNNs consists of finding the best
synapses? weights. Supervised learning can be performed using a gradient
descent method, like the one proposed by Lecun et al. [36]. One of the main
advantages of CNN, is that the models' input is a raw image rather than a set
of hand-coded features.Besides the methods using deep architecture, there are many others in the
literature, but some aspects of the evaluation of these methods still deserve
attention. For example, validation methods could be improved in [10], [30],
[38], [39], [40], [41], [42] in order to consider situations where the subject
in the test set is not in the training set (i.e. test without subject
overlap), accuracy is somewhat low in [38], [1], [43], [44], and the
recognition time in [7], [43], [16] could be improved so as to perform real
time evaluations.
Trying to cope with some of these limitations while keeping a simple solution,
in this paper, we present a deep learning approach combining standard methods,
like image normalizations, synthetic training-samples generation (for example,
real images with artificial rotations, translation and scaling) and
Convolutional Neural Network, into a simple solution that is able to achieve a
very high accuracy rate of 96.76% in the CK+ database for 6 expressions, which
is the state-of -the-art. The training time is significantly smaller if
compared with other methods in the literature and the whole facial expression
recognition system can operate in real time in standard computers. We have
examined the performance of our system using the Extensive Cohn?Kanade (CK+)
database [4], the Japanese Female Facial Expression (JAFFE) database [5] and
the Binghamton University 3D Facial Expression (BU-3DFE) database [6],
achieving a better accuracy in the CK+ database, which contains more samples
(important for deep learning techniques) than the JAFFE and BU-3DFE databases.
In addition, we have performed an extensive validation, with cross-database
tests (i.e. training the method using one database and evaluating its accuracy
using another one). In summary, the main contributions of this work are:
* i.
an efficient method for facial expression recognition that operates in real
time;
* ii.
a study of the effects of image pre-processing operations in the facial
expression recognition problem;
* iii.
a set of pre-processing operations for face normalization (spatial and
intensity) in order to decrease the need of controlled environments and to
cope with the lack of data;
* iv.
a study to handle the variability in the accuracy caused by the presentation
order of the samples during training; and
* v.
a study of the performance of the proposed system with different cultures and
environments (cross-database evaluation).
This work extends the one presented in the 28th SIBGRAPI (Conference on
Graphics, Patterns and Images) [45] as follows:
* i.
it presents a deeper literature review which were used to enlarge the
comparisons of the results;
* ii.
it presents the results using a new implementation based on a different
framework (from ConvNet, a Matlab based implementation [46], to Caffe, a C++
based implementation [47]), which consequently reduced the total training time
by almost a factor of four;
* iii.
it presents results showing a reduced recognition time, which is now real
time;* iv.
it presents results showing better accuracy due to longer training and small
changes (see below) in the method;
* v.
it includes improved experimental methodology, as for example, using training,
validation and test sets, instead of training and test sets only; and
* vi.
it presents a more complete evaluation including cross-database tests.
The changes in the method that allowed better accuracy were as follows:
* a.
The synthetic samples have a slight different generation process, allowing
larger variation among them (now synthetic samples can be the original image
rotated, scaled or translated, instead of only rotated);
* b.
We increased the number of synthetic samples, from 30 to 70 (motivated by the
previous item); and
* c.
The logistic regression loss function was replaced by a SoftmaxWithLoss
function (described in Section 3).
The remainder of this paper is organized as follows: the next section presents
the most recent related work, while Section 3 describes the proposed approach.
In Section 4, the experiments we have performed to evaluate our system are
presented and compared with several recent facial expression recognition
methods. Finally, we conclude in Section 5.
## 2\. Related work
Several facial expression recognition approaches were developed in the last
decades with an increasing progress in recognition performance. An important
part of this recent progress was achieved thanks to the emergence of deep
learning methods [7], [10], [12] and more specifically with Convolutional
Neural Networks [14], [11], which is one of the deep learning approaches.
These approaches became feasible due to: the larger amount of data available
nowadays to train learning methods and the advances in GPU technology. The
former is crucial for training networks with deep architectures, whereas the
latter is crucial for the low cost high-performance numerical computations
required for the training procedure. Surveys of the facial expression
recognition research can be found in [3], [48].
Some recent approaches for facial expression recognition have focused on
uncontrolled environments (e.g. not frontal face, images partially overlapped,
spontaneous expressions and others), which is still a challenging problem
[12], [49], [50]. This work will focus on more controlled environments and
evaluation among different ethnic groups, the latter is a more challenging
scenario in facial expression recognition. This section discusses recent
methods that achieve high accuracy in facial expression recognition using a
comparable experimental methodology (as explained in Section 4) or methods
that are based on deep neural networks.
Liu et al. [7] proposed a novel approach called boosted deep belief network
(BDBN). The BDBN is composed by a set of classifiers, named by the authors as
weak classifiers. Each weak classifier is responsible for classifying one
expression. Their approach performs the three learning stages (feature
learning, feature selection and classifier construction) iteratively in a
unique framework. Their experiments were conducted using two public databases
of static images, Cohn?Kanade [4] and JAFFE [51], and achieved an accuracy of
96.7% and 91.8%, respectively. They also performed experiments on less
controlled scenarios, using a cross-database configuration (training with theCK+ and testing in JAFFE) and achieved an accuracy of 68.0%. All images were
firstly preprocessed based on the given eye coordinates, i.e. performing
alignment and crop. The training and the test adopted a one-versus-all
classification strategy, i.e. they used a binary classifier for each
expression. The time required to train the network was about 8 days. The
recognition was calculated as a function of the weak classifiers. In their
method, they use six or seven classifiers, depending on the amount of
expressions to be recognized (one for each expression). Each classifier took
30 ms to recognize each expression, with a total recognition time of about
0.21 s. The recognition time was reported by the authors using a 6-core 2.4
GHz PC.
Song et al. [10], developed a facial expression recognition system that uses a
deep Convolutional Neural Network and runs on a smartphone. The proposed
network is composed of five layers and 65,000 neurons. According to the
authors, it is common to have an overfitting when using a small amount of
training data and such a big network. Therefore, the authors applied data
augmentation techniques to increase the amount of training data and used the
drop-out [52] during the network training. The experiments were performed
using the CK+ [4] dataset, and other three datasets created by the authors.
The images of the CK+ dataset were firstly cropped to focus on regions that
contains facial changes caused by an expression. The experiments performed by
the authors follow a 10-fold cross validation, but they do not mention if
there were images of the same subject in more than one fold. Therefore, we
assumed that there was an overlap among subjects in the training and test
sets. An accuracy of 99.2% was achieved in the CK+ database while recognizing
only five expressions (anger, happy, sad, surprise and neutral).
Burkert et al. [11] also proposed a method based on Convolutional Neural
Networks. The authors claim that their method is independent of any hand-
crafted feature extraction (i.e. uses the raw image as input). Their network
architecture consists of four parts. The first part is responsible for the
automatic data preprocessing, while the remaining parts carried out the
feature extraction process. The extracted features are classified into a given
expression by a fully connected layer at the end of the network. The proposed
architecture comprises 15 layers (7 convolutions, 5 poolings, 2 concatenations
and 1 normalization layer). They evaluated their method with the CK+ database
and the MMI database, achieving an accuracy of 99.6% and 98.63%, respectively.
Despite the high accuracy, in their experimental methodology they did not
guarantee that subjects used in training were not used in test. As will be
discussed in Section 4, this is an important restriction that should be
enforced in order to perform a fair evaluation of facial expression
recognition methods [44], [53].
Liu et al. [12] proposed an action unit (AU) inspired deep networks (AUDN) in
order to explore a psychological theory that expressions can be decomposed
into multiple facial expression action units. The authors claim that the
method is able to learn: (i) informative local appearance variation; (ii) an
optimal way to combine local variations; (iii) and a high-level representation
for the final expression recognition. Experiments were performed in the CK+
[4], MMI [54] and SFEW [55] datasets. The latter contains images captured from
various movies under uncontrolled scenarios, representing the real-world
environment. Experiments were performed using a cross-validation approach
without subject overlap between training and test groups, and evaluating the
six basic expressions. The method achieves an accuracy of 93.70% in the CK+
database, 75.85% in the MMI database and 30.14% in SFEW database.
Ali et al. [13] proposed a collection of boosted neural network ensembles formultiethnic facial expression recognition. The proposed model is composed by
three main steps: firstly a set of binary neural networks are trained,
secondly the predictions of these neural networks are combined to compose the
ensemble's collection and finally these collections are used to detect the
presence of an expression. The multicultural facial expression database was
created by the authors with images from three different databases, which
contains images from Japanese (JAFFE), Taiwanese (TFEID), Caucasians (RaFD),
and Moroccans subjects. The authors reported the result of recognizing five
expressions (anger, happy, sad, surprise and fear) in two different
experimental approaches. In the first, they trained and evaluated the system
in the multicultural database achieving an accuracy of 93.75%. The second
experiment was performed to evaluate the proposed method in a less controlled
environment. The method was trained with two databases (TFEID and RaFD) and
evaluated in the JAFFE database, achieving an accuracy of 48.67%.
Shan et al. [8] performed a study using local binary patterns (LBP) as feature
extractor. They combined and compared different machine learning techniques
like template matching, support vector machine (SVM), linear discriminant
analysis and linear programming to recognize facial expressions. The authors
also conducted a study to analyze the impact of image resolution in the
accuracy result and concluded that methods based on geometric features do not
handle low-resolution images very well, whereas those based on appearance,
like Gabor wavelets and LBP, are not so sensitive to the image resolution. The
best result achieved in their work was an accuracy of 95.1% using SVM and LBP
in the CK+ database. Using a cross-database validation (training with the CK+
and testing with JAFFE) to evaluate the proposed system in a less controlled
scenario, the authors achieved an accuracy of 41.3%. The images were firstly
cropped using the eye positions. The experimental setup used was a 10-fold
cross validation scheme without subject overlap. The training and the
recognition times were not mentioned by the authors.
A video-based facial expression recognition system was proposed by Byeon and
Kwak [14]. They developed a 3D-CNN having an image sequence (from neutral to
final expression) using 5 successive frames as 3D input. Therefore, the CNN
input is H*W*5 (where _H_ and _W_ are the image height and width,
respectively, and 5 is the number of frames). The authors claim that the 3D
CNN method can handle some degrees of shift and deformation invariance. With
this approach, they achieved an accuracy of 95%, but the method relies on a
sequence containing the full movement from the neutral to the expression. The
experiments were carried out with 10 persons only on a non-usual dataset. The
training and recognition times were not mentioned by the authors.
Another video-based approach, proposed by Fan and Tjahjadi [16], used a
spatial?temporal framework based on histogram of gradients and optical flow.
Their method comprises three phases: pre-processing, feature extraction and
classification. In the pre-processing phase, the detection of facial landmarks
was performed and a face alignment was carried out (in order to reduce
variations in the head pose). In the feature extraction phase, a framework
that integrates dynamic information extracted from the variation in the facial
shape caused by the expressions was employed. In the last phase, the
classification, a SVM classifier with a RBF kernel was used. The experiments
were carried out using the CK+ and the MMI databases. The accuracy achieved by
the authors in the CK+ database for seven expressions was 83.7% and in the MMI
database was 74.3%. The training time was not mentioned, while the recognition
time was about 350 ms per image in the CK+ database and 520 ms in the MMI
database.
In comparison with the methods above, this work: presents a higher accuracy inthe CK+ and JAFFE databases (including the cross-database validation) and a
smaller training and evaluation time than Liu et al. [7], [12], Shan et al.
[8] and Fan and Tjahjadi [16]; a more robust evaluation methodology (without
subject overlap between training and test) than Song et al. [10] and Bukert et
al. [11]; recognition of six and seven expression, instead of only five or six
as done by Song et al. [10], Bukert et al. [11] and Ali et al. [13]; validates
the proposed method on three largely used databases, to allow for a fair
comparison with other methods in the literature, instead of using unusual non-
public databases like Byeon and Kwak [14]. Many of the works mentioned here
present a very high accuracy that cannot be fairly compared with our method
because they allow for subject overlap in the training and test sets.
Preliminary experiments performed with our method considering such overlapping
scenarios also showed accuracies closer to 100% without much effort.
## 3\. Facial expression recognition system
Our system for facial expression recognition performs the three learning
stages in just one classifier (CNN). The proposed system operates in two main
phases: training and test. During training, the system receives a training
data comprising grayscale images of faces with their respective expression id
and eye center locations and learns a set of weights for the network. To
ensure that the training performance is not affected by the order of
presentation of the examples, a few images are separated as validation and are
used to choose the final best set of weights out of a set of trainings
performed with samples presented in different orders. During test, the system
receives a grayscale image of a face along with its respective eye center
locations, and outputs the predicted expression by using the final network
weights learned during training.
An overview of the system is illustrated in Fig. 2. In the training phase, new
images are synthetically generated to increase the database size. After that,
a rotation correction is carried out to align the eyes with the horizontal
axis. Subsequently, the image is cropped to remove background information and
to keep only expression specific features. A down-sampling procedure is
carried out to get the features in different images in the same location.
Thereafter, the image intensity is normalized. The normalized images are used
to train the Convolutional Neural Network. The output of the training phase is
the set of weights of the round that achieved the best result with the
validation data after a few training rounds considering data in different
orders. The testing phase use the same methodology as the training phase:
spatial normalization, cropping, down-sampling and intensity normalization.
Its output is a single number ? the id ? of one of the six basic expressions.
The expressions are represented as integer numbers (0 ? angry, 1 ? disgust, 2
? fear, 3 ? happy, 4 ? sad and 5 ? surprise).
1. Download: Download high-res image (682KB)
2. Download: Download full-size image
Fig. 2. Overview of the proposed facial expression recognition system. The
system operates in two main phases: training and test. The training phase
takes as input a set of images with a face, its eyes locations and expression
id, and outputs the set of weights of the round that achieved the best result
with the validation data after a few training rounds considering the data in
different orders. The test phase receives the weight set from the learning
step and a face image with its eyes locations, and outputs the expression id
of the image.
### 3.1. Synthetic sample generation
Unfortunately, the spatial normalization employed is not enough to ensure that
all faces will have the eyes correctly aligned due to imperfections in the eyedetection procedure. Fortunately, CNN's are very good at learning
transformation invariant functions (i.e. they can handle distorted images
[56]). However, one of the main problems of deep learning methods is that they
need a lot of data in the training phase to perform this task properly [56].
Unfortunately, the amount of data available in public datasets is not enough
to achieve such behavior in our application.
To address this problem Simard et al. [56] proposed the generation of
synthetic images (i.e. real images with artificial rotations, translations and
skewing) to increase the database, this process is referred as data
augmentation. The authors show the benefits of applying combinations of
translations, rotations and skewing for increasing the database. Following
this idea, in this paper, we use a 2D Gaussian distribution (?=3 pixels and
?=0) to introduce random noise in the locations of the center of the eyes.
Synthetic images are generated by considering the normalized versions of noisy
eye locations. For each image, 70 additional synthetic images were generated.
As it can be seen in Fig. 3, points are generated for both eyes following a
Gaussian distribution centered in their original location. The new eye center
position is therefore equivalent to the original one but disturbed by a
Gaussian noise. As the new values given to the normalization procedure are not
the real eye center, the resulting images will be either disturbed by a
translation, a rotation and/or a scale, or not disturbed at all. The specific
value of the Gaussian standard deviation needs to be carefully chosen. A very
small deviation could cause no variation in the original data and generate a
lot of useless equal images. On the other hand, a big deviation for each eye
could introduce too much translation, rotation and/or scale noise in the
images making the scenario more complex for the classifier to learn the
expression features. A standard deviation of ?=3 pixels was empirically
chosen. It is important to note that the synthetic data is only used in the
training.
1. Download: Download high-res image (776KB)
2. Download: Download full-size image
Fig. 3. Illustration of the synthetic sample generation. The Gaussian
synthetic sample generation procedure increases the database size and
variation, adding a noise (Gaussian with ?=3 pixels) in the images considering
a controlled environment. The new images are generated using the normalization
step and the new eye points. The green crosses are the original eye points,
while the red ones are the synthetic points. (For interpretation of the
references to color in this figure caption, the reader is referred to the web
version of this paper.)
### 3.2. Rotation correction
The images in the databases, and also in real environments, vary in rotation,
brightness and size even for images of the same subject. These variations are
not related to the face expression and can affect the accuracy rate of the
system. To address this problem, the face region is aligned (with rotation
normalization) with the horizon and a center point in order to correct
possible geometric issues like rotations and translations. To perform this
alignment two information are needed, the facial image and the center of both
eyes. There are already a lot of methods in the literature that are able to
find the eyes, and the others facial points, with high precision [57], [58],
[59], [60], [61], and this is not the focus of this work. Cheng et al. [61]
developed a CUDA version of DRMF [60], allowing for real-time facial keypoints
detection, while keeping an accurate detection of these keypoint even with the
face partially occluded (a shape RMSE below 0.05 fraction of inter-ocular
distance).To perform the face alignment, a rotation transformation is applied to align
the eyes with the horizontal axis of the image and an affine translation
defined by the locations of the eyes to centralize the face in a specific
point of the image. The rotation makes the angle formed by the line segment
going from one eye center to the other, and the horizontal axis to be zero.
Rotations and translations in the images are not related to the facial
expression and therefore should be removed to avoid negatively affecting the
accuracy rate of the system. The rotation correction procedure is shown in
Fig. 4.
1. Download: Download high-res image (234KB)
2. Download: Download full-size image
Fig. 4. Rotation correction example. The non-corrected input image (left) is
rotated (right) using the line segment going from one eye center to the other
(red line) and the horizontal axis (blue line). The 10° is just an example of
a possible correction. (For interpretation of the references to color in this
figure caption, the reader is referred to the web version of this paper.)
The input to this procedure can be an original or a synthetic image. The
rotation correction, for the synthetic generated images, may not carry out a
perfect align with the horizontal axis because the eye center is the real
position disturbed by a random Gaussian noise. Therefore, it will generate
images disturbed by rotations and translations, which increases the variation
in the training examples.
### 3.3. Image cropping
As shown in Fig. 2, the original image has a lot of background information
that is not important to the expression classification procedure. This
information could decrease the accuracy of the classification because the
classifier has one more problem to solve, i.e. discriminating between
background and foreground. After the cropping, all image parts that do not
have expression specific information are removed. The cropping region also
tries to remove facial parts that do not contribute for the expression (e.g.
ears, part of the forehead, etc.). Therefore, the region of interest is
defined based on a ratio of the inter-eyes distance. Consequently, our method
is able to handle different persons and image sizes without human
intervention. The cropping region is delimited by a vertical factor of 4.5
(considering 1.3 for the region above the eyes and 3.2 for the region below)
applied to the distance between the eyes middle point and the right eye
center. The horizontal cropping region is delimited by a factor of 2.4 applied
to this same distance. These factor values were determined empirically. An
example of this procedure is illustrated in Fig. 5.
1. Download: Download high-res image (326KB)
2. Download: Download full-size image
Fig. 5. Image cropping example. The spatial normalization is carried out using
half of the inter-eyes distance (_?_). The input image (left) is cropped
(right) using a horizontal factor (?*2.4) to crop in the horizontal and a
vertical factor (?*4.5 considering 1.3 for the region above the eyes and 3.2
for the region below) to crop in the vertical. This operation aims to remove
all non-expression features, such as background and hair.
### 3.4. Down-sampling
The down-sampling operation is performed to reduce the image size for the
network and to ensure scale normalization, i.e. the same location for the face
components (eyes, mouth, eyebrow, etc.) in all images. The down-sampling uses
a linear interpolation approach. After this re-sampling, one can guarantee
that the eye center will be approximately in the same position. This procedure
helps the CNN to learn which regions are related to each specific expression.The down-sampling also enables the convolutions to be performed in the GPU
since most of the graphics card nowadays have limited memory. The final image
is down-sampled, using a linear interpolation, to 32 ×32 pixels.
### 3.5. Intensity normalization
The image brightness and contrast can vary even in images of the same person
in the same expression, therefore, increasing the variation in the feature
vector. Such variations increase the complexity of the problem that the
classifier has to solve for each expression. In order to reduce these issues
an intensity normalization was applied. A method adapted from a bio-inspired
technique described in [62], called contrastive equalization, was used.
Basically, the normalization is a two step procedure: firstly a subtractive
local contrast normalization is performed; and secondly, a divisive local
contrast normalization is applied. In the first step, the value of every pixel
is subtracted from a Gaussian-weighted average of its neighbors. In the second
step, every pixel is divided by the standard deviation of its neighborhood.
The neighborhood for both procedures uses a kernel of 7 ×7 pixels (empirically
chosen). An example of this procedure is illustrated in Fig. 6.
1. Download: Download high-res image (206KB)
2. Download: Download full-size image
Fig. 6. Illustration of the intensity normalization. The figure shows the
image with the original intensity (left) and its intensity normalized version
(right).
Eq. (1) shows how each new pixel value is calculated in the intensity
normalization procedure:(1)x?=x??nhgx?nhgxwhere x? is the new pixel value, _x_
is the original pixel value, ?nhgx is the Gaussian-weighted average of the
neighbors of _x_ , and ?nhgx is the standard deviation of the neighbors of
_x_.
### 3.6. Convolutional Neural Network
The architecture of our Convolutional Neural Network is represented in Fig. 7.
The network receives as input a 32x32 grayscale image and outputs the
confidence of each expression. The class with the maximum value is used as the
expression in the image. Our CNN architecture comprises 2 convolutional
layers, 2 sub-sampling layers and one fully connected layer. The first layer
of the CNN is a convolution layer, that applies a convolution kernel of 5 ×5
and outputs 32 images of 28 ×28 pixels. This layer is followed by a sub-
sampling layer that uses max-pooling (with kernel size 2 ×2) to reduce the
image to half of its size. Subsequently, a new convolution layer performs 64
convolutions with a 7 ×7 kernel to map of the previous layer and is followed
by another sub-sampling, again with a 2 ×2 kernel. The outputs are given to a
fully connected hidden layer that has 256 neurons. Finally, the network has
six or seven output nodes (one for each expression that outputs their
confidence level) that are fully connected to the previous layer.
1. Download: Download high-res image (226KB)
2. Download: Download full-size image
Fig. 7. Architecture of the proposed Convolutional Neutral Network. It
comprises of five layers: two convolutional layers, two sub-sampling layers
and one fully connected layer.
The first layer of the network (a convolution layer) aims to extract
elementary visual features, like oriented edges, end-point, corners and shapes
in general, like described by Lecun et al. [36]. In the facial expression
recognition problem, the features detected are mainly the shapes, corners and
edges of eyes, eyebrow and lips. Once the features are detected, its exact
location is not so important, just its relative position compared to the other
features. For example, the absolute position of the eyebrows is not important,but their distances from the eyes are, because a big distance may indicate,
for instance, the surprise expression. This precise position is not only
irrelevant but it can also pose a problem, because it can naturally vary for
different subjects in the same expression. The second layer (a sub-sampling
layer) reduces the spatial resolution of the feature map. According to Lecun
et al. [36], this operation aims to reduce the precision with which the
position of the features extracted by the previous layer are encoded in the
new map. The next two layers, one convolutional and one sub-sampling, aim to
do the same operations that the first ones, but handling features in a lower
level, recognizing contextual elements (face elements) instead of simple
shapes, edges and corners. The concatenation of sets of convolution and sub-
sampling layers achieve a high degree of invariance to geometric
transformation of the input. The last hidden layer (a fully connected layer)
receives the set of features learned and outputs the confidence level of the
given features in each one of the considered expressions.
This network uses the stochastic gradient descent method to calculate the
synaptic weights between the neurons, this method was proposed by Buttou [63].
The initial value of these synapses for the convolutions and for the fully
connected layer are generated using the Xavier filler, proposed by Glorot et
al. [64], that automatically determines the scale of the initialization based
on the number of input and output neurons. The loss is calculated using a
logistic function of the soft-max output (known as _SoftmaxWithLoss_). The
activation function of the neurons is a ReLu (rectified linear unit), defined
as f(z)=max(z,0). The ReLu function generally learns much faster in deep
architectures [65].
## 4\. Experiments and discussions
The experiments were performed using three publicly available databases in the
facial expression recognition research field: The Extended Cohn?Kanade (CK+)
database [4], the Japanese Female Facial Expressions (JAFFE) database [5] and
the Binghamton University 3D Facial Expression (BU-3DFE) database [6].
Accuracy is computed considering one classifier to classify all learned
expressions. In addition, to allow for a fair comparison with some methods in
the literature, accuracy is also computed considering one binary classifier
for each expression, as used in [7].
The implementation of the pre-processing steps was done in-house using OpenCV,
C++ and a GPU based CNN library (Caffe [47]). All the experiments were carried
out using an Intel Core i7 3.4 GHz with a NVIDA GeForce GTX 660 CUDA Capable
that has 1.5 Gb of memory in the GPU. The environment of the experiments was
Linux Ubuntu 12.04, with the NVIDIA CUDA Framework 6.5 and the cuDNN library
installed. The preprocessing step (rotation correction, cropping, down-
sampling and intensity normalization) took only 0.02 s and the network
recognition (classification step) took in average 0.01 s.
In this section, a study is carried out showing the impact of every
normalization step in the accuracy of the method. Firstly, we describe the
databases used for the experiments. Secondly, the metrics used to evaluate the
system accuracy are explained. Thirdly, the results of the tuning experiments
and the influence of each pre-processing step is presented. Fourthly, the
results with different databases are shown and discussed in details. Finally,
a comparison with several recent facial expression recognition methods that
uses the same evaluation methodology is presented and the limitations of our
method are discussed.
### 4.1. Database
The presented system was trained and tested using the CK+ database [4], the
JAFFE database [5] and the BU-3DFE database [6]. The CK+ database comprises100 university students with age between 18 and 30 years old. The subjects in
the database are 65% female, 15% are African-American and 3% are Asian or
south American. The images were captured from a camera located directly in
front of the subject. The students were instructed to perform a series of
expressions. Each sequence begins and ends with the neutral expression. All
images in the database are 640 by 480 pixel arrays with 8-bit precision for
grayscale values. Each image has a descriptor file with its facial points,
these points were used to normalize the facial expression image. The facial
points in the database are coded using the facial action coding system (FACS)
[66]. The database creators used the active appearance models (AAMs) to
automatically extract the facial points. The database contains images for the
following expressions: neutral, angry, contempt, disgust, fear, happy, sad and
surprise. To do a fair comparison with the major part of the recent methods
[16], [7], [44], [43], [67], [8], in our experiments the contempt expression
images were not used. Some examples of the CK+ database images are shown in
Fig. 8.
1. Download: Download high-res image (381KB)
2. Download: Download full-size image
Fig. 8. Example of the images in the CK+ database. In (1), the subject is in
the neutral expression. In (2), the subject is in the surprise expression. In
(3), the subject is in the disgust expression. In (4), the subject is in the
fear expression.
To perform a fair evaluation of the proposed method, the database was
separated in 8 groups without subject overlap between groups (i.e. if an image
of one subject is in one group, no image of the same subject will be in any
other group). Each group contains about 12 subjects. This methodology ensures
that testing groups do not have subjects from the training group and is also
used by many methods in the literature [16], [44], [43], [67], [68], [69],
[8], [7], [70]. As discussed by Girard et al. [53], this methodology
(different subjects in training/testing groups and cross-validation) ensures
the generalizability of classifiers. Zavaschi et al. [44] also discuss and
support this data separation procedure. They conduct two experiments using the
same methodology, just changing the way that the data groups are separated. In
one experiment the groups contain images of the same subject (not the same
images), while in the other experiment they guarantee that images of the same
subject are not in the training and testing groups at same time. In the first
experiment an accuracy of 99.40% was achieved, while in the second the
accuracy goes down to 88.90%. This result shows that methods which are
evaluated without the same subjects in training/testing groups (which we
believe to be a fairer evaluation) generally presents a lower accuracy than
those that do not guarantee this constraint. We also confirmed these results
with preliminary experiments using our method.
To verify the generalization of the proposed method some cross-database
experiments were also performed. These experiments used the JAFFE and the
BU-3DFE databases. The JAFFE database consists of 213 images from 10 Japanese
female subjects. In this database, there are about 4 images in each one of the
six basic expressions and one image of the neutral expression from each
subject. All images in the dataset are 256 by 256 pixel arrays with 8-bit
precision for grayscale values. As this database is smaller, the groups are
separated by subject, i.e. each group contains images of just one subject, so
10 groups were formed. Some examples of the JAFFE database images are shown in
Fig. 9.
1. Download: Download high-res image (452KB)
2. Download: Download full-size imageFig. 9. Example of the images in the JAFFE database. In (1), the subject is in
the surprise expression. In (2), the subject is in the happy expression. In
(3), the subject is in the sad expression. In (4), the subject is in the sad
expression.
Another database used to evaluate the proposed method is the Binghamton
University 3D Facial Expression (BU-3DFE) [6]. The BU-3DFE database contains
64 subjects (56% female and 44% male), ranging age between 18 years to 70
years old, with a variety of ethnic/racial ancestries, including White, Black,
East-Asian, Middle-east Asian, Indian and Hispanic Latino. All images in the
dataset are 156 by 209 pixel arrays. This database was also separated in 8
groups, without subject overlap between groups. Each group contains about 8
subjects. Some examples of the BU-3DFE database images are shown in Fig. 10.
1. Download: Download high-res image (339KB)
2. Download: Download full-size image
Fig. 10. Example of the images in the BU-3DFE database. In (1) the subject is
in the fear expression. In (2) the subject is in the neutral expression. In
(3) the subject is in the fear expression. In (4) the subject is in the fear
expression.
All databases contain a lot of images of the same subjects for each expression
and these images are very similar to each other. So, only 3 frames of each
expression (i.e. most expressive frames) and 1 frame for the neutral
expression (i.e. least expressive frame), of each subject, were used in this
work. Following this methodology, the resulting database sizes are as follows,
for the CK+ database 2100 samples (without the synthetic samples) and 147,000
samples (with the synthetic samples), for the JAFFE database 213 samples
(without the synthetic samples) and 14,910 samples (with the synthetic
samples) and for the BU-3DFE database 1344 samples (without the synthetic
samples) and 94,080 samples (with the synthetic samples).
### 4.2. Metrics
To allow for a fair comparison of the presented method with the literature,
the accuracy was computed in two different ways. In the first, one classifier
for all basic expression is used. The accuracy is computed simply using the
average, _C_ _nclass_ , of the _n_ -class classifier accuracy per expression,
_C_ _nclassE_ , i.e. number of hits of an expression per amount of data of
that expression, see the following
equation:(2)Cnclass=?1nCnclassEn,CnclassE=HitETEwhere _Hit_ _E_ is the number
of hits in the expression _E_ , _T_ _E_ is total number of samples of that
expression and _n_ is the number of expressions to be considered.
In the second, one binary classifier for each expression performs a one-
versus-all classification, as proposed in [7]. Using this approach, the images
are presented to _n_ binary classifiers, where _n_ is the number of
expressions being classified. Each classifier aims to answer ?yes? if the
image contains one specific expression, or ?no? otherwise. For example, if one
image contains the surprise expression, the surprise classifier should answer
?yes? and all the other five classifiers should answer ?no?. The only
difference for this classifier from the architecture presented in Section 3 is
that only two outputs are required for each classifier. The accuracy is
computed using the average, _C_ _bin_ , of the binary classifier accuracy per
expression, _C_ _binE_ , i.e. the number of hits of an expression plus the
number of hits of a non-expression divided per total amount of data, see the
following equation:(3)Cbin=?1nCbinEn,CbinE=HitE+HitNETwhere _Hit_ _E_ is the
number of hits in the expression _E_ , i.e. number of times the classifier _E_
responded ?yes? and the tested image was of the expression _E. Hit_ _NE_ is
the number of times the classifier _E_ responded ?no? and the tested image wasnot the expression _E. T_ is the total number of tested images and _n_ is the
number of expressions to be considered.
### 4.3. Pre-processing tuning
As described earlier, the proposed method combines a pre-processing step, that
aims to remove non-expression specific features of a facial image and a
Convolutional Neural Network to classify this preprocessed image in one of the
six (or seven) expressions. In this section, we present the impact in the
classification accuracy of each operation in the preprocessing step. As these
experiments aim only to show the impact of the operations, a simplified
version of our methodology was employed. Here, we randomly generate the order
of the samples to the network and use a simple _k_ -fold cross validation
between the 8 groups of the CK+ database. The database was divided into two
sets, training (with 7 of the groups) and test (with 1 of the groups). The
training was performed 8 times using only 2000 epochs for each of them. The
accuracy was computed per expression (_C_ 6 _classE_) and overall average for
all expressions (_C_ 6 _class_).
(_a_) _No pre-processing_ . This first experiment was carried out using the
original database, without any intervention or image pre-processing, just a
down-sampling to the image to be of the same size as the input of the CNN. In
this experiment, the average accuracy for all expressions was C6class=53.57%.
The accuracy per expression is shown in Table 1. The accuracy shown is an
average of Eq. (2) for all runs.
Table 1. Preprocessing steps tuning for the CK+ database: (a) no pre-
processing; (b) just cropping; (c) just rotation correction; (d) cropping and
rotation correction; (e) only intensity normalization; (f) both
normalizations; (g) spatial normalization and synthetic samples; and (h) both
normalizations and synthetic samples.
**Preprocessing Step**| **Angry** (%)| **Disgust** (%)| **Fear** (%)|
**Happy** (%)| **Sad** (%)| **Surprise** (%)| **Average** (%)
---|---|---|---|---|---|---|---
(a)| 28.10| 51.23| 17.91| 70.68| 20.99| 77.52| 53.57
(b)| 68.60| 79.01| 23.37| 86.39| 23.46| 87.16| 71.67
(c)| 17.17| 79.09| 00.00| 48.92| 05.05| 91.25| 61.55
(d)| 81.82| 90.74| 73.13| 95.81| 66.67| 94.50| 87.86
(e)| 27.27| 52.94| 08.22| 79.10| 18.29| 85.54| 57.00
(f)| 78.51| 93.21| 53.73| 95.29| 75.31| 93.12| 86.67
(g)| 86.05| 88.30| 69.33| 96.60| 77.11| 95.34| 87.10
(h)| **79.34**| **94.44**| **73.13**| **99.48**| **72.84**| **94.94**|
**89.76**
As it can be seen in Table 1, using only the CNN without any image pre-
processing, the recognition rate is very low compared with methods in the
literature. We believe the variation and amount of samples in the CK+ database
was small which did not allow the Convolutional Neural Network learn how to
deal with pose, environment and subject variance. Additionally, it does not
use the full range of the image space available in the network input to
represent the face.
(_b_) _Image cropping_. In order to increase our method performance, as
explained in Section 3, the image is automatically cropped to remove non-
expression specific regions, in both training and testing steps. As a results,
the average accuracy for all expressions increased to C6class=71.67%. The
accuracy per expression is shown in Table 1. Here, the down-sampling is also
performed, because the input of the proposed network is a fixed 32 ×32 pixels
image.
Compared with the result shown before, we can note a significant increase ofthe recognition rate by adding only the cropping process. The main reason for
the accuracy increase is that with the cropping, we remove a lot of
unnecessary information that the classifier will need to handle to determine
the subject expression and use better the image space available in the network
input.
(_c_)_Rotation correction_. A rotation correction (and the down-sampling) is
performed in the image to remove rotations that are not related to expression
facial changes (that can be pose-specific or caused by a camera movement), in
both training and testing steps. The average accuracy for all expressions was
C6class=61.55%.
Note that, this result is applying just the rotation correction, but not the
cropping. Compared with the result of no pre-processing we can note an
increase of the accuracy in about 8.00%. This increase might be caused by the
lower variation that the network needs to handle. With the rotation
correction, the facial elements (eyes, mouth, and eyebrows) stay mostly in the
same pixel space, but still has the influence of the background and does not
use the full range of the image space available in the network input to
represent the face, as in a).
(_d_) _Spatial normalization_. As seen before, the image cropping and the
rotation correction applied separately, increase the classifier accuracy. This
happens because both procedures reduce the problem complexity. Here, we
discuss the full spatial normalization, composed by the image cropping,
rotation correction and down-sampling. With the operations combined, the
average accuracy for all expressions was C6class=87.86%.
As expected, joining both procedures in the pre-processing step increase the
accuracy. This happens because a lot of variation not related to the
expression was removed from the image. Although the Convolutional Neural
Network could handle these variations, we would need a bigger database (that
we do not have) and maybe a more complex architecture.
(_e_) _Intensity normalization_. The spatial normalization procedure
significantly increases the overall accuracy of the system. The intensity
normalization is used to remove brightness variation in the images in both
steps, training and testing. This experiment was performed using just the
intensity normalization. It uses the same methodology described before. The
average accuracy for all expressions was C6class=57.00%.
As it can be seen, by just applying the intensity normalization the classifier
accuracy was also slightly increased.
(_f_) _Spatial and intensity normalization_. Combining the spatial (rotation
correction, cropping and down-sampling) and intensity normalizations, we
remove a big part of the variations unrelated to the facial expression and
leave just the expression specific variation that is not related to the pose
or environment. This experiment was done using the same methodology described
before. The average accuracy for all expression was C6class=86.67%. The
accuracy per expression is shown in Table 1.
As it can be seen, the accuracy of applying both normalization procedures is
lower than the one that uses only the spatial normalization. The result of the
fear expression has a very low accuracy, which reduces the overall recognition
average.
(_g_) _Spatial normalization and synthetic samples_. The result of the spatial
and intensity normalization and only the spatial normalization gives a false
impression that the intensity normalization might decrease the accuracy of the
method. To verify this assumption, a new experiment was conducted using only
the spatial normalization procedure and the additional synthetic samples for
training. For the synthetic samples generation, thirty more samples weregenerated for each image using a Gaussian standard deviation of 3 pixels
(?=3). The average accuracy for all expression was C6class=89.11%. The
accuracy per expression is shown in Table 1.
This result increases the accuracy of applying just the spatial normalization
(87.86%). However, this accuracy is outperformed by the next experiment, that
apply both normalizations and the synthetic sample generation, showing that
the synthetic sample generation procedure indeed increases the robustness of
the classifier (motivated by the increase of the samples and its variation).
(_h_) _Spatial normalization, intensity normalization and synthetic samples_.
The best result achieved in our method applies the three image pre-processing
steps: spatial normalization, intensity normalization and synthetic samples
generation. This experiment is performed using the same methodology described
before. The average accuracy for all expression was C6class=89.79%. The
accuracy of this experiment shows that combining the three techniques (spatial
normalization, intensity normalization and synthetic samples) is better than
using them individually.
Table 1 shows the mean accuracy for each expression using all the
preprocessing steps already discussed for the CK+ database: (a) no
preprocessing, (b) cropping, (c) rotation correction, (d) spatial
normalization (cropping and rotation correction), (e) intensity normalization,
(f) both normalizations (spatial and intensity), (g) spatial normalization
using the synthetic samples and (h) both normalizations and the synthetic
samples. The accuracy is computed using the six class classifier (_C_ 6
_classE_).. The best accuracies achieved are highlighted in bold.
### 4.4. Results
As discussed before, a simplified training/testing methodology was used to
evaluate the impact of the pre-processing steps. In contrast to the tuning
experiments that used only training and test sets, this section separates the
databases for each experiment in three main sets: training set, validation set
and test set.
In addition, it was also mentioned that the gradient descent method was used
for training the network. Such methods might be influenced by the order of
presentation of the samples to the network during training, which causes a
variation in accuracy. As can be seen in Table 2 the accuracy increases (or
decreases) in about 4.00% only with the order change of the training samples
(as it can be seen in the values highlighted in bold). Table 2 shows the
accuracy for the same training, performed 10 times, each one with a random
presenting order of the training samples. To be less affected by this accuracy
variation, we propose a training methodology that uses a validation set to
choose the best network weights based on different trainings, illustrated in
Fig. 2. Therefore, the final accuracy result of our experiments is computed
using the network weights of the best run out of 10 runs, having a validation
set for accuracy measurement. Each run has a random presentation order of the
training samples. The weights of the best run in the validation set are later
used to evaluate the test set and compute the final accuracy.
Table 2. Impact of the presentation order in the accuracy.
**Presentation order**| **Accuracy** (%)
---|---
**1**| **88.18**
2| 86.36
3| 86.36
4| 86.36
5| 88.18
**6**| **84.55**7| 85.45
8| 84.55
9| 87.27
10| 88.18
**Average: 86.71%**
**Standard deviation: 1.43%**
To verify that the proposed approach handles well other databases and even in
unknown environments, some experiments with the BU-3DFE database, the JAFFE
database and cross-databases experiments were performed. The experiments with
other databases follow the same approach that the CK+ database. In the cross-
database experiments, the network was trained with the CK+ database and the
accuracy evaluation was calculated using the BU-3DFE database or in the JAFFE
database. In the cross-database experiments, no images from the BU-3DFE
database or JAFFE database was used during the network training.
_CK+ database experiment_. The database was separated into 8 groups of non-
overlapping subjects (with about 12 subjects each). Seven groups were used to
compose the training set, whereas the eighth group (also with about 12
subjects) was shared between the validation set and test set, where 11
subjects were used for validation and 1 subject for testing. Our experiments
follow a _k_ -fold cross-validation configuration, in which, each time, one
group is separated for validation/test and the other seven for training. When
a group is selected for validation/test, a leave-one-out procedure is
performed within the 12 subjects (having 11 for validation and 1 for test) for
each of the training runs. For each configuration of validation and test
subjects, the training is carried out 10 times, changing the presentation
order of the training images. The validation group is used to select the best
epoch for each run during training and to select the run with the best
presentation order. Based on these information (best epoch and presentation
order), the best network weights are selected and used to compute the accuracy
of the test set. With this experimental configuration, the training of the
network is performed about 960 times (8 groups * 12 subjects per group * 10
runs with different presenting order). The training of each run took only 2
min, therefore the total training of the system takes about 20 min (2 min * 10
runs). The overall experiment time was about 32 hs (including all combinations
of training, validation and test). The results of this experiment are computed
as an average of the 96 runs (8 folds * 12 subjects per group * 1 best
configuration out of 10 runs).
Table 3 shows the best result achieved (using both normalizations and the
synthetic samples) for _C_ 6class and _C_ _bin_. As it can be seen, the binary
classifier approach increases the accuracy. It happens because in this
approach the hit can be achieved six times (one for each classifier), instead
of using just one classifier, where each sample has just one chance to be
properly classified. The binary classifier approach was employed to allow a
fair comparison with some methods in the literature that just report these
results. We think that the six-class classifier (_C_ 6class) is a fairer
evaluation method. However, the _C_ _bin_ classification approach is a useful
method when we are interested in just one expression. The standard deviation
reported in the table is based on the runs of all subjects.
Table 3. Accuracy for both classifiers using all processing steps and the
synthetic samples for six expressions on the CK+ database.
**Classifier**| **Angry** (%)| **Disgust** (%)| **Fear** (%)| **Happy** (%)|
**Sad** (%)| **Surprise** (%)
---|---|---|---|---|---|---
_C_ 6 _classE_| 93.33| 100.00| 96.00| 98.55| 84.52| 99.20_C_ _binE_| 98.27| 99.37| 99.24| 99.68| 98.17| 98.81
**Average of** _C_**6** _class_ : **96.76** %±**0.07**
**Average of** _C_ _bin_ : **98.92** %±**0.02**
The training parameter values that achieve the results shown in Table 3 are
shown in Table 4. The same parameter values are used in the experiments on
other databases.
Table 4. Training parameters.
**Parameter**| **Value**
---|---
Momentum| 0.95
Learning rate| 0.01
Epochs| 10,000
Loss function| _SoftmaxWithLoss_
Gaussian standard deviation| 3
Synthetic samples amount| 70
Using the result shown in Table 3, the confusion matrix shown in Table 5 was
created for the six-class classifier.
Table 5. Confusion matrix using both normalizations and synthetic samples for
six expressions on the CK+ database.
Empty Cell| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Angry| 126| 6| 2| 0| 1| 0
Disgust| 0| 177| 0| 0| 0| 0
Fear| 0| 0| 72| 0| 3| 0
Happy| 3| 0| 0| 204| 0| 0
Sad| 3| 0| 1| 0| 71| 9
Surprise| 1| 0| 1| 0| 0| 247
Based on the results of the six-class classifier, we can note that the
disgust, happy and surprise expressions achieve an accuracy rate higher than
98%. While the angry and fear expression were about 93% and 96%, respectively.
The sad expression achieves the smallest recognition rate, with only 84.52%.
Looking at the confusion matrix, the sad expression was confused in the
majority of the time with the surprise expression. This shows that the
features of these two expressions are not well separated in the pixel space,
i.e. they are very similar to each other in some cases. Fig. 11 shows some
examples of the misclassification.
1. Download: Download high-res image (405KB)
2. Download: Download full-size image
Fig. 11. In (1), the expected expression was sad, but the method returned
fear. In (2), the expected expression was angry, but the method returned fear.
In (3), the expected expression was sad, but the method returned angry. In
(4), the expected expression was angry, but the method returned sad.
Fig. 12 shows an illustration of the learned kernels and the generated maps
for each convolution layer. In the first convolution layer, the input image is
processed by the 32 learned kernels and generates 32 output maps. In the
second convolution layer, the 64 learned kernels are used to generate new maps
for each one of the 32 maps of the previous layer. The kernels shown in Fig.
12 were learned in the training using the CK+ database for the six basic
expressions. As it can be seen in the figure, after the second convolutional
layer, the generated maps are focused on regions near the eyes, mouth and
nose. Indeed, these regions are more critical for facial expression analysis,
as suggested by psychological studies [71].
1. Download: Download high-res image (495KB)
2. Download: Download full-size imageFig. 12. Illustration of the learned kernels and the generated maps for each
convolution layer. In the first convolution layer, the input image is
processed by the 32 learned kernels and generates 32 output maps. In the
second convolution layer, the 64 learned kernels are used to generate new maps
for each one of the 32 maps of the previous layer. The sub-sampling layers are
not represented in this image. Only a subset of the 32 kernels for the first
layer and of the 64 kernels for the second layer are shown. The generated maps
were equalized to allow for a better visualization.
Instead of recognizing only six expressions, we can also recognize the neutral
expression, which results in a classifier that recognizes seven expressions.
The result of the seven expressions classifier to the CK+ database, using the
same methodology of the six-class is shown in Table 6. The standard deviation
reported in the table is between the runs of all subjects.
Table 6. Accuracy for both classifiers using all processing steps and the
synthetic samples for seven expressions.
**Classifier**| **Neutral** (%)| **Angry** (%)| **Disgust** (%)| **Fear** (%)|
**Happy** (%)| **Sad** (%)| **Surprise** (%)
---|---|---|---|---|---|---|---
_C_**7** _classE_| 95.15| 91.11| 99.44| 92.00| 100.0| 82.14| 98.80
_C_ _binE_| 97.49| 97.82| 99.76| 99.11| 99.76| 98.79| 98.87
**Average of** _C_ 7 _class_ : **95.79** %±**0.06**
**Average of** _C_ _bin_ : **98.80** %±**0.01**
As it can be seen, for the binary classifier approach, we have a slight
decrease in accuracy, from 98.90% to 98.80%. On the other hand, in the seven-
class classifier the decrease was larger, from 96.7% to 95.7%. It happens
because, in the seven-class classifier approach, one more output was included
in the network. On the other hand, in the binary-class approach, one new
classifier was inserted, keeping the others unchanged. The confusion matrix
for the seven expressions is shown in Table 7.
Table 7. Confusion matrix using both normalizations and synthetic samples for
seven expressions on the CK+ database.
Empty Cell| Neutral| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---|---
Neutral| 294| 11| 1| 1| 0| 0| 2
Angry| 8| 123| 1| 0| 3| 0| 0
Disgust| 0| 1| 176| 0| 0| 0| 0
Fear| 6| 0| 0| 69| 0| 0| 0
Happy| 0| 0| 0| 0| 207| 0| 0
Sad| 0| 3| 0| 3| 0| 69| 9
Surprise| 2| 0| 0| 1| 0| 0| 246
_BU-3DFE database experiment_. This experiment follows the same approach as
the CK+ database, the only difference is that in the BU-3DFE database the
groups have about only eight subjects. The results of this experiment is
computed as an average of the 64 runs (8 folds * 8 subjects per group * 1 best
configuration out of 10 runs). The result for six and seven expressions for
both classifiers is shown in Table 8. The standard deviation reported in the
table is between the runs of all subjects.
Table 8. Accuracy using six and seven (six basic plus neutral) expressions for
the BU-3DFE and JAFFE databases.
**Train**| **Test**| **Classifier**| **6-expressions**| **7-expressions**
---|---|---|---|---
BU-3DFE| BU-3DFE| _C_ _nclass_| 72.89%±0.05| 71.62%±0.04
BU-3DFE| BU-3DFE| _C_ _bin_| 90.96%±0.01| 91.89%±0.01
JAFFE| JAFFE| _C_ _nclass_| 53.44%±0.15| 53.57%±0.13JAFFE| JAFFE| _C_ _bin_| 84.48%±0.05| 86.74%±0.03
As it can be seen, the accuracy for the BU-3DFE decreased compared with the
CK+ database. One possible reason is that this database has more subjects from
different ethnicities and light conditions, and is smaller than the CK+. In
addition, we have an increase in the accuracy for the _C_ _bin_ classifier on
seven expressions, whereas we have a decrease in accuracy for the _C_ _nclass_
classifier on seven expressions. The confusion matrices for this experiment,
on six and seven expressions, can be seen in Table 13 and in Table 14,
respectively, in the appendices (Appendix A).
The BU-3DFE database contains 3D models of faces in the six basic expressions
and the neutral expressions. Commonly, this database is used for 3D
reconstruction and facial expression recognition with 3D data. However, in
this work, just the 2D image of the subjects are used to recognize the
expressions. There are other works in the literature that presents higher
facial expression recognition accuracies, but using 3D information to infer
the expressions [72], [73], [74].
A better evaluation of the proposed method in real environments is the cross-
database experiments, i.e. train the method with one database and test with
another (in this case the BU-3DFE).
To perform the cross-database experiment, seven groups of the CK+ database
were used to train the network and one was used to be the validation set (to
choose the best network weights based on the best epoch presentation order of
the training set). The BU-3DFE was used to test the network. The training was
done eight times, each one with a different validation set. We ran each
configuration (training set plus validation set) 10 times, each one with a
different presenting order in the training samples. The result in the cross-
database experiment is computed as an average of the 8 runs to consider
different combinations of training and validation sets (8 folds * 1 best
configuration out of 10 runs) showing all the BU-3DFE images to test the
network. The results are shown in Table 9.
Table 9. Cross-database experiment for the BU-3DFE and JAFFE databases.
**Train**| **Test**| **Classifier**| **6-expressions** (%)| **7-expressions**
(%)
---|---|---|---|---
CK+| BU-3DFE| _C_ _nclass_| 45.91| 42.25
CK+| BU-3DFE| _C_ _bin_| 81.97| 83.50
CK+| JAFFE| _C_ _nclass_| 38.80| 37.36
CK+| JAFFE| _C_ _bin_| 79.60| 82.10
The confusion matrices for this experiment, on six and seven expressions, can
be seen in Table 15 and in Table 16 respectively, in the appendices (Appendix
A).
_JAFFE database experiment_. This experiment follows a slightly different
approach from the CK+ and BU-3DFE experiments in regards to the number of
groups. The JAFFE database contains images from only 10 subjects, therefore,
as done by other works in the literature [7], [8], the images were separated
in 10 groups, each one with just one subject. The test was carried out using a
10-fold cross validation, the training group contains eight subjects, the
validation group one subject and the testing group one subject. The result of
this experiment is computed as an average of the 10 runs (10 folds * 1
subjects per group * 1 best configuration out of 10 runs). The result for six
and seven expressions for both classifiers is shown in Table 8. The standard
deviation reported in the table is between the runs of all subjects.
As it can be seen in Table 8, compared with the CK+ and BU-3DFE results, the
accuracy decreased considerably. It also happens in other works [7], [8],[44], motivated mainly due to the small database. The required data amount
that methods for facial action (or expression) recognition need in the
training phase to achieve a good accuracy is studied by Girard et al. [53]. In
one of their experiments, the recognition rate varies from 63% to 94% as the
training set increased from 8 subjects to 64 subjects. Once the JAFFE dataset
contains images from only 10 subjects, it is unfair to compare its result with
the CK+ or the BU3DFE datasets that contain, respectively, 100 subjects and 64
subjects.
This problem of small amount of data is more emphasized in the technique used
in this work. Convolutional Neural Networks, generally, requires a big amount
of data to adjust its parameters. Therefore, there are other approaches in the
literature that do not require a so high amount of data and consequently
achieve better results than the method presented here [44], [8]. The confusion
matrices for this experiment, on six and seven expressions, can be seen in
Table 17 and in Table 18, respectively, in the appendices (Appendix A).
The cross-database experiment was also performed to the JAFFE database. In
this experiment the network is trained and validated only on the CK+ database
and the tests were carried out on the JAFFE database. This experiment follows
the same approach described for the BU-3DFE. The result in the cross-database
experiment is computed as an average of the 8 runs to consider different
combinations of training and validation sets (8 folds * 1 best configuration
out of 10 runs) showing all the JAFFE images to test the network. The results
for this experiment are shown in Table 9.
One motivation for the small accuracy reported in Table 9 is the cultural
differences over the training subjects and the testing subjects. While in the
BU-3DFE we have some subjects of the same ethnicity of the CK+ database, the
same does not happen with the JAFFE database. The confusion matrices for this
experiment, on six and seven expressions, can be seen in Table 19 and in Table
20, respectively, in the appendices (Appendix A).
### 4.5. Comparisons
Table 10 summarizes all results presented in this section, showing the
evolution of the proposed method by changing the image pre-processing steps.
Table 10. Preprocessing comparison.
**Preprocessing**| **Accuracy** (%)
---|---
None| 53.57
Cropping| 71.67
Rotation correction| 61.55
Spatial normalization| 87.86
Intensity normalization| 57.00
Spatial and intensity normalization| 86.67
Spatial normalization and synthetic samples| 89.11
**Spatial and intensity normalization and synthetic samples**| **89.79**
As it can be seen in Table 10, the best performance was achieved using both
normalization procedures and the synthetic samples (as it can be seen in the
row highlighted in bold). This table just summarizes the experiments performed
to choose the best configuration to the proposed classifier, using a
simplified training methodology.
Table 11 shows the results of the CK+ database using the full validation
methodology (with training, validation and test sets) explained before. In
this table, a comparison between the proposed method and other methods in the
literature that use the same experimental methodology (i.e. perform a _k_
-fold cross-validation and guarantee that the same subject is not in the
training and testing groups) is presented.Table 11. Comparison for the CK+ database.
**Method**| **Classifier**| **6-expressions**| **7-expressions**
---|---|---|---
Fan and Tjahjadi [16]a| _C_ _nclass_| 83.70| ?
_C_ _bin_| ?| ?
|
|
|
Zavaschi et al. [44]| _C_ _nclass_| ?| 88.90
_C_ _bin_| ?| ?
|
|
|
Rivera et al. [43]| _C_ _nclass_| ?| 89.30
_C_ _bin_| ?| ?
|
|
|
Gu et al. [67]| _C_ _nclass_| 91.51| ?
_C_ _bin_| ?| ?
|
|
|
Zhong et al. [68]| _C_ _nclass_| 93.30| ?
_C_ _bin_| ?| ?
|
|
|
AUDN [12]| _C_ _nclass_| 93.70| ?
_C_ _bin_| ?| ?
|
|
|
Liu et al. [76]a| _C_ _nclass_| 94.19| ?
_C_ _bin_| ?| ?
|
|
|
Lee et al. [69]a| _C_ _nclass_| 94.90| ?
_C_ _bin_| ?| ?
|
|
|
LBP+SVM [8]| _C_ _nclass_| 95.10| 91.40
_C_ _bin_| ?| ?
|
|
|
BDBN [7]| _C_ _nclass_| ?| ?
_C_ _bin_| 96.70| ?
|
|
|
IntraFace [70]a| _C_ _nclass_| 96.40| ?_C_ _bin_| ?| ?
|
|
|
**Proposed**| _C_ _nclass_| **96.76**| **95.75**
_C_ _bin_| **98.92**| **98.80**
a
The authors also recognize the contempt expression. They were not placed in
the 7-expression column because our 7-expression recognition system is based
on the six basic expression plus the neural expression (not the contempt
expression).
As it can be seen in Table 11, the proposed method achieves competitive
results in the CK+ database for all experiment configurations (as it can be
seen in the row highlighted in bold). Besides, the training and recognition
times are also smaller than the others. The total time to train the system is
with the CK+ is about 20 min. The whole experimentation time including all the
_k_ -fold configurations of the proposed method was 32 h, and the recognition
is real time (only 0.01 s for each image), almost 100 images can be processed
per second. In comparison with Liu et al. [7], their training took eight days
and the recognition was about 0.21 s per image. Note that we disregard the
hardware used for computing the time. Shan et al. [8] and Zhon et al. [68] did
not report the training and recognition time. The method proposed by Lee et
al. [69] requires manual image pre-processing before the facial expression
recognition.
It is important to note that although there are some other methods in the
literature that report accuracies higher than ours (98.92%), the results are
not comparable. As discussed before, a fair evaluation method for facial
expression recognition should guarantee that images of a subject are not
present in both training and testing sets at the same time. Accuracy achieved
without this constraint overcomes the presented result. Some of these methods
randomly select data to the folds [39], [40], [41], [42]. Some other do not
mention if this constraint was kept in the images chosen for each fold [77],
[78], [10] therefore they were assumed to have an overlap. There are also
methods which select only a subset of the database (or a subset of the six
basic expressions) to evaluate the accuracy [79], [80], [81], [13]. Rivera et
al. [43] also reported an accuracy of 99.50% in the CK [82] database, which is
a previous version of the CK+ [4] database with less subjects and image
sequences. However, as it can be seen in Table 11, the results with CK+ tend
to be worse than the ones with the CK.
Some methods in the literature [8], [7], [67] also performed the cross-
database experiment in the JAFFE database (training in the CK+ database and
testing in the JAFFE). A comparison of this work with these methods, for the
cross-database experiment, is shown in Table 12. In our literature review, we
did not find any work using a cross-database evaluation with the BU-3DFE for
expression recognition. The results achieved by the proposed method in the
cross-database experiments with the JAFFE database are highlighted in bold.
Table 12. Comparison for the JAFFE cross-database experiment.
**Method**| **Classifier**| **6-expressions**| **7-expressions**
---|---|---|---
LBP+SVM [8]| _C_ _nclass_| ?| 41.30
_C_ _bin_| ?| ?
|
|
|BDBN [7]| _C_ _nclass_| ?| ?
_C_ _bin_| ?| 68.00
|
|
|
Gu et al. [67]| _C_ _nclass_| 55.87| ?
_C_ _bin_| ?| ?
|
|
|
**Proposed**| _C_ _nclass_| **38.80**| **37.36**
_C_ _bin_| **79.60**| **82.10**
Comparing the presented method with Shan et al. [8] our accuracy was about 4%
smaller using 7 expressions and the _C_ _nclass_ classifier. They did not
report the result of the six basic expressions. On the other hand, compared
with _Liu et al._ [7], using the binary classifier approach the proposed
method significantly increases the accuracy, from 68.0% to 82.0%. Ali et al.
[13] also perform a cross-database validation in their work, achieving an
accuracy of 48.67% training in the RAFD database and testing with the JAFFE
database. Unfortunately, we cannot compare with our results due to the
discrepancy in the experimental methodology. The first is the database, in our
case the training was carried out with the CK+ database, while theirs with the
RAFD database. Secondly, they recognize only five expressions (anger, happy,
surprise, sad and fear).
### 4.6. Limitations
As discussed before, the presented method needs the locations of each eye for
the image pre-processing steps. The eye detection can be easily included in
the system adopting methods available in the literature [59], [61], while
still keeping whole method (including the eyes detection) in real time. In
addition, as shown in Table 1, the accuracy of some expressions, e.g. sad, was
about 84%, whereas the accuracy of the whole method was about 96%. This
suggests that the variation between these classes are not enough to separate
them. One approach to address this problem is to create a specialized
classifier for those expressions, to be used as a second classifier. Another
limitation of the presented method is the controlled environment of the input
images with the frontal face of the subjects. All these limitations, could be
addressed with a large set of training data, which will allow for a deeper
network, capable of handling such constraints.
## 5\. Conclusion
In this paper, we propose a facial expression recognition system that uses a
combination of standard methods, like Convolutional Neural Network and
specific image pre-processing steps. Experiments showed that the combination
of the normalization procedures improve significantly the method's accuracy.
As shown in the results, in comparison with the recent methods in the
literature, that use the same facial expression database and experimental
methodology, our method achieves competitive results and presents a simpler
solution. In addition, it takes less time to train and its recognition is
performed in real time. Finally, the cross-database experiments show that the
proposed approach also works in unknown environments, where the testing image
acquisition conditions and subjects vary from the training images, but still
has room left for improvement.
As explained in Section 1, the use of Convolutional Neural Networks aims to
decrease the need for hand-coded features. Its input can be raw images,
instead of an already selected set of features. It happens because this neuralnetwork model is able to learn the set of features that best models the
desired classification. To perform such learning, Convolutional Neural
Networks need a large amount of data, that we do not have. This is a
constraint of deep architectures, motivated by the large amount of parameters
that need adjustment during training. To address this problem (our limited
data), the pre-processing operations were applied to the images, in order to
decrease the variations between images and to select a subset of the features
to be learned, which reduces the need for a large amount of data. If we had a
better set of images, with more variation and more samples (millions), these
pre-processing operations could not be necessary to achieve the reported
accuracy and even the cross-database validation could be improved.
Preliminary experiments were performed with deeper architectures, trained with
a big amount of data. In these experiments, a deep Convolutional Neural
Network composed by 38 layers and trained with about 982,800 images from 2662
subjects, proposed by Parkhi et al. [83] to recognize faces, was briefly
studied. The already trained model was used as a pre-trained feature extractor
plugged as input of a simple two-layered neural network trained with the CK+
database. In this experiment, no preprocessing operation was applied. Despite
the experiment simplicity, the results achieved were promising and even
increase the accuracy achieved in the cross-database experiments (reported in
Section 4), with the cost of decreasing the accuracy in the same database
experiments. These results indicate that a deep learning approach of such type
can be a better way to produce a discriminative model for facial expression
recognition, allowing it to work in uncontrolled scenarios, which is one of
the current challenges in this field.
As future work, the application of this feature extraction method will be
investigated in other problems. In addition, we want to investigate other
learning methods in order to increase the method robustness in unknown
environments (e.g. with varying light conditions, culture and others). Also,
more tests will be performed using the face descriptor proposed by Parkhi et
al. [83], using fine adjustment techniques, which aims to tune an already
trained deep neural network in order to focus on more specific features (in
our case, expressions).
## Conflict of interest
We wish to confirm that there are no known conflicts of interest associated
with this work.
## Acknowledgment
We would like to thank Universidade Federal do Espírito Santos ? UFES (project
SIEEPEF, 5911/2015), Fundação de Amparo Pesquisa do Espírito Santo ? FAPES
(grants 65883632/14, 53631242/11, and 60902841/13), Coordenação de
Aperfeiçoamento de Pessoal de Nível Superior ? CAPES (grant 11012/13-7 and
scholarship) and Conselho Nacional de Desenvolvimento Científico e Tecnológico
? CNPq (grants 552630/2011-0 and 312786/2013-1).
## Appendix A.
See Table 13, Table 14, Table 15, Table 16, Table 17, Table 18, Table 19,
Table 20.
Table 13. Confusion matrix for six expressions on the BU-3DFE database.
Empty Cell| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Angry| 109| 26| 7| 2| 10| 3
Disgust| 15| 12| 14| 11| 0| 7
Fear| 12| 21| 61| 10| 9| 19
Happy| 0| 6| 6| 159| 3| 0
Sad| 17| 3| 14| 5| 124| 5Surprise| 7| 6| 6| 5| 11| 134
Table 14. Confusion matrix for seven expressions on the BU-3DFE database.
Empty Cell| Neutral| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---|---
Neutral| 191| 11| 4| 5| 1| 14| 6
Angry| 22| 93| 24| 3| 0| 15| 0
Disgust| 0| 9| 117| 20| 6| 0| 7
Fear| 21| 3| 23| 47| 6| 10| 22
Happy| 5| 0| 6| 2| 160| 0| 1
Sad| 28| 10| 0| 8| 3| 113| 6
Surprise| 13| 0| 6| 7| 6| 5| 132
Table 15. Confusion matrix for six expressions on the BU-3DFE database in the
cross-database experiment.
Empty Cell| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Angry| 55| 30| 0| 0| 23| 49
Disgust| 12| 57| 9| 6| 5| 71
Fear| 4| 3| 13| 12| 24| 76
Happy| 6| 8| 5| 111| 22| 22
Sad| 6| 0| 3| 3| 51| 105
Surprise| 5| 0| 0| 3| 7| 154
Table 16. Confusion matrix for seven expressions on the BU-3DFE database in
the cross-database experiment.
Empty Cell| Neutral| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---|---
Neutral| 116| 1| 0| 4| 0| 24| 87
Angry| 77| 28| 25| 0| 0| 5| 22
Disgust| 23| 8| 62| 3| 9| 10| 44
Fear| 29| 0| 3| 7| 11| 25| 57
Happy| 36| 3| 8| 2| 100| 14| 11
Sad| 21| 7| 0| 3| 3| 54| 80
Surprise| 1| 3| 2| 0| 3| 5| 155
Table 17. Confusion matrix for six expressions on the JAFFE database.
Empty Cell| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Angry| 168| 32| 26| 10| 32| 2
Disgust| 79| 129| 21| 1| 30| 1
Fear| 38| 17| 98| 6| 72| 57
Happy| 9| 6| 6| 224| 19| 24
Sad| 53| 26| 42| 14| 123| 21
Surprise| 7| 1| 61| 17| 21| 163
Table 18. Confusion matrix for seven expressions on the JAFFE database.
Empty Cell| Neutral| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---|---
Neutral| 156| 33| 3| 4| 22| 8| 35
Angry| 6| 175| 17| 28| 2| 39| 3
Disgust| 5| 84| 118| 12| 0| 41| 1
Fear| 24| 30| 0| 103| 0| 80| 43
Happy| 46| 12| 1| 3| 210| 6| 10
Sad| 8| 55| 27| 42| 12| 116| 19
Surprise| 66| 2| 0| 36| 11| 6| 149
Table 19. Confusion matrix for six expressions on the JAFFE database in the
cross-database experiment.
Empty Cell| Angry| Disgust| Fear| Happy| Sad| Surprise---|---|---|---|---|---|---
Angry| 2| 2| 2| 2| 11| 11
Disgust| 1| 3| 0| 0| 7| 18
Fear| 1| 0| 2| 0| 9| 20
Happy| 1| 0| 2| 18| 11| 0
Sad| 0| 2| 0| 1| 18| 10
Surprise| 1| 0| 0| 1| 3| 25
Table 20. Confusion matrix for seven expressions on the JAFFE database in the
cross-database experiment.
Empty Cell| Neutral| Angry| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---|---
Neutral| 4| 0| 0| 0| 0| 20| 5
Angry| 5| 4| 2| 0| 3| 10| 6
Disgust| 3| 2| 4| 0| 1| 7| 12
Fear| 0| 0| 0| 3| 0| 9| 20
Happy| 2| 2| 0| 2| 17| 8| 1
Sad| 1| 2| 2| 0| 1| 18| 7
Surprise| 0| 1| 0| 0| 1| 2| 26
Recommended articles"
136,138,Facial expression recognition with identity and emotion joint learning,"['M Li', 'H Xu', 'X Huang', 'Z Song', 'X Liu']",2018,120,Affective Faces Database,"FER, facial expression recognition","In this work, besides training deep-learned facial expression feature (emotional  latent face  identity feature such as the shape or appearance of face. We propose an identity and emotion",No DOI,… Transactions on affective …,https://ieeexplore.ieee.org/document/8528894,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
137,139,Facial expression recognition: A survey,"['Y Huang', 'F Chen', 'S Lv', 'X Wang']",2019,189,"Binghamton University 3D Facial Expression, Japanese Female Facial Expression, MMI Facial Expression",FER,-of-the-art FER approaches are presented and analysed.  FER datasets and summarise  four FER-related elements of datasets that may influence the choosing and processing of FER,No DOI,Symmetry,https://www.sciencedirect.com/science/article/pii/S1877050915021225,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Facial Expression Recognition: A Survey
Search 
## Outline
1. Abstract
2. 3. Keywords
4. References
## Cited by (142)
## Procedia Computer Science
Volume 58, 2015, Pages 486-491
# Facial Expression Recognition: A Survey?
Author links open overlay panelJyoti Kumari, R. Rajesh, K.M. Pooja
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.procs.2015.08.011Get rights and content
Under a Creative Commons license
open access
## Abstract
Automatic facial expression recognition system has many applications
including, but not limited to, human behavior understanding, detection of
mental disorders, and synthetic human expressions. Two popular methods
utilized mostly in the literature for the automatic FER systems are based on
geometry and appearance. Even though there is lots of research using static
images, the research is still going on for the development of new methods
which would be quiet easy in computation and would have less memory usage as
compared to previous methods. This paper presents a quick survey of facial
expression recognition. A comparative study is also carried out using various
feature extraction techniques on JAFFE dataset.
* Previous article in issue
* Next article in issue
## Keywords
Facial Expression Recognition(FER)
LBP
LDP
LGC
HOG ;
View PDF
Special issue articlesRecommended articles"
138,140,Facial expressions of emotion (KDEF): Identification under different display-duration conditions,"['MG Calvo', 'D Lundqvist']",2008,671,Karolinska Directed Emotional Faces,facial expression recognition,"expression of each model is identified. These data have provided researchers on the processing  of affective facial expressions  face database, the Karolinska Directed Emotional Faces (",No DOI,Behavior research methods,https://link.springer.com/article/10.3758/BRM.40.1.109,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
139,141,Fast facial emotion recognition using convolutional neural networks and Gabor filters,"['MMT Zadeh', 'M Imani', 'B Majidi']",2019,110,Affective Faces Database,neural network,face databases are not reliable for the real world applications. This article presents a new  database called RAF-DB that  [13] proposed to train a CNN network for the final face emotion,No DOI,2019 5th Conference on …,https://ieeexplore.ieee.org/document/8734943,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
140,142,Former-dfer: Dynamic facial expression recognition transformer,"['Z Zhao', 'Q Liu']",2021,116,Static Facial Expression in the Wild,FER,"According to the data type, the FER can be divided into static FER (SFER) and dynamic  FER (DFER) • We propose a dynamic facial expression recognition transformer for the in-the-wild",No DOI,Proceedings of the 29th ACM International Conference …,https://github.com/zengqunzhao/Former-DFER,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
141,143,"Framework for reliable, real-time facial expression recognition for low resolution images","['RA Khan', 'A Meyer', 'H Konik', 'S Bouakaz']",2013,180,MMI Facial Expression,facial expression recognition,"–Kanade (CK+) posed facial expression database, spontaneous expressions of MMI facial  expression database and FG-NET facial expressions and emotions database (FEED) and",No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865513001268,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Framework for reliable, real-time facial expression
recognition for low resolution images
Search 
## Outline
1. 2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Pyramid of local binary pattern
8. 4\. Psycho-visual experiment
9. 5\. Expression recognition framework
10. 6\. Experiment and results
11. 7\. Conclusions and future work
12. Acknowledgment
13. References
Show full outline
## Cited by (120)
## Figures (5)
1. 2. 3. 4. 5.
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Pattern Recognition Letters
Volume 34, Issue 10, 15 July 2013, Pages 1159-1168
# Framework for reliable, real-time facial expression recognition for low
resolution images
Author links open overlay panelRizwan Ahmed Khan a b, Alexandre Meyer a b,
Hubert Konik a c, Saïda Bouakaz a b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2013.03.022Get rights and content
### Highlights
* ?
Facial expressions can be analyzed automatically by mimicking human visual
system.
* ?
Proposed descriptor has strong discriminative ability.* ?
Proposed framework is robust for low resolution images and spontaneous
expressions.
* ?
Proposed framework generalizes well on unseen data.
* ?
The proposed framework can be used for real-time applications.
## Abstract
Automatic recognition of facial expressions is a challenging problem specially
for low spatial resolution facial images. It has many potential applications
in human?computer interactions, social robots, deceit detection, interactive
video and behavior monitoring. In this study we present a novel framework that
can recognize facial expressions very efficiently and with high accuracy even
for very low resolution facial images. The proposed framework is memory and
time efficient as it extracts texture features in a pyramidal fashion only
from the perceptual salient regions of the face. We tested the framework on
different databases, which includes Cohn?Kanade (CK+) posed facial expression
database, spontaneous expressions of MMI facial expression database and FG-NET
facial expressions and emotions database (FEED) and obtained very good
results. Moreover, our proposed framework exceeds state-of-the-art methods for
expression recognition on low resolution images.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
Low resolution images
Local binary pattern
Image pyramid
Salient facial regions
## 1\. Introduction
Communication in any form i.e. verbal or non-verbal is vital to complete
various routine tasks and plays a significant role in daily life. Facial
expression is the most effective form of non-verbal communication and it
provides a clue about emotional state, mindset and intention (Ekman, 2001).
Human visual system (HVS) decodes and analyzes facial expressions in real time
despite having limited neural resources. As an explanation for such
performance, it has been proposed that only some visual inputs are selected by
considering ?salient regions? (Zhaoping, 2006), where ?salient? means most
noticeable or most important.
For computer vision community it is a difficult task to automatically
recognize facial expressions in real-time with high reliability. Variability
in pose, illumination and the way people show expressions across cultures are
some of the parameters that make this task difficult. Low resolution input
images makes this task even harder. Smart meeting, video conferencing and
visual surveillance are some of the real world applications that require
facial expression recognition system that works adequately on low resolution
images. Another problem that hinders the development of such system for real
world application is the lack of databases with natural displays of
expressions (Valstar and Pantic, 2010). There are number of publicly available
benchmark databases with posed displays of the six basic emotions (Ekman,
1971) exist but there is no equivalent of this for spontaneous basic emotions.
While, it has been proved that Spontaneous facial expressions differ
substantially from posed expressions (Bartlett et al., 2002). In this work, we
propose a facial expression recognition system that caters for illuminationchanges and works equally well for low resolution as well as for good
quality/high resolution images. We have tested our proposed system on
spontaneous facial expressions as well and recorded encouraging results.
We propose a novel descriptor for facial features analysis, pyramid of local
binary pattern (PLBP) (refer Section 3). PLBP is a spatial representation of
local binary pattern (LBP) (Ojala et al., 1996) and it represents stimuli by
its local texture (LBP) and the spatial layout of the texture. We combined
pyramidal approach with LBP descriptor for facial feature analysis as this
approach has already been proved to be very effective in a variety of image
processing tasks (Hadjidemetriou et al., 2004). Thus, the proposed descriptor
is a simple and computationally efficient extension of LBP image
representation, and it shows significantly improved performance for facial
expression recognition tasks for low resolution images. We base our framework
for automatic facial expression recognition (FER) on human visual system (HVS)
(refer Section 5), so it extracts PLBP features only from the salient regions
of the face. To determine which facial region(s) are the most important or
salient according to HVS, we conducted a psycho-visual experiment using an
eye-tracker (refer Section 4). We considered six universal facial expressions
for psycho-visual experimental study as these expressions are proved to be
consistent across cultures (Ekman, 1971). These six expressions are anger,
disgust, fear, happiness, sadness and surprise. The novelty of the proposed
framework is that, it is illumination invariant, reliable on low resolution
images and works adequately for both i.e. posed and spontaneous expressions.
Generally, facial expression recognition system consists of three steps: face
detection, feature extraction and expression classification. The same has been
shown in Fig. 1. In our framework we tracked face/salient facial regions using
Viola?Jones object detection algorithm (Viola and Jones, 2001) as it is the
most cited and considered the fastest and most accurate pattern recognition
method for face detection (Kolsch and Turk, 2004). The second step in the
framework is feature extraction, which is the area where this study
contributes. The optimal features should minimize within-class variations of
expressions, while maximize between class variations. If inadequate features
are used, even the best classifier could fail to achieve accurate recognition
(Shan et al., 2009). Section 3 presents the novel method for facial features
extraction which is based on human visual system (HVS). To study and
understand HVS we performed psycho-visual experiment. Psycho-visual
experimental study is briefly described in Section 4. Expression
classification or recognition is the last step in the pipeline. In literature
two different ways are prevalent to recognize expressions i.e. direct
recognition of prototypic expressions or recognition of expressions through
facial action coding system (FACS) action units (AUs) (Ekman and Friesen,
1978). In our proposed framework, which is described in Section 5 we directly
classify six universal prototypic expressions (Ekman, 1971). The performance
of the framework is evaluated for five different classifiers (from different
families i.e. classification tree, instance based learning, SVM, etc.) and
results are presented in Section 6. Next section presents the brief literature
review for facial features extraction methods.
1. Download: Download high-res image (83KB)
2. Download: Download full-size image
Fig. 1. Basic structure of facial expression recognition system pipeline.
## 2\. Related work
In the literature, various methods are employed to extract facial features and
these methods can be categorized either as appearance-based methods or
geometric feature-based methods._Appearance-based methods._ One of the widely studied method to extract
appearance information is based on Gabor wavelets (Littlewort et al., 2006,
Tian, 2004, Donato et al., 1999). Generally, the drawback of using Gabor
filters is that it produces extremely large number of features and it is both
time and memory intensive to convolve face images with a bank of Gabor filters
to extract multi-scale and multi-orientational coefficients. Another promising
approach to extract appearance information is by using Haar-like features, see
Yang et al. (2010). Recently, texture descriptors and classification methods
i.e. local binary pattern (LBP) (Ojala et al., 1996) and local phase
quantization (LPQ) (Ojansivu and Heikkilä, 2008) are also studied to extract
appearance-based facial features. Zhao and Pietikäinen (2007) proposed to
model texture using volume local binary patterns (VLBP) an extension to LBP,
for expression recognition.
_Geometric-based methods._ Geometric feature-based methods (Zhang and Ji,
2005, Pantic and Patras, 2006, Valstar et al., 2005, Bai et al., 2009)
extracts shapes and locations of facial components information to form a
feature vector. The problem with using geometric feature-based methods is that
they usually require accurate and reliable facial feature detection and
tracking which is difficult to achieve in many real world applications where
illumination changes with time and images are recorded in very low resolution.
Generally, we have found that all the reviewed methods for automatic facial
expression recognition are computationally expensive and usually requires
dimensionally large feature vector to complete the task. This explains their
inability for real-time applications. Secondly, in literature, very few
studies exist that tackles the issue of expressions recognition from low
resolution images, this adds to lack of applicability of expression
recognition system for real world applications. Lastly, all of the reviewed
methods, spend computational time on whole face image or divides the facial
image based on some mathematical or geometrical heuristic for features
extraction. We argue that the task of expression analysis and recognition
could be done in more conducive manner, if only some regions are selected for
further processing (i.e. salient regions) as it happens in human visual
system. Thus, our contributions in this study are:
* 1.
We propose a novel descriptor for facial expression analysis i.e. pyramid of
local binary pattern (PLBP), which outperforms state-of-the-art methods for
expression recognition on low resolution images (spatially degraded images).
It also performs better than other state-of-the-art methods for good
resolution images (with no degradation).
* 2.
As the proposed framework is based on human visual system it algorithmically
processes only salient facial regions which reduces the length of feature
vector. This reduction in feature vector length makes the proposed framework
suitable for real-time applications due to minimized computational complexity.
## 3\. Pyramid of local binary pattern
The proposed framework creates a novel feature space by extracting proposed
PLBP (pyramid of local binary pattern) features only from the visually salient
facial region (see Section 4 for psycho-visual experiment). PLBP is a
_pyramidal-based spatial_ representation of local binary pattern (LBP)
descriptor. PLBP represents stimuli by their local texture (LBP) and the
spatial layout of the texture. The spatial layout is acquired by tiling the
image into regions at multiple resolutions. The idea is illustrated in Fig. 2.
If only the coarsest level is used, then the descriptor reduces to a global
LBP histogram. Comparing to the multi-resolution LBP of Ojala et al. (2002),our descriptor selects samples in a more uniformly distributed manner, whereas
Ojala?s LBP takes samples centered around a point leading to missing some
information in the case of face (which is different than a repetitive
texture).
1. Download: Download high-res image (187KB)
2. Download: Download full-size image
Fig. 2. Pyramid of local binary pattern. First row: stimuli at two different
pyramid levels, second row: histograms of LBP at two respective levels, and
third row: final descriptor.
LBP features were initially proposed for texture analysis (Ojala et al.,
1996), but recently they have been successfully used for facial expression
analysis (Zhao and Pietikäinen, 2007, Shan et al., 2009). The most important
property of LBP features are their tolerance against illumination changes and
their computational simplicity (Ojala and Pietikäinen, 1999, Ojala et al.,
1996, Ojala et al., 2002). The operator labels the pixels of an image by
thresholding the 3 × 3 neighborhood of each pixel with the center value and
considering the result as a binary number. Then the histogram of the labels
can be used as a texture descriptor. Formally, LBP operator takes the
form:(1)LBP(xc,yc)=?n=07s(in-ic)2nwhere in this case _n_ runs over the eight
neighbors of the central pixel c, ic and in are the gray level values at _c_
and _n_ and s(u) is 1 if u?0 or 0 otherwise.
Later, the LBP operator is extended to use neighborhood of different sizes
(Ojala et al., 2002) as the original operator uses 3 × 3 neighborhood. Using
circular neighborhoods and bilinearly interpolating the pixel values allow any
radius and number of pixels in the neighborhood. The LBP operator with _P_
sampling points on a circular neighborhood of radius _R_ is given
by:(2)LBPP,R=?p=0P-1s(gp-gc)2pwhere, gc is the gray value of the central
pixel, gp is the value of its neighbors, _P_ is the total number of involved
neighbors and _R_ is the radius of the neighborhood.
Another extension to the original operator is the definition of _uniform
patterns_ , which can be used to reduce the length of the feature vector and
implement a simple rotation-invariant descriptor. A local binary pattern is
called uniform if the binary pattern contains at most two bitwise transitions
from 0 to 1 or vice versa when the bit pattern is traversed circularly.
Accumulating the patterns which have more than two transitions into a single
bin yields an LBP operator, denoted LBPP,Ru2 patterns. These binary patterns
can be used to represent texture primitives such as spot, flat area, edge and
corner.
We extend LBP operator so that the stimuli can be represented by its local
texture and the spatial layout of the texture. We call this extended LBP
operator as pyramid of local binary pattern or PLBP. PLBP creates the spatial
pyramid by dividing the stimuli into finer spatial sub-regions by iteratively
doubling the number of divisions in each dimension. It can be observed from
Fig. 2 that the pyramid at level _l_ has 2 _l_ sub-regions along each
dimension (R0,?Rm-1). Histograms of LBP features at the same levels are
concatenated. Then, their concatenation at different pyramid levels gives
final PLBP descriptor (as shown in Fig. 2). It can be defined
as:(3)Hi,j=?l?xyI{fl(x,y)=i}I{(x,y)?Rl}where l=0,?,m-1, i=0,?,n-1. _n_ is the
number of different labels produced by the LBP operator and(4)I(A)=1ifAis
true0otherwiseWhile, the dimensionality of the descriptor can be calculated
by:(5)N?l4lWhere, in our experiment (see Section 6) l=1 and _N_ = 59 as we
created pyramid up to level 1 and extracted 59 LBP features using LBP8,2u2
operator, which denotes a uniform LBP operator with eight sampling pixels in a
local neighborhood region of radius 2. This pattern reduces the histogram from256 to 59 bins. In our experiment we obtained 295 dimensional feature vector
from one facial region i.e. mouth region (59 dimensions/sub-region), since we
executed the experiment with the pyramid of level 1 (the same is shown in Fig.
2).
### 3.1. Novelty of the proposed descriptor
There exist some methods in literature that uses pyramid of LBP for different
applications and they look similar to our proposed descriptor, i.e. Wang et
al., 2011, Guo et al., 2010, Moore and Bowden, 2011. Our proposition is novel
and there exist differences in the methodology that creates differences in the
extracted information. Method for face recognition proposed in Wang et al.
(2011) creates pyramid before applying LBP operator by down sampling original
image i.e. scale-space representation, whereas we propose to create the
spatial pyramid by dividing the stimuli into finer spatial sub-regions by
iteratively doubling the number of divisions in each dimension. Secondly, our
approach reduces memory consumption (do not requires to store same image in
different resolutions) and is computationally more efficient. Guo et al.
(2010) proposed approach for face and palmprint recognition based on
multiscale LBP. Their proposed method seems similar to our method for
expression recognition but how multiscale analysis is achieved deviates our
approach. Approach proposed in Guo et al. (2010) achieves multiscale analysis
using different values of _P_ and _R_ , where LBP(P,R) denotes a neighborhood
of _P_ equally spaced sampling points on a circle of radius _R_ (discussed
earlier). Same approach has been applied by Moore and Bowden (2011) for facial
features analysis. Generally the drawback of using such approach is that it
increases the size of the feature histogram and increases the computational
cost. Moore and Bowden (2011) reports dimensionality of feature vector as high
as 30,208 for multiscale face expression analysis as compared to our
proposition which creates 590 dimensional feature vector (see Section 5) for
the same task. We achieve the task of multiscale analysis much more
efficiently than any other earlier proposed methods. By the virtue of
efficient multiscale analysis our framework can be used for real time
applications (see Table 1 for the time and memory consumption comparison)
which is not the case with other methods.
Table 1. Comparison of time and memory consumption.
Empty Cell| Shan et al. (2009)(a)| Shan et al. (2009)(b)| Bartlett et al.
(2003)| PLBP
---|---|---|---|---
Memory (feature dimension)| 2478| 42,650| 92,160| 590
Time (feature extraction time)| 0.03 s| 30 s| ?| 0.01 s
As mentioned earlier, we base our framework for facial expression recognition
on human visual system (HVS), which selects only few facial regions (salient)
to extract information. In order to determine the saliency of facial region(s)
for a particular expression, we conducted psycho-visual experiment with the
help of an eye-tracker. Next section briefly explains the psycho-visual
experimental study.
## 4\. Psycho-visual experiment
The aim of our experiment was to record the eye movement data of human
observers in free viewing conditions. The data were analyzed in order to find
which components of face are salient for specific displayed expression.
### 4.1. Participants, apparatus and stimuli
Eye movements of fifteen human observers were recorded using video based eye-
tracker (EyelinkII system, SR Research), as the subjects watched the
collection of 54 videos selected from the extended Cohn?Kanade (CK+) database
(Lucey et al., 2010), showing one of the six universal facial expressions(Ekman, 1971). Observers include both male and female aging from 20 to 45
years with normal or corrected to normal vision. All the observers were naïve
to the purpose of an experiment.
### 4.2. Eye movement recording
Eye position was tracked at 500 Hz with an average noise less than 0.01°. Head
mounted eye-tracker allows flexibility to perform the experiment in free
viewing conditions as the system is designed to compensate for small head
movements.
### 4.3. Psycho-visual experiment results
In order to statistically quantify which region is perceptually more
attractive for specific expression, we have calculated the average percentage
of trial time observers have fixated their gazes at specific region(s) in a
particular time period. As the stimuli used for the experiment is dynamic i.e.
video sequences, it would have been incorrect to average all the fixations
recorded during trial time (run length of the video) for the data analysis as
this could lead to biased analysis of the data. To meaningfully observe and
analyze the gaze trend across one video sequence we have divided each video
sequence in three mutually exclusive time periods. The first time period
correspond to initial frames of the video sequence i.e. neutral face. The last
time period encapsulates the frames where the expression is shown with full
intensity (apex frames). The second time period is a encapsulation of the
frames which has a transition of facial expression i.e. transition from
neutral face to the beginning of the desired expression (i.e. neutral to the
onset of the expression). Then the fixations recorded for a particular time
period are averaged across fifteen observers. For drawing the conclusions we
considered second and third time periods as they have the most significant
information in terms of specific displayed expression. Conclusions drawn are
summarized in Fig. 3. Refer Khan et al. (2012a) for the detailed explanation
of the psycho-visual experimental study.
1. Download: Download high-res image (235KB)
2. Download: Download full-size image
Fig. 3. Summary of the facial regions that emerged as salient for six
universal expressions. Salient regions are mentioned according to their
importance (for example facial expression of ?fear? has two salient regions
but mouth is the most important region according to HVS).
Conclusions drawn from this psycho-visual experimental study suggests that for
some expressions (i.e. happiness, sadness and surprise) only one facial region
is salient while for other expressions two facial regions are salient or
contain most discriminative information. We argue that the task of expression
analysis and recognition could be done in more conducive manner, if only same
perceptual salient regions are selected for further processing as it happens
in human visual system. By processing only perceptual salient regions the
proposed framework (refer Section 5) reduces the feature vector
dimensionality. This reduction in feature vector length makes the proposed
framework suitable for real-time applications due to minimized computational
complexity.
Machine learning research community has also proposed a mathematical model
called feature subset selection (FSS), to reduce feature vector
dimensionality. The objective of FSS is to reduce the number of features used
to characterize a dataset so as to improve a learning algorithm performance on
a given task (Aha and Bankert, 1994), and it has recently been used for facial
expression recognition application (Dornaika et al., 2011). Most FSS methods
involve evaluating different feature subsets, employ some criterion such as
probability of error (Jain and Zongker, 1997). One difficulty with thisapproach when applied to real problems with large feature dimensionality, is
the high computational complexity involved in searching the exponential space
of feature subsets (Guo and Dyer, 2003). Jain and Zongker (1997) evaluated
different search algorithms for FSS and found that the sequential forward
floating selection (SFFS) algorithm proposed by Pudil et al. (1994) performed
best. However, SFFS is very time consuming when the number of features is
large. For example, Vailaya (2000) used the SFFS method to select 67 features
from 600 for a two-class problem and reported that SFFS required 12 days of
computation time.
As the proposed framework is based on psycho-visual experimental study and the
notion of saliency, it does not employ FSS technique to reduce feature vector
dimensionality. Rather reduction in feature vector dimensionality is inherited
due to the notion of saliency. Proposed framework (Section 5) extracts PLBP
features only from the perceptual salient facial region(s) which contains
highly discriminative information as suggested by Itti et al. (1998) and
proved by the recorded results (refer Table 1, Table 2). Specifically Table 1
proves that the dimensionality of the proposed descriptor is very low as
compared to other state-of-the-art descriptor. Thus, making it suitable for
real-time applications. In the perspective of this study, FSS still can be
utilized to reduce the dimensionality of proposed descriptor even futher (i.e.
to reduce dimensionality from hundreds to tens) on the cost of increasing
computational complexity.
Table 2. Comparison with the state-of-the-art methods for posed expressions.
Empty Cell| Sequence num.| Class num.| Performance measure| Recog. rate (%)
---|---|---|---|---
Littlewort et al. (2006)| 313| 7| Leave-one-out| 93.3
Zhao and Pietikäinen (2007)| 374| 6| 2-Fold| 95.19
Zhao and Pietikäinen (2007)| 374| 6| 10-Fold| 96.26
Kotsia et al. (2008)| 374| 6| 5-Fold| 94.5
Tian (2004)| 375| 6| ?| 93.8
Yang et al. (2010)(a)| 352| 6| 66% split| 92.3
Yang et al. (2010)(b)| 352| 6| 66% split| 80
**Ours**| **309**| **6**| **10-Fold**| **96.7**
**Ours**| **309**| **6**| **2-Fold**| **95.2**
## 5\. Expression recognition framework
Feature selection along with the region(s) from where these features are going
to be extracted is one of the most important step to recognize expressions. As
the proposed framework draws its inspiration from the human visual system
(HVS), it extracts proposed features i.e. PLBP, only from the perceptual
salient facial region(s) which were determined through psycho-visual
experiment. Schematic overview of the framework is presented in Fig. 4. Steps
of the proposed framework are as follows:
* 1.
The framework first localizes salient facial regions using Viola?Jones object
detection algorithm (Viola and Jones, 2001). We selected this algorithm as it
is the most cited and considered the fastest and most accurate pattern
recognition method for face and facial region detection (Kolsch and Turk,
2004).
* 2.
Then, the framework extracts PLBP features from the mouth region, feature
vector of 295 dimensions (f1,?,f295). The classification (?Classifier-a? in
Fig. 4) is carried out on the basis of extracted features in order to make two
groups of facial expressions. First group comprises of those expressions that
has one perceptual salient region i.e. happiness, sadness and surprise whilethe second group is composed of those expressions that have two or more
perceptual salient regions i.e. anger, fear and disgust (see Section 4.3).
Purpose of making two groups of expressions is to reduce feature extraction
computational time.
* 3.
If the stimuli is classified in the first group, then it is classified either
as happiness, sadness or surprise by the ?Classifier-b? using already
extracted PLBP features from the mouth region.
* 4.
If the stimuli is classified in the second group, then the framework extracts
PLBP features from the eyes region and concatenates them with the already
extracted PLBP features from the mouth region, feature vector of 590
dimensions (f1,?,f295+f1,?,f295). Then, the concatenated feature vector is fed
to the classifier (?Classifier-c?) for the final classification. It is worth
mentioning here that for the expression of ?disgust? nose region emerged as
one of the salient regions but the framework do not explicitly extracts
features from this region. This is due to the fact that, the region of nose
that emerged as salient is the upper nose (wrinkles) area which is connected
and already included in the localization of the eyes region, refer Fig. 3.
1. Download: Download high-res image (182KB)
2. Download: Download full-size image
Fig. 4. Schematic overview of the framework.
## 6\. Experiment and results
We performed person-independent facial expression recognition using proposed
PLBP features.1 We performed four experiments to test different scenarios.
* 1.
First experiment was performed on the extended Cohn?Kanade (CK+) database
(Lucey et al., 2010). This database contains 593 sequences of posed universal
expressions.
* 2.
Second experiment was performed to test the performance of the proposed
framework on low resolution image sequences.
* 3.
Third experiment tests the robustness of the proposed framework when
generalizing on a new dataset.
* 4.
Fourth experiment was performed on the MMI facial expression database (parts
IV and V of the database) (Valstar and Pantic, 2010) which contains
spontaneous/natural expressions.
For the first two experiments we used all the 309 sequences from the CK+
database which have FACS coded expression label (Ekman and Friesen, 1978). The
experiment was carried out on the frames which covers the status of onset to
apex of the expression, as done by Yang et al. (2010). Region of interest was
obtained automatically by using Viola?Jones object detection algorithm (Viola
and Jones, 2001) and processed to obtain PLBP feature vector. We extracted LBP
features only from the salient region(s) using LBP8,2u2 operator which denotes
a uniform LBP operator with eight sampling pixels in a local neighborhood
region of radius 2. Only exception was in the second experiment, when we
adopted LBP4,1u2 operator when the spatial facial resolution gets smaller than
36 × 48.
In our framework we created image pyramid up to level 1, so in turn got five
sub-regions from one facial region i.e. mouth region (see Fig. 2). In total we
obtained 295 dimensional feature vector (59 dimensions/sub-region). As
mentioned earlier we adopted LBP4,1u2 operator when the spatial facialresolution was 18 × 24. In this case we obtained 75 dimensional feature vector
(15 dimensions/sub-region).
We recorded correct classification accuracy in the range of 95% for image
pyramid level 1. We decided not to test framework with further image pyramid
levels as it would double the size of feature vector and thus increase the
feature extraction time and likely would add few percents in the accuracy of
the framework which will be insignificant for a framework holistically.
### 6.1. First experiment: posed expressions
This experiment measures the performance of the proposed framework on the
classical database i.e. extended Cohn?Kanade (CK+) database (Lucey et al.,
2010). Most of the methods in literature report their performance on this
database, so this experiment could be considered as the benchmark experiment
for facial expression recognition framework.
The performance of the framework was evaluated for five different classifiers:
* 1.
Support vector machine (SVM) with _?_ 2 kernel and ?=1
* 2.
C4.5 decision tree (DT) with reduced-error pruning
* 3.
Random forest (RF) of 10 trees
* 4.
2 Nearest neighbor (2NN) based on Euclidean distance
* 5.
Naive Bayes (NB) classifier
Above mentioned classifiers are briefly described below.
Support vector machine _(SVM)._ SVM performs an implicit mapping of data into
a higher dimensional feature space, and then finds a linear separating
hyperplane with the maximal margin to separate data in this higher dimensional
space (Vapnik, 1995). Given a training set of labeled examples
{(xi,yi),i=1,?,l} where xi?Rn and yi?{-1,1}, a new test example _x_ is
classified by the following function:(6)f(x)=sgn?i=1l?iyiK(xi,x)+bwhere ?i are
Langrange multipliers of a dual optimization problem that describe the
separating hyperplane, K(.,.) is a kernel function, and _b_ is the threshold
parameter of the hyperplane. We used Chi-Square kernel as it is best suited
for histograms. It is given by:(7)K(x,y)=1-?i2×(xi-yi)2(xi+yi)
_Classification Trees._ A classification tree is a classifier composed by
nodes and branches which break the set of samples into a set of covering
decision rules. In each node, a single test is made to obtain the partition.
The starting node is called the root of the tree. In the final nodes or
leaves, a decision about the classification of the case is made. In this work,
we have used C4.5 paradigm (Quinlan, 1993). Random forest (RFs) are
collections of decision trees (DTs) that have been constructed randomly. RFs
generally performs better than DT on unseen data.
_Instance Based Learning. k_ -NN classifiers are instance-based algorithms
taking a conceptually straightforward approach to approximate real or discrete
valued target functions. The learning process consists in simply storing the
presented data. All instances correspond to points in an _n_ -dimensional
space and the nearest neighbors of a given query are defined in terms of the
standard Euclidean distance. The probability of a query _q_ belonging to a
class _c_ can be calculated as
follows:(8)p(c|q)=?k?KWk·1(kc=c)?k?KWk(9)Wk=1d(k,q)_K_ is the set of nearest
neighbors, _kc_ the class of _k_ and d(k,q) the Euclidean distance of _k_ from
_q_.
_Naive Bayes classifiers._ The naive-Bayes (NB) classifier uses the Bayestheorem to predict the class for each case, assuming that the predictive genes
are independent given the category. To classify a new sample characterized by
_d_ genes X=(X1,X2,?,Xd), the NB classifier applies the following
rule:(10)CN-B=argmaxcj?Cp(cj)?i=1dp(xi|cj)where CN-B denotes the class label
predicted by the naive-Bayes classifier and the possible classes of the
problem are grouped in C={c1,?,cl}.
#### 6.1.1. Results
The framework achieved average recognition rate of 96.7%, 97.9%, 96.2%, 94.7%
and 90.2% for SVM, 2 nearest neighbor (2NN), random forest (RF), C4.5 decision
tree (DT) and naive-Bayes (NB) respectively using 10-fold cross validation
technique. One of the most interesting aspects of our approach is that it
gives excellent results for a simple 2NN classifier which is a non-parametric
method. This points to the fact that framework do not need computationally
expensive methods such as SVM, random forests or decision trees to obtain good
results. In general, the proposed framework achieved high expression
recognition accuracies irrespective of the classifiers, proves the descriptive
strength of the extracted features (feature minimizes within-class variations
of expressions, while maximizes between class variations). For comparison and
reporting results, we have used the classification results obtained by the SVM
as it is the most cited method for classification in the literature.
#### 6.1.2. Comparisons
We chose to compare average recognition performance of our framework with the
framework proposed by Shan et al. (2009) with different SVM kernels. Our
choice was based on the fact that both have common underlying descriptor i.e.
local binary pattern (LBP), secondly framework proposed by Shan et al. (2009)
is highly cited in the literature. Our framework obtained average recognition
percentage of 93.5% for SVM linear kernel while for the same kernel Shan et
al. (2009) have reported 91.5%. For SVM with polynomial kernel and SVM with
RBF kernel our framework achieved recognition accuracy of 94.7% and 94.9%
respectively, as compared to 91.5% and 92.6%.
In terms of time and memory cost of feature extraction process, we have
measured and compared our descriptor with the frameworks proposed by Shan et
al. (2009) and Bartlett et al. (2003). The results are presented in Table 1.
Shan et al. (2009) have reported their results for two set of features i.e.
LBP and Gabor. In Table 1 ?Shan et al. (2009)(a)? corresponds to results
obtained with LBP features, while ?Shan et al. (2009)(b)? corresponds to Gabor
feature result. Table 1 shows the effectiveness of the proposed descriptor for
facial feature analysis i.e. PLBP, for real-time applications as it is memory
efficient and its extraction time is much lower than other compared descriptor
(see Section 5 for the dimensionality calculation). In Table 1 feature
dimension reported are stored in a data type ?float? and float occupies four
bytes. The proposed framework is compared with the other state-of the-art
frameworks using same database (i.e. Cohn?Kanade database) and the results are
presented in Table 2.
Table 2 shows the comparison of the achieved average recognition rate of the
proposed framework with the state-of-the-art methods using same database (i.e.
Cohn?Kanade database). Results from Yang et al. (2010) are presented for the
two configurations. ?Yang et al. (2010)(a)? shows the result when the method
was evaluated for the last three frames from the sequence while ?Yang et al.
(2010)(b)? presents the reported result for the frames which encompasses the
status from onset to apex of the expression. It can be observed from Table 2
that the proposed framework is comparable to any other state-of-the-art method
in terms of expression recognition accuracy. The method discussed in ?Yang et
al. (2010)(b)? is directly comparable to our method (frames which covers thestatus of onset to apex of the expression). In this configuration, our
framework is better in terms of average recognition accuracy.
In general, Table 1, Table 2 show that the framework is better than the state-
of-the-art frameworks in terms of average expression recognition performance,
time and memory costs of feature extraction processes. These results show that
the system could be used with the high degree of confidence for real-time
applications as its unoptimized Matlab implementation runs at more than 30
frames/second.
### 6.2. Second experiment: low resolution image sequences
Most of the existing state-of-the-art systems for expressions recognition
report their results on high resolution images without reporting results on
low resolution images. As mentioned earlier there are many real world
applications that require expression recognition system to work amicably on
low resolution images. Smart meeting, video conferencing and visual
surveillance are some examples of such applications. To compare with Tian?s
work (Tian, 2004), we tested our proposed framework on low resolution images
of four different facial resolutions (144 × 192, 72 × 96, 36 × 48, 18 × 24)
based on Cohn?Kanade database. Tian?s work can be considered as the pioneering
work for low resolution image facial expression recognition. Fig. 5 shows the
images at different spatial resolution along with the average recognition
accuracy achieved by the different methods. Low resolution image sequences
were obtained by down sampling the original sequences. All the other
experimental parameters i.e. descriptor, number of sequences and region of
interest, were same as mentioned earlier in the Section 6.
1. Download: Download high-res image (189KB)
2. Download: Download full-size image
Fig. 5. Robustness of different methods for facial expression recognition with
decreasing image resolution. PHOG[ICIP] corresponds to framework proposed by
Khan et al. (2012b), Gabor[CVPRW] corresponds to Tian?s work (Tian, 2004),
LBP[JIVC] and Gabor[JIVC] corresponds to results reported by Shan et al.
(2009)
Fig. 5 reports the recognition results of the proposed framework with the
state-of-the-art methods on four different low facial resolution images.
Reported results of our proposed method i.e. are obtained using support vector
machine (SVM) with _?_ 2 kernel and ?=1. In Fig. 5 recognition curve for our
proposed method is shown as _PLBP-SVM_ , recognition curves of LBP (Shan et
al., 2009) and Gabor (Shan et al., 2009) are shown as _LBP[JIVC]_ and
_Gabor[JIVC]_ respectively, curve for Tian?s work (Tian, 2004) is shown as
_Gabor[CVPRW]_ while Khan et al. (2012b) proposed system?s curve is shown as
_PHOG[ICIP]_. Results reports in LBP (Shan et al., 2009) and Gabor (Shan et
al., 2009), the different facial image resolution are 110 × 150, 55 × 75, 27 ×
37 and 14 × 19 which are comparable to the resolutions of 144 × 192, 72 × 96,
36 × 48, 18 × 24 pixels in our experiment. Referenced figure shows the
supremacy of the proposed framework for low resolution images. Specially for
the smallest tested facial image resolution (18 × 24) our framework performs
much better than any other compared state-of-the-art method.
Results from the first and second experiment show that the proposed framework
for facial expression recognition works amicably on classical dataset (CK
dataset) and its performance is not effected significantly for low resolution
images. Secondly, the framework has a very low memory requirement and thus it
can be utilized for real-time applications.
### 6.3. Third experiment: generalization on the new dataset
The aim of this experiment is to study how well the proposed framework
generalizes on the new dataset. Valstar et al. (2005) have reported such dataearlier to show generalization ability of their expression recognition system.
Thus, this experiment helps to understand how the framework will behave when
it will be used to classify expressions in real life videos. We used image
sequences from CK+ dataset and FG-NET FEED (facial expressions and emotion
database) (Wallhoff, 2006). FG-NET FEED contains 399 video sequences across 18
different individuals showing seven facial expressions i.e. six universal
expression (Ekman, 1971) plus one neutral. In this dataset individuals were
not asked to act rather expressions were captured while showing them video
clips or still images i.e. natural expressions.
The experiment was carried out on the frames which covers the status of onset
to apex of the expression as done in the previous experiment. This experiment
was performed in two different scenarios, with the same classifier parameters
as the first experiment:
* (a)
In the first scenario samples from the CK+ database were used for the training
of different classifiers and samples from FG-NET FEED (Wallhoff, 2006) were
used for the testing. Obtained results are presented in Table 3.
Table 3. Average recognition accuracy (%).
Empty Cell| SVM| C4.5 DT| RF| 2NN
---|---|---|---|---
_Training on CK+ database and testing it with FG-NET FEED_
Training samples| 96.7| 94.7| 96.2| 97.9
Test samples| 81.9| 74.8| 79.5| 83.1
_Training on FG-NET FEED and testing it with CK+ database_
Training samples| 92.3| 91.2| 90.5| 93.3
Test samples| 80.5| 77.3| 79| 84.7
* (b)
In the second scenario we used samples from the FG-NET FEED for the training
and testing was carried out with the CK+ database samples. Results obtained
are presented in last two rows of Table 3.
This experiment simulates the real life situation when the framework would be
employed to recognize facial expressions on the unseen data. Obtained results
are presented in Table 3. Reported average recognition percentages for
training phase were calculated using 10-fold cross validation method. It can
be observed from the result that generally average recognition accuracy drops
by 15% across different classifiers during testing phase. This could be due to
the fact that the two databases used for training and testing phase have
inherent differences i.e. played emotions differ from the natural/spontaneous
ones. Obtained results can be probably further improved by training
classifiers on more than one dataset before using in real life scenario.
### 6.4. Fourth experiment: spontaneous expressions
Spontaneous/natural facial expressions differ substantially from posed
expressions (Bartlett et al., 2002). The same has also been proved by
psychophysical work (Ekman, 2001). To test the performance of the proposed
framework on the spontaneous facial expressions we used 392 video segments
from parts IV and V of the MMI facial expression database (Valstar and Pantic,
2010). Parts IV and V of the database contains spontaneous/naturalistic
expressions recorded from 25 participants aged between 20 and 32 years in two
different settings. Due to ethical concerns the database contains only the
video recording of the expressions of happiness, surprise and disgust (Valstar
and Pantic, 2010).
The framework achieved average recognition rate of 91%, 91.4%, 90.3% and 88%
for SVM, 2-nearest neighbor, random forest and C4.5 decision tree respectively
using 10-fold cross validation technique. Algorithm of Park and Kim (2008) forspontaneous expression recognition achieved results for three expressions in
the range of 56?88% for four different configurations which is less than
recognition rate of our proposed algorithm, although results cannot be
compared directly as they used different database.
## 7\. Conclusions and future work
We presented a novel descriptor and framework for automatic and reliable
facial expression recognition. Framework is based on initial study of human
vision and works adequately on posed as well as on spontaneous expressions.
The key conclusions drawn from the study are:
* 1.
Facial expressions can be analyzed automatically by mimicking human visual
system i.e. extracting features only from the salient facial regions.
* 2.
Features extracted using proposed pyramidal local binary pattern (PLBP)
operator have strong discriminative ability as the recognition result for six
universal expressions is not effected by the choice of classifier.
* 3.
The proposed framework is robust for low resolution images, spontaneous
expressions and generalizes well on unseen data.
* 4.
The proposed framework can be used for real-time applications since its
unoptimized Matlab implementation run at more than 30 frames/second on a
Windows 64 bit machine with i7 processor running at 2.4 GHz having 6 GB of
RAM.
In future we plan to investigate the effect of occlusion as this parameter
could significantly impact the performance of the framework for real world
applications. Secondly, the notion of movement could improve the performance
of the proposed framework for real world applications as the experimental
study conducted by Bassili (1979) suggested that dynamic information is
important for facial expression recognition. Another parameter that needs to
be investigated is the variations of camera angle as for many applications
frontal facial pose is difficult to record. Lastly, in future we would also
like to evaluate the proposed framework on streaming artifacts i.e. ringing,
contouring, posterization, etc.
## Acknowledgment
This research work is funded by the Région Rhône-Alpes, France through ARC 6.
Recommended articles"
142,144,Fusing aligned and non-aligned face information for automatic affect recognition in the wild: a deep learning approach,"['BK Kim', 'SY Dong', 'J Roh', 'G Kim']",2016,139,Expression in-the-Wild,deep learning,We evaluate the proposed framework on the facial expression recognition 2013 (FER-2013)   Our final ensemble-based FER system achieves great performance on this in-the-wild,No DOI,Proceedings of the IEEE …,http://ieeexplore.ieee.org/abstract/document/7789677/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
143,145,Fusion of feature sets and classifiers for facial expression recognition,"['THH Zavaschi', 'AS Britto Jr', 'LES Oliveira']",2013,167,Extended Cohn-Kanade,classifier,classifiers based on the under-pinning concept of “over-produce and choose”. The pool of  base classifiers  different databases (JAFFE and Cohn-Kanade) we demonstrate the efficiency,No DOI,Expert Systems with …,https://www.sciencedirect.com/science/article/pii/S095741741200930X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Fusion of feature sets and classifiers for facial expression
recognition
Search 
## Outline
1. Abstract
2. 3. 4. Keywords
5. 1\. Introduction
6. 2\. Methodology overview
7. 3\. Feature sets
8. 4\. Experiments and discussion
9. 5\. Conclusion
10. Acknowledgments
11. References
Show full outline
## Cited by (111)
## Figures (13)
1. 2. 3. 4. 5. 6.
Show 7 more figures
## Tables (7)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Expert Systems with Applications
Volume 40, Issue 2, 1 February 2013, Pages 646-655
# Fusion of feature sets and classifiers for facial expression recognition
Author links open overlay panelThiago H.H. Zavaschi a, Alceu S. Britto Jr. a,
Luiz E.S. Oliveira b, Alessandro L. Koerich a b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.eswa.2012.07.074Get rights and content
## Abstract
This paper presents a novel method for facial expression recognition that
employs the combination of two different feature sets in an ensemble approach.
A pool of base support vector machine classifiers is created using Gaborfilters and Local Binary Patterns. Then a multi-objective genetic algorithm is
used to search for the best ensemble using as objective functions the
minimization of both the error rate and the size of the ensemble. Experimental
results on JAFFE and Cohn-Kanade databases have shown the efficiency of the
proposed strategy in finding powerful ensembles, which improves the
recognition rates between 5% and 10% over conventional approaches that employ
single feature sets and single classifiers.
### Highlights
? A novel method for facial expression recognition that employs an ensemble of
classifiers. ? A pool of base SVM classifiers is created using Gabor filters
and LBP as feature sets. ? A multi-objective genetic algorithm is used to
search for the best ensemble. ? Experiments on JAFFE and Cohn-Kanade databases
show the efficiency of the proposed method.
* Previous article in issue
* Next article in issue
## Keywords
Face recognition
Emotion recognition
Ensemble of classifiers
Feature selection
## 1\. Introduction
Automatic facial expression recognition has been a subject of investigation in
the last years due to the great number of potential day-to-day applications
such as human?computer interaction (HCI), emotion analysis, automated tutoring
systems, smart environments, operator fatigue detection in industries,
interactive video, indexing and retrieval of image and video databases, image
understanding, and synthetic face animation (Aleksic & Katsaggelos, 2006).
Furthermore, automatic facial expression recognition systems can provide a
less intrusive method to apprehend the emotion activity of a person of
interest (Bashyal & Venayagamoorthy, 2008). As pointed out by Lyons, Akamatsu,
Kamachi, and Gyoba (1998), facial expression recognition is also a necessary
step towards a computer facilitated human interaction system as facial
expressions play a significant role in conveying human emotions. Any natural
HCI system thus should take advantage of the human facial expressions.
In 1971, Ekman and Friesen (1971) postulated six primary emotions that possess
each a distinctive content together with a unique facial expression. These
prototypic emotional displays are also referred to as so called basic
emotions. They seem to be universal across human ethnicity and cultures and
comprise happiness, sadness, fear, disgust, surprise and anger. Due to the
advancements accomplished in related research areas such as face detection and
recognition in the beginning of the 90s, researchers renewed the interest for
facial expression recognition (Fasel & Luettinb, 2003). A pioneering work in
this field was presented by Mase and Pentland back in 1991 (Mase & Pentland,
1991).
Since then a lot of effort has been made to build more reliable automatic
facial expression recognition. The methods reported in the literature can be
classified basically into geometry analysis and appearance-based. The former
takes into account some predefined geometric positions, also known as fiducial
points, as facial features to represent facial expressions (Besinger et al.,
2010, Geetha et al., 2009, Pantic and Patras, 2006, Wong and Cho, 2009, Zhang
and Ji, 2005). However, the geometric feature-based representation commonly
requires accurate and reliable facial feature detection and tracking, which is
difficult to accommodate in many situations (Shan, Gong, & McOwan, 2009).
The second approach models the appearance changes of the faces through anholistic spatial analysis. Among the tools used for this approach are
Principal Component Analysis (PCA) (Turk & Pentland, 1991), Independent
Component Analysis (Belhumeur, Hespanha, & Kriegman, 1997), Gabor filters
(Lyons et al., 1999, Tan and Triggs, 2007), and Local Binary Patterns (LBP)
(Ojala et al., 1996, Tan and Triggs, 2007). According to the literature, Gabor
filters lead superior performance for facial analysis and for this reason they
have been widely adopted (Lyons et al., 1999, Xiea et al., 2009, Zhang et al.,
1998). The downside, though, is the elevated computational cost in terms of
time and memory usage. Recently LBP have been introduced as effective
appearance features for facial image analysis (Shan et al., 2005, Shan et al.,
2009, Tan and Triggs, 2007). Experiments have demonstrated that when compared
with Gabor filters, the simple LBP features save much computational resource
whilst retaining facial information in an efficient way (Shan et al., 2009,
Zavaschi et al., 2011).
Though much progress has been made, recognizing facial expressions with a high
accuracy remains difficult due to the subtlety, complexity, and variability of
facial expressions. An efficient way to deal with complex pattern recognition
problems, which is the case of face expression recognition, is to build
ensemble of classifiers to take advantage of the inherent diversity introduced
by classifiers trained with different feature sets (Zavaschi et al., 2011).
Several studies have been published demonstrating the benefits of the
combination paradigm over the individual classifier models (Kuncheva, 2004).
During the last years, a considerable amount of research has gone into
ensemble of classifiers. According to the literature, the most popular methods
for ensembles creation are Bagging (Breiman, 1996), Boosting (Freund &
Schapire, 1996) and Random Subspaces (Ho, 1998). The effectiveness of such
methods comes from the diversity caused by re-sampling the training set or
even by varying the subset of features to train the component classifiers. In
addition, some attempts have been made to incorporate the diversity into
ensemble creation methods by over-producing classifiers and then choosing some
of them to compose the ensemble. In this direction, an interesting alternative
to bring diversity to the ensemble is to combine classifiers trained with
different feature sets. The efficiency of such a strategy has been reported by
several authors (Kittler et al., 1998, Liu and Wang, 2006, Oza and Tumer,
2008).
In this paper we propose an ensemble of classifiers based on the under-pinning
concept of ?over-produce and choose?. The pool of base classifiers is created
using the two more prominent feature sets currently used for facial expression
recognition, namely, Gabor filters and LBP. Then a multi-objective genetic
algorithm is used to search for the best ensemble using as objective functions
the accuracy and the size of the ensemble. Two different experimental
protocols were employed to evaluate the proposed approach. In the first one
the subjects can be part of both the training and testing set (not with the
same images) while in the second experiment the subjects used for training are
not included in the testing set. The first protocol is very often used in the
literature due to the small size of the public datasets, however, the second
protocol seems to be more realistic since during the deployment phase the
system would have to classify expressions from people never seen by the
system.
Through a set of comprehensive experiments on two different databases (JAFFE
and Cohn-Kanade) we demonstrate the efficiency of the proposed strategy by
finding powerful ensembles, which succeed in improving the recognition of
facial expression from 5% to 10% when compared to conventional approaches that
employ single feature vectors and single classifiers. Furthermore, the resultsreported in this paper compare favorably to other results found in the
literature.
This paper is organized as follows: Section 2 outlines the proposed
methodology to create ensemble of classifiers. Section 3 introduces the
feature sets used to train the pool of base classifiers. The experimental
results are presented in Section 4. Finally, conclusions are stated in the
last section.
## 2\. Methodology overview
In this section we outline the approach proposed to generate ensemble of
classifiers for automatic facial expression recognition which is based on a
two step paradigm: ?overproduce and choose? which is depicted in Fig. 1. At
the first step, a pool of classifiers is created by varying the parameters of
Gabor filters ? orientation and scale ? as well as the parameters of the LBP
operators ? number of points and radius of a circular mask. Once this pool of
classifiers has been trained, at the second step is suggested to choose the
members of the team which are small (few classifiers) and accurate (few
errors). The second step can be performed by any search algorithm.
1. Download: Download full-size image
Fig. 1. The overview of the proposed method to generate ensemble of
classifiers.
Building an ensemble of classifiers can be formulated as a multi-objective
problem since we want to minimize not only the error rate of the ensemble but
also the number of the classifiers in the ensemble. In this context, multi-
objective genetic algorithms (MOGA) are more suitable than single genetic
algorithms (GA) because they can provide a set of solutions known as Pareto-
optimal. Single GA, on the other hand, converge to a specific region of the
search space depending on the weights assigned for each objective. More
details about the limitations of the single GA for multi-objective
optimization problems can be found in Deb (2001). In this work we have used
the Non-Dominated Sorting Genetic Algorithm II (NSGA II) to build an ensemble
of classifiers while minimizing both the error rate and the number of
classifiers of the ensemble (Deb, Agarwal, & Meyarivan, 2002).
The idea behind the NSGA II is that a ranking selection method is used to
emphasize good points and a niche method is used to maintain stable
subpopulation of good points. It differs from simple GA only in the way the
selection operator works. The crossover and mutation remain as usual. Before
the selection is performed, the population is ranked based on an individual?s
non-domination. The non-dominated individuals present in the population are
first identified from the current population. Then, all these individuals are
assumed to constitute the first non-dominated front in the population and
assigned a large dummy fitness value. The same fitness value is assigned to
give an equal reproductive potential to all these non-dominated individuals.
More details about NSGA II can be found in Deb et al. (2002).
When discussing ensembles of classifiers one could argue that diversity of the
classifiers is one objective that should be considered (Santos, Sabourin, &
Maupin, 2009). We agree with that, but in this work we selected as objective
to be optimized the accuracy and the size of the ensemble because of the
nature of the application. Since facial expression recognition usually is
applied to on-line systems, performance is a crucial requirement that this
kind of application should meet. Therefore smaller ensembles appear more
suitable in this case.
Let _A_ = {_C_ 1,_C_ 2, ? , _C_ _L_} be a set of _L_ classifiers and _B_ a
chromosome of size _L_ of the population. The relationship between _A_ and _B_
is straightforward, i.e., the gene _i_ of the chromosome _B_ is represented bythe classifier _C_ _i_ from _A_. Thus, if a chromosome has all bits selected,
all classifiers of _A_ will be included in the ensemble.
## 3\. Feature sets
This section presents the feature sets that have been chosen to model the
facial expressions.
### 3.1. Gabor filters
Gabor filters have been successfully applied to facial expression recognition
(Koutlas & Fotiadis, 2008) and for this reason they were chosen as one feature
set used to train our base classifiers. A family of Gabor kernel is the
product of a Gaussian envelope and a plane wave, as defined in Eq.
(1)(1)?u,v(z)=?ku,v?2?2e-?ku,v?2/?2[eiku,vz-e-?2/2]where _z_ = (_x_ , _y_) is
the variable in the spatial domain and _k_ _u_ ,_v_ (Eq. (2)) is the frequency
vector, which determines the scales and orientations of Gabor
kernels.(2)k?,v=kmaxfvei??where kmax=?2,f=2, and ?u=??8, where _?_ and _v_ are
orientation and scale factors, respectively. By varying _?_ and _v_ we can
selected different kernels. Fig. 2 shows an example for _?_ = 0,1, ? , 7 and
_v_ = 0,1, ? , 4.
1. Download: Download full-size image
Fig. 2. Example of the Gabor filters for 8 orientations (columns) and 5 scales
(rows).
Given and image _I_(_z_), its Gabor transformation at a particular position
can be computed by a convolution with Gabor Kernels using Eq.
(3).(3)Gu,v=I(z)×?u,v(z)
The magnitude of the resulting complex image is given by Eq.
(4).(4)|G|=Re(G)2+Im(G)2
All features derive from ? _G_ ?and the feature vector _F_ _k_ ,_N_ is given
by Eq. (5)(5)Fk,l=?i=xl-kxl+k?j=yl-kyl+k|Gi,j|,l=0,1,?,N;k=1,3,5,7,9.where _N_
is the number of the fiducial points marked in the face image, _x_ _l_ and _y_
_l_ are the coordinates of the fiducial point _l_ , and _k_ is the number of
neighboring pixels used to form the regions. Koutlas and Fotiadis (2008)
proposed a set of 20 fiducial points which where derived from 74 different
landmarks. According to the authors, such points lie around prominent features
of the face that contain the most significant information regarding the muscle
movement which is responsible for facial expressions. Fig. 3 shows the 20
fiducial points used in this work.
1. Download: Download full-size image
Fig. 3. The 20 fiducial points proposed by Koutlas and Fotiadis (2008).
Here it is important to mention that those points can be defined either
manually or automatically. In our research those points were manually located
in the subjects face. For each fiducial point a mask of size _k_ × _k_ is
used to compute the feature vector according to Eq. (5). In our experiments we
have tested _k_ = {1,3,5,7,9}.
As mentioned before, we extracted five feature sets based on scales with 160
components each, eight feature sets based on orientations with 100 components
each, and one feature set with 800 components combining scales and
orientations. Considering the five different masks, we have 70 different
feature sets that will be used to train 70 classifiers.
### 3.2. Local binary patterns (LBP)
LBP operators have also been successfully applied to facial expression
recognition (Shan et al., 2009) and for this reason they were selected as
another feature set used to train our base classifiers. The original LBP
proposed by Ojala et al. (1996) labels the pixels of an image by thresholding
a 3 × 3 neighborhood of each pixel with the center value and considering the
results as a binary number and the 256-bin histogram of the LBP labelscomputed over a region is used as texture descriptor. Fig. 4 illustrates this
process.
1. Download: Download full-size image
Fig. 4. The original LBP operator.
The limitation of the basic LBP operator is its small neighborhood which can
not absorb the dominant features in large scale structures. To surpass this
problem the operator was extended to cope with bigger neighborhoods (Ojala,
Pietikinen, & Menp, 2002). Using circular neighborhoods and bilinearly
interpolating the pixel values allow any radius and number of pixels in the
neighborhood. Fig. 5 exemplifies the extended LBP operator where (_P_ , _R_)
stands for a neighborhood of _P_ equally spaced sampling points on a circle of
radius of _R_ that from a circularly symmetric neighbor set.
1. Download: Download full-size image
Fig. 5. Three examples of the extended LBP operator (Shan et al., 2009): the
circular (8; 1) neighborhood, the circular (12; 1.5) neighborhood, and the
circular (16; 2) neighborhood, respectively.
The operator LBP _P_ ,_R_ produces 2 _P_ different output values corresponding
to the 2 _P_ different binary patterns that can be formed by the _P_ pixels in
the neighbor set. However, certain bins contain more information than others,
hence, it is possible to use only a subset of the 2 _P_ LBPs. Those
fundamental patterns are known as uniform patterns. An LBP is called uniform
if it contains at most two bitwise transitions from 0 to 1 or vice versa when
the binary string is considered circular. For example, 00000000, 001110000 and
11100001 are uniform patterns. It is observed that uniform patterns account
for nearly 90% of all patterns in the (8, 1) neighborhood and for about 70% in
the (16, 2) neighborhood in texture images (Ojala et al., 2002).
Accumulating the patterns which have more than two transitions into a single
bin yields an LBP operator, denoted LBPP,Ru2, with less than 2 _P_ bins. For
example, the number of labels for a neighborhood of 8 pixels is 256 for the
standard LBP but 59 for LBP _u_ 2. Thereafter, an histogram of the frequency
of the different labels produced by the LBP operator can be built.
According to Shan et al. (2009), an interesting way of using LBP in face
images consists in equally divide the image into _n_ small zones _Z_ 0,_Z_ 1,
? , _Z_ _n_ to extract the LBP histograms. The features extracted from each
zone are then concatenate into a single vector. Fig. 6 exemplifies this
process.
1. Download: Download full-size image
Fig. 6. LBP features extracted from a zoned face image (Shan et al., 2009).
In our experiments the faces were divided into 42 zones (7 × 6). Three
different configurations of the LBP operator were considered:
LBP8,1u2,LBP8,2u2,LBP16,2u2. The first two configurations produce a feature
vector of 59 components per zone, summing up 2478 components while the last
one produces a feature vector of 243 components per zone, summing up, 10,206
components. In summary, we have 3 different feature configurations that will
be used to train 3 classifiers.
## 4\. Experiments and discussion
Two experimental protocols were employed to evaluate the proposed ensemble
method for facial expression recognition. In Experiment I, subjects that
participate in the training set could be part of the testing set. Of course
that those images used for training were not used for testing. In the
Experiment II, the subjects used for training were not used for testing. Due
to the small size of the public datasets used for this kind of research, the
first experimental protocol is very often found in the literature. However,
the second protocol is far more realistic since during the deployment phasethe system would have to classify expressions from subjects that were not used
to train the system.
We have employed SVMs as base classifiers. The computational cost of training
a pool of SVMs is high, but on the other hand, the classification process is
almost instantaneous because, given an instance, only its position in the
feature space relative to the optimal hyperplane is evaluated. The pairwise
strategy, where _d_(_d_ ? 1)/2 classifiers are trained and organized as a
tree, was employed due the facial expression recognition is a multi-class
problem. Assuming that _d_ denotes the number of classes we end up with
twenty-one classifiers since we consider seven different classes of facial
expressions. Such a tree is transposed from the leaves to the root, where the
decision about the final class (facial expression) is taken.
Finally, it is important to mention that in all experiments we have used a
10-fold cross validation procedure, similar with that used by Zhang et al.
(1998). In the next subsections we describe the JAFFE and the Cohn-Kanade
databases as well as we report the experiments carried out on both of these
databases.
### 4.1. Databases
The JAFFE database (Lyons et al., 1998) contains 10 female subjects and 213
images of facial expressions. Each image has a resolution of 256 × 256 pixels.
The number of images corresponding to each of the 7 categories of expression
(neutral, happiness, sadness, surprise, anger, disgust and fear) is almost the
same. An example of these categories is presented in Fig. 7. The names of the
subjects are not revealed but they are referred with their initials: KA, KL,
KM, KR, MK, NA, NM, TM, UY, and YM.
1. Download: Download full-size image
Fig. 7. Example of the seven categories of facial expressions taken from the
JAFFE database.
According to Bashyal and Venayagamoorthy (2008), each image in the database
was rated by 91 experimental subjects for degree of each of the six basic
expressions present in the image. The semantic rating of the images showed
that the error for the fear expression was higher than that for any other
expression but there exist a number of cases even for other expressions in
which the expression getting highest semantic rating is different from the
expression label of the image.
The Cohn-Kanade database consists of image sequences depicting the evolvement
of every facial expression from the neutral state until it reaches its highest
intensity in the last frame. The Cohn-Kanade database is encoded into
combinations of action units. These combinations were translated into facial
expressions according to Pantic and Rothkrantz (2000) in order to define the
corresponding ground truth for the facial expressions. All the subjects were
taken under consideration to form the database, composed of 1,281 images, for
the experiments. Fig. 8 shows some examples of this dataset. Differently from
the JAFFE Database where all the subjects have all the seven different facial
expression, in this database few subject have the seven expressions. This
leads to an unbalanced dataset.
1. Download: Download full-size image
Fig. 8. Example of the seven categories of facial expressions taken from the
Cohn-Kanade database.
### 4.2. Experiments on JAFFE database
According to the proposed methodology, the first step consists in training the
pool of base classifiers. All the classifiers are SVMs trained with Gaussian
kernel using LibSVM (Fan, Chen, & Lin, 2005). Kernel parameters such as _C_
and _?_ were defined through a grid search using cross validation. Fig. 9shows the accuracy of the 73 classifiers for experiments I and II using JAFFE
database. The classifiers were split into three groups: 3 LBP, 30 Gabor scale-
based, and 40 Gabor orientation-based classifiers. As we can observe, the
performance of the classifier for the Experiment II is much worse than the
performance achieved in Experiment I.
1. Download: Download full-size image
Fig. 9. Accuracy of the classifiers on JAFFE database: (a) Experiment I and
(b) Experiment II.
After training the pool of classifiers they are used as input to the MOGA. In
this work we have used the NSGA II multi-objective genetic algorithm to build
the ensemble of classifiers. The NSGA II is based on bit representation, one-
point crossover, bit-flip mutation, and roulette wheel selection (with
elitism). The following parameters were employed: population = 100, number of
generations = 300, probability of crossover = 0.7, probability of mutation =
0.01, and niche distance = 0.05. The size of the chromosome is 73, since we
have 73 classifiers. The error rate of the ensemble is computed through the
Sum rule (Kittler et al., 1998). Other fusion rules such as Max, Min, Average,
and Product were also tried out to compute the error rate of the ensemble but
the Sum rule was the one that produced the best results. We have used the one-
max problem to define the probabilities of crossover and mutation, since it is
probably the most frequently used test function in research on genetic
algorithms due to its simplicity (Cantu-Paz, 2000). The population size and
the number of generations were empirically defined.
Fig. 10 shows the evolution of the population in the objective plane for
Experiments I and II. As we can observe, in both cases the algorithm converges
toward the Pareto-front producing a set of possible solutions. In order to
perform the search here we also have used 10-fold cross validation. Each
experiment was replicated 10 times to verify the reproducibility. Therefore,
all the results presented here are the average of these 10 replications.
1. Download: Download full-size image
Fig. 10. Evolution of the population in the objective plane: (a) Experiment I
and (b) Experiment II.
The next step consists in choosing the best ensemble of classifiers from the
Pareto. As mentioned before, high accuracy is important but the size of the
ensemble also is an important issue for this kind of application. As we can
observe from Fig. 10 the ensembles that provide the best trade-off between
accuracy and size are located close to the end of the Pareto. The selected
ensemble are marked with an arrow in Fig. 10aa and b. The selected classifiers
and their individual performances are reported in Table 1. Here it is
important to remark that the selected ensembles were present in all the 10
replications, what guarantees that the ensembles were not found accidentally.
Table 1. Selected classifiers ? JAFFE database.
Experiment I| Experiment II
---|---
Feature set| Accuracy (%)| Feature set| Accuracy (%)
LBP8,2| 87.3| LBP8,1| 60.6
Gabor scale 5, mask 3 × 3| 91.6| LBP8,2| 60.6
Gabor orientation 3, mask 7 × 7| 80.7| LBP16,2| 59.3
Gabor orientation 6, mask 7 × 7| 76.6| Gabor orientation 2, mask 1 × 1| 41.0
Gabor orientation 8, mask 7 × 7| 85.9| Gabor orientation 3, mask 5 × 5| 41.8
| | Gabor orientation 6, mask 9 × 9| 41.2
All classifiers| 92.5| All classifiers| 49.0
Ensemble| 96.2| Ensemble| 70.0
In spite of the same size (5 and 6 classifiers for Experiments I and II,respectively), the composition of the ensemble is totally different, with the
exception of the LBP classifier LBP8,2. As we can notice from Table 1 the
problem of Experiment II is quite more difficult than the problem of
Experiment I. However, the proposed methodology was able to find suitable
ensembles for both experiments.
In the case of Experiment I, the ensemble brought an improvement of about 5%
compared to the best classifier. A more impressive improvement, though, was
achieved in Experiment II where the ensemble improve the recognition rate in
about 10% relative to the best single classifier. A quick look on the
performance of the selected classifiers for Experiment II would suggest that
we could discard the three Gabor-based classifiers since they have a poor
performance when compared with the LBP-based classifiers. In spite of the poor
performance, these weak classifiers are very important since they provide
complementary information which is crucial for the good performance of the
ensemble. By removing the three Gabor-based classifiers the performance of the
ensemble would drop to 62%.
Table 2, Table 3 compare the confusion matrices for both experiments
considering all the classifiers and the ensemble produced by the proposed
method. Table 2 shows us that most confusions of the Experiment I have been
solved by the ensemble. In Experiment II, several confusions also have been
solved, e.g., class Sad (SA), however, there is a lot of room for improvement.
A possible alternative to further reduce these confusion would be to use
images from other databases to increase the training set.
Table 2. Confusion matrices for Experiment I ? JAFFE database.
Empty Cell| All classifiers| Ensemble
---|---|---
Empty Cell| HA| FE| AN| SA| DI| SU| NE| HA| FE| AN| SA| DI| SU| NE
HA| 28| | | | | | 3| 28| | | | | | 3
FE| | 31| | | | 1| | | 32| | | | |
AN| | | 28| | 2| | | | | 29| | 1| |
SA| 1| 1| | 27| 1| | 1| 1| 1| | 29| | |
DI| | | 1| 2| 25| 1| | | | 1| | 28| |
SU| | | 1| | | 29| | | | | | | 29|
NE| | | | | | 1| 29| | | | | | | 30
Table 3. Confusion matrices for Experiment II ? JAFFE database.
Empty Cell| All classifiers| Ensemble
---|---|---
Empty Cell| HA| FE| AN| SA| DI| SU| NE| HA| FE| AN| SA| DI| SU| NE
HA| 17| 4| 1| 3| 2| 2| 2| 27| 3| | | | 1|
FE| 1| 15| 2| 3| 7| 1| 3| | 20| | 3| 5| 4|
AN| 2| 4| 17| | 7| | | | | 24| | 5| | 1
SA| 3| 7| 1| 9| 8| | 3| 1| 2| 4| 15| 7| 1| 1
DI| 2| 5| 4| 1| 16| | 1| | 1| 3| 5| 20| |
SU| 1| 3| 1| | 1| 29| 2| 1| 3| | | | 24| 2
NE| 4| 4| | 2| 1| 6| 13| 1| 4| 3| | | 3| 19
Table 4 shows the performance of different approaches reported in the
literature on JAFFE database. To the best of our knowledge, all works have
used the protocol we have applied in Experiment I. Some of these results are
not comparable directly as some authors exclude some classes of the problem.
In spite of this fact, we can see that the proposed methodology compares
favorably to the literature.
Table 4. Comparison with different approaches on JAFFE database.
Reference| Accuracy (%)| Features
---|---|---Zhang et al. (1998)| 90.1| Geometry and Gabor
Bashyal and Venayagamoorthy (2008)| 90.2| Gabor and LVQ
Koutlas and Fotiadis (2008)| 92.3| Gabor filters
Liu and Wang (2006)| 92.5| Gabor filters
Oliveira et al. (2011)| 94.0| 2DPCA with feature selection and SVM
Shih et al. (2008)| 94.1| 2D-LDA and SVM
Liao et al. (2006)| 94.5| LPB, Tsallis entropies, global appearance
Cheng et al. (2010)| 95.2| Gaussian process
Zhi and Ruan (2008)| 95.9| 2D locality preserving projections
Proposed approach| 96.2| Ensemble based on Gabor and LBP
### 4.3. Experiments on Cohn-Kanade database
The same protocol used for JAFFE database was applied to the experiment on
Cohn-Kanade. Fig. 11 shows the accuracy of the 73 classifiers for experiments
I and II using Cohn-Kanade database. Here the classifiers were separated into
the same three groups and the like in the previous experiment the performance
of the classifier for the Experiment II is worse than the performance achieved
in Experiment I.
1. Download: Download full-size image
Fig. 11. Accuracy of the classifiers on Cohn-Kanade database: (a) Experiment I
and (b) Experiment II.
However, by comparing Fig. 9, Fig. 11 it is clear that the Cohn-Kanade
database is less complex than the JAFFE database. This could be explained by
the fact that facial expression images were extracted from video sequences
which reduces considerably the variability of the same subject, as depicted in
Fig. 12. This explains the compelling performance of some classifiers,
especially in Experiment I where the same subject participate in both training
and testing sets.
1. Download: Download full-size image
Fig. 12. Small variability of the Cohn-Kanade database.
Alike the experiments on JAFFE database, here the algorithm also converges
toward the Pareto-front producing a set of possible solutions. The selected
ensemble are marked with an arrow in Fig. 13a and b. The selected classifiers
and their individual performances are reported in Table 5. Again, the selected
ensembles were present in all the 10 replications what guarantee that they
were not found accidentally.
1. Download: Download full-size image
Fig. 13. Evolution of the population in the objective plane: (a) Experiment I
and (b) Experiment II.
Table 5. Selected classifiers ? Cohn-Kanade database.
Experiment I| Experiment II
---|---
Feature set| Accuracy (%)| Feature set| Accuracy (%)
LBP8,2| 99.0| LBP8,2| 84.3
Gabor scale 6, mask 1 × 1| 98.7| Gabor scale 1, mask 7 × 7| 78.7
All classifiers| 98.3| All classifiers| 79.2
Ensemble| 99.4| Ensemble| 88.9
As mentioned before, this dataset is less complex than the previous one so it
requires smaller ensembles to reduce the overall error rates. In both cases,
the best classifier (LBP8,2) was selected together with a Gabor scale-based
classifiers. Differently from the Experiment I where a single classifier
almost reached the upper-limit in terms of correct classification (99%), in
the Experiment II we got an improvement of more than 4% compared to the best
classifier. This corroborates to our previous findings that weaker classifiers
can bring important information to the ensemble. Table 6 shows the confusionmatrix for Experiment II where we can observe that several confusions with the
class ?Fear? were solved. According to Zhang et al. (1998), fear is the most
difficult expression to be recognized, even by humans.
Table 6. Confusion matrices for Experiment II ? Cohn-Kanade database.
Empty Cell| All classifiers| Ensemble
---|---|---
Empty Cell| HA| FE| AN| SA| DI| SU| NE| HA| FE| AN| SA| DI| SU| NE
HA| 234| 10| | | 1| 1| 15| 247| 3| | | | | 11
FE| 18| 70| 3| 6| 8| 19| 29| 12| 122| | 1| | 3| 15
AN| 4| 8| 37| 8| 9| 3| 15| 5| 1| 45| 8| 12| | 13
SA| | 5| 7| 105| 2| 7| 27| | 2| 11| 121| | 1| 18
DI| 11| 3| 8| 8| 74| 4| 6| | | 15| 5| 86| | 8
SU| 5| | | 2| | 212| 6| 3| 1| | | | 218| 3
NE| 9| 5| 5| 2| | 4| 270| 5| 2| 3| 1| | | 280
Table 7 shows the performance of different approaches reported in the
literature on Cohn-Kanade database. However, a direct comparison is not
possible due to the differences in the experimental protocol. For instance,
Shan et al., 2009, Bartlett et al., 2003 have partitioned the dataset randomly
into groups of roughly equal numbers of subjects where one group was used as
the test data, while the remaining groups were used as the training data to
train classifiers. We can see that the proposed methodology compares favorably
to the literature regardless the differences in the experimental protocol.
Table 7. Comparison with different approaches on Cohn-Kanade database.
Reference| Accuracy (%)| Features
---|---|---
Shan et al. (2009)| 79.1| LBP + template matching
Cohen et al. (2003)| 73.2| Geometric + Tree-augmented-Naive Bayes
Bartlett et al. (2003)| 86.9| Gabor filter + SVM
Bartlett et al. (2005) (Exp. II)| 89.1| Gabor filter + SVM
Shan et al. (2009)| 88.9| LBP-based + SVM
Shan et al. (2009)| 86.8| Gabor filter + SVM
Proposed approach (Exp. I)| 99.4| Ensemble based on Gabor and LBP
Proposed approach (Exp. II)| 88.9| Ensemble based on Gabor and LBP
## 5\. Conclusion
In this paper, we have presented a novel method for facial expression
recognition that relies on the combination of two different feature sets in an
ensemble approach to improve the recognition accuracy. The proposed approach
combines two different features sets, namely Gabor filters and LBP that
operate in different representation spaces. The recognition rate resulting
from the combination of both feature sets into an ensemble of classifiers is
significantly better than that achieved by individual features sets and single
classifiers. For instance, in the case of Experiment I, the ensemble brought
an improvement of about 5% compared to the best individual classifier. A more
impressive improvement was achieved in Experiment II where the ensemble
improves the recognition rate in about 10% compared with the best individual
classifiers.
Compared with other results available in the literature that use the same
experimental protocol (Bashyal and Venayagamoorthy, 2008, Cheng et al., 2010,
Koutlas and Fotiadis, 2008, Liao et al., 2006, Liu and Wang, 2006, Oliveira et
al., 2011, Shih et al., 2008, Zhang et al., 1998, Zhi and Ruan, 2008), the
results reported in this paper represent a slight improvement in terms of
recognition rate. Recent works in facial expression recognition report
recognition rates between 90% and 96%. It is important to notice that the two
databases do not convey realistic scenario regarding the acquisition ofsamples. Situations such as low and changing illumination, noise addition or
scaling are not addressed in both databases. However, such databases are
publicly available and have been used by many researchers for evaluation and
benchmarking.
In spite of the good results achieved, there are some shortcomings related to
the proposed approach. The first shortcoming is the necessity of locating the
fiducial points in the case of the Gabor features. Since there is no reliable
algorithm to locate such points in a face image, the incorrect location leads
to noisy feature vectors which can decrease the accuracy of the corresponding
classifier. However, in the scope of this paper, it would be impractical to
study the impact of the mislocation of fiducial points for the ensemble.
Nevertheless, this problem can be somehow alleviated by the ensemble. As
stated in Section 4, even if a classifier presents a poor performance it could
be important to the ensemble. Another shortcoming is the increase of the
complexity of the whole system since it requires the extraction of two sets of
features and the training and selection of the classifiers. Since this
additional computational effort is only required at the developing phase, the
percent rise in the facial expression recognition rate afforded by the
proposed approach is worthwhile.
In summary, the main contribution of this paper is a novel approach that
creates ensemble of classifies from a pool of base classifiers trained with
two feature sets which are widely used for automatic facial expression
recognition. By varying the parameters of the Gabor filters and LBP, seventy-
three classifiers where trained and further used as input of a multi-objective
genetic algorithm that returns a set of possible ensembles. The proposed
approach is effective and the improvements reported in this paper are
significant. Hence, it is logical to conclude that ensemble of classifiers is
a promising research direction in facial expression recognition.
## Acknowledgments
The authors would like to acknowledge the National Council for Scientific and
Technological Development (CNPq) for the financial support under the Grants
471.496/2007l-3, 309.295/2007-6 and 306.703/2010-6.
Recommended articles"
144,146,Generative moment matching networks,"['Y Li', 'K Swersky', 'R Zemel']",2015,1034,Toronto Face Database,machine learning,"On MNIST and the Toronto Face Dataset (TFD) we demonstrate improved results over  comparable baselines, including GANs. Source code for training GMMNs is available at https://",No DOI,… conference on machine learning,https://arxiv.org/abs/1502.02761,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
145,147,Going deeper in facial expression recognition using deep neural networks,"['A Mollahosseini', 'D Chan']",2016,1245,"Acted Facial Expressions In The Wild, Affective Faces Database, MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network","We evaluate the proposed method on well-known publicly available facial expression  databases: CMU MultiPIE [15], MMI [36], Denver Intensity of Spontaneous Facial Actions (DISFA) [",No DOI,2016 IEEE Winter …,https://arxiv.org/abs/1511.04110,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,
146,148,Hard negative generation for identity-disentangled facial expression recognition,"['X Liu', 'BVKV Kumar', 'P Jia', 'J You']",2019,107,"CMU Multi-PIE, MMI Facial Expression","FER, facial expression recognition","representations of RML achieve superior performance on the CK + , MMI and Oulu-CASIA   The performances of the FER systems usually depend heavily on facial expression",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320318303819,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Hard negative generation for identity-disentangled facial
expression recognition
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Hard negative generation
8. 4\. Radial metric learning
9. 5\. Numerical experiments
10. 6\. Conclusions
11. References
12. Vitae
Show full outline
## Cited by (86)
## Figures (13)
1. 2. 3. 4. 5. 6.
Show 7 more figures
## Tables (8)
1. Algorithm 1
2. Algorithm 2
3. Table 1
4. Table 2
5. Table 3
6. Table 4
Show all tables
## Pattern Recognition
Volume 88, April 2019, Pages 1-12
# Hard negative generation for identity-disentangled facial expression
recognition
Author links open overlay panelXiaofeng Liu a b c, B.V.K. Vijaya Kumar c d,
Ping Jia a b, Jane You a b e
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2018.11.001Get rights and content
## Highlights* ?
We extract identity-disentangled representations for facial expression
recognition (FER) without requiring expressive-neutral pairs in a testing
task.
* ?
Proposing a novel recognition via generation scheme as a substitution of
conventional hard sample mining.
* ?
The distance comparisons are largely reduced from _K_ 3(triplet loss) to 2 K,
where _K_ is the number of sample in a training batch.
* ?
Our new architecture achieves the state-of-the-art on 3 popular FER datasets,
which do not have neutral expression samples.
* ?
We alleviate the difficulty of threshold validation and anchor selection in
conventional deep metric learning.
* ?
We learn distance metrics with much fewer distance calculations and training
iterations, without sacrificing the performance.
* ?
We optimize the softmax loss and metric learning loss jointly based on their
characteristics and tasks.
* ?
A novel approach to generate photorealistic and identity-preserved normalized
face image.
## Abstract
Various factors such as identity-specific attributes, pose, illumination and
expression affect the appearance of face images. Disentangling the identity-
specific factors is potentially beneficial for facial expression recognition
(FER). Existing image-based FER systems either use hand-crafted or learned
features to represent a single face image. In this paper, we propose a novel
FER framework, named _identity-disentangled facial expression recognition
machine_ (IDFERM), in which we untangle the identity from a query sample by
exploiting its difference from its references (_e.g_., its mined or generated
frontal and neutral normalized faces). We demonstrate a possible ?recognition
via generation? scheme which consists of a novel hard negative generation
(HNG) network and a generalized radial metric learning (RML) network. For FER,
generated normalized faces are used as hard negative samples for metric
learning. The difficulty of threshold validation and anchor selection are
alleviated in RML and its distance comparisons are fewer than those of
traditional deep metric learning methods. The expression representations of
RML achieve superior performance on the CK + , MMI and Oulu-CASIA datasets,
given a single query image for testing.
* Previous article in issue
* Next article in issue
## Keywords
Hard negative generation
Adaptive metric learning
Face normalization
Facial expression recognition
## 1\. Introduction
Facial expression is the most natural and expressive nonverbal channel for
humans to communicate their emotions [1]. Therefore, facial expression
recognition (FER) has been an important and active topic for a wide range ofapplications including soft biometrics, digital entertainment, health care,
robot systems and human-computer interaction (HCI). Ekman and Friesen
postulated the universality of neutral (Ne) and six prototypical human facial
expressions, namely, anger (An), disgust (Di), fear (Fe), happiness (Ha),
sadness (Sa) and surprise (Su) [2].
The performances of the FER systems usually depend heavily on facial
expression representations, which are affected by pose and illumination
variations as well as facial morphology variations (i.e._,_ identity-specific
factors). As some facial expressions involve subtle facial muscle movements,
the extracted expression-related information from different classes (in this
paper, class refers to expression) can be overwhelmed by high-contrast
identity-specific geometric or appearance features degrading FER performance.
As illustrated schematically in Fig. 1, we want the intra-class distances of
happy face images from different people to be smaller than the inter-class
distances between face images of different expressions from the same subject.
However, the nuisance identity factors often dominate the representation of
the image in pixel space causing two images of the same subject with different
expressions to be closer to each other than the same-expression images from
two different subjects. This is because the extracted facial representation
often contains identity-specific information that is irrelevant and may be
counter-productive for the FER task. These identity-specific factors may
degrade the FER performance on new identities unseen in the training data.
1. Download: Download high-res image (185KB)
2. Download: Download full-size image
Fig. 1. Illustration of the desired representations in the Face Expression
Recognition (FER) feature space. The ?class? here refers to the facial
expression.
Aided by the advances in deep learning for computer vision [3], much progress
has been made on extracting a set of features to represent a single facial
expression image [4], [5]. The hand-crafted features are constructed by
exploiting domain knowledge of the specific relationships within pixels so
that the features are invariant to some simple transformations (_e.g_.,
translation and scaling). More recently, feature-learning approaches are being
investigated because of their ability to produce features that are tolerant to
complex transformations. In the case of FER, identity-associated factors may
fall into this category. Yet, unfortunately, due to the tight coupling of the
various nuisance factors, when we try to reduce the sensitivity to identity
with these state-of-the-art strategies, we are unlikely to satisfactorily
disentangle all variations in facial morphology.
Our preliminary work [6], [7] proposed to improve the facial expression
recognition performance by disentangling the identity factors in a face image
through the metric learning method. To alleviate the slow convergence caused
by trivial training samples in metric learning, [6] compared the query image
to a negative set containing other facial expression images from the same
subject as shown in Fig. 3. However, in practice, the structure of real-world
FER datasets results in a significant constraint: the dataset may not contain
images of every facial expression for every subject. Actually, we may not need
to compare a query facial expression with any other class of facial
expressions. According to some psychology and anatomy research [8], the muscle
activities of different facial expressions initiate from the neutral face as
illustrated schematically in Fig. 2. This is also the fundamental principle
underlying the action units (AUs) and Facial Action Coding System (FACS)
proposed by Ekman [2]. Since the expressive classes are naturally more
discriminative from each other than from the neutral face, in the trainingstage, we may want to emphasize the neutral-expressive distance more than
requiring large distance between those expressive classes. The neutral face
images can be the ideal hard samples which can improve the learning efficiency
of metric learning. However, expressive-neutral face image pairs of every
person may not be always available in real world applications and in some FER
image datasets.
1. Download: Download high-res image (308KB)
2. Download: Download full-size image
Fig. 2. A schematic depicting the 6 basic facial expressions and their
relationships to the neutral face. Representations in the outer ring
correspond to higher-intensity facial expressions compared to those in the
inner ring.
The above insights suggest that we should generate the frontal and neutral
normalized face image of the query image and disentangle the possibly counter-
productive identity information and we proposed _identity-disentangled facial
expression recognition machine_ (IDFERM) towards this goal. IDFERM consists of
two main parts, namely, the hard negative generation (HNG) network and the
radial metric learning (RML) network. Specifically, given a query facial
expression image, its normalized reference will be synthesized using an HNG
network trained using expression-normalize face image pairs of the same
subject. Then, the query-reference pairs are fed into the radial metric
learning (RML) network, which uses an inception style convolutional (Conv)
layers group and a unified two-branch fully connected (FC) layers framework to
extract the contrast of query-reference pair by simultaneously optimizing the
softmax loss and RML loss. By pushing the representation of facial expression
images away from their generative references and pulling them close to their
cluster centers of each expression, the RML can disentangle those nuisance
factors to balance the intra- and inter- class variations.
Unlike other image-based FER systems that produce a representation from a
single input image, RML utilizes the expression-reference pair to disentangle
the identity-specific factors. In contrast to video-based FER methods [9],
[10] or real facial expression image pair-based methods [6], [7], [11], our
method employs synthetically generated references to address the real-world
limitation that sometimes the dataset does not contain all possible facial
expression examples for some subjects.
The preliminary versions of the concepts in this paper were published in the
2017 Biometrics workshop of _IEEE Conference on Computer Vision and Pattern
Recognition_ [6] and 2018 IEEE International Conference on _Identity, Security
and Behavior Analysis_ [7]. In this paper, we extend those basic concepts in
the following ways:
* 1)
We investigate the prior relationship of different expression classes and
propose a novel recognition via generation scheme as a possible substitution
of conventional hard sample mining.
* 2)
We design an end-to-end IDFERM to extract identity-disentangled
representations for FER without requiring real expression-neutral pair inputs
in the FER datasets.
* 3)
The number of needed distance comparisons for adapted RML is orders of
magnitude smaller than the number needed for conventional metric learning
approach.
* 4)
We conduct all experiments using the new architecture and test on moredatasets without neutral expression samples.
In summary, this paper makes the following contributions.
* ?
We propose a generalized metric learning loss function with adaptively learned
reference threshold which alleviates the difficulty of threshold validation
and anchor selection.
* ?
With the identity-aware HNG for hard negative generation, the adapted RML
learns distance metrics with fewer input iterations and distance calculations,
without sacrificing the performance for identity-invariant FER.
* ?
We jointly optimize the softmax loss and metric learning loss in a unified
two-branch FC layer metric learning CNN framework based on their
characteristics and tasks.
* ?
The proposed HNG network is a novel approach to generate photorealistic and
identity-preserved normalized face image by combining prior knowledge from
data distribution and domain knowledge of faces.
* ?
Using numerical experiments, we demonstrate that the proposed method achieves
promising results on CK + , MMI and Oulu-CASIA data sets.
The rest of this paper is organized as follows. Section 2 briefly reviews
related work in the literature. Section 3 introduces in detail the proposed
normalized face generation with perceptron generative adversarial networks.
Section 4 shows its application to FER with RML network. Section 5 reports the
experimental results and ablation study of the auxiliary parts in HNG network.
Finally, Section 6 provides our conclusions.
## 2\. Related work
Despite receiving considerable research attention, FER remains very
challenging [12]. Research developments in deep learning, especially the
success of convolutional neural networks (CNN), have made high-accuracy image
classification possible in recent years. Deep learning-based FER methods have
emerged starting with Bengio et al. [13] who described the use of carefully
designed CNN to learn expression features from raw pixels. Despite its
popularity, current softmax loss-based approach does not explicitly reward
intra-class compactness and inter-class separation, and identity-related
factors remain major obstacles for FER. Machine recognition usually is based
on similarity metrics, but those metrics may be more sensitive to identity
than expressions. To decouple these two types of similarity and exploit the
appearance information, substantial efforts have been dedicated to extracting
features by learning [14]. Given that the expressions are formed by relaxing
or contracting some facial muscles that result in temporally deformed facial
features, identity-disentangled representations for FER normally separate a
face with expression into a main component neutral face that encodes identity
cues and an action component that encodes motion cues (such as movements of
eye brows, cheeks, lips, eyelids and nose) which are related to the AUs and
FACS [2].
FER is certainly not unique among computer vision applications that have to
cope with nuisance factors causing variability in the data. Deep metric
learning approaches have been shown to be successful for person and vehicle
identification tasks [15], [16], [17], which also exhibit large intra-class
variations. The initial work in this domain [18] involves training a Siamese
network. Pairwise examples are fed into two symmetric sub-networks and the
network is updated using contrastive loss function, _i.e_., their extractedrepresentations should be close to each other if the inputs have the same
class label, otherwise the distance between these representations should be
large. One improvement is the triplet loss [19], in which, the inputs are
triplets, each consisting of a query, a positive example and a negative
example. An anchor is chosen from the query or positive examples, then the
method requires the difference of the distance from the anchor point to the
positive or query example and from the anchor point to the negative example to
be larger than a fixed margin _?_. Recently, some variants of this offering
faster and more stable convergence have been developed. The (_N_ \+ 1)-tuplet
loss [20] incorporated multiple negative examples while the coupled cluster
loss (CCL) [15] incorporated multiple positive examples in a tuplet. The
center of positive examplesc+ is set as the anchor in CCL. By comparing each
example withc+ instead of each other, the number of distance evaluations
needed are reduced significantly.
For the situation shown in Fig. 3, the triplet loss, (_N_ \+ 1)-tuplet loss
and CCL are all 0, since the distances between the anchor and positive
examples are indeed smaller than the distance between the anchor and negative
examples for a margin _?_. This means the loss function will learn to neglect
such non-trivial samples. We will need many more input iterations with
properly selected anchors to correct this situation. The fixed threshold in
the contrastive loss was also proven to be sub-optimal [21]. The difficulty of
threshold validation and anchor selection have long been significant
challenges until our initial work [6], which included an adaptive (_N +
M_)-tuplet clusters loss function.
1. Download: Download high-res image (249KB)
2. Download: Download full-size image
Fig. 3. Failed case of (a) triplet loss, (b) (_N_ \+ 1)-tuplet loss, and (c)
Coupled clusters loss. The preliminary (_N + M_)-tuplet clusters loss (d) uses
two thresholds to avoid the anchor selection issue and threshold-parameters
_i.e_., _T_ and ?, do not need manual tuning [6], [7]. We use x+(yellow
points) and x? (squares) to denote the positive and negative examples of a
query example _x_ , meaning that x+is the same class of _x_ , while x?is not.
_f_( · ) is an embedding kernel.
Also, the traditional online or offline mini-batch sample selection is a large
additional computational burden and can result in poor local optima [22].
Generating all possible pairs or triplets would result in quadratic and cubic
complexity, respectively and most of these pairs or triplets are not very
useful for the training [6]. Our initial work [6] utilized identity-aware
hard-negative mining and online positive mining for FER, but it still suffers
from the dataset-sensitive and computationally-expensive example mining to
provide nontrivial tuplets.
Several approaches have been proposed for generative models. Conventional
methods such as Principal Components Analysis (PCA), Independent Components
Analysis (ICA), Gaussian Mixture Model (GMM), _etc_., have difficulty in
modeling complex patterns of irregular distributions [23]. Recently,
Restricted Boltzmann machines (RBM), Hidden Markov Model (HMM), Markov Random
Field (MRF) _etc.,_ have been employed for modeling images of digits, texture
patches, and well-aligned faces [24]. However, the limited ability of feature
representations restricts further development. Since deep hierarchical
architectures of the recent generative models are capable of capturing complex
structure of data, generated images from these deep hierarchical structures
are more realistic. The denoising auto-encoder (DAE) pairs a differentiable
encoder and decoder, which encodes an image sample _x_ to a latent
representation _z_ and then decodes the _z_ back to another image x? [25]. Forthe normalized face generation task, pose and expression are regarded as the
noise to be mitigated. The main limitation of this approach is that the
squared pixel-wise reconstruction error would cause the generated samples to
look blurry as they generate the mean image of the distribution. Generative
Adversarial Network (GAN) [26] simultaneously trains two networks: a
generative network _Gen_ to synthesize images (maps latents _z_ to image
space), and a discriminative network _Dis_ to discriminate between real
training images from generated images. With the GAN, an expected image can be
generated from a randomly sampled vector _z_ from a certain distribution.
Normally, the GAN schemes are not well-matched to supervised recognition
tasks. The GAN-generated results are expected to align with the central part
of the data distribution, while the boundary between classes in feature space
is more important for classification. Limited research has been devoted to
this topic. The semi-GAN [27] adds an extra task for a discriminator network
to improve semi-supervised recognition task. The face rotator schemes proposed
by Tran [28] generates a frontal face as the preprocessing for the face
recognition network.
## 3\. Hard negative generation
As we are trying to disentangle identity-related factors from a facial
expression image _x,_ a reference neutral face image from the same subject is
required to obtain the difference between the neutral face and _x_ for FER.
However, such a neutral face reference image is not always available in real
world application scenarios. Instead of mining several negative samples, we
directly use the generated normalized face image as negative sample. The goal
of the hard negative generation (HNG) network is to produce a photorealistic
and identity-preserved normalized face imagex? from the probe image _x_. The
network architecture and loss functions are illustrated in Fig. 4.
1. Download: Download high-res image (554KB)
2. Download: Download full-size image
Fig. 4. Framework of our IDFERM, in which, the left and right side are the HNG
and RML network, respectively. We feed the input image _x_ to an encoder-
decoder structure and generate its transformed output x?. The _Dis_ is trained
to determine if its input is from the guiding set (real image) or the encoder-
decoder structure (generated image), thus encouraging the encoder-decoder
structure to generate images more similar to the images in our guiding set.
The guiding set contains all of the target images _y_ , which is a compilation
of numerous frontal and neutral real face images. The Light-CNN is used to
extract the identity feature for identity similarity measurements, and the
VGG-Face is adopted to embed image for feature level perceptual similarity
measurements. The mined real expressive face images and their corresponding
generted normalized face images are feeded to the RML, an adaptive deep metric
learning framework, to disentangle the identity factors in a face image for
FER by minimazing both the softmax (cross-entropy) loss and RML loss.
Our HNG network is composed of five major components: 1) an Encoder network
_Enc_ , (2) a Decoder network _Dec_ , (3) a Discriminator network _Dis_ , (4)
the _Light CNN_ network and (5) the _VGG-facenet_. The function of _Enc_ and
_Dec_ network is the same as that in denoising auto-encoder [25]. In DAE, the
output is not required to be exactly the same as the input. For example, the
denoising auto-encoder takes in an image that has been corrupted by some form
of ?noise?, and is forced to output a denoised version of that image by
requiring the output image to be similar to the original ?clean? image. In our
application, a face image with expression (input image _x_) can be regarded as
a copy of neutral face image (target image y) that has been corrupted by
expressions. The denoising (disentangling expression from a face image) isachieved by requiring our output x? to be close to the target image y, as the
DAE requires its generated image to be close to a clean target image instead
of the original noisy image. The _Enc_ maps the input sample image _x_ to a
latent representation _z_ through a learned distribution P(_z|x_), while the
_Dec_ generates predicted a facial image x? corresponding to _z_. The function
of the _Dec_ and _Dis_ is the same as that in the GAN [26]. The _Dec_ network
tries to generate the real distribution by the loss of _Dis_ which learns to
distinguish between generated image x?and real image in the guiding set _g_.
The guiding set contains all of the target images _y_ , which is a compilation
of numerous frontal and neutral real face images. The input-target pairs {_x
i, yi_} from multiple identities are required to learn the parameters _?_ of
the differentiable encoder _? Enc_ and decoder _? Dec_, where _x_ is a face
image with expression and _y_ is the frontal neutral face image of that
person. In our experimental setting, five different loss functions are used to
combine the advantages of high quality GAN and stable auto-encoder which
encodes the data into a latent space _z_. In this section, we show how the
multiple objective functions are employed for different parts to generate the
facial reference images for FER.
### 3.1. Feature space perceptual loss
The squared error loss between the CNN feature representations is adopted to
represent the feature-level perceptual loss. We make use of the independently-
trained and fixed VGG-FaceNet [29] to model this semantic feature-level loss.
Although it comprises 14 Conv layers and 3 FC layers, we omitted the deeper
layers after the 5th Conv layer because their limited spatial resolution
cannot support good image reconstruction performance. Denoted by ? _l_ , the
feature map of the _l_ th convolution layer of VGG-FaceNet is used to extract
the feature representations using the standard forward-propagation process.
The semantic perceptual loss between two images x?and _y_ on the _l_ th
convolutional layer is defined as the following squared-error loss between the
two feature
maps.(2)Lfeat(x?,y)=1Wl×Hl?n=1Wl?m=1Hl??l,n,m(x?)??l,n,m(y)?22where _W l_ and
_H l_ denote the width, height of the _l_ th feature map and ? _l, n, m_ is
the value of the _l_ th feature map at point (_n, m_). In our experiment, _l_
=5 based on empirical testing.
### 3.2. Symmetry loss
Symmetry is an inherent property of normal human faces. The symmetry loss of a
face image takes the
form:(3)Lsym(x?)=1H×W/2?n=1W/2?m=1H|x?n,m?x?W?(n?1),m|where _W_ and _H_ are
the width and the height of the images and (_n, m_) denotes the pixel of the
generated image. The |?| denotes the absolute value. For simplicity, when
training our model with the symmetry loss, all the inputs are aligned and
detected if the occluded parts are on the right side of image. If not, images
are flipped so that the occluded parts are on the right side. Real-world
images may not exhibit the strict symmetry of pixel values. Considering the
consistency of the pixel difference inside a local area, and the gradients at
a point along all directions are largely preserved under different
illuminations, minimizing a symmetry loss in the Laplacian space should
emphasize human faces.
### 3.3. Adversarial loss
We introduce a discriminator _Dis_ which serves as a supervisor to push the
synthesized image to reside in the manifold of frontal neutral face images. It
can reduce the blur effect and produce visually pleasing results.
The _Dis_ aims to discriminate the predicted frontal neutral face image xi?
from real ones _g i_ in the guiding set, and is trained concurrently with thetransform network (_Enc_ and _Dec_). The transform network tries to ?trick?
the _Dis_ to classify the generated images as real. Formally, the
discriminator is trained to minimize the following binary cross entropy
loss:(4)LGAN?Dis(gi,xj?)=?log(Dis(gi))?log(1?Dis(xj?))
With respect to _Dec_ , the parameters are trained by minimizing the following
loss:(5)LGAN?Dec(xj?)=?log(Dis(xj?))
### 3.4. Identity-Preserving loss
Synthesizing the frontal neutral face image while preserving the identity is a
critical part of IDFERM. We introduce a direct supervision to reward the
perceptual similarity between input and generated images using the face
verification network. In our approach, we use the pre-trained Light CNN, a
compact network that has only 4 convolution layers with Max-Feature-Map
operations and 4 max-pooling layers [30]. In this work, the identity-
preserving loss is defined based on the activations of the last two layers of
the Light CNN:(6)Lid=?l=121Wl×Hl?n=1Wl?m=1Hl|?l,n,m(x?)??l,n,m(x)|where _W l,
Hl_ denotes the width and height of the _l_ th layer, _?_ _l, n, m_ is the
value of the feature map (_n, m_) point and |?| denotes the absolute value.
### 3.5. Pixel-wise loss
Adversarial training is known to be sensitive to hyper parameters. Adding the
following pixel-wise L1 loss(7)Lpixel=1W×H?n=1W?m=1H|x?n,m?yn,m|in the image
space with a relatively small weight is one method to stabilize the training
and accelerate the optimization. x?n,m and x _n, m_ are the pixel level gray
values of the (_n, m_)th pixel.
Using judicious selection of aforementioned loss functions, we train the _Enc,
Dec_ and _Dis_ simultaneously. The error signal from adversarial loss and
symmetry loss are not back-propagated to _Enc_. Several tradeoff parameters
constrained between 0 and 1 are used to balance the aforementioned loss
functions. The weights _?_ 1 and _?_ 2 shown in Algorithm 1 are the tradeoff
parameters for Lfeat, Lid and Lpixel for the _Enc_ and _Dec_. The parameter
_?_ 3 is used to weight the Lsym in _Dec_. As _Dec_ also receives the error
signal from the _Dis_ , a parameter _?_ is used to weight the ability of
fooling the discriminator.
Algorithm 1. Training the HNG network.
**_?_** _Enc,_ **_?_** _Dec,_ **_?_** _Dis_ **?** initialize network
parameters
---
**Repeat**
** _X_** ? random mini-batch from dataset
** _Z_** ? _Enc_(**_X_**)
X? ? _Dec_(**_Z_**)
Lfeat **?** 12Wl×Hl?n=1Wl?m=1Hl?l,n,m(x?)??l,n,m(y)22
Lsym **?** 1W/2×H?n=1W/2?m=1H|x?n,m?x?W?(n?1),m|
LGAN?Dis??log(Dis(gi))?log(1?Dis(xj?))
LGAN?Dec??log(Dis(xi))
Lid??l=121Wl×Hl?n=1Wl?m=1Hl|?l,n,m(x?)??l,n,m(x)|
Lpixel?1W×H?n=1W?m=1H|x?n,m?yn,m|
**//** Update parameters according to gradients
** _?_** _Enc_ ? ???Enc(Lfeat+?1Lid+?2Lpixel)
** _?_** _Dec_ ????Dec(Lfeat+?1Lid+?2Lpixel+?3Lsym+?LGAN?Dec)
** _?_** _Dis_ ????Dis(LGAN?Dis)
**Until** deadline
We show some of the input-output pairs of our HNG network in Fig. 5. As the
common quantitative metrics (e.g., log-likelihood of a set of validation
samples) are often not very informative for perceptual generative models [31],we provide a qualitative comparison of visual quality and a quantitative
evaluation of identity-preservation in Section 5.
1. Download: Download high-res image (466KB)
2. Download: Download full-size image
Fig. 5. Input-output pairs of the proposed reference generation (HNG) network
from the CK + , MMI and Oulu-CASIA dataset. Top row in each pair: a subject in
a database with different expressions (from left to right: angry, disgust,
fear, happy, neutral, sad and surprise). Bottom row in each pair: generated
normalized face images from the input of the expressive face images in the row
above.
Unlike previous generative methods that utilize their intermediate features
for the recognition tasks, the resulting expression- and pose- disentangled
face image has potential for several downstream applications, such as facial
expression or face recognition, and attribute estimation.
## 4\. Radial metric learning
The proposed RML only requires the comparison of the representation of the
query sample _f i_ with the representation of its generated reference fi? and
its cluster center Cyi. We introduce a distance _T_ from the query sample _x_
to control the relative boundary (T??2)and (T+?2)for the intra-class center
and generated references, respectively. The RML loss function is formulated as
follows.(8)L({xi}i=1K,{xi?}i=1K;f)=1K?i=1K{max(0,D(fi,Cyi)?T+?2)+max(0,?2+T?D(f?i,Cyi))}
Only if the distances from all online mined examples _f i_ to its updated Cyi
are smaller than (T??2) and the distances from all the generated references
f?ito its updated Cyiare larger than (T+?2), the loss L({xi}i=1K,{xi?}i=1K;f)
can get a zero value. A simplified geometric interpretation of this is shown
in Fig. 6.
1. Download: Download high-res image (300KB)
2. Download: Download full-size image
Fig. 6. The proposed radial metric learning (RML) framework. A small circle
without border is the representation of a sample (_i.e_., facial expression
image) and the different classes are represented by different colors. The
small gray circles with colored border are their corresponding generated
references (i.e._,_ normalized face images). The orange points with colored
border are the cluster centers of each classes. The big dashed circles are the
boundaries of each classes in the feature space, which are expected to have
small radius and far away from each other.
By assigning different values for _T_ and _?_ , we define a flexible learning
task with adjustable difficulty for the network. We do not use the special
case that requires inter-class variation to be zero (i.e._, T_ =?/2) as the
center loss [32] for the FER training set usually contains some unreliable
labels [33]. However, these two hyper-parameters need manual tuning and
validation. In here, we formulate the reference distance _T_ to be a function
S( · , · ) which should be trained automatically, instead of a constant.
Inspired by the Mahalanobis distance matrix M in Mahalanobis distance D (Eq.
9), which is a positive semi-definite (PSD) matrix and can be calculated via
the linear fully connected layer as in [34], we try to automatically train
both S and D. Since the difference of the reference distance and the distance
function need to be calculated in two terms in Eq. 8, a possible solution is
to calculate (S?D) function via the linear FC
layer.(9)D(f1,f2)=?f1?f2?M2=(f1?f2)tM(f1?f2)
Since the metric M itself is quadratic, we assume that S has a simple
quadratic form:(10)S(f1,f2)=12f1tA?f1+12f2tA?f2+f1tB?f2+ct(f1?f2)+bwhere A?and
B? are both the _d_ × _d_ real symmetric matrices (not necessarily positive
semi-definite), _c_ is a _d_ -dimensional vector, and _b_ is the bias term.Then, a new quadratic expression H(f1,f2)=S(f1,f2)?D(f1,f2) is defined to
combine the reference distance function S and the Mahalanobis distance metric
function D. Substituting S(_f_ 1,_f_ 2) and D(_f_ 1,_f_ 2) into H(_f_ 1,_f_
2), we
get:(11)H(f1,f2)=12f1t(A??2M)f1+12f2t(A??2M)f2+f1t(B?+2M)f2+ct(f1?f2)+b(12)H(f1,f2)=12f1tAf1+12f2tAf2+f
1tBf2+ct(f1?f2)+bwhere
A=(A??2M) and B=(B?+2M). Suppose A is PSD and B is negative semi-definite
(NSD), A and B can be factorized as LATLA andLBTLB. Then H(_f_ 1,_f_ 2) can be
rewritten as
follows:(13)H(f1,f2)=12f1tLAtLAf1+12f2tLAtLAf2+f1tLBtLBf2+ct(f1?f2)+b(14)H(f1,f2)=12(LAf1)t(LAf1)+12(LAf2
)t(LAf2)+(LBf1)t(LBf2)+ct(f1?f2)+b
Motivated by the above, we propose a general, computationally feasible loss
function. Following the notations in the preliminaries and denoting (LA, LB,
_c_)t as W which can be learned via the linear fully connected layer, we
have:(15)L(W,{xi}i=1K,{xi?}i=1K;f)=1K?i=1K{max(0,?2?H(fi,Cyi))+max(0,H(f?i,Cyi)+?2)}
Moreover, we simplify ?2to be the constant 1, since changing it to any other
positive value results only in the matrices being multiplied by a
corresponding factor. Our hinge-loss like function is given as
follows.(16)L(W,{xi}i=1K,{xi?}i=1K;f)=1K?i=1K{max(0,1?H(fi,Cyi))+max(0,H(f?i,Cyi)+1)}
By doing this, the adaptive threshold can be seamlessly factorized into a
linear-fully connected layer for end-to-end learning [34]. The RML loss can
also be easily used as a drop-in replacement for the triplet loss and its
variants, as well as used in tandem with other performance-boosting approaches
and modules, including modified network architectures, pooling functions, data
augmentations or activation functions.
For a training batch consisting of _K_ query samples, the number of input
passes required to evaluate the necessary embedding feature vectors in our
application is _K_ , and the total number of distance comparisons can be 2
_K_. Normally, _K_ is much larger than 2. In contrast, triplet loss and (_N_
\+ 1)-tuplet loss require _O_(_K_ 3) comparisons, the contrast loss and CCL
require _O_(_K_ 2) comparisons, and the (_N + M_)-tuplet cluster loss requires
2(_N + M_)*_K_ comparisons after a strict example mining scheme using the
special structure of some FER datasets (i.e._,_ each subject has all 6
expressions). Here _N_ and _M_ are the number of mined positive samples and
the number of mined negative samples, respectively. Even for a dataset of a
moderate size, it is computationally impractical to load all possible
meaningful triplets into the processor memory for model training. With
predefined anchors (i.e._,_ Cyi and f?i), we also alleviate the difficulty of
anchor selection [6].
??The inception convolutional FER network and two-branch FC layer joint metric
learning architecture proposed in our preliminary paper [6] are used in our
framework in Fig. 4. The convolutional groups are made up of a 1 × 1, 3 × 3
and 5 × 5 Conv layers in parallel.
Combining the metric learning loss and softmax loss is an intuitive idea to
possibly achieve better performance [35]. However, combining them directly on
the last FC layer is sub-optimal. The basic idea of building two-branch FC
layers after the deep convolution groups is to combine two losses at different
levels of tasks. We learn the detailed features shared between the same
expression class with the expression classification (EC) branch, while
exploiting semantic representations via the metric learning (ML) branch to
handle the significant appearance changes from different subjects. The
connecting layer embeds the information learned from the expression label-
based detail task to the identity label-based semantic task, and balances the
weights in the two task streams. This type of combination can effectivelyalleviate the interference of identity-specific attributes. The inputs of
connecting layer are the output vectors of the former FC layers- _FC_ 2-2 and
_FC_ 2-3, which have the same dimension denoted as _D_ input. The output of
the connecting layer, denoted as _FC_ 4 with dimension _D_ output, is the
feature vector fed into the second layer of the ML branch. The connecting
layer concatenates two input feature vectors into a larger vector and maps it
into a _D_ output dimension
space:(17)FC2?4=Pt[FC2?2;FC2?3]=P1tFC2?2+P2tFC2?3where **P** is a (2 _D_ input
× 2 _D_ output) matrix, **P 1** and **P 2** are _D_ input × _D_ output
matrices.
Regarding the sampling strategy, every training image is used as a query
example in an epoch. In practice, the softmax loss will only be calculated for
the query examples. The relative importance of the two loss functions is
managed by a weight ?. During the testing stage, this framework takes one
query image and its generated reference image as input, and determines the
classification result through the EC branch with the softmax loss function.
Our disentangled feature learning scheme is described in Algorithm 2.
Algorithm 2. Disentangled feature learning algorithm.
**Input**
---
Randomly chose _K_ query examples{xi}i=1K
and their generated references {xi?}i=1K
**Output: The parameters of the FER network _? FER_**
**1.** while not converge do
**2.** map examples to feature plane with CNN to get: {fi}i=1Kand{f?i}i=1K
**3.** calculate the cluster centers Ci for each class
**4.** LRML?1K?i=1K{max(0,?H(fi,Cyi)+1)+max(0,H(f?i,Cyi)+1)}
**5.** Lsoftmax? ?log(efyi?max(fj)/?jefj?max(fj))
**6.** Compute the joint loss Lsoftmax+?LRML
**7.** Compute the backpropagation error
**8.** Update the parameters
**End while**
## 5\. Numerical experiments
In this section, we compare the IDFERM with state-of-art methods on three
benchmark datasets, i.e., CK + , MMI and Oulu-CASIA datasets. Details of our
training data are provided in Section 5.1, followed by our preprocessing
methods in Section 5.2, and the implementation details in Section 5.3. In
Section 5.4, we report a series of ablation experiments to analyze the
function of our auxiliary networks. Numerical experiment results are shown in
Section 5.5.
### 5.1. Training data
Besides the FER datasets, a variety of large datasets of facial images for
face recognition are publicly available online. We give some samples from the
VGG-Face dataset Fig. 7. We use the VGG-Face dataset [29] to extend our data
for neutral face generation. It contains approximately 2.6 million face
images, but very few of these fit our requirements of neutral expression,
front-facing, having no occlusion, and of sufficient resolution for face
region. We use the Google Cloud Vision API to remove those images that look
blurry, with high emotion score or eyeglasses or tilt or pan angles beyond 5°.
These frontal and neutral face images are used as our target and guiding set
samples. Their corresponding non-compliant images from the same subject are
used as the inputs. All the samples are aligned and cropped to 64 × 64 Gy
images. After filtering, we have about 12 K target images (< 0.5% of the
original set) and 50 K input-target pairs. These data are used for pretrainingto initialize the network parameters and then fine-tuned using the CMU Multi-
PIE [36].
1. Download: Download high-res image (541KB)
2. Download: Download full-size image
Fig. 7. Samples from the VGG-Face dataset. Each row contains the face images
of the same person.
The CMU Multi-PIE itself is a facial expression dataset, but the facial
expression labels it uses are slightly different from modern expression
classification system. There are 4 expressions are useable (114,305 neutral
images, 19,817 surprise images, 22,696 disgust images and 47,388 happy
images), while the squint and scream are not regarded as expression now. Some
samples are shown in Fig. 8. It contains more than 750,000 images of 337
people taken from fifteen directions, and in nineteen illumination conditions.
There are four recording sessions in which subjects were instructed to display
neutral, happy, disgust and surprise facial expressions. It is more close to
our FER dataset in testing stage than the filtered VGG face dataset. We
selected only the five groups of the nearly frontal view faces (?45° to +
45°). The neutral images from the 0° view are used as our target image and the
guiding set.
1. Download: Download high-res image (195KB)
2. Download: Download full-size image
Fig. 8. Samples from the CMU Multi-PIE dataset. Each row contains images of
the same person.
### 5.2. Preprocessing
We follow the [6] to locate the 49 facial landmarks. Then, face alignment is
done to reduce in-plane rotation and crop the region of interest based on the
coordinates of these landmarks to a size of 64 × 64. An augmentation procedure
is employed to increase the number of training images and alleviate the chance
of over-fitting. We crop five 60 × 60 size patches from the center and four
corners, flip them horizontally and transfer them to grayscale images. All the
images are processed with the standard histogram equalization and linear plane
fitting to remove unbalanced illumination. Finally, we normalize them to have
zero mean and unit variance. In the testing phase, a single center crop with
the size of 60 × 60 is used as input data.
### 5.3. Implementation details
We use 64 × 64 Gy images as the input-target pairs for the neutral face
generation training. The filtered VGG-FaceNet and Multi-PIE images are used to
pre-train the neutral face generation network. We construct the guiding set
using the filtered VGG-FaceNet frontal neutral view and the 0° view neutral
images from the Multi-PIE. Following the experimental protocol in [6], we pre-
train our inception style convolutional groups, two branch FC layers on with
204,156 frontal view (?45°to 45°) face images selected from the CMU Multi-PIE
dataset for 300 epochs, optimizing the joint loss using stochastic gradient
descent with a momentum coefficient of 0.9. The initial network learning rate,
batch size, and weight decay parameter are set to 0.1, 128, 0.0001,
respectively based on optimizing the parameter choices using the validation
set. If the training loss increased by more than 25% or the validation
accuracy does not improve for ten epochs, the learning rate is halved and the
previous network with the best loss is reloaded. We select the highest
accuracy training epoch as our pre-trained model. In the fine-tuning stage,
the mini-batch set size is fixed to two times the number of expression classes
of the dataset. Random search is employed to select 2 images from each
expression class to form the mini-batch set. The tuplet-size is set to 12. In
all our experiments, we set ?= 0.1, ?1=3×10?2, ?2=10?2, ?3=0.3 determined bymanual tuning. The weight of joint learning ?=1. In the testing phase, only
the convolutional groups and expression classification branch with softmax are
used to recognize a single facial expression image.
The details of the encoder of HNG can be found in [7]. We fixed the latent
vector dimension to be 256 and found this configuration to be sufficient for
generating images for FER. A series of fractional-stride convolutions (FConv)
transforms the 256-dim vector z?R256 into a synthetic image x??R64×64, which
is of the same size as _x._ To further incorporate the prior knowledge of the
frontal neutral face's distribution into the training process, we introduce a
discriminator _Dis_ to distinguish the generated face image from the real
images in the guiding set.
The Leaky ReLU nonlinearities [37] are used in some Conv layers, where
LReLU(_x_) =max(x,0)+?min(x,0). In our experiments, we set ?=0.1. Optimizing
this minimax objective function will continuously push the output of the
generator to match the target distribution of the guiding set thus making the
synthesized facial images to be more photorealistic. All the CNN architectures
are implemented with the widely used deep learning tool ?Tensorflow [38].?
### 5.4. Ablation study
The Light CNN and the first five layers of the VGG-FaceNet are used to embed
the input, target or output images for the similarity measurements in
different feature spaces. It is obvious that these two networks incur
additional computation cost. We show in this section that they are needed.
The difference of our models trained with and without the Lid is subtle in
visual appearance, as can be seen in Fig. 9, but its effect on improving the
identity likeness of the generated faces can be measured by evaluating the
similarity of the input-outputs pairs using VGG-FaceNet. Fig. 10 shows the
distributions of L2 distances between the embeddings of the facial expression
images and their corresponding synthesized results, for models trained with
and without this loss. Schroff et al. [18] consider two FaceNet embeddings to
encode the same person if their L2 distance is less than 1.242. All of the
synthesized images using the identity-preserving loss pass this test using
FaceNet, but about 2% of the images would be identified as a different subject
by FaceNet when not using the identity-preserving loss. We investigated the
effect of the weight of identity preserving loss and show the identity
inconsistent percentage in Table 1.
1. Download: Download high-res image (769KB)
2. Download: Download full-size image
Fig. 9. Examples of input, target, generated normalized face by RG network,
generated normalized face by RG network without identity-preserving loss and
reconstructed neutral face using only the auto-encoder (AE) structure. Real
images are from CK + dataset which has an additional calss called contempt
(Co) class.
1. Download: Download high-res image (124KB)
2. Download: Download full-size image
Fig. 10. Histogram of VGG-Face net L2 error between the input face and the
normalized pairs on the FER data collection. Blue: with the identity
preserving loss which calculated by the Light CNN. Orange: without the
identity preserving loss. The 1.242 threshold was used by Schroff et al. [18]
to cluster identities in the LFW dataset. Without the Light CNN, about 2% of
the generated neutral faces would not be considered from the same subject as
the query faces.
Table 1. Percentage of identity errors as a function of the _?_ 1, the weight
of identity-preserving loss term.
_?_ 1| 0| 0.001| 0.005| 0.01| 0.015| 0.02| 0.025| 0.03| 0.035| 0.04| 0.05---|---|---|---|---|---|---|---|---|---|---|---
Percentage (%)| 2.13| 1.77| 1.06| 0.32| 0.13| 0.11| 0.1| 0.1| 0.1| 0.1| 0.1
The VGG-FaceNet is employed to calculate the feature level perceptual loss,
which is expected to make the generated result to keep more perceptually
important image attributes, for example sharp edges and textures. This loss
was empirically given the largest weight in our experiments. In practice,
without this part, we could never avoid the collapse of the adversarial
training to generate the human face structure.
We also analyzed the effect of different hyper-parameter values in RML. The
parameter _?_ is used to balance the softmax loss and metric learning loss. We
can see from the Fig. 11 (reproduced below) that the highest accuracy is
achieved when ?? [0.95,1]. As can be seen in Fig. 12 below, the networks were
not sensitive to ? ? [0.075, 0.125].
1. Download: Download high-res image (181KB)
2. Download: Download full-size image
Fig. 11. Facial recognition accuracy on CK + (7-class) dataset as a function
of _?_ , the parameter used to balance the softmax loss and metric learning
loss.
1. Download: Download high-res image (174KB)
2. Download: Download full-size image
Fig. 12. Facial recognition accuracy on CK + (7-class) dataset as a function
of parameter ? in LReLU.
### 5.5. Experimental results of FER
To evaluate the effectiveness of the proposed method, extensive experiments
have been conducted on four well-known publicly available facial expressions
datasets: CK + , MMI and Oulu-CASIA. Resulting IDFERM confusion matrices are
shown in Fig. 13.
1. Download: Download high-res image (727KB)
2. Download: Download full-size image
Fig. 13. Confusion matrix of the proposed IDFERM evaluated in the (a) CK +
seven-class, (b) CK + eight-class, (c) MMI and (d) Oulu-CASIA database. The
predicted labels and the ground truth labels are shown in the ordinate and
abscissa, respectively.
**CK + Dataset** [39]: We conducted both seven-class and eight-class
expression recognition experiments (_i.e_., without or with neutral
expression). In the setting without neutral sample, we directly compare to the
most nontrivial hard negative samples (i.e., generated normalized face), which
not only relaxes the requirements on the dataset (_i.e_., needing images of
all different expressions of the same person) to extract the identity-
disentangled expression representation but also reduces the number of
comparisons in training stage. The training time of metric learning part is
largely reduced as shown in Table 2. We note that [7] and IDFERM do need an
additional training stage (around 6 hours) for normalized face generation, but
the trained HNG network works for all of the FER datasets in our experiments
without fine-tuning and can be regarded as a ready-made tool for several down-
stream tasks.
Table 2. Comparation of the metric learning training time on the datasets with
a Titan X GPU.
Metric Learning Training Time| CK + (7-class)| CK + (8-class)| MMI| Oulu-CASIA
VIS
---|---|---|---|---
| | | |
Triplet Loss| 5 h 24mins| ?| 3 h 14mins| ?
2B(N + M) Softmax [6]| 1 h 47mins| ?| 54 mins| ?Data Augmentation [7]| 3 h 11mins| ?| 2 h 8mins| ?
IDFERM| 42 mins| 1 h 4 mins| 30 mins| 56 mins
In the testing stage, IDFERM recognizes a query facial expression image in
about 50 ms, which is satisfactory for many applications. Video-based methods
normally need a relatively longer sampling time (>0.25 s) to collect the whole
expression change session.
From Table 3, we can see that the identity-disentangled representation with
adaptive metric learning methods achieve higher accuracy than previous works.
Therefore, comparing the query sample with its generated normalized face
rather than all its other expressive query faces as in [6] is more efficient,
which is consistent with the relationship of those expressions as analyzed in
Fig. 2. However, just adding the generated neutral face images to original
dataset enlarged the number of comparisons [7]. As an efficient hard negative
mining scheme, the HNG offers the most nontrivial hard negative samples and
the RML can efficiently utilizes them and outperforms the other methods.
Table 3. Performance compares of the rank-1 recognition accuracy on the CK +
dataset in terms of 7 expressions and the MMI dataset (without neutral
expression).
Methods| CK + (seven-class)| MMI
---|---|---
MSR [13]| 91.4%| N/A
BNBN [42]| 96.7%| N/A
IBCNN [4]| 95.1%| N/A
STM [43]| 96.3%| N/A
CER(video) [44]| 92.34%| 70.12%
CDMML(video) [45]| 96.6%| N/A
STMExplet(video) [9]| 94.19%| 75.12%
DTAGN(video) [10]| 97.25%| 70.2%
IACNN [11]| 95.37%| 71.55%
2B(N + M) Softmax [6]| 97.1%| 78.53%
Data Augmentation [7]| 97.49%| 80.26%
IDFERM| 98.35%| 81.13%
The improved accuracy compared to the other methods is appealing in the image-
based 7-class CK + , MMI and Oulu-CASIA setting which do not have real-neutral
samples as training data. It also generalized well in dataset with neutral
expressions as shown in Table 4. With the added generated samples, the
accuracy of neutral class is improved to 99% as shown in Fig. 13(b).
Table 4. Performance comparison of the rank-1 recognition accuracy on the CK +
dataset in terms of the 8 expressions (with neutral expression).
Methods| CK + (eight-class)
---|---
AUDB [46]| 93.70%
CNN + AD [47]| 96.4%
FN2EN [5]| 96.8%
IDFERM| 97.76%
Benefitting from the generated data, the proposed IDFERM outperforms our
earlier approach [6] by 2.6% on MMI dataset and 1.25% on CK + dataset. Using
the same generated images as in [7], we achieve 0.87% and 0.86% improvements
on MMI and CK + datasets respectively, and the number of comparisons per
training batch is reduced from 2(_N + M_)_K_ (the _N_ and _M_ in MMI are 5 and
5 respectively [7]) to 2 K. As a consequence, the training time of the earlier
approach [7] for MMI dataset is 2hours 8mins, while the IDFERM needs only
30mins on a single Titan X GPU as shown in Table 2. Considering the improved
performance and reduced training time, the IDFERM is significantly moreefficient than [7] to utilize the generated data.
Limited training data has long been a challenge for facial expression
recognition. For example, [11] utilized the FER-2013 dataset for pre-training
and then fine-tuned their facial expression recognition network in CK + /MMI
datasets. The FER-2013 dataset is even larger than the Multi-PIE dataset. We
chose the Multi-PIE pretraining for fair comparison with previous works [6].
We added a comparison of recognition accuracy with/without the pre-training,
and show the results in Table 5. We can see that the pre-training can improve
the performance consistently.
Table 5. Comparation of the performance with/without pre-training using Multi-
PIE on CK + dataset.
Empty Cell| 2B(N + M) [6]| IDFERM
---|---|---
Empty Cell| Without Pre-training| With Pre-training| Without Pre-training|
With Pre-training
CK + (7-class)| 97.03%| 97.10%| 98.32%| 98.35%
MMI| 78.46%| 78.53%| 81.11%| 81.13%
**MMI Dataset** [40]: This dataset consists of 213 sequences, 208 sequences
from this data set containing frontal-view faces of 31 subjects were used in
our experiment as in [6]. Since the actual location of the peak frame is not
provided, we collect three frames in the middle of each image sequence and
associate them with the labels, which results in 624 images in our experiments
as in [6], [11]. We divided the MMI dataset into 10 subsets for person-
independent ten-fold cross validation. The sequence-level predictions are
obtained by choosing the class with the highest average score of the three
images. Consequently, 10-fold cross validation was conducted. This dataset
could be suitable to measure the recognition performance in realistic
situations when compared to other datasets.
With the identity-disentangled FER representation, the proposed methods
achieve substantial improvements over the previous best performance in MMI
dataset as shown in Table 3 and Fig. 13(c). The HNG can further boost the
accuracy by incorporating the prior information of normalized face and the
relationships of expressions within an applicable framework. Note that the
image sequences in the MMI dataset contain a full temporal pattern of
expressions, _i.e_., from neutral to apex, and then relaxed, and are
especially favored by these methods exploiting temporal information.
**Oulu-CASIA VIS Dataset** [41]: This dataset consists of 480 image sequences
of 80 individuals. This dataset is captured under the visible (VIS) normal
illumination conditions and is a subset of Oulu-CASIA NIR-VIS dataset. Each
individual poses six basic expressions as in MMI dataset. Only the last three
frames are used for individual-independent 10-fold cross validation, and the
total number of images is 1440 as in [5].
In Oulu-CASIA dataset, the IDFERM performs well in recognizing fear and happy
expressions, while angry is the hardest expression, which is mostly confused
with disgust as shown in Fig. 13(d). The performance results are shown in
Table 6 and are similar to those on the CK + and MMI datasets.
Table 6. Performance comparison of the rank-1 recognition accuracy on the
Oulu-CASIA VIS dataset in terms of the 6 expressions (without neutral
expression).
Methods| Oulu-CASIA VIS
---|---
STM-ExpLet(video) [9]| 74.59%
DTAGN(video) [10]| 81.46%
PPDN [48]| 84.59%FN2EN [5]| 87.1%
IDFERM| 88.25%
## 6\. Conclusions
We proposed and investigated a novel recognition via generation scheme termed
IDFERM to disentangle the identity factors from other factors that are
responsible for facial expression. The anchor-selection and threshold-tuning
problems present in previous approaches have been addressed in our proposed
adaptive deep metric learning paradigm. The identity-preserving neutral face
image generation is efficient for hard negative mining which requires fewer
similarity comparisons. However, our gray-scale image processing can lead to
information loss as the image quality is not emphasized in our framework as in
conventional image generation methods. Also, the adversarial game at image-
level is usually time-consuming. In future work, we intend to apply some
commonly used visual quality assessment methods for the generated images on
top of our model for better texture. Recent feature -level GAN's backbone can
be utilized to extend our framework for faster, more stable convergence
training, and more complex data structure (e.g., color images). We also expect
that the application of recognition via generation idea can facilitate several
other closely related tasks, e.g., face recognition, person re-ID, and pose-
invariant classification.
Recommended articles"
147,149,Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods,"['M Patacchiola', 'A Cangelosi']",2017,255,Expression in-the-Wild,neural network,"an approach based on Convolutional Neural Networks (CNNs)  on recently released in-the-wild  datasets. Moreover, we  ) with MAE expressed in degrees and Accuracy expressed in",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320317302327,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,"## Title: Head pose estimation in the wild using Convolutional Neural
Networks and adaptive gradient methods
* * View Open Manuscript
* Other access options
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Convolutional Neural Networks
8. 4\. Experiments
9. 5\. Conclusions
10. Acknowledgment
11. References
12. Vitae
Show full outline
## Cited by (188)
## Figures (15)
1. 2. 3. 4. 5. 6.
Show 9 more figures
## Tables (4)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
## Pattern Recognition
Volume 71, November 2017, Pages 132-143
# Head pose estimation in the wild using Convolutional Neural Networks and
adaptive gradient methods
Author links open overlay panelMassimiliano Patacchiola, Angelo Cangelosi
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2017.06.009Get rights and content
## Highlights
* ?
A convolutional neural network approach for head pose estimation is proposed.* ?
The performance of different network architectures has been measured.
* ?
The use of adaptive gradient methods leads to the state-of-the-art in wild
datasets.
* ?
We release a library based on our work which is available under open source
licence.
## Abstract
Head pose estimation is an old problem that is recently receiving new
attention because of possible applications in human-robot interaction,
augmented reality and driving assistance. However, most of the existing work
has been tested in controlled environments and is not robust enough for real-
world applications. In order to handle these limitations we propose an
approach based on Convolutional Neural Networks (CNNs) supplemented with the
most recent techniques adopted from the deep learning community. We evaluate
the performance of four architectures on recently released in-the-wild
datasets. Moreover, we investigate the use of dropout and adaptive gradient
methods giving a contribution to their ongoing validation. The results show
that joining CNNs and adaptive gradient methods leads to the state-of-the-art
in unconstrained head pose estimation.
* Previous article in issue
* Next article in issue
## Keywords
Convolutional Neural Networks
Head pose estimation
Adaptive gradient
Deep learning
## 1\. Introduction
In the last few years major advancements in robotics, augmented reality and
driving assistance have highlighted the need for robust methods to estimate
the head pose in real-world scenarios. For instance, robots are gradually
leaving factories and becoming part of our lives as companions and as
assistants. It has been shown that in human-robot interaction a coarse pose
estimation of the head is a fundamental prerequisite for building trust with
users during joint-attention tasks [1]. In the context of autonomous cars a
driving assistance system could take advantage of head pose estimation for
decelerating the car when pedestrians do not notice the presence of the
vehicle [2]. Moreover a similar system can be installed inside the vehicle and
used to monitor the driver?s awareness. The need of a robust head pose
estimation is not limited to these domains. There have been significant
applications in surveillance and anomaly detection, human-computer interaction
and crowd behavioural dynamics analysis [3]. All of these unconstrained
scenarios need an estimator which is resistant to variable environmental
conditions, and which can evaluate the focus of attention in absence of more
accurate information such as the gaze. Here it is necessary to specify what we
consider as a wild environment. We define as taken in a wild environment those
face images exhibiting a large variety in appearance (pose, expression,
ethnicity, age, gender, etc.), environmental conditions (artificial light,
shadows, etc.), and containing relevant occlusions (sunglasses, masks,
scarves, etc.). We will show how Convolutional Neural Networks (CNNs) can be
considered one of the best algorithms for robust head pose estimation in a
wild environment.
We can summarise the main contribution of our work in three points:* 1.
As far as we know this is the first work that has deeply investigated the use
of CNNs in head pose estimation. Our main contribution is a rigorous
evaluation of multiple CNN models and factors. The results are compared with
other algorithms, and show how an approach based on CNNs, dropout and adaptive
gradient methods represents the state of the art in head pose estimation.
* 2.
Deep learning is a rapidly growing field, which is bringing new techniques
that can significantly improve the performance of CNNs. Because these
techniques have been released in the last few years, there is still a
validation process for establishing their cross-domain usefulness. We explored
the role of adaptive gradient methods and we gave a valuable contribution to
their ongoing validation.
* 3.
The results obtained in this work have been used to implement a Python library
called Deepgaze. The library includes pre-trained CNNs based on Tensorflow [4]
which can run in real-time on GPUs and mobile devices. Deepgaze is released
under an open-source license and is available for both academic and commercial
purposes. The software is available on the author?s repository.1
## 2\. Related work
The head pose estimation problem has been investigated from different points
of view and with different techniques. Devices such as laser pointers, camera
arrays, stereo-cameras, magnetic and inertial sensors, have been used to get a
stable estimation in controlled situations [5]. More recently some good
results have been obtained with commercial depth cameras [6]. However the use
of these devices is not always feasible due to space constraints and to
technical problems when operating outdoors. Our work made use of RGB images
taken from monocular cameras which permits the greatest portability in real-
world applications, given the proliferation of such cameras in mobile phones
and laptops. This introduction is thus limited to this kind of approach. A
complete description of all the methods available is out of the scope of this
article so we refer the reader to a recent survey [5].
The main branches of a functional taxonomy in head pose estimation can be
considered to be appearance-based methods, model-based methods, manifold
embedding methods and nonlinear regression methods. Appearance-based methods
compare the view of a person?s head with discrete models which represent pose
labels [7], [8]. With different kinds of template matching techniques it is
possible to evaluate the similarity of the input features with the exemplar
set. Appearance-based methods are quite simple to implement but suffer from
some serious limitations. For instance, they cannot estimate discrete pose
locations without using interpolation methods. They assume that there is a
close similarity between the image and the pose space, and they can easily
associate a pose based on the resemblance with a wrong model. A common
solution to compensate these errors is to filter and convolve the models [9].
The effect of these manipulations is to highlight some features (e.g. vertical
and horizontal lines) and to remove part of the variations across the models.
However this solution is expensive because it requires to manually find the
right filters and to test the effects on the pose estimation. For all these
limitations the use of appearance-based algorithms has been decreasing over
time.
Model-based methods use geometric information, non-rigid facial models or
landmark locations to estimate the head pose. The most common model-based
approach consists in finding coplanar facial key-points and to estimate the
distance from a reference coordinate system [10]. This method requires highprecision and does not work for certain degenerative angles. Another common
approach consists in evaluating the position of multiple non-coplanar key-
points. The head pose is estimated assuming fixed geometric relationships
between the landmarks and comparing the position of the points with an average
mask obtained through anthropometric measurements [11]. Although there have
been improvements in key-points detection and tracking in real world
conditions [12], [13], the landmarks detection is the major limitation of this
approach. In general we can say that the accuracy of model-based methods is
correlated with the quality and quantity of the geometric cues extrapolated
from the image. In real world scenarios occlusions can often obscure facial
landmarks having a negative effect on the head pose prediction.
Manifold embedding methods consider an high-dimensional image to be
constrained in a low-dimensional manifold in which the head pose is estimated.
Dimensionality reduction techniques can be considered as part of the manifold
embedding category. In the standard approach principal component analysis
(PCA) is used in order to project the input images in a subspace and compare
them to the models [14]. The main problem of manifold embedding is that the
pose must be recovered across multiple sources of variation. In this sense,
other manifold embedding techniques have recently shown to be promising. In
[15] the problem of the source variation has been tackled learning a
similarity kernel through geometric invariant features. This method showed a
good reliability on benchmark datasets. However further research is needed in
order to reach state of the art performances.
Nonlinear regression methods use a labelled training set to create a nonlinear
mapping from images to poses. We can consider CNNs as part of these methods.
Nonlinear methods have many advantages. These algorithms work properly with
high and low resolution images, and they have demonstrated their
representational ability in tolerating systematic errors in the training set
data. For instance, an approach based on a multilayer perceptron [16] had for
long time the lowest reported mean angular error on the Prima head pose
dataset [5]. The main disadvantages of these methods are two. The first is the
need of a consistent dataset in order to train the parameters. The second is
the need of a precise head localisation before the pose estimation step. The
first issue can be overcome thanks to recently released datasets containing a
large number of images. The second issue can be attenuated using CNNs instead
of multi layer perceptrons which guarantee a good shift and distortion
invariance. In this sense CNNs could be the elective technique for mastering
the head pose estimation problem, since recent advances in deep learning made
possible to easily train complex CNNs on large datasets. Despite their
potential, the use of CNNs for head pose estimation has been sporadic. An
approach based on CNNs and energy-based models has been proposed for
simultaneous face detection and pose estimation [17]. The authors trained the
model on a dataset containing images taken in laboratory conditions, and
validated it on datasets containing frontal faces with in-plane rotations and
faces in profile. The prediction of the network was limited to two degrees of
freedom. This work is valuable but it is ten years old and at that time
advanced techniques for training deep networks were not available. Moreover
the results relative to the head pose estimation accuracy were not included. A
more recent work investigated the use of CNNs with low-resolution images from
monocular cameras [18], obtaining the best reported result on the Biwi Kinect
Head Pose dataset [19]. The authors did not use any of the most recent
techniques available such as dropout or adaptive methods, and their results
have been validated on a single dataset that does not contain in-the-wild
images. However the results obtained confirm the validity of CNNs in head poseestimation. In [20] CNNs have been used for monitoring the driver alertness.
The authors used a six-layer CNN to classify five discrete head poses: left,
right, up, down and frontal. Because of the limits implicit in a discrete
classification with only five poses we cannot compare this work with the
others presented here. In [21] the authors used deep convolutional networks to
estimate the head pose in multimodal RGB-D videos. Depth information is not
always available due to space constraints and to problems operating outdoors.
The authors did not test the effect of different numbers of layers or
parameters, moreover they did not use any kind of adaptive gradient method or
regularisation to improve the performance of the network.
Given the lack of satisfying work we wanted to add something more complete and
modern to the existing literature. In the next sections we shortly explain how
CNNs work and how dropout and adaptive gradient methods can boost their
performance. The next section is not intended to be an exhaustive description
of the mechanics behind CNNs, instead it is a way to introduce some key
concepts and to define the notation used in the rest of the article.
## 3\. Convolutional Neural Networks
In recent years deep convolutional networks have showed their strength in
numerous pattern recognition contests. Some remarkable achievements have been
recently obtained in object detection [22], facial expression recognition [23]
and scene classification [24]. This technology is increasingly used in
commercial applications such as content filtering in social networks,
recommendation systems in e-commerce websites or image classifiers in web-
search engines. The deep learning revolution has been driven by the diffusion
of cheap graphics processing units (GPUs), originally used for video games.
The use of GPUs speeds up matrix and vector multiplications, which are the
core operations in neural network training. Because the general introduction
of CNNs is well established by now, we will not extend this section any
further. Instead we refer the reader to the following article [25]. In the
next section we give a brief description of the main building blocks of a CNN,
introducing the notation used in the rest of the article.
### 3.1. Notation
We define a CNN as an ensemble of many single units grouped in three-
dimensional layers. The units inside a layer are connected to a small region
of the layer before it with connections called kernels (or filters, weights,
parameters). The input to a CNN is a matrix _X_ of dimension _m_ × _m_ × _r_ ,
where _m_ is the height and width of the matrix and _r_ is the number of
channels. The convolutional layer has _k_ kernels of size _n_ × _n_ × _q_ ,
where _n_ < _m_ and _q_ ? _r_. The convolution multiplies each element of _X_
with its local neighbours, weighted by the kernel _W_ , generating _k_ feature
maps of size m?n+1. The convolutional layer is often followed by a mean or max
pooling layer which permits subsampling the maps over a _p_ × _p_ local
region, with 2 ? _p_ ? 5. During the training phase the kernels are adjusted
following a well-known algorithm called backpropagation [26]. The
backpropagation minimises a loss (or error) function _J_(_w_) with an
iterative process of gradient descent that updates _w_ at t+1 using the
gradient information at _t_ , as expressed in the following equation:
(1)wt+1=wt???E|J(wt)|+?vt
The expectation is approximated with the cost and gradient over the full
training set. The value _?_ is called learning rate and corresponds to the
step taken by the algorithm in the direction of the gradient. The value _?_ ?
[0, 1] is called momentum and is a technique for accumulating a velocity
vector _v_ in the direction of persistent reduction of the loss function. A
variation of the standard gradient descent is called Stochastic GradientDescent (SGD). The SGD computes the gradient using a few training examples or
mini-batches instead of the whole training set. Using the stochastic approach
the variance is reduced leading to a more stable convergence.
There are different kinds of loss functions. In our experiments we used the
sum of squares of the differences between the target value _y_ and the
estimated value y^: (2)J(w)=?n=1N(yn?y^n)2+??l=1Lwl2
The additive factor _?_ is an _L_ 2 regularisation term, used in each hidden
layer _l_ to prevent a very large growth of the parameters during the
minimisation process.
In the next sections we will focus on some recent techniques developed by the
deep learning community in the very last few years, which made the training of
deep architectures more manageable leading to better results. These techniques
are dropout and adaptive gradient methods.
### 3.2. Dropout
In networks with a large number of parameters the generalisation on a new set
of data can be compromised due to memorisation of the training set which leads
to overfitting. Dropout [27] addresses this problem by randomly dropping units
and connections during the training phase, preventing co-adaptation. We can
summarise the operations involved in dropout in a few lines. Let?s define _r_
as a vector of Bernoulli random variables, where each variable has probability
_p_ of being 1 and a probability 1?p of being 0. For each hidden layer _l_ the
vector _r_ is sampled and then multiplied elementwise with _y_ the output
vector of that layer. The result is a thinned vector y?:
(3)r(l)?Bernoulli(p)y?(l)=r(l)?y(l)
Experimental evidence shows how dropout introduces noise in the gradients and
produces a cancellation effect [27]. To deal with this issue it is recommended
to use a higher learning rate and momentum. In order to use a higher learning
rate without a very large growth of the weights, another form of
regularization can be used at the same time, such as _L_ 2 or max-norm. The
probability _p_ is another hyperparameter to tune. However numerous
experimental results suggest that a value of p=0.5 produces the best
performance [27], so we used this value in our experiments. A graphical
representation of dropout is presented in Fig. 1.
1. Download: Download high-res image (88KB)
2. Download: Download full-size image
Fig. 1. Graphical representation of the dropout process. The figure on the
left represents a generic neural network composed of three layers. The figure
on the right represents the same network after applying the dropout with 50%
probability of keeping a unit.
### 3.3. Adaptive gradient methods
Neural networks are not off-the-shelf algorithms, and generally the research
of the best hyperparameters can be extremely time consuming. One of the most
important parameters is the learning rate. When the learning rate is too high
the optimisation can diverge, on the contrary if it is too low the
optimisation can be slow. To solve these problems some adaptive gradient
methods have been proposed recently. Adaptive gradient methods use first order
information to approximate second order information and then find an optimal
step size.
One of the best adaptive methods introduced recently is Adagrad [28]. This
optimiser incorporates information about the features to control the gradient
step. The procedure associates a low learning rate with frequently occurring
features and high learning rate with infrequent features. Therefore, the
adaptation facilitates identifying the most predictive features and it is well
suited for sparse data. The authors tested the algorithm on different imageand text databases [28], showing how it outperforms the non-adaptive
counterpart. The updating rule for the weights _w_ following the Adagrad
algorithm can be expressed with the following equation: (4)wt+1=wt??Gt+??gt
The matrix _G_ is a diagonal matrix where each entry in the main diagonal is
the sum of the squares of the previous gradients up to time _t_. The value ?
is a small value used to avoid division by zero. The symbol ? represent a
matrix-vector multiplication. The main problem with Adagrad is the
accumulation of squared gradients in _G_ that lead to an infinitesimally small
learning rate _?_ and then to a loss in knowledge accumulation.
To solve the problem related with Adagrad an extension called Adadelta [29]
has been proposed. Adadelta does not accumulate all the past gradients but it
constrains the window to a fixed interval. The denominator of the Eq. (4) is
then replaced by a moving average _E_[_g_ 2] which represents all the past
squared gradients. The advantage of this solution is that the moving average
depends only on the previous average and the current gradient, as follows:
(5)wt+1=wt??E[g2]t+?gt
As experimentally shown in [29] Adadelta is particularly robust and it
guarantees convergence with different learning rate values. Another effective
method to solve the Adagrad issue is an unpublished algorithm called RMSProp
[30]. RMSProp has an updating rule which is similar to Eq. (5) with the only
difference being that it introduces a decaying value _?_ which specifies how
long the old gradients in _E_[_g_ 2] are kept. The value _E_[_g_ 2] at time
_t_ is then updated in this way: (6)E[g2]t=?E[g2]t?1+gt2
The authors suggest some default values for the learning rate and the decaying
value which should be closer to ?=0.001 and ?=0.9. In our experiments we used
this configuration.
Finally we want to introduce another method called Adaptive Moment Estimation
(Adam) [31]. Like Adadelta and RMSProp, Adam stores a decaying average of past
gradients and squared gradients. The two decaying averages are then used to
estimate _m_ 1 and _m_ 2 which are the first and second moment (mean and
variance) of the gradient. The updating rule for Adam can be expressed as
follows: (7)wt+1=wt??m2+?m1
The two moments _m_ 1 and _m_ 2 are taken at time _t_ and before the weights
update they are corrected to limit a bias toward zero during the first steps.
The moments are regulated by two decaying factors _?_ 1 and _?_ 2\. The
authors suggest to initialise these parameters to standard values ?1=0.9 and
?2=0.999. We used these values in our experiments.
## 4\. Experiments
In this section we report the results obtained using CNNs, dropout and
adaptive gradient methods on three public datasets: the Prima head-pose
dataset [32], the Annotated Facial Landmarks in the Wild (AFLW) dataset [33],
and the Annotated Face in the Wild (AFW) dataset [34]. The former is a well-
known dataset which has been around for more than ten years, and it is
considered a classic benchmark for head pose algorithms. The second is a
recently released in-the-wild dataset, and it has the largest number of
annotated poses currently available. The third is a small in-the-wild dataset
used mainly for benchmarks. Other details about these datasets are available
in Sections 4.1?4.3.
In these experiments we used a total of four networks: A, B, C, D. The
architecture A is a standard LeNet-5 [35] and with six layers and 4.3 × 106
parameters. The graphical representation of this architecture is presented in
Fig. 2. The architecture B has one more convolutional layer and one more
pooling layer. The number of parameters is slightly higher (4.6 × 106). The
idea is to keep the architectures as similar as possible to isolate the effectof the additional layers. It is hard to define a priori how many parameters
lead to a good performance, for this reason the third and fourth networks have
more weights. The architecture C has a high number of parameters (8.5 × 106).
The fourth architecture (D) is similar to a standard AlexNet [22] and it has a
total of 9.0 × 106 parameters. A graphical comparison of the four
architectures is reported in Fig. 3.
1. Download: Download high-res image (136KB)
2. Download: Download full-size image
Fig. 2. Graphical representation of a Convolutional Neural Network with two
convolutional (C1 and C2), two subsampling (P1 and P2) and two fully connected
(D1 and D2) layers. The label above each layer specifies number of elements
and size (rows × columns) of the future maps. For the dense layers we reported
the number of units. We used the following colour convention to identify the
different layers: green for convolution, orange for subsampling, and red for
dense layers. (For interpretation of the references to colour in this figure
legend, the reader is referred to the web version of this article.)
1. Download: Download high-res image (406KB)
2. Download: Download full-size image
Fig. 3. Comparison of the four architectures used in our experiments. The
label below each layer represents the number and size (rows × columns) of the
future maps. For the dense layers we reported only the number of units. The
networks are organised in order of complexity, on the left there is the
network with less parameters and on the right the network with more. Network A
has 4.3 × 106 parameters, whereas network B has two more layers and a total of
4.6 × 106. Network C has 8.5 × 106 parameters, whereas network D has two more
layers for a total of 9.0 × 106 parameters. We used the following colour
convention to identify the different layers: green for convolution, orange for
subsampling, and red for dense layers. (For interpretation of the references
to colour in this figure legend, the reader is referred to the web version of
this article.)
The approach we used is based on a divide-and-conquer strategy. We trained
different CNNs for each degree of freedom. This kind of strategy has the
advantage of splitting the main problem into different sub-problems which are
easier to manage. Having a specialised network for roll, pitch and yaw,
permits fine tuning the network for a specific degree of freedom without
losing the predictive power obtained on another one. Our methodology consisted
of two parts. First, we evaluate which optimiser was better suited to a
specific dataset. Second, we tested CNNs with a variable number of layers and
parameters to understand the impact of deeper architectures. As discussed in
[5] the Mean Absolute Error (MAE) and the Standard Deviation (STD) are the
best metric of accuracy in head pose datasets with discrete or continuous
labels. We report both MAE and STD and we use them for comparing results. In
all of the experiments we used the same hardware configuration: a multi-core
workstation with 32GB of RAM and a GPU NVIDIA Tesla K-40. The experiments have
been implemented in Python using the TensorFlow library [4].
### 4.1. Prima head-pose dataset
This dataset consists of 2790 monocular face images of 15 subjects. The
subjects range in age from 20 to 40 years old, five possessing facial hair and
seven wearing glasses. Pitch and yaw angles are in the range [?90?,90?] for a
total of 93 discrete poses for each person (Fig. 4). Two series of pictures
with different lighting conditions are provided increasing the total number of
available pictures to 186 per person. The head pose is estimated by
directional evaluation, as a result this dataset is extremely challenging
because the predictor must deal with substantial errors and poor uniformitybetween subjects.
1. Download: Download high-res image (204KB)
2. Download: Download full-size image
Fig. 4. This figure represents a collection of images taken from the Prima
dataset as they appear after cropping and scaling.
Many different methods have been tested on the Prima dataset. In [14] a
performance comparison is made between high-order Singular Value Decomposition
(SVD), Principal Component Analysis (PCA) and locally embedded analysis.
Neural networks-based methods have been tested in [16], [36]. This dataset is
the only one in which the human performance has been measured [32].
#### 4.1.1. Methods
This experiment investigated the influence of three factors (network type,
optimiser, dropout) and it consisted of two phases:
* 1.
Optimiser selection. In the first phase we used a standard network to select
the best optimiser for this dataset. The standard network used is LeNet-5 [35]
shown in Fig. 2. The optimisers used were the four described in Section 3.3
plus SGD and SGD with momentum.
* 2.
Network selection. In the second phase we used the best optimiser selected
previously for finding the best architecture for each degree of freedom.
In this experiment we used the sigmoid activation function which produced a
continuous output in the range [0, 1]. As a loss function we used the sum of
squares of the differences between the target value _y_ and the estimated
value y^, reported in Eq. (2), with ?=5×10?4. We trained the networks for 20,
000 epochs, using mini-batches of size 64. The network weights were sampled
from a Gaussian distribution (?=0,?=0.1), and any values that had a magnitude
more than two standard deviations from the mean were dropped and re-sampled.
The weights were updated using Eq. (1). For each optimiser we used the
learning rate value recommended by the authors (see Section 3.3). When the
authors did not suggest any standard value or when the recommended values did
not lead to convergence we used a grid-search procedure. Starting from ?=0.1
we observed the loss function in the first 1000 epochs. In case of divergence
the learning rate was divided by 2 and the procedure repeated. The value which
permitted the fastest convergence was then selected and used for the training.
Because we used dropout we set the value of the momentum to 0.95 as
recommended in [27]. The faces were isolated using the bounding boxes included
in the dataset and then resized to 64 × 64 pixels.
Two kinds of cross-validation tests were applied to this dataset, in
accordance with the procedures reported in [37]. Testing on known faces was
done by dividing the images per series into two separate folds. The two folds
contained images of the same subjects taken under different lighting
conditions. Testing on unknown faces is done using the Jack-Knife (leave-one-
out) procedure, which consists of training the algorithm on all the subjects
but one, which is used for testing. The procedure was repeated fifteen times,
leaving out each subject. The mean value of all the measurements is considered
to be the final score. We used the leave-one-out procedure to select the best
optimiser and the best network. In a second moment the best configuration was
tested with the known-subjects procedure. During the training we did not use
any kind of early stopping technique. The use of the early stopping requires
to monitor the error on a validation set taken from the test set, but the
standard procedure reported in [37] does not take it into account. Considering
that the other methods used the standard procedure, we decided to keep the
test set unchanged in order to have a fair comparison. However the observationof the error can be useful for understanding if there is overfitting. For this
reason, in a separate phase, we generated a validation set randomly picking
50% of the images contained in the test set. We observed the root mean square
error (RMSE) for both training and validation set.
Training the architecture A for 20, 000 epochs on the two-fold dataset took
4.2 h, while training it on the fifteen-fold dataset took 18.75 h.
#### 4.1.2. Results
The results in term of MAE for the first phase (optimiser selection) are
reported in Table 1. We analysed the convergence speed, observing the loss
value during each of the 20, 000 epochs. For each optimiser we considered the
loss values obtained from the mean of the 15 subject in the leave-one-out
test, and we plotted the resulting graphs for both pitch (Fig. 5) and yaw
(Fig. 6). The Adam optimiser had the lowest MAEs for both pitch and yaw (10.71
± 11.04, 7.74 ± 8.03). A similar performance has been obtained with RMSProp
(10.75 ± 10.51, 8.3 ± 8.17).
Table 1. In this table we report the results obtained on the Prima dataset for
leave-one-out (unknown subjects) test using different optimisers. These
results have been obtained training the architecture A for 20, 000 epochs
(18.75 h). The results are in terms of MAE (accuracy) with MAE expressed in
degrees and Accuracy expressed in percentage. The best scores are in bold.
Optimiser| Pitch| Yaw
---|---|---
Adadelta [29]| 20.71(35.3)| 13.7(35.23)
Adagrad [28]| 12.57(53.69)| 9.23(54.73)
Adam [31]| **10.71(60.93)**| **7.74(62.33)**
RMSProp [30]| 10.75(57.67)| 8.30(58.92)
SGD| 12.87(52.11)| 9.06(56.06)
SGD (Momentum)| 13.26(49.35)| 8.66(56.81)
1. Download: Download high-res image (187KB)
2. Download: Download full-size image
Fig. 5. Comparison of different optimisers (trained on network A) for the
estimation of the pitch angle on the Prima dataset.
1. Download: Download high-res image (181KB)
2. Download: Download full-size image
Fig. 6. Comparison of different optimisers (trained on network A) for the
estimation of the yaw angle on the Prima dataset.
In the second phase (network selection) we used Adam to train the four
architectures shown in Fig. 3. The best performances for both pitch and yaw
are obtained with the architecture A (10.71 ± 11.04, 7.74 ± 8.03). A
comparison of the four architectures is shown in Fig. 7. Mapping the
continuous output in 9 discrete categories for pitch and 13 discrete
categories for yaw, we were able to interpret the results in terms of
classification. The best architecture (A) have an accuracy of 60.93% (pitch)
and 62.33% (yaw) on unknown subjects. The accuracy is even higher on known
subjects where it reaches 69.26% (pitch) and 67.61% (yaw). The normalised
confusion tables for the best architecture are reported in form of heatmaps in
Fig. 8.
1. Download: Download high-res image (303KB)
2. Download: Download full-size image
Fig. 7. Comparison of the performances of the four networks trained with the
Adam optimiser in pitch and yaw estimation of unknown subjects (Prima
dataset). The STD has been shrunk by a factor of two for graphical reason.
1. Download: Download high-res image (92KB)
2. Download: Download full-size imageFig. 8. Representation of the confusion tables for pitch (left) yaw angle
(right) of the best architecture (A) on unknown subjects (Prima dataset). Each
row of the tables represents the instances in a predicted class while each
column represents the instances in an actual class.
We used the best architecture also for the know-subjects test. We obtained a
MAE of 8.06 ± 8.88 for the pitch estimation (accuracy 73.91%), and a MAE of
6.93 ± 7.32 for the yaw estimation (accuracy 66.6%).
To investigate the presence of overfitting, we trained the best architecture
using the Adam optimiser on the unknown-subjects test and we generated a
validation set randomly picking 50% of the images from the test set. We
monitored the RMSE for both training and validation set. The results for the
pitch and yaw estimation are reported in Figs. 9 and 10 and represent the mean
RMSE for each one of the 15 subjects considered in the leave-one-out
procedure.
1. Download: Download high-res image (120KB)
2. Download: Download full-size image
Fig. 9. Performance of the best architecture (A) in terms of RMSE (degrees) on
the training and validation set for the estimation of the pitch angle on the
Prima dataset (unknown subjects test).
1. Download: Download high-res image (116KB)
2. Download: Download full-size image
Fig. 10. Performance of the best architecture (A) in terms of RMSE (degrees)
on the training and validation set for the estimation of the yaw angle on the
Prima dataset (unknown subjects test).
We did some experiments to check if data augmentation of the images
(horizontal flip) had an impact on the final score. We did not notice any
significant improvement.
The best results on the Prima dataset have been obtained with the smallest
network (A). However it is possible that networks with less parameters could
perform better than our best architecture. This conclusion is reasonable since
the Prima datasets contains a few thousands images and overfitting problems
could deteriorate the performances of larger networks. To further investigate
this hypothesis we used the Adam optimiser to train two networks. The networks
were similar to LeNet-5 [35] with six layers organised as in Fig. 2. The first
network had 2.1 × 106 parameters and the following architecture: C1 = 32(64 ×
64), P1 = 32(32 × 32), C2 = 64(32 × 32), P2 = 64(16 × 16), D1 = 128, D2 = 1.
The second network had 0.5 × 106 parameters and the following architecture: C1
= 16(64 × 64), P1 = 16(32 × 32), C2 = 32(32 × 32), P2 = 32(16 × 16), D1 = 64,
D2 = 1. The results did not show any major improvement except for the first
architecture that got a slightly lower MAE (10.57 ± 10.78) and an higher
accuracy (61.4%) for the pitch estimation on unknown subjects. The same
network had a worse performance in yaw estimation with an MAE of 8.36 ± 8.42
and accuracy of 57.67%. The network with 0.5 × 106 parameters reported a score
of 11.14 ± 11.24 (accuracy 58.02%) for pitch and 8.38 ± 8.57 (accuracy 57.53%)
for yaw, which is significantly below the score obtained with architecture A.
Also in the known-subjects test we did not observe any major improvement. The
network with 2.1 × 106 parameters obtained an MAE of 8.24 ± 9.81 (accuracy
71.15%) for pitch and 7.23 ± 7.43 (accuracy 64.51%) for yaw. The network with
0.5 × 106 parameters obtained an MAE of 8.9 ± 10.35 (accuracy 70.82%) for
pitch and 7.23 ± 7.42 (accuracy 64.19%) for yaw. We hypothesise that the use
of dropout and L2 regularization helped to prevent overfitting in larger
architectures leading to stabler solutions.
The comparison between our approach and other methods is reported in Table 2.
CNNs perform better than any other methods on the two-fold test for knownsubjects and in the leave-one-out test for unknown subjects. The comparison in
terms of MAE between human and our method shows how CNNs outperform naive
humans in both pitch and yaw, and pre-trained humans only for the yaw
estimation. The comparison in terms of accuracy (Table 2) shows that our
method has the best reported scores for tests on known and unknown subjects.
In this case the performance of pre-trained humans for pitch estimation on
unknown subjects (59%) is lower than our score (60.93%).
Table 2. In this table we compare the results of our method with the result
obtained by other authors on the Prima head pose dataset. These results have
been obtained training architecture A for 20, 000 epochs on both unknown
(18.75 h) and know (4.2 h) test with the Adam optimiser. The results are in
term of MAE(accuracy), with MAE expressed in degrees and Accuracy expressed in
percentage. The best scores are in bold.
Method| Unknown Subjects| Known Subjects
---|---|---
Empty Cell| Pitch| Yaw| Pitch| Yaw
Human Performance [37]| 12.6(48)| 11.9(42.4)| ?| ?
Human Performance (training) [37]| 9.4(59)| 11.8(40.7)| ?| ?
Locally Embedded Analysis [14]| ?| ?| 17.44(50.61)| 15.88(45.16)
High-order SVD [14]| ?| ?| 17.97(54.84)| 12.9(49.25)
PCA [14]| ?| ?| 14.98(57.99)| 14.11(55.2)
Neural Network [36]| ?| ?| 12.77(52.1)| 12.3(41.8)
Large Margin Likelihoods [46]| ?| ?| 10.5| 9.1
Associative Memories [37]| 15.9(43.9)| 10.3(50.04)| 10.1(61.7)| 8.5(60.8)
Steerable Filters [47]| 13.8| 11.0| 12.4| 9.6
Steerable Filters (manual) [47]| 11.4| 9.97| 10.1| 8.7
CNNs [our]| **10.57(61.4)**| **7.74(62.33)**| **8.06(73.91)**| **6.93(66.6)**
### 4.2. Annotated facial landmarks in the wild
The AFLW dataset [33] provides a collection of 21,997 annotated images
gathered from an image hosting website. The dataset contains a large variety
of appearances (age, ethnicity, occlusions, expressions, etc.), lighting and
environmental conditions for both genders (56% female, 44% male). Images
compatible with the datasets are shown in Fig. 11. As reported by the authors
the ratio of non-frontal faces (66%) is higher than in other databases. The
head pose is estimated from 21 manually annotated landmarks using the POSIT
algorithm [38].
1. Download: Download high-res image (300KB)
2. Download: Download full-size image
Fig. 11. This figure represents a collection of images which are compatible
with the AFLW and the AFW datasets (the licenses do not allow publishing the
original images). The AFLW and the AFW datasets contain a large variety of
appearances (age, ethnicity, occlusions, expressions, etc.), lighting and
environmental conditions for both genders.
Different methods have been tested on this dataset [15], [34], [39], [40],
[41]. To compare our work with the others we used the data reported in [15],
where the authors describe the performance in terms of MAE for all of these
methods, although limited to the yaw angle. The accuracy has been measured by
dividing the range [?90?,90?] into steps of 15°, and it is intended as the
percentage of images within ± 15 degrees of error. In our measurement of the
yaw accuracy we used the same metric to enable a comparison with these works.
#### 4.2.1. Methods
In this experiment we manipulated two factors: network type and optimiser. The
experiment was divided into two phases:
* 1.Optimiser selection. In the first phase we used a standard network to select
the best optimiser for this dataset. We trained the LeNet-5 [35] using the
four optimisers described in Section 3.3 plus SGD and SGD with momentum.
* 2.
Network selection. In the second phase we used the optimiser selected
previously for finding the best architecture among the four previously
described (Fig. 3).
The AFLW dataset is extremely challenging because the distribution of poses is
not uniform. Roll has a mean and standard deviation of 1.07 ± 14.04°, pitch
?8.1±13.4?, and yaw 1.91 ± 41.8°. Because of this asymmetrical distribution it
is difficult to uniformly map the angles in a continuous interval without
losing precision. For this reason we decided to take only images above and
below 2.698 standard deviations (1.5 the interquartile range of the lower and
higher quartile). The final distribution contained poses in the following
ranges: roll=[?25?,25?], pitch=[?45?,45?], yaw=[?100?,100?]. To further
compensate for the high variability in the angle distribution, we decided to
use the hyperbolic tangent activation function. Using the hyperbolic function
the output of the networks was constrained to within the range [?1,1] instead
of [0, 1]. We considered only faces with a bounding box greater or equal to 64
× 64 pixels. In total we discarded very few images (less than 5%) because the
mean and standard deviation of the bounding boxes was 300 ± 343 pixels, with
84% of the samples distributed between 110 and 647 pixels. In the first phase
we trained the networks for 20, 000 epochs and in the second phase for 30, 000
(mini-batches of size 64). In both phases we used dropout whit p=0.5. In the
second phase we decided to extend the number of epochs to 30000 because the
training was much faster compared to the Prima dataset (e.g. 8.0 vs. 18.75 h
for network A). Training the network for more epochs generally leads to
stabler solutions and it is recommended when using dropout without time
constraint [22].
We applied a five-fold cross-validation procedure to test our networks on the
dataset. After randomly dividing the dataset into five folds, we trained the
models on four of the five folds and we tested them on the remaining one. The
procedure was repeated five times, independently testing the model on each one
of the five folds. The mean of the five tests was considered to be the final
score. The total number of images included in each one of the five folds was
16, 696, whereas the number of images left for the test was 4173. To test the
classification accuracy we split the yaw poses range [?100?,100?] into
different categories using steps of 15°. The accuracy is intended as the
percentage of images with ± 15° error. This measure is in line with the one
used in [15] and makes it possible to compare the results obtained with the
different methods reported in that article. To have more reliable results we
took the mean of the five folds.
The hyper-parameters selection followed the same methodology described in
Section 4.1.1. The training of roll, pitch and yaw for a single experimental
condition (20, 000 epochs) took 8.0 h on the smallest architecture (A) and 9.8
h on the largest architecture (D).
#### 4.2.2. Results
The results for the first phase (optimiser selection) are reported in Table 3.
The results show that the RMSProp had the lowest reported MAE for roll, pitch
and yaw (4.4 ± 4.35, 7.15 ± 6.0, 11.04 ± 10.86). As it is possible to see in
Fig. 12 the RMSProp had the fastest convergence rate and it reached the lowest
loss values.
Table 3. In this table we report the results obtained on the AFLW dataset for
the five-fold cross validation test. These results have been obtained trainingthe architecture A for 30, 000 epochs (14.6 h). The results are in terms of
MAE(accuracy). MAE is expressed in degrees and Accuracy is expressed in
percentage. The best scores are in bold.
Optimiser| Roll| Pitch| Yaw
---|---|---|---
Adadelta [29]| 6.26(22.2)| 9.98(42.67)| 21.87(22.19)
Adagrad [28]| 5.27(69.23)| 8.24(51.29)| 17.83(28.8)
Adam [31]| 4.81(72.41)| 7.78(53.49)| 13.5(38.56)
RMSProp [30]| **4.4(75.14)**| **7.15(55.98)**| **11.04(44.54)**
SGD| 7.0(58.64)| 9.89(42.15)| 22.98(22.23)
SGD momentum| 6.17(63.19)| 10.3(42.37)| 21.31(22.52)
1. Download: Download high-res image (281KB)
2. Download: Download full-size image
Fig. 12. Comparison of the convergence speed between the six optimisers used
to train architecture A for yaw estimation on the AFLW dataset. The loss
values are the mean of the five fold.
The results for the second phase (network selection) are reported in Fig. 13.
The architecture B had the lowest MAE (4.15 ± 3.87, 6.8 ± 5.64, 9.51 ± 9.21).
Mapping the continuous output in three discrete categories for roll, nine for
pitch and 13 for yaw, we were able to interpret the results in terms of
classification. The accuracy for roll, pitch and yaw was originally 76.55%,
57.68% and 48.55%. If we consider an error of ± 15° the accuracy drastically
increases to 99.66%, 97.24% and 92.47%. To better visualise the accuracy we
report in Fig. 14 the confusion table of the best network (second
architecture, RMSProp optimiser) for roll, pitch and yaw classification.
1. Download: Download high-res image (347KB)
2. Download: Download full-size image
Fig. 13. Comparison in term of MAE between four architectures for roll pitch
and yaw using the RMSProp optimiser on the AFLW dataset. The STD has been
shrunk by a factor of two for graphical reason.
1. Download: Download high-res image (94KB)
2. Download: Download full-size image
Fig. 14. Representation of the confusion tables for roll (left), pitch
(centre) and yaw (right) of the best architecture (B) trained with the RMSProp
optimiser on the AFLW dataset. Each row of the tables represents the instances
in a predicted class while each column represents the instances in an actual
class.
Similarly to what we did for the Prima dataset, we investigated the presence
of overfitting in a separate phase. We trained the best architecture using
RMSProp on the five-fold test and we generated a validation set randomly
picking 50% of the images from the test fold. We monitored the RMSE for both
training and validation set. The results for the yaw estimation are reported
in Fig. 15 and represent the mean RMSE for each one of the five folds in
20,000 epochs.
1. Download: Download high-res image (145KB)
2. Download: Download full-size image
Fig. 15. Performance of the best architecture (A) in terms of RMSE (degrees)
on the training and validation set for the estimation of the yaw angle on the
AFLW dataset.
The comparison with other methods (Table 4) shows that CNNs perform better
than any other algorithm in terms of MAE and accuracy.
Table 4. In this table we report the results in term of MAE (degrees) and
accuracy (percentage) obtained with different methods for the yaw estimation
on the AFLW and the AFW datasets. Our results have been obtained training thearchitecture B for 30, 000 epochs (14.6 h) with the RMSProp optimiser. MAE is
expressed in degrees and Accuracy is expressed in percentage. The best scores
are in bold.
Method| AFLW| AFW
---|---|---
Mixture of trees [34]| 46.54(15.72)| 40.17(26.07)
Patch Based [39]| 38.39(23.87)| 41.67(21.36)
Feature Embedding [41]| 33.01(32.82)| 28.15(40.38)
Learning Manifold [40]| 16.31(63.13)| 18.26(58.33)
Approximate View Manifold [15]| 17.48(58.05)| 17.2(58.33)
CNNs [our]| **9.51(92.47)**| **16.73(75.29)**
### 4.3. Annotated face in the wild
The AFW dataset is a benchmark proposed in [34] to test the performance of
face detection and head pose estimation methods. Similarly to the AFLW
dataset, the AFW is composed of images sampled from social networks. It
contains 205 images with 468 faces. Most of the images contain cluttered
backgrounds with large variations in both face viewpoint and appearance (age,
occlusion, expression, etc.). Each face is labeled with a bounding box, 13
discrete viewpoints (range [?90?,90?]) along pitch and yaw directions, and
three discrete viewpoints along the roll direction (left, centre, right). This
dataset differs from similar collections in its annotation of multiple, non-
frontal faces in a single image. Pictures compatible with this dataset are
shown in Fig. 11. Given the low number of images this dataset is generally
used only for testing [15], [34]. In our case we used the AFW dataset to test
the best architectures trained on the AFLW dataset. In this way we have one
more in-the-wild benchmark for comparing our work with other approaches.
#### 4.3.1. Methods
In this experiment we used the whole AFLW dataset (20, 869 faces) to train the
architecture B. The network was trained for 30, 000 epochs. We used the
RMSProp optimiser with the same hyper-parameters as used in the previous
experiment: ?=0.001 and ?=0.9. Each face presented in the images was cropped
and resized to 64 × 64 pixels. We removed smaller images (only five in total).
The trained network was tested on the AFW dataset. To measure the network
performance we used the average of five learning cycles. The CNN was
initialised five times with random weights, trained on the AFLW dataset and
tested on the AFW dataset. The average of the five MAEs has been used as the
final score.
#### 4.3.2. Results
We obtained an MAE and STD of 16.73 ± 17.17 and an accuracy of 32.96% which
reaches 75.29% when considering an error of ± 15°. This is the best score ever
reported on this dataset. The comparison of CNNs with other methods is
reported in Table 4. We did some additional tests augmenting the data in the
AFLW dataset by horizontal flipping. These experiments did not lead to any
major improvement in the final performance.
### 4.4. Discussion
We now detail the results achieved. The first phase of each experiment
consisted of the optimiser selection. The results on the Prima dataset show
the superiority of Adam on the other methods. The use of adaptive methods has
been extremely important also for in-the-wild datasets where they
significantly outperform SGD. In the AFLW dataset the RMSProp had the lowest
reported MAE and the highest accuracy. It must be pointed out that in the last
epochs the SGD with momentum reaches similar loss values of Adam and RMSProp,
however the results in terms of MAE are higher. We hypothesise that the
flexibility of Adam and RMSProp allowed exploring the space better than SGDwith momentum, especially because of the high variability of the input
features. Another advantage of the adaptive methods is the convergence speed.
As it is possible to observe from Fig. 5, Fig. 6 and 12, Adam and RMSProp
converge to a very low loss value during the first epochs.
The second phase of the experiments consisted of the architecture selection.
In the Prima dataset, comparing architecture A and its deeper counterpart (B)
we see that the second architecture had an highest MAE for pitch (+0.92?) and
yaw (+0.66?) estimation. The same effect has been observed between network C
and D (+2.54?,+0.81?). We can conclude that adding another convolutional layer
or adding more parameters did not lead to any improvement. The reasons could
be the small size of the Prima dataset and the controlled environment where
the pictures have been taken. In the AFLW dataset, comparing architecture A
with architecture B we can see that the second architecture had lowest MAEs
for roll, pitch and yaw (?0.15?,?0.18?,?1.38?). Augmenting the number of
parameters did not lead to any improvement. In this case having an additional
convolutional layer made a significant difference, and we can conclude that
estimating the head pose in unconstrained datasets requires more complex
architectures.
In a separate phase we investigated the presence of overfitting during the
training. In the Prima dataset we monitored the RMSE of the best architecture
trained with the Adam optimiser for pitch (Fig. 9) and yaw (Fig. 10). In both
cases the RMSE on the validation set decreased stably without significantly
diverging from the training error. This result seems to indicate that there is
an extremely low overfitting and that the CNN has a good generalisation
ability for unknown subjects. In the AFLW dataset we observed the RMSE of the
best architecture for the yaw prediction (Fig. 15). Also in this case the
validation error decreased stably in accordance with the training curve
meaning that the network could effectively generalise to unseen data.
Comparing the RMSE of the Prima and AFLW datasets it is possible to notice
that in the AFLW the gap between training and validation is reduced thanks to
the large amount of images available.
We stated in the introduction that the use of nonlinear regression methods can
tolerate systematic errors in the training set. Our experimental results
confirm this statement. The accuracy of CNNs is higher than any other method
meaning that the networks can grab the general rules and are not heavily
affected by mislabelling. To visualise the accuracy we reported the confusion
tables as heatmaps for both Prima (Fig. 8) and AFLW (Fig. 14) datasets. The
darker cells are disposed along the main diagonal, meaning that the prediction
has a high accuracy with false positives constrained to within the proximity
of the correct category. The main difference between Prima and AFLW is in the
prediction of values which are close to ± 90°. The reason for this difference
is the AFLW dataset does not have a uniform distribution and values distant
from the mean are very limited.
## 5\. Conclusions
In this article we introduced the use of dropout and adaptive gradient methods
for head pose estimation with CNNs. Our approach is significantly different
from previous research [17], [18], [20], [21], and show how using the most
recent deep learning techniques leads to the state-of-the-art in constrained
and unconstrained datasets. Our method should be considered as part of a
broader system, in particular it can be used in conjunction with a face
detector. We implemented the system in Python using TenworFlow [4] and OpenCV
[42]. We used the Viola-Jones object detection framework [43] as a face
detector and two CNNs of type B trained on the AFLW dataset as head pose
estimator (pitch and yaw). We acquired camera frames with a resolution of 640× 480 from a commercial webcam, and then we isolated the faces using the
Viola-Jones algorithm. Finally we gave as input the isolated faces to the CNNs
obtaining the head pose. The whole system runs at 15 frames per second on a
standard laptop (intel core i5, 8GB RAM) without the use of a GPU. It must be
pointed out that an inadequate face detector can be a significant bottleneck.
In our case removing the face detector from the loop allows executing the head
pose estimation at 20 frames per second. Going to the real-world is not
straightforward and different problems can arise. The major problem we
experienced during our tests was the increasing in pose estimation errors when
the face detector returned a frame which was not well centred on the subjects
face. In this case we found useful to train the networks on an augmented
version of the AFLW dataset, where the centre of the frame was randomly
shifted in one of four directions.
Although our approach is highly competitive, it can be further improved. Other
factors can play an important role as pointed out by recent literature. In our
opinion future research should particularly focus on the impact of weight
initialization [44] and sample selection [45]. Moreover the results can be
further improved once in-the-wild datasets with more images and an extended
range of poses are available.
## Acknowledgment
This material is based upon work supported by the Air Force Office of
Scientific Research grant number: A9550-15-1-0025, Air Force Materiel Command,
USAF under Award No. FA9550-15-1-0025.
We gratefully acknowledge the support of NVIDIA Corporation with the donation
of the Tesla K40 GPU used for this research.
Recommended articles"
148,150,Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition,"['BK Kim', 'H Lee', 'J Roh', 'SY Lee']",2015,167,Static Facial Expression in the Wild,facial expression recognition,We present a pattern recognition framework to improve  its application to static facial  expression recognition in the wild ( released for the 3rd Emotion Recognition in the Wild (EmotiW),No DOI,Proceedings of the 2015 ACM on …,https://dl.acm.org/doi/10.1145/2818346.2830590,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
149,151,Hierarchical committee of deep convolutional neural networks for robust facial expression recognition,"['BK Kim', 'J Roh', 'SY Dong', 'SY Lee']",2016,307,Static Facial Expression in the Wild,neural network,"-video emotion recognition (acted facial expression recognition in the wild, AFEW), whereas   : AFEW and image-based static facial expression recognition in the wild (SFEW). We",No DOI,Journal on Multimodal User Interfaces,https://link.springer.com/article/10.1007/s12193-015-0209-0,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
150,152,How we've taught algorithms to see identity: Constructing race and gender in image databases for facial analysis,"['MK Scheuerman', 'K Wade', 'C Lustig']",2020,211,Radboud Faces Database,"classifier, machine learning","to current database annotation approaches in machine learning  of machine learning model  building: the data it is limited to  For example, Radboud Faces Database (RAFD) contained",No DOI,Proceedings of the ACM …,https://dl.acm.org/doi/10.1145/3392866,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
151,153,Human-computer interaction using emotion recognition from facial expression,"['F Abdat', 'C Maaoui', 'A Pruski']",2011,137,Affective Faces Database,facial expression recognition,"Our facial expression recognition system is presented in section 3,  to emotion recognition  based on facial expression analysis.To detect the face in the image, we have used the face",No DOI,2011 UKSim 5th European …,https://ieeexplore.ieee.org/document/6131215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
152,154,Hybrid deep neural networks for face emotion recognition,"['N Jain', 'S Kumar', 'A Kumar', 'P Shamsolmoali']",2018,312,Toronto Face Database,"deep learning, neural network","This paper proposed a deep learning technique in the  Now a day's due to quantity and  variety of datasets, deep learning  have just utilized the extra dataset for the training. Accordingly,",No DOI,Pattern Recognition …,https://www.sciencedirect.com/science/article/pii/S0167865518301302,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Hybrid deep neural networks for face emotion recognition
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Proposed model
8. 4\. Experiment and evaluation
9. 5\. Conclusion
10. References
Show full outline
## Cited by (221)
## Figures (6)
1. 2. 3. 4. 5. 6.
## Tables (4)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
## Pattern Recognition Letters
Volume 115, 1 November 2018, Pages 101-106
# Hybrid deep neural networks for face emotion recognition
Author links open overlay panelNeha Jain a, Shishir Kumar a, Amit Kumar a,
Pourya Shamsolmoali b c, Masoumeh Zareapoor b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patrec.2018.04.010Get rights and content
## Highlights
* ?
A novel Hybrid CNN-RNN model for facial emotion recognition.
* ?
The model deeply extracts the relations between facial images.
* ?
The model evaluated based on several hyperparameters.
* ?
Applicability of the model in real-time applications.
## AbstractDeep Neural Networks (DNNs) outperform traditional models in numerous optical
recognition missions containing Facial Expression Recognition (FER) which is
an imperative process in next-generation Human-Machine Interaction (HMI) for
clinical practice and behavioral description. Existing FER methods do not have
high accuracy and are not sufficient practical in real-time applications. This
work proposes a Hybrid Convolution-Recurrent Neural Network method for FER in
Images. The proposed network architecture consists of Convolution layers
followed by Recurrent Neural Network (RNN) which the combined model extracts
the relations within facial images and by using the recurrent network the
temporal dependencies which exist in the images can be considered during the
classification. The proposed hybrid model is evaluated based on two public
datasets and Promising experimental results have been obtained as compared to
the state-of-the-art methods.
* Previous article in issue
* Next article in issue
## Keywords
Emotion recognition
Deep learning
Recurrent neural networks
Convolutional Neural Networks
Hybrid CNN-RNN
## 1\. Introduction
Facial and emotional expressions are the most significant nonverbal ways for
expressing internal emotions and intentions.?Facial Action Coding system
(FACS) is a useful structure that classifies the human facial actions by their
advent on the face using Action Units (AU). An AU is one of 46 minor elements
of visible facial motion or its relatedform changes.Facial expressions have
worldwide meaning, and these emotions have been accepted for tens and even
hundreds of years and it was the main reason for us to select facial
expressions for the research?. These days, interest in emotion recognition
(ER) has skyrocketed, while it stayed as single main difficulties in the area
of human-computer interaction. The cornerstone of the most relevant research
is to build a reliable conversation and communication among human and computer
(machine). The importance of ER methods can be achieved by either ?make humans
to understand computer/machine accurately and conversely?. Facial Expression
Recognition (FER) is a challenging task in machine learning with a wide-
ranging of applications in healthcare, human-computer interaction, and gaming.
Emotion recognition is challenging due to several input modalities, have a
significant role in understanding it. ?The mission of recognizing of the
emotions is mostly difficult dueto two main reasons: 1) There is not largely
available database of training images and 2) classifying emotion could not be
simplebased on whether the input image is static or aevolution frame into a
facial expression. The finaldifficulty is mostly for the real-time detection
while facial expressions differenthusiastically?. Ekman et al. [1] counted six
expressions (surprise, fear, happiness, anger, disgust, and sadness) as main
emotional expressions that are common among human beings. Mostly the big
overlap between the emotion classes makes the classification task very
difficult. This paper proposed a deep learning technique in the context of
emotional recognition, in order to classify emotion labels from the images.
Too many methods and research has been developed in this regards, however,
most current works are appeared focusing on hand-engineered features [2], [3].
Now a day's due to quantity and variety of datasets, deep learning is becoming
as mainstream techniques in all computer visions tasks [4], [5]. Conventional
convolutional neural systems have a noteworthy constraint that they simplyhandle spatial image. The essential commitment of this work is to display the
spatio-worldly development of outward appearances of a man in the Images
utilizing a ?Recurrent Neural Network (RNN) which embedded with a
Convolutional Neural Network (CNN) in a form of CNN?RNN design?. We
additionally introduce a neural system based element level combination
procedure to join diverse modalities for the last emotion forecast. The
pioneering works in emotion recognition based deep learning [6], [7] has
achieved the state-of-the-art. The cornerstone of these proposed models [6],
[8] is an average-based aggregation for visual features. A little distinguish
from current works, we proposed an RNN to classify the facial emotion. The
proposed model explores feature level fusion strategy and proves the moderate
improvement by this model. The other parts of the paper are organized as: next
section delivers the related work in what we follow. Section 3 presents the
proposed network. The results and experiments are included in Section 4. At
the end, we have concluded our observation in Section 5.
## 2\. Related work
?Generally, research works in this area have been focused on identifying human
emotion in the base of video footage or based on audiovisual records (mixing
speech recognition and video techniques). Several papers pursue to identify
and match faces [20], nevertheless most works did not use deep learning to
extract emotions from images?. Customarily, calculations for mechanized
outward appearance acknowledgment comprises of three primary modules, viz.
enlistment, highlight extraction, and arrangement. Point by point study of
various approaches in every one of these means can be found in [9]. ?Customary
calculations for full of feeling registering from faces utilize designed
highlights for example, Histogram of Oriented Gradients [11], Local Binary
Patterns [10] and facial historic points [12]?. Since the greater parts of
these highlights are hand-created for their particular use of acknowledgment,
so the generalization in the particular situation is necessary, such as, high
variability in lighting, subjects ethnicity, visual determination, and so on.
Interestingly, the powerful methodologies for accomplishing a great
acknowledgment for series of marking errand are alluded to separate the
transient relations of edges in an arrangement. Separating these transient
relations have been examined utilizing customary techniques before. Cases of
these endeavors are Concealed Markov Models [13], [14], [47], [48] ?(which
join the data and then apply division on recordings), Spatio Temporal Shrouded
Markov Models by combing S-HMM and T-HMM [15], Dynamic Bayesian Networks? [16]
is related to multi-tactile data combination paradigm, Bayesian transient
models to catch the dynamic outward appearance progress, and Conditional
Irregular Fields (CRFs) [17], [18] and their augmentations. Recently,
""Convolutional Neural Networks"" (CNN) has turned into the most mainstream
approach in the deep learning techniques. AlexNet [19] depends on the
conventional layered engineering which comprises of a few convolution layers,
max-pooling layers and Rectified Linear Units (ReLUs). Szegedy et al. [20]
presented GoogLeNet which is made out of numerous ""Beginning? layers.
Commencement applies a few convolutions on the include outline distinctive
scales. Mollahosseini et al. [21] have utilized the Inception layer for the
undertaking of outward appearance acknowledgment and accomplished best in
class comes about. Following the accomplishment of Inception layers, a few
varieties of them have been proposed [22]. ?RNNs recently have greatly
succeeded in handling sequential data such as speech recognition [23], natural
language processing [24], [25], action recognition [26], and so on. Then RNN
is additionally has been improved to treat the images [27] by scanning the
parts of images into sequences in certain directions. Due to the capability ofrecollecting information about the past inputs, RNN has the ability to learn
relative dependencies with images, which is advantageous in comparison with
CNN. The reason is CNN may fail to learn the overall dependencies because of
the locality of convolution and pooling layers. Therefore, RNN is generally
combined with CNN in order to achieve better achievement in image processing
tasks such as image recognition [28] and segmentation [29]?. Conventional
Recurrent Neural Networks (RNNs) can learn fleeting progression by mapping
input successions to a grouping of concealed states, and furthermore mapping
the covered up states to yields. Zhang et al. [30] ?proposed a novel deep
learning framework called as a spatial-temporal recurrent neural network
(STRNN) to unify the learning of two different signal sources into a spatial-
temporal dependency model?. Khorrami et al. in [31], [45], [46], developed a
method which used the CNN and RNN in order to perform emotion recognition on
video data. Chernykh et al. [32] and Fan et al. [33] proposed CNN + RNN models
for the video and speech recognition. In spite of the fact that RNNs have
demonstrated promising execution on different assignments, it is difficult for
them to learn long haul successions. This is mostly the result of
vanishing/detonating slopes issue [34] that can be understood by having a
memory for recalling and overlooking the past states. ?Xie and Hu [42]
presented a new CNNmodel that used convolutional modules. tominimize
redundancy of same features learned, considers communal information among
filters of the same layer, and offers the top set of features for the next
layer.A distinguishedapplication of a CNN to real-time detection of emotions
from facial expressions is by Oullet [43]. Theymade a game, while a CNN was
applied to a video stream to grab the subject's facial expressions, performing
as a controller for the game. This work established the possibility of
executing a CNN in real-time by means of a running-average of the perceived
emotions from the input stream, decreasing the special effects of variation
and noise. A latest development by Levi et al. [44] illustrated important
upgrading in facial emotion recognition using a CNN. They listed two main
drawbacks: 1) the small amount of available data for training deep CNNs and 2)
appearance dissimilarity generally affected by dissimilarities in
illumination?.
Distinct from other work including video and RNN strategies, [35], in this
paper we don't utilize LSTMs. However, we utilize IRNNs [36] that is made out
of amended straight units (ReLUs) what's more; utilize a unique introduction
system in view of scaled varieties of the character grid. These components of
IRNNs are gone for giving a substantially less difficult system to managing
with the vanishing and detonating inclination issue thought about to the more
perplexing LSTM system. Late work has contrasted IRNNs and LSTMs and found
that IRNNs can yield equivalent outcomes in a few errands, including issues
which include long haul conditions? [36]. We give point by point details of
the CNN and the RNN structure the in next Section. Moreover, we concatenated
the CNN highlights to a permanent distance feature vectors and furthermore,
trained on SVM.
## 3\. Proposed model
The opposition dataset has only a single emotion label for each picture and do
not have relation to each casing. This presents a great deal of commotion if
the picture labels are utilized as focuses on preparing a CNN on the singular
image. Our visual highlights are in this way given by a CNN prepared on a mix
of two extra emotion datasets of static pictures. In addition, utilizing extra
information covers a bigger assortment of age and character rather than the
test information where a similar performing artist/on-screen character might
show up in numerous clips. For the CNN training we used two large emotiondatasets, MMI Facial Expression Database (TFD) [37] it consists more than 2900
images of 75 subjects and ?Japanese Female Facial Expression (JAFFE) Database
[38] containing 213 pictures, which have seven basic expressions: angry, sad,
surprise, happy, disgust, fear, and neutral?.
For the preprocessing, we represent fluctuating lighting conditions
(specifically, crosswise over datasets) we connected histogram evening out. We
utilized the adjusted appearances gave by the coordinators to remove
highlights from the CNN. ?The arrangement includes a joined facial key point's
location and following methodology clarified in [39]. Extraordinary confront
location, as well as arrangement procedures, have been utilized for MMI Facial
Expression and the JAFFE Datasets?. Keeping in mind the end goal to be ready
to use the extra datasets, we re-adjusted all datasets to JAFFE utilizing the
accompanying method:
* 1.
We distinguished five facial key focuses for all pictures in the JAFFE and MMI
preparing set utilizing the convolutional neural system course strategy in
[40].
* 2.
for every dataset, the mean shape have been processed by averaging the
directions of main focuses.
* 3.
The datasets have been mapped by utilizing a closeness change among the mean
shapes. By processing one change for each dataset the nose, eyes, and mouth is
generally in a similar area holding a slight measure of variety. We included
an uproarious fringe for MMI and JAFFE-faces as appearances were edited all
the more firmly contrasted with JAFFE.
* 4.
JAFFE-faces approval test sets were mapped to utilize the change construed on
the preparation set.
Additionally, dataset standardization has been performed by using the standard
deviation and mean picture from the consolidated JAFFE and MMI (JAFFE + MMI).
Fig. 1 represents the samples face emotion data. For the implementation and
the evaluation of the proposed model the 70% of each dataset used for training
and the rest 30% for testing.
1. Download: Download high-res image (252KB)
2. Download: Download full-size image
Fig. 1. Sample of JAFFE dataset for five type of emotion (Ang, Sad, Fea, Hap,
Sur).
### 3.1. Convolution neural network architecture
Emotion Recognition data comes in various sizes and resolutions, so we try to
propose a model which can handle any type of input. In our approach, ?we
considered a class of networks with 6 convolutional layers plus 2 fully
connected layers?, each with a ReLu activation function, and dropout for
training.
Plus 2 fully connected layers?, each with a _ReLu_ activation function, and
dropout for training. Furthermore, we performed regularization for each weight
matrix _W_ that limits the size of the weights at individual layer by adding a
term to the loss equal to some fixed hyperparameter. We explain these in Eq.
(1), where _x_ be the output of a particular neuron in the network and _p_ the
dropout
possibility.(1)ReLu(x)=max(0,x)Dropout(x,p)={x,withprob.px,withprob.1?pReg(w)=??w?22
Combinations of two deep learning initializer algorithms have been used to
perform parameter updates based on the gradient of the loss function called as
Momentum and Adam [41], [42]. Eq. (2) describes this update, where _X t_ isparameter matrix at iteration t. vt is the velocity vector at iteration t, and
? is the rate of learning.(2)vt=?vt?1?a?Xt?1Xt=Xt?1+vt
Eq. (3) illustrates the Adam update and its combination with the momentum
update.(3)?mt=?1mt?1+(1??1)?Xt?1vt=?2vt?1+(1??2)?(Xt?1)2Xt=Xt?1?amtvt+?
_?1,?2 ? [0,1]_ and ? are hyperparameters, _m t_ is the momentum vector with
t iteration, _v t_ is the velocity vector, and the learning rate of _?_. Adam
is the actual update algorithm due to information usage for the primary and
the secondary moments of the gradient.
The CNN is used primarily for feature extraction and we have just utilized the
extra dataset for the training. Accordingly, we hunt down a model that have
better communalize to different datasets. Profound models are known to learn
portrayals to have better communalize to different datasets. By the way, it
has been found out that the deep structure rapidly over-fitted, and
communalize severely to the test dataset. This could be because of the
generally little measure of marked information accessible for the emotion
detection tasks. Consequently, ?we build different connections between 6
layers which seem to have decent tended to the over-fitting issues. At the
end, we expanded the filter size from 3 to 5 and the numbers of channels are
8-16-32-64-128-256. For the experimentations data augmentation has been used?
((horizontal, vertical and rotation flipping with 0.25 probability), and
dropout is used (with the rate of 0.5).
RNNs are a form of neural network that converts the order of inputs into a
series of outputs. In separately time step _t_ , an unknown parameter _h t_ is
calculated according to the unknown parameter at time _t_ _? 1_ and the input
_x t_ at time _t_(4)ht=?(Winxt+Wrecht?1)
While ? _W in_ is the weight of input matrix, _W rec_ is the matrix of
recurrent and _?_ is the hidden activation function. Respectively time step
similarly calculates the outputs, relying on the existing hidden
state?:(5)yt=f(Woutht)
While _W out_ is the result weighted parameters and _f_ is the activation
function of the output. An instance of an RNN in which merely the last phase
creates the output which illustrated in Fig. 3.
?An RNN model has been used, that previously discussed by using rectified
linear units (ReLUs) and recurrent matrix, which is adjusted with scaled
deviations of the distinctiveness matrix? [42]. The distinctiveness
initialization model confirms good gradient movement at the commencement of
training and it consents to train it on moderately extensive orders. The RNN
ha been trained to categorize the images by inserting the extracted features
of each image from the CNN serially network and finally using the Softmax for
the prediction. In the implementation the Gradient clipping rated to 1.0 and a
batch size set to 32. We tested the model by using several layers of the CNN
as input features and picked the output of every third convolutional layer
right after max pooling, as this achieved the highest result on validation
data.
### 3.2. Regression CNN
Firstly we used a single CNN model to train the datasets. At each time trained
a single image, the corresponding image passed through the CNN model, the
details of the model shown in Fig. 2.
1. Download: Download high-res image (132KB)
2. Download: Download full-size image
Fig. 2. CNN Architecture, the network contains six convolutional layers
containing 8, 16, 32, 64, 128, and 256 filters; each of size 5 × 5 and 3 × 3
followed by ReLU activation functions. 3 × 3 max-pooling layers added just
after every first five convolutional layers and average pooling at the lastconvolution layer. Every convolutional layer has two fully-connected layers
and 200 hidden units.
Two fully-connected layers with 200 hidden units for the approximation of the
valence label have been used. For the cost function the mean squared error has
been used. For the network training stochastic gradient descent while the
batch size sets to 32 and the weight decay sets to 1E-4. Moreover, the
learning rate at the beginning sets to 5e-3 which decrees by 0.01 every 20
epochs.
### 3.3. Combining with recurrent neural networks (RNNs)
In the proposed model like the model which presented by [31], we propose to
combine the sequential information by using RNN to spread information. The CNN
model used for feature extraction to fix all of its parameters and to
eliminate the regression layer. For the processing, when the image passed to
the network, 200-dimensional vectors will be extracted from the fully-
connected layers. For the assumed time t, we take P frames from the past (i.e.
[_t_ ? P, _t_]). Then passes every frame from time _t_ ? P to t to the CNN and
extract P vectors fully for each image. Each and every vector goes through a
node of the RNN model. Then every node of the RNN returns some results of
valence label. The overall proposed method illustrated in Fig. 3. The mean
squared error has been used for the cost function while optimizing.
1. Download: Download high-res image (97KB)
2. Download: Download full-size image
Fig. 3. Hybrid CNN-RNN Network Architecture.
## 4\. Experiment and evaluation
For the data preprocessing, we initially identify the face in every outline
utilizing face and point of interest finder. Then map the distinguished
landmark points to characterized pixel areas in a request to guarantee
correspondence concerning outlines. After the normalization the nose, mouth
and nose organizes, while processing each face image through the CCN mean
subtraction and contrast normalization applied. We tested the proposed models
on a normal PC with Intel(R) Core(TM) i7-8700 K and 24 GB of RAM.
### 4.1. Compare the CNN with hybrid CNN-RNN
Fig. 4 shows the loss and the prediction accuracy of the Hybrid CNN-RNN model
for training vas validation for one set of the Images. These charts clearly
illustrate the smooth performance of the proposed model.
1. Download: Download high-res image (185KB)
2. Download: Download full-size image
Fig. 4. Loss and the Prediction Accuracy for Hybrid CNN-RNN model.
Table 1 presents the prediction accuracy of the proposed single frame
regression CNN and Hybrid CNN-RNN technique implemented for predicting valence
scores of subjects to developing a set of the dataset. Finally, when combining
the information and using the Hybrid CNN-RNN model with the ReLU, a
significant performance could be achived.
Table 1. Overall accuracy and mean accuracy for the different models.
Method| Overall accuracy| Mean class accuracy
---|---|---
CNN| 76.51%| 74.33%
CNN - RNN| 91.20%| 89.13%
CNN - RNN + ReLU| **94.46%**| **93.67%**
Fig. 5 displays the Roc curve and the Precision-Recall curve of the proposed
hybrid model. As it is visible the proposed model has the ability to with the
least number of errors used for the face emotion detection.
1. Download: Download high-res image (109KB)
2. Download: Download full-size imageFig. 5. Roc and the Precision-Recall Curve.
We evaluated the special effects of two hyperparameters in the results of
Hybrid proposed model, namely the number of hidden units and the number of
hidden layers. Table 2 concluded that, the best result can achieve with 150
hidden units and in the other cases rather than improvement in the performance
resulted in decreases. Table 3 ?shows that increasing the number of hidden
layers resulted to improve the overall performance of the proposed model.
Hence, based on the experiments, the best results obtained by the 6 hidden
layers?.
Table 2. Result of altering the number of hidden units.
Method| Prediction accuracy| Loss
---|---|---
Hybrid CNN-RNN, hidden units = 50| 92.32%| 4.73%
Hybrid CNN-RNN, hidden units = 100| 93.57%| 4.72%
Hybrid CNN-RNN, hidden units = 150| **94.21%**| **4.43%**
Hybrid CNN-RNN, hidden units = 200| 92.53%| 4.68%
Table 3. Result of altering the number of hidden layers.
Method| Prediction accuracy| Loss
---|---|---
HybridCNN-RNN, hidden layers = 4| 94.57%| 4.28%
HybridCNN-RNN, hidden layers = 5| 94.73%| 4.22%
Hybrid CNN-RNN, hidden layers = 6| **94.91%**| **3.98%**
The confusion matrices of CNN and Hybrid CNN-RNN models on the testing sets
are presented in Fig. 6. Hybrid CNN-RNN model could achieve an accuracy of
94.72%, while a single CNN can reach only to 71.42%. The combined model not
only increases the overall accuracy of the proposed CNN model but also it
reduces the false detection of the model. As it is clearly visible in the Fig.
6 the best detection are for the Ang, Neu, and Sur emotions.
1. Download: Download high-res image (371KB)
2. Download: Download full-size image
Fig. 6. Confusion matrices on JAFFE Datasets.
Table 4 indications the performance of proposed Hybrid CNN-RNN model in
comparison with other approaches evaluated on the JAFFE and MMI datasets. The
proposed CNN-RNN model achieved equal or greater performance as compared to
the four other state-of-the-art methods [30], [31], [32], [33].
Table 4. Proposed model versus other models performance comparison on JAFFE
and MMI dataset.
Method| Accuracy of JAFFE| Accuracy of MMI
---|---|---
Zhang et al. [30]| **94.89%**| 91.83%
Khorrami et al. [31]| 82.43%| 81.48%
Chernykh et al. [32]| 73%| 70.12%
Fan et al. [33]| 79.16%| 77.83%
Proposed model| **94.91%**| **92.07%**
### 4.2. Comparison of the proposed model with other approaches
Our model has slightly better performance than the model which proposed by
Zhang et al. [30], While, the other models have the lower performance in
comparison with the proposed model.
## 5\. Conclusion
In this paper, a model has been proposed for face emotion recognition. We
proposed a hybrid deep CNN and RNN model. In addition, the proposed model
evaluated under different circumstances and hyper parameters to properly
tuning the proposed model. Particularly, it has been found that the
combination of the two types of neural networks (CNN-RNN) cloud significantlyimprove the overall result of detection, which verified the efficiency of the
proposed model.
Special issue articlesRecommended articles"
153,155,Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks,"['H Yang', 'Z Zhang', 'L Yin']",2018,132,"Binghamton University 3D Facial Expression, Oulu-CASIA","FER, classification","(FER-Net) is finetuned for expression classification. After the corresponding prototypic facial  expressions are regenerated from each facial image, we output the last FC layer of FER-Net",No DOI,… Conference on Automatic Face & …,http://ieeexplore.ieee.org/document/8373843/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
154,156,Identity-aware convolutional neural network for facial expression recognition,"['Z Meng', 'P Liu', 'J Cai', 'S Han']",2017,368,"MMI Facial Expression, Static Facial Expression in the Wild","CNN, neural network","of a neuron with probability of 0.6. In Eq. 8, λ2 is set to 5 for CK+/SFEW and 2 for MMI, while   Labeled subject IDs are provided in the CK+ and MMI databases and we manually labeled",No DOI,… on Automatic Face & …,https://ieeexplore.ieee.org/document/7961791,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
155,157,"Illusionary order: Online databases, optical character recognition, and Canadian history, 1997–2010",['I Milligan'],2013,127,Toronto Face Database,classification,", and the Toronto Star online. Yet the issues of poor ocr in these databases make this a very   , the decision was made to adhere to ProQuest's classification system (which is based upon",No DOI,Canadian Historical Review,https://www.utpjournals.press/doi/abs/10.3138/chr.694,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
156,158,Image based facial micro-expression recognition using deep learning on small datasets,"['MA Takalkar', 'M Xu']",2017,103,Affective Faces Database,deep learning,"It is not straightforward to recognise the genuine emotion shown on one’s face. Thus recognising   II database, we implemented the DLib face detector in OpenCV to detect and crop the",No DOI,2017 international conference on digital …,https://ieeexplore.ieee.org/document/8227443,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
157,159,Image based static facial expression recognition with multiple deep network learning,"['Z Yu', 'C Zhang']",2015,736,"Acted Facial Expressions In The Wild, CMU Multi-PIE, Static Facial Expression in the Wild, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",Two large datasets: the Toronto Face Dataset and the Google dataset were combined to  train the CNN network. The Google dataset happens to be the very dataset provided to FER-,No DOI,Proceedings of the 2015 ACM on international …,https://dl.acm.org/doi/10.1145/2818346.2830595,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
158,160,Image ratio features for facial expression recognition application,"['M Song', 'D Tao', 'Z Liu', 'X Li']",2009,127,"Japanese Female Facial Expression, Toronto Face Database","classification, classifier, facial expression recognition","database, and the Japanese Female Facial Expression database.  asymmetric facial  expressions based on our own facial expression  of our combined expression recognition system.",No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/19884092/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
159,161,Impact of deep learning approaches on facial expression recognition in healthcare industries,"['C Bisogni', 'A Castiglione', 'S Hossain']",2022,109,Static Facial Expression in the Wild,"deep learning, machine learning","For experimental purposes, three benchmark databases, static facial expressions in the  wild, Cohn-Kanade, and Karolinska directed emotional faces, are employed with some existing",No DOI,IEEE Transactions …,https://ieeexplore.ieee.org/document/9674818,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
160,162,"Induced disgust, happiness and surprise: an addition to the mmi facial expression database","['M Valstar', 'M Pantic']",2010,695,MMI Facial Expression,"deep learning, facial expression recognition, machine learning",notation is valuable for researchers who wish to build automatic basic emotion detection  systems. Not all affective states can be categorised into one of the six basic emotions.,No DOI,Proc. 3rd Intern. Workshop on EMOTION  …,https://www.researchgate.net/publication/284473673_Induced_disgust_happiness_and_surprise_An_addition_to_the_mmi_facial_expression_database,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
161,163,"Integrating faces, fingerprints, and soft biometric traits for user recognition","['AK Jain', 'K Nandakumar', 'X Lu', 'U Park']",2004,199,Toronto Face Database,classification,"an algorithm for age classification from facial images based on cranio-facial changes in  feature- However, they do not provide any accuracy estimates for their classification scheme.",No DOI,Biometric Authentication: ECCV …,https://link.springer.com/chapter/10.1007/978-3-540-25976-3_24,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
162,164,Intra-class variation reduction using training expression images for sparse representation based facial expression recognition,"['SH Lee', 'KN Plataniotis', 'YM Ro']",2014,141,"CMU Multi-PIE, Toronto Face Database","FER, classification, classifier, facial expression recognition",", “A 3D facial expression database for facial behavior research,”  Toronto, Toronto, ON,  Canada. His research interests include face detection/tracking, facial expression recognition, face",No DOI,IEEE Transactions on …,https://ieeexplore.ieee.org/document/6874505,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
163,165,Joint fine-tuning in deep neural networks for facial expression recognition,"['H Jung', 'S Lee', 'J Yim', 'S Park', 'J Kim']",2015,933,"MMI Facial Expression, Oulu-CASIA","deep learning, neural network",We designed our network with a moderate depth and a moderate number of parameters   facial expression recognition database is too small— there are only 205 sequences in the MMI,No DOI,Proceedings of the IEEE …,https://ieeexplore.ieee.org/document/7410698,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
164,166,Karolinska directed emotional faces,"['D Lundqvist', 'A Flykt', 'A Öhman']",1998,3821,Karolinska Directed Emotional Faces,facial expression recognition,"All the participants were instructed to try to evoke the emotion that was to be expressed  and to make the expression strong and clear. In a validation study (Goeleven et al., 2008), a",No DOI,Cognition and Emotion,https://kdef.se/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
165,167,Learning affective features with a hybrid deep model for audio–visual emotion recognition,"['S Zhang', 'S Zhang', 'T Huang', 'W Gao']",2017,346,Affective Faces Database,CNN,"tasks to initialize our CNN and 3D-CNN, respectively. Then,  For each frame in the video  segment, we run face detection,  audio-visual face database of affective and mental states,” IEEE",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7956190,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
166,168,Learning affective video features for facial expression recognition via hybrid deep learning,"['S Zhang', 'X Pan', 'Y Cui', 'X Zhao', 'L Liu']",2019,147,MMI Facial Expression,"CNN, deep learning, machine learning","This deep fusion network is used to jointly learn  Vector Machine (SVM) is employed for  facial expression classification  -based facial expression datasets, ie, BAUM-1s, RML and MMI,",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/8658192/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
167,169,Learning deep global multi-scale and local attention features for facial expression recognition in the wild,"['Z Zhao', 'Q Liu', 'S Wang']",2021,223,"Expression in-the-Wild, Static Facial Expression in the Wild","FER, deep learning, facial expression recognition","This paper aims at static FER in the wild, in which occlusion and pose are two key issues,   Fan, JC Lam, and VO Li, “Video-based emotion recognition using deeply-supervised neural",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/9474949,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
168,170,Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition,"['M Liu', 'S Shan', 'R Wang', 'X Chen']",2014,455,MMI Facial Expression,facial expression recognition,"We demonstrate the proposed method for expression recognition on four databases: CK+,  MMI, Oulu-CASIA VIS, and AFEW. As shown in Table 1,2,3,4, we separate the results into",No DOI,… vision and pattern recognition,https://openaccess.thecvf.com/content_cvpr_2014/papers/Liu_Learning_Expressionlets_on_2014_CVPR_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
169,171,Learning to disentangle factors of variation with manifold interaction,"['S Reed', 'K Sohn', 'Y Zhang']",2014,287,CMU Multi-PIE,"deep learning, machine learning",groups of hidden units that each learn to encode a distinct factor  disentangled features  learned on the CMU Multi-PIE dataset.  in pose estimation and face verification on CMU Multi-PIE.,No DOI,… on machine learning,http://proceedings.mlr.press/v32/reed14.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
170,172,Learning to optimize neural nets,"['K Li', 'J Malik']",2017,150,Toronto Face Database,neural network,"in stochasticity of gradients and the neural net architecture. More specifically, we  neural  net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset",No DOI,arXiv preprint arXiv:1703.00441,https://arxiv.org/abs/1703.00441,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
171,173,Machine learning methods for fully automatic recognition of facial expressions and facial actions,"['MS Bartlett', 'G Littlewort', 'C Lainscsek']",2004,216,Extended Cohn-Kanade,machine learning,"A second version of the system detects 18 action units of the Facial Action Coding  System (FAcs), we conducted cal investigations of machine learning methods applied to this",No DOI,"… on Systems, Man …",https://www.researchgate.net/publication/220755152_Machine_Learning_Methods_for_Fully_Automatic_Recognition_of_Facial_Expressions_and_Facial_Actions,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
172,174,Machine learning models that remember too much,"['C Song', 'T Ristenpart', 'V Shmatikov']",2017,622,Toronto Face Database,machine learning,a machine learning pipeline consists of several steps shown in Figure 1. The pipeline starts  with a set of labeled data  We exploit the fact that modern machine learning models have vast,No DOI,Proceedings of the 2017 ACM …,https://arxiv.org/abs/1709.07886,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
173,175,Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion (JACFEE): Reliability data and cross-national differences,"['M Biehl', 'D Matsumoto', 'P Ekman', 'V Hearn']",1997,734,Japanese Female Facial Expression,"classification, classifier","in both recognition data and intensity ratings. We, however, believe this classification to  be  Is there universal recognition of emotion from facial expression? A review of cross-cultural",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1023/A:1024902500935,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
174,176,Meta-analysis of the first facial expression recognition challenge,"['MF Valstar', 'M Mehu', 'B Jiang', 'M Pantic']",2012,390,Affective Faces Database,facial expression recognition,IEEE conference on Face and Gesture Recognition 2011. It  : AU detection and classification  of facial expression imagery in  analysis of facial expressions consider facial affect (emotion),No DOI,IEEE Transactions on …,https://pubmed.ncbi.nlm.nih.gov/22736651/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
175,177,Morphed emotional faces: emotion detection and misinterpretation in social anxiety,"['K Heuer', 'WG Lange', 'L Isaac', 'M Rinck']",2010,136,Karolinska Directed Emotional Faces,classifier,"This resulted in three different types of emotional faces (anger, happy, disgust), but four  different interpretation categories (anger, happy, disgust, contempt). This way, the probability of",No DOI,Journal of behavior …,https://pubmed.ncbi.nlm.nih.gov/20511123/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
176,178,Multi angle optimal pattern-based deep learning for automatic facial expression recognition,"['DK Jain', 'Z Zhang', 'K Huang']",2020,115,MMI Facial Expression,"deep learning, machine learning",the required label for the facial expressions.The major key  the facial alignment. The  proposed MAOP-DL validates its effectiveness on two standard databases such as CK+ and MMI,No DOI,Pattern Recognition Letters,https://www.sciencedirect.com/science/article/pii/S0167865517302313,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Multi angle optimal pattern-based deep learning for
automatic facial expression recognition
Search 
## Outline
1. Highlights
2. Abstract
3. 4. Keywords
5. MSC
6. 1\. Introduction
7. 2\. Related work
8. 3\. Overall flow of proposed MATP-DESURF-based FER
9. 4\. Foreground extraction
10. 5\. MATP-DESURF-based facial key points
11. 6\. PPCSO-based feature selection
12. 7\. LSTM-CNN classification
13. 8\. Experiments
14. 9\. Conclusion
15. Acknowledgments
16. References
Show full outline
## Cited by (70)
## Figures (15)
1. 2. 3. 4. 5. 6.
Show 9 more figures
## Tables (5)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
## Pattern Recognition Letters
Volume 139, November 2020, Pages 157-165
# Multi angle optimal pattern-based deep learning for automatic facial
expression recognition
Author links open overlay panelDeepak Kumar Jain a b, Zhang Zhang a b, Kaiqi
Huang a b
Show more
Outline
Add to Mendeley
Share
Citehttps://doi.org/10.1016/j.patrec.2017.06.025Get rights and content
## Highlights
* ?
Rectification of problems due to sudden illumination changes and proper
alignment of feature set for an effective FER.
* ?
Isolation of foreground from the background of an image to facilitate the
facial key points extraction.
* ?
Elimination of unwanted noise and smoothen by using AMF filtering technique.
* ?
MATP-based texture pattern and DESURF-LOP-based facial key points to the FER
outperform the traditional methods.
* ?
PPCSO-based feature selection and LSTM-CNN based classification recognize the
expressions accurately.
## Abstract
Facial Expression Recognition (FER) plays the vital role in the Human Computer
Interface (HCI) applications. The illumination and pose variations affect the
FER adversely. The projection of complex 3D actions on the image plane and the
inaccurate alignment are the major issues in the FER process. This paper
presents the novel Multi-Angle Optimal Pattern-based Deep Learning (MAOP-DL)
method to rectify the problem from sudden illumination changes, find the
proper alignment of a feature set by using multi-angle-based optimal
configurations. The proposed method includes the five major processes as
Extended Boundary Background Subtraction (EBBS), Multi-Angle Texture
Pattern+STM, Densely Extracted SURF+Local Occupancy Pattern (LOP), Priority
Particle Cuckoo Search Optimization (PPCSO) and Long Short-Term Memory
-Convolutional Neural Network (LSTM-CNN). Initially, the EBBS algorithm
subtracts the background and isolates the foreground from the images which
overcome the illumination and pose variation. Then, the MATP-STM extracts the
texture patterns and DESURF-LOP extracts the relevant key features of the
facial points. The PPCSO algorithm selects the relevant features from the
MATP-STM feature set to speed up the classification. The employment of LSTM-
CNN predicts the required label for the facial expressions.The major key
findings of the proposed work are clear image analysis, effective handling of
pose/illumination variations and the facial alignment. The proposed MAOP-DL
validates its effectiveness on two standard databases such as CK+ and MMI
regarding various metrics and confirm their assurance of wide applicability in
recent applications.
* Previous article in issue
* Next article in issue
## Keywords
STM
SURF
CNN
LSTM
## MSC
41A10
65D05
65D17
## 1\. Introduction
Rapid growth of Human-Computer Interfaces (HCI) induces the challenges in
Facial Expression Recognition (FER). Even though the frontal view recognitionis well on the basis of the appearance or geometry model, the Multiview Facial
Expression Recognition (MFER) is the challenging issue [8]. Illumination
conditions, age and size variations in the facial image and videos that lead
to the development of Automatic FER in three stages such as face detection and
tracking,feature extraction and emotion classification in [19] and [13].With
numerous development of applications such as automatic video indexing,
surveillance and assisted living, detection and recognition of actions in
natural settings is an important stage. Recently, the two type of datasets
with the same actions in the wild and video clips introduces the new
complexities to the recognition community. The lack of constraints on such
type of data makes the classification as challenging one. Recently, an
Automated Face Recognition (AFR) [5] is an attractive research study due to
the extensive applications of mobile phone authentication to surveillance.
There are two types of environments available for AFR such as controlled
(frontal, neutral expressions and uniform illumination) and uncontrolled
environment (arbitrary poses, non-uniform illumination and occlusions.
AFR in an uncontrolled environment [16] is the challenging task due to its
intense pose variations which are considered as major uncertain blocks in AFR.
Temporal alignment and the semantic aware dynamic representation were the
major issues in FER due to the repeatable property of low-level features.
Manual designed cuboids have the capability to capture the low-level
information instead of high-level information provided the less discriminative
capability [12]. The major requirement like all the images is available prior
to congealing leads to offline tasks with low efficiency [18]. The combination
of expression dynamics with the 3D facial geometry [21] provided the wealth
information that opens up the new researches [37] on the capture of a face in
all motions and detection of cues.
The provision of temporal coherence and the achievement of required matching
accuracy depend on the estimation of similarity between the expressions. The
data driven approaches [11], [32] predict the k-nearest neighbors which are
considered as the candidates for each frame. The proposal of expression
transfer method refined the expression retrieved from the database.The dense
registration of point clouds is the computational task in AFR due to the large
size 3D vertices in the facial mesh. Besides, the existence of landmarks is
the pre-requisite for FER. The partial separation of facial cues induced the
complications in automatic FER applications. Hence, the issues addressed in
the automatic FER are foreground/background separation, facial key point?s
extraction, the dimensionality of features and multi-labeling-based
recognition. The major contributions of proposed MAOP-DL model are listed as
follows:
* ?
The Extended Boundary Background Subtraction (EBBS) isolates the background
and foreground of images that made the system as adaptive to sudden
illumination changes.
* ?
The combination of Multi-Angular Texture Pattern (MATP) and the Spatio-
Temporal Matching (STM) algorithm extracts the different features that provide
the clear image analysis and proper alignment.
* ?
The Densely Extracted SURF model integrated with the Local Occupancy Pattern
(LOP) algorithm govern the facial key points to accurately identify the muscle
variations during the expression.
* ?
Finally, the application of Priority Particle Cuckoo Search Optimization(PPCSO) algorithm on MATP-features reduces the dimensionality and the
utilization of Long Short Term Memory-Convolution Neural Network (CNN)
improves the classification performance effectively.
The rest of the paper is organized as follows. Section 2 presents a
description about the previous research studies relevant to an automated FER
and their issues. Section 3 describes the implementation process of proposed
MATP-DESURF model for an effective FER. Section 4 investigates the performance
of proposed system with the various performance metrics over the existing
methods. Finally, Section 5 presents the conclusion and future enhancement of
proposed research work.
## 2\. Related work
Due to the small, partially visible and the occluded relevant objects in human
interactions, the recognition of mutual contexts between each other is the
difficult task. Yao et al. [31] proposed the mutual context model in which
objects and human pose are jointly modeled in recognition of human-pose
interaction activities. The interference learned from the Action Units (AU)
was static and the temporal changes in facial expressions led to more
discrimination in intensity levels. Rudovic et al. [20] proposed the novel
Conditional Ordinal Random Field (CORF) model to model the context-sensitive
approach for facial action recognition.). Oh et al. [17]proposed Radial Basis
Function RBF-Neural network on the basis of the combination of Principle
Component Analysis (PCA) and Linear Discriminant Analysis (LDA). The improper
facial alignments and the occlusions are the major issues in traditional PCA-
LDA-based dimensionality reduction techniques. Siddiqi et al. [27] introduced
the robust FER system that employed the Step-Wise LDA (SWLDA) to select the
localized features from the expression frames. Happy et al. [6] proposed the
novel facial landmark detection and the salient patch-based FER under the
diverse resolutions of the image. Fast network training with low human
supervision in an optimization process decreases the network generalization
ability.
On the basis of learning, several approaches are evolved in research studies
to deal with the issues in an accurate recognition. Barakova et al. [1]
proposed the interpretation method of emotions that were detected in facial
expressions in the context of events. The genetic search and the optimization
of parameters were regarded as the next step toward the automatic analysis of
facial expressions. Iosifidis et al. [7] proposed the graph-based embedded ELM
algorithm that exploited both the penalty and SL criteria in infinite
dimensional spaces. The dense registration of point clouds is the
computational task in AFR due to the large size 3D vertices in the facial
mesh. Besides, the existence of landmarks is the pre-requisite for facial
expression recognition. Zhao et al. [35] presented the probabilistic framework
on the basis of Bayesian Belief Network (BBN) on the basis of Statistical
Feature Models (SFM) and Gibbs?Boltzmann distribution for geometric and
appearance features. Blur and illumination were the major problems and by
using the blurring operation of given images convex set was formed.
Vageeswaran et al. [28] proposed the blur-robust algorithm to solve the convex
optimization problems. Recovery of High Resolution (HR) images from the Low-
Resolution (LR) sequences was the major objective of superresolution and
achieved by hallucination methods. Ma et al. [15] reviewed the several maps-
based models under pose, illumination and expressions (hallucination
problems). The modeling of mappings was performed by using the redundant and
sparse representation.
The selection of representative features and the uncertain class labels made
the feature selection as critical task. Wang et al. [30] proposed theunsupervised feature selection for the recognition of facial features in
presence of class labels. The utilization of Zernike Moments (ZM) offered the
promising recognition rates with its rotational invariances. Sariyanidi [22]
modified the ZM to obtain the local representation by computing the moments of
each pixel of face image on the basis of the local neighborhood. Sariyanidi et
al. [23] reviewed and analyzed several states of art solutions by splitting up
the pipeline of face recognition process into four stages as registration,
representation, dimensionality reduction and recognition. Senechal et al. [24]
utilized the multi-kernel SVM for each AU with the Local Gabor Binary Pattern
histograms and the Active Appearance Model (AAM) coefficients. The capture of
complex decision boundary is the difficult task in spontaneous emotions
analysis. Shan [25]constructed the strong classifier by using the combination
of weak classifiers with the AdaBoost approach. The features to recognize the
smile expression are the intensity difference between the pixels exist in the
grayscale images.
During the image annotation process, the selection of features prior to
investigation of properties of feature combination do not provide any
significant contribution. Zhang et al. [33] introduced the regularization-
based feature selection algorithm to leverage both the sparsity and clustering
properties of features for annotation phase. Complex variations, gross errors
and preservation of local details of image introduced the difficulties in
shape priors modeling. Zhang et al. [34] proposed the Sparse Shape Composition
model (SSC) to address the aforementioned challenges. The partial separation
of facial cues induced the complications in automatic FER applications and
such problem is regarded as identity-independent ER problem. Kung et al. [10]
proposed the Dual Subspace Non-negative Graph Embedding (DSNGE) for the
representation of expressive images on two subspaces as identity and
expression. Borude et al. [2] discussed the methods available for facial
features tracking under various illuminations, pose variations and lighting
conditions. Guo et al. [4] proposed the dynamic facial expression recognition
which was formulated with the longitudinal group wise registration problem.The
major contributions of the dynamic recognition are diffeomorphic growth model,
built of salient longitudinal facial expression, sparse representation
(spatial and temporal domain) guided the recognition effectively. The survey
of traditional research studies indicated that the presence of illumination
and pose variations, dimensionality of features were the major issues in FER
applications.
## 3\. Overall flow of proposed MATP-DESURF-based FER
An accurate automated recognition of facial expressions with the diverse
illumination/pose variations, high dimensionality of features and the exact
facial key points is the major focus of the research work proposed in this
paper. Fig. 1 shows the overall workflow of proposed MATP-DESURF-based FER.
1. Download: Download high-res image (681KB)
2. Download: Download full-size image
Fig. 1. Overall Flow of Proposed MAOP-DL for FER.
Initially, the RGB image from the real-time database is considered as the
input to the system. The presence of noise introduces the difficulties in
pixel intensity estimation. Hence, the input image is preprocessed with the
Adaptive Median Filtering (AMF) to remove the noise and smoothen the image
that leads to intensity normalization. Then, the EBBS algorithm is applied to
the noise-free image to isolate the background and foreground. The feature
points are necessary to recognize the emotions in HCI applications.
This paper proposes the novel MATP combined with Densely Extracted SURF model,
LOP and STM algorithm to extract the feature key points. The split-up ofextracted patterns into several grids and the extraction of facial key point
features increase the dimensionality. Hence, the PPCSO is used in a proposed
system to select the optimal relevant features. The optimally selected result
is matched with the training feature grid through LSTM-CNN classification to
recognize the expressions. The isolation of background and foreground prior to
the feature extraction and the dimensionality reduction through the
optimization in proposed work provided the properly aligned feature set for
FER.
## 4\. Foreground extraction
The noise removal is the initial stage of proposed work. The presence of
impulsive noise and the distortion in object boundaries induce the
difficulties in image processing. In this paper, the AMF [26] is used to
remove the noise and reduce the distortion (excessive thinning and thickening)
in two levels. The application of size of the window to filter the image
pixels is adaptive in nature. The noise present in the image is removed
through AMF as follows:
Level 1: Check whether the median pixel value of the image (_I med_)is in
between the ranges of minimum and maximum pixels (Imin,Imax)and if it exists,
then the value is not considered as an impulse. Then, the algorithm goes to
level 2 to check the next pixel. Otherwise, the size of the window is
increased and the level 1 is repeated until the median value is not an
impulse.
Level 2: Check the current pixel is in the ranges of minimum and maximum
pixels and if it exists, then the filtered image pixel is unchanged.
Otherwise, the pixel is regarded as corrupted and the filtered image pixel is
assigned the median value for level 2.
An isolation of background from the foreground of noise-free image is the next
stage in proposed work.This paper utilizes the EBBS algorithm to extract the
foreground from the noise-free image. The algorithm comprises three essential
processes such as cell formation, directionality estimation, and the intensity
depth analysis to extract the foreground from the images. To implement this
processes, the window formation is the prior step. Hence, the window necessary
to project the input image is formed as follows:(1)W=I(i?1toi+1,j?1toj+1)Once
the window is formed, the magnitude that represents the multiple cells of
window is computed. The normal distribution of (G) values in each cell and the
weight for window are responsible for the magnitude estimation. In the
proposed EBBS algorithm, the directionality-based ROI with depth analysis
plays the major role in foreground extraction. The mathematical formulation of
directionality (D¯x,y) is defined as follows:(2)D¯x,y=?2?2G(x,y,?2)where, _?_
= Standard deviation and _x,y_ = Size of Window By convoluting the
directionality (D¯x,y) with the window formed (W), the weight corresponds to
the magnitude estimation is updated such as(3)W¯(x,y)=W(x,y)*D¯x,yWhere
_W_(_x, y_)= Initial Window and W¯(x,y)= Size of window. The updated weight
values from the Eq. (3)are used to estimate the magnitude for depth
analysis(Te) as follows:(4)Te(x,y)=?(W(x,y)×W¯(x,y))?W(x,y)2?W2¯The magnitudes
obtained from Eq. (4) are compared with the updated weight values. If the
magnitude is greater than the updated weight value, then the encoded form of
likelihood value (L) for the patterns necessary to isolate foreground and
background is predicted. The Algorithm 1 to extract the foreground output as
follows
1. Download: Download high-res image (143KB)
2. Download: Download full-size image
Algorithm 1. EBBS algorithm.
The encoded values (E) are multiplied with the average value of magnitude ofthe intensity depth to extract the patterns. If the value of the patterns are
greater than zero, then the foreground of the image. The values are replaced
with zero otherwise.
The cells that represent the noise-free image are formed and the binary
difference of pixel values is sequentially estimated as shown in Fig. 2.
1. Download: Download high-res image (417KB)
2. Download: Download full-size image
Fig. 2. Cell formation.
Figs. 3 and 4 clearly depicts the foreground extraction based on the
directionality values. From Fig. 3, the connected components from the
magnitude estimation are predicted. The layers from the magnitude values are
separated for foreground and background. By performing the deep intensity
analysis through the encoding process, the background of the image is
subtracted and foreground necessary for FER is extracted.
1. Download: Download high-res image (190KB)
2. Download: Download full-size image
Fig. 3. Directionality-based ROI extraction.
1. Download: Download high-res image (110KB)
2. Download: Download full-size image
Fig. 4. Foreground extraction.
## 5\. MATP-DESURF-based facial key points
The muscle variations of facial key points (left eye, right eye, nose and
mouth) play the major role in FER analysis. Hence, the prediction of facial
key points is the next stage of proposed work. The output from the EBBS
algorithm is considered as the input to the MATP algorithm.
### 5.1. Multi-Angle texture pattern
In this stage, the window size is extended to 5 × 5 for projection of EBBS
output. Then, the median value of projected image is computed. Initially, the
window over the EBBS image is formed with the size of 5 × 5. Within this
window, the cells with 3 × 3 is extracted separately. By applying the angle-
based difference estimation, the rules required for the vector prediction is
formed.
The algorithm to compute the patterns in multi-angular form is listed as
follows (see Algorithm 2).
1. Download: Download high-res image (112KB)
2. Download: Download full-size image
Algorithm 2. Multi-angular texture pattern.
The magnitude value corresponding to the difference between the window
formation (T1, T2) are mathematically expressed as
follows:(5)mag=(double((T1(3,4)?T2(3,3))2+((T1(3,4)?T2(3,3))2)The comparison
between the center pixel with the neighboring pixels is performed and then the
decimal coding is performed to extract the patterns. The bitwise OR operation
is performed with two types of patterns (Pt2?Pt1) extracted the relevant
patterns and then mesh grid is constructed. The coordination among the values
in the meshgrid is predicted with sparse form.
Finally, the angle-based texture patterns as shown in Fig. 5 that are
necessary to predict the facial key points are predicted. The STM comprises
the four major processes such as index estimation, cluster formation, template
separation and tag representation. The STM is spanned by 3D blocks densely
sampled from the dictionary that cover the local variations of spatial and
temporal space. The application of bank of learned filters extract the low-
level features from each block. The features are directly learned from the
input dataset proved the generalizability. In proposed work, the query
patterns derived from the MATP process is passed to the cluster formation. Theindex corresponding to the dense samples from the dictionary and the input
MATP are estimated and then clustered.
1. Download: Download high-res image (106KB)
2. Download: Download full-size image
Fig. 5. MATP Pattern.
The STM are arranged in four parts as Left eye, right eye, nose and mouth as
shown in Figs. 6 and 7. Using this STM,the patterns relevant to the facial key
points are extracted through the DESURF-LOP process.
1. Download: Download high-res image (300KB)
2. Download: Download full-size image
Fig. 6. STM Block Diagram.
1. Download: Download high-res image (354KB)
2. Download: Download full-size image
Fig. 7. STM arrangement.
### 5.2. DESURF-LOP
The histogram relevant to the facial key point features is the next stage of
proposed work. The unique coordinates in sparse form are extracted into
separate clusters as follows:(6)Cxy={i,Sxy=S(x?1)(y?1)0,elsewhere i = 1 to N.
The pattern of image pixel at and and their distance (_D i_) are used to find
the coefficients of matrix called Hessian matrix (H(S,_?_)) as
follows:(7)H(S,?)=(Lxx(C,?)Lxy(C,?)Lxy(C,?)Lyy(C,?))where,(8)Lxx(C,?)=?i=1NDi(PT(C)xy)*?Then,
the matching points of STM output with the testing input are predicted as
follows:(9)Vs={?dx,?|dx|,?dy,?|dy|}The region corresponds to the matching
points are extracted as follows:(10)Rxy=PT(Vs)Finally, the features relevant
to the histogram of matching points are extracted as
follows:(11)Fi=Bi,wherei=1toNFig. 8 shows the pictorial representation of
working process in DESURF-LOP-based facial key points prediction.With increase
of features, the computations required for classification is more. Hence, the
selection of number of relevant features is the prior stage to classification.
Optimization procedures play the major role in feature selection process.
Among various procedures, the Cuckoo Search (CS) is used to select the
features necessary for classification. But, the enhanced form of CS is used in
this paper to improve the classification performance further.
1. Download: Download high-res image (440KB)
2. Download: Download full-size image
Fig. 8. DESURF-LOP Facial key point feature extraction.
## 6\. PPCSO-based feature selection
The characteristics of suitable fitness function assigning and the objective
function formulation with either maximizing or minimizing objective is
inherited to find the relevant features. The fitness of the solution is
directly proportional to the objective function value of maximization problem.
The fitness function formulation in traditional method are modified with the
help of searching radius and the length of particles
as(12)Ft={Mean(C),(k×l)}Where, _C={F(1), F(2),... ... F(N)}_ denotes the
cuckoo particles(13)k=k+r(N?1)×?Ci _r_ =searching radius
_N_ =Length of the particles(14)l=1?mean(C)kThe feature vectors are modeled
as cuckoos in this CS algorithm and find the coordinates of each cuckoo is
defined by using the fitness function. By using the prediction of best cluster
point and the value (d) defined in (15), the radius of cluster is
updated.(15)d=Min(Ft)±(?×(Max(Ft)?Min(Ft)))where _?_ =1 The index represent
the Cluster Head (CH) is computed by comparing the value of current cluster
with best cluster point. Once the Cluster head is formed, then the new value
of coordinates to be found in order to reach the best cluster point. Mutation
and crossover in Cuckoo search(CS) are responsible for updating the positionsof cuckoos. The mathematical formulation of reproduction and mutation to
select the cluster head and the updated radius are listed as
follows:(16)Xupdate(i)=x(i?1)+((O?1?)*Cos(Ft))(17)Yupdate(n)=y(i?1)+((O?1?)*Cos(Ft))Based
on the comparison between the fitness function with the probability of laying
eggs defined in (18) serves the base for mutation and they described as
follows:(18)C(m)=(1?i?1(M?1)1?)(19)Xmut(i)=x(i?1)+(C(m)×(Max(x)?Min(x)))(20)Ymut(n)=y(i?1)+(C(m)×(Max
(y)?Min(y)))After
the mutation is over, the mean value of fitness function corresponds to the
new coordinates is predicted. Then, check whether the computed values are
greater than zero or not. If they are greater than zero, then changes in
radius is allowed and cuckoos corresponds to feature vectors are moved towards
that point. Otherwise, the changes in radius are prevented. Then, the average
magnitude of the fitness function is estimated which serves as the base value
for selected feature set (SF) and tested feature set (TS). The Algorithm 3 for
PPCSO is listed
1. Download: Download high-res image (218KB)
2. Download: Download full-size image
Algorithm 3. Priority particle cuckoo search optimization.
## 7\. LSTM-CNN classification
The model proposed in this paper isolates the foreground and background prior
to feature extraction. Besides, the relevant feature selection through the
PPCSO reduces the dimensionality of features. Hence, the labelling of facial
expression is the final stage of this work. The features (SF) and tested
features (TS) from PPCSO are passed to the LSTM-CNN as shown in Fig. 9.
Initially, the labels are initialized as ?n? and L=1. The features corresponds
to the F1 are assigned as S. The maximum and the mean of selected features
(SF) are computed and they can be regarded as M and N respectively. The limit
of sub-divided intervals for the classification process lies in the ranges
between (1<1n<N). Then, the rules necessary to perform the classification
process are extracted as follows.(21)R=SF(M?N)*LtThe neighbor link parameter
(_? t_) and the kernel function (K) are necessary for accurate
classification(22)?t=SF?1TS(t)(23)K=R?1?(t)The training feature set with the
neighbor link and the kernel parameter for the mapping process is constructed
by(24)SFi=Ki+?i=R?1?(i)+?iThe probability distribution on feature set for
neighboring features to update the kernel function is computed as
follows:(25)?(t)=1(2?)n21n?i=1Nie[?(Trt?St)?1(Trt?Rj)2?2]
1. Download: Download high-res image (332KB)
2. Download: Download full-size image
Fig. 9. LSTM-CNN classification.
Finally, the Point Kernel Classifier (PKC) checks whether the value of
selected features is compared with the probability distribution for each
column (t) (_S t_ > ?(_t_)) defined in (25). The kernel function formulation
for the class labels are listed as
follows:(26)Vt(TS)=?n=1N?m=1M(?SFp,m?tiTSp,m)If the probability distribution
function is greater than count of selected features _SF t_, then the
corresponding labels (_C_ =_L_(?(_t_)) are assigned to the images to indicate
the expressions (anger, disgust, fear, happiness, sadness, and surprise).
## 8\. Experiments
The proposed MAOP-DL is evaluated on two databases. The experimental details
are shown and the comparative analysis is discussed in this section.
### 8.1. Databases
**CK+ Database** : The Cohn?Kanade database[9] contains the expressions of 100
university students with the age variations of 18 to 30. Among them, the 65%
are female, 15% are African?American and 3% are either Asian or Latino. Thereare six emotions based on the prototype description are the subjects of this
database. To validate the effectiveness of proposed work, image sequences from
96 subjects are selected.Fig. 10 shows some normalized samples with all
expressions.
1. Download: Download high-res image (78KB)
2. Download: Download full-size image
Fig. 10. Example of six basic expressions from the Cohn?Kanade + database
(anger, disgust, fear, happiness, sadness, and surprise).
**MMI Database** : This database is the most challenging database [29]
compared to CK+database. There are 30 students and research staff members with
the age of 19 to 62 are included in this database. Among them, 44% are female
with the background of European, Asian or South American ethnic
background.Typical pictures of persons showing emotions can be seen in Fig.
11. There are 213 image sequences with the labels of six expressions are
included in the database. To validate the proposed system, there are 205
images are selected. Table 1 Presents the abbreviations of Conventional
learning methods used for Comparison.
1. Download: Download high-res image (164KB)
2. Download: Download full-size image
Fig. 11. Example images from the MMI. The emotions from left to right are:
Anger, Disgust, Fear, Happiness, Sadness, Surprise.
Table 1. Abbreviations of Methods.
Method| Description
---|---
ADL| Patches selected by ADaboost are used
AFL| All the patches of whole Face are used
CPL| Common Patches are used
CSPL| Common and Specific Patches are used
### 8.2. Results comparisons
We investigate the performance of Proposed MOAP-DL over the existing single
scale methods regarding the recognition rate and F1-measure as in Table 2 for
CK+ database in this section. In existing methods, the CSPL provides the
better F1 measures of 0.7440, 0.9134, 0.8432, 0.9462, 0.8619 and 0.9870 for
the expressions of anger, disgust, fear, happiness sadness and surprise
respectively. The recognition rate of CSPL [37] is 0.8989 which is also higher
than the existing methods. But, the proper alignment and optimized texture
features in proposed MAOP-DL increased the F1-measure to 0.9482, 0.9558,
0.9487, 0.8592, 0.8816 and 0.9440 respectively. Besides, the recognition rate
also improved to 0.9617. The comparative analysis shows that the proposed
MAOP-DL improves the F1-measure by 20.42(anger), 4.24(Disgust),
10.55(Fear),1.97(sadness) compared to CSPL respectively,however the expression
of Happy and surprise failed to recognise accurately. The recognition rate for
MAOP-DL also 6.28% higher than the CSPL method.
Table 2. Recognition Rate and F-1 measure analysis for CK+ database.
Expression| AFL| ADL| CPL| CSPL| MOAP-DL
---|---|---|---|---|---
Anger| 0.6407| 0.6281| 0.7144| 0.7440| 0.9482
Disgust| 0.8782| 0.8776| 0.8927| 0.9134| 0.9558
Fear| 0.8235| 0.8206| 0.8209| 0.8432| 0.9487
Happiness| 0.9416| 0.9381| 0.9305| 0.9462| 0.8592
Sadness| 0.8204| 0.8346| 0.8515| 0.8619| 0.8816
Surprise| 0.9806| 0.9791| 0.9827| 0.9870| 0.9440
Recognition Rate| 0.8694| 0.8226| 0.8842| 0.8989| 0.9617
Table 3 presents the comparative analysis of recognition rate analysis ofproposed and the existing methods (sparse+A+T, sparse+A and conventional+A)
[4] various expressions. In existing methods, the sparse+A+T provides the
better recognition rate of 95.8, 97.9, 96.5, 98.2, 94.5 and 97% for the
expressions of anger, disgust, fear, happiness sadness and surprise
respectively. But, the proper alignment and optimized texture features in
proposed MAOP-DL increased the recognition rate to 98.6745, 98.8218, 98.9691,
98.6745, 98.5272 and 98.6745% respectively. The comparative analysis shows
that the proposed MAOP-DL improves the recognition rate by 2.91, 0.92, 2.51,
0.48, 4.02 and 1.7% compared to sparse+A+T respectively.
Table 3. Recognition Rate analysis for MMI database.
Expression| Conventional+A| Sparse+A| Sparse+A+T| MOAP-DL
---|---|---|---|---
Anger| 92.2| 93.8| 95.8| 98.67
Disgust| 95.1| 97.1| 97.9| 98.82
Fear| 93.9| 96| 96.5| 98.96
Happiness| 97.5| 97.6| 98.2| 98.67
Sadness| 91.9| 92.3| 94.5| 98.52
Surprise| 94.5| 95.3| 97| 98.67
We compare the accuracy of recognition with the following methods to show the
effectiveness of MAOP-DL: Graph-preserving Sparse NMF (GSNMF) [36], Common and
Specific Patches (CSPL) [38] and Dual Sub-space NGE(DSNGE) [10] in Table 4.
Table 4. Recognition Accuracy Analysis for CK+ database.
Methods| CK+
---|---
GSNMF [36]| 72.17
CSPL [38]| 89.9
DSNGE [10]| 94.82
Ours| 96.17
In existing methods, the DSNGE provides better performance (94.92%) than the
existing methods. The foreground extraction prior to the pattern extraction
and feature selection and the enhanced STM models with the multi-angular
patterns in proposed MAOP-DL further improves the recognition accuracy to
96.17%. The comparison between the MAOP-DL with the DSNGE shows that the MAOP-
DL provided the 1.3% improvement compared to DSNGE models respectively.
In existing methods, the CNN provides better performance (97.81%) than the
existing methods. The foreground extraction prior to the pattern extraction
and feature selection and the enhanced STM models with the multi-angular
patterns in proposed MAOP-DL further improves the recognition accuracy to
98.72% as shown in Table 5. The comparison between the MAOP-DL with the CNN
shows that the MAOP-DL provided the 0.91% improvement compared to CNN models
respectively.
Table 5. Recognition Accuracy Analysis for MMI database.
Methods| MMI
---|---
CSPL [38]| 73.53
3D-CNN [3]| 95
CNN [14]| 97.81
Ours| 98.72
Fig. 12(a) and (b) shows the ROC performance analysis of proposed MAOP-DL with
the existing SVM for CK+ database and MMI database respectively. From the Fig.
12(a) and (b), it is observed that the proposed MAOP-DL provides the high true
positive rate for small values of false positive rate due to the angle-based
texture pattern and optimal selected features compared to SVM models
respectively for CK+ and MMI database.1. Download: Download high-res image (607KB)
2. Download: Download full-size image
Fig. 12. ROC analysis of (a) CK+ database and (b) MMI database.
## 9\. Conclusion
This paper proposed the MAOP-DL for the clear image analysis and effective
learning of facial expressions. The selection of key point features based on
intensity difference analysis, relevant feature selection and the neural
network based classification carried out in proposed MAOP-DL enhanced the
recognition performance compared to the existing methods. The effectiveness of
proposed MAOP-DL is validated over the number of state-of-art methods on CK+
and MMI database with the parameters of recognition rate and F1- measure. The
novel feature extraction based on the multi angle and the noise removal
through the EBBS contributed towards the illumination variation handling and
the matching accuracy effectively.The major limitations of proposed work are
less performance in involuntary expression handling,low intensity and short
duration expressions are not handled,Spot and recognition of muscle
variations,due to these limitations observed in short duration FER
analysis,Future work should focus on the analysis of
microexpression(involuntary expression with low intensity and short duration)
by extending the multi-angle-based feature descriptors to spot and recognize
it.
## Acknowledgments
This work is supported by the National Key Research and Development Program of
China (2016YFB1001005), the National Natural Science Foundation of China
(Grant No. 61473290, Grant No. 61673375), the National High Technology
Research and Development Program of China (863 Program) under Grant
2015AA042307, and the Projects of Chinese Academy of Science (Grant No. QYZDB-
SSW-JSC006, Grant No. 173211KYSB20160008).
Special issue articlesRecommended articles"
177,179,Multi-pie,"['R Gross', 'I Matthews', 'J Cohn', 'T Kanade']",2010,2679,CMU Multi-PIE,"classifier, facial expression recognition, machine learning","The range of facial expressions captured in Multi-PIE (neutral, smile, surprise, squint, disgust  In this paper we introduced the CMU Multi-PIE face database. Multi-PIE improves upon the",No DOI,Image and vision …,https://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
178,180,Multi-region ensemble convolutional neural network for facial expression recognition,"['Y Fan', 'JCK Lam', 'VOK Li']",2018,168,"Acted Facial Expressions In The Wild, Affective Faces Database","facial expression recognition, neural network","face on facial expression recognition. Our proposed method is evaluated based on two  well-known publicly available facial expression  database, Acted Facial Expressions in the Wild (",No DOI,… Conference on Artificial Neural Networks …,https://arxiv.org/abs/1807.10575,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
179,181,Multi-task convolutional neural network for pose-invariant face recognition,"['X Yin', 'X Liu']",2017,381,CMU Multi-PIE,"CNN, classification, neural network",classification error is more likely to happen compared to controlled datasets with discrete pose  angles. This work utilizes all data in the Multi-PIE  of variations on Multi-PIE. We also apply,No DOI,IEEE Transactions on Image Processing,https://ieeexplore.ieee.org/document/8080244,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
180,182,Multi-task pose-invariant face recognition,"['C Ding', 'C Xu', 'D Tao']",2015,303,CMU Multi-PIE,"classification, deep learning, machine learning, neural network","on FERET, CMU-PIE, and Multi-PIE databases shows that the  Multi-task learning (MTL) is  a machine learning technique  protocol for the pose problem on Multi-PIE, we adopt the three",No DOI,IEEE Transactions on image Processing,https://ieeexplore.ieee.org/document/7006757,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
181,183,Multi-view facial expression recognition,"['Y Hu', 'Z Zeng', 'L Yin', 'X Wei', 'X Zhou']",2008,167,Binghamton University 3D Facial Expression,facial expression recognition,"faculties from State University of New York at Binghamton. The  expressions, captured by a  3D face scanner. With the  in the multi-view emotion recognition experiment. The best average",No DOI,… & Gesture Recognition,https://ieeexplore.ieee.org/document/4813445,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
182,184,Multimodal 2D+ 3D facial expression recognition with deep fusion convolutional neural network,"['H Li', 'J Sun', 'Z Xu', 'L Chen']",2017,241,"Binghamton University 3D Facial Expression, Toronto Face Database","CNN, FER, classification, classifier, deep learning, facial expression recognition, machine learning, neural network",has never been used to learn 3D facial representations in 3D FER. This motivates us to fill   Expression) Database [59] has been the benchmarking for static 3D FER [12]. It includes 100,No DOI,IEEE Transactions on Multimedia,https://ieeexplore.ieee.org/document/7944639,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
183,185,Multimodal learning for facial expression recognition,"['W Zhang', 'Y Zhang', 'L Ma', 'J Guan', 'S Gong']",2015,133,Toronto Face Database,facial expression recognition,"+ database as an example, C is defined as two to distinguish whether the inputs is the latent  facial expression we aim to recognize.  CK+ database, we find that the expression process is",No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S003132031500151X,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Multimodal learning for facial expression recognition
Typesetting math: 100%
Search 
## Outline
1. 2. Abstract
3. 4. Keywords
5. 1\. Introduction
6. 2\. Multimodal FER by integrating texture and landmark
7. 3\. Experimental results
8. 4\. Conclusion
9. Conflict of interest
10. Acknowledgment
11. References
Show full outline
## Cited by (97)
## Figures (9)
1. 2. 3. 4. 5. 6.
Show 3 more figures
## Tables (8)
1. Table
2. Table 1
3. Table 2
4. Table 3
5. Table 4
6. Table 5
Show all tables
## Pattern Recognition
Volume 48, Issue 10, October 2015, Pages 3191-3202
# Multimodal learning for facial expression recognition
Author links open overlay panelWei Zhang a, Youmei Zhang a, Lin Ma b, Jingwei
Guan a, Shijie Gong a
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2015.04.012Get rights and content
### Highlights
* ?
Multimodal learning for facial expression recognition (FER) is proposed.
* ?
The first attempt to do FER from the joint representation of texture andlandmarks.
* ?
The multimodal structure combines feature extraction and classification
together.
* ?
Structured regularization is used to enforce the sparsity of different
modalities.
## Abstract
In this paper, multimodal learning for facial expression recognition (FER) is
proposed. The multimodal learning method makes the first attempt to learn the
joint representation by considering the texture and landmark modality of
facial images, which are complementary with each other. In order to learn the
representation of each modality and the correlation and interaction between
different modalities, the structured regularization (SR) is employed to
enforce and learn the modality-specific sparsity and density of each modality,
respectively. By introducing SR, the comprehensiveness of the facial
expression is fully taken into consideration, which can not only handle the
subtle expression but also perform robustly to different input of facial
images. With the proposed multimodal learning network, the joint
representation learning from multimodal inputs will be more suitable for FER.
Experimental results on the CK+ and NVIE databases demonstrate the superiority
of our proposed method.
* Previous article in issue
* Next article in issue
## Keywords
Multimodal learning
Facial expression recognition
Texture
Landmark
## 1\. Introduction
Facial expression presents a rich source of affective information and thus is
one of the most direct ways for us to understand the psychological state of a
person. Automatic facial expression recognition (FER) is an important and
challenging problem in the communities of computer vision and pattern
recognition, which attracts much attention recently due to its potential
applications in many areas such as human?machine interfaces [21], robotics
[22], driver safety [23], communication and health-care [24].
There exist a number of FER approaches in the past years. Generally speaking,
the current methods can be broadly classified into two categories based on the
availability of the data for recognition. The first category can be regarded
as texture-based methods [4], [5], [6], [7]. Texture modality for FER
represents the facial image information, which displays face expression in
pixel space. As such, texture-related features are extracted from the pixel
value, which is capable of capturing detailed and subtle information of facial
expression. On the other hand, the features are very sensitive to the image
changes, such as luminance and masking effects. Furthermore, the texture-
related features correlate very closely to each individual for FER. The other
category is the landmark-based methods [2], [25], [31]. Landmark indicates
face key points, the corresponding movements of which can help capture the
facial expression. However, the landmark movements cannot efficiently capture
the subtle changes, which may not be able to distinguish the expressions with
similar landmark information.
If the texture modality (facial image) is available, facial features are
extracted from the images which are further fed into classifiers forrecognition. The method in [5] firstly convolves the video clip with Gabor
motion energy (GME) filter in a filter bank for feature extraction. In order
to make the problem close to reality, the first six frames are employed for
feature extraction. Afterwards, support vector machine (SVM) is employed to
train the features for expression recognition. Similarly, SVM is also employed
in [4] for FER. Prior to being fed into SVM, non-negative matrix factorization
(NMF) [14] is performed by minimizing a cost function. Firstly, local patches
are extracted from each facial image, based on which the NMF is performed to
reconstruct a sparse and part-based representation of the patches. Then SVM
comes in handy to perform classification. Moreover, Yang et al. [6] proposed
to represent the dynamics of facial expression for recognition. Haar-like
features are employed for the sake of simplicity and effectiveness. The
K-means clustering method is employed to generate the temporal pattern models
of the expressions, and the Adaboost learning is employed as the classifier
for FER.
If the landmark modality (face key point) is available, features can be
extracted from the landmarks for FER. Similar to texture-based methods, most
landmark-based methods extract handcrafted features from input landmark before
performing recognition. In [25], Perveen et al. proposed to search the
bounding boxes which help compute facial characteristic points (FCP). The
facial animation parameters, such as the openness of eyes, width of eyes and
height of eyebrows, are then evaluated via referring to the FCPs. With these
animation parameters, the expression can be further recognized by employing
the Gini Index [28]. More recently, Lorincz et al. [2] did a pioneering work
on extracting features from the landmark in 3D space for FER. Only the
landmark information is incorporated in 3D constrained local model (CLM). Such
process makes the proposed FER robust against head pose variations.
Additionally, they use either dynamic time warping (DTW) or global alignment
(GA) kernel algorithm to deal with multi-frames considering the spatio-
temporal attribute of facial expression, and the landmark is tracked by using
3D CLM. Afterwards, the Euclidean distance is calculated to build matrix,
where the nearest correlation matrix is found with kernel, and the gram matrix
by DTW kernel or global alignment kernel is further employed for SVM training.
Finally, in order to minimize the classification error, the best parameters
are searched for both kernels. With such processes, state-of-the-arts FER
performance was obtained. He et al. [31] conducted spontaneous facial
expression recognition based on landmarks. First, they normalized the
sequences according to the pupil?s coordinates. Afterwards, they labeled
landmarks on the onset and apex images manually and tracked landmarks on the
whole sequences. The features depicting the point distance variation are
extracted and the hidden Markov model (HMM) is employed to recognize the
facial expression.
Although tremendous progresses of FER have been made in the past few decades,
the problem remains with great challenges. Mostly, all previous work treats
the texture or landmark modality independently, where only the texture or
landmark modality is employed for FER. It has been demonstrated that each
single modality is useful for FER. However, one single modality alone cannot
help obtain the details of facial expression variation while avoiding the
extraneous affections. The texture modality captures the detailed changes of
the face information, which will be helpful for recognizing the subtle facial
expression. However, external variations, such as the lightning condition and
masking effect, will significantly affect the texture features, which will
make the textural-based FER very sensitive. On the contrary, the landmark
modality presents more robust property to the external affections. However,the landmark modality just simply outlines the shapes and contours of the face
which is lack of sufficient detailed information. In this case, the landmark
modality cannot accurately distinguish the subtle facial expression,
specifically for the two expressions with similar landmark information.
Texture and landmark modalities seem to be complementary to each other.
Therefore, how to integrate the two modalities to improve the performance of
the FER system remains an open question. The two modalities are of great
difference, where the texture modality mostly describes the facial detailed
expression, specifically the facial image content, and the landmark modality
describes the positions of face key points.
Nowadays, some algorithms were proposed to address the representation learning
for multiple modalities. In [8], [9], [10], [11], multimodal deep belief
network (DBN) [1] is developed for learning the joint representations from the
input multiple modalities. In [8], the video and audio inputs were employed to
learn a bimodal DBN. In order to further discover the correlations among the
two modalities, both modalities are presented during feature learning but only
a single modality is used for supervised training, which means that the deep
autoencoder is trained to reconstruct both modalities when given only one
modality (video or audio input). In [10], the multimodal DBN is trained to
learn the joint representation of the multimodal data, specifically the text
and image modalities. Firstly, two DBNs are trained for image and text
respectively. To form a multimodal DBN, the two trained DBNs are combined by
learning a joint RBM on top of them. In [11], Deep Boltzmann Machine (DBM) is
employed to train each modality. In order to form a multimodal DBM, the two
trained DBMs are combined by adding an additional layer of binary hidden units
on top of them. From the work in [10], [11], it is possible for the model to
find representations such that some hidden units are tuned only for one
modality while others are tuned only for the other modalities [8].
Besides, there exists another defect with previous methods on FER. As
aforementioned, the previous FER methods can be regarded as a type of two-step
methods. Firstly, handcrafted features from texture or landmark modality are
extracted, which are expected to represent the expression. Subsequently, the
classifiers, such as SVM or Adaboost, or employed for training on the
extracted features for FER. Therefore, in such cases, features are the key
components of the whole FER system. If the features can accurately depict the
expression and are of great discriminations to different expressions, the
classifier can recognize the expressions well. However, all the features are
tuned by hand and thus can hardly ensure the classifier to distinguish the
expression well. Therefore, it would be better to have feature extraction and
classification assembled together to be globally optimized for FER.
In this paper, we make the first attempt to employ different modalities and
assemble the feature representation and classification together for FER.
Specifically, the facial texture and landmark modalities are combined together
to benefit from the inherent properties of the two different modalities. A
joint representation for FER is learned from the texture and landmark
modalities. In order to ensure that the two modalities interact with each
other for the joint representation, a structured regularization method is
employed for each modality to control the connection tightness of
representations. FER is then performed based on the learnt representation.
With such multimodal learning process, the proposed FER method can not only
ensure the robustness of the system to time resolution of the expressions but
also make the method robust against head pose variations. Additionally, the
multimodal learning combines feature extraction and classification together
and thus avoids the cumbersome task of features? handcrafting.The rest of this paper is organized as follows. In Section 2, our proposed
multimodal learning method is introduced. Experimental results are given and
discussed in Section 3. Finally, Section 4 concludes the paper.
## 2\. Multimodal FER by integrating texture and landmark
The proposed multimodal FER is introduced to jointly learn the representations
from multimodal inputs, specifically the texture and landmark modalities. The
texture modality is a collection of local image patches cropped from the
positions indicated by the face key points, while the landmark modality
depicts movements of facial key points in the face expression sequence. The
data processing details are given in Section 3.2.
### 2.1. Multimodal learning architecture
The proposed multimodal learning architecture is illustrated in Fig. 1, which
takes different numbers and types of modalities as inputs and outputs the
final classification results. The proposed multimodal learning architecture
not only considers each modality property but also accounts for the
interactions of different modalities. The proposed multimodal learning
architecture is built by stacking several layers together and feeding the
hidden representation of the _k_ th layer as the input into the (k+1)th layer.
1. Download: Download high-res image (222KB)
2. Download: Download full-size image
Fig. 1. Multimodal learning architecture for FER.
The multimodal learning architecture in Fig. 1 can be formulated
as(1)L^=?(fk(f(k?1)?f2(f1SR(x1,x2,?,xm))))Eq. (1) represents the global
function of the proposed multimodal learning method. L^ is the output class
label of the multimodal learning network. f1SR(·) is the function that firstly
maps the visual input layer to the first hidden layer. As our method targets
at a multimodal learning network, f1SR(·) is an auto-encoder (AE) with the
structured regularization (SR), which enforces the modality-specific sparsity
and density of each modality. As illustrated in Fig. 2(a), AE is a simple
learning circuit aiming to transform inputs into outputs with the least
possible amount of distortion, where _z_ _i_ is the reconstructed signal of
_x_ _i_. It can be observed that AE treats each input node of different
modalities equally, where the contributions of different modalities to the
hidden nodes cannot be well learned. However, different modalities may
contribute differently to the specific classification task, as demonstrated in
Section 3. To overcome this limitation and fully exploit the contributions of
different modalities, AE with SR is employed, which allows the network to
distinguish different modalities for individual treatments. Fig. 2(b)
illustrates the structure of AE with SR, where the connections between the
visual input nodes and hidden nodes as well as the weights are learnt in a
data-driven manner, which can distinguish and learn the representation from
different multimodal inputs for the final classification task.
1. Download: Download high-res image (369KB)
2. Download: Download full-size image
Fig. 2. The structure of AE without SR (a) and with SR (b).
After the AE with SR mapping process, different modalities have been
transformed to the first hidden layer, each node of which takes different
modalities into consideration. AEs, f2,?,fk, are thus employed to map the
feature to the final representation for the classification. By stacking
several AEs, the non-linear properties are fully exploited to generate the
final joint representation of the multimodal inputs (x1,x2,?,xm). Afterwards,
?(·) denoting the classifier, such as SVM, KNN, and softmax, takes the joint
representation as the input to perform the final classification tasks.
The training process can be performed greedily layer by layer. This stackingarchitecture ensures the scalability of the learning ability. On one hand,
more layers can help improve the nonlinearity representation ability of the
neural network. On the other hand, more layers will inevitably introduce more
parameters, especially for the top fully connected layers. Intuitively, more
parameters demand more training data to build a robust deep network and avoid
the over-fitting problem. Therefore, the depth of the proposed network should
be adaptively determined by the specific problem and the number of the
training samples at hand.
#### 2.1.1. Autoencoder (AE)
Each layer constituting the multi-layer learning architecture is an
autoencoder (AE) shown in Fig. 2, which consists of two components, the
encoder and decoder. An encoder e(·) encodes the input x?Rd to some hidden
representation e(x)?Rdh, while a decoder d(·) decodes the obtained hidden
representation back to a reconstructed version of _x_ , to make the
reconstructed signal to be as close as possible to the input. Therefore, the
encoder process can be viewed as a single mapping function
f:Rd?Rdh:(2)yi=f(xi)=?(Wxi+b),where _y_ _i_ represents the encoder output and
_x_ _i_ represents the input of the encoder. W?Rd×dh and _b_ are the mapping
weight and encoder bias, respectively. _?_ denotes a non-linear function,
which can employ sigmoid, tanh, and rectified linear unit (ReLU) function.
With this non-linear mapping process, AE can present strong feature learning
capabilities [12].
In order to obtain the encoder parameters, the following optimization problem
needs to be solved by minimizing the reconstruction error introduced from
AE:(3)minW,b,c(l(x,W,b,c)),where _c_ is the decoder bias, l(x,W,b,c) denotes
the loss function to capture the reconstruction error. There are some
alternatives to define the loss functions, such as the squared error or
Kullback?Leibler divergence (KLD) while the feature values lie in [0,1].
Taking the squared error as the reconstruction error, l(x,W,b,c), Eq. (3) can
be further represented
as(4)l(x,W,b,c)=12n?i=1n?zi?xi?22,(5)yi=?(Wxi+b),(6)?i=yiyi?yi,(7)zi=W??i+c.where
_y_ _i_ is the obtained hidden representations through the feedforward
encoder, Eq. (7) represents the decoder with the bias _c_ , and _z_ _i_ is the
reconstructed signal through performing a round of feedforward encoder and
backward decoder. In order to reduce the effect of filter scale, the _L_
2-normalization is normally performed on all hidden nodes of the encoder level
as expressed in Eq. (6).
As aforementioned, if training each modality separately and learning a joint
representation (e.g. RBM) on top of them, it is possible for the model to find
representations such that some hidden units are tuned only for one modality
while others are tuned only for the other modalities [8]. Similarly, if we
simply employ AE in Eq. (2) to map the multimodal inputs into the hidden
nodes, the network is to connect all nodes of visible layer to nodes of the
hidden layer, which means that all the different modality features are treated
equally. Ignoring the specific properties of different modalities, AE will be
trained to the form that some hidden nodes are strongly connected with some
individual modality inputs while weakly connected to other modalities. As
such, the correlations between different modalities cannot be well learned and
represented. Therefore, to overcome this limitation, we employed the
structured regularization (SR) [17], [18], which allows the network to
distinguish different modalities for individual treatment. Also the modality-
specific sparsity and modality-specific density of the features from different
modalities are enforced and further learned. SR is employed in the layer with
multimodal inputs of Fig. 1 to distinguish and learn the representation fromdifferent multimodal inputs.
#### 2.1.2. Structured regularization (SR)
As aforementioned, the SR function is employed for AE with multimodal inputs
inspired by [17], [18]. Suppose Sr,i as an K×N modality binary matrix, where
_K_ denotes the numbers of modalities and _N_ indicates the number of units in
corresponding modality. For SR, each modality will be used as a regularization
group separately for each hidden unit, applied in a manner similar to the
group regularization, compared with the traditional regularization that treats
each input unit equally and ignores the relationship and correlation between
different modalities. SR is defined
as(8)SR(W[1])=?j=1M?k=1K(?i=1NSr,i|(Wi,j[1])P|)1/pwhere _M_ denotes the total
number of hidden units. _K_ is the total number of the modalities. _N_
indicates the total number of input units in each modality. The regularization
can be viewed as the summation of the corresponding Minkowski distance. For
p?1, the Minkowski distance is a metric as a result of the Minkowski
inequality. When p<1, the Minkowski distance violates the triangle inequality.
In the limiting case of _p_ reaching infinity, the regularization will be
changed to the summation of Chebyshev
distance:(9)SR(W[1])=?j=1M?k=1K(maxi(Sk,i|Wi,j[1]|))which only penalizes the
maximum weight from each input unit to each hidden unit. In order to prevent
over-constraining, the regularization function is modified to penalize nonzero
weight maxima for each modality for each hidden unit without additional
penalty for larger values of these maxima. The regularization function in Eq.
(9) are further modified
as(10)SR(W[1])=?j=1M?k=1KB((maxi(Sk,i|Wi,j[1]|))>0)where _B_ indicates a
Boolean function that takes a value of 1 if its variable is true, and 0
otherwise. The regularization function in Eq. (10) performs a direct penalty
on the number of modalities used for each weight, without further constraining
the weights of modes with nonzero maxima.
By integrating SR into the multimodal AE training as in [17], the objective
function can be further represented
as(11)?W[1]?=argminW[1]?i=1n[1]?zi[1]?xi[1]?22+?·SR(W[1])where(12)zi[1]=?j=1k[1]?j[1]Wi,j[1]where
?j[1] is the hidden node generated by the encoder of the multimodal AE, while
zi[1] is signal reconstructed by the decoder from ?j[1]·n[1] is the number of
the input nodes including all the modality features, and k[1] is the number of
the hidden nodes of the multimodal AE. Wi,j[1] is the corresponding weights of
the multimodal AE by introducing SR. _?_ is the parameter to balance the error
and the regularization terms, which is experimentally set to 3× _e_ ?4 in
practice.
Fig. 2(b) illustrates the structure of AE with SR to demonstrate how SR with
AE works for the multimodal inputs. By integrating SR into AE, the connection
between the visual input layer and the first hidden layer is learned. As Eq.
(10) shows, to minimize SR(W[1]), the zero number of Wi,j[1] should be as
large as possible. As such, only some effective nodes of the visual input
layer get connected with those of the first hidden layer. The comparison of
the structures of AE with and without SR demonstrates that the multimodal
network could distinguish different modalities and learn the correlations
between them automatically.
### 2.2. Multimodal FER
Based on the learning architecture in previous section, we propose a simple
network for FER. A softmax layer is added on top of the multimodal learning
architecture, which takes the learned joint representation as inputs and
outputs the classification results for each facial expression. For each
expression, the softmax layer will determine whether the given inputs,including the texture and landmark modalities, will result in the specific
expression or not. Consequently, the number of output nodes in the
classification layer (top layer) is two. We can employ the introduced network
including SR in the multimodal AE to build the network. The depth of the
network depends on the problem and the number of training samples. As
aforementioned, insufficient training samples will incur overfitting with high
probabilities, for the specific FER case, due to the constraint of the
training sample number, only one hidden layer is employed, of which the
network structure is I?H?C. Fig. 3 shows the structure of our network
succinctly. Take the experiment conducted on the CK+ database as an example,
_C_ is defined as two to distinguish whether the inputs is the latent facial
expression we aim to recognize. _I_ is defined as the size of the data from
all the multimodal inputs, which is set as 4040 in this paper. _H_ denotes the
size of the hidden layer nodes. As facial expressions affect the eyes and
mouth significantly in each frame, the patches covering eyes and mouth are
extracted and resized into 16×16 and 16×10, respectively. Afterwards, these
corresponding patches will be concatenated as individual vectors,
respectively. Supposing that _F_ is the number of frames imported to the
network, the size of eye and mouth modalities become 256× _F_ and 160× _F_ by
temporally concatenating the modality vector from each frame. Furthermore, the
displacements of the landmarks provide more persuasive representation than
static coordinates. Besides, we suppose that features in different directions
contribute differently to FER. As a result, the displacements of the landmarks
in _X_ and _Y_ directions are separated as different modalities. As there are
68 marked face key points in every frame, temporally concatenating the
landmark displacement results in a vector with size of 68× _F_ for each
direction. By considering the texture and landmark modalities together, the
vector size is 4040, with _F_ equals to 5, which is fed into the network for
further training. The output of each hidden nodes is generated by a sigmoid
function ?(a)=1/(1+exp(?a)) of the weighted
input:(13)hj[1]=?(?i=1IxiWi,j[1])(14)p(o|x;?)=?(?i=1Hhi[1]Wi[2])where _o_
denotes the output nodes which indicate whether the expression exists or not,
and _?_ indicates all the parameters in the network, specifically W[1] and
W[2].
1. Download: Download high-res image (221KB)
2. Download: Download full-size image
Fig. 3. The structure of the network.
The object of the learning network is to realize the non-linear mapping
function for the FER. The inference can be realized by the following
function:(15)o^=argmaxop(o|x;?)As mentioned before, each facial expression
will be treated separately, for each of which we construct a network for the
classification. Consequently, Eq. (15) will help distinguish whether the
multimodal input are the facial expression that we aim to recognize.
In order to make the inference, we need to obtain the parameters of the
constructed network, specifically the parameters of the two layers,
respectively. For the parameters W[1] in the multimodal layer, AE is first
pretrained to obtain the initialized parameters. Specifically, the parameters
of multimodal layer are firstly pretrained, which simply learns features from
unlabeled data automatically aiming to transform inputs into outputs with the
least possible amount of distortion. With the process of pre-training, the
constructed network can effectively avoid the risk of trapping in poor local
optima. After the pre-training process, the fine-tuning process needs to be
further performed to make the network more suitable for FER. Thereby, a log-
likelihood function is employed as the object function for further trainingthe parameters W[2] in the softmax layers and fine-tuning the parameters W[1]
in the multimodal layer:(16)???=argmax??t=12logP(L^=L|x;?)??SR(W[1])where _L_
represents the label of the inputs and L^ represents the outputs of the
network. For the parameter training, traditional back-prorogation (BP) [26] is
employed to fine-tune parameters of the constructed deep network. This
algorithm is first proposed by Rumelhart and McCelland, the essence of which
is to minimize the mean squared error between actual output and desired output
based on gradient descent. BP algorithm is especially powerful because it can
extract regular knowledge from input data and memory on the weights in the
network automatically [17]. Simultaneously, it can improve generalization
performance of the learning system, which is fabulous when used in FER.
**Algorithm 1**
Multimodal learning for facial expression recognition.
_TRAINING_ :
---
**Input:** {Xtrain,Ltrain}
**Output:** ?,L^train
1:| Initialize W[1] and W[2] randomly;
2:| Pretraining: W[1] is pretrained based on
?W[1]?=argminW[1]?i=1n[1]?zi[1]?xi[1]|22+?·SR(W[1]) to learn the connections
between the visual input layer and the contributions of different modalities
to the hidden nodes on the benefit of SR;
3:| Finetuning: _?_ is updated according to
???=argmax??t=12logP(L^=L|x;?)??SR(W[1]) to strengthen the recognition
capability of the network;
4:| Record _?_ for the multimodal learning network.
_TESTING_ :
**Input:** _?_ , _X_ _test_
**Output:** L^test
1:| Generate the output class labels L^test of _X_ _test_ based on _?_
according to Eqs. (13), (14);
2:| Output the labels of facial expressions L^test.
Furthermore, in order to prevent over-fitting in training neural network,
drop-out is introduced. Typically the outputs of neurons are set to zero with
a probability of _p_ in the training stage and multiplied with 1?p in the test
stage. By randomly masking out the neurons, dropout is an efficient
approximation of training many different networks with shared weights. In our
experiments, we applied the dropout to all the layers and the probability is
set as _p_ =0.2.
We summarize our proposed multimodal learning for FER as in Algorithm 1. _X_
_train_ is the training sample which contains both textures and landmarks of
facial expression. And _L_ _train_ denotes its corresponding labels. Based on
the training samples, the parameters _?_ of the multimodal learning network,
specifically W[1] and W[2] are trained and learned. For testing, when imported
the testing sample _X_ _test_ to the trained network, the output class label
L^test is generated based on the learned parameters _?_.
## 3\. Experimental results
In order to evaluate the effectiveness of the proposed method, Cohn?Kanade
Extended Dataset (CK+) [15] and the natural visible and infrared facial
expression (NVIE) database [30] are employed for experimental results.
Firstly, the detailed information of the database are introduced. Afterwards,
we will present how to process the input data to obtain the multimodal inputs
for the proposed multimodal FER, including training and testing. Finally,
experimental results are provided to demonstrate the effectiveness of theproposed multimodal method, as well as the performance comparison of the
multimodal inputs and unimodal input.
### 3.1. Database
The Cohn?Kanade Extended Dataset (CK+) [15] is built by Kanade et al., which
is developed for automated facial analysis and has been widely used for
testing the performance of FER algorithms. In this dataset, the facial
behaviors of 210 adults are recorded using two hardware synchronized Panasonic
AG-7500 cameras and participants are 18?50 years of age. There are posed and
non-posed expressions concurrently in the dataset. The facial expression
dynamics of sequences in the CK+ dataset starts from neutral expression and
ends on the apex of the expression. Since we need data with labels for
training and testing, only posed expressions with explicit labels are
selected. There are totally 123 subjects with 593 frontal image sequences in
our input data, where 327 sequences are annotated with the emotion labels
(1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sad and 7=surprise). Each
frame in the sequence is digitized into either 640×490 or 640×480 pixel arrays
with 8-bit gray scale or 24-bit color values, and 68 face key points are
detected by AAM [24] for each frame, which are regarded as the facial
landmark. In this paper, six emotions are selected for FER testing and the
inventory of each expression used in this experiment is shown in Table 1. When
imported to the recognition system, the samples of the certain expression are
set as positive with the rest as negative. Obviously, the positive samples of
expression ?Fear? and ?Sad? are less than others. As the luminance information
is more important for FER, the color frame is converted into gray ones to only
preserve the luminance components before further processing.
Table 1. The number of expressions.
Emotion| Anger| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Number| 45| 59| 25| 69| 28| 83
The natural visible and infrared facial expression (NVIE) database is newly
developed for expression analysis. This database includes two sub-databases,
that are posed database consisting of apex images and spontaneous database
containing images and landmarks from onset to apex images. As the posed
database with only apex frame could not meet our requirement, we did not take
the posed one into consideration. For the spontaneous database, the facial
images were recorded by DZ-GX25M camera with resolution 704×480 under three
different conditions: illumination from left, front and right. There are 105
subjects under front illumination, 111 subjects under left illumination and
112 subjects under right illumination, respectively. A total of 28 landmarks
are located and tracked on each image. Different from the CK+ database, the
labels of samples in the NVIE database are assigned values from 0 to 2 to
every expression. The larger the value, the more likely the sample belongs to
that expression.
### 3.2. Data processing
As aforementioned, the databases contain both texture and landmark modalities
for each facial image. These two modalities reflect different properties of
the facial expression, which should be considered together for FER. As
introduced in [13], the preprocessing of the data is critical to learning
process. In the following, the texture and landmark modalities of the facial
image will be first processed, respectively, before being fed into the
multimodal FER system, as illustrated in Fig. 4.
1. Download: Download high-res image (214KB)
2. Download: Download full-size image
Fig. 4. The structure of our approach.#### 3.2.1. Texture modality
The definition of facial expression is based on the action unit (AU), which is
relevant to the brows, eyes, bridge of the nose and mouth. As a result, the
image patches are extracted around eyes and mouth from one frame, where the
patches around eyes should cover the brows as well as bridge of the nose.
These extracted image patches contain the most pivotal facial features related
to expressions. As Fig. 5 shows, the green points are landmarks on the face,
while the red border are the trim lines. We clip the patches according to the
landmarks on the bridge and the tip of the nose. In order to cover the whole
subject, for the CK+ database, the size of the eye and mouth patches are
defined as 100×100 and 160×100, respectively. After that, these patches are
further downsampled by ten times for dimension reduction, which can further
reduce the parameter number and the computational complexity for training and
testing. Finally, the image patch is concatenated into row vector before
further normalization. The resulting vector size of the eye and mouth is
16×16=256 and 16×10=160, respectively. For the NVIE database, we clip 40×40
eye-patch and 40×60 mouth-patch. Afterwards, the patches are further
downsampled to 20×20 and 20×30. After concatenating them together, the final
vector size is 256×2+160=672 for the CK+ database and 400×2+600=1400 for the
NVIE database, which represents the input of the textural modality for one
frame.
1. Download: Download high-res image (138KB)
2. Download: Download full-size image
Fig. 5. The schematic diagram of eye and mouth patch extraction. (For
interpretation of the references to color in this figure, the reader is
referred to the web version of this paper.)
#### 3.2.2. Landmark modality
The generation of facial expression is a dynamic process. Therefore, for the
landmark modality, movements of the landmarks between the current frame and
the previous one in video flow provide more insightful representation of
facial expression than static landmarks. Additionally, we are not sure whether
the head positions in the images from different people remain unchanged or
not. As a result, we calculate the different value between current frame and
previous one as the movements of landmarks. Assuming that Xt+1i and Yt+1i are
the _i_ th _X_ and _Y_ coordinates in the current frame, respectively, _X_ _i_
_t_ and _Y_ _i_ _t_ are the _i_ th _X_ and _Y_ coordinates in the previous
frame, the landmark movements can be calculated
as(17)?Xti=Xt+1i?Xti?Yti=Yt+1i?YtiNote that the first frame of the input
sequence has no previous frame for reference, which only serves as reference
and is excluded from the landmark modality for FER. After obtaining the
movements from each frame, the movements are concatenated as the input of the
landmark modality, which results in the size of the landmark input modality as
68×2=136 for the CK+ database and 28×2=56 for the NVIE database.
#### 3.2.3. Modality-specific normalization
As the extracted vectors are from two different modalities, a normalization
method under the incentive of [4], [27] is employed to make the network robust
to illumination and contrast variations. In addition, the normalization
process is vital to the network training. The procedure of normalization could
be summarized as follows. Firstly, the mean value of the texture and landmark
from one frame is obtained. Then the difference between real and the mean
value is calculated to remove the individual difference for texture and
landmark modalities. Finally, the standard deviation is divided to make the
data to be normally distributed. Supposing _P_ _j_ as the _j_ th pixel value
of stretched patch (texture modality) or the _j_ th coordinate of landmarkmodality in the row vector, _J_ as the number of pixels in one patch or the
number of landmark modality in one frame, the normalized result P^j of the
input data is obtained by(18)?=?j=1JPjJ?=?j=1J[Pj??]2P^j=Pj???+Cwhere _C_ is a
constant avoiding the numerator be divided by zero. The normalization makes
the network robust to illumination and contrast variation as demonstrated by
[20]. Fig. 6 shows the intensity of input data before and after normalization
in histogram. It can be observed that the values of the input data before
normalization are mostly around 1. After pre-processing, the input data
subjects to normal distribution approximately, which tends to be more suitable
for network training [29].
1. Download: Download high-res image (139KB)
2. Download: Download full-size image
Fig. 6. Histogram of intensity of input data before and after normalization.
(a) Before Normalization (b) After Normalization.
#### 3.2.4. Temporal cascading
The importance of facial dynamics in FER has been established in many vision
experiments [16], [19]. As stated in [5], facial dynamics is about motion
among frames, rather than static patterns. Additionally, the inputs fed to the
network are a row vector conventionally, which makes that the cascading the
multi-frame in the same video together becomes an essential work. After
integrating the texture and landmark modalities from different frames in the
same sequence as one row, the multimodal input data for the network is
prepared. The corresponding input data and labels can be obtained for further
training. As long as the training is finished, the feedforward network with
learned parameters can be employed to recognize facial expressions from
texture and landmark modalities.
### 3.3. Multimodal learning FER results
The experimental settings are as follows. For each facial expression in the
two databases, 2/3 of the whole samples are randomly selected to form the
training set, with the rest as testing samples. The network was trained and
tested for five times, with the average experimental results as the network?s
performance. The receiver operating characteristic (ROC) curves are employed
as the criterion to evaluate the performance, which is more general and
reliable than recognition accuracy [6] for evaluating the FER system. The _X_
-coordinate of the ROC curve is FP/N, where _N_ represents the number of
negative samples and _FP_ (false positive) as the number of samples
incorrectly labeled as belonging to the positive class. Analogically, the _Y_
-coordinate is TP/P, where _P_ indicates the positive samples and _TP_ (true
positive) presents the number of samples correctly labeled as belonging to
_P_. To draw a complete ROC curve, the threshold value ranges from 0 to 1 with
0.01 as the step size to obtain the curve. As aforementioned, there are two
units in the output layer. The value of only one unit is employed for ROC
curve generation. If the value is larger than the threshold, the unit is set
to 1, and 0 otherwise. The area under the ROC curve (AUC) is digital
representation of the performance. Obviously, the larger the AUC, the better
is the classifier.
#### 3.3.1. Comparison to prior study on FER
In order to efficiently assess the performance of our algorithm, we compare it
with existing state-of-the-arts FER algorithms. We first use the first six
frames of every labeled sequence as inputs. As the first frame only serves as
reference, only the texture and landmark modalities from the rest five frames
are extracted for training and testing. The performance is compared with the
recent work done by Lorincz et al. [2], which achieved start-of-the-art
performance. The corresponding results are illustrated in Table 2. It isnoteworthy here that the experimental results from [2] are employed for
performance comparison. Fig. 7 shows the ROC curves of six expressions.
Table 2. Comparison to prior study on FER (first six frames).
Method| Angary| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---
Wu [5]| 0.829| 0.677| 0.667| 0.877| 0.784| 0.879| 0.786
Long [3]| 0.774| 0.711| 0.692| 0.894| 0.848| 0.891| 0.802
Jeni [4]| 0.817| 0.908| 0.774| 0.938| 0.865| 0.886| 0.865
DTW [2]| 0.873| 0.893| 0.793| 0.892| 0.843| 0.909| 0.867
GA [2]| 0.921| 0.905| 0.887| 0.910| 0.871| 0.930| 0.904
Proposed algorithm| 0.948| 0.929| 0.890| 0.916| 0.903| 0.930| 0.919
1. Download: Download high-res image (434KB)
2. Download: Download full-size image
Fig. 7. ROC curves of six different emotions.
Obviously, the performance of algorithms in [3], [5] is inferior to other
methods. Although the dynamic characteristics of facial expression have been
considered and a spatiotemporal GME filter is employed, the texture modality
is only adopted for FER in [5]. Long et al. [3] proved that learning
spatiotemporal filters with ICA works better than spatiotemporal Gabor
features. Yet the final result relies largely on the handcrafted features.
Jeni et al. [4] and Lorincz et al. [2] yield satisfactory results. Jeni et al.
[4] removed personal mean texture manually. Only selected portions of the face
image are employed, where the overall change of the face is neglected.
Conversely, Lorincz et al. [2] used only the landmarks and neglected the
texture one, with some important details of face missing.
With the first six frames as inputs, our algorithm produced better results
than the existing algorithms. Since texture describes the face details and
landmark outlines the shapes and contours, the proposed method integrates them
together to exploit the complementarity of them. However, through the
observation of the CK+ database, we find that the expression process is
incomplete in the first six frames. In order to improve the recognition
performance, we also take six frames which contain the first, the middle four
and the last frame of sequence as inputs. The experimental results displayed
in Table 3 demonstrate that the substantial change of expression reduce the
recognition difficulty and can generate better recognition results.
Table 3. Comparison to prior study on FER (First?Last).
Method| Anger| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---
Yang [6]| 0.973| 0.941| 0.916| 0.991| 0.978| 0.998| 0.966
Long [3]| 0.933| 0.988| 0.964| 0.993| 0.991| 0.999| 0.978
Jeni [4]| 0.989| 0.998| 0.977| 0.998| 0.994| 0.994| 0.992
DTW [2]| 0.991| 0.994| 0.987| 0.999| 0.995| 0.996| 0.994
GA [2]| 0.986| 0.993| 0.986| 1.000| 0.984| 0.997| 0.991
Proposed algorithm| 0.995| 0.999| 0.967| 0.999| 1.000| 1.000| 0.993
Furthermore, Fig. 7 indicates that the performance of recognition on emotions
?Fear? and ?Sad? is inferior than the others, because AUC under the ROC curves
of these two expression is less. The reason may be attributed to the extreme
lack of training samples of these two emotions referring to Table 1. Hence.
the equilibrium, correctness and scale of the dataset are crucial for training
a successful neural network.
#### 3.3.2. Unimodality vs. multimodality
The core idea of this paper is to address the integration of the texture and
landmark modalities for FER. Therefore, it is necessary to compare the
performance with unimodality and multimodality, respectively. Table 4 showsthe corresponding experimental results, where the texture and landmark
modalities are extracted from the integral multimodality dataset. The average
performance of recognition results prove that multimodality is more reliable
than unimodality for FER. It can be observed that the texture modality alone
as the input data performs worse than the landmark modality. This is probably
because the texture modality only covers portions of the face while landmarks
can outline the shape and contour of the whole face. Moreover, the detailed
change of face information that presented by texture and the global change of
face represented by landmark can be viewed as complementary to each other.
Consequently, when combined together, they yield the best FER results.
Table 4. Comparison of the algorithms with unimodality and multimodality.
Inputs| Anger| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---
Texture| 0.770| 0.790| 0.584| 0.921| 0.577| 0.877| 0.753
Landmark| 0.906| 0.893| 0.803| 0.924| 0.703| 0.910| 0.856
Multimodality| 0.948| 0.929| 0.890| 0.916| 0.903| 0.930| 0.919
However, it seems that the recognition of ?Happy? is improved by integrating
both the texture and landmark modalities. It is easy to recognize ?Happy? in
this dataset. The texture and landmark modality alone already performs well.
However, by integrating them together, the network will be much larger, which
requires more training data. In this case, lack of training data can somewhat
lead to overfitting, which results in the performance degradation.
Another issue is that the modality number may affect the FER performance. As
shown in Table 5, FER is first performed with two modalities as inputs, where
the left eye, right eye, and mouth are combined together as the texture
modality, and the X-displacement and Y-displacement are combined together as
landmark modality. The performance result is illustrated in the second row of
Table 5. Furthermore, these components can be treated separately, which are
regarded as five different modalities and fed into the recognition network.
The corresponding results are illustrated in the third row of Table 5. It can
be observed that treating these five modalities separately will help produce
better results. Moreover, it can be concluded that our proposed multimodal
learning network is scalable to different numbers of modalities. As such, by
introducing more related modalities, the FER results will be improved further.
Table 5. Comparison of the algorithms with different numbers of modalities.
Inputs| Anger| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---
(Left-eye + Right-eye + mouth) + (X-displacement + Y-displacement)| 0.923|
0.890| 0.746| 0.923| 0.870| 0.927| 0.879
Left-eye + Right-eye + mouth + X-displacement + Y-displacement| 0.948| 0.929|
0.890| 0.916| 0.903| 0.930| 0.919
#### 3.3.3. Comparison of the algorithms with and without pretraining
Table 6 displays the comparison of algorithms with and without pretraining. It
is demonstrated that, once pretraining is added to the network, the
performance is improved by six percent. BP is based on local gradient descent,
and starts usually at some random initial points, which may cause poor local
optima. If pretraining is employed to initialize the parameters, the network
could be fine-tuned on the pretrained parameters. In this case, the parameters
of the network will avoid the risk of getting stuck at local optima. Table 6
illustrates the FER results with and without pretraining, which demonstrate
the necessity of pretraining.
Table 6. Comparison of the algorithms with and without AE.
Method| Anger| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---BP| 0.907| 0.915| 0.708| 0.909| 0.764| 0.895| 0.850
BP + pretraining| 0.948| 0.929| 0.890| 0.916| 0.903| 0.930| 0.919
#### 3.3.4. One hidden layer vs. multiple hidden layers
The number of the hidden layers _H_ and the number of the units _U_ in each
layer are the hyper-parameters of the network, which is very important for the
network performance. In order to find the best parameters _H_ and _U_ , we
test various combinations of them on the facial expression ?Anger?. Fig. 8(a)
reveals the regularity as follows. Once the number of hidden layer is set to
more than one, 50 units tend to give the best performance. More units will
result in more parameters to be learned. However, the training samples cannot
afford a network with too many parameters, which map the inputs from visible
layer to hidden layer. On the other hand, if the number of hidden units is too
small, it is hard to represent the 4040 input nodes. Our intention is to build
a deep network to learn the nonlinear property of the texture and landmark
modalities for FER. However, the lack of training data cannot afford a deep
architecture for FER. It can be observed that Fig. 8(b) shows the network with
one hidden layer and 100 hidden units performed the best. As the number of
hidden layer increased, the performance on FER will be degraded. Hence, as
illustrated in Fig. 3, the settings of one hidden layer and 100 hidden units
are adopted for FER in this paper.
1. Download: Download high-res image (464KB)
2. Download: Download full-size image
Fig. 8. The recognition result with respect to the number of (a) hidden
layers, (b) hidden units.
#### 3.3.5. The proposed method vs. other classifiers
As aforementioned, we integrate texture and landmark modalities together as
the input and use multimodal learning method to perform the FER. To
demonstrate that the proposed method indeed performs better than other
algorithms, we compare the method to other two classifiers, specifically SVM
and KNN. We first use the same row vector with 4040 units as the input to the
classifiers. Experimental results demonstrate that SVM performs better than
KNN. However, both results are not satisfactory. To further prove that AE with
SR can not only integrate texture and landmark together but also automatically
extract the meaningful features for FER, we import the learned feature of the
hidden layer into SVM and KNN, respectively. As shown in Table 7, after
performing the multimodal feature learning, the performances of both SVM and
KNN are significantly improved. We can conclude that the multimodal learning
can effectively learn the representation from the multimodal inputs.
Table 7. Comparison of the recognition results using proposed method and other
classifiers.
Method| Anger| Disgust| Fear| Happy| Sad| Surprise| Average
---|---|---|---|---|---|---|---
KNN using the row vector| 0.309| 0.493| 0.383| 0.412| 0.434| 0.355| 0.400
SVM using the row vector| 0.892| 0.843| 0.496| 0.836| 0.756| 0.887| 0.785
KNN using the hidden layer units| 0.863| 0.786| 0.636| 0.842| 0.787| 0.865|
0.797
SVM using the hidden layer units| 0.901| 0.876| 0.696| 0.879| 0.832| 0.910|
0.849
Proposed algorithm| 0.948| 0.929| 0.890| 0.916| 0.903| 0.930| 0.919
#### 3.3.6. Experiments on spontaneous database
There are great differences between posed and spontaneous facial expression.
The former is acted intentionally, while the latter is displayed unconsciously
by subjects. The posed expressions are captured by asking subjects to perform
different expressions in front of a camera, which are usually exaggerated. Thespontaneous ones are more natural and different from the posed one both in
appearance and timing. The recognition of spontaneous seems to have more
profound theoretical and practical significances. However, its expression
recognition is thus harder. In this experiment, ?Happy?, ?Fear? and ?Disgust?
are selected as samples to conduct the three-class classification. Fig. 9
illustrates the comparison results of the proposed method and He?s method on
the NVIE database. For ?Disgust?, ?Fear? and ?Happy?, the recognition accuracy
is measured by the ratio of the correctly recognized specific expression over
the total number of specific expression samples. For ?Total accuracy?, the
recognition accuracy is calculated by the ratio of all correctly recognized
samples over all the total number of the samples. It can be observed that for
the comparison of the spontaneous FER, the proposed method performs better
than He?s method. For ?Total accuracy?, 10% accuracy improvement is obtained
by our proposed multimodal learning method.
1. Download: Download high-res image (155KB)
2. Download: Download full-size image
Fig. 9. Comparison with the method [31] on the NVIE database.
## 4\. Conclusion
In this paper, we presented a multimodal FER algorithm, where the texture and
landmark are integrated together to boost the FER performance. In order to
avoid handcrafted features, which are cumbersome and time-consuming, the joint
representation for FER is learned from the built neural network. By
incorporating SR into AE, the proposed network can not only distinguish each
modality but also learn the correlation and interaction between the texture
and landmark modalities, which are complementary to each other. Various
experimental results and comparisons have demonstrated the superiority of the
proposed method over the existing ones.
## Conflict of interest
None declared.
## Acknowledgment
This work was supported by the NSFC Grant nos. 61203253 and 61233014, Research
Found of Outstanding Young Scientist Award of Shandong Province (BS2013DX023),
Independent Innovation Foundation of Shandong University (IIFSDU) 2013TB004,
and Program of Key Lab of ICSP MOE China 2013000002.
Special issue articlesRecommended articles"
184,186,Occlusion aware facial expression recognition using CNN with attention mechanism,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,852,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild, Static Facial Expression in the Wild",CNN,"that are common in the wild. In this paper, we propose a convolution neutral network (CNN)  with attention mechanism (ACNN) that can perceive the occlusion regions of the face and",No DOI,IEEE Transactions on Image …,https://ieeexplore.ieee.org/document/8576656,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
185,187,On solving the face recognition problem with one training sample per subject,"['J Wang', 'KN Plataniotis', 'J Lu', 'AN Venetsanopoulos']",2006,132,Toronto Face Database,"classification, classifier",to be recognized using a generic training database which consists of images from subjects  other than those under consideration. Many state-of-the-art face recognition solutions can be,No DOI,Pattern recognition,https://www.sciencedirect.com/science/article/pii/S0031320306001233,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: On solving the face recognition problem with one training
sample per subject
Typesetting math: 100%
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Generic learning framework
6. 3\. Feature extraction techniques
7. 4\. Multi-learner solution
8. 5\. Experiments and results
9. 6\. Conclusion
10. Acknowledgments
11. References
12. Vitae
Show full outline
## Cited by (98)
## Figures (4)
1. 2. 3. 4.
## Tables (9)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Pattern Recognition
Volume 39, Issue 9, September 2006, Pages 1746-1762
# On solving the face recognition problem with one training sample per subject
Author links open overlay panelJie Wang, K.N. Plataniotis, Juwei Lu, A.N.
Venetsanopoulos
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2006.03.010Get rights and content
## Abstract
The lack of adequate training samples and the considerable variations observedin the available image collections due to aging, illumination and pose
variations are the two key technical barriers that appearance-based face
recognition solutions have to overcome. It is a well-documented fact that
their performance deteriorates rapidly when the number of training samples is
smaller than the dimensionality of the image space. This is especially true
for face recognition applications where only one training sample per subject
is available. In this paper, a recognition framework based on the concept of
the so-called generic learning is introduced as an attempt to boost the
performance of traditional appearance-based recognition solutions in the one
training sample application scenario. Different from contemporary approaches,
the proposed solution learns the intrinsic properties of the subjects to be
recognized using a generic training database which consists of images from
subjects other than those under consideration. Many state-of-the-art face
recognition solutions can be readily integrated in the proposed framework. A
novel multi-learner framework is also proposed to further boost recognition
performance.
Extensive experimentation reported in the paper suggests that the proposed
framework provides a comprehensive solution and achieves lower error
recognition rate when considered in the context of one training sample face
recognition problem.
* Previous article in issue
* Next article in issue
## Keywords
Face recognition
Generic learning
One training sample per subject
## 1\. Introduction
Face recognition (FR) has been an active research area in the past two decades
with a lot of encouraging results reported in literature [1], [2]. However, it
still remains a difficult problem far from being solved. This is due to the
fact that 2-D image representations of the human faces exhibit large
variations due to illumination, expression, pose and aging variations. At the
same time, image examples available for training face recognition machines are
limited which makes the task of characterizing subjects? intrinsic properties
a difficult one.
Among the various FR methodologies, appearance-based approaches which treat
the face image as a holistic pattern seem to be the most successful [3] with
the methods such as the well-known principal component analysis (PCA) [4] (an
unsupervised learning technique) and linear discriminant analysis (LDA) [5],
[6], [7], [8], [9] (a supervised learning technique) dominating the
literature. However, when it comes to realistic FR applications, appearance-
based methodologies often suffer from the so-called small sample size (SSS)
problem where the number of training samples is much smaller than the
dimensionality of the image space [10], [11]. In such an application scenario,
the problem of estimating the intra- and inter-scatter matrices needed for
recognition becomes either ill- or poorly posed [11].
The one training sample per subject problem can be viewed as an extreme SSS
problem that severely challenges conventional FR procedures. It is not hard to
see that in such a case, supervised learning techniques may not be applicable
since the intra-subject information cannot be obtained from one training
sample. However, the one training sample problem is a realistic problem which
exists in many FR applications such as surveillance photo identification,
forensic identification and access control. One possible solution is to apply
an unsupervised technique on the given samples (one per subject). In Ref.[12], an extension of the standard PCA was proposed, denoted as projection-
combined PCA ((PC)2A). The proposal introduced a novel pre-processing scheme
by combining the original image with its first-order projection map followed
by a standard PCA. In order to enhance performance, following the introduction
of (PC)2A framework, Chen [13] proposed an enhanced (PC)2A solution by
including a second-order projection map while Zhang introduced a SVD
perturbation in the pre-processing step [14]. All reported better performance
compared to that of the standard PCA. In addition, Tan proposed a self-
organizing map (SOM) based algorithm [15] to solve the one training sample
face recognition problem. As stated in Ref. [15], the SOM algorithm can
extract local facial features even with a single image sample due to its
unsupervised, nonparametric nature, resulting in a lower recognition error
rate compared to PCA. However, the performance of these approaches was
evaluated using frontal face patterns where only small variations are
considered. If more complex variations such as aging-, illumination- and pose-
variations are included, the recognition performance is still in question
[16].
A different approach to this problem is to artificially generate extra samples
for each subject under consideration. In Ref. [17], Huang et al. proposed a
scheme to construct local facial feature bunches by moving the original image
in four directions. In the method presented in Ref. [18], the sample size for
each subject was increased by partitioning a single face image into a set of
non-overlapping facial blocks. With the available extra training samples,
traditional LDA-style based techniques were applied. However, as indicated in
Ref. [19], the generated facial images are highly correlated and should not be
considered as truly independent training samples, i.e., the created variations
are usually not large enough to cover those observed in reality. Given the 3-D
nature of the human face, in Ref. [20], a 2D-to-3D reconstruction approach was
proposed in the hope that the generated virtual faces with their
illumination-, expression- and pose-variations will allow for the effective
application of a traditional PCA or LDA solution. However, such implementation
is very complex and computationally demanding given the fact that 3-D model
should be constructed. In such a case, some of the facial feature points such
as eyes, nose tip and mouth should be located accurately, which is an
unrealistic requirement in practice.
In this paper, we introduced the so-called generic learning (GL) framework in
an attempt to address the one training sample problem. In the GL-based
recognition system, the training of appearance-based algorithms is performed
using a generic database which contains subjects different from those to be
identified. The learning machine is trained to extract discriminant
information from subjects other than those that will be called to recognize
when in operation. The principle behind the framework is that the discriminant
information pertinent to the specific subjects (those to be identified) could
be learnt from other subjects due to the fact that human faces exhibit similar
intra-subject variations [21]. This well understood principle is driving the
popular Bayes recognition engine [22] or the unsupervised, PCA-like,
solutions. As a matter of fact, PCA can be viewed as a generic learning
technique since its implementation does not require the information regarding
underlying class structure of the data. In other words, it does not target the
specific classes. The Bayesian solution for face recognition [22] is also a
generic learning technique. It treats the recognition as a two class
classification problem, inter-subject variation class and intra-subject
variation class, without specifying such variation belongs to which specific
subjects. In addition, the idea of generic learning has been proposedpreviously as a potential candidate in solving the one training sample
recognition problem. In Ref. [19], the expression-invariant subspace is learnt
by training a PCA machine on the expression-varied images from a set of
subjects different from those under consideration. The method in Ref. [23]
solves the free pose face recognition problem from a single frontal face image
by collecting a generic data set to extract a view-invariant subspace for the
locally LDA engine. Wang et al. [16] proposed a solution to the one training
sample problem by introducing a feature selection mechanism in the feature
space obtained by applying the PCA technique on a generic database.
Since the collection of a generic database is controlled by the system
designer, it is reasonable to assume that at least two image samples are
available for each generic subject. Therefore, any available appearance-based
FR methodology can be readily integrated within the proposed framework. To the
authors? knowledge, a complete and systematic examination of the performance
of the state-of-the-art FR methodologies using a generic training set is not
available. In this paper, extensive experimentation is used to determine how
the state-of-the-art FR methodologies function within the generic learning
framework. The results reported here could be used to determine what specific
FR procedure should be used to solve the one training sample recognition
problem. A multi-learner framework is proposed to boost the generic
performance when a large-scale generic database is available. It is well known
that when a large training set and a large number of subjects are considered,
the recognition performance of appearance-based FR techniques, especially that
of linear approaches, deteriorates due to the fact that the distribution of
face patterns are no longer convex as assumed by linear models. To enhance
performance, the so-called principle of ?divide and conquer? is invoked.
According to it, the large-scale generic database is decomposed into small
subsets. A set of FR subsystems are then generated using the generic database
components. The final result is then obtained by aggregating the outputs of
the various FR subsystems. Extensive experimental results suggest that the
proposed multi-learner framework significantly improves the recognition
performance when a large-scale generic database is available.
The rest of the paper is organized as follows: Section 2 formulates the face
recognition problem under the one training sample scenario and describes the
generic learning framework; in Section 3, a set of state-of-the-art
appearance-based FR solutions which are to be integrated in the generic
learning framework are briefly reviewed for completeness. A multi-leaner
framework to improve the recognition performance is introduced in Section 4
with the description of framework design and the discussion of combination
strategies. Extensive experimentations are presented in Section 5 followed by
a conclusion drawn in Section 6.
## 2\. Generic learning framework
A one training sample face recognition problem can be formulated as follows:
Let S={Si}i=1H be the identity label set of _H_ subjects of interest. Let
G={gi}i=1H, denoted as _gallery_ , be a set of prototypes consisting of _H_
face images, each of which is corresponding to a different subject of
interest, i.e., ID(gi)=Si,i=1,?,H, where ID(·) represents the corresponding
identity. Let p be the unknown face image to be identified, denoted as
_probe_. In appearance-based learning methodologies, each face image is
represented as a column vector of length J(=Iw×Ih) by lexicographic ordering
of the pixel elements, i.e., gi,p?RJ, where (Iw×Ih) is the image size, and RJ
denotes the J-dimensional real space. Thus the objective of an FR task is to
determine the identity of p, i.e., ID(p), where ID(p)?{S1,?,SH}.
Within the generic learning framework, the FR system is built through trainingan appearance-based algorithm on a generic database which contains subjects
different from those of interest. Let Z={Zi}i=1C be the generic database,
containing _C_ subjects with each subject Zi={zij}j=1Ci, consisting of Ci face
images zij with a total of N=?i=1CCi face images available in the database,
where zij?RJ. The subjects in the generic database do not overlap with those
of interest, i.e., ID(Z)?S=?. In the training session, a feature subspace is
extracted through training a feature extraction algorithm on the generic
database. In appearance-based FR solutions, the objective of feature
extraction is to find a transformation ? based on optimization of certain
separability criteria, which produces a low-dimensional feature representation
yij=?(zij) intrinsic to face objects, where yij?RF and ?F?J. While in the
operation session, both gallery samples G and the probe p are projected into
the feature space to get the corresponding feature representations. A nearest
neighbor classifier is then applied to determine the identity of p by
comparing the distances between p and G in the feature space. The identity is
then determined as the one with the smallest distance,
i.e.,(1)ID(p)=ID(gi*),i*=argmini=1H??(p)-?(gi)?,where ?·? denotes the distance
metric. Fig. 1 depicts the diagram of the generic learning framework.
1. Download: Download full-size image
Fig. 1. Generic learning framework.
The generic learning framework is a realistic solution to the one training
sample problem, since a reasonably sized generic database is always available.
Without the sample size limitation of generic database, the available FR
methodologies can be readily integrated in the generic learning framework.
It is well known that there are two key modules in general face recognition
system, feature extraction (FE) and classification. In this paper, we focus on
the feature extraction module. As for classification, a nearest neighbor
classifier is applied for all experiments. The performance of many state-of-
the-art feature extraction methodologies have been examined within the generic
learning framework. In detail, the following techniques have been evaluated in
this paper: PCA [4], IntraBayes (a maximum likelihood version of the Bayes
solution [22]), Fisherface method (FLDA) [5], regularized discriminant
analysis (RLDA) [11], kernel-PCA (KPCA) [24], generalized discriminant
analysis (GDA) [25] and kernel direct discriminant analysis (KDDA) [26]. These
techniques are briefly reviewed in the followed section for completeness.
## 3\. Feature extraction techniques
Feature extraction is one of the most important modules in face recognition
system which produces lower dimensional feature representations of the face
samples with enhanced discriminatory power for classification purposes.
Among various FE solutions, linear techniques are commonly used in literature
whose objective is to produce a transformation matrix _A_ transforming the
original image space to a lower dimensional feature space,
i.e.,(2)?(x)=ATx,x?RJ,?(x)?RF,where _A_ is a J×F transformation matrix. In
general, PCA [4], LDA [5] and Bayes Solution [22] are the three most important
linear FE techniques. PCA is an unsupervised learning technique which provides
an optimal, in the least mean square error sense, representation of the input
in a lower dimensional space. It produces the most expressive subspace for
face representation but is not necessarily the most discriminating one. This
is due to the fact that the underlying class structure of the data is not
considered in the PCA technique. LDA, however, is a supervised learning
technique which provides a class specific solution. It produces the optimal
feature subspace in such a way that the ratio of between-class scatter and
within-class scatter (Sb/Sw) is maximized [5]. LDA based solutions are
generally believed to be superior to those unsupervised solutions, however,they are more subjectable to the so-called ?SSS? problem where the number of
training samples per subject is much smaller than the dimensionality of the
face data J=(Iw×Ih). In such a case, Sw is singular, which makes the direct
optimization of the ratio Sb/Sw impossible. In order to solve the SSS problem,
many strategies have been proposed in literature, such as Fisherface method
[5] and regularized discriminant analysis [11]. Bayes solution is also an
effective FE technique which produces the intra-subject feature space and
inter-subject feature space by maximizing the covariance matrices estimated
from the intra-subject difference samples (image difference samples from same
subject) and the inter-subject difference samples (image difference samples
from different subjects). IntraBayes solution is a maximum likelihood version
of the standard Bayes method in which only intra-subject variation is
analyzed; however, the performance is still satisfying as reported [22].
Therefore, the recognition is performed by determining whether the image
difference of the probe and the gallery sample belong to intra-subject class
or not.
Although linear solutions are successful in many cases, as the complexity of
the face pattern increases, i.e., more complicated variations are included
such as aging and pose variations, they cannot provide satisfying performance.
In such a case, nonlinear models are introduced to capture the highly non-
convex and complex distribution.
Kernel mechanism is the key component in the available nonlinear FE
techniques. The premise behind the kernel based solutions is to find a
nonlinear transform from the original image space (RJ) to a high-dimensional
feature space F by using a nonlinear function ?(·), i.e., ?:z?RJ->?(z)?F. In
the high-dimensional feature space F, the convexity of the distribution is
expected to be retained so that a traditional linear methodology can be
applied [27], [28]. Without defining the exact representation of the nonlinear
function ?(·), it is revealed that the dot product in the feature space F can
be replaced by a kernel function k(·) defined in the original image space RJ
[27], [28]. Let zi?RJ, zj?RJ be two vectors in the original image space, the
dot product of their feature representations ?(zi)?F, ?(zj)?F can be computed
by a kernel function defined in RJ, i.e., ?(zi)·?(zj)=k(zi,zj). Therefore, the
key task of designing a kernelized FE solution is to represent the linear FE
procedure by using dot product forms. Among the available nonlinear FE
solutions, KPCA [24], GDA [25] and KDDA [26] are the most commonly used
techniques.
## 4\. Multi-learner solution
It is well known in appearance-based FR methodologies, accurate estimation of
embedded parameters (e.g., Sb, Sw in LDA based solutions) requires a large
number of training samples due to the high-dimensionality of the input space
J(=Iw×Ih) [10]. Therefore, a large-scale generic database is required to
provide a good generic behavior. However, as the size of the database
increases, the distribution of face patterns becomes more complex. In such a
case, the available FE techniques, especially for those linear ones, may not
be capable of capturing these increased variations, resulting in a performance
degradation. A possible solution is to decompose a complex global FR task into
a set of simpler local FR subtasks based on the principle of the so called
?divide and conquer? [29]. Therefore, a multi-learner strategy is proposed to
further boost the recognition performance.
### 4.1. Database decomposition
From a designer's point of view, the central issue in a multi-learner system
is how to decompose the database into smaller subsets which are used to
construct a set of FR subsystems. In this work, we propose a decompositionscheme which randomly samples subjects in the generic database, building new
training sets by using the identified subjects and their corresponding images.
It is different from the scheme commonly used under the traditional FR
scenarios that samples the images for each subject [30], [31], [32]. In the
authors? opinion, the proposed scheme is a sound design strategy for the
following reasons:
(1) It allows for the combination of many relatively weak recognizers, thus
improving inter-subject variation determination and recognition performance.
In almost all appearance-based FR solutions, the within- and between-class
scatters (Sw and Sb) are the two most important parameters which are used to
characterize the intrinsic properties of the intra-subject and inter-subject
variations. It is further noticed in Ref. [22] that the orientations of these
distributions provide important information, in other words, the eigenvectors
of Sw and Sb are the key factors on which FR performance depends. This is
usually the case in most FR solutions, where the feature subspace is learned
by eigen-analysis of Sw, Sb or St=Sw+Sb and the obtained eigenvectors are used
to form the transformation matrix. Therefore, accurate estimation of the Sw
and Sb eigenvectors critically affects the recognition performance. In FR
applications, the inter-subject variations are expected to be large while the
intra-subject variations are expected to be small, meaning that the
eigenvectors of Sb with large eigenvalues and the eigenvectors of Sw with
small eigenvalues are of most importance. However, the larger in magnitude the
eigenvalue is, the higher the estimation variance of the corresponding
eigenvector could be and that can negatively impact recognition performance.
In the following, we will show this by proving the relationship between the
lower bound of the estimation variance of the eigenvector and its
corresponding eigenvalue.
Let x1,?,xm be _m_ samples drawn from a Gaussian distribution X?N(?,?), where
xi?RJ. Let ei,?i be the corresponding eigenvector and eigenvalue of ?. Thus it
can be easily deduced that ?=?i=1F?ieieiT, and ?-1=?i=1F(1/?i)eieiT, where
F=rank(?). Therefore, the probability distribution function of f(X) can be
expressed
as(3)f(X)=1(2?)N/2|?|1/2exp-12(X-?)T?-1(X-?)=Cexp-12?i=1F1?i(X-?)TeieiT(X-?)where
C=1/(2?)N/2|?|1/2 is the normalization factor.
Let ?^,?^ be the unbiased estimated mean and covariance matrix, respectively
[33], and e^i be the corresponding eigenvector,
where(4)?^=1m?i=1mxi,?^=1m-1?i=1m(xi-?^)(xi-?^)T.The corresponding e^i should
be also considered an unbiased estimator. Let us define the variance
associated with estimating e^i=[e^i1,?,e^iJ] as the sum of the estimation
variance for each of its elements, i.e.,(5)var{e^i}=?j=1Jvar{e^ij}.Using the
Cramer?Rao lower bound definition in Ref. [34], the estimation variance of _j_
th element of the unbiased estimator e^i is bounded from below
as(6)var{e^ij}?[I-1(ei)]jj,where I(ei) is the Fisher Information Matrix, with
its (j,k)th element defined as(7)[I(ei)]jk=-E?2logf(X)?eij?eik.With the
definition of f(X) in Eq. (3), it can be shown
that(8)?2logf(X)?eij?eik=-12?i?2[(X-?)TeieiT(X-?)]?eij?eik=-12?i2(xj-?j)(xk-
?k)=-(xj-?j)(xk-?k)?i,where xi,?i denote the ith element of the vector X and
?, respectively. Therefore,(9)[I(ei)]jk=E(xj-?j)(xk-?k)?i,I(ei)=1?i?.Thus the
Cramer?Rao lower bound (CRLB) of var{e^ij}
is(10)var{e^ij}??i[?-1]jjand(11)var{e^i}=?j=1Jvar{e^ij}??itrace{?-1}=?i?k=1F?k.
It can be observed from Eq. (11)
that(12)if:?m>?n,then:?m?k=1F?k>?n?k=1F?k.Thus the CRLB of the estimation
variance of the eigenvectors corresponding to larger eigenvalues is greater
than that of the eigenvectors corresponding to smaller eigenvalues. In contextof FR applications, the most important discriminant information exists in the
subspace specified by the significant eigenvectors of Sb corresponding to
large eigenvalues. However, according to Eq. (12), the corresponding
estimation is not very reliable. The analysis provided supports the authors?
claim that inter-subject related information which is estimated from data sets
using generic subjects results in weak or unstable recognition procedures in
most cases due to the nature of training. Therefore, combining a set of weak
learners generated by sampling the generic subjects helps to reduce the
variance of the estimation of Sb, and thereafter, improves the recognition
performance.
(2) It is robust against overfitting.
As additional subjects and corresponding image samples are included in the
generic database, the dimensionality of the extracted feature space (_F_)
increases. For example, F=Nsbj-1 for LDA based solutions and F=Nsmpl-1 for PCA
based solutions, where Nsbj and Nsmpl denote the number of training subjects
and the number of training samples, respectively. From classification's point
of view, the dimensionality of the input space increases which introduces more
free parameters in the classification model. However, the number of the
samples used to train the classifier does not change. This is because the
nearest neighbor classifier is trained using the samples available in the
gallery set. This could lead to severe overfitting phenomenon resulting in
poor performance [33], [35]. Therefore, decomposing the generic database into
smaller subsets containing fewer subjects helps to reduce the feature subspace
dimensionality, thereafter, the overfitting can be alleviated.
For the reasons explained previously, the overall generic training database is
decomposed into _M_ training subsets, each of which contains _S_ subjects
randomly selected from the generic database without replacement. While the
decomposition scheme is designed, one question should be answered. How to
determine the number of subjects in each subset (_S_) and the number of
subsets (_M_)? There is no single answer to this question. There is no
systematic way to determine these two parameters a priori in order to achieve
optimal results in terms of recognition rate. A similar problem exists in
clustering applications where the number of clusters has to be determined in
advance for example in K-means algorithm [33]. One possible solution is to
determine these parameters experimentally using a validation set. Since the
focus of this paper is not in designing a clustering solution, the
determination of the optimal value for these two parameters is left for future
research. However, their influence on the recognition performance is analyzed
experimentally and details will be provided in the experiment section.
### 4.2. Framework design
Fig. 2 depicts the multi-learner framework. Let Tk={Tik}i=1S be the _k_ th
generic subset containing _S_ subjects. Each subject Tik={tijk}j=1Sik consists
of Sik face images tijk, where tijk?RJ. Therefore, a total of Nk=?i=1SSik
images are available in the _k_ th generic subset. In the training session,
for each generic subset, a feature extractor is applied independently to
produce a corresponding feature subspace specified by a linear or nonlinear
mapping ?k(·),k=1,?,M. Let us take PCA as an example, _M_ PCA transformation
matrices APCAk,k=1,?,M, are obtained as
follows:(13)?PCAk(x)=(APCAk)Tx,APCAk=argmaxA|ATStkA|,Stk=1Nk?i=1S?j=1Sik(tijk-t¯k)(tijk-t¯k)T,t¯k=1Nk
?i=1S?j=1Siktijk.
1. Download: Download full-size image
Fig. 2. Multi-learner framework.
After feature extraction, in real operations, both gallery samples and the
probe are projected into these feature subspaces to get the correspondingfeature representations Gk and Pk
i.e.,(14)Gik=?k(gi),Pk=?k(p),i=1,?,H,k=1,?,M.Therefore the probe's identity is
determined independently by each of _M_ FR subsystems which is the output of a
nearest neighbor classifier applied in the corresponding feature subspace,
i.e.,(15)IDk(p)=ID(gi*),k=1,?,M,i*=argmini=1H?Pk-Gik?.The final determination
is then obtained by aggregating the outputs of all FR subsystems.
### 4.3. Combination strategy
The final determination of probe's identity is obtained by combining the
decisions from all FR subsystems. In this paper, sum rule and majority vote
are selected as the combination strategies for their simplicity and robustness
[29], [36].
#### 4.3.1. Measurement transformation
In addition to probe's label, each FR subsystem also outputs some measurements
which represent the confidence degree of the corresponding decision. For
nearest neighbor classifier, the associated distance between the probe and
each gallery sample can be used as a similarity (dissimilarity) measure. The
smaller the distance, the higher the confidence that the probe belongs to the
corresponding subject. In order to combine these measures, the outputs should
be firstly transformed to a unified domain such that they have similar scales
and the same physical meaning.
Let dki be the distance between the probe and the gallery sample gi in the _k_
th feature subspace, where i=1,?,H,k=1,?,M. Thus the distances obtained by all
FR subsystems are formulated as follows:(16)DG=d11?d1H???dM1?dMH.
In order to convert these distances to the same range, a normalization
operator is firstly applied to scale the outputs to zero mean and unit
standard deviation [37], i.e.,(17)dki=dki-??,where ? and ? are the mean and
variance of all distances dki,i=1,?,H,k=1,?,M. The scaled distances are then
transformed to confidence measures by using an activation function. Here a
sigmoid function followed by a unity of sum normalization is applied to
approximate the posterior probability [37],
i.e.,(18)rki=11+exp(dki),rki=rki?i=1Hrki,where rki denotes the class score,
approximating the posterior probability p(ID(p)=ID(gi)|p) outputted by the _k_
th FR subsystem.
#### 4.3.2. Combination rules
_Sum rule_ ?The combination by using a sum rule is to sum up the class scores
as a final score. The identity is then determined by choosing the one with the
largest final score, i.e.,(19)ID(p)=ID(gi*),i*=argmaxiRG-Sumi,RG-
Sumi=?k=1Mrki.
_Majority vote_ ?For majority vote rule, each FR subsystem ballots a vote to
the subject to which it believes that the probe image belongs. The final
identity is therefore determined as the one which receives the majority votes,
i.e.,(20)ID(p)=ID(gi*),i*=argmaxiRG-Majorityi,RG-
Majorityi=?k=1MI(k,i),I(k,i)=1ifIDk(p)=ID(gi),0otherwise,where IDk(p) is the
identity of p outputted by the _k_ th FR subsystem.
With the development of the multi-classifier/learner research, many state-of-
the-art combination strategies have been proposed such as rule based schemes
[36], order statistics [38] and stacked generalization [39]. However, the
focus of this paper is to discuss the generic learning solution to the one
training sample problem and the purpose of proposing a multi-learner framework
is to improve the generic performance of the available FE techniques when a
large-scale database is available. Although only simple combination schemes
are utilized in this paper, performance improvement by introducing a multi-
learner framework is still obvious from the experiments. Therefore extensive
examination of different combination strategies is not included in this paper.However, how combination schemes further affect the recognition performance is
still an important problem under investigation.
## 5\. Experiments and results
In this paper, three sets of experiments have been performed to demonstrate
the effectiveness of the generic learning framework under the one training
sample scenario. The first experiment is to evaluate the recognition
performance of the state-of-the-art feature extraction techniques embedded in
the generic learning framework. The second experiment is performed to depict
how a multi-learner framework improves the recognition performance when a
large generic database is available. In the third experiment, some state-of-
the-art one training sample solutions by only using the gallery samples,
denoted as specific learning, are performed for comparison purposes.
In order to make the database ? _generic_ ? as well as reasonably sized, the
whole data set consisting of 5676 images of 1020 subjects is collected from 5
well-known databases, FERET [40], [41], PIE [42], AR [43], Aging [44] and
BioID [45]. Since our work does not include a face detection procedure, all
images should be manually aligned and normalized which requires the coordinate
information of some facial feature points. The FERET database [40], [41]
contains 14,501 images of 1209 subjects. Among them, 3817 images of 1200
subjects are officially provided with the eye location data. In addition, 1016
more images have been manually determined the eye coordinates by the authors.
Therefore, altogether 3881 images of 750 subjects with at least 3 images per
subject are collected from the FERET database to form the data set. The PIE
database [42] was created by the research group from Carnegie Mellon
University which contains 41,368 images of 68 subjects. The database covers a
wide range of image variations due to different illuminations, poses and
expressions. Among them, 3805 images are provided with the coordinate
information of facial feature points. From these 3805 images, 816 images of 68
subjects (12/subject) are selected to form our data set. For each subject,
seven different poses and five different lighting conditions are included.
Following the PIE's naming rule, pose group [27,37,05,29,11,07,09] is
selected, which contains both horizontal and vertical rotations. Images with
pose variations are under a normal lighting condition and with neutral
expressions. For illumination variations, 5 frontal face images with neutral
expressions are randomly selected from all 21 different illumination
conditions with room lighting on. The AR database was created by Aleix
Martinez and Robert Benavente in the Computer Vision Center (CVC) at the UAB
[43]. It contains over 4000 color images of 126 subjects. The database
contains the frontal face images with various facial expressions, illumination
conditions, and occlusions (sun glasses and scarf). Currently, 119 subjects
with 4 images per subject are provided with the information of eye
coordinates. All these 476 images are included in our data set. The FG-NET
Aging database [44] was developed as a part of the European Union project FG-
NET (Face and Gesture Recognition Research Network). It contains 1002
color/gray scale images of 82 subjects at different ages, each of which is
provided with feature point coordinates. Among them, some of the low-quality
or extremely difficult recognized images are discarded (e.g., baby images and
old adult images are not selected at the same time for a specific subject).
Finally 276 images of 63 subjects are included. The BioID database which
contains 1521 face images was designed for face detection purposes [45].
However, each subject in the database has more than two image samples which
makes it applicable for face recognition applications. Thus 227 images of 20
subjects are selected to form our data set. The detailed configuration of the
whole data set is illustrated in Table 1.Table 1. Data set configuration
Database| No. of subjects selected| No. of images per subject| No. of images
selected
---|---|---|---
FERET| 750| ?3| 3881
AR| 119| 4| 476
Aging| 63| ?3| 276
BioID| 20| ?6| 227
PIE| 68| 12| 816
Total| 1020| ?3| 5676
The color images are firstly transformed to gray scale images by taking the
luminance component in YCbCr color space. Thus all images are preprocessed
according to the recommendation of the FERET protocol, which includes: (1)
images are rotated and scaled so that the centers of the eyes are placed on
specific pixels and the image size is 150×130; (2) a standard mask is applied
to remove non-face portions; (3) histogram equalized and image normalized to
have zero mean and unit standard deviation. Then each image is finally
represented as a vector of length 17,154.
### 5.1. Experiment I?generic learning: single feature extractor
The first experiment is to examine the recognition performance of various
feature extraction techniques applied on the whole generic database.
#### 5.1.1. Database
From the whole data set which contains the image samples of 1020 subjects, 60
subjects are randomly selected to form a test set to simulate a real FR task,
denoted as _TstD_. For each subject, one frontal image with neutral expression
is selected to form the gallery of size |G|=60 while the remaining images in
the _TstD_ are treated as probes. Therefore, the objective of an FR task is to
determine the probe's identity, i.e., which gallery subject the probe belongs
to. In the remaining 960 subjects of the data set, SG subjects with the
corresponding image samples are randomly selected to form the generic database
_GenD_ used for training. In order to examine how the size of generic database
affects the recognition performance, a set of experiments are performed
corresponding to SG=[60,80,100,150,200,250,300,400,600,800]. In order to
increase the experiment accuracy, the selection of _TstD_ is repeated 10
times. In addition, for each specific _TstD_ , the selection of SG subjects to
form the _GenD_ is also repeated 10 times. Therefore, altogether 100
repetitions have been made for a specific SG value. The results reported here
are the average of all 100 repetitions.
#### 5.1.2. Protocol and setting
The feature subspace is obtained through training a feature extraction
algorithm on _GenD_. In this paper, seven different feature extraction
techniques are performed, i.e., PCA, IntraBayes (abbreviated as Bayes in the
tables and figures), FLDA, RLDA, KDDA, GDA and KPCA. For RLDA approach, the
regularization parameter ? is set to 0,1, and 0.001 (a value suggested in Ref.
[46]). Thus the corresponding RLDA solutions are denoted as RLDA0,RLDA1 and
RLDA?, respectively. For kernel based approaches, radio basis function (RBF)
is selected as the kernel function for KDDA, KPCA and GDA, i.e.,
k(zi,zj)=exp(-?zi-zj?2/?2). The kernel parameter is selected as ?2=108 for all
three approaches so that the possible influence due to the kernel parameters
can be eliminated.
It is well known that the maximum feature space dimensionality equals to
Nsmpl-1 for PCA based solutions and Nsbj-1 for LDA based solutions based on
the assumption that the dimensionality of image space is larger than the
number of training samples, where Nsmpl is the number of training samples andNsbj is the number of training subjects. In addition, RLDA, KDDA and GDA
approaches are performed by using the MATLAB code provided by the original
authors of Refs. [11], [26], [25], in which some specific schemes are utilized
to further reduce the feature space dimensionality by only keeping the
significant eigenvectors of the between-class scatter with the corresponding
eigenvalues greater than a threshold. As for intra-subject space (IntraBayes),
since the covariance matrix of intra-subject image differences (SIntra) is
equivalent to the within-class scatter (Sw) [21], the corresponding feature
space dimensionality equals to (Nsmpl-Nsbj) [5]. Table 2 lists the maximum
feature space dimensionality obtained by different feature extraction
techniques, averaged over 100 repetitions.
Table 2. Feature space dimensionality
FE Tech.| No. of subjects/no. of images in generic database
---|---
Empty Cell| 60/| 80/| 100/| 150/| 200/| 250/| 300/| 400/| 600/| 800/
Empty Cell| 339| 447| 554| 835| 1118| 1393| 1671| 2241| 3344| 4451
PCA| 338| 446| 553| 834| 1117| 1392| 1670| 2240| 3343| 4450
Bayes| 279| 367| 454| 685| 918| 1143| 1371| 1841| 2744| 3651
FLDA| 59| 79| 99| 149| 199| 249| 299| 399| 599| 799
RLDA0,1,?| 47| 63| 79| 119| 159| 199| 239| 319| 479| 639
KPCA| 338| 446| 553| 834| 1117| 1392| 1670| 2240| 3343| 4450
GDA| 59| 79| 99| 149| 199| 249| 299| 355| 346| 341
KDDA| 58| 68| 74| 80| 82| 84| 89| 89| 88| 87
In the test, both the gallery images and the probe images are projected into
the extracted feature space and a nearest neighbor classifier is applied on
the projection coefficients. The identity of the probe is determined by
comparing the distances between the probe and each gallery sample in the
feature space. Thus the probe belongs to the corresponding gallery subject
with the smallest distance. In the PCA- and IntraBayes-based feature spaces,
Mahalanobis distance is used for classification for its better performance,
while for others Euclidean distance is utilized. Correct recognition rate
(CRR) is then evaluated as the measure of recognition performance.
#### 5.1.3. Results and analysis
The best found correct recognition rates (_BstCRR_) with respect to different
feature extraction techniques are listed in Table 3. It is well known that CRR
is a function of feature number and the best found CRR is the one with the
peak value corresponding to the optimal feature number (NOpt) which is
obtained by exhaustively searching all possible feature numbers. Table 4 ranks
the best found CRRs with rank 1 indicating the best performance while rank 9
indicating the worst performance.
Table 3. Best found CRR (%) for single feature extractors
FE Tech.| No. of subjects in the generic database (SG)
---|---
Empty Cell| 60| 80| 100| 150| 200| 250| 300| 400| 600| 800
PCA| 64.90| 65.17| 65.11| 65.57| 65.74| 65.79| 65.95| 66.46| 66.70| 67.05
NOpt| 101.6| 98.3| 89.9| 82| 78.6| 74.5| 73.1| 74.5| 65.7| 60.9
Bayes| 68.79| 69.52| 69.88| 70.77| 70.60| 70.73| 71.07| 71.09| 71.58| 71.82
NOpt| 186.8| 174.4| 179.0| 150.1| 156.9| 151.6| 133.1| 122.2| 116.4| 114.6
FLDA| 65.41| 64.32| 62.15| 57.90| 53.45| 49.29| 45.39| 37.70| 26.57| 17.93
NOpt| 57.4| 74.4| 91.9| 125.8| 149.5| 171.1| 166.9| 200.0| 241.4| 284.7
RLDA0| 69.54| 71.57| 71.69| 71.30| 69.62| 67.65| 65.99| 61.75| 52.97| 36.08
NOpt| 45.4| 60.3| 76.7| 115.7| 154.2| 194.6| 236.1| 316.2| 476| 636
RLDA?| 70.31| 72.83| 74.14| 75.32| 75.85| 76.06| 76.55| 76.80| 76.98| 77.16
NOpt| 43.2| 59.4| 74.6| 112.8| 150.9| 189.2| 231.7| 310.3| 469.7| 627.8RLDA1| 62.45| 64.18| 65.38| 66.92| 67.84| 68.48| 69.19| 69.34| 70.16| 70.80
NOpt| 40| 50.7| 70.7| 107.5| 147.7| 187.9| 227.4| 305.6| 465.1| 625.2
KPCA| 55.11| 55.85| 56.12| 56.74| 57.11| 57.17| 57.28| 57.45| 57.48| 57.55
NOpt| 283.3| 350.9| 400.5| 649.2| 756.9| 723.8| 746.8| 730.2| 830| 834.6
GDA| 64.77| 66.48| 66.64| 68.78| 70.37| 71.72| 73.45| 75.43| 78.25| 79.93
NOpt| 58.0| 72.6| 77.3| 74.8| 72.4| 65.0| 57.5| 53.7| 44.1| 47.2
KDDA| 69.99| 71.26| 71.77| 73.50| 74.31| 74.71| 75.62| 76.61| 77.95| 79.48
NOpt| 56.6| 65.4| 67.8| 71.1| 66.4| 58.5| 54.6| 42.8| 31.0| 32.2
Table 4. Rank of the best found CRR for single feature extractors
SG| Rank 1| Rank 2| Rank 3| Rank 4| Rank 5| Rank 6| Rank 7| Rank 8| Rank 9
---|---|---|---|---|---|---|---|---|---
60| RLDA?| KDDA| RLDA0| Bayes| FLDA| PCA| GDA| RLDA1| KPCA
80| RLDA?| RLDA0| KDDA| Bayes| GDA| PCA| FLDA| RLDA1| KPCA
100| RLDA?| KDDA| RLDA0| Bayes| GDA| RLDA1| PCA| FLDA| KPCA
150| RLDA?| KDDA| RLDA0| Bayes| GDA| RLDA1| PCA| FLDA| KPCA
200| RLDA?| KDDA| Bayes| GDA| RLDA0| RLDA1| PCA| KPCA| FLDA
250| RLDA?| KDDA| GDA| Bayes| RLDA1| RLDA0| PCA| KPCA| FLDA
300| RLDA?| KDDA| GDA| Bayes| RLDA1| RLDA0| PCA| KPCA| FLDA
400| RLDA?| KDDA| GDA| Bayes| RLDA1| PCA| RLDA0| KPCA| FLDA
600| KDDA| GDA| RLDA?| Bayes| RLDA1| PCA| KPCA| RLDA0| FLDA
800| GDA| KDDA| RLDA?| Bayes| RLDA1| PCA| KPCA| RLDA0| FLDA
It can be observed that supervised learning techniques outperform those
unsupervised ones. The best two approaches are always supervised techniques,
i.e., RLDA?/ KDDA/ RLDA0/GDA, regardless of the size of the generic database.
In comparing with linear solutions, RLDA? outperforms others if the
regularization factor ? is appropriately chosen. IntraBayes solution comes as
the second which outperforms FLDA and PCA. As for nonlinear solutions, KDDA is
superior to GDA and KPCA with KPCA giving the worst performance. If a large
generic database is available (SG?400), KDDA,RLDA? and GDA give the best
performance followed by the IntraBayes solution, RLDA1 and PCA while KPCA,
RLDA0 and FLDA are the worst, i.e.,
RLDA??KDDA?GDA>IntraBayes>RLDA1>PCA>KPCA>RLDA0>FLDA. If a small database is
available (SG<150), the overall ranks change to
RLDA??KDDA?RLDA0>IntraBayes>GDA?PCA>RLDA1>FLDA>KPCA. With a medium sized
generic database (150<SG<400), the corresponding ranks become
RLDA??KDDA>GDA?IntraBayes>RLDA1?PCA?RLDA0>KPCA>FLDA.
As indicated in Table 3, _BstCRR_ is affected by the size of the generic
database used for training. For RLDA1,?, KDDA and GDA, a significant
improvement of _BstCRR_ can be observed as more subjects are included. This is
due to the fact that the increasing of training samples helps to reduce the
variance in the estimation of Sw and Sb. Therefore, the learning capacity of
the algorithm increases resulting in a better performance [33]. However, only
marginal improvement can be observed for PCA, KPCA and IntraBayes solutions.
This can be explained as follows. PCA and KPCA are both unsupervised
techniques without considering class label. They produce feature subspaces by
maximizing both inter- and intra-subject variations. By increasing training
subjects, both inter-subject and intra-subject variations increase. Thus, the
extracted feature subspace contains more inter-subject variations which are
useful for classification as well as more intra-subject variations which have
negative effects. Although learning more inter-subject information will
improve the classification performance, such improvement is discounted by
retaining more intra-subject variations in the reduced feature subspace due to
the nature of unsupervised learning. Therefore, only marginal improvement can
be observed. IntraBayes solution performed here is a maximum likelihoodversion of the standard Bayes method [22] in which only intra-subject
variations are considered. With the increasing of training subjects, more
intra-subject difference samples are available for training, reducing the
variance in the estimation of the intra-subject subspace. Therefore, the
recognition performance improved. However, since only intra-subject variations
are considered in the IntraBayes method, the performance improvement due to
the increasing of inter-subject information cannot be observed. Therefore,
only marginal improvement is achieved compared to those LDA based solutions
which use both intra- and inter-subject information. While for _FLDA_ and
RLDA0, the recognition performance deteriorates as the number of training
subjects increases. This can be expected. Both these two methods use the
conventional Fisher's criterion, which produces the optimal features by
maximizing the ratio of between-class scatter and within-class scatter
(Sb/Sw). For _FLDA_ solution [5], the LDA procedure is performed after a PCA
feature reduction. Therefore, Sw is estimated in the MFLDA=N-SG dimensional
subspace [5]. As for RLDA0 [7], Sb is firstly diagonalized and Sw is estimated
in the between-class subspace with the dimensionality of MRLDA0=SG-1. Thus,
with the increasing of the number of subjects while keeping the number of
samples per subject approximately fixed, both the values of MFLDA and MRLDA0
increase resulting in a higher dimensional Sw estimation. Without losing
generality, let us assume the number of samples per subject is same for all
subjects, denoted as _C_. Therefore, the number of training samples available
to estimate Sw is N=C×SG while the number of elements of Sw matrix to be
estimated is (N-SG)2/2=(C-1)2SG2/2 for _FLDA_ and (SG-1)2/2 for RLDA0,
respectively (please note that ?/2? is used due to the fact that Sw is a
symmetric matrix). Thus the number of elements to be estimated in Sw is of
order O(SG2); however, the number of training samples is of order O(SG). This
indicates that increasing number of subjects (SG) but constant number of
samples per subject (_C_) worsen the SSS problem. This significantly increases
the variance in the estimation of Sw [11]. Therefore, a worse recognition
performance is observed. This suggests that a regularization scheme to reduce
the estimation variance of Sw in LDA based solutions is necessary not only
when the number of training samples for each class is small but also under the
scenario when the number of classes is large.
The best found CRR can be viewed as a potential recognition capacity which
requires a feature selection procedure to achieve the optimal performance.
However, systematically determining the optimal feature number NOpt a priori
is a difficult task. Therefore, in addition to _BstCRR_ , CRR with all
obtained features denoted as _AllCRR_ is also reported. In such a case, a
feature selection procedure can be circumvented which is more applicable in
practice. In addition, for PCA and IntraBayes solutions, a well known e%
energy capturing rule is adopted to determine the feature number, i.e., _N_
features are retained for performance evaluation such that
?i=1N?i/?i=1Nall?i?e%, where Nall is the number of total extracted features
and ?i is the ith eigenvalue of the sample covariance matrix sorted in a
descending order. Table. 5 lists the CRRs with all extracted features for
RLDA0,?,1, FLDA, KDDA, GDA and KPCA as well as the CRRs for PCA and IntraBayes
following an e% energy capturing rule where e=[70,80,90,100]. In order to
evaluate the disparity between the _BstCRR_ and _AllCRR_ , their difference is
also listed, denoted as ?, where ?=BstCRR-AllCRR. It can be observed that ?
increases as the size of generic database (number of subjects included)
increases except for RLDA0. This is especially true for GDA, PCA and
IntraBayes. Fig. 3 depicts the CRR of PCA and IntraBayes with the database
size (SG) of 60 and 800 vs number of features. Obviously, monotonic increasingcurves can be observed when SG=60; however, when SG=800, _CRR_ decreases
rapidly as the feature number increases with the peak CRR achieved if only
small percent of features are utilized. This is consistent with our previous
consideration that the increasing of the number of generic subjects results in
a higher dimensional feature subspace, which leads to a severe overfitting.
However, for FLDA, RLDA0,? KPCA and KDDA, such disparity is still small (??5%)
even with a large generic database (SG=800).This indicates that the
identification with all extracted features is a practical and reasonable
choice for these techniques.
Table 5. CRR (%) for single feature extractors with all features
FE Tech.| No. of subjects in the generic database (SG)
---|---
Empty Cell| 60| 80| 100| 150| 200| 250| 300| 400| 600| 800
PCA-70%| 50.84| 52.40| 53.35| 54.38| 55.33| 56.24| 56.54| 57.04| 57.28| 57.58
80%| 58.90| 60.92| 62.26| 63.08| 64.12| 64.41| 64.43| 65.09| 65.45| 65.31
90%| 63.96| 64.26| 64.04| 63.13| 62.56| 61.97| 61.37| 60.30| 58.96| 58.44
100%| 60.35| 57.09| 54.37| 46.98| 39.87| 33.04| 28.21| 20.57| 12.35| 8.42
?| 4.55| 7.27| 10.74| 8.59| 25.87| 32.75| 37.74| 45.89| 54.35| 58.63
Bayes-70%| 50.60| 53.31| 55.64| 59.27| 61.60| 62.86| 64.00| 65.51| 66.36|
66.81
80%| 59.04| 61.91| 64.12| 66.86| 68.37| 68.99| 69.88| 70.45| 70.71| 70.73
90%| 65.26| 67.67| 68.50| 70.03| 69.62| 69.23| 68.89| 67.49| 65.53| 63.50
100%| 67.98| 67.20| 65.71| 61.21| 56.01| 51.74| 47.10| 36.27| 23.27| 15.93
?| 0.81| 2.32| 4.17| 9.56| 14.59| 18.99| 23.97| 34.82| 48.31| 55.89
FLDA| 65.17| 64.15| 61.82| 57.27| 52.41| 47.75| 43.17| 35.05| 22.55| 14.77
?| 0.24| 0.17| 0.33| 0.63| 1.04| 1.54| 2.22| 2.65| 4.02| 3.26
RLDA0| 69.45| 71.36| 71.59| 71.25| 69.48| 67.56| 65.87| 61.66| 52.90| 36.05
?| 0.09| 0.21| 0.1| 0.05| 0.14| 0.09| 0.12| 0.09| 0.07| 0.03
RLDA?| 70.09| 72.60| 73.89| 74.95| 75.44| 75.60| 76.01| 76.08| 76.02| 76.15
?| 0.22| 0.23| 0.25| 0.37| 0.41| 0.46| 0.54| 0.72| 0.96| 1.01
RLDA1| 60.39| 61.69| 62.51| 62.88| 63.63| 63.76| 63.78| 63.93| 63.92| 63.69
?| 2.06| 2.49| 2.87| 4.04| 4.21| 4.72| 5.41| 5.41| 6.24| 7.11
KPCA| 55.06| 55.80| 56.07| 56.68| 56.97| 56.90| 56.86| 56.81| 56.72| 56.58
?| 0.06| 0.05| 0.05| 0.06| 0.14| 0.27| 0.42| 0.64| 0.76| 0.97
GDA| 64.68| 66.28| 65.93| 66.44| 65.67| 64.83| 63.80| 63.94| 65.52| 66.46
?| 0.09| 0.2| 0.71| 2.34| 4.7| 6.89| 9.65| 11.49| 12.73| 13.47
KDDA| 69.85| 71.13| 71.90| 73.22| 73.66| 73.77| 74.45| 75.16| 75.22| 75.39
?| 0.14| 0.13| 0.17| 0.28| 0.65| 0.94| 1.17| 1.45| 2.73| 4.09
1. Download: Download full-size image
Fig. 3. Correct recognition rate of PCA and IntraBayes vs. feature number.
Similarly, the corresponding _AllCRR_ s are ranked in Table 6. For PCA and
IntraBayes, CRRs corresponding to 80% energy capturing rule are used for
ranking. The comparative ranking of _AllCRR_ is similar to that of _BstCRR_
except that the _AllCRR_ of GDA deviates much from its optima when facing a
large generic database, resulting in a rank degradation.
Table 6. Rank of CRR for single feature extractors with all features
SG| Rank 1| Rank 2| Rank 3| Rank 4| Rank 5| Rank 6| Rank 7| Rank 8| Rank 9
---|---|---|---|---|---|---|---|---|---
60| RLDA?| KDDA| RLDA0| FLDA| GDA| RLDA1| Bayes| PCA| KPCA
80| RLDA?| RLDA0| KDDA| GDA| FLDA| Bayes| RLDA1| PCA| KPCA
100| RLDA?| KDDA| RLDA0| GDA| Bayes| RLDA1| PCA| FLDA| KPCA
150| RLDA?| KDDA| RLDA0| Bayes| GDA| PCA| RLDA1| FLDA| KPCA
200| RLDA?| KDDA| RLDA0| Bayes| GDA| PCA| RLDA1| KPCA| FLDA
250| RLDA?| KDDA| Bayes| RLDA0| GDA| PCA| RLDA1| KPCA| FLDA300| RLDA?| KDDA| Bayes| RLDA0| PCA| GDA| RLDA1| KPCA| FLDA
400| RLDA?| KDDA| Bayes| PCA| GDA| RLDA1| RLDA0| KPCA| FLDA
600| RLDA?| KDDA| Bayes| GDA| PCA| RLDA1| KPCA| RLDA0| FLDA
800| RLDA?| KDDA| Bayes| GDA| PCA| RLDA1| KPCA| RLDA0| FLDA
The experimental results demonstrate that the generic learning framework with
a reasonably sized generic database can provide a satisfying recognition
performance under the one training sample scenario. In particular, supervised
techniques outperform unsupervised techniques in most cases and nonlinear
solutions depict better performance compared to linear solutions as the size
of generic database increases.
### 5.2. Experiment II?generic learning: multiple feature extractors
The goal of this second experiment is to test the hypothesis that a multi-
learner framework, operating on a large generic database, achieves better
recognition performance compared to that of single recognizer.
#### 5.2.1. Database
The same test set _TstD_ in experiment I is utilized in this experiment with
the same gallery and probe configuration. At the same time, a subset of 900
subjects are randomly selected from the remaining 960 subjects to form a
large-scale _GenD_. From _GenD_ , _M_ training subsets SGenDi,i=1,?,M, are
generated. Each subset contains the image samples of _S_ subjects selected
from total 900 subjects in _GenD_ without replacement. In this experiment, _M_
is set to 1?100 and _S_ is varied among [60,80,100,150,200]. The selection of
_TstD_ is repeated 10 times and the reported here results are the average of
10 repetitions.
#### 5.2.2. Protocol and setting
In the training session, a feature extractor is applied on each generic
subset, resulting in _M_ feature subspaces. All seven different feature
extraction techniques applied in experiment I have been performed in this
experiment as well. In the test, all gallery images and the probe are
projected to these feature subspaces, respectively. In each feature subspace,
a single nearest neighbor classifier is applied to determine the probe's
identity independently. The final determination is obtained by aggregating the
decisions from all classifiers with a sum rule and a majority vote rule. For
comparison purposes, the recognition performance of the single recognizer by
applying the feature extraction algorithm on the whole _GenD_ is also
evaluated.
#### 5.2.3. Results and analysis
The best found CRRs of the multi-learner solution (BstCRRM) as well as the
single solution (BstCRRS) with respect to different feature extraction
techniques are listed in Table 7. BstCRRM is obtained by exhaustively
searching all possible feature numbers for each FR subsystem. In order to save
high computational cost, the searching procedure is performed based on the
assumption that the feature subspace of each FR subsystem has the same
dimensionality. Similar to experiment I, CRRs by using all features are also
listed in Table 8, denoted as AllCRRM/AllCRRS.
Table 7. Best found CRR (%) for multiple learner solution (100 FR subsystems)
and single solution
FE Tech.| Single FR| No. of subjects in each generic subset (S)
---|---|---
Empty Cell| 900| 60| 80| 100| 150| 200| 60| 80| 100| 150| 200
Empty Cell| Empty Cell| Majority vote| Sum rule
PCA| 67.54| 69.14| 68.86| 68.79| 68.95| 68.98| 68.63| 68.62| 68.67| 68.81|
68.75
NOpt| 77| 95.5| 82.8| 76.6| 78.3| 72| 87.1| 83.6| 82.8| 76.8| 69.3Bayes| 72.66| 73.62| 73.88| 73.52| 73.57| 73.31| 73.02| 73.33| 73.02| 73.12|
73.18
NOpt| 123| 183.9| 192.7| 180.8| 175.5| 132.3| 182| 183| 163.5| 148.4| 145.6
FLDA| 16.43| 80.52| 79.48| 78.45| 75.10| 71.77| 78.94| 78.72| 76.39| 73.08|
68.80
NOpt| 88| 13| 17.2| 19| 15.7| 21.3| 8| 7| 8.9| 10.6| 10.1
RLDA0| 35.54| 79.94| 79.49| 78.48| 75.96| 73.64| 78.74| 78.34| 77.46| 75.27|
73.02
NOpt| 713| 25.3| 45.2| 65.9| 110.1| 149.8| 22.9| 45.1| 61.7| 109.1| 148.9
RLDA?| 77.75| 79.48| 79.59| 79.36| 78.78| 78.26| 78.81| 79.04| 78.94| 78.25|
78.09
NOpt| 713| 23| 39.1| 58.7| 101.1| 144| 21.9| 41.3| 57.4| 101.7| 144.7
RLDA1| 71.59| 79.96| 80.28| 79.38| 79.19| 78.74| 79.11| 79.56| 79.46| 79.56|
78.36
NOpt| 705| 22.4| 40.1| 55.4| 96.4| 139.1| 23| 38.3| 53.7| 95.3| 136.5
KPCA| 60.14| 55.23| 55.90| 56.34| 56.78| 57.06| 55.20| 55.90| 56.27| 56.78|
57.02
NOpt| 73.5| 213.8| 239.2| 302.9| 466.3| 610.4| 203.1| 245| 289.1| 465| 616
GDA| 81.39| 80.17| 81.39| 81.73| 82.30| 82.12| 79.05| 79.54| 80.58| 80.54|
81.20
NOpt| 30| 19.1| 18.1| 16.4| 16.9| 19.5| 11.8| 8.8| 9.3| 13.6| 15.1
KDDA| 80.81| 82.04| 82.53| 82.61| 82.62| 82.09| 80.63| 81.28| 81.33| 81.55|
81.43
NOpt| 22| 12.8| 11.8| 13.2| 16.5| 20.6| 9| 11.8| 12.3| 14.3| 16.1
Table 8. CRR (%) for multiple learner solution (100 FR subsystems) and single
solution with all features
FE Tech.| Single FR| No. of subjects in each generic subset (S)
---|---|---
Empty Cell| 900| 60| 80| 100| 150| 200| 60| 80| 100| 150| 200
Empty Cell| Empty Cell| Majority vote| Sum rule
PCA| 7.74| 64.00| 61.37| 57.73| 49.59| 42.6| 63.91| 60.93| 57.62| 49.48| 42.34
Bayes| 14.10| 72.86| 71.95| 69.75| 64.41| 60.27| 72.30| 71.63| 69.48| 64.38|
59.76
FLDA| 12.99| 76.62| 74.38| 71.51| 65.07| 59.14| 75.37| 73.02| 70.40| 64.35|
58.25
RLDA0| 35.29| 76.70| 77.24| 77.39| 75.36| 73.18| 76.51| 77.15| 76.88| 74.70|
72.36
RLDA?| 76.91| 75.90| 76.56| 76.98| 76.98| 77.23| 76.08| 76.74| 77.01| 76.99|
76.79
RLDA1| 63.7| 65.74| 66.44| 66.64| 66.68| 66.83| 66.18| 66.73| 67.36| 66.91|
67.15
KPCA| 58.01| 55.04| 55.82| 56.26| 56.70| 56.73| 55.13| 55.82| 56.23| 56.67|
56.73
GDA| 67.17| 76.38| 75.22| 73.92| 71.70| 68.94| 75.39| 74.24| 72.98| 70.63|
68.32
KDDA| 75.39| 77.35| 77.43| 77.34| 77.53| 76.97| 76.97| 77.31| 77.22| 76.86|
76.70
It is evident in Table 7 that the multi-learner solution outperforms the
single solution except for _KPCA_. In particular, for FLDA and RLDA0 which are
failed with a large-scale database, the improvement by introducing a multi-
learner framework is considerable. Comparing BstCRRM and BstCRRS when
S=60,M=100 and majority vote scheme is used, obvious improvement can be
observed. For FLDA, RLDA0 and RLDA1, more than 8% CRR improvement is achieved.
While for other approaches, such improvement ranges between 1% and 2% exceptfor KPCA and GDA. When comparing _AllCRR_ , as indicated in Table 8, the
advantage of applying a multi-learner framework is more obvious. For PCA,
IntraBayes and GDA, whose _AllCRR_ deviate much from their corresponding
optima when facing a large-scale database, the performance improvement is
considerable with the CRR enhancement of 66.26%,58.76% and 9.19%,
respectively. In addition, it can be observed that there is no significant
difference between the two combination schemes with majority vote rule
slightly better than sum rule.
Comparing BstCRRM/AllCRRM with respect to different subject number in each
generic subset, it can be observed that increasing the size of each generic
subset does not necessarily result in an obvious CRR enhancement although the
learning capacity of each base leaner may increase as demonstrated in Tables 3
and 5. The improvement of BstCRRM/AllCRRM when S=200 with respect to that of
S=60 is less than 2%, some are even negative. However, the computational cost
is tripled. In addition, Fig. 4 depicts the relationship between
BstCRRM/AllCRRM and the number of subsets (_M_). It can be observed that the
CRRs increase rapidly at the initial stage, where M<20; however, such
improvement becomes less obvious as more base learners are included. These
phenomena can be expected. It is well known that a necessary and sufficient
condition for combining a set of learners to be more accurate than any of its
individual members is if these base learners are accurate and diverse [47].
Since the number of the total generic subjects is fixed, continuing increasing
the number of learners (training subsets) or number of subjects in each subset
leads to heavier overlapping between different subsets, thereafter, the base
learners trained on which become more similar. The decreasing of base
learners? diversity prevents the continuous performance improvement.
1. Download: Download full-size image
Fig. 4. Best found CRR (BstCRR) and CRR with all features (_AllCRR_) vs.
number of base learners; each base learner is trained on a generic subset
containing 60 subjects; upper: PCA, IntraBayes, FLDA; middle: RLDA0, RLDA?,
RLDA1; bottom: KPCA, KDDA, GDA.
The results suggest that the proposed multi-learner framework improves the
recognition performance when a large-scale generic database is available. In
addition to the performance improvement, the multi-learner framework allows
for distributed execution of the computational load on clusters or networks of
precessing units. This is especially useful if the generic database is large
since the corresponding computational cost and the memory requirement to train
a single global FR system are immense.
### 5.3. Experiment III?specific learning
In order to verify the effectiveness of the proposed framework, a set of
experiments within the specific learning framework are performed in this
section. In contrast to generic learning, experiments reported previously the
so-called specific learning assumes that the recognition engine will be
trained on image samples coming from the subjects of interest, for example,
gallery samples (one frontal image for each subject) in our case. Therefore
only _TstD_ is used in the experiment which is the same as that in experiments
I and II. As before, the results reported here are the average of 10
repetitions to eliminate random variations in the experiment.
Three state-of-the-art learning solutions capable of dealing with the one
training sample problem are adopted. Namely, E(PC)2A1+/A2+ [13], SPCA [14] and
SOM [15] are utilized. The parameters in these algorithms are set to the
values suggested by the corresponding authors. In addition, PCA approach
trained on the gallery samples is also performed as a baseline.
Table 9 lists the best found CRRs of PCA, E(PC)2A1+/A2+, SPCA and SOM. ForPCA, E(PC)2A1+/A2+ and SPCA, the best CRRs are the peak value with the optimal
feature number Nopt which is obtained by searching all possible feature
numbers. While for SOM, the best CRR is obtained by searching all possible _k_
values, a parameter of _k_ NN ensemble defined in Ref. [15].
Table 9. Best found CRR (%) for specific learning
Empty Cell| PCA| E(PC)2A1+| E(PC)2A2+| SPCA| SOM
---|---|---|---|---|---
| 66.00| 59.59| 54.16| 64.13| 58.75
Nopt/k*| 58| 55| 59| 59| 60
Surprisingly enough, in the experiments performed here, the standard PCA
achieves the best recognition rate. This suggests that although the state-of-
the-art specific approaches such as E(PC)2A1+/A2+, SPCA and SOM demonstrate
satisfying performance in frontal face recognition tasks where small
variations are considered [13], [14], [15], their performance under realistic
applications with considerable pose, aging variations is still in question.
Comparing the recognition performance of specific solutions vs. generic
solutions, it is easy to see that within the generic framework, available FR
modules such as IntraBayes, RLDA, KDDA and GDA outperform specific approaches
if a reasonably sized generic database is available for training.
In the above sets of experiments, we discussed issues pertinent to the
application of the generic learning framework to the one training sample per
subject face recognition problem. Some of our findings are highlighted in the
following:
* ?
Generic learning framework should be viewed as a reasonable solution to the
one training sample problem. Available appearance-based face recognition
algorithms can be readily embedded into the framework. Compared to specific
learning, where the training samples are limited to those corresponding
subjects of interest, generic learning demonstrates the better generalization
performance due to the fact that the estimation of the face pattern
distribution is performed on a much larger generic database which contains
more discriminatory information. Increased estimation accuracy leads to better
recognition performance.
* ?
In general, supervised solutions (LDA-based) exhibit better recognition
performance compared to unsupervised solutions when used within generic
learning framework. In addition, nonlinear approaches are superior to those
linear ones when facing a large generic database which contains more complex
data distribution. This is consistent with the results reported in traditional
FR scenarios, where at least two training samples are available for each
subject and same subjects are present in both training and testing sets.
* ?
Performance of FLDA and RLDA0 deteriorates rapidly when used in conjunction
with a large training database. This indicates that the supervised techniques
are subjectable to the SSS problem not only under the traditional scenario
where the number of the training samples for each class is small but also when
the number of classes is large. Therefore, a regularization step is also
necessary for supervised solutions if large number of classes are considered.
* ?
Multi-learner framework is an efficient and cost-effective strategy to boost
the recognition performance when a large generic database is available. In
particular: (1) trained on a smaller data set, techniques such as FLDA and
RLDA0 are less subjectable to the SSS problem, resulting in better
performance. It can be easily observed that the performance of FLDA and RLDA0within the multi-learner framework is even better than that of a stand-alone
RLDA?, a regularized solution with a fine tuned regularization parameter. This
indicates that a determination-of-regularization-parameter procedure can be
circumvented. (2) A multi-learner solution can alleviate overfitting. This is
especially useful when the multi-learner is invoked to boost the performance
of techniques such as PCA, IntraBayes and GDA. As it was shown, the
recognition performance of such methods when all extracted features are used
is inferior to the performance obtained using a smaller ?optimal? feature set,
the determination on which, however, requires feature selection in real
applications. Using the multi-learner framework, feature selection can be
avoided. (3) From a computational complexity point of view, the method is
efficient, since the FR subtasks can be processed in parallel and the
computation cost can be distributed to different work stations.
## 6\. Conclusion
In this paper, the so-called generic learning framework was introduced to
address the one training sample face recognition problem. An FR system is
trained on a generic database which then could be applied to a specific FR
task. The generic database is built off-line using images from subjects other
than those to be identified during actual applications. Extensive
experimentation has been performed to determine the recognition performance of
many state-of-the-art FR algorithms within the generic learning framework. In
addition, a multi-learner framework is proposed to further boost the
recognition performance. Experimentation results suggest that, under the one
training sample scenario, generic learning solutions are superior to those
specific learning solutions and the combination of a set of local simpler
solutions outperforms a global solution in terms of recognition performance as
well as the computation efficiency.
## Acknowledgments
This work is partially supported by a grant provided by the Bell University
Laboratory at the University of Toronto. The first author also acknowledges
the support provided by the Ontario Centres of Excellence through CITO Student
Internship Program. The authors would like to thank the FERET Technical Agent,
the US National Institute of Standards and Technology (NIST) for providing the
FERET database and FG-NET consortium for providing Aging database.
Recommended articles"
186,188,Patch-gated CNN for occlusion-aware facial expression recognition,"['Y Li', 'J Zeng', 'S Shan', 'X Chen']",2018,177,Expression in-the-Wild,CNN,"Then, via a proposed Patch-Gated Unit, PG-CNN reweighs each patch by the unobstructed-  The proposed PG-CNN is evaluated on two largest in-the-wild facial expression datasets (",No DOI,2018 24th international …,https://ieeexplore.ieee.org/document/8545853/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
187,189,Peak-piloted deep network for facial expression recognition,"['X Zhao', 'X Liang', 'L Liu', 'T Li', 'Y Han']",2016,369,CMU Multi-PIE,FER,"Multi-PIE  FER datasets available, we pre-trained GoogLeNet [24] on a large-scale face  recognition dataset, the CASIA Webface dataset [32]. This network was then fine-tuned for FER.",No DOI,Computer Vision–ECCV …,https://arxiv.org/abs/1607.06997,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
188,190,Perceptual and affective mechanisms in facial expression recognition: An integrative review,"['MG Calvo', 'L Nummenmaa']",2016,315,"Affective Faces Database, Karolinska Directed Emotional Faces","classification, classifier, facial expression recognition","We conclude that facial expression recognition, as it has been investigated in conventional   One such method is the affective priming paradigm, whereby an emotional face is briefly",No DOI,Cognition and Emotion,https://pubmed.ncbi.nlm.nih.gov/26212348/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
189,191,Practical emotional neural networks,"['E Lotfi', 'MR Akbarzadeh-T']",2014,136,Affective Faces Database,neural network,"is associated with motivation from emotion to improve or  emotional neural networks (Khashman,  2010). This paper aims to review and develop neural networks motivated from emotion",No DOI,Neural Networks,https://www.sciencedirect.com/science/article/pii/S0893608014001488,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Practical emotional neural networks
Typesetting math: 100%
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Features of the published models and the foundation of the proposed model
6. 3\. The proposed limbic based artificial emotional neural networks
7. 4\. Experimental studies
8. 5\. Conclusions
9. Acknowledgments
10. References
Show full outline
## Cited by (111)
## Figures (10)
1. 2. 3. 4. 5. 6.
Show 4 more figures
## Tables (7)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Neural Networks
Volume 59, November 2014, Pages 61-72
# Practical emotional neural networks
Author links open overlay panelEhsan Lotfi a, M.-R. Akbarzadeh-T. b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neunet.2014.06.012Get rights and content
## Abstract
In this paper, we propose a limbic-based artificial emotional neural network
(LiAENN) for a pattern recognition problem. LiAENN is a novel computational
neural model of the emotional brain that models emotional situations such as
anxiety and confidence in the learning process, the short paths, the
forgetting processes, and inhibitory mechanisms of the emotional brain. In themodel, the learning weights are adjusted by the proposed anxious confident
decayed brain emotional learning rules (ACDBEL). In engineering applications,
LiAENN is utilized in facial detection, and emotion recognition. According to
the comparative results on ORL and Yale datasets, LiAENN shows a higher
accuracy than other applied emotional networks such as brain emotional
learning (BEL) and emotional back propagation (EmBP) based networks.
* Previous article in issue
* Next article in issue
## Keywords
Amygdala
BELBIC
Cognition
Emotional state
Learning
Emotion
## 1\. Introduction
In the literature, there are three paradigms regarding the concepts of neural
networks and emotions. The first is related to emotion recognition and
expression using artificial neural networks, known as affective computing
(Caridakis et al., 2008, Fragopanagos and Taylor, 2005, Ioannou et al., 2005,
Mermillod et al., 2010, Rao et al., 2011). The second paradigm is the modeling
of emotion as a biological process via connectionist approaches for
neuropsychological issues (Frewen et al., 2008, Grossberg, 1975, Grossberg and
Seidman, 2006, Levine, 2007). And the third which is less noted and will be
addressed here is associated with motivation from emotion to improve or create
artificial intelligence tools such as artificial emotional neural networks
(Khashman, 2010). This paper aims to review and develop neural networks
motivated from emotion and is concerned with the methods in which researchers
have applied them successfully in various artificial intelligence-based
application domains such as intelligent control, prediction and classification
as well as pattern recognition. In this framework, we can explicitly address
the brain emotional learning (BEL) based neural networks (Lotfi &
Akbarzadeh-T, 2013a) and the emotional back propagation (EmBP) based neural
networks (Khashman, 2010, Khashman, 2012). Here we attempt to review them,
introduce their abilities and drawbacks and to propose a powerful applied
emotional neural network beneficial to engineering and real world problems.
These applied networks, each of which is associated with an emotional learning
algorithm, have been produced through conceptual models called computational
models of emotion (Marsella, Gratch, & Petta, 2010). BEL based networks have
been created via anatomical computational models and EmBP based networks have
been made via appraisal computational models of emotion. In the anatomical
view, the focus is on the emotional brain. Emotional brain refers to the
portions of the human brain that process external emotional stimuli such as
reward and punishment received from the outside world. Emotional brain has a
superior feature that is fast reacting. Researchers do not have an agreement
on the source of this fast processing. Some researchers believe that this
feature is obtained because of the existence of short paths in the emotional
brain. Others like Pessoa, 2008, Pessoa, 2009 argue that cortical transmission
is fast enough that this short path is unnecessary and that emotional stimuli
are still subject to intentional control. However, in this approach, what
motivates employing models of emotional brain in engineering applications is
the high speed of emotional processing possibly due to the inhibitory synapses
and the short paths in the emotional brain (Lotfi & Akbarzadeh-T, 2014). It is
likely that the most important characteristic of the practical models producedbased on the emotional brain and especially the models including the short
paths and the inhibitory connections is fast learning and quick reacting.
In contrast to the BEL based networks, EmBP based networks are more motivated
by the appraisal approach of emotion. According to this approach, emotional
states can be appraised as situational maps and can be elicited by appraisal
variables. In the appraisal approach, the links between emotional states and
situations is usually defined by If?Then roles, actually the description
levels higher than anatomical view are considered here. The innovative aspect
of EmBP networks is applying the emotional states and situations in the
learning process of artificial neural networks. Although the basic motivation
behind the use of EmBP networks, like many other models in the appraisal
approach, is the building of human-like agents that have emotions (Khashman,
2008). EmBP based networks have been successfully applied in various
engineering applications. The present paper considers the neural networks
which are motivated by emotion on one side, and those with successful
engineering applications on the other side. In Sections 1.1 BEL based neural
networks, 1.2 EmBP based neural networks we review these networks in detail
and in Section 2, present their features and introduce our aims. The proposed
method is then presented in Section 3. Experimental results are evaluated
through several simulations in Section 4. Finally, conclusions are drawn in
Section 5.
### 1.1. BEL based neural networks
These networks are inspired by the anatomical findings of LeDoux, 1991,
LeDoux, 1996, LeDoux, 2000. The important findings of LeDoux include the
characterizing of signal propagation paths in the emotional brain. He argues
that due to the existence of shorter paths in the emotional brain, emotional
stimuli are processed much faster than normal stimuli. This fast processing
has motivated the researchers to model the emotional brain and employ the
resulting models in various engineering applications. Studies of the neural
basis of the emotional brain are described by the limbic system (LS) theory of
emotion. As shown in Fig. 1, LS consists of a complex set of structures
located in the cortical or subcortical areas such as (LeDoux, 1996) amygdala
(AMYG), orbitofrontal cortex (OFC), thalamus, sensory cortex, hypothalamus and
hippocampus. Among these structures, AMYG plays a critical role in emotional
learning and reacting. AMYG stores the emotional memories and responds to each
input stimulus retrieved by them. AMYG is a permanent memory (Fadok et al.,
2010, Griggs et al., 2013, Lamprecht et al., 1997, Yeh et al., 2004), which
has a forgetting process (Hardt et al., 2013, Kim et al., 2011) and is
involved in the attention process (Bianchin et al., 1999, Rolls, 1992).
1. Download: Download full-size image
Fig. 1. The limbic system in the brain from Lotfi and Akbarzadeh-T (2014).
LeDoux argues that there are two different ways that external stimuli can
reach the AMYG. One is short and fast but imprecise and comes directly from
the thalamus. And the other is long and slow but precise and comes from the
sensory cortex. These paths are presented in Fig. 2. Thus AMYG is properly
situated to reach the stimulus extremely quickly and produce the required
reaction. Thus emotional stimuli such as fear can bring about quick reactions,
usually when there is no chance for the rational mind to process the danger.
AMYG is the storage of emotional memories and responsible for emotional
stimuli. AMYG receives reward signals in the learning process and interacts
with the OFC. OFC receives connections from the sensory cortex and AMYG. AMYG
responds to the emotional stimulus. OFC then evaluates the AMYG?s response and
tries to prevent inappropriate answers based on the context provided by the
hippocampus (Balkenius & Morén, 2001).1. Download: Download full-size image
Fig. 2. The routes of sensory information for modeling.
from Lotfi and Akbarzadeh-T (2014).
The BEL algorithm of the AMYG?OFC network model was first proposed by Morén
and Balkenius in 2000 (Balkenius and Morén, 2001, Morén, 2002, Morén and
Balkenius, 2000). The AMYG?OFC model in Fig. 3 learns to react to the new
stimulus based on the history of input rewards and punishment signals.
Additionally, in the model, AMYG learns to associate with emotionally charged
and neutral stimuli. The OFC prevents the formation of inappropriate
experiences and learning connections. AMYG?OFC model consists of two
subsystems which attempt to respond correctly to emotional stimuli. Each
subsystem consists of a number of nodes which are related to the dimension of
each stimulus. At first, the stimulus enters the thalamus part of the model to
calculate the maximum input and submits it to AMYG as one of them. The OFC
does not receive any input from thalamus. Instead, it receives AMYG?s output
in order to update its learning weights (i.e. OFC weights w1,w2,w3 in Fig. 3;
Morén & Balkenius, 2000). Although Morén (2002) defined an internal reinforce
R0 to update the OFC?s weights as follows, (1)R0={[?Ai?REW]+??Oiif
(REW?0)[?Ai??Oi]+Otherwise , it is not clear how values are assigned to the
REW signal while this signal plays a pivotal role in the AMYG learning
process. Lucas, Shahmirzadi, and Sheikholeslami (2004) explicitly determined
the reward signal _REW_ and proposed the BEL base controller named BELBIC
which has been successfully utilized in various control applications (Beheshti
and Hashim, 2010, Chandra, 2005, Daryabeigi et al., 2010, Dehkordi, Kiyoumarsi
et al., 2011, Dehkordi, Parsapoor et al., 2011, Jafarzadeh, 2008, Lucas, 2011,
Mehrabian and Lucas, 2005, Mehrabian et al., 2006, Rouhani et al., 2007,
Sadeghieh et al., 2012) and problem predictions (Abdi et al., 2011, Babaie et
al., 2008, Gholipour et al., 2004, Lotfi and Akbarzadeh-T, 2012, Lotfi and
Akbarzadeh-T, 2013b). The reward signal _REW_ modified by Lucas et al. (2004),
Babaie et al. (2008) and Abdi et al. (2011) is as follows:(2)REW=?jwjrj. Here
r stands for the factors of the reinforcement agent and w represents the
related weights which are selective and model sensitive. For example,
Dehkordi, Parsapoor et al. (2011) defined the weights in a specific form for a
special speed control application and Khalilian, Abedi, and Zadeh (2012) used
another form of weights for position control of a hybrid stepper motor. These
weights are problem specific and can be arranged in a way that solves a
special problem. These equations for _REW_ make the AMYG?OFC network model
sensitive and lead to the low ability of the model with changes in the subject
matter. It also renders the models ineffective in learning different patterns
with opposite behaviors. The first model free version of BEL named BELPR
(brain emotional learning based pattern recognizer) has been proposed by Lotfi
and Akbarzadeh-T (2013a). The model free property of BELPR was achieved by
using the activation functions in the architecture and the target value in the
learning phase. Instead of reward signals in the learning phase, BELPR employs
the target (T) of input pattern in the following way: (3)REW=T.
1. Download: Download full-size image
Fig. 3. AMYG?Orbitofrontal model proposed by Morén (2002).
Actually BELPR applied the target T to produce internal reinforcement R0
presented in Eq. (1). Putting the target instead of Eq. (2) holds this point
that the model can be adjusted by pattern-target samples. A decay rate in the
learning rules which controls the effects of using targets is used in BELPR
and also ADBEL (Lotfi & Akbarzadeh-T, 2014). Thus the learning rules of BELPR
are as follows:(4)vjnew=(1??)vjold+?max(T?Ea,0)Pjfor
j=1?n+1(5)wjnew=wjold+?(E?T)pjfor j=1?n where T is the target value, P is theinput pattern, ? and ? are learning rates and ? is decay rate in AMYG learning
rule, where T?Ea is calculated error as an internal reinforcer and the _max_
operator causes the monotonic learning. The added decay rate has in fact a
neurobiological basis, simulating the forgetting role of AMYG and is discussed
in Lotfi and Akbarzadeh-T (2014).
### 1.2. EmBP based neural networks
In contrast to the BEL based networks which are based on anatomical
approaches, EmBP based neural networks emphasize the appraisal approach.
Appraisal approach is the most fruitful source for designing symbolic AI
systems (Marsella et al., 2010). It states that an emotion is a personal
appraisal of person?environment relationship. Frijda and Swagerman (1987),
Lazarus (1991), Ortony, Clore, and Collins (1988) and Scherer (2001) are among
pioneers of appraisal approaches including the following models: ACRES (Frijda
& Swagerman, 1987), AR (Elliott, 1992), EM (Reilly, 1996), WILL (Moffat,
Frijda, & Phaf, 1993), TABASCO (Staller & Petta, 2001), FLAME (El-Nasr, Yen, &
Ioerger, 2000), EMILE (Gratch, 2000), CBI (Marsella, Johnson, & LaBore, 2003),
ACTAFFAC (Rank, 2004, Rank and Petta, 2005), PARLEE (Bui, 2004), EMA (Marsella
& Gratch, 2009), THESPIAN (Si, Marsella, & Pynadath, 2005), FEARNOT (Aylett,
Louchart, Dias, Paiva, & Vala, 2005) as well as PEACTIDM (Marinier & Laird,
2008). This approach emphasizes that an emotion must be appraised through
situation maps. The maps have been frequently defined by If?Then roles. For
example, in the FLAME model, fuzzy sets were applied to present the emotions
and fuzzy rules were used to define the maps from events to emotions and
emotions to actions. In this framework, some researchers investigated the
impact of emotion upon learning. For example Poel, den Akker, Nijholt, and van
Kesteren (2002) proposed the SHAME model to investigate the emotional states
during learning, and Khashman (2008) investigated the effect of the added
emotional factors on learning and decision making capabilities of the neural
network. The innovative aspect of Khashman, 2008, Khashman, 2009a, Khashman,
2010 is applying the emotional states in the learning process of Multilayer
Perceptron (MLP). He used anxiety and confidence as emotional states affecting
the learning process and modified back the propagation (BP) learning
algorithm, using them. The resulting algorithm was named the EmBP learning
algorithm. EmBP has additional emotional weights that are updated using two
emotional parameters: anxiety and confidence. He assumes that the anxiety
level is high at the beginning of a learning task and the confidence level is
low. After some time, practice and positive feedbacks decrease the anxiety
level as the confidence level grows. He sets the initial confidence
coefficient value to ?0? and defines anxiety coefficient (?) and confidence
coefficient (k) values;(6)?=YAvPAT+E and, (7)k=?0??i where YAvPAT is the
average value of all patterns presented to the neural network in each
iteration, E is the error feedback, ?0 is the anxiety coefficient value at the
first iteration and ?i is the anxiety coefficient value in the subsequent
iteration. According to Eqs. (6), (7), these variables (i.e. anxiety and
confidence) are not independent and essentially opposite. At the beginning of
a learning task, high anxiety causes more attention to be devoted to the
current new learning samples. However, a greater reliance on previously
learned samples is resulted from a high level of confidence. In other words,
anxiety and confidence can maintain a balance of attention between the new and
previously learned data. The EmBP profits from this mechanism and yields
higher performance in the learning process.
Additionally, EmBP has been improved by adding cognitive and emotional
neurons. In the resulting architecture named DuoNN (Khashman, 2010), a dorsal
neuron for cognition and a ventral neuron for emotion are added to a hiddenlayer, as illustrated in Fig. 4. The DuoNN presented in Fig. 4 is motivated by
the attention?emotion interaction model of Fragopanagos and Taylor (2006). The
emotional processing occurs in a ventral network and cognitive processing is
located in a dorsal network. Fragopanagos and Taylor (2006) argue that emotion
can guide the individual?s attention which in turn controls cognition. In this
model, a dorsal attention circuit of dorso-lateral prefrontal cortex and a
ventral attention circuit are in interaction with OFC and in turn, all of them
are in interaction with AMYG. In the DuoNN, dorsal and ventral circuits are
modeled by two hidden neurons where the anxiety and the confidence
coefficients affect the learning of the weights. Furthermore, DuoNN can
successfully control attention in the learning process. In the learning
algorithm, the high value of anxiety reduces the system?s attention to the
derivative of errors in the output while an increase in the confidence level
which is the result of a decrease in stress level means that more attention is
attributed to previous adjustments.
1. Download: Download full-size image
Fig. 4. DouNN proposed by Khashman (2010).
Although the main motivation towards Khashman?s emotional modeling is to
simulate the human emotions, these networks have been successfully utilized in
various applications such as pattern recognition and classification (Khashman,
2009b, Khashman, 2009c, Khashman, 2012, Maglianesi and Stegmayer, 2012),
optimization (Yang, Wang, Yuan, & Yin, 2012) and decision making (Khashman,
2011, Lim and Jain, 2010). In the next section we present the features of the
networks and try to apply them in order to improve the emotional neural
networks.
## 2\. Features of the published models and the foundation of the proposed
model
As it was reviewed, applied artificial emotional neural networks are inspired
by emotions or based on anatomical studies or model the emotional states in
the learning algorithms. Despite all the existing fundamental differences
among these networks, they have two features in common: first, they have
proved to be significantly effective in engineering applications and second,
they have exhibited high speed and quick convergence in these applications.
For example, by including emotional states, the EmBP algorithm has increased
the learning speed compared to BP in facial detection, and BELPR algorithm has
shown very low computational complexity. This distinguished feature is not
accidental and it is also present in the biological processes relating to
emotions. According to Goleman (2006), processing emotions in the brain is
extremely fast which provides the capability of having quick reactions or
responses.
Each of these methods holds a unique view regarding emotions and they have
been successful in modeling the respective features based on it. The
neuropsychological features and related applications are summarized in Table
1. For example, BELPR and ADBEL models include the features of memory decay
and inhibitory connections but do not model attention and emotional states in
the learning process, something that can be important from the appraisal view,
and also something which is properly modeled in EmBP networks. However, these
networks do not include the other neurological features of the emotional brain
such as inhibitory connections which control emotional responses and the
decaying process. It seems that the method employed in the EmBP algorithm for
modeling emotional states in the learning process can be a turning point in
including emotional descriptions in the neural processing of emotion. In other
words, it can be a turning point in expressing emotional states in the form of
anatomical models, instead of If?Then rules, in the appraisal approach.Therefore, it can be employed in adding the effects of emotional states in BEL
models. We have assumed the following in our ambitious view: the more a model
can incorporate the behavioral characteristics of neurological emotions the
more effective it can be in, firstly, explaining the biological considerations
of emotions and, second, its engineering applications. The development of such
methods has lead to the development of artificial intelligence instruments. On
the other hand, since they are successful in solving real world problems, if
they are based on neurological emotions, they are more reliable in explaining
biological issues.
Table 1. The summary of reviewed applied artificial emotional neural networks
(+: feature presence, ?: absence and ?: not examined).
Model| Architectures| AMYG?OFC| BELBIC| MLP| DouNN| BELPR| AMYG?OFC
---|---|---|---|---|---|---|---
Learning algorithm| BEL| BEL| EmBP| EmBP| DBEL| ADBEL
Reference| Morén (2002)| (Lucas et al., 2004)| Khashman (2008)| Khashman
(2010)| (Lotfi & Akbarzadeh-T, 2013a)| (Lotfi & Akbarzadeh-T, 2013b)
Neuro-psychological features| Long term memory| +| +| +| +| +| +
Forgetting process| ?| ?| ?| ?| +| +
Inhibitory task| +| +| ?| ?| +| +
Attention process| ?| ?| +| +| ?| ?
Supervised learning| ?| ?| +| +| +| +
Emotional states| ?| ?| +| +| ?| ?
Application| Simple pattern learning| +| +| +| +| +| +
Intelligent control| ?| +| ?| ?| ?| ?
Classification| ?| ?| +| +| +| ?
Prediction| ?| ?| +| +| +| +
Facial recognition| ?| ?| +| +| +| ?
Here, we aim to propose a model which can simulate emotional states as well as
EmBP networks on one hand, and model neurological emotional processes and BEL
networks on the other hand. We expect the model to not only function as well
as the mentioned networks, but to have a better application in artificial
intelligence. Furthermore, we have identified areas of possible improvement
within these networks and algorithms:
* ?
Firstly, BEL models do not include the emotional states. Secondly, in BEL
models, network nodes can be programmed identical to the artificial perceptron
model, something which does not happen in such models. This is only possible
by adding a bias to the model?s nodes and using the activation function. The
BEL models have properly modeled the important foundations of the emotional
brain and what we do, equip them with more biological features of emotion. In
fact, we add emotional states and have brought them to a lower level of
processing (Kasabov, 2014) i.e. the level of neurons. Although at this level,
applying a more biological type of neuron, e.g. spiking neurons (Kasabov,
Dhoble, Nuntalid, & Indiveri, 2013), may be more interesting. A simple model
such as perceptron can properly introduce this issue.
* ?
The EmBP networks are obviously different from the neurophysiological
processes of the brain but it can be said that they are somehow close to the
common multilayer artificial networks and they have been able to successfully
model emotional behaviors by adding the related coefficients. Here, we attempt
to study the way emotional states and behaviors are modeled in a model close
to the brain physiology. We combine and develop these features in the form of
a network model in order to obtain a more comprehensive simulation of emotions
on one hand and improve the results in engineering applications on the otherhand. The proposed model is discussed in Section 3.
## 3\. The proposed limbic based artificial emotional neural networks
Here we propose a novel applied neural model of emotion to use the advantages
of using emotional states in learning, like EmBP networks, and apply the
inhibitory connections in the structure, like BEL networks. The proposed
method named LiAENN can be used for multiple-inputs multiple-outputs pattern
recognition, classification and prediction problems. The multiple-input
single-output architecture of LiAENN is proposed in Fig. 5; and Fig. 6 shows
the multiple-input multiple-output architecture. In the figures, the solid
lines and the dashed lines present the data flow and the learning flow
respectively. The input pattern is illustrated by vector p0<j<n+1 beginning
the data flow and feed forward computing and t is the target value beginning
the learning flow as well as backward learning computing.
1. Download: Download full-size image
Fig. 5. Proposed computational model of AMYG?OFC interaction. p1,p2,?,pn are
input pattern, E in final output and t is the related target in the learning
process. The solid lines present the data flow and learning lines are
presented by dashed lines. The model has two internal outputs: Eo that is used
for adjusting its own weights and Ea that is used for adjusting Amygdala and
OFC weights (see Eqs. (12), (13), (14), (23), (24), (25)).
1. Download: Download full-size image
Fig. 6. Proposed multiple-inputs multiple-outputs architecture of LiAENN. Each
output unit is associated with one OFC and AMYG part interacted separately.
The details of each output unit are presented in Fig. 5.
### 3.1. Feed forward computing
The input signal p0<j<n+1 enters the thalamus and then goes to the sensory
cortex. The AMYG receives the input pattern p1,p2,?,pn from sensory cortex,
and receives pn+1 from the thalamus. pn+1 calculated by the following formula
is the output of thalamus and one of AMYG inputs: (8)pn+1=meanj=1?n(pj) and as
illustrated in Fig. 5. vn+1s are related weights. Actually the _mean_ operator
simulates the imprecise information coming from thalamus as discussed in
Section 1.1. Furthermore the OFC receives the input pattern including
p1,p2,?,pn from the sensory cortex. And according to the biological process
there is not any connection between thalamus and OFC directly. AMYG and OFC
are the two main subsystems. AMYG is modeled by a two layer perceptron with
single output neuron and two hidden neurons, and also OFC is modeled by
another two layer perceptron as presented in Fig. 5, to inhibit the AMYG
responses. AMYG and OFC make two internal outputs. Ea is the internal output
of AMYG and Eo is the output of OFC. They are calculated by the following
formulas;
(9)Ea=fa2(v1,12fa1(?j=1n+1(vj,11pj)+ba11)+v2,12fa1(?j=1n+1(vj,21pj)+ba21)+ba12)(10)Eo=fo2(w1,12fo1(?j
=1n(wj,11pj)+bo11)+w2,12fo1(?j=1n(wj,21pj)+bo21)+bo12)
where ba12 is the bias of the single output neuron in AMYG, ba11 is the bias
of the first hidden neuron, ba21 is related to the second hidden neuron, ps
are the elements of the input pattern, vs are related learning weights where
the superscript shows the number of layers and the subscript shows the related
connection between two neurons. For example, v2,12 is the AMYG weight in the
output layer located between the second neuron in the hidden layer and the
output neuron. In the equations, fa1 is the first layer?s activation function
of AMYG and fa2 is the second layer?s activation function, bo is OFC bias, ws
are learning weights of OFC and fo is the activation function of OFC. Actually
Eq. (9) is the feed forward computation of AMYG as a two layer perceptron with
two hidden nodes and single output node, and Eq. (10) is the feed forward
computation of OFC. Finally, final output is simply calculated by thefollowing formula: (11)E=Ea?Eo.
Actually this subtraction implements the inhibitory task of OFC.
### 3.2. Learning backward computing
Firstly, the learning weights of AMYG must be adjusted. This adjustment is
based on the error back propagation algorithm. Let T be the target value
associated with the input pattern p. Thus the error is (12)err=T?Ea and the
sensitivity (Hagan, Demuth, & Beale, 1996) of the second layer is;
(13)S2=?2×(fa2)?×err where symbol × denotes simple multiplication. Now, the
second layer?s weights can be updated as follows;
(14)v1,12=v1,12??×S2×(v1,12×fa1(?j=1n+1(vj,11×pj)+ba11))(15)v2,12=v2,12??×S2×(v2,12×fa1(?j=1n+1(vj,2
1×pj)+ba21))(16)ba22=ba22??×S2
where ? is the learning rate.
Then, the sensitivity on the hidden layer must be calculated and for the first
hidden neuron is (17)S1=(fa1)?×v1,12×S2 that updates the learning weights of
the first hidden neuron as follows; (18)vj,11=(1??)×vj,11??×S1×pjfor
j=1?(n+1)(19)ba11=(1??)×ba11??×S1. Similarly, for the second hidden neuron is
as follows; (20)S1=(fa1)?×v2,12×S2(21)vj,21=(1??)×vj,21??×?×S1×pj+k×?vj,21for
j=1?(n+1)(22)ba21=(1??)×ba21??×?×S1+k×?ba21 where ? and k are updated by Eqs.
(6), (7) at each iteration and ? is decay rate in AMYG learning rule. In the
proposed model, the emotional states including anxiety ? and confidence k have
been modeled only at the second hidden neuron of AMYG. We assume that all
AMYG?s neurons are involved in the forgetting process and only one AMYG?s
neuron includes anxiety and confidence states.
After updating the AMYG weights, OFC weights should be updated through similar
steps. The OFC must be adjusted to correct the AMYG response. So the error for
OFC backpropagation is (23)err=T?Ea+Eo and the OFC weights are updated as
follows;
(24)S2=?2×(fo2)?×err(25)w1,12=w1,12??×S2×(v1,12×fo1(?j=1n(wj,11×pj)+bo11))(26)w2,12=w2,12??×S2×(
w2,12×fo1(?j=1n(wj,21×pj)+bo21))(27)bo22=bo22??×S2
and the rules for the first and second hidden neurons are as follows;
(28)wj,11=wj,11??×(fo1)?×w1,12×S2×pjfor
j=1?(n)(29)bo11=bo11??×(fo1)?×w1,12×S2(30)wj,21=wj,21??×(fo1)?×w2,12×S2×pjfor
j=1?(n)(31)bo21=bo21??×(fo1)?×w2,12×S2.
According to Eqs. (28), (29), (30), (31), OFC does not include emotional
states and decay mechanisms, because they have not been confirmed in the
neuropsychological literature. Table 2 summarizes the agreements of the
proposed model and the cognitive studies presented in Introduction section.
Table 2. The connection and functional level agreements about the limbic
system.
Comparison level| Neuropsychophysiological motivation| Model implementation
---|---|---
Connection level| The sensory cortex and thalamus?AMYG plastic connections|
Weighted connections v1,v2,?,vn+1
The sensory cortex?OFC plastic connections| Weighted connections w1,w2,?,wn
Functional level| AMYG| Forgetting mechanism| Decay rate in AMYG learning
Anxiety state| ? in Eqs. (21), (22)
Confident state| k in Eqs. (21), (22)
OFC| Inhibitory task| OFC learns by Step 2 of the algorithm to correct AMYG
output by Eq. (11)
Now let us generalize the above model to a multi-input/output architecture (n
is the number of inputs and m is the number of outputs). Fig. 6 shows the
result. In the proposed architecture, there are m OFC parts and m AMYG parts.
The proposed model is a multi-input/output architecture that can be learned by
following Anxious Confident Decayed Brain Emotional Learning Rules (ACDBEL).In the algorithm, the inputs are the random learning weights of AMYG and OFC.
And the notation (?)i belongs to the ith AMYG and OFC parts.
**_ACDBEL Algorithm_**
** _Step 1-AMYG Part_** ;
* ?
Take kth pattern-target sample pair
* ?
pn+1=meanj=1?n(pj)
* ?
For each output node i=1?m do the following step
**%_Use the following equations to calculate the output_**
EaEa=fa2((v1,12)i×fa1(?j=1n+1((vj,11)i×pj)+(ba11)i)+(v2,12)i×fa1(?j=1n+1((vj,21)i×pj)+(ba21)i)+(ba12)i)
* ?
Update input weight j of AMYG part i, for j=1?n+1
* **%_calculate the error and sensitivity on second layer_** err=T?EaS2=?2×(fa2)?×err.
* **%_Update the weights and bias of second
layer_**(v1,12)i=(v1,12)i??×S2×(v1,12)ifa1(?j=1n+1((vj,11)i×pj)+(ba11)i)(v2,12)i=(v2,12)i??×S2×(v2,12)ifa1(?
j=1n+1((vj,21)i×pj)+(ba21)i)(ba11)i=(ba21)i??×S2
* **%_calculate sensitivity on first neuron of first layer_** S1=(fa1)?×(v1,12)i×S2
* **%_Update the weights and bias connected to first neuron of first
layer_**(vj,11)i=(vj,11)i??×?×S1×pj+k×(?vj,11)i(ba11)i=(ba11)i??×?×S1+k×(?ba11)i
* **%_calculate sensitivity on the second neuron of first layer_** S1=(fa1)?×(v2,12)i×S2.
* **%_Update the weights and bias of second neuron of first
layer_**(vj,21)i=(vj,21)i??×S1×pj(ba21)i=(ba21)i??×S1
* ?
If k< number of training patterns then k=k+l and proceed to the first.
* ?
Let epoch=epoch+1 and k=1.
* **%_Update the anxiety and confidence coefficients_** ?=YAvPAT+errk=1??.
* **%_Update learning weight_** ?
* ?
if (current_performance/previous_perf)>1.04
?=?×0.7
else ?=?×1.05 end.
* ?
If the stop criterion has not satisfied proceed to the first.
**_Step 2-OFC Part_** ;
* ?
Take kth pattern-target sample pair
* ?
For each output node i=1..m do the following step
* **%_Use the following equations to calculate the outputs_**
Ea=fa2((v1,12)i×fa1(?j=1n+1((vj,11)i×pj)+(ba11)i)+(v2,12)i×fa1(?j=1n+1((vj,21)i×pj)+(ba21)i)+(ba12)i)Eo=fo2(
(w1,12)i×fo1(?j=1n((wj,11)i×pj)+(bo11)i)+(w2,12)i×fo1(?j=1n((wj,21)i×pj)+(bo21)i)+(bo12)i)
* ?
Update input weight j of OFC part i, for j=1?n
* **%_calculate the error and sensitivity on second layer_** err=T?Ea+EoS2=?2×(fa2)?×err.
* **%_Update the weights and
bias_**(w1,12)i=(w1,12)i??×S2×(w1,12)ifo1(?j=1n((wj,11)i×pj)+(bo11)i)(w2,12)i=(w2,12)i??×S2×(w2,12)ifo1(?
j=1n((wj,21)i×pj)+(bo21)i)(bo22)i=(bo22)i??×S2
* **%_calculate sensitivity on the first layer and_**
* **%_Update the weights and bias of first
layer_**(wj,11)i=(wj,11)i??×(fo1)?×(w1,12)i×S2×pj(wj,21)i=(wj,21)i??×(fo1)?×(w2,12)i×S2×pj(bo11)i=(bo11)i?
?×(fo1)?×(w1,12)i×S2(bo21)i=(bo21)i??×(fo1)?×(w2,12)i×S2.* ?
If k< number of training patterns then k=k+l and proceed to the first.
* ?
Let epoch=epoch+1 and k=1
* **%_Update learning weight_** ?
* ?
if (current_performance/previous_perf)>1.04
?=?×0.7
else ?=?×1.05 end.
* ?
If the stop criterion has not satisfied proceed to the start of Step 2.
In the ACDBEL algorithm, the learning weights ? and ? are updated at the end
of each iteration adaptively. See ?%_Update learning weight_ ?? in the
algorithm. According to this step, if performance is increased then the
learning rate should be decreased. As mentioned in our previous work (Lotfi &
Akbarzadeh-T, 2014), this adaptation may increase the general performance of
the model. The LiAENN network is trained by the ACDBEL algorithm which is
neuro-psychologically motivated and can be used in classification and
prediction of problems. Although from a biological point of view, the AMYG is
responsible for emotional stimuli, we can apply its artificial model LiAENN to
make a response for any input patterns. The architecture presented in Fig. 5
can be used for single class classification and prediction, and the multi
output architecture presented in Fig. 6 can be used for multi class
classification problems.
## 4\. Experimental studies
A toolbox of proposed algorithms has been prepared and is accessible at
http://bitools.ir/projects.html. This toolbox has been written and evaluated
on Matlab2010b. Our aim in this section is to assess the application of the
proposed method in facial detection and emotion recognition, and compare it
with other applied emotional networks such as EmBP and BEL networks,
respectively presented in Sections 4.1 Comparative studies with EmBP networks,
4.2 Comparative studies with BEL networks.
### 4.1. Comparative studies with EmBP networks
The changes in the facial features complicate the face recognition task and
researchers have tried to provide methods capable of recognizing human faces.
DuoNN has been applied to recognize a person upon presenting his/her facial
image (Khashman, 2010). According to the results, the emotional networks
present better results than conventional networks. The adopted test bench was
?ORL Database of Faces? that is accessible at
http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html. As
illustrated in Fig. 7, the ORL database includes ten different images of 40
people with different genders, ethnicities, and ages. Here we compare our
method with DuoNN and EmBP on ORL dataset. All testing conditions are the same
as reported by Khashman (2010). For example, the image size is 10 × 10 pixels,
the patterns size is 100, the training samples number is 200, the testing
samples number is 200 and the initial weights are randomly selected between
[?0.30.3]. Table 3 shows the parameters used in the learning phase.
1. Download: Download full-size image
Fig. 7. Examples of ORL images.
From Khashman (2010).
Table 3. The learning parameters used in ORL face recognition using DuoNN,
EmBP and LiAENN.
Parameter| DuoNN and EmBP| LiAENN
---|---|---Input neurons| 100| 100
Hidden neurons| 80| 80
Output neurons| 40| 40
Learning coefficients| 0.0059| 0.0059
Random initial weights range| ?0.3 to +0.3| ?0.3 to +0.3
Convergence error| 0.007| 0.007
The stopping criterion in learning process is to reach a certain error that is
0.007. Fig. 8 presents the error of the first 10 learning epochs for three
methods. According to Fig. 8, the weights of the proposed method rapidly
converge during the first 2 epochs. Convergence is smooth for DuoNN and it is
with a damp for EmBP. After the first 2 epochs, the error slightly changed for
the proposed LiAENN.
1. Download: Download full-size image
Fig. 8. The error during first 10 learning epochs of (top) EmBP, from Khashman
(2010), (middle) DuoNN, from Khashman (2010), (bottom) proposed LiAENN.
Table 4 illustrates the comparative results between the proposed LiAENN, EmBP
and DuoNN. Table 4 clearly presents the high ability of LiAENN with respect to
the other methods. According to Table 4, LiAENN just needs 1879 learning
epochs while EmBP and DuoNN need many more epochs (Table 4; 16 307 and 5030
epochs respectively) to reach the error 0.007. The overall correct recognition
percentage of LiAENN is 99.50% while it is 84.50% for DouNN and 72.75% for
EmBP. Additionally, LiAENN can reach its highest confidence after learning.
LiAENN presents a significant improvement in term of accuracy and time
complexity.
Table 4. The comparative results of ORL face detection using three methods
with stop criterion error = 0.007.
Model| EmBPa| DuoNNa| LiAENN
---|---|---|---
Anxiety coefficient| 0.011425| 0.011423| 0
Confidence coefficient| 0.461276| 0.516024| 1
Iterations| 16 307| 5030| 1879
Correct recognition percentage (training)| 75%| 89.5%| 100%
Correct recognition percentage (testing)| 70.5%| 79.5%| 99.5%
Correct recognition percentage (overall)| 72.75%| 84.5%| 99.5%
a
From Khashman (2010).
### 4.2. Comparative studies with BEL networks
In order to investigate the role of anxiety and confidence coefficients, the
proposed ACDBEL can be compared with a BEL network such as BELPR. The
structure of ACDBEL and BELPR are similar and what differentiates them is
applying the emotional coefficients while the ACDBEL is closer to biological
features. ACDBEL profits from both the emotional and decaying coefficients
while BELPR just uses decaying coefficient. Here we utilize LiAENN to classify
the Yale dataset and compare it with the model free BELPR. The dataset
contains 165 grayscale images in GIF format of 15 individuals. There are 11
images per subject, one per different facial expression or configuration:
center-light, w/glasses, happy, left-light, w/no glasses, normal, right-light,
sad, sleepy, surprised, and winking. Here, the first 8 images of each class
are used for training and the remaining images are used for testing. The
examples of Yale are presented in Fig. 9. The dataset includes 15 classes. The
targets should be encoded with binary numbers (i.e. 15 binary number for 15
classes) and the input patterns should be involved through one feature
extraction step.
1. Download: Download full-size imageFig. 9. Examples of Yale images.
In the literature, a dimensionality reduction step (Hussein Al-Arashi,
Ibrahim, & Azmin Suandi, 2014) has been applied to classify this dataset
(Hussein Al-Arashi et al., 2014, Yin et al., 2014). A well-known method for
the dimensionality reduction is principal component analysis (PCA; Chen and
Xu, 2014, Chen et al., 2013; Hussein Al-Arashi et al., 2014). PCA normally
transform the original feature space to a lower dimensional feature space and
can be considered as a preprocessing step. The PCA calculates the data
covariance matrix and then finds the eigenvalues and the eigenvectors of the
matrix. According to the PCA algorithm, the only terms corresponding to the K
largest eigenvalues are kept. Here we used PCA as a preprocessing step. The
resulting reduced feature vectors then can be considered for classification.
Here we used the first 100 features for classification. So the number of input
neurons is 100, the number of hidden neurons is 30 and the output neurons is
15. The learning and structural parameters of the methods are presented in
Table 5. The number of input neurons depends on the number of attributes and
the number of output neurons depends on the number of classes in each dataset.
All learning and testing conditions of ACDBEL and BELPR are the same. The
values ? and ? are set at 0.01 and the values ?=0. In the learning process,
the stop criterion is the maximum epoch (i.e. the maximum number of learning
epochs has been reached). The maximum and minimum values of the inputs have
been determined and the normalized data (between 0 and 1) have been used to
adjust the weights. Here, the training and the testing approaches are repeated
5 times and the average accuracy is recorded.
Table 5. The learning parameters used in Yale dataset classification using
BELPR and LiAENN.
Model| BELPR| LiAENN
---|---|---
Input neurons| 100| 100
Hidden neurons| 30| 30
Output neurons| 15| 15
Learning coefficients| 0.01| 0.01
Random initial weights range| ?0.3 to +0.3| ?0.3 to +0.3
Table 6 presents the average correct detection percentage obtained from BELPR
and LiAENN during 5 executions. The average results indicated in Table 6 are
based on the Student?s t-test with 95% confidence. It is obvious that ACDBEL
with anxiety and confidence rates can significantly improve the accuracy
during the learning epochs. By increasing the number of learning epochs, the
accuracy of ACDBEL is increased significantly. The higher accuracy in 10 000
and 15 000 epochs is obtained from ACDBEL. According to Table 6, ACDBEL shows
a higher accuracy than BELPR based on correct detection percentage.
Table 6. The accuracy of the Yale classification results obtained from BELPR
and LiAENN.
Epochs model| 10 000| 15 000
---|---|---
Empty Cell| BELPR| LiAENN| BELPR| LiAENN
Correct recognition percentage (training)| 84.5%| 98.7%| 86.4%| 98.5%
Correct recognition percentage (testing)| 63.7%| 66.2%| 63.9%| 80.0%
Correct recognition percentage (overall)| 72.1%| 80.0%| 74.2%| 95.1%
The mean squared error (MSE) of the results during learning epochs of ACDBEL
are presented in Fig. 10. The left side of Fig. 10 shows the performance
measure in the AMYG learning step of ACDBEL. The right side of Fig. 10 shows
the performance measure of OFC learning step of ACDBEL. As illustrated in the
figures, MSE<0.1 is obtained during the first 10 epochs of each two parts ofthe model and it is shown that these parts can effectively increase the
learning accuracy.
1. Download: Download full-size image
Fig. 10. The mean squared error during first 10 learning epochs of (left) AMYG
learning step (Step 1 in the proposed algorithm) and (right) OFC learning step
(Step 2 in the proposed algorithm).
Table 7 summarizes the percentage improvement of our method in two datasets
ORL and Yale. Our method improves the previous emotional model results. The
best detection accuracy of the ORL and Yale datasets are 79.50% (Table 4) and
63.94% (Table 6) respectively obtained from DuoNN and BELPR. ACDBEL improves
the recognition accuracy about 24.5% and 25.2% respectively. The percentage
improvement of our method is summarized in Table 7 which is calculated through
the following formulas: (32)Percentage improvement=100×proposed method
result?compared resultcompared result.
Table 7. Percentage improvement of the proposed model.
Problem| ORL| Yale
---|---|---
Compared best emotional model| DuoNN| BELPR
Average accuracy of the best published emotional model| 79.5%| 63.9%
Accuracy of our model| 99.5%| 80.0%
Percentage improvement| 24.5%| 25.2%
The increased accuracy of the proposed model, compared to DuoNN, is due to the
use of OFC?AMYG inhibitory structure along with emotional coefficients. The
OFC output corrects the final response and thus increases the accuracy of the
model. Also, the increased accuracy of the proposed model compared to BELPR is
because of incorporating the emotional coefficients of anxiety, something
which is neglected in BELPR. Although the emotional factors incorporated in
Eqs. (21), (22) are inspired by biological features but from the viewpoint of
artificial learning algorithms, they are a kind of variable learning
coefficient which performs dedicated learning speed adjustments. In other
words, they increase the learning speed of the new samples at the beginning of
the learning process and also draw the network?s attention to prior learnings
and reduce changes in the weights.
## 5\. Conclusions
A novel applied computational model of emotion named LiAENN is presented here.
The learning weights of LiAENN are adjusted by the proposed ACDBEL algorithm.
In contrast to BEL based networks, ACDBEL considers the emotional states and
in contrast to EmBP based networks, it incorporates the anatomical bases of
emotion. Actually, a common appraisal-anatomical modeling of emotion is
applied here to produce LiAENN architecture with ACDBEL learning algorithm.
The emotional states applied in the learning algorithm are anxiety and
confidence and the anatomical features which have been utilized in the
architecture are fast and imprecise paths in the emotional brain, inhibitory
task of OFC and forgetting process of AMYG. The toolbox of the model has been
written on Matlab2010b and is accessible at
http://www.bitools.ir/projects.html. In numerical studies, LiAENN was utilized
to recognize the facial datasets ORL and Yale. According to the results, the
performance of proposed method is higher than that of the EmBP based emotional
networks in facial detection. Furthermore, in the Yale classification problem,
LiAENN with ACDBEL learning algorithm is more accurate than BELPR. From
another point of view, the added anxiety and confidence states have in fact a
neuropsychological basis and yield better learning and reacting as illustrated
here. ACDBEL is based on neurophysiological aspect of the emotional brain and
also the appraisal situation of emotion, is model free and can be used inclassification, prediction and facial recognition problems. The proposed
method can adjust its own weights in an online manner and correct the wrong
answers simultaneously. Also, the learning rates ? and ? which were constants
in the previous BEL based algorithms are adaptively updated during the
learning process. The proposed method is general which can accommodate various
applications and it can be improved in many respects. In our future work, we
concentrate on the operators in this model. The neurophysiological study of
the brain indicates that there is a type of uncertainty in the behavioral and
physiological functions of emotion. The operators of the neural networks are
founded based on the lowest level of this uncertainty. Operators of addition,
subtraction and multiplications do not exist in the physiological behaviors of
the brain the way they are used in the models. We intend to incorporate this
uncertainty in our proposed model by the placement of the fuzzy operators
including t-norm and s-norm, so that the model acts more similarly to the
brain?s behavior. We hope that the resulting model will be more successful in
engineering and artificial intelligence applications.
Lastly, although the proposed model is inspired by the biological features of
the emotional brain and facial datasets were the focus of the assessments,
being model-free (a feature inherited from BELPR models) can greatly extend
its applications in areas such as prediction functions. This is something
which can be considered in future studies.
## Acknowledgments
The authors thank the reviewers for their excellent feedback on the paper.
Recommended articles"
190,192,Predicting long-term outcome of Internet-delivered cognitive behavior therapy for social anxiety disorder using fMRI and support vector machine learning,"['KNT Månsson', 'A Frick', 'CJ Boraxbekk']",2015,170,Karolinska Directed Emotional Faces,machine learning,"demonstrated that initial activations of the visual cortex, in response to emotional face  stimuli, predicted symptom improvement with CBT, and that brain measures vastly improved",No DOI,Translational …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4354352/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
191,193,Presentation and validation of the Radboud Faces Database,"['O Langner', 'R Dotsch', 'G Bijlstra']",2010,2954,"Affective Faces Database, Radboud Faces Database","classification, facial expression recognition","available Radboud Faces Database (RaFD), a face database containing Caucasian face   We asked participants to pick the emotion label that best fitted the shown facial expression.",No DOI,… and emotion,https://www.tandfonline.com/doi/full/10.1080/02699930903485076,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,
192,194,Pyramid with super resolution for in-the-wild facial expression recognition,"['TH Vo', 'GS Lee', 'HJ Yang', 'SH Kim']",2020,138,Expression in-the-Wild,"FER, facial expression recognition","Wang, “Video facial emotion recognition based on local enhanced motion history image and   Xia, “Bi-modality fusion for emotion recognition in the wild,” ICMI 2019 Proceedings of the",No DOI,IEEE Access,http://ieeexplore.ieee.org/document/9143068/,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
193,195,Real time emotion recognition from facial expressions using CNN architecture,"['MA Ozdemir', 'B Elagoz', 'A Alaybeyoglu']",2019,107,Karolinska Directed Emotional Faces,CNN,"In this study, we proposed CNN based LeNet architecture for facial expression recognition  to estimate emotion states of human. We merged 3 different datasets (KDEF, JAFFE and our",No DOI,2019 medical …,https://ieeexplore.ieee.org/document/8895215,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
194,196,Real-time face detection and lip feature extraction using field-programmable gate arrays,"['D Nguyen', 'D Halupka', 'P Aarabi']",2006,147,Toronto Face Database,classifier,This paper proposes a new face detection technique that utilizes a naive Bayes classifier  to detect faces in an image based on only image edge direction information. This technique,No DOI,IEEE Transactions on …,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ccb214f4273b8eb7e2c155beb89f9318c347d60f,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
195,197,Recognition of affect in the wild using deep neural networks,"['D Kollias', 'MA Nicolaou', 'I Kotsia']",2017,161,"Acted Facial Expressions In The Wild, Affective Faces Database, Expression in-the-Wild","CNN, classification, deep learning, neural network",(such as subjects reacting to an unexpected development in a  end-to-end deep CNN and  CNN-RNN architectures (Section 3)  that a major challenge in facial expression and emotion,No DOI,Proceedings of the …,https://ieeexplore.ieee.org/document/8014981,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
196,198,Recognition of facial expressions of emotion is related to their frequency in everyday life,"['MG Calvo', 'A Gutiérrez-García']",2014,134,Karolinska Directed Emotional Faces,facial expression recognition,"on facial expression recognition has consistently found that happy expressions are   Consistent with this, in the current study, nearly one-third (29 %) of all the observed emotional faces",No DOI,Journal of Nonverbal …,https://link.springer.com/article/10.1007/s10919-014-0191-3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
197,199,Recognition thresholds for static and dynamic emotional faces.,"['MG Calvo', 'P Avero', 'A Fernández-Martín', 'G Recio']",2016,113,Karolinska Directed Emotional Faces,"classification, classifier","To this end, we varied the degree of intensity of emotional expressions unfolding from a  neutral face, by means of graphics morphing software. The resulting face stimuli (photographs",No DOI,Emotion,https://pubmed.ncbi.nlm.nih.gov/27359222/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
198,200,Recognizing action units for facial expression analysis,"['YI Tian', 'T Kanade', 'JF Cohn']",2001,2384,Extended Cohn-Kanade,"classification, classifier",We classify each of the wrinkles into one of two states:  of seven subjects from the Cohn-Kanade  database. Of the 72  of 46 subjects from the CohnKanade database and tested on 50,No DOI,IEEE Transactions on pattern …,https://ieeexplore.ieee.org/document/908962,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
199,201,Recognizing facial expression: machine learning and application to spontaneous behavior,"['MS Bartlett', 'G Littlewort', 'M Frank']",2005,912,Extended Cohn-Kanade,machine learning,"We first report performance for generalization to novel subjects within the Cohn-Kanade and   Shown are 4 subjects from the Cohn-Kanade dataset posing disgust containing AU’s 4,7",No DOI,2005 IEEE Computer …,https://ieeexplore.ieee.org/document/1467492/1000,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
200,202,Recover canonical-view faces in the wild with deep neural networks,"['Z Zhu', 'P Luo', 'X Wang', 'X Tang']",2014,152,Expression in-the-Wild,neural network,"Thc first term in Equation (1) measures the face's symmetry, which is the difference between  the left half and the right half of the face, and the second term measures the rank of the face.",No DOI,arXiv preprint arXiv:1404.3543,https://arxiv.org/abs/1404.3543,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
201,203,Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild,"['S Li', 'W Deng', 'JP Du']",2017,1700,Expression in-the-Wild,"CNN, classification, classifier, deep learning, machine learning","For all we know, RAF-DB is the first database that contains compound expressions in the wild.  Our  So in this paper, we directly trained our deep learning system on the big enough self-",No DOI,… of the IEEE conference on computer …,https://ieeexplore.ieee.org/document/8099760,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
202,204,Robust facial expression recognition based on local directional pattern,"['T Jabid', 'MH Kabir', 'O Chae']",2010,448,Japanese Female Facial Expression,"classification, classifier, facial expression recognition, machine learning","Two well-known machine learning methods, template matching and support vector machine,  are used for classification using the Cohn-Kanade and Japanese female facial expression",No DOI,ETRI journal,https://onlinelibrary.wiley.com/doi/abs/10.4218/etrij.10.1510.0132,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
203,205,Shape analysis of local facial patches for 3D facial expression recognition,"['A Maalej', 'BB Amor', 'M Daoudi', 'A Srivastava']",2011,107,Binghamton University 3D Facial Expression,"classification, classifier, deep learning, facial expression recognition, machine learning, neural network",► We address the 3D facial expression recognition problem using Riemannian geometry.  ► We propose local shape  [12] at Binghamton University. It was designed for research on,No DOI,Pattern Recognition,https://www.sciencedirect.com/science/article/pii/S0031320311000756,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Shape analysis of local facial patches for 3D facial
expression recognition
Search 
## Outline
1. Abstract
2. 3. 4. Keywords
5. 1\. Introduction
6. 2\. Related work
7. 3\. Database description
8. 4\. 3D facial patches-based representation
9. 5\. Framework for 3D shape analysis
10. 6\. Feature vector generation for classification
11. 7\. Recognition experiments
12. 8\. Conclusions
13. References
14. Vitae
Show full outline
## Cited by (74)
## Figures (8)
1. 2. 3. 4. 5. 6.
Show 2 more figures
## Tables (3)
1. Table 1
2. Table 2
3. Table 3
## Pattern Recognition
Volume 44, Issue 8, August 2011, Pages 1581-1589
# Shape analysis of local facial patches for 3D facial expression recognition
Author links open overlay panelAhmed Maalej a b, Boulbaba Ben Amor a b,
Mohamed Daoudi a b, Anuj Srivastava c, Stefano Berretti d
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.patcog.2011.02.012Get rights and content
## Abstract
In this paper we address the problem of 3D facial expression recognition. We
propose a local geometric shape analysis of facial surfaces coupled with
machine learning techniques for expression classification. A computation of
the length of the geodesic path between corresponding patches, using aRiemannian framework, in a shape space provides a quantitative information
about their similarities. These measures are then used as inputs to several
classification methods. The experimental results demonstrate the effectiveness
of the proposed approach. Using multiboosting and support vector machines
(SVM) classifiers, we achieved 98.81% and 97.75% recognition average rates,
respectively, for recognition of the six prototypical facial expressions on
BU-3DFE database. A comparative study using the same experimental setting
shows that the suggested approach outperforms previous work.
### Highlights
? We address the 3D facial expression recognition problem using Riemannian
geometry. ? We propose local shape analysis of faces coupled with machine-
learning techniques. ? A comparative study shows that the suggested approach
outperforms previous work.
* Previous article in issue
* Next article in issue
## Keywords
3D facial expression classification
Shape analysis
Geodesic path
Multiboosting
SVM
## 1\. Introduction
In recent years, 3D facial expression recognition has received growing
attention. It has become an active research topic in computer vision and
pattern recognition community, impacting important applications in fields
related to human?machine interaction (e.g., interactive computer games) and
psychological research. Increasing attention has been given to 3D acquisition
systems due to the natural fascination induced by 3D objects visualization and
rendering. In addition 3D data have advantages over the 2D data, in that 3D
facial data have high resolution and convey valuable information that
overcomes the problem of pose/lighting variations and the detail concealment
of low resolution acquisition.
In this paper we present a novel approach for 3D identity-independent facial
expression recognition based on a local shape analysis. Unlike the identity
recognition task that has been the subject of many papers, only few works have
addressed 3D facial expression recognition. This could be explained through
the challenge imposed by the demanding security and surveillance requirements.
Besides, there has long been a shortage of publicly available 3D facial
expression databases that serve the researchers exploring 3D information to
understand human behaviors and emotions. The main task is to classify the
facial expression of a given 3D model, into one of the six prototypical
expressions, namely _Happiness_ , _Anger_ , _Fear_ , _Disgust_ , _Sadness_ and
_Surprise_. It is stated that these expressions are universal among human
ethnicity as described in [1], [2].
The remainder of this paper is organized as follows. First, a brief overview
of related work is presented in Section 2. In Section 3 we describe the
BU-3DFE database designed to explore 3D information and improve facial
expression recognition. In Section 4, we summarize the shape analysis
framework applied earlier for 3D curves matching by Joshi et al. [3], and
discuss its use to perform 3D patches analysis. This framework is further
expounded in Section 5, so as to define methods for shapes analysis and
matching. In Section 6 a description of the feature vector and used
classifiers is given. In Section 7, experiments and results of our approach
are reported, and the average recognition rate over 97% is achieved usingmachine-learning algorithms for the recognition of facial expressions such as
multiboosting and SVM. Finally, discussion and conclusion are given in Section
8.
## 2\. Related work
Facial expression recognition has been extensively studied over the past
decades especially in 2D domain (e.g., images and videos) resulting in a
valuable enhancement. Existing approaches that address facial expression
recognition can be divided into three categories: (1) _static_ versus
_dynamic_ ; (2) _global_ versus _local_ ; (3) _2D_ versus _3D_. Most of the
approaches are based on feature extraction/detection as a mean to represent
and understand facial expressions. Pantic and Rothkrantz [4] and Samal and
Iyengar [5] presented a survey where they explored and compared different
approaches that were proposed, since the mid 1970s,for facial expression
analysis from either static facial images or image sequences. Whitehill and
Omlin [6] investigated on the Local versus Global segmentation for facial
expression recognition. In particular, their study is based on the
classification of action units (AUs), defined in the well-known Facial Action
Coding System (FACS) manual by Ekman and Friesen [7], and designating the
elementary muscle movements involved in the bio-mechanical of facial
expressions. They reported, in their study on face images, that the local
expression analysis showed no consistent improvement in recognition accuracy
compared to the global analysis. As for 3D facial expression recognition, the
first work related to this issue was presented by Wang et al. [8]. They
proposed a novel geometric feature based facial expression descriptor, derived
from an estimation of primitive surface feature distribution.
A labeling scheme was associated with their extracted features, and they
constructed samples that have been used to train and test several classifiers.
They reported that the highest average recognition rate they obtained was 83%.
They evaluated their approach not only on frontal view facial expressions of
the BU-3DFE database, but they also tested its robustness to non-frontal
views. A second work was reported by Soyel and Demirel [9] on the same
database. They extracted six characteristic distances between 11 facial
landmarks, using Neural Network architecture that analysis the calculated
distances, they classified the BU-3DFE facial scans into seven facial
expressions including neutral expression. The average recognition rate they
achieved was 91.3%. Mpiperis et al. [10] proposed a joint 3D face and facial
expression recognition using bilinear model. They fitted both formulations,
using symmetric and asymmetric bilinear models to encode both identity and
expression. They reported an average recognition rate of 90.5%. They also
reported that the facial expressions of disgust and surprise were well
identified with an accuracy of 100%. Tang and Huang [11] proposed an automatic
feature selection computed from the normalized Euclidean distances between two
picked landmarks from 83 possible ones. Using regularized multi-class Adaboost
classification algorithm, they reported an average recognition rate of 95.1%,
and they mentioned that the surprise expression was recognized with an
accuracy of 99.2%.
In this paper, we further investigate the problem of 3D identity-independent
facial expression recognition. The main contributions of our approach are the
following: (1) We propose a new process for representing and extracting
patches on the facial surface scan that cover multiple regions of the face and
(2) we apply a framework to derive 3D shape analysis to quantify similarity
measure between corresponding patches on different 3D facial scans. Thus, we
combine a local geometric-based shape analysis approach of 3D faces and
several machine-learning techniques to perform such classification.## 3\. Database description
BU-3DFE is one of the very few publicly available databases of annotated 3D
facial expressions, collected by Yin et al. [12] at Binghamton University. It
was designed for research on 3D human face and facial expression and to
develop a general understanding of the human behavior. Thus the BU-3DFE
database is beneficial for several fields and applications dealing with human
computer interaction, security, communication, psychology, etc. There are a
total of 100 subjects in the database, 56 females and 44 males. A neutral scan
was captured for each subject, then they were asked to perform six expressions
namely: Happiness (HA), Anger (AN), Fear (FE), Disgust (DI), Sad (SA) and
Surprise (SU). The expressions vary according to four levels of intensity
(low, middle, high and highest or 01?04). Thus, there are 25 3D facial
expression models per subject in the database. A set of 83 manually annotated
facial landmarks is associated to each model. These landmarks are used to
define the regions of the face that undergo specific deformations due to
single muscles movements when conveying facial expression [7]. In Fig. 1, we
illustrate examples of the six universal facial expressions 3D models
including the highest intensity level.
1. Download: Download high-res image (536KB)
2. Download: Download full-size image
Fig. 1. Examples of 3D facial expression models (first row 3D shape models,
second row 3D textured models) of the BU-3DFE database.
## 4\. 3D facial patches-based representation
Most of the earlier work in 3D shape analysis use shape descriptors such as
curvature, crest lines, shape index (e.g., ridge, saddle, rut, dome, etc.).
These descriptors are defined based on the geometric and topological
properties of the 3D object, and are used as features to simplify the
representation and thus the comparison for 3D shape matching and recognition
tasks. Despite their rigorous definition, such features are computed based on
numerical approximation that involves second derivatives and can be sensitive
to noisy data. In case of 3D facial range models, the facial surface labeling
is a critical step to describe the facial behavior or expression, and a robust
facial surface representation is needed. In Samir et al. [13] the authors
proposed to represent facial surfaces by an indexed collections of 3D closed
curves on faces. These curves are level curves of a surface distance function
(i.e., geodesic distance) defined to be the length of the shortest path
between a fixed reference point (taken to be the nose tip) and a point of the
extracted curve along the facial surface. This being motivated by the
robustness of the geodesic distance to facial expressions and rigid motions.
Using this approach they were able to compare 3D shapes by comparing facial
curves rather than comparing corresponding shape descriptors.
In our work we intend to further investigate on local shapes of the facial
surface. We are especially interested in capturing deformations of local
facial regions caused by facial expressions. Using a different solution, we
compute curves using the Euclidean distance which is sensitive to deformations
and thus can better capture differences related to variant expressions. To
this end, we choose to consider _N_ reference points (landmarks) {rl}1?l?N
(Fig. 2(a)) and associated sets of level curves {c?l}1????0 (Fig. 2(b)). These
curves are extracted over the patches centered at these points. Here ? stands
for the value of the distance function between the reference point _r_ _l_ and
the point belonging to the curve c?l, and ?0 stands for the maximum value
taken by ?. Accompanying each facial model there are 83 manually picked
landmarks, these landmarks are practically similar to the MPEG-4 feature
points and are selected based on the facial anatomy structure. Given thesepoints the feature region on the face can be easily determined and extracted.
We were interested in a subset of 68 landmarks laying within the face area,
discarding those marked on the face border. Contrary to the MPEG-4 feature
points specification that annotates the cheeks center and bone, in BU-3DFE
there were no landmarks associated with the cheek regions. Thus, we add two
extra landmarks at both cheeks, obtained by extracting the middle point along
the geodesic path between the mouth corner and the outside eye corner.
1. Download: Download high-res image (1MB)
2. Download: Download full-size image
Fig. 2. (a) 3D annotated facial shape model (70 landmarks); (b) 3D closed
curves extracted around the landmarks; (c) 3D curve-based patches composed of
20 level curves with a size fixed by a radius ?=20 mm; (d) extracted patches
on the face.
We propose to represent each facial scan by a number of patches centered on
the considered points. Let _r_ _l_ be the reference point and _P_ _l_ a given
patch centered on this point and localized on the facial surface denoted by
_S_. Each patch will be represented by an indexed collection of level curves.
To extract these curves, we use the Euclidean distance function ?rl?p? to
characterize the length between _r_ _l_ and any point _p_ on _S_. Using this
function we defined the curves as level sets
of(1)?rl?.?:c?l={p?S|?rl?p?=?}?S,??[0,?0].Each c?l is a closed curve,
consisting of a collection of points situated at an equal distance ? from _r_
_l_. Fig. 2 resumes the scheme of patches extraction.
## 5\. Framework for 3D shape analysis
Once the patches are extracted, we aim at studying their shape and design a
similarity measure between corresponding ones on different scans under
different expressions. This is motivated by the common belief that people
smile, or convey any other expression, the same way, or more appropriately
certain regions taking part in a specific expression undergo practically the
same dynamical deformation process. We expect that certain corresponding
patches associated with the same given expression will be deformed in a
similar way, while those associated with two different expressions will deform
differently. The following sections describe the shape analysis of closed
curves in R3, initially introduced by Joshi et al. [3], and its extension to
analyze shape of local patches on facial surfaces.
### 5.1. 3D curve shape analysis
We start by considering a closed curve ? in R3. While there are several ways
to analyze shapes of closed curves, an elastic analysis of the parametrized
curves is particularly appropriate in 3D curves analysis. This is because (1)
such analysis uses a square-root velocity function representation which allows
us to compare local facial shapes in presence of elastic deformations, (2)
this method uses a square-root representation under which the elastic metric
reduces to the standard L2 metric and thus simplifies the analysis, (3) under
this metric the Riemannian distance between curves is invariant to the re-
parametrization. To analyze the shape of ?, we shall represent it
mathematically using a square-root representation of ? as follows; for an
interval _I_ =[0,1], let ?:I?R3 be a curve and define q:I?R3 to be its square-
root velocity function (SRVF), given by:(2)q(t)???(t)???(t)?.
Here _t_ is a parameter ?I and ?.? is the Euclidean norm in R3. We note that
_q_(_t_) is a special function that captures the shape of ? and is
particularly convenient for shape analysis, as we describe next. The classical
elastic metric for comparing shapes of curves becomes the ?L2?metric under the
SRVF representation [14]. This point is very important as it simplifies the
calculus of elastic metric to the well-known calculus of functional analysisunder the ?L2?metric. Also, the squared L2-norm of _q_ , given by:
?q?2=?S1<q(t),q(t)>dt=?S1???(t)?dt, which is the length of ?. In order to
restrict our shape analysis to closed curves, we define the set:
C={q:S1?R3|?S1q(t)?q(t)?dt=0}?L2(S1,R3). Notice that the elements of C are
allowed to have different lengths. Due to a non-linear (closure) constraint on
its elements, C is a non-linear manifold. We can make it a Riemannian manifold
by using the metric: for any u,v?Tq(C), we define:(3)?u,v?=?S1?u(t),v(t)?dt.
So far we have described a set of closed curves and have endowed it with a
Riemannian structure. Next we consider the issue of representing the _shapes_
of these curves. It is easy to see that several elements of C can represent
curves with the same shape. For example, if we rotate a curve in R3, we get a
different SRVF but its shape remains unchanged. Another similar situation
arises when a curve is re-parametrized; a re-parameterization changes the SRVF
of curve but not its shape. In order to handle this variability, we define
orbits of the rotation group _SO_(3) and the re-parameterization group ? as
the equivalence classes in C. Here, ? is the set of all orientation-preserving
diffeomorphisms of S1 (to itself) and the elements of ? are viewed as re-
parameterization functions. For example, for a curve ?:S1?R3 and a function
?:S1?S1, ???, the curve ??? is a re-parameterization of ?. The corresponding
SRVF changes according to q(t)???(t)q(?(t)). We set the elements of the
orbit:(4)[q]={??(t)Oq(?(t))|O?SO(3),???},to be equivalent from the perspective
of shape analysis. The set of such equivalence classes, denoted by
S?C/(SO(3)×?) is called the _shape space_ of closed curves in R3. S inherits a
Riemannian metric from the larger space C due to the quotient structure.
The main ingredient in comparing and analysing shapes of curves is the
construction of a geodesic between any two elements of S, under the Riemannian
metric given in Eq. (3). Given any two curves ?1 and ?2, represented by their
SRVFs _q_ 1 and _q_ 2, we want to compute a geodesic path between the orbits
[_q_ 1] and [_q_ 2] in the shape space S. This task is accomplished using a
_path-straightening approach_ which was introduced in [15]. The basic idea
here is to connect the two points [_q_ 1] and [_q_ 2] by an arbitrary initial
path ? and to iteratively update this path using the negative gradient of an
energy function E[?]=12?s???(s),??(s)?ds. The interesting part is that the
gradient of _E_ has been derived analytically and can be used directly for
updating ?. As shown in [15], the critical points of _E_ are actually geodesic
paths in S. Thus, this gradient-based update leads to a critical point of _E_
which, in turn, is a geodesic path between the given points. In the remainder
of the paper, we will use the notation dS(?1,?2) to denote the length of the
geodesic in the _shape space_ S between the orbits _q_ 1 and _q_ 2, to reduce
the notation.
### 5.2. 3D patches shape analysis
Now, we extend ideas developed in the previous section from analyzing shapes
of curves to the shapes of patches. As mentioned earlier, we are going to
represent a number of _l_ patches of a facial surface _S_ with an indexed
collection of the level curves of the ?rl?.? function (Euclidean distance from
the reference point _r_ _l_). That is, Pl?{c?l,??[0,?0]}, where c?l is the
level set associated with ?rl?.?=?. Through this relation, each patch has been
represented as an element of the set S[0,?0]. In our framework, the shapes of
any two patches are compared by comparing their corresponding level curves.
Given any two patches _P_ 1 and _P_ 2, and their level curves {c?1,??[0,?0]}
and {c?2,??[0,?0]}, respectively, our idea is to compare the patches curves
c?1 and c?2, and to accumulate these differences over all ?. More formally, we
define a distance dS[0,?0] given by:(5)dS[0,?0](P1,P2)=?0LdS(c?1,c?2)d?.
In addition to the distance dS[0,?0](P1,P2), which is useful in biometry andother classification experiments, we also have a geodesic path in S[0,?0]
between the two points represented by _P_ 1 and _P_ 2. This geodesic
corresponds to the optimal elastic deformations of facial curves and, thus,
facial surfaces from one to another. Fig. 3 shows some examples of geodesic
paths that are computed between corresponding patches associated with shape
models sharing the same expression, and termed _intra-class geodesics_. In the
first column we illustrate the source, which represents scan models of the
same subject, but under different expressions. The third column represents the
targets as scan models of different subjects. As for the middle column, it
shows the geodesic paths. In each row we have both the shape and the mean
curvature mapping representations of the patches along the geodesic path from
the source to the target. The mean curvature representation is added to
identify concave/convex areas on the source and target patches and equally
spaced steps of geodesics. This figure shows that certain patches, belonging
to the same class of expression, are deformed in a similar way. In contrast,
Fig. 4 shows geodesic paths between patches of different facial expressions.
These geodesics are termed _inter-class geodesics_. Unlike the intra-class
geodesics shown in Fig. 3, these patches deform in a different way.
1. Download: Download high-res image (2MB)
2. Download: Download full-size image
Fig. 3. Examples of intra-class (same expression) geodesic paths with shape
and mean curvature mapping between corresponding patches.
1. Download: Download high-res image (823KB)
2. Download: Download full-size image
Fig. 4. Examples of inter-class (different expressions) geodesic paths between
source and target patches.
## 6\. Feature vector generation for classification
In order to classify expressions, we build a feature vector for each facial
scan. Given a candidate facial scan of a person _j_ , facial patches are
extracted around facial landmarks. For a facial patch _P_ _j_ _i_ , a set of
level curves {c?}ji are extracted centered on the _i_ th landmark. Similarly,
a patch _P_ _ref_ _i_ is extracted in correspondence to landmarks of a
reference scans _ref_. The length of the geodesic path between each level
curve and its corresponding curve on the reference scan is computed using a
Riemannian framework for shape analysis of 3D curves (see 5.1 3D curve shape
analysis, 5.2 3D patches shape analysis). The shortest path between two
patches at landmark _i_ , one in a candidate scan and the other in the
reference scan, is defined as the sum of the distances between all pairs of
corresponding curves in the two patches as indicated in Eq. (5). The feature
vector is then formed by the distances computed on all the patches and its
dimension is equal to the number of used landmarks _N_ =70 (i.e., 68 landmarks
are used out of the 83 provided by BU-3DFED and the two additional cheek
points). The _i_ th element of this vector represents the length of the
geodesic path that separates the relative patch to the corresponding one on
the reference face scan. All feature vectors computed on the overall dataset
will be labeled and used as input data to machine-learning algorithms such as
multiboosting and SVM, where multiboosting is an extension of the successful
Adaboost technique for forming decision committees.
## 7\. Recognition experiments
To investigate facial expression recognition, we have applied our proposed
approach on a dataset that is appropriate for this task. In this section, we
describe the experiments, obtained results and comparisons with related work.
### 7.1. Experimental setting
For the goal of performing identity-independent facial expression recognition,the experiments were conducted on the BU-3DFE static database. A dataset
captured from 60 subjects were used, half (30) of them were female and the
other half (30) were male, corresponding to the high and highest intensity
levels 3D expressive models (03?04). These data are assumed to be scaled to
the true physical dimensions of the captured human faces. Following a similar
setup as in [16], we randomly divided the 60 subjects into two sets, the
training set containing 54 subjects (648 samples), and the test set containing
six subjects (72 samples).
To drive the classification experiments, we arbitrarily choose a set of six
reference subjects with its six basic facial expressions. We point out that
the selected reference scans do not appear neither in the training nor in the
testing set. These references, shown in Fig. 5, with their relative expressive
scans corresponding to the highest intensity level, are taken to play the role
of representative models for each of the six classes of expressions. For each
reference subject, we derive a facial expression recognition experience.
1. Download: Download high-res image (156KB)
2. Download: Download full-size image
Fig. 5. Different facial expression average recognition rates obtained using
different reference subjects (using multiboost-LDA).
### 7.2. Discussion of the results
Several facial expression recognition experiments were conducted with changing
at each time the reference. Fig. 5 illustrates the selected references
(neutral scan). Using the _Waikato Environment for Knowledge Analysis (Weka)_
[17], we applied the multiboost algorithm with three weak classifiers, namely,
Linear Discriminant Analysis (LDA), Naive Bayes (NB), and Nearest Neighbor
(NN), to the extracted features, and we achieved average recognition rates of
98.81%, 98.76% and 98.07%, respectively. We applied the SVM linear classifier
as well, and we achieved an average recognition rate of 97.75%. We summarize
the resulting recognition rates in Table 1.
Table 1. Classification results using local shape analysis and several
classifiers.
Classifier| Multiboost-LDA| Multiboost-NB| Multiboost-NN| SVM-Linear
---|---|---|---|---
Recognition rate| **98.81%**| 98.76%| 98.07%| 97.75%
We note that these rates are obtained by averaging the results of the 10
independent and arbitrarily run experiments (10-fold cross-validation) and
their respective recognition rate obtained using the multiboost-LDA
classifier. We note that different selections of the reference scans do not
affect significantly the recognition results and there is no large variations
in recognition rates values. The reported results represent the average over
the six runned experiments. The multiboost-LDA classifier achieves the highest
recognition rate and shows a better performance in terms of accuracy than the
other classifiers. This is mainly due to the capability of the LDA-based
classifier to transform the features into a more discriminative space and,
consequently, result in a better linear separation between facial expression
classes.
The average confusion matrix relative to the best performing classification
using multiboost-LDA is given in Table 2.
Table 2. Average confusion matrix given by multiboost-LDA classifier.
%| AN| DI| FE| HA| SA| SU
---|---|---|---|---|---|---
AN| **97.92**| 1.11| 0.14| 0.14| 0.69| 0.0
DI| 0.56| **99.16**| 0.14| 0.0| 0.14| 0.0
FE| 0.14| 0.14| **99.72**| 0.0| 0.0| 0.0HA| 0.56| 0.14| 0.0| **98.60**| 0.56| 0.14
SA| 0.28| 0.14| 0.0| 0.0| **99.30**| 0.28
SU| 0.14| 0.56| 0.0| 0.0| 1.11| **98.19**
In order to better understand and explain the results mentioned above, we
apply the multiboost algorithm on feature vectors built from distances between
patches for each class of expression. In this case, we consider these features
as weak classifiers. Then, we look at the early iterations of the multiboost
algorithm and the selected patches in each iteration.
Fig. 6 illustrates for each class of expression the most relevant patches.
Notice that, for example, for the Happy expression the selected patches are
localized in the lower part of the face, around the mouth and the chin. As for
the Surprise expression, we can see that most relevant patches are localized
around the eyebrows and the mouth region. It can be seen that patches selected
for each expression lie on facial muscles that contribute to this expression.
1. Download: Download high-res image (489KB)
2. Download: Download full-size image
Fig. 6. Selected patches at the early few iterations of multiboost classifier
for the six facial expressions (Angry, Disgust, Fear, Happy, Sadness,
Surprise) with their associated weights.
### 7.3. Comparison with related work
In Table 3 results of our approach are compared against those reported in
[11], [9], [8], on the same experimental setting (54 versus 6 subject
partitions) of the BU-3DFE database. The differences between approaches should
be noted: Tang et al. [11] performed automatic feature selection using
normalized Euclidean distances between 83 landmarks, Soyel et al. [9]
calculated six distances using a distribution of 11 landmarks, while Wang et
al. [8] derived curvature estimation by locally approximating the 3D surface
with a smooth polynomial function. In comparison, our approach capture the 3D
shape information of local facial patches to derive shape analysis. For
assessing how the results of their statistical analysis will generalize to an
independent dataset, in [8] a 20-fold cross-validation technique was used,
while in [11], [9] the authors used 10-fold cross-validation to validate their
approach.
Table 3. Comparison of this work with respect to previous work [11], [9], [8].
Cross-validation| This work| Tang et al. [11]| Soyel et al. [9]| Wang et al.
[8]
---|---|---|---|---
10-fold| **98.81%**| 95.1%| 91.3%| ?
20-fold| **92.75%**| ?| ?| 83.6%
### 7.4. Non-frontal view facial expression recognition
In real world situations, frontal view facial scans may not be always
available. Thus, non-frontal view facial expression recognition is a
challenging issue that needs to be treated. We were interested in evaluating
our approach on facial scan under large pose variations. By rotating the 3D
shape models in the _y_ -direction, we generate facial scans under six
different non-frontal views corresponding to 15?, 30?, 45?, 60?, 75? and 90?
rotation. We assume that shape information is unavailable for the occluded
facial regions due to the face pose. For each view, we perform facial patches
extraction around the visible landmarks in the given scan. In cases where a
landmark is occluded, or where the landmark is visible, but the region nearby
is partially occluded, we treat it as a missing data problem for all faces
sharing this view. In these cases, we are not able to compute the geodesic
path between corresponding patches. The corresponding entries in the distance
matrix are blank and we fill them using an imputation technique [18]. In ourexperiments we employed the mean imputation method, which consists of
replacing the missing values by the means of values already calculated in
frontal view scenario obtained from the training set. Let
dijk=dS[0,?0](Pik,Pjk) be the geodesic distance between the _k_ th patch
belonging to subjects _i_ and _j_ (i?j). In case of frontal view (_fv_), the
set of instances _X_ _i_ _fv_ relative to the subject _i_ need to be labeled
and is given by:
Xifv=di11?di1k?di1N?????dij1?dijk?dijN?????diJ1?diJk?diJNwhere _N_ is the
number of attributes. In case of non-frontal view (_nfv_), if an attribute _k_
is missing, we replace the _k_ th column vector in the distance matrix _X_ _i_
_nfv_ by the mean of geodesic distances computed in the frontal view case,
with respect to the _k_ th attribute and given by: mkfv=?j=1Jdijk/J, where _J_
is the total number of instances.
Xinfv=di11?mkfv?di1N?????dij1?mkfv?dijN?????diJ1?mkfv?diJNTo evaluate the
robustness of our approach in a context of non-frontal views, we derive a
view-independent facial expression recognition. Error recognition rates are
evaluated throughout different testing facial views using the four classifiers
trained only on frontal view facial scans. Fig. 7 shows the average error
rates of the four classification methods. The multiboost-LDA shows the best
performance for facial expression classification on the chosen database. From
the figure, it can be observed that the average error rates increase with the
rotation angle (values from 0° to 90° of rotation are considered), and the
multiboost-LDA is the best performing methods also in the case of pose
variations. As shown in this figure, recognition accuracy remains acceptable,
even only 50% of data (half face) are available when we rotate the 3D face by
45° in _y_ -direction.
1. Download: Download high-res image (172KB)
2. Download: Download full-size image
Fig. 7. The average error rates of six expressions with different choices of
views corresponding to the best reference and using different classifiers.
### 7.5. Sensitivity to landmarks mis-localization
It is known that the automatic 3D facial feature points detection is a
challenging problem. The most difficult task remains the localization of
points around the eyebrow regions, which appear to play an important role in
the expression of emotions. The effect of the mis-localization of the
landmarks has been addressed in a specific experiment. We considered the
eyebrow regions in that the points in these regions are expected to be the
most difficult to detect automatically. In these regions, we added noise to
the landmarks provided with the BU-3DFED. In particular, we added noise to the
position of the landmarks by moving them randomly in a region with a radius of
10 mm, as illustrated Fig. 8 by blue circles. Then we performed expression
recognition experiments with such noisy landmarks. The results are reported in
Fig. 8. It can be noted that with the multiboost-LDA algorithm the lower
decrease in the recognition rate is observed, and even with a recognition rate
equal to 85.64% the result still outperforms the one reported in Wang et al
[8].
1. Download: Download high-res image (285KB)
2. Download: Download full-size image
Fig. 8. Recognition experiment performed adding noise to the eyebrow landmarks
(random displacement). (For interpretation of the references to color in this
figure legend, the reader is referred to the web version of this article.)
## 8\. Conclusions
In this paper we presented a novel approach for identity-independent facial
expression recognition from 3D facial shapes. Our idea was to describe thechange in facial expression as a deformation in the vicinity of facial patches
in 3D shape scan. An automatic extraction of local curve-based patches within
the 3D facial surfaces was proposed. These patches were used as local shape
descriptors for facial expression representation. A Riemannian framework was
applied to compute the geodesic path between corresponding patches.
Qualitative (inter- and intrageodesic paths) and quantitative (geodesic
distances) measures of the geodesic path were explored to derive shape
analysis. The geodesic distances between patches were labeled with respect to
the six prototypical expressions and used as samples to train and test
machine-learning algorithms. Using multiboost algorithm for multi-class
classification, we achieved a 98.81% average recognition rate for six
prototypical facial expressions on the BU-3DFE database. We demonstrated the
robustness of the proposed method to pose variations. In fact, the obtained
recognition rate remains acceptable (over 93%) even half of the facial scan is
missed.
The major limitation of our approach is that the 68 landmarks we used to
define the facial patches were manually labeled. For our future work we are
interested in detecting and tracking facial feature points, as proposed in
[19], [20], for automatic 3D facial expression recognition.
Recommended articles"
204,206,Simultaneous facial feature tracking and facial expression recognition,"['Y Li', 'S Wang', 'Y Zhao', 'Q Ji']",2013,257,MMI Facial Expression,facial expression recognition,"the extended CohnKanade database and test on the MMI facial expression database [53].  Since most of the image sequences on the MMI database have only single AU active, we only",No DOI,IEEE Transactions on image …,https://pubmed.ncbi.nlm.nih.gov/23529088/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
205,207,Single sample face recognition via learning deep supervised autoencoders,"['S Gao', 'Y Zhang', 'K Jia', 'J Lu']",2015,265,Toronto Face Database,"deep learning, machine learning","based deep neural networks, and driven by the SSPP face  a supervised auto-encoder to  build the deep neural network.  the Toronto Face Database, which is a very large dataset to",No DOI,IEEE transactions on …,https://ieeexplore.ieee.org/document/7124463,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
206,208,Spatio-temporal convolutional features with nested LSTM for facial expression recognition,"['Z Yu', 'G Liu', 'Q Liu', 'J Deng']",2018,158,Oulu-CASIA,classification,"Oulu-CASIA: We also consider for experiments the Oulu-CASIA dataset, which is a little bit  more  This dataset is more difficult to classify than CK+. In the cases of disgust, fear, happiness",No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231218308634,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Spatio-temporal convolutional features with nested LSTM for
facial expression recognition
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Related work
6. 3\. Spatio-Temporal convolutional features with nested LSTM
7. 4\. Experiments and results
8. 5\. Conclusion
9. Acknowledgements
10. References
11. Vitae
Show full outline
## Cited by (118)
## Figures (10)
1. 2. 3. 4. 5. 6.
Show 4 more figures
## Tables (9)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Neurocomputing
Volume 317, 23 November 2018, Pages 50-57
# Spatio-temporal convolutional features with nested LSTM for facial
expression recognition
Author links open overlay panelZhenbo Yu a, Guangcan Liu a, Qingshan Liu a,
Jiankang Deng b
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2018.07.028Get rights and content
## Abstract
In this paper, we propose a novel end-to-end architecture termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM), which learns the
muti-level appearance features and temporal dynamics of facial expressions in
a joint fashion. More precisely, 3DCNN is used to extract spatio-temporal
convolutional features from the image sequences that represent facial
expressions, and the dynamics of expressions are modeled by Nested LSTM, which
is actually coupled by two sub-LSTMs, saying T-LSTM and C-LSTM. Namely, T-LSTM
is used to model the temporal dynamics of the spatio-temporal features in each
convolutional layer, and C-LSTM is adopted to integrate the outputs of all
T-LSTMs together so as to encode the multi-level features encoded in the
intermediate layers of the network. We conduct experiments on four benchmark
databases, CK+, Oulu-CASIA, MMI and BP4D, and the results show that the
proposed method achieves a performance superior to the state-of-the-art
methods.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression recognition
LSTM
3DCNN
Multi-level features
## 1\. Introduction
Facial Expression Recognition (FER) [1], in general, is to automatically group
various kinds of facial muscle motions into similar emotion categories purely
based on the visual information in images or videos. Due to its potentials in
a broad range of applications such as face recognition [2], [3], [4], [5],
face alignment [6], [7], [8], [9], [10], [11], [12], [13], [14] and human-
computer interface [15], FER has received extensive attentions in the
literatures, e.g., [16], [17], [18]. Essentially, facial expression is a
dynamic process consisting of multiple stages, mainly including neutral,
onset, apex and offset [19], so how to learn the dynamics of facial
expressions is a key issue in FER [20].
Early FER methods [21], [22] are often built upon some pre-defined features
such as the Gabor filters, haar-like features and Local Binary Patterns (LBP).
These methods may work well only on limited occasions, as the pre-defined
features are incapable of fitting well with the data from a wide range of
applications. To overcome this issue, it would be natural to consider the deep
learning methods such as Convolutional Neural Network (CNN) [23], which can
seamlessly integrate feature extraction and expression classification into a
unified procedure. Extensive experiments demonstrate that CNN achieves
substantial improvement in recognition accuracy over conventional methods
[16], [24], [25], [26], but most of CNN-based methods consider a video as a
collection of multiple static images, and thus they may not handle well the
dynamic nature of facial expressions.
In order to make better use of the features that capture the motion of facial
muscles, sequence-based methods [25], [27], [28], [29], which represent an
expression by a sequence of images with known time stamp, have emerged as a
preferable choice. To analyze sequential data, the deep learning community has
also established several tools, e.g., Recurrent Neural Network (RNN) [30],
Long Short-Term Memory (LSTM) [31], [32], [33], [34] and 3D Convolutional
Neural Network (3DCNN) [35]. Especially, the CNN-RNN (or CNN-LSTM) framework
attracts much attention [25], [27], [28], in which RNN (or LSTM) takes the
appearance features extracted by CNN over individual frames as inputs and
encodes the temporal dynamics for later use, because it can combine the
advantages of CNN and RNN to model both the appearance features and temporaldynamics simultaneously. Recently, researchers have investigated the framework
of 3DCNN-RNN (or 3DCNN-LSTM) [36], [37]. Unlike CNN, which only deals with 2D
inputs, 3DCNN takes image sequences as inputs and can therefore extract
directly the spatio-temporal features underlying the image sequences. Despite
of the considerable improvement attained with the help of deep learning in
recent few years, existing methods often use only the outputs of the last
fully-connected layer as features for classification, discarding much useful
information encoded in the intermediate layers of the network. As can be seen
from Fig. 1, early convolutional layers extract fine-grained details (e.g.,
local boundaries or illuminations) of faces, while later layers capture more
detailed information, e.g., the appearance patterns of mouths and eyes. It can
be seen that the features from all layers of 3DCNN indeed provide FER with a
hierarchical representation of multi-level features from fine to coarse. Such
a hierarchical representation, intuitively, would be more effective than the
features contained in the last layer only.
1. Download: Download high-res image (258KB)
2. Download: Download full-size image
Fig. 1. Visualization of the convolutional features extracted from different
layers of 3DCNN. The blue and red points correspond to the low and high
response values, respectively. The emotion label for the input image sequence
is surprise.
In this work, we propose an end-to-end FER method that can involve various
visual clues, including the multi-level appearance features and the temporal
dynamics of facial expressions. To this end, we propose a novel architecture
termed Spatio-Temporal Convolutional features with Nested LSTM (STC-NLSTM),
which is illustrated in Fig. 2. In general, our STC-NLSTM contains three major
components: (1) a 3DCNN module consisting of multiple convolutional layers,
(2) multiple temporal-LSTM (T-LSTM) modules each of which corresponds to one
layer of the 3DCNN, and (3) a convolutional-LSTM (C-LSTM) module that takes
the outputs of T-LSTMs as inputs1. Given a sequence of images that represent
an emotion class, first, the 3DCNN module extracts the spatio-temporal
convolutional features of the expression for later use. Second, T-LSMT takes
the spatio-temporal features as inputs and produces compact features that
encode the appearance features as well as the temporal dynamics. Third, C-LSTM
plays the role of integrating the outputs of all T-LSTMs together and encoding
the multi-level features contained in each convolutional layer. Finally, the
softmax classifier is used to categorize the given sequence into one of the
six basic emotion classes. In contrast to the existing sequence-based methods
[25], [28], our STC-NLSTM can utilize not only the appearance features as well
as the temporal dynamics of facial expressions, but also the multi-level
semantics encoded in the individual layers of the network, so as to attain
more reliable classification results. Experiments on CK+ [38], Oulu-CASIA
[39], MMI [19] and BP4D [40] show that the proposed STC-NLSTM is superior to
the state-of-the-art methods.
1. Download: Download high-res image (226KB)
2. Download: Download full-size image
Fig. 2. Architecture of the proposed STC-NLSTM, which consists of 3DCNN and
Nested LSTM, and which is coupled by temporal-LSTM (T-LSTM) and convolutional-
LSTM (C-LSTM). In the figure above, the term ?ST-Convs? standards for the
spatio-temporal convolutional features.
The rest of this paper is organized as follows. Section 2 provides a brief
survey for FER. Section 3 introduces the proposed STC-NLSTM method. Section 4
shows some empirical results and Section 5 concludes this paper.
## 2\. Related workDeep learning methods have exhibited superior performance for FER, showing
dramatic improvement in accuracy and robustness over the conventional methods
based on pre-defined features [16], [24], [25], [26], [28], [40], [41], [42],
[43]. According to how an expression is represented, existing methods can be
roughly divided into two categories: image-based and sequence-based methods.
In general, FER is a special pattern recognition problem, and thus the
techniques for generic classification can be naturally applied to FER. Yu et
al. [44] proposed a FER method that combines together an ensemble of multiple
CNNs by minimizing a mixture of the log likelihood loss and the hinge loss.
Kim et al. [45] devised a recognition framework by combining multiple CNNs to
form a hierarchical network, and they won the first place of EmotiW 2015, an
international competition of FER. Bargal et al. [41] established a hybrid
network that combines VGG16 [46] with Residual Network [47] to learn the
appearance features of expressions, and they used SVM to produce the final
classification results. Yao et al. [48] proposed a deeper and wider network
consisting of three inception modules, and Zhao et al. [26] built a novel
peak-piloted feature transformation network to capture the intrinsic
correlations between the peak and weak expressions.
Different from the image-based methods, the sequence-based methods attempted
to well capture the temporal variations of the appearance features, which are
better for FER. Liu et al. [49] proposed a FER method termed 3DCNN-DAP (DAP
standards for deformable action part), in which 3DCNN is used to extract the
spatio-temporal features and the strong spatial structural constraints among
the dynamic action parts as well. Jung et al. [16] studied an integrated
network with joint fine-tuning to infer the appearance features and temporal
dynamics of facial expressions. Jaiswal et al [25] utilized CNN in combination
with Bi-directional LSTM (BiLSTM) for FER, achieving a performance better than
the winner of FERA 2015 [40]. Fan et al. [28] established a novel hybrid
network that combines 3DCNN and RNN in a late-fusion fashion and won the first
place in EmotoW2016 [42]. The proposed STC-NLSTM method belongs to the
sequence-based methods. Comparing to the previous works, our STC-NLSTM
provides a fine grained approach for modeling multi-level features encoded in
the intermediate layers of the network so as to achieve more accurate FER.
## 3\. Spatio-Temporal convolutional features with nested LSTM
This section details the proposed STC-NLSTM. As shown in Fig. 2, our STC-NLSTM
has three main components: (1) a 3DCNN module is used to extracted the spatio-
temporal convolutional features (ST-Convs) of facial expressions, (2) multiple
T-LSTM modules are adopted to capture the temporal dynamics of facial muscle
motions, and (3) a C-LSTM module aims to seize the multi-level features
encoded in the individual layers of the 3DCNN.
### 3.1. Spatio-temporal convolutional features
Since facial expression is essentially a dynamic process, we attempt to
extract directly the spatio-temporal features of facial expressions by a more
straightforward approach, that is, the well recognized 3DCNN, which has been
widely used in the fields of activity recognition, lip reading recognition,
gesture recognition, and so on [35]. Different from the traditional CNN that
can only deal with 2D inputs, 3DCNN takes directly the image sequences as
inputs and can therefore capture literally the spatio-temporal features of
image sequences.
Fig. 3 illustrated the architecture of the 3DCNN. Given a sequence of images
with known time stamp that represent a facial emotion class, 3DCNN processes
the sequence by multiple convolutional and pooling layers, producing a
collection of spatio-temporal features that characterize the expression.
1. Download: Download high-res image (110KB)2. Download: Download full-size image
Fig. 3. Architecture of the adopted 3DCNN.
### 3.2. Nested LSTM
To capture the multi-level features encoded in the intermediate layers of the
network, we propose the so-called Nested LSTM shown in Fig. 4, which is
composed of MSPP-norm, T-LSTM and C-LSTM. MSSP-norm aims to normalize the
spatio-temporal features of different sizes to the same dimension, while
T-LSTM and C-LSTM can capture the temporal dynamics and seize the multi-level
features encoded in the individual convolutional layers of the network
respectively.
1. Download: Download high-res image (291KB)
2. Download: Download full-size image
Fig. 4. Architecture of the proposed Nested LSTM.
#### 3.2.1. MSPP-norm
Because the spatio-temporal features extracted by different layers of 3DCNN
have different dimensions, it is impossible to input them directly to the LSTM
units, which generally require the inputs to have the same dimension. To fill
this gap, we design the Multi-dimensional Spatial Pyramid Pooling
normalization (MSPP-norm) operation, which is inspired by the Spatial Pyramid
Pooling network (SPP-net) proposed by He et al. [50]. The purpose of the MSSP-
norm is to normalize the spatio-temporal features of different sizes to the
same dimension. Given a 3D feature map of size _N_ × _a_ × _a_ , we
partition it into _N_ × _n_ × _n_ (with n=2,4,8) sub-regions and summarize
the responses within each sub-region via max pooling, resulting in a feature
vector with a fixed dimension determined by the parameter _n_.
#### 3.2.2. T-LSTM and C-LSTM
After the process of MSPP-norm, the spatio-temporal features in each layer of
3DCNN are transferred to feature vectors of the same dimension. Thus, it is
suitable to further analyze the spatio-temporal features by LSTM, which is an
advanced RNN architecture for sequential data analysis including in FER [27],
[51]. The commonly used LSTM can model the temporal information by
transforming a sequence of inputs to a sequence of outputs; this, in general,
can partially capture the correlations among the spatio-temporal features
extracted by 3DCNN. However, few conventional methods based on LSTM make full
use of the information encoded in all the convolutional layers, because it is
hard to involve all of the appearance features, temporal dynamics and multi-
level features by simply combining 3DCNN with LSTM.
To deal with the above issues, we adopt two LSTM modules, saying T-LSTM and
C-LSTM to cope with the spatio-temporal features extracted by 3DCNN. For each
feature vector corresponding to a certain convolution layer, a T-LSTM is
constructed by stacked LSTM units, which models the temporal dynamics of
facial expressions. After that, a C-LSTM is constructed to take the outputs of
T-LSTMs as inputs, so the desired multi-level features can be modeled in a
seamless way.
Suppose that there are in total _l_ convolutional layers in 3DCNN. Then the
procedure of our STC-NLSTM method can be summarized as follows:
fj=3DCNN(x),j=1,?,l,fjmspp=MSPP-
norm(fj),j=1,?,l,hj=T-LSTMj(fjmspp),j=1,?,l,h={h1,?,hl},o=C?LSTM(h),where _x_
denotes an image sequence, _f j_ is the 3D feature map produced by the _j_ th
convolutional layer of 3DCNN, _h j_ is the feature vector from the _j_ th
T-LSTM module, and _o_ denotes the final feature vector used for
classification.
## 4\. Experiments and results
### 4.1. Experimental dataTo verify the effectiveness of the proposed STC-NLSTM, we experiment with four
benchmark datasets, CK+ [38], Oulu-CASIA [39], MMI [19] and BP4D [40].
**CK+:** This dataset has six basic emotion classes, including anger(An),
disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and surprise(Su). In
addition, there is another special expression called ?contempt?. The dataset
contains in total 593 image sequences from 123 subjects, but only 309
sequences are annotated with the six basic expression labels. We divide these
309 sequences into 10 groups, of which 9 groups are used for training and the
rest for testing. In this way, we can run various FER methods multiple times
and obtain an averaged accuracy for evaluation.
_Oulu-CASIA_ : We also consider for experiments the Oulu-CASIA dataset, which
is a little bit more challenging than CK+. The dataset contains 480 image
sequences of six basic emotion classes (including An, Di, Fe, Ha, Sa and Su)
under normal illumination conditions. Each sequence begins with a neutral
expression and ends with the peak expression. The same as in CK+, a 10-fold
cross validation is performed to evaluate various FER methods.
_MMI_ : The third dataset used for experiments, MMI, is consist of 205 image
sequences of the six basic emotion classes. Unlike CK+ and Oulu-CASIA, in
which each sequence ends at the peak expression, the peak expressions in MMI
are located in the middle of the sequences. The location of the peak frame is
not provided as a prior information, which is usually the case for real-world
videos. To obtain unbiased evaluation results, we perform a 10-fold cross
validation in the same way as in CK+ and Oulu-CASIA.
_BP4D_ : The last dataset used for experiments, BP4D, is divided into a fixed
set of training, development and test data. In total, the training partition
contains 75,586 images, the development contains 71,261 images and the test
contains 75,726 images. Each of these images in BP4D are annotated with 11
Action Units. Unlike above datasets, BP4D contains large number of annotated
images which benefits deep learning algorithms and provides a good platform
for a fair evaluation due to a fixed training and test set.
Fig. 5 shows some examples sampled from the above four datasets, and Table 1
summarizes the number of sequences in each of the six emotion classes. To
better perform FER, we need to use several pre-processing techniques, mainly
including video normalization, face detection and data augmentation.
1. Download: Download high-res image (254KB)
2. Download: Download full-size image
Fig. 5. Examples of the images used in our experiments. Top: CK+; Middle:
Oulu-CASIA; Bottom: MMI.
Table 1. The number of image sequences in each of the six basic emotion
classes: anger(An),disgust(Di), fear(Fe), happiness(Ha), sadness(Sa) and
surprise(Su).
Empty Cell| An| Di| Fe| Ha| Sa| Su| All
---|---|---|---|---|---|---|---
CK+| 45| 59| 25| 69| 28| 83| 309
Oulu| 80| 80| 80| 80| 80| 80| 480
MMI| 32| 31| 28| 42| 32| 40| 205
_Video normalization_ : Since the length of the image sequences is variable,
but the dimension of the inputs for a neural network is usually fixed, the
normalization along the time axis is required as input for neural networks.
For the sequences in CK+ and Oulu-CASIA , which have respectively averaged
lengths of 18 and 22, we make each sequence into the average length via either
uniform sampling (for the sequences longer than the average) or replicating
the last frame (for the sequences shorter than the average). Regarding the MMI
dataset, which is based on video, we convert the videos into the imagesequences by uniformly selecting 10 frames per second, and normalize the
sequences into a fixed length of 22 in the same way as in CK+ and Oulu-CASIA.
_Face detection_ : We utilize the Multi-Task Cascaded Convolutional Network
(MTCCN) [52] to obtain the coordinates of two eyes at first, then determine
the final rectangular face by keeping the distances between two eyes
invariable. Finally, we turn a rectangle into a square through zero padding
and resize the square to 64 × 64 (see Fig. 6).
1. Download: Download high-res image (210KB)
2. Download: Download full-size image
Fig. 6. Some examples of the face detection results in CK+ (top), Oulu-CASIA
(middle) and MMI (bottom).
_Data augmentation_ : Facial expression datasets, e.g., CK+, Oulu-CASIA and
MMI, often contain only hundreds of image sequences. However, a typical deep
neural network has many parameters, and this will make a deep network prone to
overfitting. To handle this issue, we first flip each image sequence
horizontally so as to double the number of sequences. Then we rotate each
image by an angle in {?7.5°, ?5°, ?2.5°, 2.5°, 5°, 7.5°}, resulting in a new
dataset which is 14 times as big as the original one. Such a data augmentation
process can not only make the learnt model robust against the slight
rotational changes of the input images, but also broaden the number of
training samples so as to avoid overfitting.
### 4.2. Experimental data
To evaluate the performance of the proposed STC-NLSTM, we compare it with 12
prevalent FER methods, including 3DCNN-DAP [49], 3DSIFT [53], ARDfee [54],
CSPL [20], DTAGN [16], FN2EN [55], IDT+FV [56], LOmo [43], PPDN [26], DCPN
[57], STM-ExpLet [17] and ST-RBM [58].
In addition, to further investigate the effectiveness of the proposed Nested
LSTM, we design four baselines by amending the architecture of STC-NLSTM:
* \- _STC (i.e., 3DCNN)_ : This baseline is created by simply removing the T-LSTM and C-LSTM modules
from the architecture of STC-NLSTM, and the outputs of the last convolutional layer of 3DCNN are taken as
inputs to the softmax classifier so as to obtain the final classification results.
* \- _STC-LSTM_ : This baseline is constructed by replacing the T-LSTM and C-LSTM modules with a
traditional LSTM. Namely, the outputs of the last convolutional layer are taken as inputs to LSTM, which
produces the final feature vectors for classification.
* \- _STC-SLSTM_ : This baseline is similar to STC-LSTM. The only difference is that the outputs of all
convolutional layer are taken as inputs to LSTM by sum fusion, which computes the sum of two feature maps
at the same spatial locations and channels.
* \- _DenseNet_ : This baseline [59] is a simpler and more efficient network compared to Inception networks,
which also utilizes the middle latent representation. We compared the DenseNet to the other FER methods
under the standard setting [26], which uses the strong expressions in each sequence(e.g., the last one to
three frames) for training and testing. Because the DenseNet method is only based on the static image, we
train this baseline following to [57].
For the 12 previously proposed baselines, their results are directly quoted
from the original reports. For STC, STC-LSTM and STC-SLSTM, we obtain their
classification results using the same parametric configuration as STC-NLSTM.
As usual, we denote a deep network by a sequence of letters and numbers, e.g.,
I(64,64,22)-C(3,64)-BN-P2-FC18-S6, where I(64,64,22) means the 64 × 64 × 22
input image sequences, C(3,64) is a convolutional layer with 64 filters of 3 ×
3, BN standards for the operation of batch normalization, P2 is a 2 × 2 max
pooling layer, FC refers to a fully connected layer, and S6 denotes a softmax
layer with six outputs. For simplicity, the architecture of the 3DCNN used in
our STC-NLSTM is configured as
I(64,64,18)-C(3,64)-BN-P2-C(3,64)-BN-P2-C(3,64)-C(3,64)-P2-C(3,64)-FC18, and a
two-level LSTM architecture is used to construct the T-LSTM and C-LSTMmodules. The stride of each layer is 1 with the exception of the pooling
layer, which has a stride value of 2. Table 2 details the configurations of
the network architecture.
Table 2. Detailed configurations of the network for CK+.
Type| Patch size/stride| Input size
---|---|---
conv1| 3 × 3 × 3/1 × 1 × 1| 18 × 64 × 64 × 3
mspp1| [8,4,2]| 18 × 5376
pool1| 1 × 2 × 2/1 × 2 × 2| 18 × 32 × 32 × 64
conv2| 3 × 3 × 3/1 × 1 × 1| 18 × 32 × 32 × 64
mspp2| [8,4,2]| 18 × 5376
pool2| 1 × 2 × 2/1 × 2 × 2| 18 × 16 × 16 × 64
conv3| 3 × 3 × 3/1 × 1 × 1| 18 × 16 × 16 × 64
mspp3| [8,4,2]| 18 × 5376
conv4| 3 × 3 × 3/1 × 1 × 1| 18 × 16 × 16 × 64
mspp4| [8,4,2]| 18 × 5376
pool4| 1 × 2 × 2/1 × 2 × 2| 18 × 8 × 8 × 64
conv5| 3 × 3 × 3/1 × 1 × 1| 18 × 8 × 8 × 64
mspp5| [8,4,2]| 18 × 5376
Our model is implemented based on the TensorFlow library [60] and trained on
four GeForce Titan X (pascal) GPU with 12GB memory. The weights of the network
are initialized randomly using the ?xaiver? procedure [61]. We first set the
learning rate as 0.0025 and train the network until 300 iterations, and then
fine-tune the network by setting the learning rate to 0.000025 and running 200
iterations. In all the experiments, the weight decay parameter is consistently
set as 0.0015.
### 4.3. Experimental results
#### 4.3.1. Results on CK+
For fair comparison, we follow [20] to use 10-fold cross-validation and repeat
the procedure 4 times, resulting in 40 trials in total. Table 3 shows the
comparison results of various FER methods. Since the expressions in this
dataset are easy to classify, several methods obtain superior classification
results. In particular, as we can see from Table 4, our STC-NLSTM can achieve
an accuracy near 100%. It performs well in anger and surprise, but for sadness
, it is easy to be confused with disgust and fear. Fig. 7 compares STC-NLSTM
with STC, STC-LSTM and STC-SLSTM on each of the six basic emotion classes. It
can be seen that STC-NLSTM performs consistently better than STC, STC-LSTM and
STC-SLSTM, which confirms the effectiveness of our Nested LSTM.
Table 3. Classification accuracies on CK+. The numbers for STC, STC-LSTM, STC-
SLSTM and STC-NLSTM are averaged from 40 trails.
Method| Average accuracy
---|---
3DCNN-DAP [49]| 92.4
STM-ExpLet [17]| 94.2
LOmo [43]| 95.1
IDT+FV [56]| 95.8
FN2EN [55]| 96.9
DTAGN [16]| 97.3
ARDfee [54]| 98.7
PPDN [26]| 99.3
DCPN [57]| 99.6
STC| 98.9
STC-LSTM| 99.3
STC-SLSTM| 99.4DenseNet| 97.6
STC-NLSTM| **99.8(** ± **0.2)**
Table 4. Confusion matrix of STC-NLSTM for CK+. The labels in the leftmost and
topmost columns denote the ground truth and prediction results, respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 100| 0| 0| 0| 0| 0
Di| 0.15| 99.68| 0| 0| 0.17| 0
Fe| 0| 0| 99.71| 0| 0.29| 0
Ha| 0| 0| 0.11| 99.89| 0| 0
Sa| 0| 0.29| 0.57| 0| 99.14| 0
Su| 0| 0| 0| 0| 0| 100
1. Download: Download high-res image (202KB)
2. Download: Download full-size image
Fig. 7. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in CK+.
#### 4.3.2. Results on Oulu-CASIA
Table 5 shows the comparison results, and Table 6 gives the confusion matrix
produced by our STC-NLSTM method on the Oulu-CASIA dataset. This dataset is
more difficult to classify than CK+. In the cases of disgust, fear, happiness
and surprise, the performance is good, but the performance for anger and
sadness is slightly poor. As we can see, STC-NLSTM achieves an averaged
accuracy of 93.45%, which is 4.2% higher than the 89.6% accuracy produced by
the most close baseline, ARDfee. This illustrates the superiorities of STC-
NLSTM over the state-of-the-art FER methods. Fig. 8 shows that STC-NLSTM is
distinctly better than STC, STC-LSTM and STC-SLSTM on all expression classes
except the class ?happy?. It is shown that that FER can benefit a lot from the
modeling of the multi-level features encoded in each convolutional layer.
Table 5. Classification accuracies on Oulu-CASIA. The numbers for STC, STC-
LSTM, STC-SLSTM and STC-NLSTM are obtained by averaging the accuracies from 40
trails.
Method| Average accuracy
---|---
STM-ExpLet [17]| 74.59
DTAGN [16]| 81.64
LOmo [43]| 82.10
PPDN [26]| 84.59
DCPN [57]| 86.23
FN2EN [55]| 87.71
ARDfee [54]| 89.60
STC| 84.72
STC-LSTM| 88.98
STC-SLSTM| 90.12
DenseNet| 87.28
STC-NLSTM| **93.45(** ± **0.43)**
Table 6. Confusion matrix of STC-NLSTM for Oulu-CASIA. The labels in the
leftmost and topmost columns denote the ground truth and prediction results,
respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 89.82| 6.20| 0.75| 0| 3.23| 0
Di| 1.38| 95.20| 0.30| 0.95| 2.17| 0
Fe| 0| 0| 96.14| 0.50| 0.65| 2.71
Ha| 0| 0.90| 3.83| 94.78| 0.49| 0Sa| 4.4| 2.38| 0.56| 0| 92.66| 0
Su| 0| 0| 3.95| 0| 0| 96.05
1. Download: Download high-res image (188KB)
2. Download: Download full-size image
Fig. 8. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in Oulu-CASIA.
#### 4.3.3. Results on MMI
As shown in Table 7, the STC-NLSTM method can distinctly outperform previous
state-of-the-art methods on the MMI dataset. Table 8 shows the confusion
matrix produced by STC-NLSTM. Actually, the averaged accuracy by STC-NLSTM
reaches 84.53% (see Table 7), which is 2.9% better than ST-RBM, and which
archives the best performance among the previous reports. Fig. 9 shows that
STC-NLSTM is distinctly better than STC-SLSTM on all the emotion classes,
which is same as the results on the other three datasets.
Table 7. Classification accuracies on MMI. The numbers for STC, STC-LSTM, STC-
SLSTM and STC-NLSTM are averaged from 40 trails.
Method| Average accuracy
---|---
3DCNN-DAP [49]| 63.4
3DSIFT [53]| 64.39
DTAGN [16]| 70.24
CSPL [20]| 73.53
STM-ExpLet [17]| 75.12
ST-RBM [58]| 81.63
STC| 74.84
STC-LSTM| 80.39
STC-SLSTM| 81.92
DenseNet| 77.68
STC-NLSTM| **84.53(** ± **0.67)**
Table 8. Confusion matrix of STC-NLSTM for MMI. The labels in the leftmost and
topmost columns denote the ground truth and prediction results, respectively.
Empty Cell| An| Di| Fe| Ha| Sa| Su
---|---|---|---|---|---|---
An| 83.24| 9.04| 0| 5.36| 1.24| 1.12
Di| 6.72| 88.21| 0| 2.74| 2.33| 0
Fe| 4.34| 0| 81.24| 1.23| 1.56| 11.63
Ha| 3.62| 0| 3.16| 93.22| 0| 0
Sa| 1.55| 1.12| 9.18| 1.18| 85.77| 1.20
Su| 2.64| 0| 8.66| 3.41| 0| 85.29
1. Download: Download high-res image (184KB)
2. Download: Download full-size image
Fig. 9. Comparing STC-NLSTM with STC, STC-LSTM and STC-SLSTM on each of the
six emotion classes in MMI.
#### 4.3.4. Results on BP4D
Since the BP4D dataset has the training set and testing set, we do not need to
use 10-fold cross-validation. Different from the above three datasets, the
BP4D dataset is bigger than them. Table 9 shows the experimental results. It
can be seen that the STC-NLSTM obviously outperforms previous the state-of-
the-art methods. This result supports the conclusion that our STC-NLSTM can
also achieve the good performance on a large scale datasets.
Table 9. Performance (F1 scores) comparison on BP4D Test set.
Method| F1 Scores
---|---
LGBP [40]| 0.44GDNN [25]| 0.48
DLE [62]| 0.51
CNN+BLSTM [25]| 0.52
STC-NLSTM| **0.58**
#### 4.3.5. Influences of the number of layers
The above results illustrate that Nested LSTM plays a crucial role in our
proposed method. To be more clear, we shall investigate the influences of the
number of the convolutional layers contained in the architecture of STC-NLSTM.
Fig. 10 shows the results. It can be seen that the classification accuracy
gradually increases as the enlargement of the layer number, reaching the
maximum at 5 convolutional layers. Since the datasets are not large, the
performance drops while the number of layers exceeds 5. Regarding why the CK+
dataset is not so sensitive to the number of layers, the reason is that the
dataset is easy to classify (see Table 3).
1. Download: Download high-res image (146KB)
2. Download: Download full-size image
Fig. 10. Plotting the classification accuracy as a function of the number of
convolutional layers in STC-NLSTM.
## 5\. Conclusion
In this paper, we proposed a novel method termed STC-NLSTM for FER. Unlike
most of the existing deep learning based methods, which obtain the
classification results based on the outputs of the last fully-connected layer,
STC-NLSTM aims to taken into account the multi-level features encoded in the
intermediate layers of the network. To achieve this, the architecture of STC-
NLSTM is designed to involve three major components: 3DCNN, T-LSTMs and
C-LSTM. Each component is devised carefully to own a specific ability. The
3DCNN module plays the role of extracting the spatio-temporal convolutional
features of facial expressions. The T-LSTM modules take charge of capturing
the temporal dynamics that depict the facial appearance variations in temporal
domain, and the C-LSTM is responsible for seizing the multi-level features
encoded in the individual convolutional layers of the network. All the three
components are integrated into an end-to-end network so as to cooperate
seamlessly with each other. Experiments on four public datasets demonstrated
that STC-NLSTM is superior to the state-of-the-art methods.
## Acknowledgements
The work of Qingshan Liu is supported by National Natural Science Foundation
of China (NSFC) under Grant61532009. The work of Guangcan Liu is supported in
part by NSFC under grants 61622305 and 61502238, and in part by the Natural
Science Foundation of Jiangsu Province of China (NSFJPC) under Grant
BK20160040.
Recommended articles"
207,209,Spatio-temporal facial expression recognition using convolutional neural networks and conditional random fields,"['B Hasani', 'MH Mahoor']",2017,116,"MMI Facial Expression, Static Facial Expression in the Wild, Toronto Face Database","FER, facial expression recognition, neural network","a network for the task of facial expression recognition in videos. The first part of our network  is a Deep Neural Network  are used to evaluate the network: CK+, MMI, and FERA. We show",No DOI,… Conference on Automatic Face & …,https://ieeexplore.ieee.org/document/7961822,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
208,210,"Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark","['A Dhall', 'R Goecke', 'S Lucey']",2011,706,"Acted Facial Expressions In The Wild, Static Facial Expression in the Wild","FER, classification, classifier, facial expression recognition, machine learning","and testing protocol for expression recognition as part of the BEFIT work Acted Facial  Expressions in the Wild (AFEW) [9]. Therefore, we name it the Static Facial Expressions in the Wild (",No DOI,2011 IEEE international …,https://ieeexplore.ieee.org/document/6130508,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
209,211,Static topographic modeling for facial expression recognition and analysis,"['J Wang', 'L Yin']",2007,119,MMI Facial Expression,facial expression recognition,Maja Pantic’s group at Imperial College London for providing the MMI facial expression  database. This work is supported in part by the National Science Foundation under Grants IIS-,No DOI,Computer Vision and Image Understanding,https://www.sciencedirect.com/science/article/pii/S107731420600227X,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Static topographic modeling for facial expression recognition
and analysis
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Topographic analysis
6. 3\. Topographic context based feature extraction
7. 4\. Experiments on facial expression recognition
8. 5\. Separability analysis
9. 6\. Robustness analysis
10. 7\. Discussion
11. 8\. Concluding remarks
12. Acknowledgments
13. References
Show full outline
## Cited by (74)
## Figures (16)
1. 2. 3. 4. 5. 6.
Show 10 more figures
## Tables (7)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
Show all tables
## Computer Vision and Image Understanding
Volume 108, Issues 1?2, October?November 2007, Pages 19-34
# Static topographic modeling for facial expression recognition and analysis
Author links open overlay panelJun Wang, Lijun Yin
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.cviu.2006.10.011Get rights and content
## Abstract
Facial expression plays a key role in non-verbal face-to-face communication.It is a challenging task to develop an automatic facial expression reading and
understanding system, especially, for recognizing the facial expression from a
static image without any prior knowledge of the test subject. In this paper,
we present a topographic modeling approach to recognize and analyze facial
expression from single static images. The so-called topographic modeling is
developed based on a novel facial expression descriptor, Topographic Context
(TC), for representing and recognizing facial expressions. This proposed
approach applies topographic analysis that treats the image as a 3D surface
and labels each pixel by its terrain features. Topographic context captures
the distribution of terrain labels in the expressive regions of a face. It
characterizes the distinct facial expression while conserving abundant
expression information and disregarding most individual characteristics.
Experiments on person-dependent and person-independent facial expression
recognition using two public databases (MMI and Cohn?Kanade database) show
that TC is a good feature representation for recognizing basic prototypic
expressions. Furthermore, we conduct the separability analysis of TC-based
features by both a visualized dimensionality reduction example and a
theoretical estimation using certain separability criterion. For an in-depth
understanding of the recognition property of different expressions, the
between-expression discriminability is also quantitatively evaluated using the
separability criterion. Finally, we investigated the robustness of the
extracted TC-based expression features in two aspects: the robustness to the
distortion of detected face region and the robustness to different intensities
of facial expressions. The experimental results show that our system achieved
the best correct rate at 82.61% for the person-independent facial expression
recognition.
* Previous article in issue
* Next article in issue
## Keywords
Facial expression representation
Facial expression recognition
Topographic context
Separability analysis
## 1\. Introduction
Facial expression recognition and emotion analysis could help humanize
computers and robots. Due to the wide range of applications in human?computer
interaction, telecommunication, law enforcement and psychological research,
facial expression analysis has become an active research area. Generally, an
automatic facial expression reader consists of three major components: face
detection, facial expression representation and facial expression
classification [1], [2]. Although the face detection component is a crucial
part towards realizing an automatic expression recognition system, most
research currently concentrates on how to achieve a better facial expression
representation and feature extraction.
Facial expression representation concerns itself with the problem of facial
feature extraction for modeling the expression variations with a certain
accuracy and robustness. The Facial Action Coding System (FACS) [3], [4], a
psychological finding made over 25 years ago, is a typical example for
representing and understanding human facial expressions. In order to read
facial expressions correctly, the action units have to be detected accurately
and automatically. Based on the FACS, a number of systems were successfully
developed for facial expression analysis and recognition [4], [5], [6]. Other
systems have exploited the optical flow analysis to estimate the facial
feature motions [7], [8], [9]. In addition, several other approaches arereported in recent literatures, such as Manifold-based analysis for blended
expressions [10], Gabor-wavelet-labeled elastic graph matching for expression
recognition [11], feature selection using the AdaBoost algorithm with
classification by support vector machine [12], and learning Bayesian network
for classification [13], [14], [15], etc. Impressive results were reported,
however most representations of facial expressions are in a transformed domain
with an implicit format. We believe that good features for representing facial
expression could alleviate the complexity of the classification algorithm
design. The facial expression descriptor should have the ability to decrease
the significance of variations in age, gender and race when presenting the
same prototypic expressions. In other words, it should work in a person-
independent fashion.
Ideally, the facial expression can be modeled as a 3D face surface deformation
actuated by the movement of the facial muscles. The intensity variations on a
face image is caused by the face surface orientation and its reflectance. The
resultant texture appearance provides an important visual cue to classify a
variety of facial expressions [16]. Avoiding the ill-posed problem of 3D
reconstruction from a single image, we exploit a so-called topographic
representation to analyze facial expression on a terrain surface. Topographic
analysis is based on the topographic primary sketch theory [17], in which the
gray scale image is treated as a 3D terrain surface. Each pixel is assigned
one type of topographic label based on the terrain surface structure. We can
imagine that the facial skin ?wave? is a reflection of a certain expression.
Since the skin surface is represented by a topographic label ?map?, this ?map?
varies along with the change in facial expression. This fact suggests that
topographic features can be expected to have the robustness associated with
facial expression representation. It is thus of interest for us to investigate
the relationship between these topographic features and the corresponding
expressions in order to model the facial expression in an intuitive way.
Motivated by the topographic analysis technique [18], [19], in this paper, we
propose a novel facial expression descriptor?Topographic Context (TC)?to
represent and classify facial expressions. Topographic Context describes the
distribution of topographic labels in a region of interest of a face. We split
a face image into a number of expressive regions. In order to obtain the
topographic feature vector for an expression, the facial topographic surface
is labeled to form a terrain map. Statistics on the terrain map is then
conducted to derive the TC for each pre-defined expressive region. Finally, a
topographic feature vector is created by concatenating all the TCs of
expressive regions. With the extracted TC features, the facial expression can
be recognized using some classification algorithms.
Note that most existing work have exploited time-varying features from video
sequences for the dynamic facial expression analysis [5], [8], [9], [12],
[15], [20]. In this paper, we address the problem of facial expression
representation and classification using static frontal-view facial images.
This poses more of a challenge than using video sequences as no temporal
information is available. Although temporal dynamics reflect facial behavior,
it is essential to exploit the static image to represent and analyze
configurational information of facial expressions [6]. The basic understanding
of static facial expressions can help facilitate the application in
psychological research and law enforcement when the temporal information is
unavailable or inaccurate (e.g., due to the head motion). Our facial
expression recognition system is tested on the static facial images from two
facial expression databases, one is the commonly used Cohn?Kanade (CK)
database [21] and the other is a newly published MMI database [22]. Thissystem is completely person-independent, which means the subject to be tested
has never appeared in the training set. In fact, person-independent expression
recognition from a single static image is much more difficult due to the lack
of prior information of the recognized subjects. The experiment shows that our
TC-based expression representation has a good performance in classifying six
universal expressions in terms of accuracy and robustness.
We conducted a detailed evaluation on the separability of the TC-based
features, which includes an intuitive dimensionality reduction analysis and a
theoretical analysis using a separability criterion. The evaluation results
show that the TC-based expression features reflect intrinsic expression
characteristics and decrease the variance on the age, race and subject. In
order to evaluate the reliability of the TC-based feature representation, we
further conducted the robustness analysis in terms of two aspects: robustness
to the face region detection and robustness to the facial expression
intensity. The experiments show that the TC-based feature representation has
certain robustness to the distortion of facial landmark detection. Also, the
TC-based features can be used to recognize mid-intensity facial expressions,
although the performance is not as high as the extreme-intensity expression
case.
The remainder of this paper is organized as follows. In Section 2, the
topographic analysis is introduced. In Section 3, we present the concept of
topographic context and give our static topographic facial expression model.
The experiments on facial expression recognition are conducted in Section 4.
The performance of the TC-based facial expression representation will be
evaluated through the separability analysis in Section 5 and the robustness
analysis in Section 6, followed by a discussion in Section 7. Finally,
concluding remarks will be given in Section 8.
## 2\. Topographic analysis
In order to derive the Topographic Context of facial images, we apply the
topographic primal sketch theory [17] to study the pixel characteristics in
the face region. Topographic analysis treats the grey scale image as a terrain
surface in a 3D space. The intensity _I_(_x_ , _y_) is represented by the
height of the terrain at pixel (_x_ , _y_). Fig. 1 shows an example of a face
image and its terrain surface in the nose region. According to the property of
the terrain surface, each pixel can be assigned one of the topographic labels:
_peak_ , _ridge_ , _saddle_ , _hill_ , _flat_ , _ravine_ , or _pit_ [17].
Hill-labeled pixels can be further specified as one of the labels _convex
hill_ , _concave hill_ , _saddle hill_ or _slope hill_. Saddle hills can be
further distinguished as _concave saddle hill_ or _convex saddle hill_. Saddle
can be specified as _ridge saddle_ or _ravine saddle_. So there are a total of
12 types of topographic labels [18], [19]. In real face images the terrain
surface of the face region is mainly composed of _ridge_ , _ravine_ , _convex
hill_ , _concave hill_ , _convex saddle hill_ and _concave saddle hill_. We
illustrate these six topographic labels in Fig. 2, which will be used for our
TC-based expression representation.
1. Download: Download full-size image
Fig. 1. Face image and the 3D terrain surface of the nose region. (a) Original
face image; (b) terrain surface of the nose region of the original image; (c)
terrain surface of nose region smoothed by a Gaussian filter.
1. Download: Download full-size image
Fig. 2. A subset of the topographic labels [19]. The center pixel in each
example carries the indicated label. From left to right, the labels are:
_ridge_ , _convex hill_ , _convex saddle hill_ , _ravine_ , _concave hill_ and
_concave saddle hill_.In order to calculate the topographic labels of the input gray scale image, a
continuous surface _f_(_x_ , _y_) is used to fit the local _N_ × _N_ patch
centered at (_x_ , _y_) with the least square error. Then the first-order
derivatives ?I(x,y)?x and ?I(x,y)?y, and the second-order derivatives
?2I(x,y)?x2, ?2I(x,y)?y2 and ?2I(x,y)?x?y are estimated using _f_(_x_ , _y_).
Similar to the surface fitting approach used in [18], we use discrete
Chebyshev polynomials up to the third degree as the bases spanning the vector
space of these continuous functions. With the function _f_(_x_ , _y_), the
partial derivatives can be approximated
as(1)f(p,q)(x,y)=?i=-NN?j=-NNI(x-i,y-j)h(i;p)h(j;q)where _f_(_p_ ,_q_)(_x_ ,
_y_) means the (_p_ \+ _q_)th partial derivative at (_x_ , _y_) with _p_
along _x_ axis and _q_ along _y_ axis. _h_(_i_ ;_p_) and _h_(_i_ ;_q_) are the
smoothed differentiation filters from Chebyshev polynomials with degree _p_
and _q_ , respectively [23]. Thus the Hessian matrix is obtained as
follows:(2)H(x,y)=?2I(x,y)?x2?2I(x,y)?x?y?2I(x,y)?x?y?2I(x,y)?y2=f(2,0)(x,y)f(1,1)(x,y)f(1,1)(x,y)f(0,2)(x,y)After
applying eigenvalue decomposition to the Hessian matrix, we
get:(3)H=UDUT=[u1u2]·diag(?1,?2)·[u1u2]Twhere _?_ 1 and _?_ 2 are the
eigenvalues and **u** 1 and **u** 2 are the orthogonal eigenvectors. The
gradient magnitude ?? _I_(_x_ ,_y_)? can be calculated
as(4)??I(x,y)?=?I(x,y)?x2+?I(x,y)?y2From the calculated _?_ 1, _?_ 2, **u** 1,
**u** 2, ?? _I_(_x_ ,_y_)? and the derivatives, the terrain labels can be
assigned to pixel (_x_ , _y_) by obeying a series of rules with some pre-
defined empirical thresholds (e.g., _T_ _g_ for gradients and _T_ _?_ for
eigenvalues). For example, the ridge label is determined if the following
condition is satisfied: ?? _I_(_x_ , _y_)? ? _T_ _g_ , _?_ 1 < ? _T_ _?_ and
? _?_ 2? < _T_ _?_. The detailed labeling rules can be found in [19].
In order to reduce the influence of noise, smoothing preprocessing on the gray
scale image is executed before calculating the derivatives. In our
experiments, we use a Gaussian filter with a size of 15 × 15, a standard
deviation _?_ = 3.0. An example of a face image and the smoothed 3D terrain
surface of the nose region is shown in Fig. 1.
## 3\. Topographic context based feature extraction
By applying topographic analysis, the original gray-scale image _I_ = {_I_
_xy_} is transformed to a so-called terrain map _T_ = {_T_ _xy_}, which is
composed of the terrain labels of each pixel.
**Definition 3.1** Terrain Map
A Terrain Map describes the topographic composition of a 3D terrain surface by
indicating the type of terrain at each pixel.
In a terrain map, each pixel is represented by a certain topographic label. We
survey the distribution of topographic labels on the terrain map by
statistically analyzing 864 face images in the CK database. In real face
images the terrain surface of the face region is mainly composed of _ridge_ ,
_ravine_ , _convex hill_ , _concave hill_ , _convex saddle hill_ and _concave
saddle hill_ , which are more than 98.7% of all topographic label types. Table
1 shows our statistical results on 864 facial expression images. In the
remaining part of the paper, only these six types of terrain labels are taken
into account in our analysis.
Table 1. The ratios of 12 kinds of topographic labels in face image based on
the statistic on 864 images from CK database
Convex hill| Convex saddle hill| Ridge| Flat| Ridge saddle| Peak
---|---|---|---|---|---
20.3%| 31.8%| 6.0%| 0.0%| 0.4%| 0.4%
| | | | |
Concave hill| Concave saddle hill| Ravine| Slope| Ravine saddle| Pit14.0%| 21.8%| 4.8%| 0.0%| 0.3%| 0.2%
The terrain map can be visualized using different colors to represent the
different types of label. Fig. 3 shows the examples of facial expression
images from the CK database and their corresponding terrain maps in the face
regions. The terrain maps exhibit different patterns corresponding to
different facial expressions. In order to give an explicit and quantitative
description, we use a so-called Topographic Context to describe the
statistical property of the terrain map.
1. Download: Download full-size image
Fig. 3. Facial expression images and the corresponding terrain maps in face
regions. From left to right, _Anger_ , _Disgust_ , _Fear_ , _Happy_ , _Sad_
and _Surprise_.
### 3.1. Topographic context (TC)
The motion of facial muscles due to changing expressions leads to a variation
of facial terrain surface, which also results in the variation of image
intensities. This fact suggests that topographic features can be expected to
have the robustness for expression representation. To find an explicit
representation for the fundamental structure of facial surface details, the
statistical distributions of topographic labels within certain regions of
interest are exploited.
**Definition 3.2** Topographic Context
Topographic context is a descriptor of the terrain features inside a region of
interest of a gray-scale image. This descriptor is identified by the
distribution of topographic labels in the corresponding terrain map.
In other words, the topographic context is a vector, whose elements are the
ratios of the number of pixels with certain type of terrain label to the
number of pixels in the whole region. For a given region in the terrain map,
the topographic context can be computed as(5)e=n1n,?,nin,?,ntnwhere _n_ _i_ is
the number of pixels with the _i_ th type of terrain label and n=?i=1tni is
the total number of pixels in the referred region. _t_ denotes the number of
terrain label types, which is 6 in our approach.
As an example shown in Fig. 4, we mark two square regions on the terrain map
of a face image, and display the corresponding topographic contexts using
normalized histograms. As illustrated in this figure, the topographic contexts
reflect different characteristics of terrain features in different facial
regions.
1. Download: Download full-size image
Fig. 4. Terrain map and topographic context by histogram representation. (a)
Terrain map of the face image of Fig. 1; (b and c) topographic context of the
jowl and glabella. The bars in the histogram (from left to right) represent
_convex hill_ , _convex saddle hill_ , _concave hill_ , _concave saddle hill_
, _ridge_ and _ravine_.
### 3.2. Face model
The formation of a facial expression is a combined actuation by a number of
facial muscles. Based on the facial muscle distribution and the neuroanatomy
of facial movement [24], [25], [26], we partition the face image into eight
sub-regions, which are so-called _expressive regions_ , so as to derive the
topographic context efficiently.
**Definition 3.3** Expressive Region
An expressive region is a pre-defined region of interest that reflects muscle
actions and facial expressions. The expressive region may indicate facial
organs or a single segment of the facial surface.
To locate the expressive regions in a face, we define a face model with 64
control points. We use the Active Appearance Model (AAM), which was welldeveloped by Cootes etc. [27], to find the facial features and shapes. In the
case of the high level of expression intensity, AAM may not detect the
landmarks accurately. In order to evaluate our new facial expression features,
the necessary manual adjustment is added to alleviate the influence of the
facial landmark detection. In Section 6.1, we investigate the robustness of
facial landmark detection with the artificial landmark distortions. As a
result, the expressive regions can be constructed by polygons whose vertices
are the detected facial landmarks, as shown in Fig. 5.
1. Download: Download full-size image
Fig. 5. Face model: (a) 64 facial landmarks; (b) 8 facial expressive regions.
Note that not all of the facial areas are defined by the expressive regions.
The nose bridge is excluded because of its lack of expressive information.
Although the forehead surface may signify the expression-related furrows, due
to the occasional occlusion by hairs, we exclude this region from our regions
of interest. Currently, eight expressive regions are defined to construct the
facial terrain model for TC-feature extraction.
### 3.3. TC-based facial expression feature
With the detected facial region or features, the expression feature can be
derived. Here, we propose a novel feature, _Topographic Context_ based
expression feature, which has certain insensitivity to expression intensity
and facial region locations.
The variation of facial expressions is a result of the motion of both facial
organs and facial skin. For example, mouth opening is a distinct organ motion
in some _surprise_ expressions which present jowl surface stretching and
flattening. Through the analysis of facial terrain surface, we are able to
describe the terrain features in the expressive regions by topographic
contexts. These topographic contexts will be eventually linked to a certain
expression. The relationship between the topographic context and the
expression is illustrated by an example in Fig. 6, which shows two subjects
with three distinct prototypic facial expressions, _Surprise_ , _Anger_ and
_Happy_. The corresponding TCs of expressive region 1 and region 6 are
illustrated by their histograms.
1. Download: Download full-size image
Fig. 6. TC features of two subjects with different expressions: (a and b)
original images: _Surprise_ , _Anger_ and _Happy_ of two subjects from the CK
database; (c and d) corresponding TCs in the region 1 of subject (a) and (b);
(e and f) corresponding TCs in the region 6 of subject (a) and (b).
Given the same facial expression from different subjects, the topographic
contexts of expressive regions exhibit similar histogram characteristics. Fig.
6 illustrates the quantitative property of TCs for different subjects. It
demonstrates that the topographic label distribution reflects different facial
expressions. For example, in the expressive region 6, the _concave hill_ and
_ravine_ rarely appear in mouth region when a face shows _anger_ expression.
Moreover, the prominent ratio of _convex saddle hill_ usually results from
happiness. Similarly, the prototypic expression information can also be
uniquely represented in other seven regions.
Since the topographic context is defined in each expressive region, the eight
expressive regions will produce eight topographic contexts. The combination of
these TCs will generate a unique expression feature vector for a specific
expression. In general, the procedure for generating expression feature
vectors can be described as the following four steps:
* 1.
Detect the 64 facial landmarks;
* 2.derive terrain map _T_ = {_T_ _x_ ,_y_} from original facial image _I_ = {_I_
_x_ ,_y_} by label each pixel based on its terrain property;
* 3.
with the detected facial landmarks and the derived terrain map, compute
topographic context for each expressive region;
* 4.
combine the extracted topographic contexts of the eight expressive regions to
construct the expression features.
As a result, the expression feature vector is constructed
as(6)E=[e1,?,ek,?,eM]where _M_ is the number of expressive regions (_M_ = 8 in
our experiment). The dimensionality of the expression feature vector is _t_ ×
_M_. Because only six kinds of terrain labels are used in our algorithm (_t_ =
6), the expression feature of each face image is represented by a
48-dimensional vector, which is much lower than the dimensionality of the
original image space.
## 4\. Experiments on facial expression recognition
In this section, we conduct person-dependent and person-independent facial
expression recognition experiments using the extracted TC-based features. Two
different databases are used in our experiments. One is a newly created facial
expression database (MMI-database) from the man?machine interaction group of
Imperial College London [22] and the other is the widely used CK database
[21]. MMI database mainly provide action units based facial expression data.
Currently, only a few subjects with six universal expressions are available.
The CK database consists of video sequences of subjects displaying distinct
facial expressions, starting from neutral expression and ending with the peak
of the expression. Because some subjects only show one or two facial
expressions, we use a subset with 53 subjects for our experiment. On the
average, four expressions appear for each subject. For each expression of a
subject, the last four frames in the videos are selected. Notice that although
we use several frames from the video sequence, we only treat these frames as
static images for both training and testing without using any temporal
information. Several samples of these two expression databases are shown in
Fig. 3, Fig. 6, Fig. 7. The summary of the two databases is presented in Table
1.
1. Download: Download full-size image
Fig. 7. Expression images from the MMI database.
Although many existing classification algorithms could be employed for the
experiment on facial expression recognition, our purpose is to evaluate the
inherent discriminatory ability of the extracted TC features. The classifier
design and training are not our emphasis in this paper. Hence, several
standard and widely used classification approaches are used in our experiments
(Table 2).
Table 2. Summary of the databases used for our experiments
Database| # of subjects| # of images per subject for each expression| Overall
# of images
---|---|---|---
MMI| 5| 6| 180
CK| 53| 4| 864
### 4.1. Person-dependent recognition test
In person-dependent test, first we divide the MMI and CK database into six and
four subsets, respectively. Each subset contains all the subjects and each
subject includes a set of available prototypic expressions. Then one of the
subsets is selected for test while the remainder is used to construct the
training set. It is a so-called ?leave-one-out? cross-validation rule. Thetests are repeated six times in the MMI database and four times in the CK
database, with different test subset being used for each time. Three
classifiers, quadratic discriminant classifier (QDC), linear discriminant
analysis (LDA) and naive Bayesian network classifier (NBC), are employed to
recognize the six prototypic facial expressions. Table 3 shows the average
recognition rate. Table 4 reports the confusion matrix using the NBC
classifier on the CK database.
Table 3. Experimental results of person-dependent expression classification
Database?classifier| QDC (%)| LDA (%)| NBC (%)
---|---|---|---
MMI| 92.78| 93.33| 85.56
CK| 82.52| 87.27| 93.29
Table 4. Confusion matrix of the average case of NBC classifier on the CK
database in person-dependent expression recognition
Input?output| Anger (%)| Disgust (%)| Fear (%)| Happy (%)| Sad (%)| Surprise
(%)
---|---|---|---|---|---|---
Anger| 83.70| 5.43| 0.00| 1.09| 9.78| 0.00
Disgust| 2.78| 93.06| 0.00| 0.00| 2.08| 2.08
Fear| 1.92| 3.85| 83.65| 7.69| 2.89| 0.00
Happy| 0.00| 0.50| 1.00| 98.50| 0.00| 0.00
Sad| 1.47| 0.74| 0.00| 0.00| 97.06| 0.74
Surprise| 0.00| 1.60| 1.06| 0.53| 1.59| 95.21
As shown in Table 3, LDA achieves the highest recognition rate for the MMI
database, and NBC does for the CK database. In Table 4, we can see that
_Happy_ , _Surprise_ , _Disgust_ and _Sad_ are detected with high accuracy.
_Fear_ is sometimes confused with _Happy_ while _Anger_ is sometimes
misclassified as _Sad_.
### 4.2. Person-independent recognition test
Note that the person-independent facial expression recognition is more
challenging and necessary for practical applications. In order to train
person-independent classifiers, we need more training subjects covering
various patterns of same expression. Therefore, we use the CK database to
carry out the person-independent tests. Because 53 subjects are contained in
our database with a total of 864 images, we partition the whole set into 53
subsets, each of which corresponds to one subject and includes all the images
of this subject with different expressions. The ?leave-_v_ -out? strategy is
used for separating these subsets into training sets and test sets. This
strategy is proved to be a more elaborate and expensive version of cross-
validation [28]. In our experiment, the value of _v_ is 10. It means that for
each test, the database is partitioned into 43 subsets as the training set and
10 subsets as the test set. Note that any subject used for testing does not
appear in the training set because the random partitioning is based on the
subjects rather than the individual images. The tests are executed 20 times on
each classifier with different partitions to achieve a stable recognition
rate. The entire process guarantees every subject is tested at least once for
every classifier. For each test, all the classifiers are reset and re-trained
from the initial state. Totally four classifiers, including QDC, LDA, NBC and
support vector classifier (SVC) with RBF kernel are used in the experiments.
Table 5 reports the average recognition rates of these four classifiers, where
LDA classifier achieve the highest accuracy with 82.68% correct rate. The
confusion matrix of the average case for LDA classifier is shown in Table 6.
The expressions _Surprise_ and _Happy_ are well detected with accuracy over
95% and 91%. _Anger_ , _Disgust_ and _Sad_ are sometimes confused with eachother, while _Fear_ is sometimes misclassified as _Happy_.
Table 5. Experimental results of person-independent expression classification
Classifier| QDC| LDA| NBC| SVC
---|---|---|---|---
Recognition rate| 81.96%| 82.68%| 76.12%| 77.68%
Table 6. Confusion Matrix of the average case of LDA classifier for person-
independent expression recognition
Input?output| Anger (%)| Disgust (%)| Fear (%)| Happy (%)| Sad (%)| Surprise
(%)
---|---|---|---|---|---|---
Anger| 75.39| 14.06| 1.56| 1.56| 7.42| 0.00
Disgust| 12.50| 69.50| 10.04| 0.00| 5.87| 2.08
Fear| 3.72| 2.93| 68.88| 21.54| 1.33| 1.60
Happy| 0.00| 2.70| 4.46| 91.76| 1.08| 0.00
Sad| 13.20| 5.28| 1.94| 0.18| 79.40| 0.00
Surprise| 0.00| 1.47| 1.32| 0.00| 1.62| 95.59
## 5\. Separability analysis
In order to further study the property of the TC-based expression
representation, we investigate the inter-expression discriminability of TC-
based expression features in this section. First of all, we examine the
separability of the TC-based features in a low-dimensional space for an
intuitive demonstration. Then, we give an in-depth theoretical analysis to
quantitatively evaluate the between-expression separability using certain
separability criterion, and show how the separability criterions used in this
study agree with the results of human perception for distinguishing facial
expressions.
### 5.1. Separability in a low-dimensional space: an intuitive example
High-dimensional TC-based expression features are hard to be visualized. It is
intuitive for us to observe the clustering characteristic of the expression
features in a low-dimensional space (i.e., 3D space). Here, we apply the
Principal Component Analysis (PCA) to investigate the inter-expression
discriminability of TC-based expression feature. To do so, we project the
extracted TC-based features into a 3D space through the PCA dimensionality
reduction.
Although a complex classification algorithm or a well-trained classifier can
improve the performance, it heavily relies on the training data and lacks
enough generalization ability. The unsupervised learning, for example,
clustering characteristic gives a clear clue for an in-depth study on the
separability of extracted features. We found that the TC-based facial feature
exhibits a good inter-expression separability even in a low-dimensional space,
especially for the distinct expressions _Happy_ and _Surprise_. An example of
separability study by reducing the dimensionality to a visualized space is
given in Fig. 8. The TC features of 30 expression images (10 subjects, each of
which has three expression images, _Happy_ , _Surprise_ and _Disgust_) are
projected to a 3D space by PCA capturing 61.9% of the total energy. The
selected subjects cover the variations of gender, race and face shapes.1 As we
can see, the output samples are roughly clustered in three sets which
correspond to the three types of expressions. In general, the TC-based
expression feature vectors exhibit a good ability in distinguishing
expressions even in a very low-dimensional space.
1. Download: Download full-size image
Fig. 8. 3D projections by applying PCA to reduce the dimensionality of TC-
based expression features. The three expressions, _Happy_ , _Surprise_ and
_Disgust_ are roughly distinguishable.For comparison, the original intensity image is also projected to the 3D space
by applying PCA. As shown in Fig. 9, the three expression samples of each
subject are distributed together. The three expressions are completely
indistinguishable from projections of the original intensity images. The
clustering sample intuitively demonstrates that the separability of TC-based
expression feature is much superior to the intensity-based features, which
exhibit the clustering property by subjects rather than by expressions.
1. Download: Download full-size image
Fig. 9. 3D projections by applying PCA to reduce the dimensionality of
intensity-based expression features. The three expressions, _Happy_ ,
_Surprise_ and _Disgust_ are completely mixed together.
Note that a visualized expression manifold by the Lipschitz embedding has been
successfully used by Chang and Turk [10]. However, in [10], the constructed
manifold is built for individual subject with a plenty of expression images of
the _same_ subject. Here, instead, we investigate the separability and
clustering characteristic of extracted expression features using _multi_
-subject and _multi_ -expression data in a visualized 3D space. In general,
the TC-based expression descriptor reflects more intrinsic expression property
and alleviates the variations of non-expression factors, such as race and
gender. Although the separability of other three expressions is hard to be
visualized in 3D space, we can still distinguish them in a higher dimensional
space, which is proved by the recognition experiments.
In the following section, we will give a theoretical analysis of the
expression separability through the evaluation and comparison between the TC-
based features and the intensity-based features.
### 5.2. Separability analysis of TC-based features
In order to quantitatively measure the separability of the expression
features, a computable criterion should be defined first. There are some
existing criteria, which are mainly used for feature selection [29], [30], for
example, probability based criterion, within-class and between-class distance
based criterion, etc. Bayesian decision theory provides the probability based
separability criterion (e.g., correct probability or error probability), which
can be used to evaluate and select the extracted features. In our multi-
category case for expression classification, it is more efficient to compute
the correct probability rather than the error probability. The correct
probability metric is defined
as(7)P(correct)=?i=1cP(x?Ri,?i)=?i=1c?Rip(x|?i)P(?i)dxwhere x?Ri means the
feature space divided by the classifier, _c_ is the number of classes, _?_ _i_
denotes the label of class and _P_(_?_ _i_) is the prior probability of _?_
_i_. Although the definition in Eq. (7) is simple, it is not feasible to
calculate the correct probability in practice because it is difficult to
estimate the _class-conditional probability density functions p_(_x_ ? _?_
_i_) and the multiple integration has to be executed in high-dimensional
space. Alternatively, the separability criterion based on within-class and
between-class distance is more feasible and empirical. Suppose xk(i) and xl(j)
are the _d_ -dimensional features with the label _?_ _i_ and _?_ _j_ ,
respectively. The definition of average between-class distance in the case of
multiple categories is as
follow:(8)J1(x)=12?i=1cPi?j=1cPj1ninj?k=1ni?l=1nj?(xk(i),xl(j))where, _n_ _i_
, _n_ _j_ are the numbers of samples in class _?_ _i_ and _?_ _j_ , _P_ _i_ ,
_P_ _j_ are the class-prior probabilities. ?(xk(i),xl(j)) denotes the distance
between two samples, which is usually represented by the Euclidean
distance(9)?(xk(i),xl(j))=(xk(i)-xl(j))T(xk(i)-xl(j))In order to represent the
_J_ 1(_x_) in a compact form, two new concepts, _within-class scatter matrix_**S** _w_ and _between-class scatter matrix_ **S** _b_ , are introduced
[29](10)Sw=?i=1cPi1ni?k=1ni(xk(i)-mi)(xk(i)-mi)T(11)Sb=?i=1cPi(mi-m)(mi-m)Twhere
**m** _i_ is the mean of samples in the _i_ th class(12)mi=1ni?k=1nixk(i)**m**
is the mean of all the samples.(13)m=?i=1cPimiWith the definitions of Eqs.
(10), (11), we can get _J_ 1(_x_) in the following form [30],
[31]:(14)J1(x)=tr(Sw+Sb)_J_ 1(_x_) is an efficient and computable separability
criterion for feature selection. But it is not appropriate for comparing two
different features because the value of calculated _J_ 1(_x_) depends on the
scale and dimensionality of the feature space. Since the TC-based features and
intensity-based features lie in two completely different spaces with different
scales and dimensionalities, it is not easy to normalize the value of _J_
1(_x_) for comparison. Here we use a comparable separability criterion to
avoid the normalization. The new metric, _J_ 2(_x_), is defined as a natural
logarithm of the ratio of determinant of within-class scatter matrix and
between-class scatter matrix. _J_ 2(_x_) is intrinsically normalized in the
comparable scale and reflects the separability of the features. For the value
of _J_ 2(_x_), the larger the values are, the better the samples can be
separated.(15)J2(x)=ln?(Sb+Sw)|?(Sw)|We conducted the calculations of _J_
2(_x_) on CK test set, which includes 864 facial expression images of 53
subjects. PCA is used to reduce the dimensionality of the extracted TC-based
expression features. The selected separability metrics are calculated for each
reduced dimensionality. For comparison, the similar experiment is also
conducted using intensity features. Note that TC-based features and intensity
features locate in completely different feature spaces, where the intensity
feature has much higher dimensionality than the TC-based feature has.
Therefore we normalize the calculated values by the percentage of retained
energy rather than by the dimensionality. The facial expression separability
and human subject separability are examined, respectively. Each examination is
conducted by comparing the TC-based features and intensity features using the
metric measurement _J_ 2(_x_). Fig. 10 shows the experimental results of the
calculated value of _J_ 2(_x_). As shown in Fig. 10a, for the same ratio of
retained energy, the value _J_ 2(_x_) of TC-based features is always higher
than that of intensity-based features, which means that the TC-based features
always exhibit much better between-expression separability than the intensity
feature does.
1. Download: Download full-size image
Fig. 10. Separability criterion (_J_ 2(_x_)) comparison of TC-based features
and intensity feature: (a) separability of expressions; (b) separability of
subjects.
The subject separability performance is also evaluated using the similar
experimental strategy. Here, we re-label the facial images based on subjects
rather than expressions. There are totally _c_ = 53 classes in our test data.
The estimated performance curve of separability criterion _J_ 2(_x_) as to the
ratio of retained energy is shown in Fig. 10b. The separability curve of TC-
based features is always lower than that of intensity-based features, which
means that the intensity features have much better discriminability for
subjects than the TC-based features have.
### 5.3. Between-expression separability analysis
The person-independent experiment in Section 4 (see Table 6) has shown that
the correct recognition rates for different expressions are different.
_Surprise_ and _Happy_ are the two most distinguishable expressions among the
six prototype facial expressions, while _Anger_ , _Disgust_ and _Sad_ are
sometimes confused with each other. As a result, the confusion matrix of
recognition exhibits a considerable imbalance. The automatic facial expressionrecognition system reported by Cohen etc. [20], which used different
expression features and classifiers, shows the similar imbalance
characteristic in classifying six prototype expressions. Here, we study the
expression separability in order to provide insights into the intrinsic
property of the facial expressions. We attempt to quantitatively explain the
expression separability for better understanding the machine recognition and
human cognition.
In order to evaluate the between-expression separability, the two-category
separability criteria are used. Different from the criteria used in Section
5.2, here, the metrics for _within-class matrix_ and _between-class scatter
matrix_ are specified for the two-category case. Assume that the six
prototypic expressions are indexed from 1 to 6, we want to estimate the
separability between expression _e_ _i_ and _e_ _j_ , where 1 ? _e_ _i_ , _e_
_j_ ? 6 and _e_ _i_ ? _e_ _j_. In the test data set, we only use those images
with these two types of expressions. The scatter matrices are calculated for
the two-category case
as(16)Sw(ei,ej)=1n?k=1nei(xk(ei)-mei)(xk(ei)-mei)T+?l=1nej(xl(ej)-mej)(xl(ej)-mej)T(17)Sb(ei,ej)=PeiPej(mei-
mej)(mei-mej)Twhere n=nei+nej and Pei+Pej=1. We estimate the separability
criterion _J_ 2(_x_) for each pair of selected expressions while excluding
other expressions. For each pair of selected expressions, the scatter matrices
are computed using Eqs. (16), (17). Then the separability criterion _J_ 2(_x_)
is calculated. The evaluation is conducted on the extracted TC-based features
with the entire energy conserved.2
Table 7 illustrates the separability of all pairs of prototypic expressions,
where the larger values of _J_ 2(_x_) indicates better discriminability
between the two expressions. The results support the confusion matrix shown in
Table 6. For example, expressions _Happy_ and _Surprise_ have the highest
recognition rates. Conformably, these two expressions have a larger
separability value _J_ 2(_x_) than other expressions. Also, expressions
_Anger_ , _Disgust_ and _Sad_ are easily misclassified because the
separability value _J_ 2(_x_) is fairly small for any two of these three
expressions.
Table 7. Confusion matrix of the expression separability criterion of eJ2(x)
Empty Cell| Anger| Disgust| Fear| Happy| Sad| Surprise
---|---|---|---|---|---|---
Anger| _N/A_| 3.432| 8.890| 7.179| 4.957| 9.066
Disgust| 3.432| _N/A_| 5.357| 8.290| 4.192| 8.544
Fear| 8.890| 5.357| _N/A_| 4.570| 8.400| 7.225
Happy| 7.179| 8.290| 4.570| _N/A_| 12.490| 15.395
Sad| 4.957| 4.192| 8.400| 12.490| _N/A_| 9.335
Surprise| 9.066| 8.544| 7.225| 15.395| 9.335| _N/A_
N/A, not applicable.
In short, the recognition performance of the six prototypic expressions is
mostly affected by the intrinsic appearance characteristics of expressions.
The quantitative separability analysis is consistent with the recognition
property for different expression, which is also obey the human vision
perception rule.
## 6\. Robustness analysis
### 6.1. Robustness to facial landmarks detection
As we know, the topographic context is defined on the individual expressive
region of a face. It is conceivable that the performance of facial expression
recognition is affected by the expressive region extraction, in other words,
by the facial feature detection.
An accurate facial landmark localization algorithm could improve theperformance of the expression recognition. Most previous work in facial
expression analysis is under controlled conditions. For real applications, the
possible complicated environment (e.g., complex background and uncontrolled
illumination) could make the detection of facial landmarks inaccurate and
unreliable.
In order to evaluate the TC-based feature as to how sensitive it is to the
expressive region detection, we conduct two simulation experiments. In our
first experiment, we simulate an uncontrolled environment by adding a Gaussian
noise _k_ · _N_(0, 0.7) with a magnitude _k_ to the detected landmarks. An
example of the facial landmarks with different magnitudes of noise is shown in
Fig. 11b and c. The second experiment simulates the noise through applying a
random affine transformation to the detected entire facial shape. The
transformation (i.e., rotation and translation) has the form
as(18)M=Rk10·P(-0.5,0.5)×T(5k·P(-0.5,0.5))where _P_(?0.5, 0.5) is a uniform
distribution between [?0.5, 0.5] and _k_ is the magnitude of noise. Fig. 11d?f
shows the examples after affine transformation (Fig. 12).
1. Download: Download full-size image
Fig. 11. (a) Original landmarks; (b and c) landmarks added with random
Gaussian noise with magnitude _k_ = 2 and _k_ = 5; (d?f) landmarks added after
random affine transformation with magnitude _k_ = 1, _k_ = 3 and _k_ = 4.
1. Download: Download full-size image
Fig. 12. Recognition under random Gaussian noise added to detected facial
landmarks.
For the training set, we use the clean facial landmarks. For each test image,
the modified landmarks are used to calculate the facial expression features.
The person-independent experiments described in Section 4.2 are repeated under
the noise with different magnitudes. The performance curves of all four
classifiers are recorded in Fig. 13. As shown in the figures, the recognition
rate monotonically decreases as the noise magnitude increases, but it still
maintains a fairly good performance, especially under the random Gaussian
noise. The classifiers QDC and LDA maintain the correct recognition rate
higher than 80% when _k_ = 4. In short, the experimental results demonstrate
that the TC-based facial expression features are not very sensitive to facial
landmark detection. The main reason is that this feature representation is
based on regional terrain label statistics. These statistical characteristics
are not affected by local noises or by a certain shape distortion seriously.
1. Download: Download full-size image
Fig. 13. Recognition under random affine transformation noise added to
detected facial landmarks.
### 6.2. Robustness to expression intensity
Most of approaches for automatic facial expression analysis attempt to
recognize a set of prototypic emotional expressions (e.g., fear, sadness,
disgust, anger, surprise and happiness). In everyday life, however, such
prototypic expressions with deliberate (i.e., posed) facial action rarely
occur. Instead, the spontaneous expressions with subtle facial action appear
more frequently [32]. Moreover, the posed facial action tasks typically differ
in appearance and timing from the authentic facial expressions induced through
events in the normal environment of the subject. Therefore, investigation of
recognition performance on different degrees of _expression intensity_ (or
degree of expression saturation) is of interest to us. Low-intensity
expressions mostly reflect the spontaneity of human emotions. A database with
authentic face expressions from 28 subjects was created by Sebe et al. [32],
which includes four spontaneous expressions (neutral, happy, surprise and
disgust). Bartlett et al. tested their algorithm on facial expression withdifferent intensities measured by manual FACS coding [33]. Tian et al.
combined Gabor features and neural network to discriminate eye status and
compared their method with manual markers [34]. Although there is some
existing work targeting the spontaneous expression analysis, so far, no
standard definition and quantitative measurement of _expression intensity_ has
been reported.
As we know, recognizing a slight or low-intensity spontaneous expression is a
very challenging task. Even for human vision system, it is difficult to
interpret a slight or low-intensity expression based on only a single static
image (e.g., because of the ambiguity). For the _static_ image based universal
expression recognition, most of previous research focused on facial images
with extreme expressions or sufficiently perceivable expressions [1], [35].
For low-intensity spontaneous expression recognition, it will be more
effective to explore the temporal information through the analysis of dynamic
expression sequences.
In the research on static image based facial expression recognition to date,
little work has been done on the quantitative analysis of the recognition rate
versus the expression intensity. In order to analyze the robustness and
sensitivity of TC-based facial expression representation, we conduct an
experiment to explicitly evaluate the performance of the extracted TC-based
features under different degrees of expression intensities.
To do so, we extend the subset of CK database used in Section 4 by selecting
more frames from the original video sequences. For each subject, we selected
eight frames, which correspond to eight different degrees of perceived
expressions. The selected frames are indexed from 1 to 8 based on the order of
the frames in the video sequence. Because the expression video starts from the
neutral expression to the extreme expression, the expression intensity
increases from the index 1 to the index 8. As an example, eight facial
expression images of one subject with different intensities are showed in Fig.
14. In our experiments, a total of 1672 face images, selected from 209 video
sequences, are used.
1. Download: Download full-size image
Fig. 14. Eight facial images with different expression intensities.
The experimental strategy is same as the one used in Section 4.2. The recorded
recognition performance is reported in Fig. 15. Moreover, the same criterion
_J_ 2(_x_) that is used in Section 5 is applied to evaluate the between-
expressions separability in different level of expression intensity. The
calculated values of _J_ 2(_x_) corresponding to expression intensity indexed
from 1 to 8 are recorded in Fig. 16.
1. Download: Download full-size image
Fig. 15. Facial expression recognition performance under different facial
expression intensity. _X_ coordinate denotes the index of the test image in
the selected video sequence, which displays the expression from low-intensity
to extreme.
1. Download: Download full-size image
Fig. 16. Separability criterion (_J_ 2(_x_)) under different expression
intensities.
Note that since there is no existing solution for the quantitative measurement
of the expression intensity, our performance analysis in terms of expression
intensity using frame indexes is approximate. Since different subjects may
exhibit different styles of expressions (e.g., timing, speed, etc.), the
frame-indexing based approach may not be able to extract the same expression
intensity at a same indexed frame across different videos.
Experimental results on both expression recognition and between-expressionseparability illustrate the following properties of the TC-based facial
expression recognition. First, the expression intensity does affect the
recognition performance. Especially, when the expression intensity increases
from index 1 to 5, the correct recognition rates are dramatically improved. In
general, the recognition rate monotonically grows when the expression
intensity increases. Second, when the expression reaches a certain degree of
intensity, the correct recognition rate does not have a significant increment.
In other words, when expressions reach the ?saturation? status with the high
level of intensity, the performance of the recognition is not further
improved.
## 7\. Discussion
### 7.1. Advantages of TC-based feature representation
Expression feature extraction is a fundamental issue in automatic facial
expression analysis and recognition. There are, in general, two categories of
facial expression features used for classification, i.e., geometric features
and appearance features [2]. Both feature representations are sensitive to
imaging conditions and individual variations, such as face shape, scale and
other factors. Although the normalization process could reduce the effect of
imaging and individual variations, it is not trivial to obtain the normalized
features without the human interaction and a neutral face input in some cases.
For example, there is some prior work utilizing the line-based (or edge-based)
caricatures to classify expressions [36]. Because the detected edge features
are represented as binary levels (black and white), normalization is
impossible. Moreover, because edges or corners are usually detected in the
high contrast regions, it is hard to capture subtle facial features on the
entire facial region. In [36], only three types of salient expressions can be
classified correctly. However, in contrast, our topographic features are
labeled with six different categories on each pixel of the entire face region.
Since we use the statistics of the six types of topographic labels for each
pre-defined facial region, the intrinsically normalized topographic context
alleviates the influence of the individual variance, such as the face shape
and scale. Although the calculation of TC is based on the detected facial
landmarks, it is not sensitive to the facial region location because of its
inherent statistical characteristics (proven by the experiments presented in
Section 6.1). By contrast, geometric features are sensitive to the facial
feature detection. Moreover, for the robustness analysis as to the head
rotation, we compared the TC features with the Gabor-wavelet features on a
synthesized facial expression image database using a recently published 3D
facial expression database [37]. The experimental results reported in [38]
show that the TC features are much more insensitive to facial pose than the
Gabor-wavelet features.
It is worth noting that unlike most of existing approaches (e.g., FACS-based
approaches), our TC-based facial expression representation does not require
the high-resolution image input since the preprocessing of topographic
labeling smooths the image details. Hence, it is conceivable that the TC-based
features should have certain robustness to image quality.
### 7.2. Qualitative comparison with existing work
Identifying facial expression is a challenging problem in computer vision
community. However, there is no baseline algorithm or standard database for
measuring the merits of new methods, or determining what factors affect the
performance. The unavailability of an accredited common database (e.g., like
FERET database for face recognition) and evaluation methodology make it
difficult to compare any new algorithm with the existing algorithms. As quoted
by some researchers [1], [6], [35], most of existing work using staticexpressions (e.g., see Table 8 in [1] and Tables 2 and 3 in [35]) were tested
on the in-house databases and some of the approaches are only evaluated by a
person-dependent test. Although the CK database is widely used, some recent
work [20], [15] only utilized a portion of the database because not all the
subjects in CK database have six prototypic expressions. This makes the
algorithm comparison not feasible without knowing the exact test set used in
their approaches.
Here, we give a brief qualitative comparison with the system recently reported
by Cohen et al. in [20], [15]. Both our work and the Cohen?s system selected
53 subjects from the CK database for person-independent experiments. Although
our system works on static images while Cohen?s system worked on video
sequences, both achieve similar recognition results and characteristics.
Cohen?s system achieved the best correct recognition rate at 81.80%, while our
system achieves the best recognition rate at 82.68%. Both systems show the
similar characteristics in classifying six prototypic expressions. For
example, _Surprise_ and _Happy_ , as both reported, have the highest
recognition rate. _Fear_ is easily confused with _Happy_ while _Sad_ is
confused with _Anger_.
## 8\. Concluding remarks
In this paper we proposed a novel and explicit static topographic modeling for
facial expression representation and recognition based on the so-called
Topographic Context. Motivated by the mechanism of facial expression formation
in the 3D space, we exploit the topographic analysis to study the facial
terrain map which is derived by the topographic labeling techniques at the
pixel level of detail. The facial expression is a behavior of an entire facial
surface. The distribution of topographic labels described by the Topographic
Context in expressive regions is a good reflection of this behavior. The
performance of such a new facial expression descriptor is evaluated by the
experiments of person-dependent and person-independent expression recognition
on two public facial expression databases. The experimental results
demonstrate that the TC-based expression features can capture the
characteristics of facial expressions and achieve encouraging results in
recognizing six prototypic expressions with person-independence. For a further
investigation of the TC-based facial expression features, the separability
analysis is conducted. The experimental results show that the TC-based
features are appropriate for expression recognition because they reflect more
intrinsic expression characteristics while alleviating the individual
variations, such as race, gender and face shape. Robustness of TC-based
features is evaluated in two aspects, the robustness to distortion of facial
landmark detection and the robustness to different levels of expression
intensities. Notice that the TC-based approach still has some limitations.
Because the topographic feature estimation is based on the polynomial patch
approximation, it is better than some edge detection approaches in terms of
the illumination variations. However, this approach is still the pixel-based
approach. It may not be robust to the dramatic illumination change. Due to the
lack of the illumination-varied facial expression database, our future work is
to develop a synthesis-based approach to simulate the various lighting
conditions in order to study the illumination sensitivity.
As a challenging cross-disciplinary research topic, automatic facial
expression recognition is still in its infancy and far from the real
application. Although many advances and successes are reported in recent
literature, many questions still remain open [2]. For the future work, we will
work on a dynamic system using temporal information to improve the performance
of the current static modeling. We will study the action units detection basedon the topographic context domain. The integration of the location information
and the topographic feature information could benefit the tracking of AUs.
Moreover, the investigation on quantification and estimation of expression
intensities is another future research direction.
## Acknowledgments
The authors thank Dr. Jeffrey Cohn and the face group of CMU for providing the
Cohn-Kanade database. We also thank Dr. Maja Pantic?s group at Imperial
College London for providing the MMI facial expression database. This work is
supported in part by the National Science Foundation under Grants IIS-0414029
and IIS-0541044, and NYSTAR?s James D. Watson Investigator program.
Special issue articlesRecommended articles"
210,212,Subject independent facial expression recognition with robust face detection using a convolutional neural network,"['M Matsugu', 'K Mori', 'Y Mitari', 'Y Kaneda']",2003,975,Affective Faces Database,neural network,"convolutional network architecture for robust face detection and  in the CNN, between neutral  and emotional faces. We show that the  neural networks for facial expression recognition.",No DOI,Neural networks,https://www.sciencedirect.com/science/article/pii/S0893608003001151,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Subject independent facial expression recognition with
robust face detection using a convolutional neural network
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Robust face detection using CNN
6. 3\. Rule-based analysis of facial expressions using local features detected by CNN
7. 4\. Results
8. 5\. Discussion
9. References
Show full outline
## Cited by (520)
## Figures (6)
1. 2. 3. 4. 5. 6.
## Neural Networks
Volume 16, Issues 5?6, June?July 2003, Pages 555-559
#
2003 Special issue
Subject independent facial expression recognition with robust face detection
using a convolutional neural network
Author links open overlay panelMasakazu Matsugu, Katsuhiko Mori, Yusuke
Mitari, Yuji Kaneda
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/S0893-6080(03)00115-1Get rights and content
## Abstract
Reliable detection of ordinary facial expressions (e.g. smile) despite the
variability among individuals as well as face appearance is an important step
toward the realization of perceptual user interface with autonomous perception
of persons. We describe a rule-based algorithm for robust facial expression
recognition combined with robust face detection using a convolutional neural
network. In this study, we address the problem of subject independence as well
as translation, rotation, and scale invariance in the recognition of facial
expression. The result shows reliable detection of smiles with recognition
rate of 97.6% for 5600 still images of more than 10 subjects. The proposed
algorithm demonstrated the ability to discriminate smiling from talking basedon the saliency score obtained from voting visual cues. To the best of our
knowledge, it is the first facial expression recognition model with the
property of subject independence combined with robustness to variability in
facial appearance.
* Previous article in issue
* Next article in issue
## Keywords
Convolutional neural network
Face detection
Facial expression
## 1\. Introduction
Facial expressions as manifestations of emotional states, in general, tend to
be different among individuals. For example, smiling face as it appears may
have different emotional implications for different persons in that ?smiling
face?, perceived by others, for some person does not necessarily represent
truly _smiling state_ for that person. To the best of our knowledge, only a
few algorithms (Ebine & Nakamura, 1999) have addressed robustness to such
individuality in facial expression recognition. Furthermore, in order for
facial expression recognition to be used for human?computer interaction, for
example, that algorithm must have good ability in dealing with variability of
facial appearance (e.g. pose, size, and translation invariance). Most
algorithms, so far, have addressed only a part of these problems (Földiák,
1991, Wallis and Rolls, 1997). In this study, we propose a system for facial
expression recognition that is robust to variability that originates from
individuality and viewing conditions.
Recognizing facial expression under rigid head movements was addressed by
Black and Yacoob (1995). Neural network model that learns to recognize facial
expressions from an optical flow field was reported in Rosenblum, Yacoob, and
Davis (1996). Rule-based systems were reported in Black and Yacoob, 1997,
Yacoob and Davis, 1996, in which primary facial features were tracked
throughout the image sequence.
Convolutional neural network (CNN) models (Le Cun & Bengio, 1995) as well as
neocognitrons (Fukushima, 1980) known as one of biologically inspired models,
have been used for pattern recognition tasks such as face recognition and
hand-written numeral recognition. The network includes feature detecting (FD)
layers, each of which alternated with a sub-sampling layer (pooling layer) to
obtain properties leading to translation and deformation invariance. Recently,
Fasel (2002) has proposed a model with two independent CNNs, one for facial
expression the other for face identity recognition, which are combined by a
MLP.
The proposed model in this study turns out to be much more efficient and
compact than Fasel's model. In addition, our model comes with the property of
subject independence. Specifically, the proposed system can detect smiling or
laughing faces based on difference in local features between a normal face and
those not.
In Section 2, we introduce a modular convolutional network architecture for
robust face detection and facial expression recognition involving module-based
learning based on a variant of BP. In Section 3, we propose a rule-based
algorithm that utilizes differences of specific local features, extracted in
the CNN, between neutral and emotional faces. We show that the proposed scheme
attains not only subject independence but also position independence in facial
expression recognition. In Section 4, we discuss the properties of proposed
scheme that adds to the conventional neural networks for facial expression
recognition.## 2\. Robust face detection using CNN
As in the previously proposed model (Matsugu, Mori, Ishii, & Mitarai, 2002),
internal representation of face is provided by a hierarchically ordered set of
convolutional kernels defined by the local receptive field of FD neurons. Face
model is represented as a spatially ordered set of local features of
intermediate complexity, such as eyes, mouth, nose, eyebrow, cheek, or else,
and all of these features are represented in terms of a fixed set of lower and
intermediate features.
The lower and intermediate features constitute some form of a fixed set of
figural alphabets in our CNN. Corresponding receptive fields for the detection
of these alphabetical features are learned in advance to form a local template
in the hierarchical network, and once learned, they would never be changed
during possible learning phase for object recognition in upper layers.
Our CNN model is different from the original model (Le Cun and Bengio, 1995)
in three ways. First, training of the proposed model proceeds module by module
(i.e. for each local feature class) only for FD _k_ (_k_ >1) layers. Second,
we do not train FP (or sub-sampling) layers (FP neurons perform either maximum
value detection or local averaging in their receptive fields). Third, we use a
detection result of skin color area as input to the face detection module in
FD4. The skin area is obtained simply by thresholding hue data of input image
in the range of [?0.078,0.255] for the full range of [?0.5,0.5].
The training proceeds as follows. In the first step of training, two layers
from the bottom, namely FD1 with eight modules and FD2 with four modules, are
trained using standard back-propagation with intermediate local features (e.g.
eye corners) as positive training data sets. Negative examples that do not
constitute the corresponding feature category are also used as false data.
Specifically, we trained the FD2 layer, the second from the bottom FD layer to
form detectors of intermediate features, such as end-stop structures or blobs
(i.e. end-stop structures for left and right side) and two types of
horizontally elongated blobs (e.g. upper part bright, lower part bright) with
varying size and rotation (up to 30° with rotation in-plane axis as well as
head axis). These data used for training are fragments extracted from face
images (Fig. 1).
1. Download: Download full-size image
Fig. 1. Convolutional architecture (feature pooling layers are not shown for
simplified illustration) for face detection.
More complex local feature detectors (e.g. eye, mouth detectors, but not
restricted to these) are trained in the third (FD3) or fourth (FD4) layer
using the patterns extracted from face images with geometric transform as in
the FD2 layer. As a result of these training sequences, the top FD layer, FD4,
learns to locate faces in complex scenes.
As given in Fig. 2, information concerning locations of eyes and mouth
detected by the CNN is fed to the rule-based processing module so that our
facial analysis system can deal with variability in position, size, and pose.
1. Download: Download full-size image
Fig. 2. CNN with feedback mechanism for rule-based analysis.
## 3\. Rule-based analysis of facial expressions using local features detected
by CNN
In the following, we propose a rule-based processing scheme to enhance subject
independence in facial expression recognition. We found that some of lower
level features extracted by the first FD layer of CNN were useful for facial
expression recognition. Primary features used in the analysis are horizontal
line segments and edge-like structures similar to step and roof edges
(extracted by the two modules in FP1 layer circled in Fig. 3) representingparts of eyes, mouth, and eyebrows. For example, changes in distance between
end-stops (e.g. left-corner of left eye and left side end-stop of mouth)
within facial components and changes in width of line segments in lower part
of eyes or cheeks are detected to obtain saliency scores of a specific facial
expression. Primary cues related to facial actions adopted in our rule-based
facial analysis for the detection of smiling/laughing faces are as follows.
* (1)
Distance between endpoints of eye and mouth gets _shorter_ (lip being raised)
* (2)
Length of horizontal line segment in mouth gets _longer_ (lip being stretched)
* (3)
Length of line segments in eye gets _longer_ (wrinkle around the tail of eye
gets longer)
* (4)
Gradient of line segment connecting the mid point and endpoint of mouth gets
_steeper_ (lip being raised)
* (5)
No. of step-edges in mouth get _increased_ (teeth get appeared)
* (6)
No. of edges in cheeks _increased_ (wrinkle around cheeks gets grown).
1. Download: Download full-size image
Fig. 3. Face detection by proposed convolutional NN with the results of
intermediate feature detection.
Each cue was scored based on the degree of positive changes (i.e. designated
changes as given above) to the emotional state (e.g. happiness). For example,
given a positive change in some cue by _a_ % to the feature value in neutral
face, then we give _Ca_ points (_C_ is some constant) to that feature.
Saliency score of specific emotional state is calculated using a sort of
voting scheme with the summation (with weight _p_ _k_) of scores _s_ _k_ for
respective cues given as followsS=?kpkSkAfter the voting process, the score
_S_ is normalized and thresholded for judging the facial expression (e.g.
smiling/laughing or not).
## 4\. Results
### 4.1. Face detection
In the training of our CNN, the number of facial fragment images used is 2900
for the FD2 layer, 5290 for the FD3, and 14,700 (face) for the FD4 layer,
respectively. The number of non-face images, also used for the FD4 layer, is
137. Apparently greater number of facial component images as compared with
non-face images is used to ensure robustness to varying rotation, size,
contrast, and color balance of face images. So, we used a fixed set of
transformed images for a given training sample image.
In particular, we used three different sizes of the fragment images ranging
from 0.7 to 1.5 times the original image (i.e. fragments of facial components
for modules in FD2 and FD3, entire face without background for FD4). The
performance for face images other than the training data set was tested for
over 200 face images with cluttered background and varying image-capturing
conditions.
As shown in Fig. 4, the tested images of face are of different size (from
30×30 to 240×240 in VGA image), pose (up to 30° in head axis rotation and also
in-plane rotation), and varying contrasts with cluttered background. The
convolutional network demonstrated robust face detection with 1% false
rejection rate and 6% false acceptance rate with quite good generalization
ability.
1. Download: Download full-size imageFig. 4. Face detection results in complex scenes.
### 4.2. Facial expression recognition
Fig. 5 shows a sequence of normalized saliency scores indicating successful
detection of smiling faces, around the frame number from 130 to 170, with an
appropriate threshold level of 2. The normalization was done by dividing the
weighted sum of scores by 10 and subtracting some constant. The weight for
each cue was assigned based on individuality factor, tendency of subject
dependence. So, cues of larger individuality, found heuristically in advance,
was assigned with some smaller weight values (e.g. 40), whereas cues of less
individuality was assigned with larger weights (e.g. 45).
1. Download: Download full-size image
Fig. 5. Normalized saliency score subtracted by constant value for smiling
face detection.
As shown in Fig. 6, the proposed system demonstrated the discrimination of
smiling from talking state for different persons based on the normalized
saliency score. In addition, we obtained results demonstrating reliable
detection of smiles with the recognition rate of 97.6% for 5600 still images
of more than 10 subjects.
1. Download: Download full-size image
Fig. 6. Talking faces can be discriminated from laughing face. Note that the
second face obtained higher score due to similarity to laughing face.
## 5\. Discussion
Robust face detection with invariance properties in terms of translation,
scale, and pose, inherent in our non-spiking version of CNN model (Matsugu,
2001, Matsugu et al., 2002) brings robustness to dynamical changes both in
head movements and in facial expressions. In particular, because of the
topographic property of our network preserving positional information of
respective facial features from the bottom to top layers, the translation
invariance in facial expression recognition is inherent in our convolutional
architecture.
The feedback mechanism from the top to intermediate modules is used to
identify corresponding facial features between neutral and emotional states.
Specifically, face location as detected by FD4 was used to confine the search
area of eye and mouth, and those intermediate facial features as detected in
FD3 layer were utilized for tracking primitive local features extracted by the
bottom layer FD1, which turned out to be useful for facial expression
recognition. Location information of eyes and mouth detected in the CNN are
thus transmitted, through the feedback loop from the FP3 layer to the rule-
based processing module, to confine the processing area of facial feature
analysis in the image based on differences in terms of at least six cues shown
in Section 3.
A great number of approaches (see Donato, Bartlett, Hager, Ekman, & Sejnowski,
1999 for review) have been taken for robustness in facial expression
recognition. However, most existing models do not have all of those properties
for robustness stated in the above, and many of them require explicit
estimation of motion parameters.
In contrast to a number of conventional methods, the proposed model for facial
expression recognition is much more compact and efficient with the following
distinct aspects.
* (1)
Our model requires only differences in local features, extracted by the CNN,
between a neutral face and emotional faces, instead of using differences
between adjacent frames.
* (2)For the facial analysis, we need only a single set of local features from one
neutral face as reference data.
* (3)
Unlike many other approaches, the proposed system requires no explicit
estimation of motion parameters over image sequences.
Additionally important feature of our model in terms of compactness is that we
need a single system of CNN, whereas the Fasel's model requires two CNNs in
tandem to obtain subject dependent facial expression recognition.
Conversely, it turned out that the proposed system is quite insensitive to
individuality of facial expressions mainly by virtue of the rule-based facial
analysis. Those cues exploited in the analysis undergo voting process with
weighted summation of scores for respective cues in terms of differences of
facial features in neutral and emotional states. As a result of this, voting
individuality is averaged out to obtain subject independence. In practice,
this subject independence is quite preferable since we can dispense with a
large database that contains individual facial expressions. However, Fasel's
model was not endowed with such property. We believe that it is easy to extend
the current framework to other tasks for subject independent facial expression
recognition. Although our rule-based facial analysis is useful as it is, we
may incorporate fuzzy rules to obtain more robust performance.
In conclusion, our model is the first facial expression recognition system
with subject independence combined with robustness with regard to variability
in face images in terms of appearance and location.
Special issue articlesRecommended articles"
211,213,Supervised committee of convolutional neural networks in automated facial expression analysis,"['G Pons', 'D Masip']",2017,119,"Affective Faces Database, MMI Facial Expression","CNN, classification, neural network",Learning could improve facial expression classification tasks.  application of deep CNNs to  facial expression classification [6].  In this experiment we describe this configuration as MMI+,No DOI,IEEE Transactions on Affective Computing,https://www.computer.org/csdl/journal/ta/2018/03/08039231/13rRUx0xPgA,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
212,214,Temporal multimodal fusion for video emotion classification in the wild,"['V Vielzeuf', 'S Pateux', 'F Jurie']",2017,209,Expression in-the-Wild,classification,EmotioNet Challenge: Recognition of facial expressions of emotion in the wild. arXiv preprint  arXiv: Emotion recognition in the wild with feature fusion and multiple kernel learning. In,No DOI,Proceedings of the 19th ACM International …,https://arxiv.org/abs/1709.07200,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
213,215,The Dartmouth Database of Children's Faces: Acquisition and validation of a new face stimulus set,"['KA Dalrymple', 'J Gomez', 'B Duchaine']",2013,165,Radboud Faces Database,classification,is comparable to rates from other published face databases [7]. Happy and Content were  the most accurately identified expressions; raters correctly classified 97.8% of the Happy (teeth,No DOI,PloS one,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3828408/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
214,216,The Indian spontaneous expression database for emotion recognition,"['SL Happy', 'P Patnaik', 'A Routray']",2015,120,Affective Faces Database,"classifier, facial expression recognition","facial expression recognition algorithms. In this paper, we propose and establish a new facial  expression database  face database including facial expression videos elicited by watching",No DOI,… on Affective Computing,https://arxiv.org/abs/1512.00932,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
215,217,The Karolinska directed emotional faces: a validation study,"['E Goeleven', 'R De Raedt', 'L Leyman']",2008,963,Karolinska Directed Emotional Faces,"classification, facial expression recognition","In this study, 490 pictures of human facial expressions from the Karolinska Directed Emotional  Faces database (KDEF; Lundqvist et al., Citation1998) were validated. The pictures were",No DOI,… and emotion,https://www.tandfonline.com/doi/abs/10.1080/02699930701626582,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
216,218,The MUG facial expression database,"['N Aifanti', 'C Papachristou']",2010,479,Affective Faces Database,facial expression recognition,face in human communication it is natural that much research is conducted on facial expression  recognition the six facial expressions are performed according to the ’emotion prototypes,No DOI,… Workshop on Image …,https://mug.ee.auth.gr/fed/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
217,219,The Toronto paper matching system: an automated paper-reviewer assignment system,"['L Charlin', 'R Zemel']",2013,170,Toronto Face Database,machine learning,We thank members of the Machine Learning group at the University of Toronto for their  help in developing the system (notably Hugo Larochelle and Amit Gruber) and for valuable,No DOI,,https://www.cs.toronto.edu/~lcharlin/papers/tpms.pdf,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
218,220,The elements of end-to-end deep face recognition: A survey of recent advances,"['H Du', 'H Shi', 'D Zeng', 'XP Zhang', 'T Mei']",2022,127,Toronto Face Database,deep learning,"In Section 5, we provide a review of deep learning-based methods for discriminative face   The images in PASCAL faces dataset [280] are taken from the Pascal person layout dataset [",No DOI,ACM Computing Surveys (CSUR …,https://arxiv.org/abs/2009.13290,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
219,221,The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression,"['P Lucey', 'JF Cohn', 'T Kanade', 'J Saragih']",2010,5135,Extended Cohn-Kanade,"classification, classifier, deep learning, machine learning, neural network","It involves computer vision, machine learning and behavioral sciences, and can be used  for many applications such as security [20], human-computer-interaction [23], driver safety [24],",No DOI,2010 ieee computer …,https://ieeexplore.ieee.org/document/5543262,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
220,222,The first facial expression recognition and analysis challenge,"['MF Valstar', 'B Jiang', 'M Mehu', 'M Pantic']",2011,452,Affective Faces Database,facial expression recognition,In section III we describe the challenge protocol for both the AU detection and emotion detection  sub-challenges. Section IV then describes the baseline method and the baseline results,No DOI,… & gesture recognition …,https://ieeexplore.ieee.org/document/5771374,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
221,223,The role of the amygdala in atypical gaze on emotional faces in autism spectrum disorders,"['D Kliemann', 'I Dziobek', 'A Hatri', 'J Baudewig']",2012,237,Karolinska Directed Emotional Faces,classification,"gaze in ASD we applied a facial emotion classification task, using eye tracking during  fMRI, varying the initial fixation position on faces. We hypothesized that individuals with ASD",No DOI,Journal of …,https://pubmed.ncbi.nlm.nih.gov/22787032/,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
222,224,Three convolutional neural network models for facial expression recognition in the wild,"['J Shao', 'Y Qian']",2019,181,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","CNN, FER, classification, deep learning, facial expression recognition, machine learning, neural network",Facial expressions in the wild have hundreds of thousands of variations referring to different   expressions are relatively easier to be recognized from an acted face than from a face in the,No DOI,Neurocomputing,https://www.sciencedirect.com/science/article/pii/S0925231219306137,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,"## Title: Three convolutional neural network models for facial
expression recognition in the wild
Search 
## Outline
1. Abstract
2. 3. Keywords
4. 1\. Introduction
5. 2\. Related work
6. 3\. Proposed method
7. 4\. Experimental results
8. 5\. Conclusions and future works
9. Conflict of interests
10. Acknowledgment
11. References
12. Vitae
Show full outline
## Cited by (149)
## Figures (8)
1. 2. 3. 4. 5. 6.
Show 2 more figures
## Tables (6)
1. Table 1
2. Table 2
3. Table 3
4. Table 4
5. Table 5
6. Table 6
## Neurocomputing
Volume 355, 25 August 2019, Pages 82-92
# Three convolutional neural network models for facial expression recognition
in the wild
Author links open overlay panelJie Shao, Yongsheng Qian
Show more
Outline
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.neucom.2019.05.005Get rights and content
## Abstract
Facial expression recognition (FER) in the wild is a novel and challenging
topic in the field of human emotion perception. Different kinds ofconvolutional neural network (CNN) approaches have been applied to this topic,
but few of them ever considered what kind of architecture was better for the
FER research. In this paper, we proposed three novel CNN models with different
architectures. The first one is a shallow network, named the Light-CNN, which
is a fully convolutional neural network consisting of six depthwise separable
residual convolution modules to solve the problem of complex topology and
over-fitting. The second one is a dual-branch CNN which extracts traditional
LBP features and deep learning features in parallel. The third one is a pre-
trained CNN which is designed by transfer learning technique to overcome the
shortage of training samples. Extensive evaluations on three popular datasets
(public CK+, multi-view BU-3DEF and FER2013 datasets) demonstrated that our
models were competitive and representative in the field of FER in the wild
research. We achieved significant better results with comparisons to plenty of
state-of-the-art approaches. Moreover, we provided discussions on the
effectiveness and practicability of CNNs with different feature types and
architectures for FER in the wild as well.
* Previous article in issue
* Next article in issue
## Keywords
FER in the wild
Convolutional neural network
Shallow network
Dual-branch CNN
Pretrained CNN
## 1\. Introduction
Emotion recognition by facial expression plays an important role in
intelligent social interaction. It is widely used in intelligent security [1],
robotics manufacturing [2], clinical psychology [3], multimedia [4] and
automotive security [5]. In most above-mentioned applications, their inputs
are faces captured in the real world. Nevertheless, the main contributions of
traditional facial expression recognition methods focused on expressions of
the frontal faces. Those expressions were performed by actors staying in a
controlled environment. As a result, facial expression recognition in the wild
is a novel and challenging topic due to various poses, illumination changes,
occlusions, and subtle expressions, etc.
Previous traditional methods for FER in the wild mainly focused on modeling
features, e.g., Zhong et al. [6] built a two-stage multi-task sparse learning
framework to discriminate facial patches. Zheng et al. [7] treated the feature
extraction problem as a convex optimization problem. They both applied
traditional state-of-the-art classifier for the final recognition. However,
they made good performance on frontal faces, leaving much to be desired on
non-frontal faces.
In recent years, machine learning techniques of convolutional neural networks
have achieved great success in the field of computer vision. It is wildly used
in the fields of visual object recognition [8], Natural Language Processing
[9], driverless [10], and so on. It is also a promising approach for the
research of FER. Different from traditional techniques, convolutional neural
networks can perform tasks in an end-to-end way, associating both feature
extraction and classification steps together by training. However, there are
still some problems existing in the development of deep learning network:
First, typically the efficiency of a CNN is improved by increasing the number
of neurons or the number of layers, so that the network is hoped to learn more
complex functions. For example, the early network AlexNet has 7 layers. Then
the VGG model with 16 layers appeared, followed by the GoogLeNet consisting of22 layers. Later came the ResNet model with 152 layers, and the modified
ResNet even includes thousands of layers. Although the network?s performance
has been improved, their efficiency issues appeared, namely the storage
problem of the model and the speed of prediction [11]. Secondly, it may not be
robust for the deep CNN to extract features from images with low-resolution,
high noise and various rotational changes [12], [13]. The third is the data
problem. The deeper the CNN is, the more weights there need to be determined.
Consequently, the network needs to be fed with thousands of samples in a
larger database, then it could acquire better performance. However, it is
impossible to provide large-scale samples in every application area. It means
that the shallow CNNs may have better performance in various industrial
applications than the deeper ones.
Referring to the first problem we mentioned above, increasing the depth brings
a series of negative issues such as overfitting, gradients disappearance, and
enormous computational costs. A possible solution to this problem is to create
deep sparsely compressed network. Gao Et al. [14] proposed a feed-forward
approach to build connections directly between each pair of convolutional
layers to form a dense convolutional network (DenseNet). It made use of the
short connections between the input layer and the layer close to the output to
make the convolutional network deeper, so that the training process would be
more precise and more effective. Unfortunately, most current GPUs and CPUs are
not able to efficiently run sparse network model [15]. Therefore, in the paper
we propose a shallow CNN with good performance on facial expression
recognition in the wild. So that it would be suitable for practical problems
currently.
At present, most CNN models for facial expression recognition use the features
generated by the convolution layers using the raw pixel data as the main
features. Local Binary Pattern (Local Binary Pattern, LBP) is a texture
description operator which is usually used for facial expression recognition.
It can effectively adapt to changes in illumination and local rotation [16].
Features extracted by convolutional neural network may not be robust to the
image rotation changes. We wanted to explore if there was any way to apply LBP
features along with raw pixels to a network and observe the performance of the
model when it had a combination of two different features.
The data problem, the third one we mentioned above is the most troublesome
problem we met. Facial expressions in the wild have hundreds of thousands of
variations referring to different poses, human races, genders, conventions and
environments. On the contrary, datasets of facial expression in the wild are
quite limited. Some datasets only have hundreds of samples, so it is difficult
for deeper CNN models to learn as good results as they have in some other
fields. A study on transfer learning of facial expression recognition seems to
offer a better chance of producing more accurate predictions [17], [18], [19].
Based on the above discussion, in this paper, we proposed three kinds of
convolutional neural networks for facial expression recognition in the wild.
The first one is a shallow CNN named Light-CNN. The second one is a dual-
branch CNN, which is an attempt to integrate traditional features with the
original data in a uniform network. The third method is a pre-trained CNN,
which is a deep network. We elaborated their architectures and conducted
comprehensive experiments on three public facial expression datasets: CK+,
BU-3DFE and FER2013. Plenty of comparisons were made among our three CNNs and
other state-of-the-art methods. We demonstrated that our proposed methods are
significantly better than the previous methods. Meanwhile, we also provided a
discussion on the merits and shortcomings of our three network architectures.
Our contributions are as follows:* ?
We elaborately designed three representative CNN models for facial expression
recognition in the wild, in order to discuss their advantages and
disadvantages, and to provide possible solutions for problems of over-fitting,
high computational complexity, and lack of training samples et al. in FER in
the wild by deep learning.
* ?
A large number of experiments are implemented on different facial expression
datasets, including CK+, BU-3DFE and FER2013. CK+ is a traditional facial
expression dataset. BU-3DFE has samples with different poses but captured in a
lab-controlled environment. FER2013 includes face samples captured in the real
world.
* ?
We made comparisons among the three CNN architectures, as well as the
comparison between our three methods and the state-of-the-art methods. We
provide conclusions about different network structures and demonstrated that
our proposed methods are competitive with state-of-the-art methods.
The structure of this paper is as follows: Section 2 introduces existing
state-of-the-art emotion recognition approaches based on CNN, and some
traditional feature extraction methods. We introduce our CNNs in details in
Section 3. Section 4 describes the datasets along with details of our
experiments, and then we present our results and discussions. Section 5 give a
conclusion followed by a list of references.
## 2\. Related work
Traditional methods on FER can be categorized into three major steps: facial
detection, feature extraction and classification, where face detection [20],
[21], [22] has become a well-developed technology and been applied to the
real-world applications. Extracting powerful features and designing effective
classifiers are two key components of FER. For feature-based methods, hand-
crafted features are often used to represent expression images. For example,
Gabor wavelets [23] show good robustness through capturing image edges at
different scales and orientations. Local binary pattern (LBP) is demonstrated
to be useful in FER. Ying et al. [24] proposed a facial expression recognition
method based on LBP and Adaboost in 2008. LBP was later extended for modeling
spatio-temporal features, naming LBP-TOP [25]. Later, Qi et al. [26] proposed
a new expression recognition method based on cognitive and mapping binary
patterns. They applied pseudo-3D model to segment face areas into six facial
sub-regions. Although the LBP operator is robust to monotonic gray-level
changes and computational efficiency, there are some limitations. For example,
it is sensitive to noise, and in its template, only gradients between the
central pixel and its neighborhood are considered. Thus, it inevitably loses
some information [27]. Other features, including Histograms of Oriented
Gradients (HOG) [28], Scale-Invariant Feature Transform (SIFT), and Singular
Value Decomposition (SVD) [29] have also been widely used. In [30], [31],
proved that the singular value of the image can be used as the global feature
with invariant scale of rotation shift. These special attributes of the
singular value are used to design the compact global feature of facial image
representation to improve the accuracy of low-resolution face recognition.
Facial expression images in the wild are more challenging in face detection,
facial landmark location, and pose standardization than traditional facial
expression images. Consequently, traditional methods are not suitable for the
research on FER in the wild.
In recent years, the appearance of deep learning has significantly improved
the performance of FER related tasks [32], [33], [34], [35], [36]. Then therewere two trends. On the one hand, the FER problem increasingly utilized deeper
and deeper neural networks to improve the ability of tackling big-data
problems. Mollahossein et al. [32] proposed an in-depth neural network
architecture for FER, which was inspired by GoogLeNet and AlexNet. It
outperformed traditional methods based on hand-crafted features. Training deep
networks with limited data may even result in poor performance due to over-
fitting. To solve the problem, Zhang et al. [34] proposed a deep neural
network (DNN) with the SIFT feature, which achieved the accuracy of 78.9% on
the multi-view BU-3DFE dataset. To reduce the influence of various head poses,
Jung et al. [35] proposed a jointly CNNs with facial landmarks and color
images, which achieved the accuracy of 72.5%, but the network consisted of
only three convolutional layers and two hidden layers, making it be difficult
to accurately learn facial features. Lopes et al. [36] proposed a combination
of Convolutional Neural Network and special image pre-processing steps (C-CNN)
to recognize six expressions under head pose at 0?, whose accuracy was 90.96%
on the BU-3DFE dataset. Its robustness was unknown under different head poses.
On the other hand, some works preferred to aggregate different features in
deep networks. They demonstrated that comprehensive feature representations
had better performance than single feature. For example, Majumder et al. [37]
fused LBP features and facial geometric features with a deep network-based
technique for FER in the wild, and achieved good performance. Hamester et al.
[38] proposed a new architecture by constructing a multi-channel convolutional
neural network (MCCNN). It utilized CNN and an automatic encoder to extract
features. On the contrary, Alizadeh et al. [39] claimed that hybrid feature
sets did not help in improving the model accuracy. Therefore, we attempt to
provide a dual-branch model solution in this paper which includes both
traditional texture features and raw data.
Lack of training samples is a big problem for FER in the wild using deep CNNs.
To solve this problem, some methods used pre-trained network for
classification or re-trained a network model to re-initialize the weights for
new datasets [40]. The techniques are regarded as ?transfer learning?. Ruiz-
Garcia et al. [41] used greedy layer-wise fashion to pre-train deep CNNs as a
stacked convolution auto-encoder (SCAE) for emotion recognition. Employing
SCAE as a pre-training model improves not only performance but training time.
Yanai et al. [42] sought a good combination of DCNN-related techniques. The
fine-tuning and activation features were extracted from the pre-trained DCNN.
In addition to its high classification accuracy, DCNN was very suitable for
large-scale image data.
## 3\. Proposed method
In this section, the proposed three CNNs: a Light-CNN, a dual-branch CNN and a
Pretrained CNN are described in details.
### 3.1. The Light-CNN
The Light-CNN is a shallow CNN, its architecture is shown in Fig. 1. It is a
fully convolutional neural network. It consists of 6 depthwise separable
residual convolution modules whose architectures are shown in Fig. 2. The
architecture of the module was inspired by the Xception and ResNet. We
associated the depthwise separable module with the residual network module to
build a depthwise separable residual convolution module. The depthwise
separable residual convolution module has three separable convolution layers
(SeparableConv2D) and one convolution layer. In the first SeparableConv2D
layer, we had 16 1 × 1 filters along with batch normalization, but without max
pooling. In the second SeparableConv2D layer, we had 16 3 × 3 filters along
with batch normalization, but without max pooling as well. In the third
SeparableConv2D layer, we had 16 1 × 1 filters along with batch normalization,as well as max pooling with a filter of size 2 × 2. The number of filters
gradually increases from 16 to 512 in 6 modules, as shown in Fig. 2. Each
depthwise separable residual convolution module is followed by a Rectified
Linear unit (ReLU). The images are resized to be 64 × 64 × 1 pixels before
being sent to the network. In the first and the second convolutional layer, we
have 8 3 × 3 filters respectively, with the stride of size 1, along with batch
normalization and ReLU. They extract low-level edge features of the image and
retain the details. The low-level edge features are shown in Fig. 3-a. The
deep features of the extracted image from first depthwise separable residual
convolution modules are shown in Fig. 3-b. It can be found that the deeper it
is, the more abstract the output features are.
1. Download: Download high-res image (800KB)
2. Download: Download full-size image
Fig. 1. The basic structure of the Light-CNN.
1. Download: Download high-res image (2MB)
2. Download: Download full-size image
Fig. 2. The structure of 6 depthwise separable residual convolution modules.
1. Download: Download high-res image (376KB)
2. Download: Download full-size image
Fig. 3. Feature visualization of the image.
After 6 depth wise separable residual convolution modules, we designed a
convolutional layer following with a global average pooling layer to reduce
the number of features, and to regularize the entire network to prevent
overfitting. The output of the layer is a vector whose dimension is the number
of expressions. A softmax layer is at the bottom, which is a generalization of
the logistic regression model for multi-classification problems. In the multi-
classification problem, k possibilities are predicted (k is the number of
sample tags). Assume that the input feature is x(i)??n+1, and the sample tag
is _y_(_i_), so the training set S={(x(1),y(1)),(x(2),y(2)),?,(x(m),y(m))} of
the supervised learning constitutes the classification layer. Then the
function and cost function forms are as
follows:(1)h?(x(i))=[p(y(i)=1|x(i);?)p(y(i)=2|x(i);?)?p(y(i)=k|x(i);?)]=1?j=1ke?jTx(i)[e?1Tx(i)e?2Tx(i)?e?kTx(i)]
Where ?1,?2,?,?k??n+1 is the model parameter and 1?j=1ke?jTx(i) is the
normalization term for the probability distribution, making the sum of all
probabilities equal to
1.(2)J(?)=?1m[?i=1m?j=1k1{y(i)=j}ln?jTx(i)?l=1ke?lTx(i)]
Among them, 1{} = 1 is an indicative function whose value rule is: when the
expression in the curly braces is true, the result of the function is 1,
otherwise the result is 0.
### 3.2. The dual-branch CNN
The dual-branch CNN is designed to simultaneously estimate the global features
and local texture features. Fig. 4 illustrates its flowchart. The architecture
consists of three modules: two individual CNN branch modules and a fusion
module. The first branch takes the entire image as input and extract global
features. The other branch takes the texture feature image preprocessed by LBP
as input. Finally, the third module is a fusion network that takes as input
the global and texture features. The global feature is intended to represent
the integrity of the expression, while the texture feature focuses on the
details of the description of the local area, which can directly indicate some
active expression areas on the face. These two separate branches represent
expressions from two different aspects. They are complementary and both are of
interest.
1. Download: Download high-res image (533KB)
2. Download: Download full-size imageFig. 4. Framework of the dual-branch CNN.
The Light-CNN, we introduced in the previous sub-section, is truncated to
apply for the first branch. The dimensional reduction CNN for the second
branch consists of two Convolutional layers. It reduces the dimension of LBP
features and facilitates the following combination of the two features. The
architecture details of the two branches are shown in Table I. We omitted the
architecture details between layer1 and layer28 of the first branch in Table
1.
Table 1. Architectures of the two branches.
The first branch| Empty Cell| Layer1| ?| Layer28| Layer29
---|---|---|---|---|---
Empty Cell| type| Input| ?| GlobalAveragePooling2D| Flatten
Empty Cell| size| 224 × 224 × 1| ?| 4096| 4096
The second branch| | Layer1| Layer2| Layer3| Layer4| Layer5| Layer6
| type| Input| Conv2D| Max Pooling| Conv2D| Max Pooling| Flatten
| size| 64 × 64 × 1| 4 × 4 × 32| 2 × 2| 4 × 4 × 16| 2 × 2| 2704
### 3.3. The pretrained CNN
To observe the effect of deeper CNN, the ResNet101[43] network was exploited
to construct our pretrained network model. As we didn?t have big databases to
train the network, we directly used the model which was previously trained by
ImageNet [44] dataset. ImageNet has thousands of different face images, so we
could retain the most original network parameters for initialization. Then we
trained the model and performed fine-tuning on some of the layers to extract
more specific features. Fig. 5 shows the architecture of the pretrained CNN.
The original network consists of five convolution modules. Then average
pooling is followed by a flatten layer. The output of the full connection
layer is 1000. We modified the full connection layer from 1000 to 6 or 7,
according to the number of expression categories.
1. Download: Download high-res image (497KB)
2. Download: Download full-size image
Fig. 5. The framework of the pretrained CNN.
## 4\. Experimental results
We evaluated the proposed methods on three publicly available facial
expression datasets. Some image samples are shown in Fig. 6. Images from the
CK+ Database are in the top row. Images from the BU-3DFE Database are in the
middle row. Images from the FER2013 Database are in the bottom row. The
experimental details will be described in this section.
1. Download: Download high-res image (851KB)
2. Download: Download full-size image
Fig. 6. Examples Images in CK+(top), BU-3DFE (middle), FER2013 (bottom)
Datasets.
### 4.1. Databases and Protocols
_CK+ Database:_ The Extended Cohn-Kanade (CK+) database [45] includes 593
facial expression video sequences recorded from 123 subjects ranging from 18
to 30 years old in lab-controlled environment. Most are frontal faces. We only
retained the final frames with peak expression of video sequences in our
experiments. Totally we got 327 static expression images with seven emotion
labels (anger, contempt, disgust, fear, happy, sadness, surprise). We divided
the CK+ dataset into a training set with 90% samples and a validation set with
the other 10% samples.
_BU-3DFE Database:_ The BU-3DFE multi-view facial expression database [46]
contains 100 subjects of different ethnicities, including 56 females and 44
males. Six facial expressions (anger, disgust, fear, happiness, sadness, and
surprise) are elicited by various manners and head poses. Each of themincludes 4 levels of intensities. The images are also captured in the lab-
controlled environment. These models are comprised by both 3D geometrical
shapes and color textures with 83 feature points. We use 3D facial models to
restore 2D facial images of multiple viewing angles (0?, 30?, 45?, 60? and
90?). We divided the dataset into a training set with 90% samples and a
validation set with the other 10% samples as well.
_FER2013 database:_ The FER2013 dataset [47] is a static real world facial
expression database, which consists of 35,887 48 × 48 gray face images. The
image is processed in such a way that the face is centered and the occupancy
of each face in the image is approximately the same. Each image is divided
into one of seven categories that express different facial emotions. These
facial emotions have been categorized as: anger, disgust, fear, happy, sad,
surprise and neutral. Besides the image category, images are divided into
three different sets, a training set, a validation set, and a test set. There
are approximately 29,000 training images, 4,000 verification images and 4000
images for testing. For the purpose of data enhancement, we make a mirror
image by horizontally flipping the image in the training set.
### 4.2. Experimental parameters
The experimental platform consists of AMD Ryzen 5 1600(6 × 3.2 GHz processor),
16GB memory, GTX1080 and Ubuntu 16.04 operation system. The deep learning
framework Keras is exploited. The parameter settings of the Light-CNN, the
dual-branch CNN and the pretrained CNN are presented in Table 2.
Table 2. The parameter setting of Light-CNN, Dual-Branch Network and Pre-
trained network.
Models| Parameters| Values
---|---|---
The Light-CNN| Optimizer| Adam
| Image size| 224 × 224
The dual-branch CNN| Optimizer| SGD
| Learning rate| 1e?3
| Momentum| 0.9
| Learning decaying factor| 1e?6
| Image size| 224 × 224
The pretrained CNN| Optimizer| SGD
| Learning rate| 1e-3
| Momentum| 0.9
| Learning decaying factor| 1e?6
| Image size| 224 × 224
In preprocessing, we applied Multi-Task Convolutional Neural Network (MT-
CNN)[48] for face detection. Then, the cropped face and five facial landmarks
were detected. The five landmarks indicate the centers of two eyes, the end of
the nose and two corners of the mouth. All face images were resized to 224 ×
224 pixels and aligned based on three landmarks (two center points of eyes and
the center point of mouth). In the dual-branch network, LBP feature images
were resized to 64 × 64 pixels.
For image enhancement, we used a series of random transformations to ?enhance?
the image so that the model would not be fed with two identical images [49].
It would effectively improve image utilization. The transformations included
rotation, flipping, scaling, and panning. In this paper, the width and height
displacement were used. The shifting range of width and height were set under
20%. The random rotation range was 0?20?. Both the shear range and the zoom
range were [0?0.1]. We flipped the images horizontally and applied the fill
pattern strategy to fill the newly created pixels as well.
### 4.3. Experiments on three popular datasetsWe tested our methods on three widely used FER datasets: CK+, BU-3DFE and
FER2013. The CK+ dataset includes expressions of seven labels: anger,
contempt,disgust, fear, happy, sadness, surprise. The BU-3DFE dataset has six
labels: anger, disgust, fear, happiness, sadness, and surprise. The FER2013
dataset has seven labels: anger, disgust, fear, happy, sad, surprise and
neutral.
To evaluate the overall performance, the confusion matrices of our methods on
three datasets are illustrated in Fig. 7. Fig. 7(a)?(c) are experimental
results on CK+, implemented by the Light-CNN, the dual-branch CNN and the
pretrained CNN respectively. Fig. 7(d)?(f) are experimental results on BU-3DFE
dataset with three models. Fig. 7(g)?(i) are experimental results on FER2013.
As demonstrated in these figures, the pretrained CNN resulted in higher
accuracy for most of the labels. All the three models performed well on CK+
datasets especially the Light-CNN and the pretrained CNN, as CK+ is a dataset
with facial expression samples captured in a lab-controlled environment. It is
interesting to see that the happy label has the highest accuracy in CK+ and
FER2013 datasets, which implies that the features of a happy face are more
distinguishable than other expressions. Besides, the sadness and the surprise
expressions are relatively easier to be recognized from an acted face than
from a face in the real world. Because the sad and surprise labels have high
accuracy on CK+ and BU-3DFE datasets, but fail to be good on FER2013. Their
matrices also reveal which labels are likely to be confused by the trained
networks. For example, we can see the correlation of angry label with the fear
and surprise labels. There are lots of instances that their true label is
angry but the classifier has misclassified them as fear or surprise. These
mistakes are consistent with what we see when looking at images in the
dataset; even as a human, it can be difficult to recognize whether an angry
expression is actually surprise or angry. This is due to the fact that people
do not all express emotions in the same way.
1. Download: Download high-res image (2MB)
2. Download: Download full-size image
Fig. 7. Confusion matrices for three networks on three expression databases.
(a)?(c) are confusion matrices for the Light-CNNs, dual-branch CNN and the
pretrained CNN on CK+, (d)?(f) are confusion matrices for the three networks
on BU-3DFE, (g)?(i) are confusion matrices for the three networks on FER2013.
Moreover, we plotted the obtained accuracy of FER2013 using the Light-CNN, the
dual-branch CNN and the pretrained CNN during epochs in Fig. 8. As seen in
Fig. 8, the pretrained CNN has the best validation accuracy. The performance
of the Light-CNN is close to the best one. Furthermore, one can observe that
the Light-CNN has less overfitting behavior than the others. We also provided
the number of parameters in networks and their running time on FER2013 for
comparison in Table 3. The Light-CNN has the least parameters, and it runs
much faster than the others. By integrating the results shown in Fig. 8 and
Table 3, we concluded that LBP features were not helpful in deep network. With
the development of the architecture, CNNs adopting raw pixel data is strong
enough to extract sufficient information for facial expression in the wild.
Besides, the Light-CNN got good scores on all three datasets and its
performances are quite close to those of the pretrained CNN. Besides, it runs
much faster than the pretrained CNN, which is beneficial for practical
applications.
1. Download: Download high-res image (425KB)
2. Download: Download full-size image
Fig. 8. Comparison of parameters and running time on FER2013 dataset.
Table 3. Comparison of parameters in three networks and their running time onFER2013 dataset.
Models| Parameters| Running time (h)
---|---|---
The Light-CNN| 1,108,151| 12.7
The dual-branch CNN| 64,629,847| 23.3
The pretrained CNN| 7,128,327| 18.8
### 4.4. Comparisons with the state-of-the-art methods
To evaluate the performance of the proposed algorithm with other algorithms,
Table 4 and 5 list the accuracy of our proposed and the state-of-the-art
algorithms on the CK+ and BU-3DFE databases. LBP, HOG and Gabor filters are
traditional feature descriptors in facial expression recognition and have been
widely used. However, the recognition accuracy of most traditional methods are
lower than that of deep learning.
Table 4. The accuracy (%) of different methods on CK+ dataset.
Methods| # of Expression| Accuracy(%)
---|---|---
LBP [16]| 6+neutral| 87.20
HOG [28]| 6+neutral| 89.70
Gabor filter [50]| 7| 84.80
Poursaberi et al. [51]| 6| 92.02
Deepak Ghimire [52]| 6| 94.10
AU-DNN [33]| 6+neutral| 92.05
JFDNN [35]| 6| 97.3
CNN [32]| 6| 93.2(Top-1)
C-CNN [36]| 6| 91.64
**Our Light-CNN**| **7**| **92.86**
**Our dual-branch CNN**| **7**| **85.71**
**Our pre-trained CNN**| **7**| **95.29**
For the CK+ database, the accuracy of our algorithm is superior to most of the
other advanced algorithms. The best performance of the existing deep learning
methods is 97.3%, which is achieved by Jung [35]. His network consists of
three convolutional layers and two hidden layers. The filter size in the three
convolutional layers is 5 × 5, and the numbers of hidden nodes is set to 100
and 600 respectively. But his results declined to 92.35% without joint fine-
tuning. By contrast our proposed method does not use any geometric features or
temporal video information, and improved the accuracy to 95.29% under seven
expressions.
The comparison results on BU-3DFE dataset are shown in Table 5, the accuracy
of method[53] based on HOG is 54.64%. The accuracy of multi-class SVM with LBP
and LGBP in [54] is 71.1%. Dapogny et. al. [55] proposed PCRF to capture low-
level expression transition patterns on the condition of head pose estimation
for multi-view dynamic facial expression recognition. Their average accuracy
reached 76.1%. The JFDNN [35] reaches only 72.5%, which used to get the best
result in CK+ dataset. The higher accuracies are achieved with SIFT feature
using GSRRR and DNN-Driven methods proposed in [56] and [34], which are 78.9%
and 80.1%, respectively. In addition, Lopes et al. [36] used intensity
features to recognize six expressions with frontal poses and achieved an
average accuracy of 90.96%. Our best result reaches 86.5%, which is
competitive with the above method.
Table 5. Accuracy (%) using different methods on BU-3DFE dataset.
Methods| Poses| Accuracy (%)
---|---|---
HOG [53]| 5| 54.64
LBP and LGBP [54]| 7| 71.1JFDNN [35]| 5| 72.5
PCRF [55]| 5| 76.1
CGPR [57]| 5| 76.5
GSRRR [56]| 5| 78.90
DNN-Driven [34]| 5| 80.10
C-CNN [36]| 1 (frontal)| 90.96
**Our Light-CNN**| **5**| **86.20**
**Our dual-branch CNN**| **5**| **48.17**
**Our pre-trained CNN**| **5**| **86.50**
Besides, Table 6 shows the results achieved by the competing methods on the
FER2013 database, which is the most challenging database in our experiment.
There was a leaderboard of facial expression recognition challenge on FER2013
dataset. The number one method is the RBM. Our Light-CNN model achieved the
accuracy of 68%, which is ranked #5 in the list, and the pretrained model
ranked #2 among all the participating teams. It has almost the same accuracy
with the first team.
Table 6. Accuracy (%) using different methods on FER2013 dataset.
Methods| Accuracy (%)
---|---
RBM [58]| 71.16
Kim et al. [59]| 70.58
Jeon et al. [60]| 70.47
Devries et al. [61]| 67.21
CNN [32]| 66.4 (Top-1)
Liu et al. [62]| 65.03
Shen et al. [63]| 61.86
Ergen et al. [64]| 57.10
**Our Light-CNN**| **68**
**Our dual-branch CNN**| **54.64**
**Our pre-trained CNN**| **71.14**
### 4.5. Discussion
Above all, Our CNN models achieved state-of-the-art performance without using
additional training data or functions, comprehensive data enhancement or
facial registration. It is predictable that it will success in processing
larger database in the future. Under the same conditions, the performance of
deeper pre-trained CNN was better than the others. Our experimental results
demonstrated the potential to significantly improve FER performance using pre-
trained deep network structures, which could solve the problems of the lack of
training samples and over-fitting. The Light-CNN overcome the challenge of
overfitting, and kept good performance in all the popular FER datasets (see
Table 3) as well. In addition, in our dual-branch CNN, learning features and
manual features were put into the final fusion layers to explore whether the
combination of features can improve the classification effect. The results
showed that the effect of learning deep features was not improved under the
guidance of traditional features.
## 5\. Conclusions and future works
We developed three CNN models for facial expression recognition in the wild
and evaluated their performances using different analyzing and visualization
techniques. The results demonstrated that the deeper model has better
performance on facial feature learning and emotion classification. However,
the experiments implemented by the Light-CNN proved that a shallow CNN could
also achieve good scores in facial expression recognition in the wild. In
addition, mixing feature sets do not help to improve accuracy, which means
that convolutional neutral networks can learn key facial features simply byusing raw pixel data. In future work, we will use more efficient hand-crafted
features to join our dual-branch CNN and change the fusion mode. Moreover, we
will use cross-database training network parameters to get better
generalization capabilities.
## Conflict of interests
The authors declare that there is no conflict of interests regarding the
publication of this paper.
## Acknowledgment
This work was supported by the National Natural Science Foundation of China
Youth Science Foundation project (Nos. 61802250, 61401268) and Shanghai
Natural Science Foundation of China (15ZR1418400).
Recommended articles"
223,225,Tied factor analysis for face recognition across large pose differences,"['SJD Prince', 'JH Elder', 'J Warrell']",2008,272,Toronto Face Database,machine learning,in face data across different poses. Our model was applied to both face identification and   The system described here is a pure machine learning approach that knows very little about,No DOI,… machine intelligence,https://ieeexplore.ieee.org/document/4459336,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
224,226,Torontocity: Seeing the world with a million eyes,"['S Wang', 'M Bai', 'G Mattyus', 'H Chu', 'W Luo']",2016,210,Toronto Face Database,"classification, neural network",", which covers the full greater Toronto area (GTA) with 712.5 semantic labeling and scene  type classification (recognition).  , tree detection and tree species classification as well as traffic",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1612.00423,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
225,227,Training deep neural networks on noisy labels with bootstrapping,"['S Reed', 'H Lee', 'D Anguelov', 'C Szegedy']",2014,1181,Toronto Face Database,"deep learning, neural network","On the Toronto Face Database, we show that our model  can also benefit from unlabeled  face images with no modification  we train a deep neural network with our proposed consistency",No DOI,arXiv preprint arXiv …,https://arxiv.org/abs/1412.6596,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
226,228,Unsupervised face normalization with extreme pose and expression in the wild,"['Y Qian', 'W Deng', 'J Hu']",2019,112,Expression in-the-Wild,machine learning,"in the wild with unpaired data. To this end, we propose a Face Normalization Model (FNM) to  generate a frontal, neutral expression Our FNM is an end-to-end deep learning model. FNM",No DOI,… of the IEEE/CVF Conference on …,https://openaccess.thecvf.com/content_CVPR_2019/papers/Qian_Unsupervised_Face_Normalization_With_Extreme_Pose_and_Expression_in_the_CVPR_2019_paper.pdf,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
227,229,Using colour local binary pattern features for face recognition,"['JY Choi', 'KN Plataniotis', 'YM Ro']",2010,103,Toronto Face Database,classification,"of a face image for FR purpose. We evaluate the proposed feature using three public face  databases: CMU-PIE,  As in any classification task, feature extraction is of prime importance in",No DOI,2010 IEEE International …,https://ieeexplore.ieee.org/document/5653653,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
228,230,Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing,"['TP Jung', 'TJ Sejnowski']",2019,228,Affective Faces Database,"deep learning, machine learning","For each dataset, we first individually evaluate the emotion-classification performance   , we utilized these networks on face-images using a deep network pretrained on VGG-faces",No DOI,IEEE Transactions on Affective …,https://arxiv.org/abs/1905.07039,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,
229,231,Video and image based emotion recognition challenges in the wild: Emotiw 2015,"['A Dhall', 'OV Ramana Murthy', 'R Goecke']",2015,384,"Acted Facial Expressions In The Wild, Expression in-the-Wild, Static Facial Expression in the Wild","classification, classifier, deep learning, facial expression recognition, machine learning","For the VReco sub-challenge, the facial features should  emotion recognition method on  the Acted Facial Expressions in the Wild database 5.0 and single image based facial expression",No DOI,Proceedings of the …,https://dl.acm.org/doi/10.1145/2818346.2829994,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,
