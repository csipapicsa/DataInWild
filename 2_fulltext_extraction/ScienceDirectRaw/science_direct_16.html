<html lang="en-US" data-sd-ui-what-input="mouse"><head>
      <meta name="citation_pii" content="S0020025521010136">
<meta name="citation_issn" content="0020-0255">
<meta name="citation_volume" content="582">
<meta name="citation_lastpage" content="617">
<meta name="citation_publisher" content="Elsevier">
<meta name="citation_firstpage" content="593">
<meta name="citation_journal_title" content="Information Sciences">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.ins.2021.10.005">
<meta name="dc.identifier" content="10.1016/j.ins.2021.10.005">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper ha…">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521X0028X-cov150h.gif">
<meta name="citation_title" content="A survey on facial emotion recognition techniques: A state-of-the-art literature review">
<meta property="og:title" content="A survey on facial emotion recognition techniques: A state-of-the-art literature review">
<meta name="citation_publication_date" content="2022/01/01">
<meta name="citation_online_date" content="2021/10/07">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
      <title>A survey on facial emotion recognition techniques: A state-of-the-art literature review - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0020025521010136">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team">
      <script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/arp.css">
      <link href="//cdn.pendo.io" rel="dns-prefetch">
      <link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com">
      <script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S0020025521010136?componentVersion=V11&amp;jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwMDIwMDI1NTIxMDEwMTM2IiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiNWUwYThhMzI3ZjI3YTA0OGE4NWIyM2E2YTM3MTU2NmQ2OThkZ3hycWEiLCJleHAiOjE3MzAyMDU2NDAsImlhdCI6MTczMDIwMzg0MCwidmVyc2lvbiI6MSwianRpIjoiN2MzOWEyYzYtMjc4My00NTY4LWFjZTQtMTY1ZDAxMTRmZmY4In0.nNUEcP0irNh-RyDi9bTjiv9x77In_x9sMH54Ms6HaDk" type="text/javascript"></script>
      <script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"0799754C726C61C2-1D50FE17B3C20494","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Information Sciences","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
    <script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement.min.js" async=""></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement_Module_ActivityMap.min.js" async=""></script><script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/6a62c1bc1779/RCa16d232f95a944c0aabdea6621a2ef94-source.min.js" async=""></script><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202410240101/pubads_impl.js?cb=31088506" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link id="plx-css-summary" type="text/css" rel="stylesheet" href="//cdn.plu.mx/summary.css"><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//cdn.plu.mx/extjs/xss.js"></script><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style data-styled="active" data-styled-version="6.1.13"></style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:700;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:700}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none!important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0,0,0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none!important}.onetrust-pc-dark-filter.ot-hide{display:none!important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width:426px)and (max-width:896px)and (orientation:landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:400;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:0 0;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width:400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width:550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width:550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:transparent;border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style><link type="text/css" rel="stylesheet" href="https://pendo-static-5661679399600128.storage.googleapis.com/guide.-323232.1721046486120.css" id="_pendo-css_"><style type="text/css" scoped="scoped" class=" pendo-style-D_T2uHq_M1r-XQq8htU6Z3GjHfE" style="white-space: pre-wrap;"></style><style type="text/css">.MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
</style></head>
    <body data-sd-ui-layer-boundary="true" class="toolbar-stuck" style=""><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_SVG_Hidden"></div><svg><defs id="MathJax_SVG_glyphs"><path stroke-width="1" id="MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="1" id="MJMAIN-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path stroke-width="1" id="MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="1" id="MJMAIN-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path stroke-width="1" id="MJMAIN-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path stroke-width="1" id="MJMAIN-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width="1" id="MJMAIN-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path stroke-width="1" id="MJMAIN-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path stroke-width="1" id="MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="1" id="MJMAIN-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path stroke-width="1" id="MJMATHI-46" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path stroke-width="1" id="MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="1" id="MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="1" id="MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width="1" id="MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="1" id="MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="1" id="MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path stroke-width="1" id="MJMATHI-3B1" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></defs></svg></div><div id="MathJax_Message" style="display: none;"></div>
      <script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"st210"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"sp005"},"#name":"simple-para","_":"In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments."}],"$":{"view":"all","id":"as005"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab005","lang":"en","class":"author"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banner":{"id":"Banner"},"banners":[{"id":"Banner"},{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"50401","accountName":"IT University of Copenhagen","loginStatus":"logged in","userId":"71970787","isLoggedIn":true},"cid":"271625","content-family":"serial","copyright-line":"© 2021 Elsevier Inc. All rights reserved.","cover-date-years":["2022"],"cover-date-start":"2022-01-01","cover-date-text":"January 2022","document-subtype":"fla","document-type":"article","entitledToken":"9F4F9C78B5D52A88DD2D4DFE0905D71CE93214FC85743CA7759A1343A854A46F47EEF6DC1EF12F85","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwMDIwMDI1NTIxMDEwMTM2IiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiNWUwYThhMzI3ZjI3YTA0OGE4NWIyM2E2YTM3MTU2NmQ2OThkZ3hycWEiLCJleHAiOjE3MzAyMDU2NDAsImlhdCI6MTczMDIwMzg0MCwidmVyc2lvbiI6MSwianRpIjoiN2MzOWEyYzYtMjc4My00NTY4LWFjZTQtMTY1ZDAxMTRmZmY4In0.nNUEcP0irNh-RyDi9bTjiv9x77In_x9sMH54Ms6HaDk","eid":"1-s2.0-S0020025521010136","doi":"10.1016/j.ins.2021.10.005","first-fp":"593","hub-eid":"1-s2.0-S0020025521X0028X","issuePii":"S0020025521X0028X","item-weight":"FULL-TEXT","language":"en","last-lp":"617","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au035","author-id":"S0020025521010136-08e1e96cb9164cff11d45ec3652f0328"},"$$":[{"#name":"given-name","_":"Antonio Carlos"},{"#name":"surname","_":"Sobieranski"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/project-administration"},"_":"Project administration"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar040","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"ar045","refid":"af010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"ar050","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlbTAwNSUyMiUyQyUyMnR5cGUlMjIlM0ElMjJlbWFpbCUyMiUyQyUyMmhyZWYlMjIlM0ElMjJtYWlsdG8lM0FhLnNvYmllcmFuc2tpJTQwdWZzYy5iciUyMiU3RCUyQyUyMl8lMjIlM0ElMjJhLnNvYmllcmFuc2tpJTQwdWZzYy5iciUyMiU3RA=="}]}]},"normalized-first-auth-initial":"F","normalized-first-auth-surname":"CANAL","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-embargo-opening-date","_":"2023-10-21T00:00:00.000Z"}]},"pages":[{"last-page":"617","first-page":"593"}],"pii":"S0020025521010136","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2023-10-21T00:00:00.000Z"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Information Sciences","suppl":"C","timestamp":"2022-12-13T01:01:57.434457Z","title":{"content":[{"#name":"title","$":{"id":"tm005"},"_":"A survey on facial emotion recognition techniques: A state-of-the-art literature review"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"582","vol-iss-suppl-text":"Volume 582","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":124,"freeHtmlGiven":false,"userProfile":{"departmentName":"Library","webUserId":"71970787","accountName":"IT University of Copenhagen","shibProfile":{"canRegister":false},"departmentId":"71310","hasMultipleOrganizations":false,"accountNumber":"C000050401","userName":"Gergo Gyori","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLnNFKEhra3YKVSC1wYSW9_0*","accessType":"SHIBREG","accountId":"50401","userType":"NORMAL","privilegeType":"BASIC","email":"gegy@itu.dk"},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"authenticationMethod":"SHIBBOLETH","entitled":true,"isCasaUser":false,"usageInfo":"(71970787,U|71310,D|50401,A|26,S|34,P|2,PL)(SDFE,CON|5e0a8a327f27a048a85b23a6a371566d698dgxrqa,SSO|REG_SHIBBOLETH,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"7 October 2021","Received":"22 March 2021","Revised":["3 August 2021"],"Accepted":"2 October 2021","Publication date":"1 January 2022","Version of Record":"21 October 2021"},"downloadFullIssue":true,"entitlementReason":"package","hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/ci/pta/login/redirect/home/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLnNFKEhra3YKVSC1wYSW9_0*","contactUrl":"https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLnNFKEhra3YKVSC1wYSW9_0*","userName":"Gergo Gyori","userEmail":"gegy@itu.dk","orgName":"IT University of Copenhagen","webUserId":"71970787","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":true,"hasMultiOrg":false,"userType":"SHIBREG","userAnonymity":"INDIVIDUAL","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLnNFKEhra3YKVSC1wYSW9_0*","institutionName":"your institution"},"isCorpReq":false,"isPdfFullText":false,"issn":"00200255","issn-primary-formatted":"0020-0255","issRange":"","isThirdParty":false,"pageCount":25,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"9891a1c60fb47ba70cf083b544d5b6c1","pid":"1-s2.0-S0020025521010136-main.pdf"},"pii":"S0020025521010136","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":436,"openArchiveStatus":false,"openArchiveArticleCount":24,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0020025521X0028X-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0020025521X0028X/cover/DOWNSAMPLED200/image/gif/2f528f210583e47520dfd2335aff2a79/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"4572","pixel-height":"200","pixel-width":"149"},{"attachment-eid":"1-s2.0-S0020025521X0028X-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0020025521X0028X/cover/DOWNSAMPLED/image/gif/dcc333e80638419337786eb29de04d60/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"2892","pixel-height":"150","pixel-width":"112"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S00200255.gif","title":"information-sciences","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S0020025521X0028X-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0020025521X0028X-cov150h.gif"},"volRange":"582","features":["aamAttachments","keywords","references","preview"],"titleString":"A survey on facial emotion recognition techniques: A state-of-the-art literature review","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"referenceLinks":true,"references":true,"referredToBy":true,"recommendations-entitled":true,"toc":true,"body":true,"recommendations":true,"citingArticles":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005","author-id":"S0020025521010136-8eab647bb4a8e5848b8fefd1415eb51c"},"$$":[{"#name":"given-name","_":"Felipe Zago"},{"#name":"surname","_":"Canal"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar005","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au010","author-id":"S0020025521010136-569f4bfe21f679f4cc23a7a3f4de1a64"},"$$":[{"#name":"given-name","_":"Tobias Rossi"},{"#name":"surname","_":"Müller"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar010","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au015","author-id":"S0020025521010136-4247ed67a252d8fe2d15ea3964e9e58f"},"$$":[{"#name":"given-name","_":"Jhennifer Cristine"},{"#name":"surname","_":"Matias"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar015","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au020","author-id":"S0020025521010136-dcd62acaa985bf8cfc201c280552dda2"},"$$":[{"#name":"given-name","_":"Gustavo Gino"},{"#name":"surname","_":"Scotton"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar020","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au025","author-id":"S0020025521010136-0b5f46f8b2db1d6a71dd38201ad0d89f"},"$$":[{"#name":"given-name","_":"Antonio Reis"},{"#name":"surname","_":"de Sa Junior"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar025","refid":"af015"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"author","$":{"id":"au030","author-id":"S0020025521010136-880c4e3488499a453f59ad4a409c2885"},"$$":[{"#name":"given-name","_":"Eliane"},{"#name":"surname","_":"Pozzebon"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar030","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"ar035","refid":"af010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]}]},{"#name":"author","$":{"id":"au035","author-id":"S0020025521010136-08e1e96cb9164cff11d45ec3652f0328"},"$$":[{"#name":"given-name","_":"Antonio Carlos"},{"#name":"surname","_":"Sobieranski"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/project-administration"},"_":"Project administration"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar040","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"ar045","refid":"af010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"ar050","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlbTAwNSUyMiUyQyUyMnR5cGUlMjIlM0ElMjJlbWFpbCUyMiUyQyUyMmhyZWYlMjIlM0ElMjJtYWlsdG8lM0FhLnNvYmllcmFuc2tpJTQwdWZzYy5iciUyMiU3RCUyQyUyMl8lMjIlM0ElMjJhLnNvYmllcmFuc2tpJTQwdWZzYy5iciUyMiU3RA=="}]},{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S0020025521010136-b336c9ced3f8503d7b10af4390fd774d"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing (DEC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"3201 Gov. Jorge Lacerda"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str005"},"_":"Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil"}]},{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S0020025521010136-eae0f6b32e43a72b663d4da49cfe0e10"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Post Graduate Program in Information Technology and Communication (PPGTIC), Federal University of Santa Catarina, 150 Pedro João Pereira, Araranguá, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Post Graduate Program in Information Technology and Communication (PPGTIC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"150 Pedro Joao Pereira"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str010"},"_":"Post Graduate Program in Information Technology and Communication (PPGTIC), Federal University of Santa Catarina, 150 Pedro João Pereira, Araranguá, Santa Catarina, Brazil"}]},{"#name":"affiliation","$":{"id":"af015","affiliation-id":"S0020025521010136-256c815a1b9c7e06c66e6e6bfe60929a"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"Department of Medical Clinic (DCM), Federal University of Santa Catarina, Campus Universitário, Trindade, Florianópolis, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Medical Clinic (DCM)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"organization","_":"Campus Universitário"},{"#name":"address-line","_":"Trindade"},{"#name":"city","_":"Florianópolis"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str015"},"_":"Department of Medical Clinic (DCM), Federal University of Santa Catarina, Campus Universitário, Trindade, Florianópolis, Santa Catarina, Brazil"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author at: Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing (DEC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"3201 Gov. Jorge Lacerda"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]}]}]}],"floats":[],"footnotes":[],"affiliations":{"af005":{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S0020025521010136-b336c9ced3f8503d7b10af4390fd774d"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing (DEC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"3201 Gov. Jorge Lacerda"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str005"},"_":"Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil"}]},"af010":{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S0020025521010136-eae0f6b32e43a72b663d4da49cfe0e10"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Post Graduate Program in Information Technology and Communication (PPGTIC), Federal University of Santa Catarina, 150 Pedro João Pereira, Araranguá, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Post Graduate Program in Information Technology and Communication (PPGTIC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"150 Pedro Joao Pereira"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str010"},"_":"Post Graduate Program in Information Technology and Communication (PPGTIC), Federal University of Santa Catarina, 150 Pedro João Pereira, Araranguá, Santa Catarina, Brazil"}]},"af015":{"#name":"affiliation","$":{"id":"af015","affiliation-id":"S0020025521010136-256c815a1b9c7e06c66e6e6bfe60929a"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"Department of Medical Clinic (DCM), Federal University of Santa Catarina, Campus Universitário, Trindade, Florianópolis, Santa Catarina, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Medical Clinic (DCM)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"organization","_":"Campus Universitário"},{"#name":"address-line","_":"Trindade"},{"#name":"city","_":"Florianópolis"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]},{"#name":"source-text","$":{"id":"str015"},"_":"Department of Medical Clinic (DCM), Federal University of Santa Catarina, Campus Universitário, Trindade, Florianópolis, Santa Catarina, Brazil"}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author at: Department of Computing (DEC), Federal University of Santa Catarina, 3201 Gov. Jorge Lacerda, Araranguá, Santa Catarina, Brazil."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing (DEC)"},{"#name":"organization","_":"Federal University of Santa Catarina"},{"#name":"address-line","_":"3201 Gov. Jorge Lacerda"},{"#name":"city","_":"Araranguá"},{"#name":"state","_":"Santa Catarina"},{"#name":"country","_":"Brazil"}]}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":true},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"kg005","class":"keyword","view":"all"},"$$":[{"#name":"section-title","$":{"id":"st220"},"_":"Keywords"},{"#name":"keyword","$":{"id":"k0005"},"$$":[{"#name":"text","_":"Emotion Recognition"}]},{"#name":"keyword","$":{"id":"k0010"},"$$":[{"#name":"text","_":"Facial emotion recognition"}]},{"#name":"keyword","$":{"id":"k0015"},"$$":[{"#name":"text","_":"Pattern recognition"}]},{"#name":"keyword","$":{"id":"k0020"},"$$":[{"#name":"text","_":"Systematic literature review"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"isOpen":true},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}};
      </script>
      <noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1730203840158?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AREG_SHIBBOLETH&c1=ae%3A50401&c12=ae%3A71970787 />
    </noscript>
      <a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
      <div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container u-hide-from-print"><div id="gh-profile-cnt" class="u-flex-center-ver gh-move-to-spine"><div class="popover" id="gh-profile-dropdown"><div id="popover-trigger-gh-profile-dropdown"><input type="hidden"><button class="button-link gh-icon-btn gh-user-icon u-margin-l-left gh-truncate text-s button-link-secondary button-link-medium button-link-icon-left" title="Gergo Gyori" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-person"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="button-link-text-container"><span class="button-link-text">Gergo Gyori</span></span></button></div></div></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id="institution-popover"><div id="popover-trigger-institution-popover"><input type="hidden"><button class="button-link gh-icon-btn gh-has-institution gh-truncate text-s button-link-secondary u-margin-l-left button-link-medium button-link-icon-left" id="gh-inst-icon-btn" aria-expanded="false" aria-label="Institutional Access" title="IT University of Copenhagen" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-institution gh-inst-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="button-link-text-container"><span class="button-link-text">IT University of Copenhagen</span></span></button></div></div></div><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S0020025521010136/pdfft?md5=9891a1c60fb47ba70cf083b544d5b6c1&amp;pid=1-s2.0-S0020025521010136-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="search" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ab005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry" id="ai-components-toc-entry"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#kg005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction"><span class="anchor-text-container"><span class="anchor-text">1. Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Methodology for literature review"><span class="anchor-text-container"><span class="anchor-text">2. Methodology for literature review</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0025" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. General computational flow for facial emotion recognition"><span class="anchor-text-container"><span class="anchor-text">3. General computational flow for facial emotion recognition</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0030" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Preprocessing"><span class="anchor-text-container"><span class="anchor-text">4. Preprocessing</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0070" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Feature Extraction"><span class="anchor-text-container"><span class="anchor-text">5. Feature Extraction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0075" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="6. Datasets for emotion recognition from images or videos"><span class="anchor-text-container"><span class="anchor-text">6. Datasets for emotion recognition from images or videos</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0110" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="7. Classification algorithms"><span class="anchor-text-container"><span class="anchor-text">7. Classification algorithms</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0155" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="8. Emotion Recognition Methods from Facial Images or Videos"><span class="anchor-text-container"><span class="anchor-text">8. Emotion Recognition Methods from Facial Images or Videos</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0205" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="9. Conclusion and discussions"><span class="anchor-text-container"><span class="anchor-text">9. Conclusion and discussions</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#coi005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Declaration of Competing Interest"><span class="anchor-text-container"><span class="anchor-text">Declaration of Competing Interest</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ak005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgments"><span class="anchor-text-container"><span class="anchor-text">Acknowledgments</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#bi005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (162)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (13)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 1. Number of methods related to facial emotion recognition since 2005 in our…" class="u-display-block" height="102px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr1.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0010" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 2. General Computational Flow revealed from the literature review, often applied…" class="u-display-block" height="21px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr2.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0015" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 3. The most common approaches for facial localization" class="u-display-block" height="72px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr3.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0020" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 4. Haar classifier stages used for facial localization" class="u-display-block" height="35px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr4.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0025" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 5. Facial Action Coding System (FACS)" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr5.sml" width="163px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0030" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 6. This plot was generated using only classes with more than 3 members" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr6.sml" width="219px"></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 7 more figures</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (3)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Facial emotion - Search definition for our Literature Review."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0010" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Top Accuracy(Acc) papers for each one of the most used datasets by the papers in the study."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0015" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="This table contains a total of 94 methods that were extracted from the 51 studies selected for this literature review. Some cells are left blank because of the lack of that specific information at the..."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a></li></ol><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/information-sciences" title="Go to Information Sciences on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/information-sciences" title="Go to Information Sciences on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Information Sciences</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/information-sciences/vol/582/suppl/C" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 582</span></span></a>, <!-- -->January 2022<!-- -->, Pages 593-617</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/information-sciences/vol/582/suppl/C"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521X0028X-cov150h.gif" alt="Information Sciences"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">A survey on facial emotion recognition techniques: A state-of-the-art literature review</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Felipe Zago</span> <span class="text surname">Canal</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au010" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Tobias Rossi</span> <span class="text surname">Müller</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au015" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Jhennifer Cristine</span> <span class="text surname">Matias</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au020" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Gustavo Gino</span> <span class="text surname">Scotton</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au025" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Antonio Reis</span> <span class="text surname">de Sa Junior</span> </span><span class="author-ref" id="baf015"><sup>c</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au030" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Eliane</span> <span class="text surname">Pozzebon</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span> <span class="author-ref" id="baf010"><sup>b</sup></span></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au035" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Antonio Carlos</span> <span class="text surname">Sobieranski</span> </span><span class="author-ref" id="baf005"><sup>a</sup></span> <span class="author-ref" id="baf010"><sup>b</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.ins.2021.10.005" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.ins.2021.10.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0020025521010136&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author" id="ab005" lang="en"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="as005"><div class="u-margin-s-bottom" id="sp005">In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a><span> is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of <a href="/topics/chemical-engineering/neural-network" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural networks</a><span>. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to <a href="/topics/computer-science/neural-network" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural networks</a> counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.</span></span></div></div></div></div><div id="reading-assistant-main-body-section"></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" href="/science/article/pii/S0020025521010215"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S002002552101032X"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="kg005" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="k0005" class="keyword"><span>Emotion Recognition</span></div><div id="k0010" class="keyword"><span>Facial emotion recognition</span></div><div id="k0015" class="keyword"><span>Pattern recognition</span></div><div id="k0020" class="keyword"><span>Systematic literature review</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 id="st005" class="u-h4 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div class="u-margin-s-bottom" id="p0005"><span>Facial Emotion Recognition performed computationally is a very interesting and challenging task to be explored. Besides interpreting facial emotion expression being a task naturally performed by humans, finding computational mechanisms to reproduce it in the same or similar way is still an <a href="/topics/computer-science/unsolved-problem" title="Learn more about unsolved problem from ScienceDirect's AI-generated Topic Pages" class="topic-link">unsolved problem</a> </span><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a><span>. Designing and developing algorithmic solutions able to interpret facial emotions from human faces opens a new window of possibilities for the human–computer interaction context, such as in robotics, gaming, digital marketing, <a href="/topics/computer-science/intelligent-tutor" title="Learn more about intelligent tutor from ScienceDirect's AI-generated Topic Pages" class="topic-link">intelligent tutor</a> systems, among many others </span><a class="anchor anchor-primary" href="#b0295" name="bb0295" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0295"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a><span>. The sooner we are able to design such recognizers, the better we can help to understand natural areas of psychology, <a href="/topics/earth-and-planetary-sciences/neurology" title="Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages" class="topic-link">neuroscience</a>, human cognition and learning </span><a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a><span>. Nonetheless, how human expression behavior is invariant among distinct individuals, and how biological and <a href="/topics/computer-science/social-aspect" title="Learn more about social aspects from ScienceDirect's AI-generated Topic Pages" class="topic-link">social aspects</a> may interferes in human communication over time, are interesting questions to be studied and modeled computationally, as well as its correlation.</span></div><div class="u-margin-s-bottom" id="p0010"><span><span>In order to provide analytical models for human expression recognition from <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a>, engineers, mathematicians and computer scientists are exploring distinct ways to reproduce approaches able to effectively implement efficient algorithms. There are strong correlations among </span><a href="/topics/engineering/image-processing" title="Learn more about image processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">image processing</a><span>, <a href="/topics/engineering/computervision" title="Learn more about computer vision from ScienceDirect's AI-generated Topic Pages" class="topic-link">computer vision</a><span>, pattern recognition and <a href="/topics/computer-science/artificial-intelligence" title="Learn more about artificial intelligence from ScienceDirect's AI-generated Topic Pages" class="topic-link">artificial intelligence</a> fields exploring this topic, and interesting approaches can be verified over the literature. In general, specifically due to the recent achievements in computational processing power and new architectures for high-performance computing, applications for the aforementioned fields, previously restricted by computational limitations, have had their solution achieved </span></span></span><a class="anchor anchor-primary" href="#b0060" name="bb0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0060"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a><span><span>. With the advent of the computing <a href="/topics/engineering/graphics-processing-unit" title="Learn more about graphic processing units from ScienceDirect's AI-generated Topic Pages" class="topic-link">graphic processing units</a> (GPU’s), many methods were adapted for their </span><a href="/topics/computer-science/parallel-version" title="Learn more about parallel version from ScienceDirect's AI-generated Topic Pages" class="topic-link">parallel version</a><span>, having <a href="/topics/engineering/execution-time" title="Learn more about execution time from ScienceDirect's AI-generated Topic Pages" class="topic-link">execution time</a> close to real-time </span></span><a class="anchor anchor-primary" href="#b0510" name="bb0510" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0510"><span class="anchor-text-container"><span class="anchor-text">[102]</span></span></a>. Specifically for the emotion recognition problem from facial images, those computational improvements have also provided a new set of tools, paving the way for the development of several approaches based on what we named here as classical methods<a class="anchor anchor-primary" href="#fn1" name="bfn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>.</div><div class="u-margin-s-bottom" id="p0015"><span>Another important achievement was obtained over the past few years with the advent of variants of neural networks – the Convolutional Neural Networks (CNN’s). CNN’s have being applied as generic problem-solvers, where some input signal is decomposed (de-convolved) into a set of invariant-features, providing robust mechanisms to extract relevant features (such as texture, corners and key-points). After a training step, the classifier is ready to interpret image with zero-bias in a very effective way. Consequently, CNN is ascending lately in many areas, mainly the ones that involve image processing and pattern recognition, such as those used in medical <a href="/topics/computer-science/image-classification" title="Learn more about image classification from ScienceDirect's AI-generated Topic Pages" class="topic-link">image classification</a> </span><a class="anchor anchor-primary" href="#b0325" name="bb0325" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0325"><span class="anchor-text-container"><span class="anchor-text">[65]</span></span></a>, <a class="anchor anchor-primary" href="#b0420" name="bb0420" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0420"><span class="anchor-text-container"><span class="anchor-text">[84]</span></span></a>, <a class="anchor anchor-primary" href="#b0035" name="bb0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0035"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>.</div><div class="u-margin-s-bottom"><div id="p0020">Over the literature it is possible to find many works fully dedicated to solve the emotion recognition problem from facial images, using classical methods or Neural Network based (NNB) approaches <a class="anchor anchor-primary" href="#b0135" name="bb0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0135"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a>, <a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>, <a class="anchor anchor-primary" href="#b0250" name="bb0250" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0250"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a>, <a class="anchor anchor-primary" href="#b0545" name="bb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0545"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a>. The graph of the <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig. 1</span></span></a> shows a comparison of published approaches for emotion recognition from facial expressions, obtained from our literature review. For the selected papers, two main trends can be verified: (i) classical approaches, being composed of traditional image processing, pattern recognition and miscellaneous classifiers, more specifically provided by a hand-crafted feature design process, and (ii), NNB approaches, where classical neural networks and its convolutional counterpart were considered, whose features are learned from data using generic feature extractions, more specifically for the convolutional case. One can see a considerable acceptance of NNB approaches over the past five years, more specifically justified by the emergence of CNN’s approaches.</div><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr1.jpg" height="256" alt="" aria-describedby="cn005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (80KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (80KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn005"><p id="sp010"><span class="label">Fig. 1</span>. Number of methods related to facial emotion recognition since 2005 in our database. Approaches were classified into traditional <a href="/topics/computer-science/image-processing" title="Learn more about image processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">image processing</a><span><span><span> or <a href="/topics/engineering/computervision" title="Learn more about computer vision from ScienceDirect's AI-generated Topic Pages" class="topic-link">computer vision</a> approaches (Classic) and </span><a href="/topics/computer-science/neural-network" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural networks</a> based (NNB) techniques. Only methods that had mentioned the </span><a href="/topics/computer-science/classification-algorithm" title="Learn more about classification algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification algorithm</a>, dataset and results clearly were counted.</span></p></span></span></figure></div><div class="u-margin-s-bottom" id="p0025">Given the large number of classical methods, which still can be combined among them producing hybrid designs, and the emerging CNN’s approaches, a new literature review for the area of expression and emotion recognition from facial images is needed. In order to achieve this goal, in this paper, a systematic literature review was conducted to select the published works whose inclusion and exclusion criterion matched, and a total of 51 were obtained, resulting in 94 distinct approaches. Based on the selected papers, the objective of this work was to provide answers to the following questions: (i) what is the state-of-the-art in emotion recognition, how many methods, techniques or classes of methods can be identified over the literature? (ii) what is the importance of the CNN’s as a general trend for the facial emotion recognition problem? (iii) is there a general computational workflow for facial emotion recognition that can be identified? (iv) how many databases for facial emotion recognition can be identified, containing the number of individuals and expressions? how is their acceptance in the scientific community? (v) is it possible to predict further trends to solve the problem of emotion recognition from facial images, and how far from a generic effective solution are we?.</div><div class="u-margin-s-bottom" id="p0030">Given the importance of the aforementioned research questions, the remainder of this work is organized as follows: Section <a class="anchor anchor-primary" href="#s0010" name="bs0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0010"><span class="anchor-text-container"><span class="anchor-text">2</span></span></a> provides the systematic literature review methodology used in this survey, describing in details the inclusion and exclusion criterion employed. Section <a class="anchor anchor-primary" href="#s0025" name="bs0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0025"><span class="anchor-text-container"><span class="anchor-text">3</span></span></a> presents a general computational flow we are able to identify as commonly used approach to solve the problem of emotion recognition from facial images or videos. Section <a class="anchor anchor-primary" href="#s0030" name="bs0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0030"><span class="anchor-text-container"><span class="anchor-text">4</span></span></a><span> presents the most frequently used preliminary step found in the verified works, being defined by a <a href="/topics/computer-science/preprocessing-step" title="Learn more about preprocessing step from ScienceDirect's AI-generated Topic Pages" class="topic-link">preprocessing step</a> used to locate the region of interest, as well as to perform image improvements. The process of extracting relevant information from the images, as known as feature extraction, is briefly discussed in Section </span><a class="anchor anchor-primary" href="#s0070" name="bs0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0070"><span class="anchor-text-container"><span class="anchor-text">5</span></span></a><span>. Datasets publicly available to the scientific community used to train the <a href="/topics/earth-and-planetary-sciences/computational-modeling" title="Learn more about computational models from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational models</a> are described in Section </span><a class="anchor anchor-primary" href="#s0075" name="bs0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0075"><span class="anchor-text-container"><span class="anchor-text">6</span></span></a><span>. The most used <a href="/topics/engineering/classification-algorithm" title="Learn more about classification algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification algorithms</a> are presented in Section </span><a class="anchor anchor-primary" href="#s0110" name="bs0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0110"><span class="anchor-text-container"><span class="anchor-text">7</span></span></a><span>. The <a href="/topics/computer-science/computational-approach" title="Learn more about computational approaches from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational approaches</a> and techniques used for emotion recognition from facial images or videos are described in Section </span><a class="anchor anchor-primary" href="#s0155" name="bs0155" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0155"><span class="anchor-text-container"><span class="anchor-text">8</span></span></a>. Finally, conclusions, discussions and further directions identified in this study are presented in Section <a class="anchor anchor-primary" href="#s0205" name="bs0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0205"><span class="anchor-text-container"><span class="anchor-text">9</span></span></a>.</div></section><section id="s0010"><h2 id="st010" class="u-h4 u-margin-l-top u-margin-xs-bottom">2. Methodology for literature review</h2><div class="u-margin-s-bottom" id="p0035">Over the past few years it is possible to find interesting literature review papers summarizing the most prominent works of the current state-of-the-art in the FER area. In general, journal papers such as <a class="anchor anchor-primary" href="#b0280" name="bb0280" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0280"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a>, <a class="anchor anchor-primary" href="#b0315" name="bb0315" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0315"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a>, <a class="anchor anchor-primary" href="#b0195" name="bb0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0195"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>, <a class="anchor anchor-primary" href="#b0395" name="bb0395" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0395"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a> present a very detailed description of many facial expression or emotion recognition systems, highlighting aspects of image or video acquisition, post-processing, feature extraction and recognition, as well as available datasets, essential sources to properly train a FER recognition system. Not less important, there are other papers majorly published in conferences, whose FER technologies are also presented <a class="anchor anchor-primary" href="#b0400" name="bb0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0400"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a>, <a class="anchor anchor-primary" href="#b0515" name="bb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0515"><span class="anchor-text-container"><span class="anchor-text">[103]</span></span></a>, <a class="anchor anchor-primary" href="#b0405" name="bb0405" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0405"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a>, <a class="anchor anchor-primary" href="#b0335" name="bb0335" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0335"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a>, focusing their discussion to traditional methods, or to the recently emerged convolutional neural network counterpart. Also, it is possible to find interesting literature reviews describing the combined use of FER with secondary information, such as the use of compound facial expression emotions <a class="anchor anchor-primary" href="#b0515" name="bb0515" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0515"><span class="anchor-text-container"><span class="anchor-text">[103]</span></span></a><span>, the combined use of <a href="/topics/computer-science/augmented-reality" title="Learn more about augmented reality from ScienceDirect's AI-generated Topic Pages" class="topic-link">augmented reality</a> </span><a class="anchor anchor-primary" href="#b0395" name="bb0395" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0395"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a>, gender and emotion classification using facial expression detection <a class="anchor anchor-primary" href="#b0160" name="bb0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0160"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a>, and a comparison of methods specifically designed for genuine versus posed facial expressions of emotions <a class="anchor anchor-primary" href="#b0220" name="bb0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0220"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0040"><span>In the literature review presented in this paper we intend to analyze the state-of-the-art in the area of emotion expression recognition captured from <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a> or videos, based on the methodology described by Kitchenham </span><a class="anchor anchor-primary" href="#b0260" name="bb0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0260"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a>, a well-established methodology used to provide systematic literature reviews in a verifiable and reliable manner. It is intended to provide a general panorama of the results achieved by computational facial emotion recognition, employing the most different approaches, yet mainly focuses on analyzing classical and NNB works, since those methods seems to be arising over the past few years. Classical approaches while effective, may be very dependent of the execution parameters and environment conditions to achieve efficient results. On the other hand, NNB approaches can be generalized to solve a series of problems, and have been used in the past few years to clarify the issue of facial emotion recognition as well. We will analyze the existing methods regarding in which emotion types they are applied; whether they are adaptive regarding environment changes; and whether they are able to distinguish possible faults or changes in terms of emotions; datasets and the capability to distinguish between different kind of expressions.</div><div class="u-margin-s-bottom" id="p0045">The systematic literature review for emotion expression recognition aims to postulate research questions as presented below:</div><div class="u-margin-s-bottom" id="p0050"><u><strong>Question</strong></u>: What is the state-of-the-art in computerized approaches for emotion recognition, obtained from facial images or videos, published over the past years?</div><div class="u-margin-s-bottom" id="p0055"><u><strong>Population</strong></u><span>: Related works describing a <a href="/topics/computer-science/computational-approach" title="Learn more about computational approach from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational approach</a> for expression emotion recognition from facial images or videos, obtained from raw data or well-established datasets publicly available.</span></div><div class="u-margin-s-bottom" id="p0060"><u><strong>Intervention</strong></u>: Analyze the state-of-the-art in computational emotion recognition.</div><div class="u-margin-s-bottom" id="p0065"><u><strong>Results</strong></u>: Comparison among computational expression emotion recognition approaches and datasets.</div><div class="u-margin-s-bottom" id="p0070"><u><strong>Context</strong></u>: Papers found in digital libraries such as: <em>Science Direct</em>, <em>IEEEXplore</em>, <em>Scopus</em> e <em>ACM Digital Library</em>.</div><div class="u-margin-s-bottom" id="p0075">The databases for literature review were chosen by taking into account their relevance to the areas of image processing, computer vision, pattern recognition and <a href="/topics/computer-science/artificial-intelligence" title="Learn more about artificial intelligence from ScienceDirect's AI-generated Topic Pages" class="topic-link">artificial intelligence</a><span>, more specifically those that employed the problem domain previously mentioned. Therefore, the following databases were used: ScienceDirect, IEEE Xplore Digital Library, Scopus and ACM Digital Library: (i) ScienceDirect is a platform with a considerable amount of scientific papers, including high impact factor journals, along with the area of computer vision and pattern recognition. According to the website, it hosts around 2,500 journals and more than 39,000 books until the present time; (ii) IEEE Xplore digital library is a database of scientific and technical content, including journals, conferences and books. It’s supported by the IEEE (Institute of Electrical and Electronics Engineers) and it’s publishing partners. Accordingly to the maintainer, it is composed by more than four-million full-text documents, including 195&nbsp;+&nbsp;journals and 1,800&nbsp;+&nbsp;conference proceedings; (iii) Scopus is an abstract and citation database that contains peer-reviewer literature such as journals, books and conferences proceedings. It contains researches from the fields of science, technology, medicine, social sciences, and arts and humanities; (iv) ACM Digital Library is a database of full-text papers with focus in the computational field, including journals, conferences and books, maintained by the Association for <a href="/topics/computer-science/computing-machinery" title="Learn more about Computing Machinery from ScienceDirect's AI-generated Topic Pages" class="topic-link">Computing Machinery</a>.</span></div><div class="u-margin-s-bottom" id="p0080">The access to the databases was performed through the CAPES Portal<a class="anchor anchor-primary" href="#fn2" name="bfn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>. To avoid ambiguity and redundancy among the literature, the databases used were limited exclusively to the aforementioned sources, and other sources are out of the scope of this work.</div><section id="s0015"><h3 id="st015" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Search definition</h3><div class="u-margin-s-bottom"><div id="p0085">Search definitions used to support our manuscript are presented according to the <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>. Firstly, the four databases used to initially select relevant papers is shown at the first row, as well as the search terms corresponding to facial expression recognition and facial emotion recognition. Inclusion and exclusion criterion are presented in the second and third row, respectively. These criterion are required to provide a second filter to the literature review methodology, describing particular desired aspects to be presented in the articles.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0005"><span class="captions text-s"><span id="cn070"><p id="sp075"><span class="label">Table 1</span>. Facial emotion - Search definition for our Literature Review.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left"><strong>Search Bases</strong></th><th scope="col" class="align-left"><strong>Search Terms</strong></th></tr></thead><tbody><tr class="valign-top"><td class="align-left"><em>- ACM Digital Library;</em><br> <em>- IEEE Xplore;</em><br> <em>- Science Direct;</em> <br><em>- Scopus.</em></td><td class="align-left"><em>* facial expression recognition;</em><br> <em>** facial emotion recognition;</em></td></tr><tr class="valign-top"><td class="align-left" colspan="2"><br></td></tr><tr class="valign-top"><td class="align-center" colspan="2"><strong>Inclusion criteria</strong></td></tr><tr class="valign-top"><td class="align-left" colspan="2">- Papers written in English language; <br>- Papers published between 2006 and 2019; <br>- Papers that present a facial expression/emotion recognition approach;<br> - Works with face images or sequence of images approaches to detect emotion;</td></tr><tr class="valign-top"><td class="align-left" colspan="2"><br></td></tr><tr class="valign-top"><td class="align-center" colspan="2"><strong>Exclusion Criteria</strong></td></tr><tr class="valign-top"><td class="align-left" colspan="2">- Short papers, such as abstracts or expanded abstracts;</td></tr></tbody></table></div></div></div></section><section id="s0020"><h3 id="st020" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Search execution</h3><div class="u-margin-s-bottom" id="p0090">After the initial selection of articles, a total of 117 manuscripts were filtered. These articles were elected according to their title, abstracts, keywords and images, as described by the literature review methodology. The second filter applied over the articles was based on analyzing their entire structure, which means, the documents were separated according to their contents. Articles with no significant information or lack of important data were discarded at this stage. Besides that, while reading carefully all the articles, we were able to classify them in terms of employed technologies, year of publication, dataset properties, considered expressions and result’s accuracy. After this final stage, 51 articles remained. Since some of these 51 studies propose not a single method, but multiple solutions, a total of 94 methods where analyzed (<a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a>).</div></section></section><section id="s0025"><h2 id="st025" class="u-h4 u-margin-l-top u-margin-xs-bottom">3. General computational flow for facial emotion recognition</h2><div class="u-margin-s-bottom" id="p0095">In our work, the literature review has revealed interesting aspects indicating for a general computational flow, which can be applied to solve the challenge of recognizing human facial emotion from images or videos. This flow can be illustrated in general terms according to the <a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig. 1</span></span></a><span><span> as follows: (i) image or data acquisition step: input images or video sequences are used from well-established datasets or from proper image <a href="/topics/computer-science/acquisition-system" title="Learn more about acquisition systems from ScienceDirect's AI-generated Topic Pages" class="topic-link">acquisition systems</a> and environmental sets, using bi-dimensional signals or combined with tree-dimensional acquisition devices, such as kinect or sterescopic cameras; (ii) image preparation (preprocessing) step: performed in order to improve image quality, reduce noise, reduce dimensionality in terms of spatial resolution, or by the simplification of color information into </span><a href="/topics/engineering/grayscale" title="Learn more about gray scale from ScienceDirect's AI-generated Topic Pages" class="topic-link">gray scale</a><span> one; (iii) feature selection/extraction step: aims to extract relevant features from the input signal in order to discretize the input signal and make them unique for each individual – this step behaves like a signature or fingerprint for each specific facial emotion; (iv) classification step: the previously obtained features are used in some <a href="/topics/computer-science/classification-models" title="Learn more about classification model from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification model</a>, able to provide a similarity or dissimilarity score for facial emotions, ranked according to the number of recognizable expressions in the dataset; (v) results and validation procedure: this step is often required since several models are based on a training procedure, and a validation procedure is carried in order to compute a general precision overall for the proposed approach. This last step is usually performed against a well-known ground-truth information, provided by the dataset and used to train and validate the computational recognition model proposed.</span></span></div><div class="u-margin-s-bottom"><div id="p0100">The aforementioned steps illustrated by the <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig. 2</span></span></a><span> can be identified partially or even totally for a large number of approaches. For each step block, the algorithms used may vary according to the particularity of the problem approached, as well as characteristics of the dataset used, and they can be replaced in a tailored manner by another block method independently. In the next sections the steps cited will be explained in details, providing a general overview of the approaches proposed over the literature and its main constructive <a href="/topics/computer-science/algorithmic-step" title="Learn more about algorithmic steps from ScienceDirect's AI-generated Topic Pages" class="topic-link">algorithmic steps</a>.</span></div><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr2.jpg" height="36" alt="" aria-describedby="cn010"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (19KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (19KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn010"><p id="sp015"><span class="label">Fig. 2</span>. General Computational Flow revealed from the literature review, often applied as a general problem-solver for the emotion recognition problem using images or videos as input signals.</p></span></span></figure></div></section><section id="s0030"><h2 id="st030" class="u-h4 u-margin-l-top u-margin-xs-bottom">4. Preprocessing</h2><div class="u-margin-s-bottom" id="p0105">Preprocessing step can be ignored for some approaches according to the dataset quality and whether previous preparation was done or not. For some cases, specially when a dataset containing raw images is used, preprocessing is a very important operation <a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a>, <a class="anchor anchor-primary" href="#b0340" name="bb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0340"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a>, which may be responsible for the success or failure of a recognition system. Preprocessing has the objective to slightly suppress some imperfections such as noise, reduce distortions and highlight only the most important features for the data being analyzed.</div><section id="s0035"><h3 id="st035" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Grayscale conversion</h3><div class="u-margin-s-bottom" id="p0110">Besides gray scale conversion being a simple task, it has been demonstrated that for some areas such as face recognition and emotion recognition color information can be suppressed into a single dimension with no significant loss of precision <a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a><span>. Additionally, processing color inputs still provides an increment of computation power in the order of 3 times when compared to <a href="/topics/engineering/grayscale-image" title="Learn more about gray scale images from ScienceDirect's AI-generated Topic Pages" class="topic-link">gray scale images</a> at least until (iii)-feature extraction step, where information is usually invariant from color. On the other hand, nowadays almost every single digital camera captures images and store them into proprietary format or Bayer-8 (which in practice is uni-dimensional), and them convert it to color information afterwards (usually RGB) </span><a class="anchor anchor-primary" href="#b0500" name="bb0500" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0500"><span class="anchor-text-container"><span class="anchor-text">[100]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0115">Over the literature it can be observed that reducing the input signal from color to gray scale is very common and effective, and at the same time preserving the relevant features and mitigate performance issues for the emotion recognition problem. The most common operation is the simply summation and normalization of the resulting unidimensional signal. Other approaches use the normalized grayscale conversion formula defined by the weights <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>0.2989</mn><mo is=&quot;true&quot;>,</mo><mn is=&quot;true&quot;>0.5870</mn><mo is=&quot;true&quot;>,</mo><mn is=&quot;true&quot;>0.1140</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.445ex" height="2.317ex" viewBox="0 -747.2 9233.3 997.6" role="img" focusable="false" style="vertical-align: -0.582ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMAIN-30"></use><use xlink:href="#MJMAIN-2E" x="500" y="0"></use><use xlink:href="#MJMAIN-32" x="779" y="0"></use><use xlink:href="#MJMAIN-39" x="1279" y="0"></use><use xlink:href="#MJMAIN-38" x="1780" y="0"></use><use xlink:href="#MJMAIN-39" x="2280" y="0"></use></g><g is="true" transform="translate(2781,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(3226,0)"><use xlink:href="#MJMAIN-30"></use><use xlink:href="#MJMAIN-2E" x="500" y="0"></use><use xlink:href="#MJMAIN-35" x="779" y="0"></use><use xlink:href="#MJMAIN-38" x="1279" y="0"></use><use xlink:href="#MJMAIN-37" x="1780" y="0"></use><use xlink:href="#MJMAIN-30" x="2280" y="0"></use></g><g is="true" transform="translate(6007,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(6452,0)"><use xlink:href="#MJMAIN-30"></use><use xlink:href="#MJMAIN-2E" x="500" y="0"></use><use xlink:href="#MJMAIN-31" x="779" y="0"></use><use xlink:href="#MJMAIN-31" x="1279" y="0"></use><use xlink:href="#MJMAIN-34" x="1780" y="0"></use><use xlink:href="#MJMAIN-30" x="2280" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mn is="true">0.2989</mn><mo is="true">,</mo><mn is="true">0.5870</mn><mo is="true">,</mo><mn is="true">0.1140</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-1"><math><mrow is="true"><mn is="true">0.2989</mn><mo is="true">,</mo><mn is="true">0.5870</mn><mo is="true">,</mo><mn is="true">0.1140</mn></mrow></math></script></span>, for red, green and blue channels, respectively. There are other approaches where a specific channel is taken into account as the dominant information thus generating a single channel image from it.</div></section><section id="s0040"><h3 id="st040" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Face detection</h3><div class="u-margin-s-bottom" id="p0120">The most relevant stage in recognizing facial expressions and emotions is the initial selection of the region of interest (ROI) in the input image <a class="anchor anchor-primary" href="#b0140" name="bb0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0140"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a><span>. There are some effective <a href="/topics/computer-science/algorithmic-approach" title="Learn more about algorithmic approaches from ScienceDirect's AI-generated Topic Pages" class="topic-link">algorithmic approaches</a> able to find the region of interest (the face region) despite the problematic perturbation that may affects their performance. Normally, these kind of algorithm has to get around some variations in the images, such as pose and illumination </span><a class="anchor anchor-primary" href="#b0025" name="bb0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0025"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a><span>. The variations between different faces (subjects), in general, are less impacting than environment ones, so some of these problems are usually solved with use of <a href="/topics/computer-science/preprocessing-technique" title="Learn more about preprocessing techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">preprocessing techniques</a>.</span></div><div class="u-margin-s-bottom" id="p0125">Generally, raw images provided by most datasets, have a lot of <a href="/topics/computer-science/background-information" title="Learn more about background information from ScienceDirect's AI-generated Topic Pages" class="topic-link">background information</a><span> that has no use for expression and emotion classification. Therefore, the most common preprocessing technique is to detect the subject’s face and crop the area around it, discarding useless information and even increasing its accuracy. In the context proposed by this work, the most important preprocessing step is to detect a face inside the image that is being analyzed. In this section, we describe some <a href="/topics/computer-science/detection-algorithm" title="Learn more about detection algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">detection algorithms</a> and how they work with images to achieve what they are designed for.</span></div><section id="s0045"><h4 id="st045" class="u-margin-m-top u-margin-xs-bottom">4.2.1. Viola-Jones</h4><div class="u-margin-s-bottom" id="p0130">The algorithm developed by Paul Viola and Michael J. Jones in 2003 had a great impact in the face detection area, because besides its great precision, it has low computational cost and also allows to visualize the results. Although it is still widely used, it needs a training step that can be slow, but after that training, face detection is fast.</div><div class="u-margin-s-bottom"><div id="p0135">According to the authors <a class="anchor anchor-primary" href="#b0230" name="bb0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0230"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a>, this technique is based on the characteristics of Haar-like (used for pedestrian detection), which is given by the summation of pixels that are within the area of white rectangles and then subtracted from the area of the gray rectangles. Although there are better filters, the advantage offered by Haar over the offered efficiency turns out to be advantageous according to <a class="anchor anchor-primary" href="#b0525" name="bb0525" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0525"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a>, as shown in the <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig. 3</span></span></a>-(a).</div><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr3.jpg" height="177" alt="" aria-describedby="cn015"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (43KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (43KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn015"><p id="sp020"><span class="label">Fig. 3</span>. The most common approaches for facial localization. Viola Jones uses rectangular Haar-like features as show in (a). Haar classifier rectangular blocks are illustrated in (b).</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0140">The Viola-Jones detector performs face localization and has its constructive algorithmic based in four steps:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1.</span><span><div class="u-margin-s-bottom" id="p0470">Selection of rectangular Haar-Like Features as demonstrated in the <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig. 3</span></span></a>-(a);</div></span></li><li class="react-xocs-list-item"><span class="list-label">2.</span><span><div class="u-margin-s-bottom" id="p0475">Creation of the integral image that allows quick calculations of the previous mentioned Haar characteristics;</div></span></li><li class="react-xocs-list-item"><span class="list-label">3.</span><span><div class="u-margin-s-bottom" id="p0480">Training of an AdaBoost-based classifier capable of selecting relevant characteristics;</div></span></li><li class="react-xocs-list-item"><span class="list-label">4.</span><span><div class="u-margin-s-bottom" id="p0485">Using <a href="/topics/computer-science/classifier-cascade" title="Learn more about cascade classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">cascade classifiers</a> that rule out regions where a face is unlikely to exist, and focus on regions likely to contain a face.</div></span></li></ul></div></section><section id="s0050"><h4 id="st050" class="u-margin-m-top u-margin-xs-bottom">4.2.2. Haar cascade</h4><div class="u-margin-s-bottom" id="p0145">The Haar Cascade Classifier is an object detection method that inserts Haar resources into a series of classifiers to identify objects in an image. Taking this into account, Haar’s features are based on Haar waves, rather than the usual image intensities. According to <a class="anchor anchor-primary" href="#b0460" name="bb0460" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0460"><span class="anchor-text-container"><span class="anchor-text">[92]</span></span></a>, this method was adopted and developed by Viola and Jones <a class="anchor anchor-primary" href="#b0525" name="bb0525" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0525"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a>, despite having been proposed by <a class="anchor anchor-primary" href="#b0370" name="bb0370" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0370"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a>. The set of alternative resources was born due to the computational cost of calculating resources in an image with RBG pixels.</div><div class="u-margin-s-bottom" id="p0150"><span>A Haar-like feature considers adjacent rectangular regions at a specific location in the detection window, sums up the <a href="/topics/computer-science/pixel-intensity" title="Learn more about pixel intensities from ScienceDirect's AI-generated Topic Pages" class="topic-link">pixel intensities</a> in each region and calculates the difference between these sums. The difference calculated can be used then to identify subsections in the image. Haar classifiers must be trained by giving lots of images (positive and negative in therms of containing the object to be detected). With that done, features can be extracted using sliding windows of blocks, as shown in </span><a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig. 3</span></span></a>-(b). These blocks are almost like convolution matrices that are applied in different portions of the picture to find partial matches.</div><div class="u-margin-s-bottom" id="p0155"><span>In sequence, even being able to fast calculate the sum intensities using integral images, there are lots of features extracted from the previous steps that do not add any value to the result. If a feature is detected in a region that is not of interest, it has no real value to the classifier. For that, a boosting technique is applied in Haar Cascade classifier, called <a href="/topics/engineering/adaboost" title="Learn more about AdaBoost from ScienceDirect's AI-generated Topic Pages" class="topic-link">AdaBoost</a> </span><a class="anchor anchor-primary" href="#b0125" name="bb0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0125"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>. These technique reduces the number of classifiers significantly, causing no harm to the end result.</div><div class="u-margin-s-bottom"><div id="p0160">The Haar Cascade Classifier is composed by the following stages:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1.</span><span><div class="u-margin-s-bottom" id="p0490">A section of the image is selected as working region;</div></span></li><li class="react-xocs-list-item"><span class="list-label">2.</span><span><div class="u-margin-s-bottom" id="p0495">The first stage evaluates the presence of a feature as previously detailed;</div></span></li><li class="react-xocs-list-item"><span class="list-label">3.</span><span><div class="u-margin-s-bottom" id="p0500">If the previous step’s return has a positive signal, the classifier jumps to the next stage and that goes until the last stage. On the other hand, if the previous step’s return was a negative signal, another section of the image is selected and the process restarts;</div></span></li><li class="react-xocs-list-item"><span class="list-label">4.</span><span><div class="u-margin-s-bottom" id="p0505">Finally, if the image was entirely evaluated, the classifiers ends its execution.</div></span></li></ul>The cascade process aforementioned is illustrated by the <a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig. 4</span></span></a><span>. Besides this approach is also used for several methods for object recognition and detection, it has been shown very effective for the problem of facial localization with a very good response in terms of <a href="/topics/engineering/execution-time" title="Learn more about execution time from ScienceDirect's AI-generated Topic Pages" class="topic-link">execution time</a>.</span></div><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr4.jpg" height="87" alt="" aria-describedby="cn020"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (40KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (40KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn020"><p id="sp025"><span class="label">Fig. 4</span>. Haar classifier stages used for facial localization.</p></span></span></figure></div></section><section id="s0055"><h4 id="st055" class="u-margin-m-top u-margin-xs-bottom">4.2.3. Convolutional neural network</h4><div class="u-margin-s-bottom" id="p0165">A Convolutional Neural Networks (CNN) are nowadays one of the main categories when mentioning general problem-solvers for data mapping, image recognition and classification, object detection and, clearly, face localization/recognition. The deriving classifications provided by CNN <a class="anchor anchor-primary" href="#b0005" name="bb0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0005"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a><span> occur when an image is taken and classifying it the following way: the method sees the input image as a pixel matrix; this matrix will go through a series of <a href="/topics/computer-science/convolution-filter" title="Learn more about convolution filters from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution filters</a>, pooling and functions for example softmax, that classifies an object with probabilistic values between 0 and 1; this way, with a trained network, it is possible to utilize those functions to classify images never ”seen” by the network before (zero bias) with a considerably well accuracy. Section </span><a class="anchor anchor-primary" href="#s0140" name="bs0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0140"><span class="anchor-text-container"><span class="anchor-text">7.2</span></span></a><span> will provide further details regarding the functioning of a convolutional neural network. One <a href="/topics/engineering/neural-network-architecture" title="Learn more about CNN architecture from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN architecture</a> that became very popular due to high performance in face detection was the Multi-Task Cascaded Convolutional Neural Network (MTCNN) </span><a class="anchor anchor-primary" href="#b0590" name="bb0590" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0590"><span class="anchor-text-container"><span class="anchor-text">[118]</span></span></a>. Experiments performed on benchmarks datasets of facial detection in 2016 demonstrated a performance superior to all other algorithms tested so far. Several implementations of this architecture are open for use through python libraries, further popularizing the algorithm.</div></section><section id="s0060"><h4 id="st060" class="u-margin-m-top u-margin-xs-bottom">4.2.4. Miscellaneous</h4><div class="u-margin-s-bottom" id="p0170"><span>Beyond the aforementioned methods, there are still plenty of other approaches that could be potentially used to detect faces. Some of them are based on classic computer vision approaches (such as <a href="/topics/computer-science/image-segmentation" title="Learn more about image segmentation from ScienceDirect's AI-generated Topic Pages" class="topic-link">image segmentation</a>, among others), but what is remarkable is that these technologies, besides obsolete nowadays, were essential to the development of the current technologies. Some of these miscellaneous technologies are describe below: At </span><a class="anchor anchor-primary" href="#b0580" name="bb0580" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0580"><span class="anchor-text-container"><span class="anchor-text">[116]</span></span></a><span>, three methods were used simultaneously: joint cascade detection and alignment (JDA) that has satisfactory performance with frontal detection, whereas the Deep Convolution Neural Network (DCNN) has <a href="/topics/computer-science/good-performance" title="Learn more about good performances from ScienceDirect's AI-generated Topic Pages" class="topic-link">good performances</a> with non-frontal faces and, finally, a Mixture of Trees (MoT) took place in case the other two detectors fail. Another procedure is adopted by </span><a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a><span> where two combined methods for face detection are applied, <a href="/topics/computer-science/histogram-of-oriented-gradient" title="Learn more about HOG from ScienceDirect's AI-generated Topic Pages" class="topic-link">HOG</a><span> and a <a href="/topics/computer-science/linear-classifier" title="Learn more about linear classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear classifier</a>. To run the recognizer in real time, the face location is initiated by the face detector with the first frame and uses the correlation tracker to track the face through the other frames.</span></span></div></section></section><section id="s0065"><h3 id="st065" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.3. Dimensionality reduction</h3><div class="u-margin-s-bottom" id="p0175"><span>Attempting to reduce the amount of data to work on, most of the researchers opt to apply some technique to reduce images dimensions and, therefore, reduce time consumption and lower requirement for processing power. The literature review clearly demonstrate that one of the most common and simple approach, is the down sampling itself. Down sampling is, in fact, a downscale of images with <a href="/topics/computer-science/geometric-transformation" title="Learn more about geometric transformation from ScienceDirect's AI-generated Topic Pages" class="topic-link">geometric transformation</a> that is done without losing significant </span><a href="/topics/computer-science/image-feature" title="Learn more about image features from ScienceDirect's AI-generated Topic Pages" class="topic-link">image features</a> for the recognition procedure. Scripts with this purpose are also very popular and enables researchers to process their images in machines with memory limitations.</div><div class="u-margin-s-bottom" id="p0180"><a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about Linear Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Linear Discriminant Analysis</a> (LDA) is a technique very cited on the articles reviewed by this survey. The main idea of LDA is to project some set of axis on a new and smaller set of axis in such away that the separation between the labels of the data is more evident. To do that, LDA uses two parameters, the mean of each label on the new set of axis, and the variation of each label on it. The algorithm maximizes the distance between the means and minimizes the variation for each label.</div><div class="u-margin-s-bottom" id="p0185"><span>Another widely known and accepted by the scientific community approach is the use of <a href="/topics/engineering/principal-components" title="Learn more about Principal Component from ScienceDirect's AI-generated Topic Pages" class="topic-link">Principal Component</a> Analysis (PCA). PCA is a simple, non-parametric method that intends to isolate relevant information from a large set of input variables, using linear algebra’s analytical solutions to isolate the irrelevant feature vectors and eliminate them since there is no significance to keep them into account for the recognition procedure. With low effort, PCA is capable of provide a roadmap for reducing a complex data set into simplified structures that contain the important piece of information needed </span><a class="anchor anchor-primary" href="#b0435" name="bb0435" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0435"><span class="anchor-text-container"><span class="anchor-text">[87]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0190">Performing a brief analysis of the methods presented in the articles it can be observed that among the ones that cite dimensionality <a href="/topics/computer-science/reduction-algorithm" title="Learn more about reduction algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">reduction algorithms</a>, 44% used PCA, 25% LDA and 25% Down Sampling. Only 27 methods out of the 94 analyzed mentioned explicitly if they used any dimensionality reduction algorithm. An interesting fact found was that 77.78% of the uses of down sampling were for methods that used neural networks as classifiers and 87.5% of the methods that used the PCA algorithm had some classic algorithm as a classifier.</div></section></section><section id="s0070"><h2 id="st070" class="u-h4 u-margin-l-top u-margin-xs-bottom">5. Feature Extraction</h2><div class="u-margin-s-bottom" id="p0195"><span><span>Feature extraction is a very important stage in recognition methods, being mainly applied when each sample has a large amount of data. It has as main goal to extract only the most important and descriptive piece of information from the samples, getting rid of what is not relevant for the given problem. This is usually necessary because of the high <a href="/topics/computer-science/computational-complexity" title="Learn more about computational complexity from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational complexity</a><span> of training in <a href="/topics/computer-science/classification-models" title="Learn more about classification models from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification models</a> and due to the fact that the more data there is in each sample, the more computing is needed to achieve good results on these models. In general, it is very important that the extracted characteristics are simple and independent from each other, that is, there is not such a high correlation index between the sets of characteristics. Some classical algorithms for extracting features from images are Active Shape Model (ASM), </span></span><a href="/topics/engineering/local-binary-pattern" title="Learn more about Local Binary Patterns from ScienceDirect's AI-generated Topic Pages" class="topic-link">Local Binary Patterns</a><span> (LBP) and <a href="/topics/computer-science/histogram-of-oriented-gradient" title="Learn more about Histogram of Oriented Gradients from ScienceDirect's AI-generated Topic Pages" class="topic-link">Histogram of Oriented Gradients</a> (HOG). There are many variations of these algorithms being used on the Facial Emotion Recognition task to achieve distinct results </span></span><a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a>. In the last few years there can be observed some works that used Convolutional Neural Networks to extract features as well <a class="anchor anchor-primary" href="#b0020" name="bb0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0020"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, <a class="anchor anchor-primary" href="#b0530" name="bb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0530"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0200">The ASM algorithm was proposed in 1995 <a class="anchor anchor-primary" href="#b0070" name="bb0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0070"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a> with the intent of generating points that fit the contour of the object. Both LBP and HOG work splitting the image in small pieces called cells and generating histograms. LBP was proposed in 1994 by <a class="anchor anchor-primary" href="#b0360" name="bb0360" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0360"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a> and functions making comparisons between each cell’s pixels and their eight neighbors to build a binary number. Afterwards, the algorithm frames a histogram of this numbers for each cell and concatenates the histograms. More information about LBP can be found in <a class="anchor anchor-primary" href="#b0045" name="bb0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0045"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a>. The HOG algorithm, on the other hand, instead of making comparison between pixels, makes a histogram from the gradients previously calculated from them <a class="anchor anchor-primary" href="#b0085" name="bb0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0085"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0205"><span>There are also feature extraction attempts based on the <a href="/topics/computer-science/facial-action-coding-system" title="Learn more about Facial Action Coding System from ScienceDirect's AI-generated Topic Pages" class="topic-link">Facial Action Coding System</a> (FACS) </span><a class="anchor anchor-primary" href="#b0130" name="bb0130" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0130"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a>. FACS is a coding system created in 1976 by Paul Ekman and Wallace Friesen that measures the contraction and relaxation of facial muscles with degrees of intensity. It has undergone some changes over time making it more robust <a class="anchor anchor-primary" href="#b0120" name="bb0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0120"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a>. In general, works that try to find facial emotion using FACS use specific dataset to train neural networks that have facial images cataloged by FACS experts, so the feature extraction process is not computational but human.</div><div class="u-margin-s-bottom"><div id="p0210">The FACS analyzes the emotions in the face, which are classified through the movement of the facial musculature, which is coded by the system through AU - Action Units, AD - Action Descriptors and M - Movements. Each AU, AD or M corresponds to a code used to describe the specific movement of one or more muscles of the face, eye direction or head movement. <a class="anchor anchor-primary" href="#f0025" name="bf0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0025"><span class="anchor-text-container"><span class="anchor-text">Fig. 5</span></span></a> shows the AU classification.<a class="anchor anchor-primary" href="#fn3" name="bfn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a></div><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr5.jpg" height="539" alt="" aria-describedby="cn025"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (362KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (362KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn025"><p id="sp030"><span class="label">Fig. 5</span>. <a href="/topics/computer-science/facial-action-coding-system" title="Learn more about Facial Action Coding System from ScienceDirect's AI-generated Topic Pages" class="topic-link">Facial Action Coding System</a> (FACS).</p></span></span></figure></div></section><section id="s0075"><h2 id="st075" class="u-h4 u-margin-l-top u-margin-xs-bottom">6. Datasets for emotion recognition from images or videos</h2><div class="u-margin-s-bottom" id="p0215">Datasets containing facial images or videos specifically developed for emotion recognition are not a novelty. The first studies are related to <a class="anchor anchor-primary" href="#b0410" name="bb0410" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0410"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a>, and since then the need to develop computational approaches to inspect emotion or behavior in face images or videos has been significantly growing. As a consequence, many facial expression datasets have emerged, varying in acquisition environment, number of recognizable expressions, regions, among other features <a class="anchor anchor-primary" href="#b0365" name="bb0365" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0365"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a>. On the other hand, due to the way some of these datasets were created, many other points were also included over time such as gender, ethnicity, age, image quality (size, color, older datasets performed a digitization from physical photos), and number of participants. All these variables tend to influence the quality of the chosen dataset, as well as impact the results that computational approaches for emotion recognition output.</div><div class="u-margin-s-bottom"><div id="p0220">Since datasets play a fundamental role for data-driven learning recognition based on emotion recognition <a class="anchor anchor-primary" href="#b0430" name="bb0430" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0430"><span class="anchor-text-container"><span class="anchor-text">[86]</span></span></a>, this section aims to present a review of the most important datasets for emotion recognition in images or videos, recognized and widely used by the academical society. Additionally, here we pointed relevant implementation details used for the construction of those datasets, as well as its particularities an possible drawbacks. <a class="anchor anchor-primary" href="#f0030" name="bf0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0030"><span class="anchor-text-container"><span class="anchor-text">Fig. 6</span></span></a> shows the relationship between the performance of the reviewed methods and the datasets in which they were tested.</div><figure class="figure text-xs" id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr6.jpg" height="281" alt="" aria-describedby="cn030"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (38KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (38KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn030"><p id="sp035"><span class="label">Fig. 6</span>. This plot was generated using only classes with more than 3 members. The x axis represents the datasets used for testing the methods and the y axis represents the accuracy of the methods. Each colored dot represents one benchmarked method. It is important to emphasize that some of the approaches considered for this study may have applied the same method to multiple datasets.</p></span></span></figure></div><section id="s0080"><h3 id="st080" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.1. JAFFE</h3><div class="u-margin-s-bottom" id="p0225">The Japanese Female Facial Expression (JAFFE) <a class="anchor anchor-primary" href="#b0310" name="bb0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0310"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a><span><span><span> is a freely available dataset, published in 1998 by a group of Japanese researchers from the ATR <a href="/topics/computer-science/human-information-processing" title="Learn more about Human Information Processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">Human Information Processing</a> </span><a href="/topics/computer-science/research-laboratories" title="Learn more about Research Laboratory from ScienceDirect's AI-generated Topic Pages" class="topic-link">Research Laboratory</a> and the Psychology Department of Kyushu University. The dataset </span><a href="/topics/computer-science/image-feature" title="Learn more about features images from ScienceDirect's AI-generated Topic Pages" class="topic-link">features images</a><span> of 10 Japanese women, expressing the following emotions: happiness, sadness, surprise, anger, disgust, fear and neutral. Each participant took around 3 to 4 pictures of herself for each emotion while looking through a semi-reflective plastic sheet towards the camera, totaling 219 images. The camera was also surrounded by a <a href="/topics/engineering/black-box" title="Learn more about black box from ScienceDirect's AI-generated Topic Pages" class="topic-link">black box</a> to prevent and mitigate light reflections. The hair was removed from the front of the face in a manner that the expressions were more evident. Because the photographic process was performed analogously, the photos were digitized afterward. The final digital resolution for each image is 256x256 pixels. Images were rated for each emotion by a group of 92 Japanese female undergraduate on a 5-point scale. Each image has a 5 or 6 component vector representing the average ratings for each emotion. Besides not recent, this dataset has been used for several emotion recognition approaches, such as </span></span><a class="anchor anchor-primary" href="#b0320" name="bb0320" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0320"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a>, <a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, <a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>.</div></section><section id="s0085"><h3 id="st085" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.2. Cohn-Kanade</h3><div class="u-margin-s-bottom" id="p0230">Cohn-Kanade AU-Coded Expression Database (CK) <a class="anchor anchor-primary" href="#b0240" name="bb0240" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0240"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a> is a well-established dataset, being widely used in the literature since it’s first version <a class="anchor anchor-primary" href="#b0560" name="bb0560" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0560"><span class="anchor-text-container"><span class="anchor-text">[112]</span></span></a>, <a class="anchor anchor-primary" href="#b0470" name="bb0470" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0470"><span class="anchor-text-container"><span class="anchor-text">[94]</span></span></a>, <a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#b0285" name="bb0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0285"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a>, <a class="anchor anchor-primary" href="#b0205" name="bb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0205"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a><span>. This dataset was originally created with the name CMU-Pittsburgh AU-Coded Facial Expression Image Database, and is being managed ever since, by the (AAG) Affect Analysis Group of the Research Lab at the University of Pittsburgh. Back in the day, the first version of CK dataset was developed to fill the gap of large enough facial images sets to be able to support studies of <a href="/topics/computer-science/facial-feature" title="Learn more about facial feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial feature</a> tracking and analysis methods. Nowadays, the database counts with 2 versions: the first one was released in 2000 and includes 486 sequences about 97 different posers. From the subjects, there were 69% female, 31% male, 81% Euro-American, 13% Afro-American, and 6% other groups.</span></div><div class="u-margin-s-bottom" id="p0235">The CK is composed by sequences that varies from neutral to a peak expression, being the peak expression image fully FACS as presented in the previous sections, coded and given an emotion label. However, this emotion label refers to the expression that was requested, but not necessarily the one actually expressed by the participant. In the first version of the database, images were taken in a controlled room in terms of lights, background and camera position. Besides that, all expressions were collected by asking subjects to perform them, therefore, they may differ from spontaneously taken images. The CK dataset provides 2D images with either 640x490 or 640x480 resolution and pixel array with 8-bit gray-scale or 24-bit color values.</div><div class="u-margin-s-bottom" id="p0240">The second and the current latest version of the database was named The Extended Cohn-Kanade Dataset (CK+) <a class="anchor anchor-primary" href="#b0300" name="bb0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0300"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a> and released in 2010. For this version, there were included 107 sequences of emotion transitions and 26 subjects. Besides that, non-posed images were taken this time and each image received a nominal emotion label based on the subject’s impression of each of the 7 basic emotion categories, that is, Anger, Contempt, Disgust, Fear, Happy, Sadness and Surprise. The authors (The Affect Analysis Group) is still working in what should be the third version of this dataset, whose intention is to mainly add synchronized 30-degree images from the original frontal video, enabling a series of analysis such as 3D facial and emotion recognition.</div></section><section id="s0090"><h3 id="st090" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.3. MMI</h3><div class="u-margin-s-bottom" id="p0245">MMI is a dataset created from 25 people of different ethnicities (European, Asian and South American), being 44% women and 56% men, with their age ranged from 19 to 62&nbsp;years old. The expressions were recorded/photographed in a natural way, which means participants were induced to perform expressions based on videos used to stimulate (comedy videos and disgusting content) a most reliable natural condition. Papers <a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a>, <a class="anchor anchor-primary" href="#b0080" name="bb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0080"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, <a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a> worked with this dataset, and one of the points analyzed by one of authors was about the use of glasses and the difficulty that ended up being high for the recognition of emotions.</div><div class="u-margin-s-bottom" id="p0250">Opposite to other databases that existed at the time when only 6 basic expressions were parsed, MMI has many non-basic expressions and not defined expressions, since they were performed during an expression exchange. The images in the database are all real colors but after a digitization procedure its resolution was downgraded to 720x576 pixels, and the videos were recorded at 24 frames per second (sequences range from 40 to 520 frames), ranging from neutral, expression and neutral again. In total, the dataset has one hour and thirty-two minutes of content. The backgrounds on which the photos and videos were captured is not the same for every sample. The dataset is accessible online and easily searchable, and is freely available to the scientific community.</div></section><section id="s0095"><h3 id="st095" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.4. FER2013</h3><div class="u-margin-s-bottom" id="p0255"><span>The FER2013 database was originally published in the International Conference on <a href="/topics/computer-science/machine-learning" title="Learn more about Machine Learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">Machine Learning</a> (ICML in 2013) </span><a class="anchor anchor-primary" href="#b0155" name="bb0155" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0155"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a>. This set of pictures is publicly open and was created for a project by Pierre-Luc Carrier and Aaron Courville. It was publicly shared to be used in a facial expression recognition competition in Kaggle, shortly named as ICML.</div><div class="u-margin-s-bottom" id="p0260">This dataset consists of 35.887 pictures of faces with 48x48 pixels in grayscale, all images are labeled in seven facial expressions and distributed as follows: 4953 angry images, 547 disgust images, 5121 fear images, 8989 happy images, 6077 sad images, 4002 surprise images and 6198 neutral images. The dataset was developed using the Google image search API in which was searched for face images that are within a set of 184 keywords related to emotion, such as ”happy”, ”sad”, and others.</div><div class="u-margin-s-bottom" id="p0265">During the Kaggle’s competition, 28709 images from this dataset were shared between the participants to train their neural network and 3589 images were used to the test set and validation to figure out the winning <a href="/topics/computer-science/recognition-algorithm" title="Learn more about recognition algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition algorithm</a> on the competition. After the competition’s ending, the dataset were made accessible to the general public. Besides the large number of images, this dataset has a very reduced spatial resolution since its purpose is to be used as input to train and test computational classifiers.</div></section><section id="s0100"><h3 id="st100" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.5. BU-3DFE</h3><div class="u-margin-s-bottom" id="p0270">Binghamton University 3D Facial Expression (BU-3DFE) <a class="anchor anchor-primary" href="#b0570" name="bb0570" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0570"><span class="anchor-text-container"><span class="anchor-text">[114]</span></span></a> was designed to meet the need for a dataset with emotion-classified 3D facial images, thus allowing its use for the facial expression recognition area. In 2006, the publication date of the article ”The 3D Facial Expression Database for Facial Behavior Research” <a class="anchor anchor-primary" href="#b0570" name="bb0570" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0570"><span class="anchor-text-container"><span class="anchor-text">[114]</span></span></a><span><span> did not detect any other dataset that fulfilled this role. This dataset is free for academic use. Images are linked to the <a href="/topics/computer-science/emotional-state" title="Learn more about emotional states from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional states</a> anger, disgusting, fear, happiness, sadness, surprise and neutral. In addition, there is a degree of spontaneity associated with each image. A system with 6 cameras positioned at different angles was used to capture the images. The stereo </span><a href="/topics/chemical-engineering/photogrammetry" title="Learn more about photogrammetry from ScienceDirect's AI-generated Topic Pages" class="topic-link">photogrammetry</a> technique was used to construct the three-dimensional face. A total of 100 participants had their expressions scanned, being 60 percent woman and 40 percent man, totalling 2500 3D models. In addition, all participants were part of the psychology, arts and engineering departments. Participants were asked to demonstrate the 7 emotions with 4 degrees of intensity, low, middle, high and highest. There are subsequent studies to BU-3DFE, such as BU-4DFE in which the time dimension was added, BP4D-Spontaneous and BP4D +.</span></div></section><section id="s0105"><h3 id="st105" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.6. Miscellaneous</h3><div class="u-margin-s-bottom" id="p0275">Besides the databases presented above, there are some datasets that are worth mentioning, such as CASIA Webface <a class="anchor anchor-primary" href="#b0565" name="bb0565" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0565"><span class="anchor-text-container"><span class="anchor-text">[113]</span></span></a>, FEEDB <a class="anchor anchor-primary" href="#b0475" name="bb0475" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0475"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a>, RaFD <a class="anchor anchor-primary" href="#b0270" name="bb0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0270"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a>, National Institute of Mental Health Child Emotional Faces Picture Set (NIMH-ChEFS) <a class="anchor anchor-primary" href="#b0105" name="bb0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0105"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a>, Taiwanese Facial Expression Image Database <a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>, Acted Facial Expressions In The Wild <a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0280">As a public large dataset composed from images collected from the web, <a class="anchor anchor-primary" href="#b0565" name="bb0565" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0565"><span class="anchor-text-container"><span class="anchor-text">[113]</span></span></a> contains almost five hundred thousand images from more than ten thousand subjects. <a class="anchor anchor-primary" href="#b0475" name="bb0475" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0475"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a><span> is a dataset of video sequences, recorded using <a href="/topics/computer-science/microsoft-kinect-sensor" title="Learn more about Microsoft Kinect sensor from ScienceDirect's AI-generated Topic Pages" class="topic-link">Microsoft Kinect sensor</a> and it was developed to serve as a dataset to be applied in facial recognition and facial expression/emotions studies. </span><a class="anchor anchor-primary" href="#b0270" name="bb0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0270"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a> is a initiative from the Behavioural Science Institute of the Radboud University Nijmegen. It contains 8 different emotions from 67 models and it is freely available for non-commercial scientific research by researchers who work for an officially accredited university. The <a class="anchor anchor-primary" href="#b0105" name="bb0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0105"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a> is a little bit different from the datasets presented above because it is composed by images of children only. It contains 482 high quality colored images about five emotions and of direct and averted gaze. <a class="anchor anchor-primary" href="#b0055" name="bb0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0055"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a> was developed by Brain Mapping Laboratory (National Yang-Ming University) and Integrated Brain Research Unit (Taipei Veterans General Hospital) and has 8 different kind so emotions from 40 models. The dataset is opened for scientific researches only. The <a class="anchor anchor-primary" href="#b0100" name="bb0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0100"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a> dataset proposes a face image extraction from movies, that is, a out of the scope of lab controlled datasets. The authors compare their dataset with <a class="anchor anchor-primary" href="#b0300" name="bb0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0300"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a>, <a class="anchor anchor-primary" href="#b0310" name="bb0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0310"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0285">Over the internet, there are a lot of datasets with distinct characteristics available, most of the time for free. Even though there is such a <a href="/topics/computer-science/great-number" title="Learn more about great number from ScienceDirect's AI-generated Topic Pages" class="topic-link">great number</a> of datasets, sometimes they are not exactly what researches are looking for. Therefore, likewise all datasets cited, a great quantity of projects on facial expression/emotion recognition prefer to use their own dataset, not being possible to the public to use it.</div></section></section><section id="s0110"><h2 id="st110" class="u-h4 u-margin-l-top u-margin-xs-bottom">7. Classification algorithms</h2><div class="u-margin-s-bottom" id="p0290">Besides all operations being of extreme importance to accomplish good results when classifying emotions from face expressions, usually the most valued stage in the whole process is the classification. This step is also the one with most variations between works over the literature. As mentioned on Section <a class="anchor anchor-primary" href="#s0005" name="bs0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0005"><span class="anchor-text-container"><span class="anchor-text">1</span></span></a>, there have been some arising methods based on neural network architectures over the past few years, mainly because of the ascension of computer capabilities. Taken the 51 works selected for this scope, we divided them into two distinct groups: Classic and NNB approaches.</div><div class="u-margin-s-bottom" id="p0295">The first group refers to classical techniques used to categorize facial emotion expressions, that is, traditional image processing, computer vision, pattern recognition and misc classifiers, while the second one is composed exclusively by neural network based approaches, including the CNN arising methods.</div><div class="u-margin-s-bottom" id="p0300">Both groups will be presented with more details in the next subsections, such as the algorithms included in each one of them.</div><section id="s0115"><h3 id="st115" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.1. Classical approaches</h3><div class="u-margin-s-bottom"><div id="p0305">For many years, the classical approaches for image processing in general, were the best attempt to solve some problems with high processing cost. Over time, with the development of computer capability and the advent of new architectures, new methods took place for multiple reasons, such as implementation complexity and achievement of great results. However, some methods (that we call classical in this paper) have achieved great results and are still widely used nowadays. From the chosen papers for this literature review, the classical approaches usage is shown in the <a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig. 7</span></span></a><span><span>. In the graph it is possible to see that <a href="/topics/computer-science/support-vector-machine" title="Learn more about Support Vector Machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Support Vector Machine</a><span> (SVM) is the flagship when talking about classical classification methods, corresponding to 40% of the approaches. The second mostly applied method is the <a href="/topics/computer-science/dynamic-bayesian-network" title="Learn more about Dynamic Bayesian Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Dynamic Bayesian Network</a> with only 10% of the appliances. The 13,3% corresponding to ”Others” is composed by algorithms that were applied only once over the 51 selected articles. These methods are: </span></span><a href="/topics/engineering/extreme-learning-machine" title="Learn more about extreme learning machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">extreme learning machine</a> </span><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, extreme sparse learning <a class="anchor anchor-primary" href="#b0440" name="bb0440" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0440"><span class="anchor-text-container"><span class="anchor-text">[88]</span></span></a><span>, <a href="/topics/computer-science/support-vector-regression" title="Learn more about support vector regression from ScienceDirect's AI-generated Topic Pages" class="topic-link">support vector regression</a> </span><a class="anchor anchor-primary" href="#b0600" name="bb0600" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0600"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a><span>, <a href="/topics/computer-science/sparse-representation" title="Learn more about sparse representation from ScienceDirect's AI-generated Topic Pages" class="topic-link">sparse representation</a> classification </span><a class="anchor anchor-primary" href="#b0620" name="bb0620" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0620"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a><span>, <a href="/topics/computer-science/random-decision-forest" title="Learn more about random forest from ScienceDirect's AI-generated Topic Pages" class="topic-link">random forest</a> </span><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a>, random tree <a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a>, many graph embedding <a class="anchor anchor-primary" href="#b0225" name="bb0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0225"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a> and a single modified Viola-Jones <a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a>.</div><figure class="figure text-xs" id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr7.jpg" height="199" alt="" aria-describedby="cn035"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (54KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (54KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn035"><p id="sp040"><span class="label">Fig. 7</span>. Scientific share for classical approaches verified for this literature review.</p></span></span></figure></div><section id="s0120"><h4 id="st120" class="u-margin-m-top u-margin-xs-bottom">7.1.1. Support Vector Machines (SVM)</h4><div class="u-margin-s-bottom" id="p0310">As it is possible to visualize in <a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig. 7</span></span></a><span> that support vector machines (SVMs) are by far the most applied classical method to classify emotions from face images. SVMs are supervised learning methods, that is, algorithms capable of generating input–output <a href="/topics/computer-science/mapping-function" title="Learn more about mapping functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">mapping functions</a><span> from a specific set of labeled data with one or more feature vectors. SVMs classify elements by determining boundaries, known as <a href="/topics/engineering/hyperplanes" title="Learn more about hyperplane from ScienceDirect's AI-generated Topic Pages" class="topic-link">hyperplane</a><span>, that separate the classes from each other. This <a href="/topics/computer-science/hyperplanes" title="Learn more about hyperplane from ScienceDirect's AI-generated Topic Pages" class="topic-link">hyperplane</a> can be oriented in different ways and still fulfill it’s purpose, however, SVM goal is to orient it in such a way that it is as far as possible from the closest vector (called support vector) from each class </span></span></span><a class="anchor anchor-primary" href="#b0190" name="bb0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0190"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a><span>. Initially, these <a href="/topics/engineering/machine-learning-technique" title="Learn more about machine learning techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning techniques</a> were developed in 1963 to perform classification in linearly separable sets of data </span><a class="anchor anchor-primary" href="#b0520" name="bb0520" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0520"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a><span><span>, however, with the assistance of nonlinear <a href="/topics/computer-science/kernel-function" title="Learn more about kernel functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">kernel functions</a>, it is possible to transform input data to a high-dimensional </span><a href="/topics/engineering/feature-space" title="Learn more about feature space from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature space</a> in which the input data become linearly separable and, therefore, classifiable by the SVM algorithm </span><a class="anchor anchor-primary" href="#b0540" name="bb0540" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0540"><span class="anchor-text-container"><span class="anchor-text">[108]</span></span></a><span>. Because of it’s simplicity in therms of processing power needed (in comparison with NNBs), SVM is a great tool for <a href="/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning</a> and, with the help of nonlinear kernels, is capable of competing with the newest and most complex methods in therms of results. Besides the face/emotion recognition, SVMs have been used to solve classification of all sources over the past few years, including the medical area and many others </span><a class="anchor anchor-primary" href="#b0610" name="bb0610" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0610"><span class="anchor-text-container"><span class="anchor-text">[122]</span></span></a>.</div></section><section id="s0125"><h4 id="st125" class="u-margin-m-top u-margin-xs-bottom">7.1.2. Dynamic Bayesian Network</h4><div class="u-margin-s-bottom" id="p0315"><span>The Dynamic Bayesian Networks (DBNs) are <a href="/topics/computer-science/bayesian-networks" title="Learn more about Bayesian networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian networks</a> (BN) for processes with changing environments. Therefore, to understand what a DBN is, it is necessary to understand a BN. Thus, according to </span><a class="anchor anchor-primary" href="#b0445" name="bb0445" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0445"><span class="anchor-text-container"><span class="anchor-text">[89]</span></span></a><span>, Bayesian networks (BNs) are directed acyclic graphs, which represent the probabilistic relationships between a large number of variables. It is important to note that they work with <a href="/topics/computer-science/incomplete-knowledge" title="Learn more about incomplete knowledge from ScienceDirect's AI-generated Topic Pages" class="topic-link">incomplete knowledge</a> and allow machines to make inferences, predictions and make decisions based on the probability of variables that assume some states, solving the problem.</span></div><div class="u-margin-s-bottom"><div id="p0320"><span>A DBN or temporal Bayesian network, as well as BNs, must be acyclic and targeted. The difference is that DBN is a way of extending the BN to distribute the probabilities across infinite collections of <a href="/topics/engineering/random-variable-xi" title="Learn more about random variables from ScienceDirect's AI-generated Topic Pages" class="topic-link">random variables</a>. </span><a class="anchor anchor-primary" href="#f0040" name="bf0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0040"><span class="anchor-text-container"><span class="anchor-text">Fig. 8</span></span></a> shows a DBN.</div><figure class="figure text-xs" id="f0040"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr8.jpg" height="193" alt="" aria-describedby="cn040"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr8_lrg.jpg" target="_blank" download="" title="Download high-res image (66KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (66KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr8.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn040"><p id="sp045"><span class="label">Fig. 8</span>. <a href="/topics/computer-science/dynamic-bayesian-network" title="Learn more about Dynamic Bayesian Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Dynamic Bayesian Network</a>.</p></span></span></figure></div></section><section id="s0130"><h4 id="st130" class="u-margin-m-top u-margin-xs-bottom">7.1.3. Fuzzy Logic</h4><div class="u-margin-s-bottom"><div id="p0325"><span>The fuzzy logic concept was born in 1965 with the proposal of the <a href="/topics/computer-science/fuzzy-sets" title="Learn more about fuzzy set from ScienceDirect's AI-generated Topic Pages" class="topic-link">fuzzy set</a> theory by Lofti Zadeh </span><a class="anchor anchor-primary" href="#b0585" name="bb0585" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0585"><span class="anchor-text-container"><span class="anchor-text">[117]</span></span></a><span><span> with the objective of <a href="/topics/computer-science/natural-language-processing" title="Learn more about natural language processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">natural language processing</a>. The goal of fuzzy logic in general, is to change the very basis of boolean computer processing, being able to produce results based on degrees of truth instead of 1 or 0. In other words, it is based on the idea that people make decisions over non-numerical information and, therefore, fuzzy is able to imitate these kind of decision making that sometimes is not absolute. For face emotion recognition, the fundamental operation is the selection of the most important features to be analyzed (usually made by an specialist or based on FACS) and the attribution of weights for each of these features. Besides that, the human involved has to build the </span><a href="/topics/engineering/fuzzy-rules" title="Learn more about fuzzy rules from ScienceDirect's AI-generated Topic Pages" class="topic-link">fuzzy rules</a> base, which is responsible for the evaluation of each sample and, therefore, each feature value may be classified into the best matching set. The overall process of a fuzzy system is presented in </span><a class="anchor anchor-primary" href="#f0045" name="bf0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0045"><span class="anchor-text-container"><span class="anchor-text">Fig. 9</span></span></a><span>. Firstly, the raw input need to be fuzzyfied in order to have truth based values instead of boolean ones, as discussed before. The <a href="/topics/engineering/fuzzification" title="Learn more about fuzzification from ScienceDirect's AI-generated Topic Pages" class="topic-link">fuzzification</a> process is done for every feature that is wanted to have some weight in the decision making. In the inference step, each feature is analyzed based on a previously determined set of rules with antecedents and consequences. The rules set is the very intelligence in a fuzzy set thus the determinative factor of a fuzzy system’s accuracy. Finally, the fuzzy output needs to be defuzzified in order to serve as an applicable action for the computer. Fuzzy logic have been used since it’s beginning mainly for control systems for the most different areas </span><a class="anchor anchor-primary" href="#b0505" name="bb0505" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0505"><span class="anchor-text-container"><span class="anchor-text">[101]</span></span></a>.</div><figure class="figure text-xs" id="f0045"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr9.jpg" height="155" alt="" aria-describedby="cn045"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr9_lrg.jpg" target="_blank" download="" title="Download high-res image (33KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (33KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr9.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn045"><p id="sp050"><span class="label">Fig. 9</span>. Schematic representation of a Fuzzy system to solve the recognition problem.</p></span></span></figure></div></section><section id="s0135"><h4 id="st135" class="u-margin-m-top u-margin-xs-bottom">7.1.4. Miscellaneous</h4><div class="u-margin-s-bottom" id="p0330">As showed in <a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig. 7</span></span></a><span>, some methods different from the ones presented above, were applied over the articles selected for these review. Since classical approaches, for this paper, means every single method that is not based on <a href="/topics/earth-and-planetary-sciences/artificial-neural-network" title="Learn more about artificial neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">artificial neural network</a>, there are a lot of particular methods that were applied by less than four works and will not be detailed in this document. The methods with unique occurrences were already mentioned in Section </span><a class="anchor anchor-primary" href="#s0115" name="bs0115" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0115"><span class="anchor-text-container"><span class="anchor-text">7.1</span></span></a><span> and besides them, some methods with three article usages were plotted in the graph, they were: <a href="/topics/engineering/adaboost" title="Learn more about Adaboost from ScienceDirect's AI-generated Topic Pages" class="topic-link">Adaboost</a><span>, Naive Bayes (NB), K-Nearest Neighbors (KNN), Hidden Markov Model, <a href="/topics/computer-science/euclidean-distance" title="Learn more about Euclidean Distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Euclidean Distance</a><span> and <a href="/topics/computer-science/decision-trees" title="Learn more about Decision Tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">Decision Tree</a>. Adaboost, as mentioned in Section </span></span></span><a class="anchor anchor-primary" href="#s0050" name="bs0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0050"><span class="anchor-text-container"><span class="anchor-text">4.2.2</span></span></a><span><span>, is in fact a booster and not a <a href="/topics/computer-science/classification-technique" title="Learn more about classification technique from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification technique</a> and therefore, was used in three articles in combination with other methods such as Support Vector Machines and Dynamic Bayesian Network. The Naive Bayes is a probabilistic technique and special case of Bayesian theorem that bases itself in particular features to classify an unknown sample. KNN is a method that projects the sample in a feature space and decides the class of the being-analyzed sampled based on the it’s k-nearest neighbours in the space, for that, it uses Euclidean distance for measurements. The definition of Hidden Markov Model states that it is a doubly </span><a href="/topics/earth-and-planetary-sciences/stochastic-process" title="Learn more about stochastic process from ScienceDirect's AI-generated Topic Pages" class="topic-link">stochastic process</a> with a hidden stochastic process that can only be visualized with another set of processes </span><a class="anchor anchor-primary" href="#b0385" name="bb0385" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0385"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a><span>. The <a href="/topics/engineering/euclidean-distance" title="Learn more about Euclidean Distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Euclidean Distance</a> method basically consist of comparing the euclidean distance of the being-analyzed sample with the training set. In the Decision Tree method, the leafs of the tree represent each class to which the samples must be classified. The other nodes of the tree, correspond to the splitting rules. As a sample move throw the tree, starting from the root, it will be classified to the best fitting class.</span></div></section></section><section id="s0140"><h3 id="st140" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.2. Neural Network Based Approaches (NNB)</h3><div class="u-margin-s-bottom"><div id="p0335">The use of convolutional neural networks to solve computer vision problems has been successful in various areas, including emotion processing in real time <a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a><span>. It is important to remember that what is considered in this paper as Neural Network Based Approaches include not only Convolutional Neural Networks but also, as presented by the authors, Multi-layer <a href="/topics/earth-and-planetary-sciences/self-organizing-systems" title="Learn more about Perceptrons from ScienceDirect's AI-generated Topic Pages" class="topic-link">Perceptrons</a> and Neural Networks. However, as it is visible in </span><a class="anchor anchor-primary" href="#f0050" name="bf0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0050"><span class="anchor-text-container"><span class="anchor-text">Fig. 10</span></span></a>, that the predominant method used in the set of articles selected for this work is the CCN approach, which achieved over 65% of the NNBs applications. As we are working with NNBs, the ”Neural Network” that composes more than 21% in <a class="anchor anchor-primary" href="#f0050" name="bf0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0050"><span class="anchor-text-container"><span class="anchor-text">Fig. 10</span></span></a> are in fact, as reported by the authors as Neural Network in general. Therefore, the first step is to understand what is in fact a neural network.</div><figure class="figure text-xs" id="f0050"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr10.jpg" height="177" alt="" aria-describedby="cn050"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr10_lrg.jpg" target="_blank" download="" title="Download high-res image (27KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (27KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr10.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn050"><p id="sp055"><span class="label">Fig. 10</span>. Usage of <a href="/topics/computer-science/neural-network" title="Learn more about NNB from ScienceDirect's AI-generated Topic Pages" class="topic-link">NNB</a><span> approaches on the chosen set of papers. The yellow portion in the figure represents the works that did not specify the architecture of the <a href="/topics/earth-and-planetary-sciences/artificial-neural-network" title="Learn more about Artificial Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Artificial Neural Network</a> used and, therefore, were considered as a generic Neural Networks.</span></p></span></span></figure></div><div class="u-margin-s-bottom" id="p0340">Artificial neural networks (ANN) are an attempt to simulate (simplified, of course, but still) what happens in human brain where electrical signals are forwarded through nerve cells that are only capable of communicating with each other, the neurons <a class="anchor anchor-primary" href="#b0165" name="bb0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0165"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a><span>. In other words, an <a href="/topics/engineering/artificial-neural-network" title="Learn more about artificial neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">artificial neural network</a> is a set of processing units called nodes, that simulates the neurons of an actual brain and are interconnected through a set of values called weights </span><a class="anchor anchor-primary" href="#b0075" name="bb0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0075"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a><span>. Weights are the simulation of <a href="/topics/engineering/synaptic-connection" title="Learn more about synaptic connections from ScienceDirect's AI-generated Topic Pages" class="topic-link">synaptic connections</a> that happen between neurons in the nervous system. This elements generally have variable parameters that affect the signal that pass throw them and, together with nodes, composes the basis of a ANN. The configuration of the network may vary depending on which problem it will be designated for and different configurations result in numerous types of ANNs </span><a class="anchor anchor-primary" href="#b0450" name="bb0450" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0450"><span class="anchor-text-container"><span class="anchor-text">[90]</span></span></a>. Therefore, MLP and CNN are specific configurations of ANNs and yet have different results on classifying facial emotion expressions.</div><section id="s0145"><h4 id="st145" class="u-margin-m-top u-margin-xs-bottom">7.2.1. Multi Layer Perceptron (MLP)</h4><div class="u-margin-s-bottom" id="p0345"><span>MLP is an artificial neural network architecture that uses a structure known as hidden layers to increase the degree of accuracy of its classifications. Hidden layers are layers of neurons that receive the outputs of the previous layers multiplied by their respective connection weights, add them up, and multiply by an <a href="/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages" class="topic-link">activation function</a><span>. The <a href="/topics/computer-science/sigmoid-function" title="Learn more about sigmoid function from ScienceDirect's AI-generated Topic Pages" class="topic-link">sigmoid function</a> is one of the most used activation functions and has its definition as:</span></span><span class="display"><span id="e0005" class="formula"><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>F</mi><mo stretchy=&quot;false&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>x</mi><mo stretchy=&quot;false&quot; is=&quot;true&quot;>)</mo><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn><mo is=&quot;true&quot;>-</mo><msup is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>e</mi></mrow><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>-</mo><mi is=&quot;true&quot;>x</mi></mrow></msup></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="13.414ex" height="3.471ex" viewBox="0 -945.9 5775.5 1494.4" role="img" focusable="false" style="vertical-align: -1.274ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-46"></use></g><g is="true" transform="translate(749,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(1139,0)"><use xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(1711,0)"><use xlink:href="#MJMAIN-29"></use></g><g is="true" transform="translate(2378,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3157,0)"><g transform="translate(397,0)"><rect stroke="none" width="2100" height="60" x="0" y="220"></rect><g is="true" transform="translate(873,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(59,-400)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g><g is="true" transform="translate(353,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(904,0)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-65"></use></g></g><g is="true" transform="translate(329,204)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(389,0)"><use transform="scale(0.5)" xlink:href="#MJMATHI-78"></use></g></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">F</mi><mo stretchy="false" is="true">(</mo><mi is="true">x</mi><mo stretchy="false" is="true">)</mo><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mo is="true">-</mo><mi is="true">x</mi></mrow></msup></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-2"><math><mrow is="true"><mi is="true">F</mi><mo stretchy="false" is="true">(</mo><mi is="true">x</mi><mo stretchy="false" is="true">)</mo><mo linebreak="goodbreak" is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mo is="true">-</mo><mi is="true">x</mi></mrow></msup></mrow></mfrac></mrow></math></script></span></span></span><span><span><span>The use of hidden layers was only possible thanks to the development of <a href="/topics/computer-science/training-algorithm" title="Learn more about training algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">training algorithms</a> for MLP, such as the classic </span><a href="/topics/engineering/backpropagation" title="Learn more about Backpropagation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Backpropagation</a><span><span> algorithm. <a href="/topics/chemical-engineering/backpropagation" title="Learn more about Backpropagation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Backpropagation</a> is a way to solve the problem of finding how much the weights of the network must be changed in order to the result to be closer to the correct one. It uses the observation that data at any point in the neural network is generated by a </span><a href="/topics/computer-science/composite-function" title="Learn more about composite function from ScienceDirect's AI-generated Topic Pages" class="topic-link">composite function</a> and the derivative chain rule to calculate the downward gradient and then understand how the weights should be changed. The only problem with this technique is getting stuck in </span></span><a href="/topics/engineering/local-minimum" title="Learn more about local minimums from ScienceDirect's AI-generated Topic Pages" class="topic-link">local minimums</a><span><span> and failing to converge to a global minimum. To try to avoid the previous problem some dynamic <a href="/topics/computer-science/learning-rate" title="Learn more about learning rate from ScienceDirect's AI-generated Topic Pages" class="topic-link">learning rate</a> heuristics are used. </span><a href="/topics/engineering/genetic-algorithm" title="Learn more about Genetic algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">Genetic algorithms</a> are also possible for training an MLP.</span></span></div></section><section id="s0150"><h4 id="st150" class="u-margin-m-top u-margin-xs-bottom">7.2.2. Convolutional Neural Netwoks (CNNs)</h4><div class="u-margin-s-bottom" id="p0350">Writing about CNNs in a Section on classification algorithms can be a little inaccurate. The reason for this is that although CNNs are classifiers they do not only have this role within the pipeline established at the beginning of this article, but the combination of the feature extraction step with the classification step. Convolutional neural networks have emerged as an alternative to extracting characteristics through <a href="/topics/computer-science/human-knowledge" title="Learn more about human knowledge from ScienceDirect's AI-generated Topic Pages" class="topic-link">human knowledge</a><span><span> on a given topic. The main idea of the algorithm is to use the <a href="/topics/computer-science/convolution-operation" title="Learn more about convolution operation from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution operation</a> to extract characteristics from the image, making the </span><a href="/topics/computer-science/position-information" title="Learn more about position information from ScienceDirect's AI-generated Topic Pages" class="topic-link">position information</a> of the pixel with its neighborhood more atomic for the classification process.</span></div><div class="u-margin-s-bottom"><div id="p0355"><span><span>The first operation performed in the classification step by a CNN is the convolution between matrices called filters and a <a href="/topics/engineering/subspace" title="Learn more about sub space from ScienceDirect's AI-generated Topic Pages" class="topic-link">sub space</a> of the original image, generating several new images of smaller size than the original one. There is one output image for each filter applied to the image. The data generated by this process is called feature maps. The second step is to apply nonlinearity to feature maps, where one of the common </span><a href="/topics/computer-science/nonlinear-function" title="Learn more about nonlinear functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">nonlinear functions</a><span> is Relu. The non-linear function is applied to each pixel of the feature maps, creating rectified feature maps. The new set of images obtained could be passed through the same two processes explained previously and this could be repeated many times. An optional step at this point would be to add a pooling layer to decrease the amount of data to be passed on to the next step. One of the possible common pooling functions in the CNN architectures used by the reviewed articles is max-pooling, which is basically characterized by dividing the images of the feature maps into sub spaces and taking the maximum value of the sub space as a representative of the entire subset. In the pipeline defined earlier by this article, this step would be within the dimensionality reduction box. After these preprocessing, a conventional MLP is normally used to classify the data. The <a href="/topics/computer-science/architectural-pattern" title="Learn more about architectural pattern from ScienceDirect's AI-generated Topic Pages" class="topic-link">architectural pattern</a> for CNNs can be seen in </span></span><a class="anchor anchor-primary" href="#f0055" name="bf0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0055"><span class="anchor-text-container"><span class="anchor-text">Fig. 11</span></span></a>.</div><figure class="figure text-xs" id="f0055"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr11.jpg" height="183" alt="" aria-describedby="cn055"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr11_lrg.jpg" target="_blank" download="" title="Download high-res image (93KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (93KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr11.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn055"><p id="sp060"><span class="label">Fig. 11</span>. Common <a href="/topics/computer-science/convolutional-neural-network" title="Learn more about CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN</a> Architecture and its feature extraction convolutional pool.</p></span></span></figure></div></section></section></section><section id="s0155"><h2 id="st155" class="u-h4 u-margin-l-top u-margin-xs-bottom">8. Emotion Recognition Methods from Facial Images or Videos</h2><div class="u-margin-s-bottom"><div id="p0360">Due to the increasing use of NNB methods for the emotion classification (<a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig. 1</span></span></a>), these kind of methods’s performance was expected to be superior than the classical methods, however, when analysing the accuracy of each method proposed by the selected papers, it was possible to observe that, on average, the applications that applied classical methods achieved better results (<a class="anchor anchor-primary" href="#f0060" name="bf0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0060"><span class="anchor-text-container"><span class="anchor-text">Fig. 12</span></span></a>).</div><figure class="figure text-xs" id="f0060"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr12.jpg" height="339" alt="" aria-describedby="cn060"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr12_lrg.jpg" target="_blank" download="" title="Download high-res image (105KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (105KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr12.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn060"><p id="sp065"><span class="label">Fig. 12</span>. The plot was generated filtering our data with two rules. The first one was take the combination of datasets for training that had at least two members to represents the class. The second was to delete the classes of the plot that has members only for neural based or classical algorithms.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0365">However, looking closer to this information in the general flow of computational facial emotion recognition presented on Section <a class="anchor anchor-primary" href="#s0025" name="bs0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0025"><span class="anchor-text-container"><span class="anchor-text">3</span></span></a><span>, it is possible to identify that the comparison may not be trustworthy. When observing accuracy achieved from both method, we are ignoring all the other steps of the general flow, such as preprocessing, feature extraction and database selection. Taking the method all alone, despite the other steps of the flow, the accuracy may be better in classical applications, however, every step should be analyzed to obtain a <a href="/topics/computer-science/fair-comparison" title="Learn more about fair comparison from ScienceDirect's AI-generated Topic Pages" class="topic-link">fair comparison</a>.</span></div><div class="u-margin-s-bottom"><div id="p0370"><a class="anchor anchor-primary" href="#f0065" name="bf0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0065"><span class="anchor-text-container"><span class="anchor-text">Fig. 13</span></span></a> presents a complement on the information by <a class="anchor anchor-primary" href="#f0060" name="bf0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0060"><span class="anchor-text-container"><span class="anchor-text">Fig. 12</span></span></a>. The datasets usage is unbalanced in terms of classical and NNB approaches. The CK&nbsp;+&nbsp;dataset (Section <a class="anchor anchor-primary" href="#s0085" name="bs0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0085"><span class="anchor-text-container"><span class="anchor-text">6.2</span></span></a>, which is the most used by the NNB methods, is a much wider dataset than JAFFE. That may partially explain the accuracy difference between classical and NNB approaches, giving motive to the expansion of NNB usage, since they achieved great results while working with large amounts of data.</div><figure class="figure text-xs" id="f0065"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr13.jpg" height="247" alt="" aria-describedby="cn065"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr13_lrg.jpg" target="_blank" download="" title="Download high-res image (82KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (82KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0020025521010136-gr13.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cn065"><p id="sp070"><span class="label">Fig. 13</span>. Usage of distinct datasets by both NNB and Classical approaches reflects directly in the final accuracy of the method.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0375"><span>Since each part of the general flow may influence the final result and a comparison between approaches is incomplete when looking into accuracy exclusively, this section intends to present some methods applied by 10 selected works, describing the complete process, from the dataset choice to the <a href="/topics/computer-science/classification-technique" title="Learn more about classification technique from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification technique</a>. For this purpose, a rank of both classical and NNB best accuracy papers for each principal dataset was designed (</span><a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a>). The complete data with the 94 methods analyzed is presented in <a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a>.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0010"><span class="captions text-s"><span id="cn075"><p id="sp080"><span class="label">Table 2</span>. Top Accuracy(Acc) papers for each one of the most used datasets by the papers in the study.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Dataset</th><th scope="col" class="align-left">Classification</th><th scope="col" class="align-left">Average Acc (%)</th><th scope="col" class="align-left">Best Acc (%)</th><th scope="col" class="align-left">Best Acc Paper(s)</th><th scope="col" class="align-left">Year</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Jaffe</td><td class="align-left">Classical</td><td class="align-left">92,91</td><td class="align-left">100</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">2015</td></tr><tr class="valign-top"><td class="align-left"></td><td class="align-left">NNB</td><td class="align-left">86,00</td><td class="align-left">100</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, <a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></td><td class="align-left">2015/ 2018</td></tr><tr class="valign-top"><td class="align-left">CK</td><td class="align-left">Classical</td><td class="align-left">92,40</td><td class="align-left">100</td><td class="align-left"><a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a></td><td class="align-left">2018</td></tr><tr class="valign-top"><td class="align-left"></td><td class="align-left">NNB</td><td class="align-left">87,37</td><td class="align-left">99,75</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">2015</td></tr><tr class="valign-top"><td class="align-left">CK+</td><td class="align-left">Classical</td><td class="align-left">83,88</td><td class="align-left">94,09</td><td class="align-left"><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></td><td class="align-left">2015</td></tr><tr class="valign-top"><td class="align-left"></td><td class="align-left">NNB</td><td class="align-left">96,76</td><td class="align-left">99,68</td><td class="align-left"><a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left">2017</td></tr><tr class="valign-top"><td class="align-left">FER 2013</td><td class="align-left">NNB</td><td class="align-left">64,24</td><td class="align-left">64,24</td><td class="align-left"><a class="anchor anchor-primary" href="#b0135" name="bb0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0135"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></td><td class="align-left">2018</td></tr><tr class="valign-top"><td class="align-left">Bu-3dfe</td><td class="align-left">NNB</td><td class="align-left">91,89</td><td class="align-left">91,89</td><td class="align-left"><a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left">2017</td></tr><tr class="valign-top"><td class="align-left">MMI</td><td class="align-left">Classical</td><td class="align-left">87,73</td><td class="align-left">100</td><td class="align-left"><a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></td><td class="align-left">2014</td></tr><tr class="valign-top"><td class="align-left"></td><td class="align-left">NNB</td><td class="align-left">78,40</td><td class="align-left">78,40</td><td class="align-left"><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></td><td class="align-left">2019</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0015"><span class="captions text-s"><span id="cn080"><p id="sp085"><span class="label">Table 3</span>. This table contains a total of 94 methods that were extracted from the 51 studies selected for this literature review. Some cells are left blank because of the lack of that specific information at the source papers.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Acc (%)</th><th scope="col" class="align-left">Classification</th><th scope="col" class="align-left">Test dataset</th><th scope="col" class="align-left">Training dataset</th><th scope="col" class="align-left">Ref</th><th scope="col" class="align-left">Classic (C) vs NNB (N)</th><th scope="col" class="align-left">Facial detection</th><th scope="col" class="align-left">Other preprocessing algorithms</th><th scope="col" class="align-left">Feature extraction</th><th scope="col" class="align-left">Dimensionality reduction</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">78.64</td><td class="align-left">src</td><td class="align-left">oulu-casia nir&amp;vis</td><td class="align-left">oulu-casia nir&amp;vis</td><td class="align-left"><a class="anchor anchor-primary" href="#b0620" name="bb0620" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0620"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">lbp;asm</td><td class="align-left">adaboost</td></tr><tr class="valign-top"><td class="align-left">72.09</td><td class="align-left">svm</td><td class="align-left">oulu-casia nir&amp;vis</td><td class="align-left">oulu-casia nir&amp;vis</td><td class="align-left"><a class="anchor anchor-primary" href="#b0620" name="bb0620" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0620"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">lbp;asm</td><td class="align-left">adaboost</td></tr><tr class="valign-top"><td class="align-left">87.50</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0535" name="bb0535" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0535"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">luxand facesdk</td><td class="align-left">lda</td></tr><tr class="valign-top"><td class="align-left">93.75</td><td class="align-left">naive bayes</td><td class="align-left">jaffe&nbsp;+&nbsp;tfeid&nbsp;+&nbsp;radboud</td><td class="align-left">jaffe&nbsp;+&nbsp;tfeid&nbsp;+&nbsp;radboud</td><td class="align-left"><a class="anchor anchor-primary" href="#b0010" name="bb0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0010"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left">hog;lbp;nn</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">52.00</td><td class="align-left">bn</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">standard deviation skewness kurtosis correlation coeficient mean of psd standard deviation of psd</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">70.00</td><td class="align-left">decision tree</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">standard deviation skewness kurtosis correlation coeficient mean of psd standard deviation of psd</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">49.00</td><td class="align-left">random forest</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">standard deviation skewness kurtosis correlation coeficient mean of psd standard deviation of psd</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">42.00</td><td class="align-left">random tree</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">standard deviation skewness kurtosis correlation coeficient mean of psd standard deviation of psd</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">36.00</td><td class="align-left">svm</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0615" name="bb0615" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0615"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">standard deviation skewness kurtosis correlation coeficient mean of psd standard deviation of psd</td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">93.57</td><td class="align-left">euclidian distance</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0320" name="bb0320" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0320"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">log-gabor filter</td><td class="align-left"></td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">90.90</td><td class="align-left">hmm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0560" name="bb0560" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0560"><span class="anchor-text-container"><span class="anchor-text">[112]</span></span></a></td><td class="align-left">C</td><td class="align-left">nn</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">pca</td></tr><tr class="valign-top"><td class="align-left">99.75</td><td class="align-left">elm-rbf</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">100</td><td class="align-left">elm-rbf</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">99.61</td><td class="align-left">knn</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">100</td><td class="align-left">knn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">99.71</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">100</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">radon transform</td><td class="align-left"></td><td class="align-left">pca;lda</td></tr><tr class="valign-top"><td class="align-left">94.09</td><td class="align-left">svm</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">lowpass filter</td><td class="align-left">haar-cascade;lbp</td><td class="align-left">pca;lda;resize</td></tr><tr class="valign-top"><td class="align-left">92.00</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">lowpass filter</td><td class="align-left">haar-cascade;lbp</td><td class="align-left">pca;lda;resize</td></tr><tr class="valign-top"><td class="align-left">93.43</td><td class="align-left">cnn</td><td class="align-left">rafd</td><td class="align-left">rafd</td><td class="align-left"><a class="anchor anchor-primary" href="#b0575" name="bb0575" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0575"><span class="anchor-text-container"><span class="anchor-text">[115]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">cropping thresholding</td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">61.29</td><td class="align-left">cnn</td><td class="align-left">fer2013</td><td class="align-left">sfew</td><td class="align-left"><a class="anchor anchor-primary" href="#b0580" name="bb0580" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0580"><span class="anchor-text-container"><span class="anchor-text">[116]</span></span></a></td><td class="align-left">N</td><td class="align-left">jda cnn mixture of trees</td><td class="align-left">gray-scale histogram equalization</td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">91.89</td><td class="align-left">cnn</td><td class="align-left">bu-3dfe</td><td class="align-left">bu-3dfe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">rotation correction intensity normalization opping</td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left" colspan="10"><br></td></tr><tr class="rowsep-1 valign-top"><td class="align-left"><strong>Acc (%)</strong></td><td class="align-left"><strong>Classification</strong></td><td class="align-left"><strong>Test dataset</strong></td><td class="align-left"><strong>Training dataset</strong></td><td class="align-left"><strong>Ref</strong></td><td class="align-left"><strong>Classic(C) vs NNB (N)</strong></td><td class="align-left"><strong>Facial detection</strong></td><td class="align-left"><strong>Other preprocessing algorithms</strong></td><td class="align-left"><strong>Feature extraction</strong></td><td class="align-left"><strong>Dimensionality reduction</strong></td></tr><tr class="valign-top"><td class="align-left">96.76</td><td class="align-left">cnn</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">rotation correction intensity normalization opping</td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">86.74</td><td class="align-left">cnn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">rotation correction intensity normalization opping</td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">68.00</td><td class="align-left">cnn</td><td class="align-left">rafd</td><td class="align-left">fer2013</td><td class="align-left"><a class="anchor anchor-primary" href="#b0415" name="bb0415" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0415"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a></td><td class="align-left">N</td><td class="align-left">haar cascade</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">50.50</td><td class="align-left">cnn</td><td class="align-left">fer2013</td><td class="align-left">imagenet</td><td class="align-left"><a class="anchor anchor-primary" href="#b0350" name="bb0350" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0350"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">resize</td></tr><tr class="valign-top"><td class="align-left">84.55</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0470" name="bb0470" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0470"><span class="anchor-text-container"><span class="anchor-text">[94]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">aam</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">95.00</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0050" name="bb0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0050"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left">asm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">99.00</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left">contourlet transform</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">98.00</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left">contourlet transform</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">93.96</td><td class="align-left">fuzzy</td><td class="align-left">rafd</td><td class="align-left">rafd</td><td class="align-left"><a class="anchor anchor-primary" href="#b0200" name="bb0200" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0200"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">edge detection skin color detection</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">76.60</td><td class="align-left">svm</td><td class="align-left">otcbvs</td><td class="align-left">otcbvs</td><td class="align-left"><a class="anchor anchor-primary" href="#b0175" name="bb0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0175"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left">evolutionary computation</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">87.43</td><td class="align-left">adaboost&nbsp;+&nbsp;dbn</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0285" name="bb0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0285"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">gabor;asm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">82.40</td><td class="align-left">adaboost&nbsp;+&nbsp;dbn</td><td class="align-left">mmi</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0285" name="bb0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0285"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">gabor;asm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">47.00</td><td class="align-left">euclidian distance</td><td class="align-left">feedb</td><td class="align-left">feedb</td><td class="align-left"><a class="anchor anchor-primary" href="#b0480" name="bb0480" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0480"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">heuristic</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">85.05</td><td class="align-left">cnn</td><td class="align-left">ck+;kdef</td><td class="align-left">ck+;kdef</td><td class="align-left"><a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left">gray-scale</td><td class="align-left">hog</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">56.07</td><td class="align-left">mlp</td><td class="align-left">ck+;kdef</td><td class="align-left">ck+;kdef</td><td class="align-left"><a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left">gray-scale</td><td class="align-left">hog</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">63.55</td><td class="align-left">svm</td><td class="align-left">ck+;kdef</td><td class="align-left">ck+;kdef</td><td class="align-left"><a class="anchor anchor-primary" href="#b0245" name="bb0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0245"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">gray-scale</td><td class="align-left">hog</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">82.87</td><td class="align-left">svm</td><td class="align-left">ava</td><td class="align-left">ava</td><td class="align-left"><a class="anchor anchor-primary" href="#b0530" name="bb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0530"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">hog;sift;cnn</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">94.36</td><td class="align-left">svm</td><td class="align-left">cuhk</td><td class="align-left">cuhk</td><td class="align-left"><a class="anchor anchor-primary" href="#b0530" name="bb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0530"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">hog;sift;cnn</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">93.77</td><td class="align-left">svm</td><td class="align-left">live-iq</td><td class="align-left">live-iq</td><td class="align-left"><a class="anchor anchor-primary" href="#b0530" name="bb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0530"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">hog;sift;cnn</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">86.63</td><td class="align-left">svm</td><td class="align-left">pne</td><td class="align-left">pne</td><td class="align-left"><a class="anchor anchor-primary" href="#b0530" name="bb0530" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0530"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">hog;sift;cnn</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">96.42</td><td class="align-left">fuzzy</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0140" name="bb0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0140"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">integral projection</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">94.28</td><td class="align-left">svm</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0210" name="bb0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0210"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></td><td class="align-left">C</td><td class="align-left">kinect</td><td class="align-left"></td><td class="align-left">kinect</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">54.56</td><td class="align-left">cnn</td><td class="align-left">casia webface</td><td class="align-left">casia webface</td><td class="align-left"><a class="anchor anchor-primary" href="#b0275" name="bb0275" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0275"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">lbp</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">100</td><td class="align-left">knn</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">lbp</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">97.14</td><td class="align-left">knn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">lbp</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">77.67</td><td class="align-left">svm</td><td class="align-left">bu-3dfe</td><td class="align-left">bu-3dfe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0340" name="bb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0340"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">cropping;segmentation</td><td class="align-left">lbp;gmm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">91.60</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0340" name="bb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0340"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">cropping;segmentation</td><td class="align-left">lbp;gmm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">88.10</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0340" name="bb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0340"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">cropping;segmentation</td><td class="align-left">lbp;gmm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">80.00</td><td class="align-left">svm</td><td class="align-left">multi-pie</td><td class="align-left">multi-pie</td><td class="align-left"><a class="anchor anchor-primary" href="#b0340" name="bb0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0340"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">cropping;segmentation</td><td class="align-left">lbp;gmm</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">78.80</td><td class="align-left">fuzzy</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0355" name="bb0355" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0355"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left">thresholding</td><td class="align-left">matlab</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">78.80</td><td class="align-left">fuzzy</td><td class="align-left">ebner’s</td><td class="align-left">ebner’s</td><td class="align-left"><a class="anchor anchor-primary" href="#b0115" name="bb0115" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0115"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></td><td class="align-left">C</td><td class="align-left">matlab</td><td class="align-left"></td><td class="align-left">matlab</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">75.00</td><td class="align-left">nn</td><td class="align-left">ck</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0595" name="bb0595" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0595"><span class="anchor-text-container"><span class="anchor-text">[119]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">nn</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">95.00</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0305" name="bb0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0305"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">rcsr;lbp;hog</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">96.40</td><td class="align-left">svm</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0305" name="bb0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0305"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">rcsr;lbp;hog</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">87.00</td><td class="align-left">decision tree</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0375" name="bb0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0375"><span class="anchor-text-container"><span class="anchor-text">[75]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left">template matching</td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">93.24</td><td class="align-left">cnn</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0205" name="bb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0205"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">gn</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">95.23</td><td class="align-left">cnn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0205" name="bb0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0205"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left">gn</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">68.79</td><td class="align-left">cnn&nbsp;+&nbsp;svm;</td><td class="align-left">fer2013</td><td class="align-left">fer2013</td><td class="align-left"><a class="anchor anchor-primary" href="#b0545" name="bb0545" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0545"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a></td><td class="align-left">CN</td><td class="align-left"></td><td class="align-left">rotation correction histogram equalization</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">87.72</td><td class="align-left">adaboost</td><td class="align-left">mmi</td><td class="align-left">mmi</td><td class="align-left"><a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">segmentation;thresholding</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">76.60</td><td class="align-left">naive bayes</td><td class="align-left">mmi</td><td class="align-left">mmi</td><td class="align-left"><a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">segmentation;thresholding</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">84.31</td><td class="align-left">svm</td><td class="align-left">mmi</td><td class="align-left">mmi</td><td class="align-left"><a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left">segmentation;thresholding</td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">90.00</td><td class="align-left">dbn</td><td class="align-left">dtu-aam ioid-facedatabase</td><td class="align-left">dtu-aam ioid-facedatabase</td><td class="align-left"><a class="anchor anchor-primary" href="#b0265" name="bb0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0265"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a></td><td class="align-left">C</td><td class="align-left">aam</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">80.90</td><td class="align-left">cnn</td><td class="align-left">emotiw</td><td class="align-left">emotiw</td><td class="align-left"><a class="anchor anchor-primary" href="#b0485" name="bb0485" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0485"><span class="anchor-text-container"><span class="anchor-text">[97]</span></span></a></td><td class="align-left">N</td><td class="align-left">cnn</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">87.52</td><td class="align-left">nn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0555" name="bb0555" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0555"><span class="anchor-text-container"><span class="anchor-text">[111]</span></span></a></td><td class="align-left">N</td><td class="align-left">haar cascade</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left" colspan="10"><br></td></tr><tr class="rowsep-1 valign-top"><td class="align-left"><strong>Acc (%)</strong></td><td class="align-left"><strong>Classification</strong></td><td class="align-left"><strong>Test dataset</strong></td><td class="align-left"><strong>Training dataset</strong></td><td class="align-left"><strong>Ref</strong></td><td class="align-left"><strong>Classic (C) vs NNB (N)</strong></td><td class="align-left"><strong>Facial detection</strong></td><td class="align-left"><strong>Other preprocessing algorithms</strong></td><td class="align-left"><strong>Feature extraction</strong></td><td class="align-left"><strong>Dimensionality reduction</strong></td></tr><tr class="valign-top"><td class="align-left">69.77</td><td class="align-left">cnn</td><td class="align-left">kaggle dataset</td><td class="align-left">kaggle dataset</td><td class="align-left"><a class="anchor anchor-primary" href="#b0215" name="bb0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0215"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></td><td class="align-left">N</td><td class="align-left">hog;linear classifier</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">100</td><td class="align-left">cnn</td><td class="align-left">jaffe</td><td class="align-left">jaffe</td><td class="align-left"><a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">89.58</td><td class="align-left">cnn</td><td class="align-left">kdef</td><td class="align-left">kdef</td><td class="align-left"><a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">71.9</td><td class="align-left">cnn</td><td class="align-left">kdef&nbsp;+&nbsp;jaffe&nbsp;+&nbsp;sfew</td><td class="align-left">kdef&nbsp;+&nbsp;jaffe&nbsp;+&nbsp;sfew</td><td class="align-left"><a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></td><td class="align-left">N</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">90.9</td><td class="align-left">svm</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0080" name="bb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0080"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">86.4</td><td class="align-left">svm</td><td class="align-left">mmi</td><td class="align-left">mmi</td><td class="align-left"><a class="anchor anchor-primary" href="#b0080" name="bb0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0080"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">90.38</td><td class="align-left">fcm</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0600" name="bb0600" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0600"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones modified</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">74</td><td class="align-left">viola-jones modified</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0255" name="bb0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0255"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></td><td class="align-left">C</td><td class="align-left">viola-jones modified</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">89</td><td class="align-left">adaboost</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">76</td><td class="align-left">adaboost</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">51.2</td><td class="align-left">cnn</td><td class="align-left">afew</td><td class="align-left">afew&nbsp;+&nbsp;proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">93.9</td><td class="align-left">cnn</td><td class="align-left">ck+</td><td class="align-left">afew&nbsp;+&nbsp;proprietary&nbsp;+&nbsp;ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">73.6</td><td class="align-left">cnn</td><td class="align-left">ck+</td><td class="align-left">affectnet</td><td class="align-left"><a class="anchor anchor-primary" href="#b0495" name="bb0495" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0495"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">78</td><td class="align-left">cnn</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0065" name="bb0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0065"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">64.46</td><td class="align-left">cnn</td><td class="align-left">fer2013</td><td class="align-left">fer2013</td><td class="align-left"><a class="anchor anchor-primary" href="#b0135" name="bb0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0135"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">62</td><td class="align-left">cnn</td><td class="align-left">ised</td><td class="align-left">affectnet</td><td class="align-left"><a class="anchor anchor-primary" href="#b0495" name="bb0495" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0495"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">46.5</td><td class="align-left">cnn</td><td class="align-left">jaffe</td><td class="align-left">affectnet</td><td class="align-left"><a class="anchor anchor-primary" href="#b0495" name="bb0495" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0495"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">78.4</td><td class="align-left">cnn</td><td class="align-left">mmi</td><td class="align-left">afew&nbsp;+&nbsp;proprietary&nbsp;+&nbsp;mmi</td><td class="align-left"><a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">94.48</td><td class="align-left">cnn</td><td class="align-left">multi-pie</td><td class="align-left">multi-pie</td><td class="align-left"><a class="anchor anchor-primary" href="#b0250" name="bb0250" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0250"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">41.03</td><td class="align-left">cnn</td><td class="align-left">tfd;google dataset</td><td class="align-left">afew</td><td class="align-left"><a class="anchor anchor-primary" href="#b0235" name="bb0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0235"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">96</td><td class="align-left">knn</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0490" name="bb0490" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0490"><span class="anchor-text-container"><span class="anchor-text">[98]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">95.24</td><td class="align-left">mge</td><td class="align-left">jaffe;mug;ck+</td><td class="align-left">jaffe;mug;ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0225" name="bb0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0225"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">90</td><td class="align-left">mlp</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0490" name="bb0490" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0490"><span class="anchor-text-container"><span class="anchor-text">[98]</span></span></a></td><td class="align-left">N</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">77.8</td><td class="align-left">naive bayes</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">72.9</td><td class="align-left">naive bayes</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">87</td><td class="align-left">svm</td><td class="align-left">ck</td><td class="align-left">ck</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">75.5</td><td class="align-left">svm</td><td class="align-left">ck+</td><td class="align-left">ck+</td><td class="align-left"><a class="anchor anchor-primary" href="#b0110" name="bb0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0110"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr><tr class="valign-top"><td class="align-left">71</td><td class="align-left">svm</td><td class="align-left">proprietary</td><td class="align-left">proprietary</td><td class="align-left"><a class="anchor anchor-primary" href="#b0030" name="bb0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0030"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a></td><td class="align-left">C</td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td><td class="align-left"></td></tr></tbody></table></div></div></div><section id="s0160"><h3 id="st160" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.1. Jaffe best accuracy classical approach</h3><div class="u-margin-s-bottom" id="p0380">A classical approach was demonstrated in <a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, where the Jaffe (Section <a class="anchor anchor-primary" href="#s0080" name="bs0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0080"><span class="anchor-text-container"><span class="anchor-text">6.1</span></span></a>) and CK (Section <a class="anchor anchor-primary" href="#s0085" name="bs0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0085"><span class="anchor-text-container"><span class="anchor-text">6.2</span></span></a>) datasets were used. For preprocessing, a image crop was applied on the datasets based on a particular model <a class="anchor anchor-primary" href="#b0425" name="bb0425" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0425"><span class="anchor-text-container"><span class="anchor-text">[85]</span></span></a><span>. As feature extraction techniques, the authors have chosen a quite complex processes. Firstly, a <a href="/topics/earth-and-planetary-sciences/radon-transform" title="Learn more about radon transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">radon transform</a> is applied to turn lines through images into points in the radon domain </span><a class="anchor anchor-primary" href="#b0150" name="bb0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0150"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a><span>. Besides that, to overcome the subtle changes, complexity and variability of the non-linear features, a <a href="/topics/computer-science/empirical-mode-decomposition" title="Learn more about empirical mode decomposition from ScienceDirect's AI-generated Topic Pages" class="topic-link">empirical mode decomposition</a> (EMD) based on the work by </span><a class="anchor anchor-primary" href="#b0185" name="bb0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0185"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a><span> was applied to minimize the effects of noise. As described by the authors, these method is capable of decomposing any complicated signal into oscillating component called <a href="/topics/engineering/intrinsic-mode-function" title="Learn more about intrinsic mode functions from ScienceDirect's AI-generated Topic Pages" class="topic-link">intrinsic mode functions</a><span> (IMF). These IMFs on the other hand, reflect to signal’s local characteristic features. With the signal extracted from the images, the authors applied 3 independently algorithms of dimensionality reduction: <a href="/topics/engineering/principal-components" title="Learn more about PCA from ScienceDirect's AI-generated Topic Pages" class="topic-link">PCA</a><span> &nbsp;+&nbsp;Linear <a href="/topics/computer-science/discriminant-analysis" title="Learn more about Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Discriminant Analysis</a> (LDA) </span></span></span><a class="anchor anchor-primary" href="#b0090" name="bb0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0090"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a><span>, <a href="/topics/computer-science/principal-components" title="Learn more about PCA from ScienceDirect's AI-generated Topic Pages" class="topic-link">PCA</a><span> &nbsp;+&nbsp;Local Fisher <a href="/topics/earth-and-planetary-sciences/discriminant-analysis" title="Learn more about Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Discriminant Analysis</a> (LFDA) </span></span><a class="anchor anchor-primary" href="#b0465" name="bb0465" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0465"><span class="anchor-text-container"><span class="anchor-text">[93]</span></span></a>, <a class="anchor anchor-primary" href="#b0550" name="bb0550" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0550"><span class="anchor-text-container"><span class="anchor-text">[110]</span></span></a>, <a class="anchor anchor-primary" href="#b0605" name="bb0605" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0605"><span class="anchor-text-container"><span class="anchor-text">[121]</span></span></a>, <a class="anchor anchor-primary" href="#b0390" name="bb0390" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0390"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a> and Kernel LFDA (KLFDA) <a class="anchor anchor-primary" href="#b0620" name="bb0620" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0620"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0385"><span>After the previous process, the obtained features were still subjected to a statistical analysis named <a href="/topics/food-science/analysis-of-variance" title="Learn more about ANOVA from ScienceDirect's AI-generated Topic Pages" class="topic-link">ANOVA</a> test, with the goal of selecting only the significant features and then proceed to the classification step, in which the authors have decided to apply KNN previously mentioned in Section </span><a class="anchor anchor-primary" href="#s0135" name="bs0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0135"><span class="anchor-text-container"><span class="anchor-text">7.1.4</span></span></a><span> and Gaussian <a href="/topics/computer-science/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> (Section </span><a class="anchor anchor-primary" href="#s0120" name="bs0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0120"><span class="anchor-text-container"><span class="anchor-text">7.1.1</span></span></a><span><span>) classifiers. Evaluating the model with the JAFFE dataset, both classification algorithms were able to achieve 100% accuracy. On the other hand, while using CK&nbsp;+&nbsp;dataset for evaluation, the <a href="/topics/computer-science/nearest-neighbors-classifier" title="Learn more about KNN classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN classifier</a> was able to achieve 99.31% accuracy and the </span><a href="/topics/engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> got the classification write on 99,43% of the time.</span></div></section><section id="s0165"><h3 id="st165" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.2. Jaffe best accuracy NNB approaches</h3><div class="u-margin-s-bottom" id="p0390"><span>With the highest achieved accuracy between NNB methods, a <a href="/topics/engineering/neural-network-approach" title="Learn more about CNN approach from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN approach</a> is presented capable off reaching 100% precision </span><a class="anchor anchor-primary" href="#b0380" name="bb0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0380"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a>. Three datasets were used for training in this work: Jaffe (previously presented on Section <a class="anchor anchor-primary" href="#s0080" name="bs0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0080"><span class="anchor-text-container"><span class="anchor-text">6.1</span></span></a>) with 213 images of 10 subjects, Karolinska Directed Emotional Faces (KDEF) Database <a class="anchor anchor-primary" href="#b0145" name="bb0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0145"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a> with 4900 images of 70 subjects and Static Facial Expressions In The Wild (SFEW) Database <a class="anchor anchor-primary" href="#b0095" name="bb0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0095"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a><span> with 700 images extracted from movies. As most of the methods, this project applied grayscale conversion and downscaling with <a href="/topics/computer-science/geometric-transformation" title="Learn more about geometric transformations from ScienceDirect's AI-generated Topic Pages" class="topic-link">geometric transformations</a> algorithms (Sections </span><a class="anchor anchor-primary" href="#s0035" name="bs0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0035"><span class="anchor-text-container"><span class="anchor-text">4.1 Grayscale conversion</span></span></a>, <a class="anchor anchor-primary" href="#s0065" name="bs0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0065"><span class="anchor-text-container"><span class="anchor-text">4.3 Dimensionality reduction</span></span></a>). The authors have opted to use Viola-Jones (Section <a class="anchor anchor-primary" href="#s0045" name="bs0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0045"><span class="anchor-text-container"><span class="anchor-text">4.2.1</span></span></a>) to detect faces in the datasets images, and than crop them to reduce even more the data inputted to the model, making them 256x256 pixels.</div><div class="u-margin-s-bottom" id="p0395"><span>In order to achieve good results with <a href="/topics/computer-science/neural-network-model" title="Learn more about CNN models from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN models</a>, the training set must be very large, much more than the sum of the images of each dataset chosen. Therefore, a </span><a href="/topics/computer-science/synthetic-image" title="Learn more about Synthetic Image from ScienceDirect's AI-generated Topic Pages" class="topic-link">Synthetic Image</a> Generation, as named by the authors, took place with the objective of augmenting the set for training. This process was responsible for generating new images by rotating the images already in the datasets and, therefore, feeding the model with much larger set of data. This technique is capable of augmenting the dataset and helping the CNN not to get over fitted, but in the other hand, the number of subjects does not change with this application and, therefore, it is still difficult for the model to get god results when introduced to an ”unknown face”. With the CNN trained, the authors claimed to achieve 100% precision when evaluating the model with the JAFFE dataset and over 89% when using KDEF for evaluation.</div></section><section id="s0170"><h3 id="st170" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.3. Jaffe/Cohn-Kanade best accuracy NNB approaches</h3><div class="u-margin-s-bottom" id="p0400">As for Jaffe dataset applied in a NNB method, <a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a><span><span> makes use of an <a href="/topics/engineering/extreme-learning-machine" title="Learn more about extreme learning machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">extreme learning machine</a><span> (ELM) with <a href="/topics/engineering/radial-basis-function" title="Learn more about radial basis function from ScienceDirect's AI-generated Topic Pages" class="topic-link">radial basis function</a> (RBF) kernel. As mentioned by the authors, ELM with RBF kernel (ELM-RBF) is an extension of ELM from single hidden layer </span></span><a href="/topics/computer-science/feedforward-neural-network" title="Learn more about feedforward neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">feedforward neural network</a> (SLFN) with additive neurons case to SLFNs with RBF kernel. With the same method mentioned in the previous paragraph, this approach was able to achieve 100% precision with the Jaffe dataset and over 95% accuracy in CK&nbsp;+&nbsp;images.</span></div></section><section id="s0175"><h3 id="st175" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.4. Cohn-Kanade best accuracy classical approach</h3><div class="u-margin-s-bottom" id="p0405">The work from <a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a><span> is a quite different from the ones mentioned preciously. This paper has the main goal of reviewing 22 <a href="/topics/engineering/local-binary-pattern" title="Learn more about Local Binary Pattern from ScienceDirect's AI-generated Topic Pages" class="topic-link">Local Binary Pattern</a> (LBP)-like descriptors and provide a comparison between then in the field of facial expression recognition. The methods proposed in this context was a simple parameter-free Nearest Neighbor (NN) classifier, since the real goal was to evaluate the LBP variants in terms of their performance on extracting features from the images. For the CK dataset, the authors claimed to achieve 100% score with eight descriptors (AELTP, BGC3, CSALTP, dLNP </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B1;</mi></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.488ex" height="1.394ex" viewBox="0 -498.8 640.5 600.2" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3B1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">α</mi></mrow></math></span></span><script type="math/mml" id="MathJax-Element-3"><math><mrow is="true"><mi is="true">α</mi></mrow></math></script></span><span>, LDN, nLNPd, <a href="/topics/earth-and-planetary-sciences/space-transportation-system" title="Learn more about STS from ScienceDirect's AI-generated Topic Pages" class="topic-link">STS</a>, and WLD). Besides the great results achieved, the paper does not describe any pre processing step or extremely complex process, just the LBP feature extractors associated with the NN classifier did the job quite well.</span></div></section><section id="s0180"><h3 id="st180" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.5. Extended Cohn-Kanade best accuracy classical approach</h3><div class="u-margin-s-bottom" id="p0410">The technique used by <a class="anchor anchor-primary" href="#b0170" name="bb0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0170"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a> was elaborated on top of classic techniques and was tested in the CK + (Section <a class="anchor anchor-primary" href="#s0085" name="bs0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0085"><span class="anchor-text-container"><span class="anchor-text">6.2</span></span></a>) and JAFFE (Section <a class="anchor anchor-primary" href="#s0080" name="bs0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0080"><span class="anchor-text-container"><span class="anchor-text">6.1</span></span></a>) datasets. For the authors attempted to understand the contribution of different areas in the face for the expression recognition and, therefore, applied the following method:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1.</span><span><div class="u-margin-s-bottom" id="p0510">Pre-processing - A <a href="/topics/engineering/lowpass-filter" title="Learn more about low pass filter from ScienceDirect's AI-generated Topic Pages" class="topic-link">low pass filter</a> with 3x3 Gaussian mask to remove noise from the source images was used;</div></span></li><li class="react-xocs-list-item"><span class="list-label">2.</span><span><div class="u-margin-s-bottom" id="p0515">Face detection - Viola-Jones with Adaboost learning for face detection is applied to locate the face inside the picture;</div></span></li><li class="react-xocs-list-item"><span class="list-label">3.</span><span><div class="u-margin-s-bottom" id="p0520">Detection of eyes and nose - The noise was located using the geometric position of the face and Haar cascade. The eyes had a specific Haar classifier trained for each one of them;</div></span></li><li class="react-xocs-list-item"><span class="list-label">4.</span><span><div class="u-margin-s-bottom" id="p0525">Eyebrows and mouth detection - The position of the mouth was obtained from the lower region of the nose, where a Sobel horizontal edge detector was applied to detect the upper lip position. For the detection of the eyebrows, the authors applied the same method as for the lips, however, with the subtle change that is the performance of an adaptive threshold operation before applying the operator.</div></span></li><li class="react-xocs-list-item"><span class="list-label">5.</span><span><div class="u-margin-s-bottom" id="p0530">Extraction of active facial patches - Fixed regions of the face (based on the eyes, nose…) were extracted since they might contain relevant information.</div></span></li><li class="react-xocs-list-item"><span class="list-label">6.</span><span><div class="u-margin-s-bottom" id="p0535">Feature extraction - For this action, LBP was chose because of its robustness in therms of <a href="/topics/computer-science/illumination-invariant" title="Learn more about illumination invariant from ScienceDirect's AI-generated Topic Pages" class="topic-link">illumination invariant</a> feature description.</div></span></li></ul><span>After extracting the important features from the pictures, a <a href="/topics/earth-and-planetary-sciences/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> algorithm was used to classify the image into the best matching category (anger, disgust, fear, happiness, sadness and surprise) and was capable of achieving a average accuracy of 94,09%.</span></div></section><section id="s0185"><h3 id="st185" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.6. Extended Cohn-Kanade/BU-3DFE best accuracy NNB approach</h3><div class="u-margin-s-bottom" id="p0415">The article with the best performance in the CK &nbsp;+&nbsp;dataset using CNN was written by <a class="anchor anchor-primary" href="#b0290" name="bb0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0290"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a>, in this article data sets that have a controlled environment were tested, these being CK+ (<a class="anchor anchor-primary" href="#s0085" name="bs0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0085"><span class="anchor-text-container"><span class="anchor-text">6.2</span></span></a>), JAFFE(<a class="anchor anchor-primary" href="#s0080" name="bs0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0080"><span class="anchor-text-container"><span class="anchor-text">6.1</span></span></a>), BU-3DFE<a class="anchor anchor-primary" href="#s0100" name="bs0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0100"><span class="anchor-text-container"><span class="anchor-text">(6.5)</span></span></a><span>. The authors opted to use some <a href="/topics/computer-science/preprocessing-technique" title="Learn more about preprocessing techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">preprocessing techniques</a> such as rotation correction, image cropping, down sampling and intensity normalization (</span><a class="anchor anchor-primary" href="#s0030" name="bs0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0030"><span class="anchor-text-container"><span class="anchor-text">4</span></span></a><span>) in order to have only the meaningful data for the next steps and yet maintain a general standard for the images. For the classification step, a CNN was used and for its input, the images were configured in a 32x32 matrix of pixels. After the input, the the network was composed by two convolution and <a href="/topics/engineering/subsamplings" title="Learn more about subsampling from ScienceDirect's AI-generated Topic Pages" class="topic-link">subsampling</a> sets and a fully connected layer. The algorithm was developed to identify anger, disgust, fear, happiness, sadness and surprise. The proposed method was capable of achieving a average of 96,76 for CK&nbsp;+&nbsp;and 91,89 for BU-3DFE.</span></div></section><section id="s0190"><h3 id="st190" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.7. FER 2013 best accuracy NNB method</h3><div class="u-margin-s-bottom" id="p0420">The best accuracy paper for the FER 2013 dataset is based on the usage of the already known CNN architectures in the facial expression recognition problem, these being GoogleNet, <a href="/topics/computer-science/residual-neural-network" title="Learn more about ResNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ResNet</a>, VGGNet and AlexNet. Pre-processing is not mentioned by the authors, instead focusing on the classification methods themselves and the process of applying them. The authors decided to train the models for different numbers of epochs: 6, 8, 12, 20 and 25. The top average accuracy was 64,24 achieved by the AlexNet model trained for 20 epochs. Besides being a very small number of epochs in comparison of other methods, the authors claim that the model could be suffering from overfitting, since the accuracy was better when training the model for 20 epochs than at the 25 epochs attempt.</div></section><section id="s0195"><h3 id="st195" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.8. MMI best accuracy classic method</h3><div class="u-margin-s-bottom" id="p0425">As used by a great partition of the papers analyzed on this work, <a class="anchor anchor-primary" href="#b0330" name="bb0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0330"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a><span> uses the Viola-Jones algorithm to detect faces in the images and, therefore, crop them appropriately. Additionally, a skin detector based on texture is applied to identify the non-face parts of the pictures and, finally, segment them by this characteristic. Done with preprocessing, a facial graph is built containing some interest point of the face. For the classification step, the authors used three different classifiers, all of them based on FACS (5): Adaboost, Naive Bayes and <a href="/topics/chemical-engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a>. The algorithm was developed to classify the following emotions: Surprise, Happiness, Disgust, Fear, Anger, Sadness and Neutral. The best accuracy achieved was 100% with the SVM algorithm for the neutral emotion, however, the best average accuracy across all the emotions (87,73%) was achieved by the Adaboost classifier.</span></div></section><section id="s0200"><h3 id="st200" class="u-h4 u-margin-m-top u-margin-xs-bottom">8.9. MMI best accuracy NNB method</h3><div class="u-margin-s-bottom" id="p0430">The last paper of the <a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a> presents a very interesting and dissimilar approach from the other works. <a class="anchor anchor-primary" href="#b0180" name="bb0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0180"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a><span> decided to analyze data from videos instead of images as usual. For that, facial landmarks are extracted, facial detection, alignment and input normalization are applied in the frames of the videos and attributed to a <a href="/topics/chemical-engineering/long-short-term-memory" title="Learn more about Long Short Term Memory from ScienceDirect's AI-generated Topic Pages" class="topic-link">Long Short Term Memory</a><span> (LSTM) neural network. The videos inputted to the <a href="/topics/computer-science/long-short-term-memory-network" title="Learn more about LSTM from ScienceDirect's AI-generated Topic Pages" class="topic-link">LSTM</a> are composed by 16 frames of 244 x 244 sized face images and were achieved from image sources, that being AFEW, CK&nbsp;+&nbsp;and MMI. The average accuracy for the MMI dataset was 78.40%.In addition to the accuracy in the MMI dataset, the algorithm also achieves good results in the AFEW and CK&nbsp;+&nbsp;datasets, with 51.2% and 93.9% respectively.</span></span></div></section></section><section id="s0205"><h2 id="st205" class="u-h4 u-margin-l-top u-margin-xs-bottom">9. Conclusion and discussions</h2><div class="u-margin-s-bottom" id="p0435">Computational Facial emotion recognition as performed by humans can be considered a very challenging task when using input images or videos solely. Since face-based human emotion recognition is performed in a natural way as a kind of non-verbal communication, it is also very dependent of other inputs to be effective, such as a context of application itself, and sometimes it is hard to be recognized by the available analytical ways. Besides its complexity, finding computational mechanisms that are able to effectively recognize emotion from facial images, is very desired in the most varied fields with significant implications.</div><div class="u-margin-s-bottom" id="p0440">This survey intended to present a systematic literature review of the state-of-the-art on emotion expression recognition from facial images, considering the works published over the past few years. We categorized a total of 51 manuscripts, analyzed according to its main constructive concept, totaling 94 distinct approaches, where two main trends were able to be identified: classical approaches and those based on neural networks approaches, including the emerging convolutional counterpart. For the approaches based on Digital Image Processing, Pattern Recognition and Computer Vision techniques, we named them classical approaches. In essence, classical approaches consider those based on a face detection procedure followed by some recognizer able to interpret feature vector and made some decision according to a fixed number of output classes. These approaches usually explore some <em>m</em><span>-dimensional space problem partition, looking for some discrimination among input vectors and output clusters, such as the well-know Support Vector Machines (SVM), Naive Bayes, k-Nearest Neighbors, Fuzzy, among others. On the other spectrum of problem-solvers, we have the neural network based approaches (NNB approaches), where the input layer containing a <a href="/topics/engineering/discretization" title="Learn more about discretization from ScienceDirect's AI-generated Topic Pages" class="topic-link">discretization</a><span> of the facial image or more commonly feature vectors extracted using some <a href="/topics/computer-science/feature-descriptor" title="Learn more about feature descriptor from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature descriptor</a> method is used. NNB approaches include the emerging convolutional neural network counterpart, and since its first use as computer vision solver-problems, it was also used to the facial emotion recognition as well.</span></span></div><div class="u-margin-s-bottom" id="p0445"><span>The papers analyzed here used a huge variety of techniques for the construction of the emotion <a href="/topics/computer-science/recognition-algorithm" title="Learn more about recognition algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition algorithm</a>, during the development of this survey. This survey highlights several aspects provided by the published papers over the literature. One of the most important, that arises some interest of the general public, is to answer the question if a general effective solution has been achieved to solve the facial emotion recognition problem, which can be translated in terms of the general precision. In this survey we can point to some interesting solutions, but the response is that the problem can be considered solved for some very specific situations. The most evident problem in terms of the published approaches is that some variability can always be introduced to decrease the performance of the proposed works. The use of controlled datasets or a very limited subset of those images makes in fact the proposed approaches effective, but very restricted to this specific domain. When applied in real-world problems, the precision overall tends to decrease significantly. This aspect can be mainly observed for the classical approaches when compared to the NNB ones, where a marginally better precision can be observed mainly justified due to the nature of the domain used. In general, what can be observed for classical approaches is a very high specificity and low generality to solve the emotion recognition problem. Here, we can point the approaches proposed by </span><a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a>, <a class="anchor anchor-primary" href="#b0015" name="bb0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0015"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, <a class="anchor anchor-primary" href="#b0040" name="bb0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0040"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, where a high-precision was achieved using classical approaches. For the classical approaches, the best ranked approach was <a class="anchor anchor-primary" href="#b0455" name="bb0455" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0455"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a> achieving a precision of 100 percent using the 1 nearest neighbor for classification and local binary patterns for feature extraction, and we computed a mean overall precision of 83.89 percent for the classical approaches group.</div><div class="u-margin-s-bottom" id="p0450">On the other hand, using NNB approaches, what can be observed is a discrete decrease of precision when compared to classical approaches, more specifically due the generality increase and the abstraction capability. However, the cost is to consider a large input dataset to train the network properly. In terms of precision, the best NNB approache have achieved a accuracy of 100 percent, and we computed a general precision of 77,52 percent. As conclusion, when comparing classical approaches vs NNB ones, despite the fact that NNB approaches have a lower performance for the selected articles in this survey, NNB methods provide the possibility of abstraction, applying facial emotion recognition with larger datasets. This fact allows it to be applied in less restricted scenarios than the classical methods, which requires greater restrictions for their properly operation.</div><div class="u-margin-s-bottom" id="p0455">Another aspect verified in this survey is the importance of the dataset used to train the classical and the NNB approaches. Traditional datasets such as JAFFE are very common and explored over the past few years, besides its low resolution, restrictions in size, and low variability in terms of nationality of the participants and gender. We can observe the lack of reliable datasets with good resolution, number of images, and including the most different cases and variability, made with several countries and ethnic diversity. For instance, a very small group is observed in the MMI dataset in which only 25 people were used. In addition, some datasets are obsolete, and the aforementioned drawbacks makes the tested algorithms often restricted and they end up showing good results only for a limited domain. Nonetheless, there is a remarkable difference for the datasets constructed taking into account artificial and natural expressions, where its hard to extract from a participant the real meaning for a facial expression, specially those dependent from a context. This survey suggest that datasets including natural and artificial expressions must be developed in order to increase the overall precision of the presented approaches.</div><div class="u-margin-s-bottom" id="p0460">Besides the great and interesting results achieved by some of the analyzed approaches, this study showed that the research still face difficulties with some aspects of this type of problem, especially for reliable datasets, controlled environments, due to the variation of luminosity, occlusion, among others. NNB approaches have achieved a very interesting capacity of modelling the nature of the problem, and what can be observed is a trend for the further works explore the capabilities of NNB approaches.</div></section></div><section id="coi005"><h2 id="st225" class="u-h4 u-margin-l-top u-margin-xs-bottom">Declaration of Competing Interest</h2><div class="u-margin-s-bottom" id="p0465">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</div></section><section id="ak005"><h2 id="st215" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><div class="u-margin-s-bottom" id="p0540">This work was carried out with the support of the Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq).</div></section></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="bi005"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="bs005"><ol class="references" id="reference-links-bs005"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0005" id="ref-id-b0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a></span><span class="reference" id="h0005"><div class="other-ref"><span>Albawi, S., Abed Mohammed, T., ALZAWI, S., 2017. Understanding of a convolutional neural network. DOI: 10.1109/ICEngTechnol.2017.8308186.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Albawi%2C%20S.%2C%20Abed%20Mohammed%2C%20T.%2C%20ALZAWI%2C%20S.%2C%202017.%20Understanding%20of%20a%20convolutional%20neural%20network.%20DOI%3A%2010.1109%2FICEngTechnol.2017.8308186." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0010" id="ref-id-b0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a></span><span class="reference" id="h0010"><div class="contribution"><div class="authors u-font-sans">G. Ali, M.A. Iqbal, T.S. Choi</div><div id="ref-id-h0010" class="title text-m">Boosted nne collections for multicultural facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognition, 55 (2016), pp. 14-27</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84969352788&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0010"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Boosted%20nne%20collections%20for%20multicultural%20facial%20expression%20recognition&amp;publication_year=2016&amp;author=G.%20Ali&amp;author=M.A.%20Iqbal&amp;author=T.S.%20Choi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0010"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0015" id="ref-id-b0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></span><span class="reference" id="h0015"><div class="contribution"><div class="authors u-font-sans">H. Ali, M. Hariharan, S. Yaacob, A.H. Adom</div><div id="ref-id-h0015" class="title text-m">Facial emotion recognition using empirical mode decomposition</div></div><div class="host u-font-sans">Expert Systems with Applications, 42 (2015), pp. 1261-1277</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84908052504&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0015"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20using%20empirical%20mode%20decomposition&amp;publication_year=2015&amp;author=H.%20Ali&amp;author=M.%20Hariharan&amp;author=S.%20Yaacob&amp;author=A.H.%20Adom" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0015"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0020" id="ref-id-b0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a></span><span class="reference" id="h0020"><div class="contribution"><div class="authors u-font-sans">V.M. Álvarez, R. Velázquez, S. Gutiérrez, J. Enriquez-Zarate</div><div id="ref-id-h0020" class="title text-m">A method for facial emotion recognition based on interest points</div></div><div class="host u-font-sans">2018 International Conference on Research in Intelligent and Computing in Engineering (RICE), IEEE (2018), pp. 1-4</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20method%20for%20facial%20emotion%20recognition%20based%20on%20interest%20points&amp;publication_year=2018&amp;author=V.M.%20%C3%81lvarez&amp;author=R.%20Vel%C3%A1zquez&amp;author=S.%20Guti%C3%A9rrez&amp;author=J.%20Enriquez-Zarate" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0020"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0025" id="ref-id-b0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a></span><span class="reference" id="h0025"><div class="other-ref"><span>Anila, S., Devarajan, N., 2012. Preprocessing technique for face recognition applications under varying illumination conditions. Global Journal of Computer Science and Technology.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Anila%2C%20S.%2C%20Devarajan%2C%20N.%2C%202012.%20Preprocessing%20technique%20for%20face%20recognition%20applications%20under%20varying%20illumination%20conditions.%20Global%20Journal%20of%20Computer%20Science%20and%20Technology." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0030" id="ref-id-b0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a></span><span class="reference" id="h0030"><div class="contribution"><div class="authors u-font-sans">J.N. Bailenson, E.D. Pontikakis, I.B. Mauss, J.J. Gross, M.E. Jabon, C.A. Hutcherson, C. Nass, O. John</div><div id="ref-id-h0030" class="title text-m">Real-time classification of evoked emotions using facial feature tracking and physiological responses</div></div><div class="host u-font-sans">International Journal of Human-computer Studies, 66 (2008), pp. 303-317</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-41449104132&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Real-time%20classification%20of%20evoked%20emotions%20using%20facial%20feature%20tracking%20and%20physiological%20responses&amp;publication_year=2008&amp;author=J.N.%20Bailenson&amp;author=E.D.%20Pontikakis&amp;author=I.B.%20Mauss&amp;author=J.J.%20Gross&amp;author=M.E.%20Jabon&amp;author=C.A.%20Hutcherson&amp;author=C.%20Nass&amp;author=O.%20John" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0035" id="ref-id-b0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></span><span class="reference" id="h0035"><div class="contribution"><div class="authors u-font-sans">Y. Bar, I. Diamant, L. Wolf, S. Lieberman, E. Konen, H. Greenspan</div><div id="ref-id-h0035" class="title text-m">Chest pathology detection using deep learning with non-medical training</div></div><div class="host u-font-sans">2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI), IEEE (2015), pp. 294-297</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/ISBI.2015.7163871" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0035"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84943786510&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0035"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Chest%20pathology%20detection%20using%20deep%20learning%20with%20non-medical%20training&amp;publication_year=2015&amp;author=Y.%20Bar&amp;author=I.%20Diamant&amp;author=L.%20Wolf&amp;author=S.%20Lieberman&amp;author=E.%20Konen&amp;author=H.%20Greenspan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0035"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0040" id="ref-id-b0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></span><span class="reference" id="h0040"><div class="contribution"><div class="authors u-font-sans">S. Biswas, J. Sil</div><div id="ref-id-h0040" class="title text-m">An efficient expression recognition method using contourlet transform</div></div><div class="host u-font-sans">Proceedings of the 2nd International Conference on Perception and Machine Intelligence, ACM (2015), pp. 167-174</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2708463.2709036" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0040"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84955587690&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0040"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20efficient%20expression%20recognition%20method%20using%20contourlet%20transform&amp;publication_year=2015&amp;author=S.%20Biswas&amp;author=J.%20Sil" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0040"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0045" id="ref-id-b0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a></span><span class="reference" id="h0045"><div class="contribution"><div class="authors u-font-sans">T. Bouwmans, C. Silva, C. Marghes, M.S. Zitouni, H. Bhaskar, C. Frelicot</div><div id="ref-id-h0045" class="title text-m">On the role and the importance of features for background modeling and foreground detection</div></div><div class="host u-font-sans">Computer Science Review, 28 (2018), pp. 26-91</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85047646733&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0045"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=On%20the%20role%20and%20the%20importance%20of%20features%20for%20background%20modeling%20and%20foreground%20detection&amp;publication_year=2018&amp;author=T.%20Bouwmans&amp;author=C.%20Silva&amp;author=C.%20Marghes&amp;author=M.S.%20Zitouni&amp;author=H.%20Bhaskar&amp;author=C.%20Frelicot" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0045"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0050" id="ref-id-b0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a></span><span class="reference" id="h0050"><div class="contribution"><div class="authors u-font-sans">J. Chen, D. Chen, Y. Gong, M. Yu, K. Zhang, L. Wang</div><div id="ref-id-h0050" class="title text-m">Facial expression recognition using geometric and appearance features</div></div><div class="host u-font-sans">Proceedings of the 4th international conference on internet multimedia computing and service (2012), pp. 29-33</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2382336.2382345" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0050"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84869059691&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0050"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20geometric%20and%20appearance%20features&amp;publication_year=2012&amp;author=J.%20Chen&amp;author=D.%20Chen&amp;author=Y.%20Gong&amp;author=M.%20Yu&amp;author=K.%20Zhang&amp;author=L.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0050"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0055" id="ref-id-b0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a></span><span class="reference" id="h0055"><div class="other-ref"><span>Chen, L.F., Yen, Y.S., 2007. Taiwanese facial expression image database. Brain Mapping Laboratory, Institute of Brain Science, National Yang-Ming University, Taipei, Taiwan.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Chen%2C%20L.F.%2C%20Yen%2C%20Y.S.%2C%202007.%20Taiwanese%20facial%20expression%20image%20database.%20Brain%20Mapping%20Laboratory%2C%20Institute%20of%20Brain%20Science%2C%20National%20Yang-Ming%20University%2C%20Taipei%2C%20Taiwan." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0060" id="ref-id-b0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></span><span class="reference" id="h0060"><div class="contribution"><div class="authors u-font-sans">J. Cheng, Y. Deng, H. Meng, Z. Wang</div><div id="ref-id-h0060" class="title text-m">A facial expression based continuous emotional state monitoring system with gpu acceleration</div></div><div class="host u-font-sans">2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG), IEEE (2013), pp. 1-6</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/FG.2013.6553811" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0060"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84886574024&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0060"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20facial%20expression%20based%20continuous%20emotional%20state%20monitoring%20system%20with%20gpu%20acceleration&amp;publication_year=2013&amp;author=J.%20Cheng&amp;author=Y.%20Deng&amp;author=H.%20Meng&amp;author=Z.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0060"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0065" id="ref-id-b0065" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></span><span class="reference" id="h0065"><div class="contribution"><div class="authors u-font-sans">D.Y. Choi, D.H. Kim, B.C. Song</div><div id="ref-id-h0065" class="title text-m">Recognizing fine facial micro-expressions using two-dimensional landmark feature</div></div><div class="host u-font-sans">2018 25th IEEE International Conference on Image Processing (ICIP), IEEE (2018), pp. 1962-1966</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/icip.2018.8451359" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0065"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85062920120&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0065"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognizing%20fine%20facial%20micro-expressions%20using%20two-dimensional%20landmark%20feature&amp;publication_year=2018&amp;author=D.Y.%20Choi&amp;author=D.H.%20Kim&amp;author=B.C.%20Song" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0065"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0070" id="ref-id-b0070" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a></span><span class="reference" id="h0070"><div class="contribution"><div class="authors u-font-sans">T.F. Cootes, C.J. Taylor, D.H. Cooper, J. Graham</div><div id="ref-id-h0070" class="title text-m">Active shape models-their training and application</div></div><div class="host u-font-sans">Computer Vision and Image Understanding, 61 (1995), pp. 38-59</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029182228&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0070"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Active%20shape%20models-their%20training%20and%20application&amp;publication_year=1995&amp;author=T.F.%20Cootes&amp;author=C.J.%20Taylor&amp;author=D.H.%20Cooper&amp;author=J.%20Graham" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0070"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0075" id="ref-id-b0075" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a></span><span class="reference" id="h0075"><div class="contribution"><div class="authors u-font-sans">S.S. Cross, R.F. Harrison, R.L. Kennedy</div><div id="ref-id-h0075" class="title text-m">Introduction to neural networks</div></div><div class="host u-font-sans">The Lancet, 346 (1995), pp. 1075-1079</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0028885032&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0075"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Introduction%20to%20neural%20networks&amp;publication_year=1995&amp;author=S.S.%20Cross&amp;author=R.F.%20Harrison&amp;author=R.L.%20Kennedy" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0075"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0080" id="ref-id-b0080" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></span><span class="reference" id="h0080"><div class="contribution"><div class="authors u-font-sans">A.C. Cruz, B. Bhanu, N.S. Thakoor</div><div id="ref-id-h0080" class="title text-m">One shot emotion scores for facial emotion recognition</div></div><div class="host u-font-sans">2014 IEEE International Conference on Image Processing (ICIP), IEEE (2014), pp. 1376-1380</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/ICIP.2014.7025275" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0080"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84949926556&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0080"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=One%20shot%20emotion%20scores%20for%20facial%20emotion%20recognition&amp;publication_year=2014&amp;author=A.C.%20Cruz&amp;author=B.%20Bhanu&amp;author=N.S.%20Thakoor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0080"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0085" id="ref-id-b0085" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a></span><span class="reference" id="h0085"><div class="other-ref"><span>Dalal, N., Triggs, B., 2005. Histograms of oriented gradients for human detection.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Dalal%2C%20N.%2C%20Triggs%2C%20B.%2C%202005.%20Histograms%20of%20oriented%20gradients%20for%20human%20detection." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0090" id="ref-id-b0090" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a></span><span class="reference" id="h0090"><div class="contribution"><div class="authors u-font-sans">H.B. Deng, L.W. Jin, L.X. Zhen, J.C. Huang, <em> et al.</em></div><div id="ref-id-h0090" class="title text-m">A new facial expression recognition method based on local gabor filter bank and pca plus lda</div></div><div class="host u-font-sans">International Journal of Information Technology, 11 (2005), pp. 86-96</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1016/s0022-5347(18)34576-2" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0090"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20new%20facial%20expression%20recognition%20method%20based%20on%20local%20gabor%20filter%20bank%20and%20pca%20plus%20lda&amp;publication_year=2005&amp;author=H.B.%20Deng&amp;author=L.W.%20Jin&amp;author=L.X.%20Zhen&amp;author=J.C.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0090"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0095" id="ref-id-b0095" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a></span><span class="reference" id="h0095"><div class="contribution"><div class="authors u-font-sans">A. Dhall, R. Goecke, S. Lucey, T. Gedeon</div><div id="ref-id-h0095" class="title text-m">Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark</div></div><div class="host u-font-sans">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), IEEE (2011), pp. 2106-2112</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/ICCVW.2011.6130508" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84856645363&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Static%20facial%20expression%20analysis%20in%20tough%20conditions%3A%20Data%2C%20evaluation%20protocol%20and%20benchmark&amp;publication_year=2011&amp;author=A.%20Dhall&amp;author=R.%20Goecke&amp;author=S.%20Lucey&amp;author=T.%20Gedeon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0095"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0100" id="ref-id-b0100" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a></span><span class="reference" id="h0100"><div class="contribution"><div class="authors u-font-sans">A. Dhall, R. Goecke, S. Lucey, T. Gedeon, <em> et al.</em></div><div id="ref-id-h0100" class="title text-m">Collecting large, richly annotated facial-expression databases from movies</div></div><div class="host u-font-sans">IEEE Multimedia, 19 (2012), pp. 34-41</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866094334&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0100"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Collecting%20large%2C%20richly%20annotated%20facial-expression%20databases%20from%20movies&amp;publication_year=2012&amp;author=A.%20Dhall&amp;author=R.%20Goecke&amp;author=S.%20Lucey&amp;author=T.%20Gedeon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0100"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0105" id="ref-id-b0105" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a></span><span class="reference" id="h0105"><div class="contribution"><div class="authors u-font-sans">H.L. Egger, D.S. Pine, E. Nelson, E. Leibenluft, M. Ernst, K.E. Towbin, A. Angold</div><div id="ref-id-h0105" class="title text-m">The nimh child emotional faces picture set (nimh-chefs): a new set of children’s facial emotion stimuli</div></div><div class="host u-font-sans">International Journal of Methods in Psychiatric Research, 20 (2011), pp. 145-156</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1002/mpr.343" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0105"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052014687&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0105"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20nimh%20child%20emotional%20faces%20picture%20set%20%3A%20a%20new%20set%20of%20childrens%20facial%20emotion%20stimuli&amp;publication_year=2011&amp;author=H.L.%20Egger&amp;author=D.S.%20Pine&amp;author=E.%20Nelson&amp;author=E.%20Leibenluft&amp;author=M.%20Ernst&amp;author=K.E.%20Towbin&amp;author=A.%20Angold" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0105"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0110" id="ref-id-b0110" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></span><span class="reference" id="h0110"><div class="contribution"><div class="authors u-font-sans">M.T. Eskil, K.S. Benli</div><div id="ref-id-h0110" class="title text-m">Facial expression recognition based on anatomy</div></div><div class="host u-font-sans">Computer Vision and Image Understanding, 119 (2014), pp. 1-14</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%20anatomy&amp;publication_year=2014&amp;author=M.T.%20Eskil&amp;author=K.S.%20Benli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0110"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0115" id="ref-id-b0115" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></span><span class="reference" id="h0115"><div class="contribution"><div class="authors u-font-sans">F.S. Farahani, M. Sheikhan, A. Farrokhi</div><div id="ref-id-h0115" class="title text-m">A fuzzy approach for facial emotion recognition</div></div><div class="host u-font-sans">2013 13th Iranian Conference on Fuzzy Systems (IFSC), IEEE (2013), pp. 1-4</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20fuzzy%20approach%20for%20facial%20emotion%20recognition&amp;publication_year=2013&amp;author=F.S.%20Farahani&amp;author=M.%20Sheikhan&amp;author=A.%20Farrokhi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0115"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0120" id="ref-id-b0120" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a></span><span class="reference" id="h0120"><div class="other-ref"><span>Freitas-Magalhaes, A., 2018. Facial Action Coding System 3.0-Manual de Codificacao Cientifica da Face Humana-FM BabyFACS. Leya.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Freitas-Magalhaes%2C%20A.%2C%202018.%20Facial%20Action%20Coding%20System%203.0-Manual%20de%20Codificacao%20Cientifica%20da%20Face%20Humana-FM%20BabyFACS.%20Leya." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0125" id="ref-id-b0125" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a></span><span class="reference" id="h0125"><div class="contribution"><div class="authors u-font-sans">Y. Freund, R.E. Schapire</div><div id="ref-id-h0125" class="title text-m">A decision-theoretic generalization of on-line learning and an application to boosting</div></div><div class="host u-font-sans">Journal of Computer and System Sciences, 55 (1997), pp. 119-139</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0031211090&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0125"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20decision-theoretic%20generalization%20of%20on-line%20learning%20and%20an%20application%20to%20boosting&amp;publication_year=1997&amp;author=Y.%20Freund&amp;author=R.E.%20Schapire" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0125"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0130" id="ref-id-b0130" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a></span><span class="reference" id="h0130"><div class="other-ref"><span>Friesen, E., Ekman, P., 1978. Facial action coding system: a technique for the measurement of facial movement. Palo Alto 3.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Friesen%2C%20E.%2C%20Ekman%2C%20P.%2C%201978.%20Facial%20action%20coding%20system%3A%20a%20technique%20for%20the%20measurement%20of%20facial%20movement.%20Palo%20Alto%203." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0135" id="ref-id-b0135" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></span><span class="reference" id="h0135"><div class="contribution"><div class="authors u-font-sans">Y. Gan</div><div id="ref-id-h0135" class="title text-m">Facial expression recognition using convolutional neural network</div></div><div class="host u-font-sans">Proceedings of the 2nd International Conference on Vision, Image and Signal Processing, ACM (2018), p. 29</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044267046&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0135"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20convolutional%20neural%20network&amp;publication_year=2018&amp;author=Y.%20Gan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0135"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0140" id="ref-id-b0140" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a></span><span class="reference" id="h0140"><div class="contribution"><div class="authors u-font-sans">R. Ghasemi, M. Ahmady</div><div id="ref-id-h0140" class="title text-m">Facial expression recognition using facial effective areas and fuzzy logic</div></div><div class="host u-font-sans">2014 Iranian Conference on Intelligent Systems (ICIS), IEEE (2014), pp. 1-4</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/IranianCIS.2014.6802544" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0140"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20facial%20effective%20areas%20and%20fuzzy%20logic&amp;publication_year=2014&amp;author=R.%20Ghasemi&amp;author=M.%20Ahmady" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0140"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0145" id="ref-id-b0145" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a></span><span class="reference" id="h0145"><div class="contribution"><div class="authors u-font-sans">E. Goeleven, R. De Raedt, L. Leyman, B. Verschuere</div><div id="ref-id-h0145" class="title text-m">The karolinska directed emotional faces: a validation study</div></div><div class="host u-font-sans">Cognition and emotion, 22 (2008), pp. 1094-1118</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1080/02699930701626582" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-48249119182&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20karolinska%20directed%20emotional%20faces%3A%20a%20validation%20study&amp;publication_year=2008&amp;author=E.%20Goeleven&amp;author=R.%20De%20Raedt&amp;author=L.%20Leyman&amp;author=B.%20Verschuere" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0145"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0150" id="ref-id-b0150" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a></span><span class="reference" id="h0150"><div class="other-ref"><span>Gonzales, R.C., Woods, R.E., 2002. Digital image processing.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Gonzales%2C%20R.C.%2C%20Woods%2C%20R.E.%2C%202002.%20Digital%20image%20processing." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0155" id="ref-id-b0155" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a></span><span class="reference" id="h0155"><div class="other-ref"><span>Goodfellow, I., Erhan, D., Carrier, P.L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.H., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J., Ionescu, R., Popescu, M., Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z., Bengio, Y., 2013. Challenges in representation learning: A report on three machine learning contests http://arxiv.org/abs/1307.0414.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Goodfellow%2C%20I.%2C%20Erhan%2C%20D.%2C%20Carrier%2C%20P.L.%2C%20Courville%2C%20A.%2C%20Mirza%2C%20M.%2C%20Hamner%2C%20B.%2C%20Cukierski%2C%20W.%2C%20Tang%2C%20Y.%2C%20Thaler%2C%20D.%2C%20Lee%2C%20D.H.%2C%20Zhou%2C%20Y.%2C%20Ramaiah%2C%20C.%2C%20Feng%2C%20F.%2C%20Li%2C%20R.%2C%20Wang%2C%20X.%2C%20Athanasakis%2C%20D.%2C%20Shawe-Taylor%2C%20J.%2C%20Milakov%2C%20M.%2C%20Park%2C%20J.%2C%20Ionescu%2C%20R.%2C%20Popescu%2C%20M.%2C%20Grozea%2C%20C.%2C%20Bergstra%2C%20J.%2C%20Xie%2C%20J.%2C%20Romaszko%2C%20L.%2C%20Xu%2C%20B.%2C%20Chuang%2C%20Z.%2C%20Bengio%2C%20Y.%2C%202013.%20Challenges%20in%20representation%20learning%3A%20A%20report%20on%20three%20machine%20learning%20contests%20http%3A%2F%2Farxiv.org%2Fabs%2F1307.0414." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0160" id="ref-id-b0160" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a></span><span class="reference" id="h0160"><div class="other-ref"><span>Gupta, A., Sharma, D., Sharma, S., Agarwal, A., 2020. Survey paper on gender and emotion classification using facial expression detection. Proceedings of the International Conference on Innovative Computing &amp; Communications (ICICC) 2020 10.2139/ssrn.3565808.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Gupta%2C%20A.%2C%20Sharma%2C%20D.%2C%20Sharma%2C%20S.%2C%20Agarwal%2C%20A.%2C%202020.%20Survey%20paper%20on%20gender%20and%20emotion%20classification%20using%20facial%20expression%20detection.%20Proceedings%20of%20the%20International%20Conference%20on%20Innovative%20Computing%20%26%20Communications%20(ICICC)%202020%2010.2139%2Fssrn.3565808." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0165" id="ref-id-b0165" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a></span><span class="reference" id="h0165"><div class="contribution"><div class="authors u-font-sans">K. Gurney</div><div id="ref-id-h0165" class="title text-m">An introduction to neural networks</div></div><div class="host u-font-sans">CRC Press (2014)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20neural%20networks&amp;publication_year=2014&amp;author=K.%20Gurney" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0165"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0170" id="ref-id-b0170" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></span><span class="reference" id="h0170"><div class="contribution"><div class="authors u-font-sans">S. Happy, A. Routray</div><div id="ref-id-h0170" class="title text-m">Automatic facial expression recognition using features of salient facial patches</div></div><div class="host u-font-sans">IEEE Transactions on Affective Computing, 6 (2014), pp. 1-12</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20recognition%20using%20features%20of%20salient%20facial%20patches&amp;publication_year=2014&amp;author=S.%20Happy&amp;author=A.%20Routray" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0170"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0175" id="ref-id-b0175" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></span><span class="reference" id="h0175"><div class="contribution"><div class="authors u-font-sans">B. Hernández, G. Olague, R. Hammoud, L. Trujillo, E. Romero</div><div id="ref-id-h0175" class="title text-m">Visual learning of texture descriptors for facial expression recognition in thermal imagery</div></div><div class="host u-font-sans">Computer Vision and Image Understanding, 106 (2007), pp. 258-269</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34247565455&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0175"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Visual%20learning%20of%20texture%20descriptors%20for%20facial%20expression%20recognition%20in%20thermal%20imagery&amp;publication_year=2007&amp;author=B.%20Hern%C3%A1ndez&amp;author=G.%20Olague&amp;author=R.%20Hammoud&amp;author=L.%20Trujillo&amp;author=E.%20Romero" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0175"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0180" id="ref-id-b0180" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></span><span class="reference" id="h0180"><div class="contribution"><div class="authors u-font-sans">M. Hu, H. Wang, X. Wang, J. Yang, R. Wang</div><div id="ref-id-h0180" class="title text-m">Video facial emotion recognition based on local enhanced motion history image and cnn-ctslstm networks</div></div><div class="host u-font-sans">Journal of Visual Communication and Image Representation, 59 (2019), pp. 176-185</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060222481&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0180"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Video%20facial%20emotion%20recognition%20based%20on%20local%20enhanced%20motion%20history%20image%20and%20cnn-ctslstm%20networks&amp;publication_year=2019&amp;author=M.%20Hu&amp;author=H.%20Wang&amp;author=X.%20Wang&amp;author=J.%20Yang&amp;author=R.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0180"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0185" id="ref-id-b0185" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a></span><span class="reference" id="h0185"><div class="other-ref"><span>Huang, N.E., Shen, Z., Long, S.R., Wu, M.C., Shih, H.H., Zheng, Q., Yen, N.C., Tung, C.C., Liu, H.H., 1998. The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis. Proceedings of the Royal Society of London. Series A: mathematical, physical and engineering sciences 454, 903–995.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Huang%2C%20N.E.%2C%20Shen%2C%20Z.%2C%20Long%2C%20S.R.%2C%20Wu%2C%20M.C.%2C%20Shih%2C%20H.H.%2C%20Zheng%2C%20Q.%2C%20Yen%2C%20N.C.%2C%20Tung%2C%20C.C.%2C%20Liu%2C%20H.H.%2C%201998.%20The%20empirical%20mode%20decomposition%20and%20the%20hilbert%20spectrum%20for%20nonlinear%20and%20non-stationary%20time%20series%20analysis.%20Proceedings%20of%20the%20Royal%20Society%20of%20London.%20Series%20A%3A%20mathematical%2C%20physical%20and%20engineering%20sciences%20454%2C%20903%E2%80%93995." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0190" id="ref-id-b0190" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a></span><span class="reference" id="h0190"><div class="contribution"><div class="authors u-font-sans">S. Huang, N. Cai, P.P. Pacheco, S. Narrandes, Y. Wang, W. Xu</div><div id="ref-id-h0190" class="title text-m">Applications of support vector machine (svm) learning in cancer genomics</div></div><div class="host u-font-sans">Cancer Genomics-Proteomics, 15 (2018), pp. 41-51</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1080/15332276.2019.1609343" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0190"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Applications%20of%20support%20vector%20machine%20%20learning%20in%20cancer%20genomics&amp;publication_year=2018&amp;author=S.%20Huang&amp;author=N.%20Cai&amp;author=P.P.%20Pacheco&amp;author=S.%20Narrandes&amp;author=Y.%20Wang&amp;author=W.%20Xu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0190"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0195" id="ref-id-b0195" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a></span><span class="reference" id="h0195"><div class="contribution"><div class="authors u-font-sans">Y. Huang, F. Chen, S. Lv, X. Wang</div></div><div class="host u-font-sans" id="ref-id-h0195">Facial expression recognition: A survey. Symmetry, 11 (2019), p. 1189, <a class="anchor anchor-primary" href="https://doi.org/10.3390/sym11101189" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/sym11101189</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85074271168&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0195"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Huang%2C%20Y.%2C%20Chen%2C%20F.%2C%20Lv%2C%20S.%2C%20Wang%2C%20X.%2C%202019.%20Facial%20expression%20recognition%3A%20A%20survey.%20Symmetry%2011%2C%201189.%2010.3390%2Fsym11101189." target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0195"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0200" id="ref-id-b0200" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></span><span class="reference" id="h0200"><div class="contribution"><div class="authors u-font-sans">M. Ilbeygi, H. Shah-Hosseini</div><div id="ref-id-h0200" class="title text-m">A novel fuzzy facial expression recognition system based on facial feature extraction from color face images</div></div><div class="host u-font-sans">Engineering Applications of Artificial Intelligence, 25 (2012), pp. 130-146</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80855147489&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0200"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20novel%20fuzzy%20facial%20expression%20recognition%20system%20based%20on%20facial%20feature%20extraction%20from%20color%20face%20images&amp;publication_year=2012&amp;author=M.%20Ilbeygi&amp;author=H.%20Shah-Hosseini" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0200"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0205" id="ref-id-b0205" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></span><span class="reference" id="h0205"><div class="contribution"><div class="authors u-font-sans">D.K. Jain, P. Shamsolmoali, P. Sehdev</div><div id="ref-id-h0205" class="title text-m">Extended deep neural network for facial emotion recognition</div></div><div class="host u-font-sans">Pattern Recognition Letters, 120 (2019), pp. 69-74</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060285318&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0205"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Extended%20deep%20neural%20network%20for%20facial%20emotion%20recognition&amp;publication_year=2019&amp;author=D.K.%20Jain&amp;author=P.%20Shamsolmoali&amp;author=P.%20Sehdev" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0205"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0210" id="ref-id-b0210" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></span><span class="reference" id="h0210"><div class="other-ref"><span>Jazouli, M., Majda, A., Zarghili, A., 2017. A p recognizer for automatic facial emotion recognition using kinect sensor, in: 2017 Intelligent Systems and Computer Vision (ISCV), IEEE. pp. 1–5.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Jazouli%2C%20M.%2C%20Majda%2C%20A.%2C%20Zarghili%2C%20A.%2C%202017.%20A%20p%20recognizer%20for%20automatic%20facial%20emotion%20recognition%20using%20kinect%20sensor%2C%20in%3A%202017%20Intelligent%20Systems%20and%20Computer%20Vision%20(ISCV)%2C%20IEEE.%20pp.%201%E2%80%935." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0215" id="ref-id-b0215" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></span><span class="reference" id="h0215"><div class="contribution"><div class="authors u-font-sans">J. Jeon, J.C. Park, Y. Jo, C. Nam, K.H. Bae, Y. Hwang, D.S. Kim</div><div id="ref-id-h0215" class="title text-m">A real-time facial expression recognizer using deep neural network</div></div><div class="host u-font-sans">Proceedings of the 10th International Conference on Ubiquitous Information Management and Communication, ACM (2016), p. 94</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85009904410&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0215"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20real-time%20facial%20expression%20recognizer%20using%20deep%20neural%20network&amp;publication_year=2016&amp;author=J.%20Jeon&amp;author=J.C.%20Park&amp;author=Y.%20Jo&amp;author=C.%20Nam&amp;author=K.H.%20Bae&amp;author=Y.%20Hwang&amp;author=D.S.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0215"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0220" id="ref-id-b0220" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></span><span class="reference" id="h0220"><div class="other-ref"><span>Jia, S., Wang, S., Hu, C., Webster, P.J., Li, X., 2021. Detection of genuine and posed facial expressions of emotion: Databases and methods. Frontiers in Psychology 11, 3818. https://www.frontiersin.org/article/10.3389/ fpsyg.2020.580287, DOI: 10.3389/fpsyg.2020.580287.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Jia%2C%20S.%2C%20Wang%2C%20S.%2C%20Hu%2C%20C.%2C%20Webster%2C%20P.J.%2C%20Li%2C%20X.%2C%202021.%20Detection%20of%20genuine%20and%20posed%20facial%20expressions%20of%20emotion%3A%20Databases%20and%20methods.%20Frontiers%20in%20Psychology%2011%2C%203818.%20https%3A%2F%2Fwww.frontiersin.org%2Farticle%2F10.3389%2F%20fpsyg.2020.580287%2C%20DOI%3A%2010.3389%2Ffpsyg.2020.580287." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0225" id="ref-id-b0225" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></span><span class="reference" id="h0225"><div class="contribution"><div class="authors u-font-sans">R. Jiang, A.T. Ho, I. Cheheb, N. Al-Maadeed, S. Al-Maadeed, A. Bouridane</div><div id="ref-id-h0225" class="title text-m">Emotion recognition from scrambled facial images via many graph embedding</div></div><div class="host u-font-sans">Pattern Recognition, 67 (2017), pp. 245-251</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85016085782&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0225"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20from%20scrambled%20facial%20images%20via%20many%20graph%20embedding&amp;publication_year=2017&amp;author=R.%20Jiang&amp;author=A.T.%20Ho&amp;author=I.%20Cheheb&amp;author=N.%20Al-Maadeed&amp;author=S.%20Al-Maadeed&amp;author=A.%20Bouridane" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0225"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0230" id="ref-id-b0230" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a></span><span class="reference" id="h0230"><div class="contribution"><div class="authors u-font-sans">M. Jones, P. Viola</div><div id="ref-id-h0230" class="title text-m">Fast multi-view face detection</div></div><div class="host u-font-sans">Mitsubishi Electric Research Lab TR-20003-96, 3 (2003), p. 2</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037271425&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0230"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fast%20multi-view%20face%20detection&amp;publication_year=2003&amp;author=M.%20Jones&amp;author=P.%20Viola" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0230"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0235" id="ref-id-b0235" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></span><span class="reference" id="h0235"><div class="contribution"><div class="authors u-font-sans">S.E. Kahou, C. Pal, X. Bouthillier, P. Froumenty, Ç. Gülçehre, R. Memisevic, P. Vincent, A. Courville, Y. Bengio, R.C. Ferrari, <em> et al.</em></div><div id="ref-id-h0235" class="title text-m">Combining modality specific deep neural networks for emotion recognition in video</div></div><div class="host u-font-sans">Proceedings of the 15th ACM on International conference on multimodal interaction (2013), pp. 543-550</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2522848.2531745" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0235"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84892582758&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0235"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Combining%20modality%20specific%20deep%20neural%20networks%20for%20emotion%20recognition%20in%20video&amp;publication_year=2013&amp;author=S.E.%20Kahou&amp;author=C.%20Pal&amp;author=X.%20Bouthillier&amp;author=P.%20Froumenty&amp;author=%C3%87.%20G%C3%BCl%C3%A7ehre&amp;author=R.%20Memisevic&amp;author=P.%20Vincent&amp;author=A.%20Courville&amp;author=Y.%20Bengio&amp;author=R.C.%20Ferrari" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0235"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0240" id="ref-id-b0240" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a></span><span class="reference" id="h0240"><div class="other-ref"><span>Kanade, T., Cohn, J.F., Tian, Y., 2000. Comprehensive database for facial expression analysis, in: Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), IEEE. pp. 46–53.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Kanade%2C%20T.%2C%20Cohn%2C%20J.F.%2C%20Tian%2C%20Y.%2C%202000.%20Comprehensive%20database%20for%20facial%20expression%20analysis%2C%20in%3A%20Proceedings%20Fourth%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20(Cat.%20No.%20PR00580)%2C%20IEEE.%20pp.%2046%E2%80%9353." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0245" id="ref-id-b0245" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></span><span class="reference" id="h0245"><div class="contribution"><div class="authors u-font-sans">A. Kartali, M. Roglić, M. Barjaktarović, M. Durić-Jovičić, M.M. Janković</div><div id="ref-id-h0245" class="title text-m">Real-time algorithms for facial emotion recognition: A comparison of different approaches</div></div><div class="host u-font-sans">2018 14th Symposium on Neural Networks and Applications (NEUREL), IEEE (2018), pp. 1-4</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/neurel.2018.8587011" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0245"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Real-time%20algorithms%20for%20facial%20emotion%20recognition%3A%20A%20comparison%20of%20different%20approaches&amp;publication_year=2018&amp;author=A.%20Kartali&amp;author=M.%20Rogli%C4%87&amp;author=M.%20Barjaktarovi%C4%87&amp;author=M.%20Duri%C4%87-Jovi%C4%8Di%C4%87&amp;author=M.M.%20Jankovi%C4%87" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0245"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0250" id="ref-id-b0250" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></span><span class="reference" id="h0250"><div class="contribution"><div class="authors u-font-sans">V. Khryashchev, L. Ivanovsky, A. Priorov</div><div id="ref-id-h0250" class="title text-m">Deep learning for real-time robust facial expression analysis</div></div><div class="host u-font-sans">Proceedings of the International Conference on Machine Vision and Applications, ACM (2018), pp. 66-70</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/3220511.3220518" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0250"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85053678069&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0250"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20real-time%20robust%20facial%20expression%20analysis&amp;publication_year=2018&amp;author=V.%20Khryashchev&amp;author=L.%20Ivanovsky&amp;author=A.%20Priorov" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0250"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0255" id="ref-id-b0255" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></span><span class="reference" id="h0255"><div class="contribution"><div class="authors u-font-sans">K.C. Kirana, S. Wibawanto, H.W. Herwanto</div><div id="ref-id-h0255" class="title text-m">Facial emotion recognition based on viola-jones algorithm in the learning environment</div></div><div class="host u-font-sans">2018 International Seminar on Application for Technology of Information and Communication, IEEE (2018), pp. 406-410</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20based%20on%20viola-jones%20algorithm%20in%20the%20learning%20environment&amp;publication_year=2018&amp;author=K.C.%20Kirana&amp;author=S.%20Wibawanto&amp;author=H.W.%20Herwanto" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0255"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0260" id="ref-id-b0260" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a></span><span class="reference" id="h0260"><div class="contribution"><div class="authors u-font-sans">B. Kitchenham</div><div id="ref-id-h0260" class="title text-m">Procedures for Performing Systematic Reviews. Keele University. Technical Report TR/SE-0401. Department of Computer Science</div></div><div class="host u-font-sans">Keele University, UK (2004)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Procedures%20for%20Performing%20Systematic%20Reviews.%20Keele%20University.%20Technical%20Report%20TRSE-0401.%20Department%20of%20Computer%20Science&amp;publication_year=2004&amp;author=B.%20Kitchenham" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0260"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0265" id="ref-id-b0265" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a></span><span class="reference" id="h0265"><div class="contribution"><div class="authors u-font-sans">K.E. Ko, K.B. Sim</div><div id="ref-id-h0265" class="title text-m">Development of a facial emotion recognition method based on combining aam with dbn</div></div><div class="host u-font-sans">2010 International Conference on Cyberworlds, IEEE (2010), pp. 87-91</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78751529795&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0265"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Development%20of%20a%20facial%20emotion%20recognition%20method%20based%20on%20combining%20aam%20with%20dbn&amp;publication_year=2010&amp;author=K.E.%20Ko&amp;author=K.B.%20Sim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0265"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0270" id="ref-id-b0270" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a></span><span class="reference" id="h0270"><div class="contribution"><div class="authors u-font-sans">O. Langner, R. Dotsch, G. Bijlstra, D.H. Wigboldus, S.T. Hawk, A. Van Knippenberg</div><div id="ref-id-h0270" class="title text-m">Presentation and validation of the radboud faces database</div></div><div class="host u-font-sans">Cognition and emotion, 24 (2010), pp. 1377-1388</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1080/02699930903485076" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0270"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78649717207&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0270"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Presentation%20and%20validation%20of%20the%20radboud%20faces%20database&amp;publication_year=2010&amp;author=O.%20Langner&amp;author=R.%20Dotsch&amp;author=G.%20Bijlstra&amp;author=D.H.%20Wigboldus&amp;author=S.T.%20Hawk&amp;author=A.%20Van%20Knippenberg" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0270"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0275" id="ref-id-b0275" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a></span><span class="reference" id="h0275"><div class="contribution"><div class="authors u-font-sans">G. Levi, T. Hassner</div><div id="ref-id-h0275" class="title text-m">Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</div></div><div class="host u-font-sans">Proceedings of the 2015 ACM on international conference on multimodal interaction (2015), pp. 503-510</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2818346.2830587" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0275"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959297017&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0275"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20in%20the%20wild%20via%20convolutional%20neural%20networks%20and%20mapped%20binary%20patterns&amp;publication_year=2015&amp;author=G.%20Levi&amp;author=T.%20Hassner" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0275"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0280" id="ref-id-b0280" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a></span><span class="reference" id="h0280"><div class="other-ref"><span>Li, S., Deng, W., 2018. Deep facial expression recognition: A survey. CoRR abs/1804.08348. http://arxiv.org/abs/1804.08348.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Li%2C%20S.%2C%20Deng%2C%20W.%2C%202018.%20Deep%20facial%20expression%20recognition%3A%20A%20survey.%20CoRR%20abs%2F1804.08348.%20http%3A%2F%2Farxiv.org%2Fabs%2F1804.08348." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0285" id="ref-id-b0285" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></span><span class="reference" id="h0285"><div class="contribution"><div class="authors u-font-sans">Y. Li, S. Wang, Y. Zhao, Q. Ji</div><div id="ref-id-h0285" class="title text-m">Simultaneous facial feature tracking and facial expression recognition</div></div><div class="host u-font-sans">IEEE Transactions on Image Processing, 22 (2013), pp. 2559-2573</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84877901074&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0285"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Simultaneous%20facial%20feature%20tracking%20and%20facial%20expression%20recognition&amp;publication_year=2013&amp;author=Y.%20Li&amp;author=S.%20Wang&amp;author=Y.%20Zhao&amp;author=Q.%20Ji" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0285"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0290" id="ref-id-b0290" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></span><span class="reference" id="h0290"><div class="contribution"><div class="authors u-font-sans">A.T. Lopes, E. de Aguiar, A.F. De Souza, T. Oliveira-Santos</div><div id="ref-id-h0290" class="title text-m">Facial expression recognition with convolutional neural networks: coping with few data and the training sample order</div></div><div class="host u-font-sans">Pattern Recognition, 61 (2017), pp. 610-628</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84991821737&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0290"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20with%20convolutional%20neural%20networks%3A%20coping%20with%20few%20data%20and%20the%20training%20sample%20order&amp;publication_year=2017&amp;author=A.T.%20Lopes&amp;author=E.%20de%20Aguiar&amp;author=A.F.%20De%20Souza&amp;author=T.%20Oliveira-Santos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0290"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0295" id="ref-id-b0295" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a></span><span class="reference" id="h0295"><div class="contribution"><div class="authors u-font-sans">A.T. Lopes, E. De Aguiar, T. Oliveira-Santos</div><div id="ref-id-h0295" class="title text-m">A facial expression recognition system using convolutional networks</div></div><div class="host u-font-sans">2015 28th SIBGRAPI Conference on Graphics, Patterns and Images, IEEE (2015), pp. 273-280</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/SIBGRAPI.2015.14" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0295"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959358247&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0295"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20facial%20expression%20recognition%20system%20using%20convolutional%20networks&amp;publication_year=2015&amp;author=A.T.%20Lopes&amp;author=E.%20De%20Aguiar&amp;author=T.%20Oliveira-Santos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0295"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0300" id="ref-id-b0300" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a></span><span class="reference" id="h0300"><div class="contribution"><div class="authors u-font-sans">P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews</div><div id="ref-id-h0300" class="title text-m">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</div></div><div class="host u-font-sans">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops, IEEE (2010), pp. 94-101</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77956509035&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0300"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20extended%20cohn-kanade%20dataset%20%3A%20A%20complete%20dataset%20for%20action%20unit%20and%20emotion-specified%20expression&amp;publication_year=2010&amp;author=P.%20Lucey&amp;author=J.F.%20Cohn&amp;author=T.%20Kanade&amp;author=J.%20Saragih&amp;author=Z.%20Ambadar&amp;author=I.%20Matthews" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0300"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0305" id="ref-id-b0305" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></span><span class="reference" id="h0305"><div class="contribution"><div class="authors u-font-sans">Y. Luo, L. Zhang, Y. Chen, W. Jiang</div><div id="ref-id-h0305" class="title text-m">Facial expression recognition algorithm based on reverse co-salient regions (rcsr) features</div></div><div class="host u-font-sans">2017 4th International Conference on Information Science and Control Engineering (ICISCE), IEEE (2017), pp. 326-329</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85040619917&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0305"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20algorithm%20based%20on%20reverse%20co-salient%20regions%20%20features&amp;publication_year=2017&amp;author=Y.%20Luo&amp;author=L.%20Zhang&amp;author=Y.%20Chen&amp;author=W.%20Jiang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0305"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0310" id="ref-id-b0310" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a></span><span class="reference" id="h0310"><div class="contribution"><div class="authors u-font-sans">M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba</div><div id="ref-id-h0310" class="title text-m">Coding facial expressions with gabor wavelets</div></div><div class="host u-font-sans">Proceedings Third IEEE international conference on automatic face and gesture recognition, IEEE. (1998), pp. 200-205</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84857444525&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0310"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Coding%20facial%20expressions%20with%20gabor%20wavelets&amp;publication_year=1998&amp;author=M.%20Lyons&amp;author=S.%20Akamatsu&amp;author=M.%20Kamachi&amp;author=J.%20Gyoba" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0310"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0315" id="ref-id-b0315" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a></span><span class="reference" id="h0315"><div class="other-ref"><span>Mehta, Dhwani, M.F.S., Javaid, A.Y., 2018. Facial emotion recognition: A survey and real-world user experiences in mixed reality. Sensors 18, 416.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Mehta%2C%20Dhwani%2C%20M.F.S.%2C%20Javaid%2C%20A.Y.%2C%202018.%20Facial%20emotion%20recognition%3A%20A%20survey%20and%20real-world%20user%20experiences%20in%20mixed%20reality.%20Sensors%2018%2C%20416." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0320" id="ref-id-b0320" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a></span><span class="reference" id="h0320"><div class="contribution"><div class="authors u-font-sans">N. Mehta, S. Jadhav</div><div id="ref-id-h0320" class="title text-m">Facial emotion recognition using log gabor filter and pca</div></div><div class="host u-font-sans">2016 International Conference on Computing Communication Control and automation (ICCUBEA, IEEE (2016), pp. 1-5</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20using%20log%20gabor%20filter%20and%20pca&amp;publication_year=2016&amp;author=N.%20Mehta&amp;author=S.%20Jadhav" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0320"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0325" id="ref-id-b0325" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[65]</span></span></a></span><span class="reference" id="h0325"><div class="contribution"><div class="authors u-font-sans">B.H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, <em> et al.</em></div><div id="ref-id-h0325" class="title text-m">The multimodal brain tumor image segmentation benchmark (brats)</div></div><div class="host u-font-sans">IEEE transactions on medical imaging, 34 (2014), pp. 1993-2024</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20multimodal%20brain%20tumor%20image%20segmentation%20benchmark%20&amp;publication_year=2014&amp;author=B.H.%20Menze&amp;author=A.%20Jakab&amp;author=S.%20Bauer&amp;author=J.%20Kalpathy-Cramer&amp;author=K.%20Farahani&amp;author=J.%20Kirby&amp;author=Y.%20Burren&amp;author=N.%20Porz&amp;author=J.%20Slotboom&amp;author=R.%20Wiest" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0325"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0330" id="ref-id-b0330" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></span><span class="reference" id="h0330"><div class="contribution"><div class="authors u-font-sans">S. Mohseni, N. Zarei, S. Ramazani</div></div><div class="host u-font-sans" id="ref-id-h0330">Facial expression recognition using anatomy based facial graph in: 2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE (2014), pp. 3715-3719</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84938082054&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0330"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20anatomy%20based%20facial%20graph%20in%3A%202014%20IEEE%20International%20Conference%20on%20Systems%2C%20Man%2C%20and%20Cybernetics%20&amp;publication_year=2014&amp;author=S.%20Mohseni&amp;author=N.%20Zarei&amp;author=S.%20Ramazani" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0330"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0335" id="ref-id-b0335" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a></span><span class="reference" id="h0335"><div class="contribution"><div class="authors u-font-sans">M. Moolchandani, S. Dwivedi, S. Nigam, K. Gupta</div><div id="ref-id-h0335" class="title text-m">A survey on: Facial emotion recognition and classification</div></div><div class="host u-font-sans">2021 5th International Conference on Computing Methodologies and Communication (ICCMC) (2021), pp. 1677-1686, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCMC51019.2021.9418349" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCMC51019.2021.9418349</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85106049482&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0335"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20on%3A%20Facial%20emotion%20recognition%20and%20classification&amp;publication_year=2021&amp;author=M.%20Moolchandani&amp;author=S.%20Dwivedi&amp;author=S.%20Nigam&amp;author=K.%20Gupta" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0335"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0340" id="ref-id-b0340" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></span><span class="reference" id="h0340"><div class="contribution"><div class="authors u-font-sans">D. Mushfieldt, M. Ghaziasgar, J. Connan</div><div id="ref-id-h0340" class="title text-m">Robust facial expression recognition in the presence of rotation and partial occlusion</div></div><div class="host u-font-sans">Proceedings of the South African Institute for Computer Scientists and Information Technologists Conference (2013), pp. 186-193</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2513456.2513493" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0340"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84886258295&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0340"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Robust%20facial%20expression%20recognition%20in%20the%20presence%20of%20rotation%20and%20partial%20occlusion&amp;publication_year=2013&amp;author=D.%20Mushfieldt&amp;author=M.%20Ghaziasgar&amp;author=J.%20Connan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0340"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0345" id="ref-id-b0345" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a></span><span class="reference" id="h0345"><div class="other-ref"><span>Nanni, L., Ghidoni, S., Brahnam, S., 2017. Handcrafted vs. non-handcrafted features for computer vision classification. Pattern Recognition 71, 158–172. https://www.sciencedirect.com/science/article/pii/ S0031320317302224, doi: 10.1016/j.patcog.2017.05.025.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Nanni%2C%20L.%2C%20Ghidoni%2C%20S.%2C%20Brahnam%2C%20S.%2C%202017.%20Handcrafted%20vs.%20non-handcrafted%20features%20for%20computer%20vision%20classification.%20Pattern%20Recognition%2071%2C%20158%E2%80%93172.%20https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2F%20S0031320317302224%2C%20doi%3A%2010.1016%2Fj.patcog.2017.05.025." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0350" id="ref-id-b0350" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a></span><span class="reference" id="h0350"><div class="contribution"><div class="authors u-font-sans">H.W. Ng, V.D. Nguyen, V. Vonikakis, S. Winkler</div><div id="ref-id-h0350" class="title text-m">Deep learning for emotion recognition on small datasets using transfer learning</div></div><div class="host u-font-sans">Proceedings of the 2015 ACM on international conference on multimodal interaction (2015), pp. 443-449</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2818346.2830593" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0350"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959295186&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0350"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20emotion%20recognition%20on%20small%20datasets%20using%20transfer%20learning&amp;publication_year=2015&amp;author=H.W.%20Ng&amp;author=V.D.%20Nguyen&amp;author=V.%20Vonikakis&amp;author=S.%20Winkler" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0350"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0355" id="ref-id-b0355" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a></span><span class="reference" id="h0355"><div class="other-ref"><span>Nicolai, A., Choi, A., 2015. Facial emotion recognition using fuzzy systems, in: 2015 IEEE international conference on systems, man, and cybernetics, IEEE. pp. 2216–2221.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Nicolai%2C%20A.%2C%20Choi%2C%20A.%2C%202015.%20Facial%20emotion%20recognition%20using%20fuzzy%20systems%2C%20in%3A%202015%20IEEE%20international%20conference%20on%20systems%2C%20man%2C%20and%20cybernetics%2C%20IEEE.%20pp.%202216%E2%80%932221." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0360" id="ref-id-b0360" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a></span><span class="reference" id="h0360"><div class="contribution"><div class="authors u-font-sans">T. Ojala, M. Pietikäinen, D. Harwood</div><div id="ref-id-h0360" class="title text-m">A comparative study of texture measures with classification based on featured distributions</div></div><div class="host u-font-sans">Pattern recognition, 29 (1996), pp. 51-59</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20comparative%20study%20of%20texture%20measures%20with%20classification%20based%20on%20featured%20distributions&amp;publication_year=1996&amp;author=T.%20Ojala&amp;author=M.%20Pietik%C3%A4inen&amp;author=D.%20Harwood" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0360"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0365" id="ref-id-b0365" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a></span><span class="reference" id="h0365"><div class="contribution"><div class="authors u-font-sans">M. Pantic, M. Valstar, R. Rademaker, L. Maat</div><div id="ref-id-h0365" class="title text-m">Web-based database for facial expression analysis</div></div><div class="host u-font-sans">2005 IEEE international conference on multimedia and Expo, IEEE (2005), p. 5</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Web-based%20database%20for%20facial%20expression%20analysis&amp;publication_year=2005&amp;author=M.%20Pantic&amp;author=M.%20Valstar&amp;author=R.%20Rademaker&amp;author=L.%20Maat" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0365"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0370" id="ref-id-b0370" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a></span><span class="reference" id="h0370"><div class="other-ref"><span>Papageorgiou, C.P., Oren, M., Poggio, T., 1998. A general framework for object detection, in: Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271), IEEE. pp. 555–562.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Papageorgiou%2C%20C.P.%2C%20Oren%2C%20M.%2C%20Poggio%2C%20T.%2C%201998.%20A%20general%20framework%20for%20object%20detection%2C%20in%3A%20Sixth%20International%20Conference%20on%20Computer%20Vision%20(IEEE%20Cat.%20No.%2098CH36271)%2C%20IEEE.%20pp.%20555%E2%80%93562." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0375" id="ref-id-b0375" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[75]</span></span></a></span><span class="reference" id="h0375"><div class="contribution"><div class="authors u-font-sans">N. Perveen, S. Gupta, K. Verma</div><div id="ref-id-h0375" class="title text-m">Facial expression recognition using facial characteristic points and gini index</div></div><div class="host u-font-sans">2012 Students Conference on Engineering and Systems, IEEE (2012), pp. 1-6</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/SCES.2012.6199086" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0375"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20facial%20characteristic%20points%20and%20gini%20index&amp;publication_year=2012&amp;author=N.%20Perveen&amp;author=S.%20Gupta&amp;author=K.%20Verma" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0375"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0380" id="ref-id-b0380" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></span><span class="reference" id="h0380"><div class="contribution"><div class="authors u-font-sans">R.V. Puthanidam, T.S. Moh</div><div id="ref-id-h0380" class="title text-m">A hybrid approach for facial expression recognition</div></div><div class="host u-font-sans">Proceedings of the 12th International Conference on Ubiquitous Information Management and Communication, ACM (2018), p. 60</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048394526&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0380"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20hybrid%20approach%20for%20facial%20expression%20recognition&amp;publication_year=2018&amp;author=R.V.%20Puthanidam&amp;author=T.S.%20Moh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0380"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0385" id="ref-id-b0385" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a></span><span class="reference" id="h0385"><div class="contribution"><div class="authors u-font-sans">L. Rabiner, B. Juang</div><div id="ref-id-h0385" class="title text-m">An introduction to hidden markov models</div></div><div class="host u-font-sans">ieee assp magazine, 3 (1986), pp. 4-16</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0022594196&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0385"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20introduction%20to%20hidden%20markov%20models&amp;publication_year=1986&amp;author=L.%20Rabiner&amp;author=B.%20Juang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0385"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0390" id="ref-id-b0390" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a></span><span class="reference" id="h0390"><div class="contribution"><div class="authors u-font-sans">Y. Rahulamathavan, R.C.W. Phan, J.A. Chambers, D.J. Parish</div><div id="ref-id-h0390" class="title text-m">Facial expression recognition in the encrypted domain based on local fisher discriminant analysis</div></div><div class="host u-font-sans">IEEE Transactions on Affective Computing, 4 (2012), pp. 83-92</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20in%20the%20encrypted%20domain%20based%20on%20local%20fisher%20discriminant%20analysis&amp;publication_year=2012&amp;author=Y.%20Rahulamathavan&amp;author=R.C.W.%20Phan&amp;author=J.A.%20Chambers&amp;author=D.J.%20Parish" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0390"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0395" id="ref-id-b0395" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a></span><span class="reference" id="h0395"><div class="contribution"><div class="authors u-font-sans">S. Rajan, P. Chenniappan, S. Devaraj, N. Madian</div><div id="ref-id-h0395" class="title text-m">Facial expression recognition techniques: a comprehensive survey</div></div><div class="host u-font-sans">IET Image Processing, 13 (2019), pp. 1031-1040</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1049/iet-ipr.2018.6647" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0395"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85067023248&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0395"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20techniques%3A%20a%20comprehensive%20survey&amp;publication_year=2019&amp;author=S.%20Rajan&amp;author=P.%20Chenniappan&amp;author=S.%20Devaraj&amp;author=N.%20Madian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0395"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0400" id="ref-id-b0400" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a></span><span class="reference" id="h0400"><div class="other-ref"><span>raval, D., Sakle, M., 2015. A literature review on emotion recognition system using various facial expression. International Journal of Advance Research and Innovative Ideas in Education 1, 326–329.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=raval%2C%20D.%2C%20Sakle%2C%20M.%2C%202015.%20A%20literature%20review%20on%20emotion%20recognition%20system%20using%20various%20facial%20expression.%20International%20Journal%20of%20Advance%20Research%20and%20Innovative%20Ideas%20in%20Education%201%2C%20326%E2%80%93329." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0405" id="ref-id-b0405" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a></span><span class="reference" id="h0405"><div class="other-ref"><span>Revina, I., Emmanuel, W.S., 2021. A survey on human face expression recognition techniques. Journal of King Saud University - Computer and Information Sciences 33, 619–628. https://www.sciencedirect.com/science/article/pii/ S1319157818303379, doi: 10.1016/j.jksuci.2018.09.002.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Revina%2C%20I.%2C%20Emmanuel%2C%20W.S.%2C%202021.%20A%20survey%20on%20human%20face%20expression%20recognition%20techniques.%20Journal%20of%20King%20Saud%20University%20-%20Computer%20and%20Information%20Sciences%2033%2C%20619%E2%80%93628.%20https%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2F%20S1319157818303379%2C%20doi%3A%2010.1016%2Fj.jksuci.2018.09.002." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0410" id="ref-id-b0410" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a></span><span class="reference" id="h0410"><div class="contribution"><div class="authors u-font-sans">T. Sakai, M. Nagao, S. Fujibayashi</div><div id="ref-id-h0410" class="title text-m">Line extraction and pattern detection in a photograph</div></div><div class="host u-font-sans">Pattern recognition, 1 (1969), pp. 233-248</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0002725499&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0410"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Line%20extraction%20and%20pattern%20detection%20in%20a%20photograph&amp;publication_year=1969&amp;author=T.%20Sakai&amp;author=M.%20Nagao&amp;author=S.%20Fujibayashi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0410"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0415" id="ref-id-b0415" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a></span><span class="reference" id="h0415"><div class="contribution"><div class="authors u-font-sans">V.V. Salunke, C. Patil</div><div id="ref-id-h0415" class="title text-m">A new approach for automatic face emotion recognition and classification based on deep networks</div></div><div class="host u-font-sans">2017 International Conference on Computing, Communication, Control and Automation (ICCUBEA), IEEE (2017), pp. 1-5</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/iccubea.2017.8463785" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0415"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20new%20approach%20for%20automatic%20face%20emotion%20recognition%20and%20classification%20based%20on%20deep%20networks&amp;publication_year=2017&amp;author=V.V.%20Salunke&amp;author=C.%20Patil" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0415"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0420" id="ref-id-b0420" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[84]</span></span></a></span><span class="reference" id="h0420"><div class="contribution"><div class="authors u-font-sans">W. Shen, M. Zhou, F. Yang, C. Yang, J. Tian</div><div id="ref-id-h0420" class="title text-m">Multi-scale convolutional neural networks for lung nodule classification</div></div><div class="host u-font-sans">International Conference on Information Processing in Medical Imaging, Springer (2015), pp. 588-599</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1007/978-3-319-19992-4_46" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0420"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84983670549&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0420"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-scale%20convolutional%20neural%20networks%20for%20lung%20nodule%20classification&amp;publication_year=2015&amp;author=W.%20Shen&amp;author=M.%20Zhou&amp;author=F.%20Yang&amp;author=C.%20Yang&amp;author=J.%20Tian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0420"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0425" id="ref-id-b0425" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[85]</span></span></a></span><span class="reference" id="h0425"><div class="contribution"><div class="authors u-font-sans">F.Y. Shih, C.F. Chuang</div><div id="ref-id-h0425" class="title text-m">Automatic extraction of head and face boundaries and facial features</div></div><div class="host u-font-sans">Information Sciences, 158 (2004), pp. 117-130</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0242692522&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0425"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20extraction%20of%20head%20and%20face%20boundaries%20and%20facial%20features&amp;publication_year=2004&amp;author=F.Y.%20Shih&amp;author=C.F.%20Chuang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0425"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0430" id="ref-id-b0430" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[86]</span></span></a></span><span class="reference" id="h0430"><div class="contribution"><div class="authors u-font-sans">H.C. Shin, H.R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, R.M. Summers</div><div id="ref-id-h0430" class="title text-m">Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning</div></div><div class="host u-font-sans">IEEE transactions on medical imaging, 35 (2016), pp. 1285-1298</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84969962996&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0430"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20convolutional%20neural%20networks%20for%20computer-aided%20detection%3A%20Cnn%20architectures%2C%20dataset%20characteristics%20and%20transfer%20learning&amp;publication_year=2016&amp;author=H.C.%20Shin&amp;author=H.R.%20Roth&amp;author=M.%20Gao&amp;author=L.%20Lu&amp;author=Z.%20Xu&amp;author=I.%20Nogues&amp;author=J.%20Yao&amp;author=D.%20Mollura&amp;author=R.M.%20Summers" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0430"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0435" id="ref-id-b0435" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[87]</span></span></a></span><span class="reference" id="h0435"><div class="contribution"><div class="authors u-font-sans">J. Shlens</div><div id="ref-id-h0435" class="title text-m">A tutorial on principal component analysis. arXiv preprint arXiv:1404.1100</div></div><div class="host u-font-sans"> (2014)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20tutorial%20on%20principal%20component%20analysis.%20arXiv%20preprint%20arXiv%3A1404.1100&amp;publication_year=2014&amp;author=J.%20Shlens" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0435"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0440" id="ref-id-b0440" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[88]</span></span></a></span><span class="reference" id="h0440"><div class="contribution"><div class="authors u-font-sans">S. Shojaeilangari, W.Y. Yau, K. Nandakumar, J. Li, E.K. Teoh</div><div id="ref-id-h0440" class="title text-m">Robust representation and recognition of facial emotions using extreme sparse learning</div></div><div class="host u-font-sans">IEEE Transactions on Image Processing, 24 (2015), pp. 2140-2152</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84927729481&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0440"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Robust%20representation%20and%20recognition%20of%20facial%20emotions%20using%20extreme%20sparse%20learning&amp;publication_year=2015&amp;author=S.%20Shojaeilangari&amp;author=W.Y.%20Yau&amp;author=K.%20Nandakumar&amp;author=J.%20Li&amp;author=E.K.%20Teoh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0440"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0445" id="ref-id-b0445" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[89]</span></span></a></span><span class="reference" id="h0445"><div class="other-ref"><span>Silander, T., Myllymaki, P., 2012. A simple approach for finding the globally optimal bayesian network structure. arXiv preprint arXiv:1206.6875.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Silander%2C%20T.%2C%20Myllymaki%2C%20P.%2C%202012.%20A%20simple%20approach%20for%20finding%20the%20globally%20optimal%20bayesian%20network%20structure.%20arXiv%20preprint%20arXiv%3A1206.6875." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0450" id="ref-id-b0450" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[90]</span></span></a></span><span class="reference" id="h0450"><div class="other-ref"><span>Singh, H.P., Bailer-Jones, C.A., Gupta, R., 2001. Introduction to artificial neural networks.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Singh%2C%20H.P.%2C%20Bailer-Jones%2C%20C.A.%2C%20Gupta%2C%20R.%2C%202001.%20Introduction%20to%20artificial%20neural%20networks." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0455" id="ref-id-b0455" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a></span><span class="reference" id="h0455"><div class="contribution"><div class="authors u-font-sans">K. Slimani, M. Kas, Y. El Merabet, R. Messoussi, Y. Ruichek</div><div id="ref-id-h0455" class="title text-m">Facial emotion recognition: A comparative analysis using 22 lbp variants</div></div><div class="host u-font-sans">Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence, ACM (2018), pp. 88-94</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/3177148.3180092" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0455"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85047142316&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0455"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%3A%20A%20comparative%20analysis%20using%2022%20lbp%20variants&amp;publication_year=2018&amp;author=K.%20Slimani&amp;author=M.%20Kas&amp;author=Y.%20El%20Merabet&amp;author=R.%20Messoussi&amp;author=Y.%20Ruichek" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0455"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0460" id="ref-id-b0460" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[92]</span></span></a></span><span class="reference" id="h0460"><div class="contribution"><div class="authors u-font-sans">S. Soo</div><div id="ref-id-h0460" class="title text-m">Object detection using haar-cascade classifier</div></div><div class="host u-font-sans">University of Tartu, Institute of Computer Science (2014), pp. 1-12</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85009195051&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0460"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Object%20detection%20using%20haar-cascade%20classifier&amp;publication_year=2014&amp;author=S.%20Soo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0460"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0465" id="ref-id-b0465" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[93]</span></span></a></span><span class="reference" id="h0465"><div class="other-ref"><span>Sugiyama, M., 2006. Local fisher discriminant analysis for supervised dimensionality reduction, in: Proceedings of the 23rd international conference on Machine learning, pp. 905–912.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Sugiyama%2C%20M.%2C%202006.%20Local%20fisher%20discriminant%20analysis%20for%20supervised%20dimensionality%20reduction%2C%20in%3A%20Proceedings%20of%20the%2023rd%20international%20conference%20on%20Machine%20learning%2C%20pp.%20905%E2%80%93912." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0470" id="ref-id-b0470" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[94]</span></span></a></span><span class="reference" id="h0470"><div class="contribution"><div class="authors u-font-sans">J.M. Sun, X.S. Pei, S.S. Zhou</div><div id="ref-id-h0470" class="title text-m">Facial emotion recognition in modern distant education system using svm</div></div><div class="host u-font-sans">2008 International Conference on Machine Learning and Cybernetics, IEEE (2008), pp. 3545-3548</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-57849154035&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0470"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20in%20modern%20distant%20education%20system%20using%20svm&amp;publication_year=2008&amp;author=J.M.%20Sun&amp;author=X.S.%20Pei&amp;author=S.S.%20Zhou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0470"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0475" id="ref-id-b0475" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a></span><span class="reference" id="h0475"><div class="contribution"><div class="authors u-font-sans">M. Szwoch</div><div id="ref-id-h0475" class="title text-m">Feedb: a multimodal database of facial expressions and emotions</div></div><div class="host u-font-sans">2013 6th International Conference on Human System Interactions (HSI), IEEE (2013), pp. 524-531</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/HSI.2013.6577876" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0475"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84883727507&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0475"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feedb%3A%20a%20multimodal%20database%20of%20facial%20expressions%20and%20emotions&amp;publication_year=2013&amp;author=M.%20Szwoch" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0475"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0480" id="ref-id-b0480" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a></span><span class="reference" id="h0480"><div class="contribution"><div class="authors u-font-sans">M. Szwoch, P. Pieniażek</div><div id="ref-id-h0480" class="title text-m">Facial emotion recognition using depth data</div></div><div class="host u-font-sans">2015 8th International Conference on Human System Interaction (HSI), IEEE (2015), pp. 271-277</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/HSI.2015.7170679" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0480"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84945975948&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0480"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20using%20depth%20data&amp;publication_year=2015&amp;author=M.%20Szwoch&amp;author=P.%20Pienia%C5%BCek" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0480"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0485" id="ref-id-b0485" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[97]</span></span></a></span><span class="reference" id="h0485"><div class="contribution"><div class="authors u-font-sans">L. Tan, K. Zhang, K. Wang, X. Zeng, X. Peng, Y. Qiao</div><div id="ref-id-h0485" class="title text-m">Group emotion recognition with individual facial emotion cnns and global image based cnns</div></div><div class="host u-font-sans">Proceedings of the 19th ACM International Conference on Multimodal Interaction (2017), pp. 549-552</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/3136755.3143008" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0485"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85046734007&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0485"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Group%20emotion%20recognition%20with%20individual%20facial%20emotion%20cnns%20and%20global%20image%20based%20cnns&amp;publication_year=2017&amp;author=L.%20Tan&amp;author=K.%20Zhang&amp;author=K.%20Wang&amp;author=X.%20Zeng&amp;author=X.%20Peng&amp;author=Y.%20Qiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0485"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0490" id="ref-id-b0490" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[98]</span></span></a></span><span class="reference" id="h0490"><div class="other-ref"><span>Tarnowski, P., Kolodziej, M., Majkowski, A., Rak, R.J., 2017. Emotion recognition using facial expressions., in: ICCS, pp. 1175–1184.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Tarnowski%2C%20P.%2C%20Kolodziej%2C%20M.%2C%20Majkowski%2C%20A.%2C%20Rak%2C%20R.J.%2C%202017.%20Emotion%20recognition%20using%20facial%20expressions.%2C%20in%3A%20ICCS%2C%20pp.%201175%E2%80%931184." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0495" id="ref-id-b0495" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></span><span class="reference" id="h0495"><div class="contribution"><div class="authors u-font-sans">I. Tautkute, T. Trzcinski, A. Bielski</div><div id="ref-id-h0495" class="title text-m">I know how you feel: Emotion recognition with facial landmarks</div></div><div class="host u-font-sans">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (2018), pp. 1878-1880</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=I%20know%20how%20you%20feel%3A%20Emotion%20recognition%20with%20facial%20landmarks&amp;publication_year=2018&amp;author=I.%20Tautkute&amp;author=T.%20Trzcinski&amp;author=A.%20Bielski" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0495"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0500" id="ref-id-b0500" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[100]</span></span></a></span><span class="reference" id="h0500"><div class="contribution"><div class="authors u-font-sans">G.A. Taylor</div><div id="ref-id-h0500" class="title text-m">Initial steps in image preparation</div></div><div class="host u-font-sans">American Journal of Roentgenology, 179 (2002), pp. 1411-1413</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.2214/ajr.179.6.1791411" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0500"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036891755&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0500"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Initial%20steps%20in%20image%20preparation&amp;publication_year=2002&amp;author=G.A.%20Taylor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0500"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0505" id="ref-id-b0505" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[101]</span></span></a></span><span class="reference" id="h0505"><div class="contribution"><div class="authors u-font-sans">P.R. Thrift</div><div id="ref-id-h0505" class="title text-m">Fuzzy logic synthesis with genetic algorithms., in: ICGA</div></div><div class="host u-font-sans"> (1991), pp. 509-513</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fuzzy%20logic%20synthesis%20with%20genetic%20algorithms.%2C%20in%3A%20ICGA&amp;publication_year=1991&amp;author=P.R.%20Thrift" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0505"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0510" id="ref-id-b0510" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[102]</span></span></a></span><span class="reference" id="h0510"><div class="contribution"><div class="authors u-font-sans">S. Turabzadeh, H. Meng, R.M. Swash, M. Pleva, J. Juhar</div><div id="ref-id-h0510" class="title text-m">Real-time emotional state detection from facial expression on embedded devices</div></div><div class="host u-font-sans">2017 Seventh International Conference on Innovative Computing Technology (INTECH), IEEE (2017), pp. 46-51</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/INTECH.2017.8102423" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0510"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85040773488&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0510"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Real-time%20emotional%20state%20detection%20from%20facial%20expression%20on%20embedded%20devices&amp;publication_year=2017&amp;author=S.%20Turabzadeh&amp;author=H.%20Meng&amp;author=R.M.%20Swash&amp;author=M.%20Pleva&amp;author=J.%20Juhar" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0510"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0515" id="ref-id-b0515" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[103]</span></span></a></span><span class="reference" id="h0515"><div class="other-ref"><span>Ullah, S., Tian, W., 2020. A systematic literature review of recognition of compound facial expression of emotions, in: ICVIP 2020: 2020 The 4th International Conference on Video and Image Processing, Association for Computing Machinery, New York, NY, USA. p. 116–121. doi: 10.1145/3447450.3447469.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Ullah%2C%20S.%2C%20Tian%2C%20W.%2C%202020.%20A%20systematic%20literature%20review%20of%20recognition%20of%20compound%20facial%20expression%20of%20emotions%2C%20in%3A%20ICVIP%202020%3A%202020%20The%204th%20International%20Conference%20on%20Video%20and%20Image%20Processing%2C%20Association%20for%20Computing%20Machinery%2C%20New%20York%2C%20NY%2C%20USA.%20p.%20116%E2%80%93121.%20doi%3A%2010.1145%2F3447450.3447469." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0520" id="ref-id-b0520" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a></span><span class="reference" id="h0520"><div class="contribution"><div class="authors u-font-sans">V. Vapnik</div><div id="ref-id-h0520" class="title text-m">Pattern recognition using generalized portrait method</div></div><div class="host u-font-sans">Automation and remote control, 24 (1963), pp. 774-780</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Pattern%20recognition%20using%20generalized%20portrait%20method&amp;publication_year=1963&amp;author=V.%20Vapnik" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0520"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0525" id="ref-id-b0525" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a></span><span class="reference" id="h0525"><div class="contribution"><div class="authors u-font-sans">P. Viola, M. Jones, <em> et al.</em></div><div id="ref-id-h0525" class="title text-m">Rapid object detection using a boosted cascade of simple features</div></div><div class="host u-font-sans">CVPR, 1 (1) (2001), p. 3</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Rapid%20object%20detection%20using%20a%20boosted%20cascade%20of%20simple%20features&amp;publication_year=2001&amp;author=P.%20Viola&amp;author=M.%20Jones" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0525"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0530" id="ref-id-b0530" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></span><span class="reference" id="h0530"><div class="contribution"><div class="authors u-font-sans">F. Wang, J. Lv, G. Ying, S. Chen, C. Zhang</div><div id="ref-id-h0530" class="title text-m">Facial expression recognition from image based on hybrid features understanding</div></div><div class="host u-font-sans">Journal of Visual Communication and Image Representation, 59 (2019), pp. 84-88</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1117/12.2542950" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0530"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85059672417&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0530"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20from%20image%20based%20on%20hybrid%20features%20understanding&amp;publication_year=2019&amp;author=F.%20Wang&amp;author=J.%20Lv&amp;author=G.%20Ying&amp;author=S.%20Chen&amp;author=C.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0530"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0535" id="ref-id-b0535" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a></span><span class="reference" id="h0535"><div class="contribution"><div class="authors u-font-sans">H. Wang, H. Huang, Y. Hu, M. Anderson, P. Rollins, F. Makedon</div><div id="ref-id-h0535" class="title text-m">Emotion detection via discriminative kernel method</div></div><div class="host u-font-sans">Proceedings of the 3rd international conference on pervasive technologies related to assistive environments (2010), pp. 1-7</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20detection%20via%20discriminative%20kernel%20method&amp;publication_year=2010&amp;author=H.%20Wang&amp;author=H.%20Huang&amp;author=Y.%20Hu&amp;author=M.%20Anderson&amp;author=P.%20Rollins&amp;author=F.%20Makedon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0535"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0540" id="ref-id-b0540" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[108]</span></span></a></span><span class="reference" id="h0540"><div class="contribution"><div class="authors u-font-sans">L. Wang</div></div><div class="host u-font-sans" id="ref-id-h0540">Support vector machines: theory and applications, volume 177, Springer Science &amp; Business Media (2005)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Wang%2C%20L.%2C%202005.%20Support%20vector%20machines%3A%20theory%20and%20applications.%20volume%20177.%20Springer%20Science%20%26%20Business%20Media." target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0540"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0545" id="ref-id-b0545" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a></span><span class="reference" id="h0545"><div class="contribution"><div class="authors u-font-sans">X. Wang, J. Huang, J. Zhu, M. Yang, F. Yang</div><div id="ref-id-h0545" class="title text-m">Facial expression recognition with deep learning</div></div><div class="host u-font-sans">Proceedings of the 10th International Conference on Internet Multimedia Computing and Service, ACM (2018), p. 10</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/mwc.2018.8403945" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0545"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20with%20deep%20learning&amp;publication_year=2018&amp;author=X.%20Wang&amp;author=J.%20Huang&amp;author=J.%20Zhu&amp;author=M.%20Yang&amp;author=F.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0545"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0550" id="ref-id-b0550" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[110]</span></span></a></span><span class="reference" id="h0550"><div class="contribution"><div class="authors u-font-sans">Z. Wang, Q. Ruan</div><div id="ref-id-h0550" class="title text-m">Facial expression recognition based orthogonal local fisher discriminant analysis</div></div><div class="host u-font-sans">IEEE 10th International Conference on Signal Processing Proceedings, IEEE (2010), pp. 1358-1361</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/ICOSP.2010.5656884" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0550"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78651088287&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0550"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20orthogonal%20local%20fisher%20discriminant%20analysis&amp;publication_year=2010&amp;author=Z.%20Wang&amp;author=Q.%20Ruan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0550"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0555" id="ref-id-b0555" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[111]</span></span></a></span><span class="reference" id="h0555"><div class="contribution"><div class="authors u-font-sans">D. Yang, A. Alsadoon, P. Prasad, A. Singh, A. Elchouemi</div><div id="ref-id-h0555" class="title text-m">An emotion recognition model based on facial recognition in virtual learning environment</div></div><div class="host u-font-sans">Procedia Computer Science, 125 (2018), pp. 2-10</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20emotion%20recognition%20model%20based%20on%20facial%20recognition%20in%20virtual%20learning%20environment&amp;publication_year=2018&amp;author=D.%20Yang&amp;author=A.%20Alsadoon&amp;author=P.%20Prasad&amp;author=A.%20Singh&amp;author=A.%20Elchouemi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0555"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0560" id="ref-id-b0560" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[112]</span></span></a></span><span class="reference" id="h0560"><div class="contribution"><div class="authors u-font-sans">M. Yeasin, B. Bullot, R. Sharma</div><div id="ref-id-h0560" class="title text-m">Recognition of facial expressions and measurement of levels of interest from video</div></div><div class="host u-font-sans">IEEE Transactions on Multimedia, 8 (2006), pp. 500-508</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognition%20of%20facial%20expressions%20and%20measurement%20of%20levels%20of%20interest%20from%20video&amp;publication_year=2006&amp;author=M.%20Yeasin&amp;author=B.%20Bullot&amp;author=R.%20Sharma" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0560"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0565" id="ref-id-b0565" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[113]</span></span></a></span><span class="reference" id="h0565"><div class="other-ref"><span>Yi, D., Lei, Z., Liao, S., Li, S.Z., 2014. Learning face representation from scratch. arXiv preprint arXiv:1411.7923.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Yi%2C%20D.%2C%20Lei%2C%20Z.%2C%20Liao%2C%20S.%2C%20Li%2C%20S.Z.%2C%202014.%20Learning%20face%20representation%20from%20scratch.%20arXiv%20preprint%20arXiv%3A1411.7923." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0570" id="ref-id-b0570" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[114]</span></span></a></span><span class="reference" id="h0570"><div class="other-ref"><span>Yin, L., Wei, X., Sun, Y., Wang, J., Rosato, M.J., 2006. A 3d facial expression database for facial behavior research, in: 7th international conference on automatic face and gesture recognition (FGR06), IEEE. pp. 211–216.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Yin%2C%20L.%2C%20Wei%2C%20X.%2C%20Sun%2C%20Y.%2C%20Wang%2C%20J.%2C%20Rosato%2C%20M.J.%2C%202006.%20A%203d%20facial%20expression%20database%20for%20facial%20behavior%20research%2C%20in%3A%207th%20international%20conference%20on%20automatic%20face%20and%20gesture%20recognition%20(FGR06)%2C%20IEEE.%20pp.%20211%E2%80%93216." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0575" id="ref-id-b0575" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[115]</span></span></a></span><span class="reference" id="h0575"><div class="contribution"><div class="authors u-font-sans">G. Yolcu, I. Oztel, S. Kazan, C. Oz, K. Palaniappan, T.E. Lever, F. Bunyak</div><div id="ref-id-h0575" class="title text-m">Deep learning-based facial expression recognition for monitoring neurological disorders</div></div><div class="host u-font-sans">2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE (2017), pp. 1652-1657</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85045996151&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0575"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning-based%20facial%20expression%20recognition%20for%20monitoring%20neurological%20disorders&amp;publication_year=2017&amp;author=G.%20Yolcu&amp;author=I.%20Oztel&amp;author=S.%20Kazan&amp;author=C.%20Oz&amp;author=K.%20Palaniappan&amp;author=T.E.%20Lever&amp;author=F.%20Bunyak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0575"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0580" id="ref-id-b0580" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[116]</span></span></a></span><span class="reference" id="h0580"><div class="contribution"><div class="authors u-font-sans">Z. Yu, C. Zhang</div><div id="ref-id-h0580" class="title text-m">Image based static facial expression recognition with multiple deep network learning</div></div><div class="host u-font-sans">Proceedings of the 2015 ACM on international conference on multimodal interaction (2015), pp. 435-442</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1145/2818346.2830595" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0580"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959297432&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0580"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Image%20based%20static%20facial%20expression%20recognition%20with%20multiple%20deep%20network%20learning&amp;publication_year=2015&amp;author=Z.%20Yu&amp;author=C.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0580"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0585" id="ref-id-b0585" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[117]</span></span></a></span><span class="reference" id="h0585"><div class="contribution"><div class="authors u-font-sans">L.A. Zadeh</div></div><div class="host u-font-sans" id="ref-id-h0585">Fuzzy sets. Information and control, 8 (1965), pp. 338-353</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34248666540&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0585"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Zadeh%2C%20L.A.%2C%201965.%20Fuzzy%20sets.%20Information%20and%20control%208%2C%20338%E2%80%93353." target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0585"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0590" id="ref-id-b0590" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[118]</span></span></a></span><span class="reference" id="h0590"><div class="contribution"><div class="authors u-font-sans">K. Zhang, Z. Zhang, Z. Li, Y. Qiao</div><div id="ref-id-h0590" class="title text-m">Joint face detection and alignment using multitask cascaded convolutional networks</div></div><div class="host u-font-sans">IEEE Signal Processing Letters, 23 (2016), pp. 1499-1503</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021706027&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0590"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Joint%20face%20detection%20and%20alignment%20using%20multitask%20cascaded%20convolutional%20networks&amp;publication_year=2016&amp;author=K.%20Zhang&amp;author=Z.%20Zhang&amp;author=Z.%20Li&amp;author=Y.%20Qiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0590"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0595" id="ref-id-b0595" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[119]</span></span></a></span><span class="reference" id="h0595"><div class="contribution"><div class="authors u-font-sans">L. Zhang, M. Jiang, D. Farid, M.A. Hossain</div><div id="ref-id-h0595" class="title text-m">Intelligent facial emotion recognition and semantic-based topic detection for a humanoid robot</div></div><div class="host u-font-sans">Expert Systems with Applications, 40 (2013), pp. 5160-5168</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84878326261&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0595"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Intelligent%20facial%20emotion%20recognition%20and%20semantic-based%20topic%20detection%20for%20a%20humanoid%20robot&amp;publication_year=2013&amp;author=L.%20Zhang&amp;author=M.%20Jiang&amp;author=D.%20Farid&amp;author=M.A.%20Hossain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0595"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0600" id="ref-id-b0600" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a></span><span class="reference" id="h0600"><div class="contribution"><div class="authors u-font-sans">L. Zhang, K. Mistry, M. Jiang, S.C. Neoh, M.A. Hossain</div><div id="ref-id-h0600" class="title text-m">Adaptive facial point detection and emotion recognition for a humanoid robot</div></div><div class="host u-font-sans">Computer Vision and Image Understanding, 140 (2015), pp. 93-114</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84941733865&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0600"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adaptive%20facial%20point%20detection%20and%20emotion%20recognition%20for%20a%20humanoid%20robot&amp;publication_year=2015&amp;author=L.%20Zhang&amp;author=K.%20Mistry&amp;author=M.%20Jiang&amp;author=S.C.%20Neoh&amp;author=M.A.%20Hossain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0600"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0605" id="ref-id-b0605" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[121]</span></span></a></span><span class="reference" id="h0605"><div class="contribution"><div class="authors u-font-sans">S. Zhang, X. Zhao, B. Lei</div><div id="ref-id-h0605" class="title text-m">Facial expression recognition based on local binary patterns and local fisher discriminant analysis</div></div><div class="host u-font-sans">WSEAS transactions on signal processing, 8 (2012), pp. 21-31</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84861116813&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0605"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%20local%20binary%20patterns%20and%20local%20fisher%20discriminant%20analysis&amp;publication_year=2012&amp;author=S.%20Zhang&amp;author=X.%20Zhao&amp;author=B.%20Lei" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0605"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0610" id="ref-id-b0610" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[122]</span></span></a></span><span class="reference" id="h0610"><div class="contribution"><div class="authors u-font-sans">Y.D. Zhang, Z.J. Yang, H.M. Lu, X.X. Zhou, P. Phillips, Q.M. Liu, S.H. Wang</div><div id="ref-id-h0610" class="title text-m">Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and stratified cross validation</div></div><div class="host u-font-sans">IEEE Access, 4 (2016), pp. 8375-8385</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85012969638&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0610"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20emotion%20recognition%20based%20on%20biorthogonal%20wavelet%20entropy%2C%20fuzzy%20support%20vector%20machine%2C%20and%20stratified%20cross%20validation&amp;publication_year=2016&amp;author=Y.D.%20Zhang&amp;author=Z.J.%20Yang&amp;author=H.M.%20Lu&amp;author=X.X.%20Zhou&amp;author=P.%20Phillips&amp;author=Q.M.%20Liu&amp;author=S.H.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0610"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0615" id="ref-id-b0615" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></span><span class="reference" id="h0615"><div class="contribution"><div class="authors u-font-sans">Z. Zhang, L. Cui, X. Liu, T. Zhu</div><div id="ref-id-h0615" class="title text-m">Emotion detection using kinect 3d facial points</div></div><div class="host u-font-sans">2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI), IEEE (2016), pp. 407-410</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85013057570&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0615"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20detection%20using%20kinect%203d%20facial%20points&amp;publication_year=2016&amp;author=Z.%20Zhang&amp;author=L.%20Cui&amp;author=X.%20Liu&amp;author=T.%20Zhu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0615"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bb0620" id="ref-id-b0620" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a></span><span class="reference" id="h0620"><div class="contribution"><div class="authors u-font-sans">G. Zhao, X. Huang, M. Taini, S.Z. Li, M. PietikäInen</div><div id="ref-id-h0620" class="title text-m">Facial expression recognition from near-infrared videos</div></div><div class="host u-font-sans">Image and Vision Computing, 29 (2011), pp. 607-619</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052725518&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0620"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20from%20near-infrared%20videos&amp;publication_year=2011&amp;author=G.%20Zhao&amp;author=X.%20Huang&amp;author=M.%20Taini&amp;author=S.Z.%20Li&amp;author=M.%20Pietik%C3%A4Inen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-h0620"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (162)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S0957417423021942"><span class="anchor-text-container"><span class="anchor-text">Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects</span></span></a></h3><div>2024, Expert Systems with Applications</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0005">Emotion recognition has recently attracted extensive interest due to its significant applications to human–computer interaction. The expression of human emotion depends on various verbal and non-verbal languages like audio, visual, text, <em>etc</em>. Emotion recognition is thus well suited as a multimodal rather than single-modal learning problem. Owing to the powerful feature learning capability, extensive deep learning methods have been recently leveraged to capture high-level emotional feature representations for multimodal emotion recognition (MER). Therefore, this paper makes the first effort in comprehensively summarize recent advances in deep learning-based multimodal emotion recognition (DL-MER) involved in audio, visual, and text modalities. We focus on: (1) MER milestones are given to summarize the development tendency of MER, and conventional multimodal emotional datasets are provided; (2) The core principles of typical deep learning models and its recent advancements are overviewed; (3) A systematic survey and taxonomy is provided to cover the state-of-the-art methods related to two key steps in a MER system, including feature extraction and multimodal information fusion; (4) The research challenges and open issues in this field are discussed, and promising future directions are given.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S156625352300163X"><span class="anchor-text-container"><span class="anchor-text">Emotion recognition from unimodal to multimodal analysis: A review</span></span></a></h3><div>2023, Information Fusion</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="d1e1392">The omnipresence of numerous information sources in our daily life brings up new alternatives for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data, the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep learning play in this? What is multimodality? How did it progress? What are the methods of information fusion? What are the most used datasets in each modality and in multimodal recognition? We can understand and compare the various methods by answering these questions.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="/science/article/pii/S0020025522008106"><span class="anchor-text-container"><span class="anchor-text">EEG-based cross-subject emotion recognition using Fourier-Bessel series expansion based empirical wavelet transform and NCA feature selection method</span></span></a></h3><div>2022, Information Sciences</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">There are two ways, verbal and non-verbal, through which emotions can be expressed. Hence, it is required to develop an HCI system that can identify emotions by using people’s facial or vocal expressions [6,7]. But, systems that use the subject’s facial expressions are unreliable as subjects involved in the experiments can fake emotions to achieve satisfactory performance.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-2-title" aria-controls="citing-articles-article-2" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0005">Automated emotion recognition using brain electroencephalogram (EEG) signals is predominantly used for the accurate assessment of human actions as compared to facial expression or speech signals. Various signal processing methods have been used for extracting representative features from EEG signals for emotion recognition. However, the EEG signals are non-stationary and vary across the subjects as well as in different sessions of the same subject; hence it exhibits poor generalizability and low classification accuracy for an emotion classification of cross subjects. In this paper, EEG signals-based automated cross-subject emotion recognition framework is proposed using the Fourier-Bessel series expansion-based empirical wavelet transform (FBSE-EWT) method. This method is used to decompose the EEG signals from each channel into four sub-band signals. Manually ten channels are selected from the frontal lobe, from which entropy and energy features are extracted from each sub-band signal. The subject variability is reduced using an average moving filter method on each channel to obtain the smoothened feature vector of size 80. The three feature selection techniques, such as neighborhood component analysis (NCA), relief-F, and mRMR, are used to obtain an optimal feature vector. The machine learning models, such as artificial neural network (ANN), <em>k</em>-nearest neighborhood (<em>k</em>-NN) with two (fine and weighted) functions, and ensemble bagged tree classifiers are trained by the obtained feature vectors. The experiments are performed on two publicly accessible databases, named SJTU emotion EEG dataset (SEED) and dataset for emotion analysis using physiological signals (DEAP). The training and testing of the models have been performed using 10-fold cross-validation and leave-one-subject-out-cross-validation (LOSOCV). The proposed framework based on FBSE-EWT and NCA feature selection approach shows superior results for classifying human emotions compared to other state-of-art emotion classification models.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="/science/article/pii/S0020025522006867"><span class="anchor-text-container"><span class="anchor-text">HistNet: Histogram-based convolutional neural network with Chi-squared deep metric learning for facial expression recognition</span></span></a></h3><div>2022, Information Sciences</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">In [14] (our previous work), a histogram distance metric learning has been proposed for facial expression recognition using histogram-based hand-crafted features such as LBP; however, in this study, a new CNN is proposed for facial expression recognition which learns histogram feature extraction and classification steps simultaneously. In the past few years, several deep learning based methods have been proposed for facial expression recognition [3,15–33]. In this regard, Yang et al. [16] employed the encoder of a generative adversarial network to separate expression related features from identity-related ones in the input image followed by performing facial expression recognition on the expression-related features using a CNN-based classifier.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-3-title" aria-controls="citing-articles-article-3" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0010">Facial expression recognition is a challenging problem in machine learning. There is much research has been conducted in this field; however, the accuracy of facial expression recognition, especially in uncontrolled conditions, needs much improvement. In this paper, a deep histogram metric learning in a Convolutional Neural Network (CNN) is presented for facial expression recognition. The proposed CNN utilizes a histogram calculation layer to provide statistical description of feature maps at the output of the convolutional layers. To train the proposed CNN in histogram space, a learnable matrix (equivalent to the fully connected layer) is introduced in chi-squared distance equation. Then, the modified equation is used in the loss function. The recognition rates of the proposed CNN for seven-class facial expression recognition on four well-known databases including CK+, MMI, SFEW, and RAF-DB are 98.47%, 83.41%, 61.01%, and 89.28%, respectively. The results show superiority of the proposed CNN compared to the state-of-the-art methods.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.3390/s23052455" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Emotion Recognition Using Different Sensors, Emotion Models, Methods and Datasets: A Comprehensive Review</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, Sensors</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.3390/s23031080" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Masked Face Emotion Recognition Based on Facial Landmarks and Deep Learning Approaches for Visually Impaired People</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, Sensors</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-85117617494&amp;md5=48945eb3efbaf19d9adaadc3f68977c6" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><div class="Footnotes text-xs"><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np005">Approaches based on image processing, pattern recognition, or classical artificial intelligence methods, whose task of feature selection is designed beforehand by human experts aiming to extract a given set of chosen features to train and build a recognizer (also said hand-crafted features), in contrast to CNN’s whose features are learned from data using generic feature extractors <a class="anchor anchor-primary" href="#b0345" name="bb0345" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="b0345"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a>.</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np010">A worldwide portal for scientific knowledge access, managed by the Brazilian Ministry of Education provided for the authorized institutions, including universities and research institutes, government agencies and private companies.</div></dd></dl><dl class="footnote"><dt class="footnote-label"><a class="anchor u-padding-s-hor u-padding-xs u-display-inline-block anchor-primary" href="#bfn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a></dt><dd class="footnote-detail u-padding-xs-top"><div class="u-margin-s-bottom" id="np015">Source: <a class="anchor anchor-primary" href="https://infograph.venngage.com/p/8769/microexpressions" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://infograph.venngage.com/p/8769/microexpressions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></dd></dl></div><a class="anchor abstract-link anchor-primary" href="/science/article/abs/pii/S0020025521010136"><span class="anchor-text-container"><span class="anchor-text">View Abstract</span></span></a><div class="Copyright"><span class="copyright-line">© 2021 Elsevier Inc. All rights reserved.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1084804519302759" title="A survey of emotion recognition methods with emphasis on E-Learning environments"><span class="anchor-text-container"><span class="anchor-text"><span>A survey of emotion recognition methods with emphasis on E-Learning environments</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of Network and Computer Applications, Volume 147, 2019, Article 102423</div></div><div class="authors"><span>Maryam</span> <span>Imani</span>, <span>Gholam Ali</span> <span>Montazer</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1084804519302759/pdfft?md5=4a3cd2b14683ea3acae57745f9120a68&amp;pid=1-s2.0-S1084804519302759-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article0-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1047320321002637" title="Cross-dataset emotion recognition from facial expressions through convolutional neural networks"><span class="anchor-text-container"><span class="anchor-text"><span>Cross-dataset emotion recognition from facial expressions through convolutional neural networks</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of Visual Communication and Image Representation, Volume 82, 2022, Article 103395</div></div><div class="authors"><span>William</span> <span>Dias</span>, …, <span>Anderson</span> <span>Rocha</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1047320321002637/pdfft?md5=e6e8fc1b18052b37dfc9e2e59405bcf1&amp;pid=1-s2.0-S1047320321002637-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article1-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S2666307421000073" title="Facial expression recognition via ResNet-50"><span class="anchor-text-container"><span class="anchor-text"><span>Facial expression recognition via ResNet-50</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">International Journal of Cognitive Computing in Engineering, Volume 2, 2021, pp. 57-64</div></div><div class="authors"><span>Bin</span> <span>Li</span>, <span>Dimas</span> <span>Lima</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S2666307421000073/pdfft?md5=bcef09c066ceab591c8c0451646277e9&amp;pid=1-s2.0-S2666307421000073-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S235291482030201X" title="Development of a Real-Time Emotion Recognition System Using Facial Expressions and EEG based on machine learning and deep neural network methods"><span class="anchor-text-container"><span class="anchor-text"><span>Development of a Real-Time Emotion Recognition System Using Facial Expressions and EEG based on machine learning and deep neural network methods</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Informatics in Medicine Unlocked, Volume 20, 2020, Article 100372</div></div><div class="authors"><span>Aya</span> <span>Hassouneh</span>, …, <span>M.</span> <span>Murugappan</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S235291482030201X/pdfft?md5=28659fb2bdfa93bde11ec8eab6135123&amp;pid=1-s2.0-S235291482030201X-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article3-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0031320316000534" title="Boosted NNE collections for multicultural facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Boosted NNE collections for multicultural facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition, Volume 55, 2016, pp. 14-27</div></div><div class="authors"><span>Ghulam</span> <span>Ali</span>, …, <span>Tae-Sun</span> <span>Choi</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320316000534/pdfft?md5=288b0abf1f3aef5d496e7ac31e432bd2&amp;pid=1-s2.0-S0031320316000534-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article4-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S016503272100639X" title="Facial emotion recognition in major depressive disorder: A meta-analytic review"><span class="anchor-text-container"><span class="anchor-text"><span>Facial emotion recognition in major depressive disorder: A meta-analytic review</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of Affective Disorders, Volume 293, 2021, pp. 320-328</div></div><div class="authors"><span>Fernando C.</span> <span>Krause</span>, …, <span>Michael T.</span> <span>Moore</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S016503272100639X/pdfft?md5=8185b68ce6ce84b422516e20115018d2&amp;pid=1-s2.0-S016503272100639X-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article5-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plum-sciencedirect-theme"><div class="PlumX-Summary"><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-inverse-logo.png" class="plx-logo"></div><div class="pps-cols"><div class="pps-col plx-citation"><div class="plx-citation"><div class="pps-title">Citations</div><ul><li class="plx-citation"><span class="pps-label">Citation Indexes: </span><span class="pps-count">161</span></li><li class="plx-citation"><span class="pps-label">Policy Citations: </span><span class="pps-count">1</span></li></ul></div></div><div class="pps-col plx-capture"><div class="plx-capture"><div class="pps-title">Captures</div><ul><li class="plx-capture"><span class="pps-label">Readers: </span><span class="pps-count">205</span></li></ul></div></div><div class="pps-col plx-socialMedia"><div class="plx-socialMedia"><div class="pps-title">Social Media</div><ul><li class="plx-socialMedia"><span class="pps-label">Shares, Likes &amp; Comments: </span><span class="pps-count">7</span></li></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plx-logo"></div><a target="_blank" href="https://plu.mx/plum/a/?doi=10.1016/j.ins.2021.10.005&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class="pps-seemore" title="PlumX Metrics Detail Page">View details<svg fill="currentColor" tabindex="-1" focusable="false" width="16" height="16" viewBox="0 0 16 16" class="svg-arrow"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0020025521010136" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLnNFKEhra3YKVSC1wYSW9_0*" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright © <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
      <div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
      
      <script async="" src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="text/javascript"></script>
      
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S0020025521010136","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0020-0255","volumeNumber":"582","suppl":"C","provider":"elsevier","entitlementType":"package"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1730203840153,"loadTime":""},"visitor":{"accessType":"ae:REG_SHIBBOLETH","accountId":"ae:50401","accountName":"ae:IT University of Copenhagen","loginStatus":"logged in","userId":"ae:71970787","ipAddress":"130.225.244.206","appSessionId":"d803021f-cf0c-4303-a9ac-edd1111de6ef"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
      <script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
      <script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/arp.js" type="text/javascript"></script>
      <script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:REG_SHIBBOLETH","countryCode":"DK"},"account":{"id":"ae:50401","name":"ae:IT University of Copenhagen"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
      <span id="pendo-answer-rating"></span>
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
      <script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
      <script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
      <script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'false' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>
    
  <script>
var pageDataTracker = {
    eventCookieName: 'eventTrack',
    debugCookie: 'els-aa-debugmode',
    debugCounter: 1,
    warnings: [],
    measures: {},
    timeoffset: 0

    ,trackPageLoad: function(data) {
        if (window.pageData && ((pageData.page && pageData.page.noTracking == 'true') || window.pageData_isLoaded)) {
            return false;
        }

        this.updatePageData(data);

        this.initWarnings();
        if(!(window.pageData && pageData.page && pageData.page.name)) {
            console.error('pageDataTracker.trackPageLoad() called without pageData.page.name being defined!');
            return;
        }

        this.processIdPlusData(window.pageData);

        if(window.pageData && pageData.page && !pageData.page.loadTime) {
          pageData.page.loadTime = performance ? Math.round((performance.now())).toString() : '';
        }

        if(window.pageData && pageData.page) {
            var localTime = new Date().getTime();
            if(pageData.page.loadTimestamp) {
                // calculate timeoffset
                var serverTime = parseInt(pageData.page.loadTimestamp);
                if(!isNaN(serverTime)) {
                    this.timeoffset = pageData.page.loadTimestamp - localTime;
                }
            } else {
                pageData.page.loadTimestamp = localTime;
            }
        }

        this.validateData(window.pageData);

        try {
            var cookieTest = 'aa-cookie-test';
            this.setCookie(cookieTest, cookieTest);
            if(this.getCookie(cookieTest) != cookieTest) {
                this.warnings.push('dtm5');
            }
            this.deleteCookie(cookieTest);
        } catch(e){
            this.warnings.push('dtm5');
        }

        this.registerCallbacks();
        this.setAnalyticsData();

        // handle any cookied event data
        this.getEvents();

        window.pageData_isLoaded = true;

        this.debugMessage('Init - trackPageLoad()', window.pageData);

        _satellite.track('eventDispatcher', JSON.stringify({
          eventName: 'newPage',
          eventData: {eventName: 'newPage'},
          pageData: window.pageData
        }));
    }

    ,trackEvent: function(event, data, callback) {
        if (window.pageData && pageData.page && pageData.page.noTracking == 'true') {
            return false;
        }
        
        if(!window.pageData_isLoaded) {
            if(this.isDebugEnabled()) {
                console.log('[AA] pageDataTracker.trackEvent() called without calling trackPageLoad() first.');
            }
            return false;
        }

        if (event) {
            this.initWarnings();
            if(event === 'newPage') {
                // auto fillings
                if(data && data.page && !data.page.loadTimestamp) {
                    data.page.loadTimestamp = ''+(new Date().getTime() + this.timeoffset);
                }
                this.processIdPlusData(data);
            }

            window.eventData = data ? data : {};
            window.eventData.eventName = event;
            if(!_satellite.getVar('blacklisted')) {
                this.handleEventData(event, data);
    
                if(event === 'newPage') {
                    this.validateData(window.pageData);
                }
                this.debugMessage('Event: ' + event, data);
    
                _satellite.track('eventDispatcher', JSON.stringify({
                  eventName: event,
                  eventData: window.eventData,
                  pageData: window.pageData
                }));
            } else {
                this.debugMessage('!! Blocked Event: ' + event, data);
            }
        }

        if (typeof(callback) == 'function') {
            callback.call();
        }
    }

    ,processIdPlusData: function(data) {
        if(data && data.visitor && data.visitor.idPlusData) {
            var idPlusFields = ['userId', 'accessType', 'accountId', 'accountName'];
            for(var i=0; i < idPlusFields.length; i++) {
                if(typeof data.visitor.idPlusData[idPlusFields[i]] !== 'undefined') {
                    data.visitor[idPlusFields[i]] = data.visitor.idPlusData[idPlusFields[i]];
                }
            }
            data.visitor.idPlusData = undefined;
        }
    }

    ,validateData: function(data) {
        if(!data) {
            this.warnings.push('dv0');
            return;
        }

        // top 5
        if(!(data.visitor && data.visitor.accessType)) {
            this.warnings.push('dv1');
        }
        if(data.visitor && (data.visitor.accountId || data.visitor.accountName)) {
            if(!data.visitor.accountName) {
                this.warnings.push('dv2');
            }
            if(!data.visitor.accountId) {
                this.warnings.push('dv3');
            }
        }
        if(!(data.page && data.page.productName)) {
            this.warnings.push('dv4');
        }
        if(!(data.page && data.page.businessUnit)) {
            this.warnings.push('dv5');
        }
        if(!(data.page && data.page.name)) {
            this.warnings.push('dv6');
        }

        // rp mandatory
        if(data.page && data.page.businessUnit && (data.page.businessUnit.toLowerCase().indexOf('els:rp:') !== -1 || data.page.businessUnit.toLowerCase().indexOf('els:rap:') !== -1)) {
            if(!(data.page && data.page.loadTimestamp)) {
                this.warnings.push('dv7');
            }
            if(!(data.page && data.page.loadTime)) {
                this.warnings.push('dv8');
            }
            if(!(data.visitor && data.visitor.ipAddress)) {
                this.warnings.push('dv9');
            }
            if(!(data.page && data.page.type)) {
                this.warnings.push('dv10');
            }
            if(!(data.page && data.page.language)) {
                this.warnings.push('dv11');
            }
        }

        // other
        if(data.page && data.page.environment) {
            var env = data.page.environment.toLowerCase();
            if(!(env === 'dev' || env === 'cert' || env === 'prod')) {
                this.warnings.push('dv12');
            }
        }
        if(data.content && data.content.constructor !== Array) {
            this.warnings.push('dv13');
        }

        if(data.visitor && data.visitor.accountId && data.visitor.accountId.indexOf(':') == -1) {
            this.warnings.push('dv14');
            data.visitor.accountId = "data violation"
        }
    }

    ,initWarnings: function() {
        this.warnings = [];
        try {
            var hdn = document.head.childNodes;
            var libf = false;
            for(var i=0; i<hdn.length; i++) {
                if(hdn[i].src && (hdn[i].src.indexOf('satelliteLib') !== -1 || hdn[i].src.indexOf('launch') !== -1)) {
                    libf = true;
                    break;
                }
            }
            if(!libf) {
                this.warnings.push('dtm1');
            }
        } catch(e) {}

        try {
            for (let element of document.querySelectorAll('*')) {
                // Check if the element has a src attribute and if it meets the criteria
                if (element.src && element.src.includes('assets.adobedtm.com') && element.src.includes('launch')) {
                    if (element.tagName.toLowerCase() !== 'script') {
                        this.warnings.push('dtm5');
                    }
                    if (!element.hasAttribute('async')) {
                        this.warnings.push('dtm4');
                    }
                }
            }
        } catch (e) { }
    }

    ,getMessages: function() {
        return ['v1'].concat(this.warnings).join('|');
    }
    ,addMessage: function(message) {
        this.warnings.push(message);
    }

    ,getPerformance: function() {
        var copy = {};
        for (var attr in this.measures) {
            if(this.measures.hasOwnProperty(attr)) {
                copy[attr] = this.measures[attr];
            }
        }

        this.measures = {};
        return copy;
    }

    ,dtmCodeDesc: {
        dtm1: 'satellite-lib must be placed in the <head> section',
        dtm2: 'trackPageLoad() must be placed and called before the closing </body> tag',
        dtm3: 'trackEvent() must be called at a stage where Document.readyState=complete (e.g. on the load event or a user event)',
        dtm4: 'Embed codes need to be loaded in async mode',
        dtm5: 'Embed codes not in type script',
        dv1: 'visitor.accessType not set but mandatory',
        dv2: 'visitor.accountName not set but mandatory',
        dv3: 'visitor.accountId not set but mandatory',
        dv4: 'page.productName not set but mandatory',
        dv5: 'page.businessUnit not set but mandatory',
        dv6: 'page.name not set but mandatory',
        dv7: 'page.loadTimestamp not set but mandatory',
        dv8: 'page.loadTime not set but mandatory',
        dv9: 'visitor.ipAddress not set but mandatory',
        dv10: 'page.type not set but mandatory',
        dv11: 'page.language not set but mandatory',
        dv12: 'page.environment must be set to \'prod\', \'cert\' or \'dev\'',
        dv13: 'content must be of type array of objects',
        dv14: 'account number must contain at least one \':\', e.g. \'ae:12345\''
    }

    ,debugMessage: function(event, data) {
        if(this.isDebugEnabled()) {
            console.log('[AA] --------- [' + (this.debugCounter++) + '] Web Analytics Data ---------');
            console.log('[AA] ' + event);
            console.groupCollapsed("[AA] AA Data: ");
            if(window.eventData) {
                console.log("[AA] eventData:\n" + JSON.stringify(window.eventData, true, 2));
            }
            if(window.pageData) {
                console.log("[AA] pageData:\n" + JSON.stringify(window.pageData, true, 2));
            }
            console.groupEnd();
            if(this.warnings.length > 0) {
                console.groupCollapsed("[AA] Warnings ("+this.warnings.length+"): ");
                for(var i=0; i<this.warnings.length; i++) {
                    var error = this.dtmCodeDesc[this.warnings[i]] ? this.dtmCodeDesc[this.warnings[i]] : 'Error Code: ' + this.warnings[i];
                    console.log('[AA] ' + error);
                }
                console.log('[AA] More can be found here: https://confluence.cbsels.com/display/AA/AA+Error+Catalog');
                console.groupEnd();
            }
            console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        }
    }

    ,getTrackingCode: function() {
      var campaign = _satellite.getVar('Campaign - ID');
      if(!campaign) {
        campaign = window.sessionStorage ? sessionStorage.getItem('dgcid') : '';
      }
      return campaign;
    }

    ,isDebugEnabled: function() {
        if(typeof this.debug === 'undefined') {
            this.debug = (document.cookie.indexOf(this.debugCookie) !== -1) || (window.pageData && pageData.page && pageData.page.environment && pageData.page.environment.toLowerCase() === 'dev');
            //this.debug = (document.cookie.indexOf(this.debugCookie) !== -1);
        }
        return this.debug;
    }

    ,enableDebug: function(expire) {
        if (typeof expire === 'undefined') {
            expire = 86400;
        }
        console.log('You just enabled debug mode for Adobe Analytics tracking. This mode will persist for 24h.');
        console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        this.setCookie(this.debugCookie, 'true', expire, document.location.hostname);
        this.debug = true;
    }

    ,disableDebug: function() {
        console.log('Debug mode is now disabled.');
        this.deleteCookie(this.debugCookie);
        this.debug = false;
    }

    ,setAnalyticsData: function() {
        if(!(window.pageData && pageData.page && pageData.page.productName && pageData.page.name)) {
            return;
        }
        pageData.page.analyticsPagename = pageData.page.productName + ':' + pageData.page.name;

        var pageEls = pageData.page.name.indexOf(':') > -1 ? pageData.page.name.split(':') : [pageData.page.name];
        pageData.page.sectionName = pageData.page.productName + ':' + pageEls[0];
    }

    ,getEvents: function() {
        pageData.savedEvents = {};
        pageData.eventList = [];

        var val = this.getCookie(this.eventCookieName);
        if (val) {
            pageData.savedEvents = val;
        }

        this.deleteCookie(this.eventCookieName);
    }

    ,updatePageData(data) {
        window.pageData = window.pageData || {};
        if (data && typeof(data) === 'object') {
            for (var x in data) {
                if(data.hasOwnProperty(x) && data[x] instanceof Array) {
                    pageData[x] = data[x];
                } else if(data.hasOwnProperty(x) && typeof(data[x]) === 'object') {
                    if(!pageData[x]) {
                        pageData[x] = {};
                    }
                    for (var y in data[x]) {
                        if(data[x].hasOwnProperty(y)) {
                            pageData[x][y] = data[x][y];
                        }
                    }
                }
            }
        }
    }

    ,handleEventData: function(event, data) {
        var val;
        switch(event) {
            case 'newPage':
                this.updatePageData(data);
                this.setAnalyticsData();
            case 'saveSearch':
            case 'searchResultsUpdated':
                if (data) {
                    // overwrite page-load object
                    if (data.search && typeof(data.search) == 'object') {
                        window.eventData.search.resultsPosition = '';
                        pageData.search = pageData.search || {};
                        var fields = ['advancedCriteria', 'criteria', 'currentPage', 'dataFormCriteria', 'facets', 'resultsByType', 'resultsPerPage', 'sortType', 'totalResults', 'type', 'database',
                        'suggestedClickPosition','suggestedLetterCount','suggestedResultCount', 'autoSuggestCategory', 'autoSuggestDetails','typedTerm','selectedTerm', 'channel',
                        'facetOperation', 'details'];
                        for (var i=0; i<fields.length; i++) {
                            if (data.search[fields[i]]) {
                                pageData.search[fields[i]] = data.search[fields[i]];
                            }
                        }
                    }
                }
                this.setAnalyticsData();
                break;
            case 'navigationClick':
                if (data && data.link) {
                    window.eventData.navigationLink = {
                        name: ((data.link.location || 'no location') + ':' + (data.link.name || 'no name'))
                    };
                }
                break;
            case 'autoSuggestClick':
                if (data && data.search) {
                    val = {
                        autoSuggestSearchData: (
                            'letterct:' + (data.search.suggestedLetterCount || 'none') +
                            '|resultct:' + (data.search.suggestedResultCount || 'none') +
                            '|clickpos:' + (data.search.suggestedClickPosition || 'none')
                        ).toLowerCase(),
                        autoSuggestSearchTerm: (data.search.typedTerm || ''),
                        autoSuggestTypedTerm: (data.search.typedTerm || ''),
                        autoSuggestSelectedTerm: (data.search.selectedTerm || ''),
                        autoSuggestCategory: (data.search.autoSuggestCategory || ''),
                        autoSuggestDetails: (data.search.autoSuggestDetails || '')
                    };
                }
                break;
            case 'linkOut':
                if (data && data.content && data.content.length > 0) {
                    window.eventData.linkOut = data.content[0].linkOut;
                    window.eventData.referringProduct = _satellite.getVar('Page - Product Name') + ':' + data.content[0].id;
                }
                break;
            case 'socialShare':
                if (data && data.social) {
                    window.eventData.sharePlatform = data.social.sharePlatform || '';
                }
                break;
            case 'contentInteraction':
                if (data && data.action) {
                    window.eventData.action.name = pageData.page.productName + ':' + data.action.name;
                }
                break;
            case 'searchWithinContent':
                if (data && data.search) {
                    window.pageData.search = window.pageData.search || {};
                    pageData.search.withinContentCriteria = data.search.withinContentCriteria;
                }
                break;
            case 'contentShare':
                if (data && data.content) {
                    window.eventData.sharePlatform = data.content[0].sharePlatform;
                }
                break;
            case 'contentLinkClick':
                if (data && data.link) {
                    window.eventData.action = { name: pageData.page.productName + ':' + (data.link.type || 'no link type') + ':' + (data.link.name || 'no link name') };
                }
                break;
            case 'contentWindowLoad':
            case 'contentTabClick':
                if (data && data.content) {
                    window.eventData.tabName = data.content[0].tabName || '';
                    window.eventData.windowName = data.content[0].windowName || '';
                }
                break;
            case 'userProfileUpdate':
                if (data && data.user) {
                    if (Object.prototype.toString.call(data.user) === "[object Array]") {
                        window.eventData.user = data.user[0];
                    }
                }
                break;
            case 'videoStart':
                if (data.video) {
                    data.video.length = parseFloat(data.video.length || '0');
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.open(data.video.id, data.video.length, s.Media.playerName);
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoPlay':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoStop':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                }
                break;
            case 'videoComplete':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                    s.Media.close(data.video.id);
                }
                break;
            case 'addWebsiteExtension':
                if(data && data.page) {
                    val = {
                        wx: data.page.websiteExtension
                    }
                }
                break;
        }

        if (val) {
            this.setCookie(this.eventCookieName, val);
        }
    }

    ,registerCallbacks: function() {
        var self = this;
        if(window.usabilla_live) {
            window.usabilla_live('setEventCallback', function(category, action, label, value) {
                if(action == 'Campaign:Open') {
                    self.trackEvent('ctaImpression', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                } else if(action == 'Campaign:Success') {
                    self.trackEvent('ctaClick', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                }
            });
        }
    }

    ,getConsortiumAccountId: function() {
        var id = '';
        if (window.pageData && pageData.visitor && (pageData.visitor.consortiumId || pageData.visitor.accountId)) {
            id = (pageData.visitor.consortiumId || 'no consortium ID') + '|' + (pageData.visitor.accountId || 'no account ID');
        }

        return id;
    }

    ,getSearchClickPosition: function() {
        if (window.eventData && eventData.search && eventData.search.resultsPosition) {
            var pos = parseInt(eventData.search.resultsPosition), clickPos;
            if (!isNaN(pos)) {
                var page = pageData.search.currentPage ? parseInt(pageData.search.currentPage) : '', perPage = pageData.search.resultsPerPage ? parseInt(pageData.search.resultsPerPage) : '';
                if (!isNaN(page) && !isNaN(perPage)) {
                    clickPos = pos + ((page - 1) * perPage);
                }
            }
            return clickPos ? clickPos.toString() : eventData.search.resultsPosition;
        }
        return '';
    }

    ,getSearchFacets: function() {
        var facetList = '';
        if (window.pageData && pageData.search && pageData.search.facets) {
            if (typeof(pageData.search.facets) == 'object') {
                for (var i=0; i<pageData.search.facets.length; i++) {
                    var f = pageData.search.facets[i];
                    facetList += (facetList ? '|' : '') + f.name + '=' + f.values.join('^');
                }
            }
        }
        return facetList;
    }

    ,getSearchResultsByType: function() {
        var resultTypes = '';
        if (window.pageData && pageData.search && pageData.search.resultsByType) {
            for (var i=0; i<pageData.search.resultsByType.length; i++) {
                var r = pageData.search.resultsByType[i];
                resultTypes += (resultTypes ? '|' : '') + r.name + (r.results || r.values ? '=' + (r.results || r.values) : '');
            }
        }
        return resultTypes;
    }

    ,getJournalInfo: function() {
        var info = '';
        if (window.pageData && pageData.journal && (pageData.journal.name || pageData.journal.specialty || pageData.journal.section || pageData.journal.issn || pageData.journal.issueNumber || pageData.journal.volumeNumber || pageData.journal.family || pageData.journal.publisher)) {
            var journal = pageData.journal;
            info = (journal.name || 'no name') 
            + '|' + (journal.specialty || 'no specialty') 
            + '|' + (journal.section || 'no section') 
            + '|' + (journal.issn || 'no issn') 
            + '|' + (journal.issueNumber || 'no issue #') 
            + '|' + (journal.volumeNumber || 'no volume #')
            + '|' + (journal.family || 'no family')
            + '|' + (journal.publisher || 'no publisher');

        }
        return info;
    }

    ,getBibliographicInfo: function(doc) {
        if (!doc || !(doc.publisher || doc.indexTerms || doc.publicationType || doc.publicationRights || doc.volumeNumber || doc.issueNumber || doc.subjectAreas || doc.isbn)) {
            return '';
        }

        var terms = doc.indexTerms ? doc.indexTerms.split('+') : '';
        if (terms) {
            terms = terms.slice(0, 5).join('+');
            terms = terms.length > 100 ? terms.substring(0, 100) : terms;
        }

        var areas = doc.subjectAreas ? doc.subjectAreas.split('>') : '';
        if (areas) {
            areas = areas.slice(0, 5).join('>');
            areas = areas.length > 100 ? areas.substring(0, 100) : areas;
        }

        var biblio	= (doc.publisher || 'none')
            + '^' + (doc.publicationType || 'none')
            + '^' + (doc.publicationRights || 'none')
            + '^' + (terms || 'none')
            + '^' + (doc.volumeNumber || 'none')
            + '^' + (doc.issueNumber || 'none')
            + '^' + (areas || 'none')
            + '^' + (doc.isbn || 'none');

        return this.stripProductDelimiters(biblio).toLowerCase();
    }

    ,getContentItem: function() {
        var docs = window.eventData && eventData.content ? eventData.content : pageData.content;
        if (docs && docs.length > 0) {
            return docs[0];
        }
    }

    ,getFormattedDate: function(ts) {
        if (!ts) {
            return '';
        }

        var d = new Date(parseInt(ts) * 1000);

        // now do formatting
        var year = d.getFullYear()
            ,month = ((d.getMonth() + 1) < 10 ? '0' : '') + (d.getMonth() + 1)
            ,date = (d.getDate() < 10 ? '0' : '') + d.getDate()
            ,hours = d.getHours() > 12 ? d.getHours() - 12 : d.getHours()
            ,mins = (d.getMinutes() < 10 ? '0' : '') + d.getMinutes()
            ,ampm = d.getHours() > 12 ? 'pm' : 'am';

        hours = (hours < 10 ? '0' : '') + hours;
        return year + '-' + month + '-' + date;
    }

    ,getVisitorId: function() {
        var orgId = '4D6368F454EC41940A4C98A6@AdobeOrg';
        if(Visitor && Visitor.getInstance(orgId)) {
            return Visitor.getInstance(orgId).getMarketingCloudVisitorID();
        } else {
            return ''
        }
    }

    ,setProductsVariable: function() {
        var prodList = window.eventData && eventData.content ? eventData.content : pageData.content
            ,prods = [];
        if (prodList) {
            for (var i=0; i<prodList.length; i++) {
                if (prodList[i].id || prodList[i].type || prodList[i].publishDate || prodList[i].onlineDate) {
                    if (!prodList[i].id) {
                        prodList[i].id = 'no id';
                    }
                    var prodName = (pageData.page.productName || 'xx').toLowerCase();
                    if (prodList[i].id.indexOf(prodName + ':') != 0) {
                        prodList[i].id = prodName + ':' + prodList[i].id;
                    }
                    prodList[i].id = this.stripProductDelimiters(prodList[i].id);
                    var merch = [];
                    if (prodList[i].format) {
                        merch.push('evar17=' + this.stripProductDelimiters(prodList[i].format.toLowerCase()));
                    }
                    if (prodList[i].type) {
                        var type = prodList[i].type;
                        if (prodList[i].accessType) {
                            type += ':' + prodList[i].accessType;
                        }
                        merch.push('evar20=' + this.stripProductDelimiters(type.toLowerCase()));

                        if(type.indexOf(':manuscript') > 0) {
                            /*
                            var regex = /[a-z]+:manuscript:id:([a-z]+-[a-z]-[0-9]+-[0-9]+)/gmi;
                            var m = regex.exec(prodList[i].id);
                            if(m) {
                                merch.push('evar200=' + m[1]);
                            }
                            merch.push('evar200=' + prodList[i].id);
                            */
                            a = prodList[i].id.lastIndexOf(':');
                            if(a>0) {
                                merch.push('evar200=' + prodList[i].id.substring(a+1).toUpperCase());
                            }
                        } else if(type.indexOf(':submission') > 0) {
                            merch.push('evar200=' + prodList[i].id);
                        }
                    }
                    if(!prodList[i].title) {
                        prodList[i].title = prodList[i].name;
                    }
                    if (prodList[i].title) {
                        merch.push('evar75=' + this.stripProductDelimiters(prodList[i].title.toLowerCase()));
                    }
                    if (prodList[i].breadcrumb) {
                        merch.push('evar63=' + this.stripProductDelimiters(prodList[i].breadcrumb).toLowerCase());
                    }
                    var nowTs = new Date().getTime()/1000;
                    if (prodList[i].onlineDate && !isNaN(prodList[i].onlineDate)) {
                        if(prodList[i].onlineDate > 32503680000) {
                            prodList[i].onlineDate = prodList[i].onlineDate/1000;
                        }
                        merch.push('evar122=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate)));
                        var onlineAge = Math.floor((nowTs - prodList[i].onlineDate) / 86400);
                        onlineAge = (onlineAge === 0) ? 'zero' : onlineAge;
                        merch.push('evar128=' + onlineAge);
                    }
                    if (prodList[i].publishDate && !isNaN(prodList[i].publishDate)) {
                        if(prodList[i].publishDate > 32503680000) {
                            prodList[i].publishDate = prodList[i].publishDate/1000;
                        }
                        merch.push('evar123=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                        var publishAge = Math.floor((nowTs - prodList[i].publishDate) / 86400);
                        publishAge = (publishAge === 0) ? 'zero' : publishAge;
                        merch.push('evar127=' + publishAge);
                    }
                    if (prodList[i].onlineDate && prodList[i].publishDate) {
                        merch.push('evar38=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate) + '^' + pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                    }
                    if (prodList[i].mapId) {
                        merch.push('evar70=' + this.stripProductDelimiters(prodList[i].mapId));
                    }
					if (prodList[i].relevancyScore) {
						merch.push('evar71=' + this.stripProductDelimiters(prodList[i].relevancyScore));
					}
                    if (prodList[i].status) {
                        merch.push('evar73=' + this.stripProductDelimiters(prodList[i].status));
                    }
                    if (prodList[i].previousStatus) {
                        merch.push('evar111=' + this.stripProductDelimiters(prodList[i].previousStatus));
                    }
                    if (prodList[i].entitlementType) {
                        merch.push('evar80=' + this.stripProductDelimiters(prodList[i].entitlementType));
                    }
                    if (prodList[i].recordType) {
                        merch.push('evar93=' + this.stripProductDelimiters(prodList[i].recordType));
                    }
                    if (prodList[i].exportType) {
                        merch.push('evar99=' + this.stripProductDelimiters(prodList[i].exportType));
                    }
                    if (prodList[i].importType) {
                        merch.push('evar142=' + this.stripProductDelimiters(prodList[i].importType));
                    }
                    if (prodList[i].section) {
                        merch.push('evar100=' + this.stripProductDelimiters(prodList[i].section));
                    }
                    if (prodList[i].detail) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].detail.toLowerCase()));
                    } else if(prodList[i].details) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].details.toLowerCase()));
                    }
                    if (prodList[i].position) {
                        merch.push('evar116=' + this.stripProductDelimiters(prodList[i].position));
                    }
                    if (prodList[i].publicationTitle) {
                        merch.push('evar129=' + this.stripProductDelimiters(prodList[i].publicationTitle));
                    }
                    if (prodList[i].specialIssueTitle) {
                        merch.push('evar130=' + this.stripProductDelimiters(prodList[i].specialIssueTitle));
                    }
                    if (prodList[i].specialIssueNumber) {
                        merch.push('evar131=' + this.stripProductDelimiters(prodList[i].specialIssueNumber));
                    }
                    if (prodList[i].referenceModuleTitle) {
                        merch.push('evar139=' + this.stripProductDelimiters(prodList[i].referenceModuleTitle));
                    }
                    if (prodList[i].referenceModuleISBN) {
                        merch.push('evar140=' + this.stripProductDelimiters(prodList[i].referenceModuleISBN));
                    }
                    if (prodList[i].volumeTitle) {
                        merch.push('evar132=' + this.stripProductDelimiters(prodList[i].volumeTitle));
                    }
                    if (prodList[i].publicationSection) {
                        merch.push('evar133=' + this.stripProductDelimiters(prodList[i].publicationSection));
                    }
                    if (prodList[i].publicationSpecialty) {
                        merch.push('evar134=' + this.stripProductDelimiters(prodList[i].publicationSpecialty));
                    }
                    if (prodList[i].issn) {
                        merch.push('evar135=' + this.stripProductDelimiters(prodList[i].issn));
                    }
                    if (prodList[i].id2) {
                        merch.push('evar159=' + this.stripProductDelimiters(prodList[i].id2));
                    }
                    if (prodList[i].id3) {
                        merch.push('evar160=' + this.stripProductDelimiters(prodList[i].id3));
                    }
                    if (prodList[i].provider) {
                        merch.push('evar164=' + this.stripProductDelimiters(prodList[i].provider));
                    }
                    if (prodList[i].citationStyle) {
                        merch.push('evar170=' + this.stripProductDelimiters(prodList[i].citationStyle));
                    }

                    var biblio = this.getBibliographicInfo(prodList[i]);
                    if (biblio) {
                        merch.push('evar28=' + biblio);
                    }

                    if (prodList[i].turnawayId) {
                        pageData.eventList.push('product turnaway');
                    }

                    var price = prodList[i].price || '', qty = prodList[i].quantity || '', evts = [];
                    if (price && qty) {
                        qty = parseInt(qty || '1');
                        price = parseFloat(price || '0');
                        price = (price * qty).toFixed(2);

                        if (window.eventData && eventData.eventName && eventData.eventName == 'cartAdd') {
                            evts.push('event20=' + price);
                        }
                    }

                    var type = window.pageData && pageData.page && pageData.page.type ? pageData.page.type : '', evt = window.eventData && eventData.eventName ? eventData.eventName : '';
                    if (type.match(/^CP\-/gi) !== null && (!evt || evt == 'newPage' || evt == 'contentView')) {
                        evts.push('event181=1');
                    }
                    if (evt == 'contentDownload' || type.match(/^CP\-DL/gi) !== null) {
                        evts.push('event182=1');
                    }
                    if (evt == 'contentDownloadRequest') {
                        evts.push('event319=1');
                    }
                    if (evt == 'contentExport') {
                        evts.push('event184=1');
                    }
                    if (this.eventFires('recommendationViews')) {
                        evts.push('event264=1');
                    }

                    if(prodList[i].datapoints) {
                        evts.push('event239=' + prodList[i].datapoints);
                    }
                    if(prodList[i].documents) {
                        evts.push('event240=' + prodList[i].documents);
                    }
                    if(prodList[i].size) {
                        evts.push('event335=' + prodList[i].size);
                        evts.push('event336=1')
                    }

                    prods.push([
                        ''					// empty category
                        ,prodList[i].id		// id
                        ,qty				// qty
                        ,price				// price
                        ,evts.join('|')		// events
                        ,merch.join('|')	// merchandising eVars
                    ].join(';'));
                }
            }
        }

        return prods.join(',');
    }
    ,eventFires: function(eventName) {
      var evt = window.eventData && eventData.eventName ? eventData.eventName : '';
      if(evt == eventName) {
        return true;
      }
      // initial pageload and new pages
      if((!window.eventData || evt == 'newPage') && window.pageData && window.pageData.trackEvents) {
        var tEvents = window.pageData.trackEvents;
        for(var i=0; i<tEvents.length; i++) {
          if(tEvents[i] == eventName) {
            return true;
          }
        }
      }
      return false;
    }

    ,md5: function(s){function L(k,d){return(k<<d)|(k>>>(32-d))}function K(G,k){var I,d,F,H,x;F=(G&2147483648);H=(k&2147483648);I=(G&1073741824);d=(k&1073741824);x=(G&1073741823)+(k&1073741823);if(I&d){return(x^2147483648^F^H)}if(I|d){if(x&1073741824){return(x^3221225472^F^H)}else{return(x^1073741824^F^H)}}else{return(x^F^H)}}function r(d,F,k){return(d&F)|((~d)&k)}function q(d,F,k){return(d&k)|(F&(~k))}function p(d,F,k){return(d^F^k)}function n(d,F,k){return(F^(d|(~k)))}function u(G,F,aa,Z,k,H,I){G=K(G,K(K(r(F,aa,Z),k),I));return K(L(G,H),F)}function f(G,F,aa,Z,k,H,I){G=K(G,K(K(q(F,aa,Z),k),I));return K(L(G,H),F)}function D(G,F,aa,Z,k,H,I){G=K(G,K(K(p(F,aa,Z),k),I));return K(L(G,H),F)}function t(G,F,aa,Z,k,H,I){G=K(G,K(K(n(F,aa,Z),k),I));return K(L(G,H),F)}function e(G){var Z;var F=G.length;var x=F+8;var k=(x-(x%64))/64;var I=(k+1)*16;var aa=Array(I-1);var d=0;var H=0;while(H<F){Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=(aa[Z]| (G.charCodeAt(H)<<d));H++}Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=aa[Z]|(128<<d);aa[I-2]=F<<3;aa[I-1]=F>>>29;return aa}function B(x){var k="",F="",G,d;for(d=0;d<=3;d++){G=(x>>>(d*8))&255;F="0"+G.toString(16);k=k+F.substr(F.length-2,2)}return k}function J(k){k=k.replace(/rn/g,"n");var d="";for(var F=0;F<k.length;F++){var x=k.charCodeAt(F);if(x<128){d+=String.fromCharCode(x)}else{if((x>127)&&(x<2048)){d+=String.fromCharCode((x>>6)|192);d+=String.fromCharCode((x&63)|128)}else{d+=String.fromCharCode((x>>12)|224);d+=String.fromCharCode(((x>>6)&63)|128);d+=String.fromCharCode((x&63)|128)}}}return d}var C=Array();var P,h,E,v,g,Y,X,W,V;var S=7,Q=12,N=17,M=22;var A=5,z=9,y=14,w=20;var o=4,m=11,l=16,j=23;var U=6,T=10,R=15,O=21;s=J(s);C=e(s);Y=1732584193;X=4023233417;W=2562383102;V=271733878;for(P=0;P<C.length;P+=16){h=Y;E=X;v=W;g=V;Y=u(Y,X,W,V,C[P+0],S,3614090360);V=u(V,Y,X,W,C[P+1],Q,3905402710);W=u(W,V,Y,X,C[P+2],N,606105819);X=u(X,W,V,Y,C[P+3],M,3250441966);Y=u(Y,X,W,V,C[P+4],S,4118548399);V=u(V,Y,X,W,C[P+5],Q,1200080426);W=u(W,V,Y,X,C[P+6],N,2821735955);X=u(X,W,V,Y,C[P+7],M,4249261313);Y=u(Y,X,W,V,C[P+8],S,1770035416);V=u(V,Y,X,W,C[P+9],Q,2336552879);W=u(W,V,Y,X,C[P+10],N,4294925233);X=u(X,W,V,Y,C[P+11],M,2304563134);Y=u(Y,X,W,V,C[P+12],S,1804603682);V=u(V,Y,X,W,C[P+13],Q,4254626195);W=u(W,V,Y,X,C[P+14],N,2792965006);X=u(X,W,V,Y,C[P+15],M,1236535329);Y=f(Y,X,W,V,C[P+1],A,4129170786);V=f(V,Y,X,W,C[P+6],z,3225465664);W=f(W,V,Y,X,C[P+11],y,643717713);X=f(X,W,V,Y,C[P+0],w,3921069994);Y=f(Y,X,W,V,C[P+5],A,3593408605);V=f(V,Y,X,W,C[P+10],z,38016083);W=f(W,V,Y,X,C[P+15],y,3634488961);X=f(X,W,V,Y,C[P+4],w,3889429448);Y=f(Y,X,W,V,C[P+9],A,568446438);V=f(V,Y,X,W,C[P+14],z,3275163606);W=f(W,V,Y,X,C[P+3],y,4107603335);X=f(X,W,V,Y,C[P+8],w,1163531501);Y=f(Y,X,W,V,C[P+13],A,2850285829);V=f(V,Y,X,W,C[P+2],z,4243563512);W=f(W,V,Y,X,C[P+7],y,1735328473);X=f(X,W,V,Y,C[P+12],w,2368359562);Y=D(Y,X,W,V,C[P+5],o,4294588738);V=D(V,Y,X,W,C[P+8],m,2272392833);W=D(W,V,Y,X,C[P+11],l,1839030562);X=D(X,W,V,Y,C[P+14],j,4259657740);Y=D(Y,X,W,V,C[P+1],o,2763975236);V=D(V,Y,X,W,C[P+4],m,1272893353);W=D(W,V,Y,X,C[P+7],l,4139469664);X=D(X,W,V,Y,C[P+10],j,3200236656);Y=D(Y,X,W,V,C[P+13],o,681279174);V=D(V,Y,X,W,C[P+0],m,3936430074);W=D(W,V,Y,X,C[P+3],l,3572445317);X=D(X,W,V,Y,C[P+6],j,76029189);Y=D(Y,X,W,V,C[P+9],o,3654602809);V=D(V,Y,X,W,C[P+12],m,3873151461);W=D(W,V,Y,X,C[P+15],l,530742520);X=D(X,W,V,Y,C[P+2],j,3299628645);Y=t(Y,X,W,V,C[P+0],U,4096336452);V=t(V,Y,X,W,C[P+7],T,1126891415);W=t(W,V,Y,X,C[P+14],R,2878612391);X=t(X,W,V,Y,C[P+5],O,4237533241);Y=t(Y,X,W,V,C[P+12],U,1700485571);V=t(V,Y,X,W,C[P+3],T,2399980690);W=t(W,V,Y,X,C[P+10],R,4293915773);X=t(X,W,V,Y,C[P+1],O,2240044497);Y=t(Y,X,W,V,C[P+8],U,1873313359);V=t(V,Y,X,W,C[P+15],T,4264355552);W=t(W,V,Y,X,C[P+6],R,2734768916);X=t(X,W,V,Y,C[P+13],O,1309151649);Y=t(Y,X,W,V,C[P+4],U,4149444226);V=t(V,Y,X,W,C[P+11],T,3174756917);W=t(W,V,Y,X,C[P+2],R,718787259);X=t(X,W,V,Y,C[P+9],O,3951481745);Y=K(Y,h);X=K(X,E);W=K(W,v);V=K(V,g)}var i=B(Y)+B(X)+B(W)+B(V);return i.toLowerCase()}
    ,stripProductDelimiters: function(val) {
        if (val) {
            return val.replace(/\;|\||\,/gi, '-');
        }
    }

    ,setCookie: function(name, value, seconds, domain) {
        domain = document.location.hostname;
        var expires = '';
        var expiresNow = '';
        var date = new Date();
        date.setTime(date.getTime() + (-1 * 1000));
        expiresNow = "; expires=" + date.toGMTString();

        if (typeof(seconds) != 'undefined') {
            date.setTime(date.getTime() + (seconds * 1000));
            expires = '; expires=' + date.toGMTString();
        }

        var type = typeof(value);
        type = type.toLowerCase();
        if (type != 'undefined' && type != 'string') {
            value = JSON.stringify(value);
        }

        // fix scoping issues
        // keep writing the old cookie, but make it expire
        document.cookie = name + '=' + value + expiresNow + '; path=/';

        // now just set the right one
        document.cookie = name + '=' + value + expires + '; path=/; domain=' + domain;
    }

    ,getCookie: function(name) {
        name = name + '=';
        var carray = document.cookie.split(';'), value;

        for (var i=0; i<carray.length; i++) {
            var c = carray[i];
            while (c.charAt(0) == ' ') {
                c = c.substring(1, c.length);
            }
            if (c.indexOf(name) == 0) {
                value = c.substring(name.length, c.length);
                try {
                    value = JSON.parse(value);
                } catch(ex) {}

                return value;
            }
        }

        return null;
    }

    ,deleteCookie: function(name) {
        this.setCookie(name, '', -1);
        this.setCookie(name, '', -1, document.location.hostname);
    }

    ,mapAdobeVars: function(s) {
        var vars = {
            pageName		: 'Page - Analytics Pagename'
            ,channel		: 'Page - Section Name'
            ,campaign		: 'Campaign - ID'
            ,currencyCode	: 'Page - Currency Code'
            ,purchaseID		: 'Order - ID'
            ,prop1			: 'Visitor - Account ID'
            ,prop2			: 'Page - Product Name'
            ,prop4			: 'Page - Type'
            ,prop6			: 'Search - Type'
            ,prop7			: 'Search - Facet List'
            ,prop8			: 'Search - Feature Used'
            ,prop12			: 'Visitor - User ID'
            ,prop13			: 'Search - Sort Type'
            ,prop14			: 'Page - Load Time'
            ,prop15         : 'Support - Topic Name'
            ,prop16			: 'Page - Business Unit'
            ,prop21			: 'Search - Criteria'
            ,prop24			: 'Page - Language'
            ,prop25			: 'Page - Product Feature'
            ,prop28         : 'Support - Search Criteria'
            ,prop30			: 'Visitor - IP Address'
            ,prop33         : 'Page - Product Application Version'
            ,prop34         : 'Page - Website Extensions'
            ,prop60			: 'Search - Data Form Criteria'
            ,prop63			: 'Page - Extended Page Name'
            ,prop65         : 'Page - Online State'
            ,prop67         : 'Research Networks'
            ,prop40: 'Page - UX Properties'

            ,eVar3			: 'Search - Total Results'
            ,eVar7			: 'Visitor - Account Name'
            ,eVar15			: 'Event - Search Results Click Position'
            ,eVar19			: 'Search - Advanced Criteria'
            ,eVar21			: 'Promo - Clicked ID'
            ,eVar22			: 'Page - Test ID'
            ,eVar27			: 'Event - AutoSuggest Search Data'
            ,eVar157		: 'Event - AutoSuggest Search Typed Term'
            ,eVar156		: 'Event - AutoSuggest Search Selected Term'
            ,eVar162		: 'Event - AutoSuggest Search Category'
            ,eVar163		: 'Event - AutoSuggest Search Details'
            ,eVar33			: 'Visitor - Access Type'
            ,eVar34			: 'Order - Promo Code'
            ,eVar39			: 'Order - Payment Method'
            ,eVar41			: 'Visitor - Industry'
            ,eVar42			: 'Visitor - SIS ID'
            ,eVar43			: 'Page - Error Type'
            ,eVar44			: 'Event - Updated User Fields'
            ,eVar48			: 'Email - Recipient ID'
            ,eVar51			: 'Email - Message ID'
            ,eVar52			: 'Visitor - Department ID'
            ,eVar53			: 'Visitor - Department Name'
            ,eVar60			: 'Search - Within Content Criteria'
            ,eVar61			: 'Search - Within Results Criteria'
            ,eVar62			: 'Search - Result Types'
            ,eVar74			: 'Page - Journal Info'
            ,eVar59			: 'Page - Journal Publisher'
            ,eVar76			: 'Email - Broadlog ID'
            ,eVar78			: 'Visitor - Details'
            ,eVar80         : 'Visitor - Usage Path Info'
            ,eVar102		: 'Form - Name'
            ,eVar103        : 'Event - Conversion Driver'
            ,eVar105        : 'Search - Current Page'
            ,eVar106        : 'Visitor - App Session ID'
            ,eVar107        : 'Page - Secondary Product Name'
            ,eVar117        : 'Search - Database'
            ,eVar126        : 'Page - Environment'
            ,eVar141        : 'Search - Criteria Original'
            ,eVar143        : 'Page - Tabs'
            ,eVar161        : 'Search - Channel'
            ,eVar169        : 'Search - Facet Operation'
            ,eVar173        : 'Search - Details'
            ,eVar174        : 'Campaign - Spredfast ID'
            ,eVar175        : 'Visitor - TMX Device ID'
            ,eVar176        : 'Visitor - TMX Request ID'
            ,eVar148        : 'Visitor - Platform Name'
            ,eVar149        : 'Visitor - Platform ID'
            ,eVar152        : 'Visitor - Product ID'
            ,eVar153        : 'Visitor - Superaccount ID'
            ,eVar154        : 'Visitor - Superaccount Name'
            ,eVar177        : 'Page - Context Domain'
            ,eVar189    : 'Page - Experimentation User Id'
            ,eVar190    : 'Page - Identity User'
            ,eVar199    : 'Page - ID+ Parameters'

            ,list2			: 'Page - Widget Names'
            ,list3			: 'Promo - IDs'
        };

        for (var i in vars) {
            s[i] = s[i] ? s[i] : _satellite.getVar(vars[i]);
        }
    }
};

// async support fallback
(function(w) {
	var eventBuffer = [];
	if(w.appData) {
		if(Array.isArray(w.appData)) {
			eventBuffer = w.appData;
		} else {
			console.error('Elsevier DataLayer "window.appData" must be specified as array');
			return;
		}
    }

	w.appData = [];

	var oldPush = w.appData.push;

	var appDataPush = function() {
        oldPush.apply(w.appData, arguments);
        for(var i=0; i<arguments.length; i++) {
            var data = arguments[i];
            if(data.event) {
                if(data.event == 'pageLoad') {
                    w.pageDataTracker.trackPageLoad(data);
                } else {
                    w.pageDataTracker.trackEvent(data.event, data);
                }
            }
        }
	};

	w.appData.push = appDataPush;
	for(var i=0; i<eventBuffer.length; i++) {
	    var data = eventBuffer[i];
	    w.appData.push(data);
	}
})(window);

</script><script>_satellite["_runScript1"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><script src="https://cdn.plu.mx/widget-summary.js" async=""></script><script>_satellite["_runScript2"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-hide ot-fade-in"></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" checked="" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" checked="" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" checked="" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details List‎</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Search…" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div><button aria-label="Feedback" type="button" id="_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E" data-layout="badgeBlank" class="_pendo-badge _pendo-badge_" style="z-index: 19000; margin: 0px; height: 32px; width: 128px; font-size: 0px; background: rgba(255, 255, 255, 0); padding: 0px; line-height: 1; min-width: auto; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; border: 0px; float: none; vertical-align: baseline; cursor: pointer; position: absolute; top: 67301px; left: 912px;"><img id="pendo-image-badge-19b66351" src="https://pendo-static-5661679399600128.storage.googleapis.com/D_T2uHq_M1r-XQq8htU6Z3GjHfE/guide-media-75af8ddc-3c43-49fd-8836-cfc7e2c3ea60" alt="Feedback" data-_pendo-image-1="" class="_pendo-image _pendo-badge-image" style="display: block; height: 32px; width: 128px; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; float: none; vertical-align: baseline;"></button><script>_satellite["_runScript3"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script></body></html>