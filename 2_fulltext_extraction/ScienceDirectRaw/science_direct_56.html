<html lang="en-US"><head>
      <meta name="citation_pii" content="S0925231220300783">
<meta name="citation_issn" content="0925-2312">
<meta name="citation_volume" content="388">
<meta name="citation_lastpage" content="227">
<meta name="citation_publisher" content="Elsevier">
<meta name="citation_firstpage" content="212">
<meta name="citation_journal_title" content="Neurocomputing">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.neucom.2020.01.034">
<meta name="dc.identifier" content="10.1016/j.neucom.2020.01.034">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed anâ€¦">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220X00117-cov150h.gif">
<meta name="citation_title" content="Deep convolution network based emotion analysis towards mental health care">
<meta property="og:title" content="Deep convolution network based emotion analysis towards mental health care">
<meta name="citation_publication_date" content="2020/05/07">
<meta name="citation_online_date" content="2020/01/16">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
      <title>Deep convolution network based emotion analysis towards mental health care - ScienceDirect</title>
      <link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0925231220300783">
      <meta property="og:type" content="article">
      <meta name="viewport" content="initial-scale=1">
      <meta name="SDTech" content="Proudly brought to you by the SD Technology team">
      <script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
      <link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/arp.css">
      <link href="//cdn.pendo.io" rel="dns-prefetch">
      <link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com">
      <script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S0925231220300783?componentVersion=V11&amp;jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwOTI1MjMxMjIwMzAwNzgzIiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiNWUwYThhMzI3ZjI3YTA0OGE4NWIyM2E2YTM3MTU2NmQ2OThkZ3hycWEiLCJleHAiOjE3MzAyMDU2NzAsImlhdCI6MTczMDIwMzg3MCwidmVyc2lvbiI6MSwianRpIjoiNTcwMjYzZGEtZmI2Zi00ZGJhLTljNDQtMmZiZGI1M2M1NjZmIn0.vR8QqDNb_tqfmKKCHNjMnhmdlGpCCl3irTLWDRn_cAk" type="text/javascript"></script>
      <script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"7820A6F1FE01C820-725DCF3512C53222","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Neurocomputing","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
    <script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement.min.js" async=""></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement_Module_ActivityMap.min.js" async=""></script><script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/6a62c1bc1779/RCa16d232f95a944c0aabdea6621a2ef94-source.min.js" async=""></script><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202410240101/pubads_impl.js?cb=31088506" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link id="plx-css-summary" type="text/css" rel="stylesheet" href="//cdn.plu.mx/summary.css"><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//cdn.plu.mx/extjs/xss.js"></script><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:700;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:700}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none!important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0,0,0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none!important}.onetrust-pc-dark-filter.ot-hide{display:none!important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width:426px)and (max-width:896px)and (orientation:landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:400;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:0 0;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width:400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width:550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width:550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:transparent;border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style><style data-styled="active" data-styled-version="6.1.13"></style><link type="text/css" rel="stylesheet" href="https://pendo-static-5661679399600128.storage.googleapis.com/guide.-323232.1721046486120.css" id="_pendo-css_"><style type="text/css" scoped="scoped" class=" pendo-style-D_T2uHq_M1r-XQq8htU6Z3GjHfE" style="white-space: pre-wrap;"></style><style type="text/css">.MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
</style></head>
    <body data-sd-ui-layer-boundary="true" class="toolbar-stuck" style=""><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_SVG_Hidden"><br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br><br></br></div><svg><defs id="MathJax_SVG_glyphs"><path stroke-width="1" id="MJMATHI-58" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path><path stroke-width="1" id="MJMATHI-6C" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width="1" id="MJMATHI-6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path stroke-width="1" id="MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="1" id="MJMATHI-46" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path stroke-width="1" id="MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="1" id="MJSZ1-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path stroke-width="1" id="MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="MJMAIN-2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path stroke-width="1" id="MJMATHI-4D" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path stroke-width="1" id="MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="1" id="MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="1" id="MJMAIN-2A" d="M215 721Q216 732 225 741T248 750Q263 750 273 742T284 721L270 571L327 613Q383 654 388 657T399 660Q412 660 423 650T435 624T424 600T376 575Q363 569 355 566L289 534L355 504L424 470Q435 462 435 447Q435 431 424 420T399 409Q393 409 388 412T327 456L270 498L277 423L284 348Q280 320 250 320T215 348L229 498L172 456Q116 415 111 412T100 409Q87 409 76 420T64 447Q64 461 75 470L144 504L210 534L144 566Q136 570 122 576Q83 593 74 600T64 624Q64 639 75 649T100 660Q106 660 111 657T172 613L229 571Q229 578 222 643T215 721Z"></path><path stroke-width="1" id="MJMATHI-77" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path stroke-width="1" id="MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="1" id="MJMATHI-62" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path stroke-width="1" id="MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width="1" id="MJSZ2-28" d="M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z"></path><path stroke-width="1" id="MJSZ2-29" d="M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z"></path><path stroke-width="1" id="MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="1" id="MJMATHI-6D" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="MJMATHI-61" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path stroke-width="1" id="MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="1" id="MJMAIN-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width="1" id="MJMATHI-64" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path stroke-width="1" id="MJMATHI-6F" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path stroke-width="1" id="MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="MJMATHI-4A" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path><path stroke-width="1" id="MJMATHI-57" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path stroke-width="1" id="MJMAIN-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width="1" id="MJMATHI-54" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path stroke-width="1" id="MJMATHI-53" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path stroke-width="1" id="MJMATHI-42" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path stroke-width="1" id="MJMAIN-2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width="1" id="MJMATHI-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path stroke-width="1" id="MJMATHI-3BC" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></defs></svg></div><div id="MathJax_Message" style="display: none;"></div>
      <script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"cesectitle0001"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"spara024"},"#name":"simple-para","_":"Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual's mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained â€˜in the wildâ€™ such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis."}],"$":{"view":"all","id":"abss0001"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0001","class":"author"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banner":{"id":"Banner"},"banners":[{"id":"Banner"},{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"50401","accountName":"IT University of Copenhagen","loginStatus":"logged in","userId":"71970787","isLoggedIn":true},"cid":"271597","content-family":"serial","copyright-line":"Â© 2020 Published by Elsevier B.V.","cover-date-years":["2020"],"cover-date-start":"2020-05-07","cover-date-text":"7 May 2020","document-subtype":"fla","document-type":"article","entitledToken":"A9E2A9DF2254227BC87D04F7E42DE2613957420E4D5CB64C04AAF721C6C039314149A6263CB08C71","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwOTI1MjMxMjIwMzAwNzgzIiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiNWUwYThhMzI3ZjI3YTA0OGE4NWIyM2E2YTM3MTU2NmQ2OThkZ3hycWEiLCJleHAiOjE3MzAyMDU2NzAsImlhdCI6MTczMDIwMzg3MCwidmVyc2lvbiI6MSwianRpIjoiNTcwMjYzZGEtZmI2Zi00ZGJhLTljNDQtMmZiZGI1M2M1NjZmIn0.vR8QqDNb_tqfmKKCHNjMnhmdlGpCCl3irTLWDRn_cAk","eid":"1-s2.0-S0925231220300783","doi":"10.1016/j.neucom.2020.01.034","first-fp":"212","hub-eid":"1-s2.0-S0925231220X00117","issuePii":"S0925231220X00117","item-weight":"FULL-TEXT","language":"en","last-lp":"227","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"author-id":"S0925231220300783-83d0fd1a17adc5fe4f3d348122cd0384","biographyid":"auth7Bio7","id":"au0007"},"$$":[{"#name":"given-name","_":"Huiyu"},{"#name":"surname","_":"Zhou"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0009","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"e"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA3JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWh6MTQzJTQwbGVpY2VzdGVyLmFjLnVrJTIyJTdEJTJDJTIyXyUyMiUzQSUyMmh6MTQzJTQwbGVpY2VzdGVyLmFjLnVrJTIyJTdE"}]}]},"normalized-first-auth-initial":"Z","normalized-first-auth-surname":"FEI","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-agreements","$$":[{"#name":"or-agreement","$$":[{"#name":"or-agreement-name","_":"HEFCE"},{"#name":"or-agreement-terms","_":"none"}]}]}]},"pages":[{"last-page":"227","first-page":"212"}],"pii":"S0925231220300783","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2021-03-18T00:00:00.000Z"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Neurocomputing","suppl":"C","timestamp":"2024-04-05T05:22:25.511799Z","title":{"content":[{"#name":"title","$":{"id":"tte0002"},"_":"Deep convolution network based emotion analysis towards mental health care"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"388","vol-iss-suppl-text":"Volume 388","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":61,"freeHtmlGiven":false,"userProfile":{"departmentName":"Library","webUserId":"71970787","accountName":"IT University of Copenhagen","shibProfile":{"canRegister":false},"departmentId":"71310","hasMultipleOrganizations":false,"accountNumber":"C000050401","userName":"Gergo Gyori","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLt_qTVss9so_a_5APBuClac*","accessType":"SHIBREG","accountId":"50401","userType":"NORMAL","privilegeType":"BASIC","email":"gegy@itu.dk"},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"authenticationMethod":"SHIBBOLETH","entitled":true,"isCasaUser":false,"usageInfo":"(71970787,U|71310,D|50401,A|26,S|34,P|2,PL)(SDFE,CON|5e0a8a327f27a048a85b23a6a371566d698dgxrqa,SSO|REG_SHIBBOLETH,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"16 January 2020","Received":"6 October 2019","Revised":["6 January 2020"],"Accepted":"9 January 2020","Publication date":"7 May 2020","Version of Record":"18 March 2020"},"downloadFullIssue":true,"entitlementReason":"package","hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/ci/pta/login/redirect/home/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLt_qTVss9so_a_5APBuClac*","contactUrl":"https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLt_qTVss9so_a_5APBuClac*","userName":"Gergo Gyori","userEmail":"gegy@itu.dk","orgName":"IT University of Copenhagen","webUserId":"71970787","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":true,"hasMultiOrg":false,"userType":"SHIBREG","userAnonymity":"INDIVIDUAL","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLt_qTVss9so_a_5APBuClac*","institutionName":"your institution"},"isCorpReq":false,"isPdfFullText":false,"issn":"09252312","issn-primary-formatted":"0925-2312","issRange":"","isThirdParty":false,"pageCount":16,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"3c9834b28d4c58545e2c110a9251b0d7","pid":"1-s2.0-S0925231220300783-main.pdf"},"pii":"S0925231220300783","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":541,"openArchiveStatus":false,"openArchiveArticleCount":18,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0925231220X00117-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220X00117/cover/DOWNSAMPLED200/image/gif/5505a2d23fe3beb3536730f88117429c/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"4189","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S0925231220X00117-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220X00117/cover/DOWNSAMPLED/image/gif/586e12a5dab98d3e8e24d435bb734243/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"2822","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S09252312.gif","title":"neurocomputing","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S0925231220X00117-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0925231220X00117-cov150h.gif"},"volRange":"388","features":["aamAttachments","keywords","references","biography","preview"],"titleString":"Deep convolution network based emotion analysis towards mental health care","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"referenceLinks":true,"references":true,"referredToBy":true,"recommendations-entitled":true,"toc":true,"body":true,"recommendations":true,"citingArticles":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"aut0001"},"$$":[{"#name":"author","$":{"author-id":"S0925231220300783-b7b7bcb00d3c01ff1a38c883a95eebbf","biographyid":"auth1Bio1","id":"au0001"},"$$":[{"#name":"given-name","_":"Zixiang"},{"#name":"surname","_":"Fei"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Data_curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Visualization"},"_":"Visualization"},{"#name":"cross-ref","$":{"id":"crf0001","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAxJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXppeGlhbmcuZmVpJTQwc3RyYXRoLmFjLnVrJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnppeGlhbmcuZmVpJTQwc3RyYXRoLmFjLnVrJTIyJTdE"}]},{"#name":"author","$":{"author-id":"S0925231220300783-ff2154f5f282824f9ff02646fca4e4d2","biographyid":"auth2Bio2","id":"au0002","orcid":"0000-0003-1813-5950"},"$$":[{"#name":"given-name","_":"Erfu"},{"#name":"surname","_":"Yang"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Funding_acquisition"},"_":"Funding acquisition"},{"#name":"cross-ref","$":{"id":"crf0002","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"crf0003","refid":"cor0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAyJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWVyZnUueWFuZyU0MHN0cmF0aC5hYy51ayUyMiU3RCUyQyUyMl8lMjIlM0ElMjJlcmZ1LnlhbmclNDBzdHJhdGguYWMudWslMjIlN0Q="}]},{"#name":"author","$":{"author-id":"S0925231220300783-28dbe8599d1bffb0d5ad3da0628488ae","biographyid":"auth3Bio3","id":"au0003"},"$$":[{"#name":"given-name","_":"David Day-Uei"},{"#name":"surname","_":"Li"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0004","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAzJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWRhdmlkLmxpJTQwc3RyYXRoLmFjLnVrJTIyJTdEJTJDJTIyXyUyMiUzQSUyMmRhdmlkLmxpJTQwc3RyYXRoLmFjLnVrJTIyJTdE"}]},{"#name":"author","$":{"author-id":"S0925231220300783-a2b2c89c05a8e77d7b9ce60e408b3036","biographyid":"auth4Bio4","id":"au0004"},"$$":[{"#name":"given-name","_":"Stephen"},{"#name":"surname","_":"Butler"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0005","refid":"aff0003"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA0JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXN0ZXBoZW4uYnV0bGVyJTQwc3RyYXRoLmFjLnVrJTIyJTdEJTJDJTIyXyUyMiUzQSUyMnN0ZXBoZW4uYnV0bGVyJTQwc3RyYXRoLmFjLnVrJTIyJTdE"}]},{"#name":"author","$":{"author-id":"S0925231220300783-c8e12bd73057168a51f6007d69459b4c","biographyid":"auth5Bio5","id":"au0005"},"$$":[{"#name":"given-name","_":"Winifred"},{"#name":"surname","_":"Ijomah"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0006","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA1JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXcubC5pam9tYWglNDBzdHJhdGguYWMudWslMjIlN0QlMkMlMjJfJTIyJTNBJTIydy5sLmlqb21haCU0MHN0cmF0aC5hYy51ayUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231220300783-0f2f35165c0356499a30d6457924bbe6","biographyid":"auth6Bio6","id":"au0006"},"$$":[{"#name":"given-name","_":"Xia"},{"#name":"surname","_":"Li"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0007","refid":"aff0004"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"d"}]},{"#name":"cross-ref","$":{"id":"crf0008","refid":"cor0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA2JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWxpeGlhMTExMTElNDBhbHVtbmkuc2p0dS5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIybGl4aWExMTExMSU0MGFsdW1uaS5zanR1LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"author-id":"S0925231220300783-83d0fd1a17adc5fe4f3d348122cd0384","biographyid":"auth7Bio7","id":"au0007"},"$$":[{"#name":"given-name","_":"Huiyu"},{"#name":"surname","_":"Zhou"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"id":"crf0009","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"e"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA3JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWh6MTQzJTQwbGVpY2VzdGVyLmFjLnVrJTIyJTdEJTJDJTIyXyUyMiUzQSUyMmh6MTQzJTQwbGVpY2VzdGVyLmFjLnVrJTIyJTdE"}]},{"#name":"affiliation","$":{"id":"aff0001","affiliation-id":"S0925231220300783-ec7a336976a55ca5a695cc83261e2a02"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"cetextfn0001"},"_":"Department of Design, Manufacturing and Engineering Management, University of Strathclyde, Glasgow G1 1XJ, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Design, Manufacture and Engineering Management"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G1 1XJ"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0001"},"_":"aDepartment of Design, Manufacture and Engineering Management, University of Strathclyde, Glasgow G1 1XJ, UK"}]},{"#name":"affiliation","$":{"id":"aff0002","affiliation-id":"S0925231220300783-6e0b8d0d44155ec23b07711d7bc975c1"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"cetextfn0002"},"_":"Strathclyde Institute of Pharmacy & Biomedical Sciences, University of Strathclyde, Glasgow G4 0RE, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Strathclyde Institute of Pharmacy & Biomedical Sciences"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G4 0RE"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0002"},"_":"bStrathclyde Institute of Pharmacy & Biomedical Sciences, University of Strathclyde, Glasgow G4 0RE, UK"}]},{"#name":"affiliation","$":{"id":"aff0003","affiliation-id":"S0925231220300783-942bab707014a4cdcef233d1bb0fe681"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"cetextfn0003"},"_":"School of Psychological Sciences and Health, University of Strathclyde, Glasgow G1 1QE, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychological Sciences and Health"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G1 1QE"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0003"},"_":"cSchool of Psychological Sciences and Health, University of Strathclyde, Glasgow G1 1QE, UK"}]},{"#name":"affiliation","$":{"id":"aff0004","affiliation-id":"S0925231220300783-49d3c2baaace02461f8ba69360b6d183"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"cetextfn0004"},"_":"Shanghai Jiaotong University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Shanghai Jiaotong University"},{"#name":"city","_":"Shanghai"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0004"},"_":"dShanghai Jiaotong University, Shanghai, China"}]},{"#name":"affiliation","$":{"id":"aff0005","affiliation-id":"S0925231220300783-d909299c319010f827a7059eaa9c984c"},"$$":[{"#name":"label","_":"e"},{"#name":"textfn","$":{"id":"cetextfn0005"},"_":"Department of Informatics, University of Leicester, LE1 7RH, Leicester, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Informatics"},{"#name":"organization","_":"University of Leicester"},{"#name":"city","_":"Leicester"},{"#name":"postal-code","_":"LE1 7RH"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0005"},"_":"eDepartment of Informatics, University of Leicester, LE1 7RH, Leicester, UK"}]},{"#name":"correspondence","$":{"id":"cor0001"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","$":{"id":"cetext0001"},"_":"Corresponding authors."}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff0001":{"#name":"affiliation","$":{"id":"aff0001","affiliation-id":"S0925231220300783-ec7a336976a55ca5a695cc83261e2a02"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"cetextfn0001"},"_":"Department of Design, Manufacturing and Engineering Management, University of Strathclyde, Glasgow G1 1XJ, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Design, Manufacture and Engineering Management"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G1 1XJ"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0001"},"_":"aDepartment of Design, Manufacture and Engineering Management, University of Strathclyde, Glasgow G1 1XJ, UK"}]},"aff0002":{"#name":"affiliation","$":{"id":"aff0002","affiliation-id":"S0925231220300783-6e0b8d0d44155ec23b07711d7bc975c1"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"cetextfn0002"},"_":"Strathclyde Institute of Pharmacy & Biomedical Sciences, University of Strathclyde, Glasgow G4 0RE, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Strathclyde Institute of Pharmacy & Biomedical Sciences"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G4 0RE"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0002"},"_":"bStrathclyde Institute of Pharmacy & Biomedical Sciences, University of Strathclyde, Glasgow G4 0RE, UK"}]},"aff0003":{"#name":"affiliation","$":{"id":"aff0003","affiliation-id":"S0925231220300783-942bab707014a4cdcef233d1bb0fe681"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"cetextfn0003"},"_":"School of Psychological Sciences and Health, University of Strathclyde, Glasgow G1 1QE, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Psychological Sciences and Health"},{"#name":"organization","_":"University of Strathclyde"},{"#name":"city","_":"Glasgow"},{"#name":"postal-code","_":"G1 1QE"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0003"},"_":"cSchool of Psychological Sciences and Health, University of Strathclyde, Glasgow G1 1QE, UK"}]},"aff0004":{"#name":"affiliation","$":{"id":"aff0004","affiliation-id":"S0925231220300783-49d3c2baaace02461f8ba69360b6d183"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"cetextfn0004"},"_":"Shanghai Jiaotong University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Shanghai Jiaotong University"},{"#name":"city","_":"Shanghai"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0004"},"_":"dShanghai Jiaotong University, Shanghai, China"}]},"aff0005":{"#name":"affiliation","$":{"id":"aff0005","affiliation-id":"S0925231220300783-d909299c319010f827a7059eaa9c984c"},"$$":[{"#name":"label","_":"e"},{"#name":"textfn","$":{"id":"cetextfn0005"},"_":"Department of Informatics, University of Leicester, LE1 7RH, Leicester, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Informatics"},{"#name":"organization","_":"University of Leicester"},{"#name":"city","_":"Leicester"},{"#name":"postal-code","_":"LE1 7RH"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"staff0005"},"_":"eDepartment of Informatics, University of Leicester, LE1 7RH, Leicester, UK"}]}},"correspondences":{"cor0001":{"#name":"correspondence","$":{"id":"cor0001"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","$":{"id":"cetext0001"},"_":"Corresponding authors."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{"content":[{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth1Bio1","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0010","locator":"fx1","href":"pii:S0925231220300783/fx1","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara025","view":"all"},"$$":[{"#name":"bold","_":"Zixiang Fei"},{"#name":"__text__","_":" received his Bachelor degree in Liverpool John Moores University and Master Degree in University of York. He is currently a Ph.D. student in University of Strathclyde. His major research interests include computer vision, machine learning, object recognition and deep learning."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth2Bio2","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0011","locator":"fx2","href":"pii:S0925231220300783/fx2","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara026","view":"all"},"$$":[{"#name":"bold","_":"Erfu Yang"},{"#name":"__text__","_":" is a Lecturer in University of Strathclyde under Strathclyde Chancellor's Fellowship Scheme. In 2008, he received his Ph.D. degree in robotics in the interdisciplinary area of robotics and autonomous Systems from the University of Essex. His main research interests include robotics, autonomous systems, computer vision, image/signal processing, mechatronics, data analytics, etc."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth3Bio3","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0012","locator":"fx3","href":"pii:S0925231220300783/fx3","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara027","view":"all"},"$$":[{"#name":"bold","_":"David Day-Uei Li"},{"#name":"__text__","_":" received the Ph.D. degree in electrical engineering from the National Taiwan University, Taipei, Taiwan, in 2001. He then joined the Industrial Technology Research Institute, Taiwan, working on CMOS communication chipsets. From 2007 to 2011 he worked at the University of Edinburgh before he took the lectureship in biomedical engineering at the University of Sussex. In 2014 he joined the University of Strathclyde, Glasgow, as a Senior Lecturer. His research interests include mixed signal circuits, CMOS sensors and systems, embedded systems, FLIM systems & analysis, and optical communications."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth4Bio4","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0013","locator":"fx4","href":"pii:S0925231220300783/fx4","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara028","view":"all"},"$$":[{"#name":"bold","_":"Stephen Butler"},{"#name":"__text__","_":" received an MA and Ph.D. at the University of Glasgow. He is the director of the Strathclyde Oculomotor Lab. His research interests, employing eye tracking technology lie in the cognitive neuroscience of attention and the neuropsychology of eye movements. Â He has applied his research skills to theoretical work in areas including face processing, addiction, biases in attention and brain injury, and has a broad range of experience in applying his research skills in applied fields from shipping to interface design."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth5Bio5","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0014","locator":"fx5","href":"pii:S0925231220300783/fx5","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara029","view":"all"},"$$":[{"#name":"bold","_":"Winifred Ijomah"},{"#name":"__text__","_":" isÂ a Reader in University of Strathclyde. She gained a Ph.D. in Remanufacturing from the University of Plymouth in 2002. Her research focuses on sustainable design and manufacturing, and to date has focused on product end-of-life, particularly on remanufacturing."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth6Bio6","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0015a","locator":"fx6","href":"pii:S0925231220300783/fx6","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara030","view":"all"},"$$":[{"#name":"bold","_":"Xia Li"},{"#name":"__text__","_":" is a psychiatry specialist for older adults in Shanghai Mental Health Center, Shanghai Jiaotong University, school of medicine. She has been engaged in psychogeriatric clinical practice and research for 16 years. Her interest mainly includes Dementia, behavioral problems and Depression in the elderly."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"auth7Bio7","view":"all"},"$$":[{"#name":"link","$":{"id":"celink0015","locator":"fx7","href":"pii:S0925231220300783/fx7","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","type":"simple"}},{"#name":"simple-para","$":{"id":"spara031","view":"all"},"$$":[{"#name":"bold","_":"Huiyu Zhou"},{"#name":"__text__","_":" received a Ph.D. in Computer Vision from Heriot-Watt University. He is a Reader in University of Leicester. He currently heads the Applied Algorithm and AI (AAAI) Theme and is leading the Biomedical Image Processing Lab at University of Leicester. His research interests includes machine learning, computer vision and artificial intelligence."}]}]}],"floats":[],"footnotes":[],"attachments":[{"attachment-eid":"1-s2.0-S0925231220300783-fx1.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx1/DOWNSAMPLED/image/jpeg/e18f8fe968e784b262361ea326f3765c/fx1.jpg","file-basename":"fx1","filename":"fx1.jpg","extension":"jpg","filesize":"5387","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx2.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx2/DOWNSAMPLED/image/jpeg/ec3ccab8cf07b39807cc66351eeb3e95/fx2.jpg","file-basename":"fx2","filename":"fx2.jpg","extension":"jpg","filesize":"6082","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx3.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx3/DOWNSAMPLED/image/jpeg/8071c084df46efe3850169752239d7ff/fx3.jpg","file-basename":"fx3","filename":"fx3.jpg","extension":"jpg","filesize":"8499","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx4.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx4/DOWNSAMPLED/image/jpeg/5f7593e2cf5478be656ddf11b7a68104/fx4.jpg","file-basename":"fx4","filename":"fx4.jpg","extension":"jpg","filesize":"9591","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx5.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx5/DOWNSAMPLED/image/jpeg/69e0130e5565a85f1b41589998b209ca/fx5.jpg","file-basename":"fx5","filename":"fx5.jpg","extension":"jpg","filesize":"5935","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx6.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx6/DOWNSAMPLED/image/jpeg/5e4ef3d7a9d88c4c217933a008e6908f/fx6.jpg","file-basename":"fx6","filename":"fx6.jpg","extension":"jpg","filesize":"9183","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx7.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx7/DOWNSAMPLED/image/jpeg/31fc85639b0badaaf72145660fcf3e1e/fx7.jpg","file-basename":"fx7","filename":"fx7.jpg","extension":"jpg","filesize":"6027","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0925231220300783-fx1.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx1/THUMBNAIL/image/gif/293edd082eb917e2c63515bea4982b77/fx1.sml","file-basename":"fx1","filename":"fx1.sml","extension":"sml","filesize":"10763","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx2.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx2/THUMBNAIL/image/gif/edd343eb33ab0f53907ec97632533cbe/fx2.sml","file-basename":"fx2","filename":"fx2.sml","extension":"sml","filesize":"16430","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx3.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx3/THUMBNAIL/image/gif/ac30669b956acc094e45327981202c59/fx3.sml","file-basename":"fx3","filename":"fx3.sml","extension":"sml","filesize":"17717","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx4.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx4/THUMBNAIL/image/gif/d9657974c981aa1d56e752fccdd2b5ba/fx4.sml","file-basename":"fx4","filename":"fx4.sml","extension":"sml","filesize":"18702","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx5.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx5/THUMBNAIL/image/gif/6da3be36b4c994f4f941cea7a470ba03/fx5.sml","file-basename":"fx5","filename":"fx5.sml","extension":"sml","filesize":"16477","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx6.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx6/THUMBNAIL/image/gif/bb777634453c7e04db6f41d6fcf84b29/fx6.sml","file-basename":"fx6","filename":"fx6.sml","extension":"sml","filesize":"17764","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx7.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx7/THUMBNAIL/image/gif/9a74fb681fb6034de992e1b514fd1c00/fx7.sml","file-basename":"fx7","filename":"fx7.sml","extension":"sml","filesize":"14873","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0925231220300783-fx1_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx1/HIGHRES/image/jpeg/3e4ceb8aee7e5d17b580374b752acb3a/fx1_lrg.jpg","file-basename":"fx1","filename":"fx1_lrg.jpg","extension":"jpg","filesize":"48396","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx2_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx2/HIGHRES/image/jpeg/7cd438573e7723eedeb1b74ab9c07c5d/fx2_lrg.jpg","file-basename":"fx2","filename":"fx2_lrg.jpg","extension":"jpg","filesize":"53016","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx3_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx3/HIGHRES/image/jpeg/88eb9ad5ea1f540bd51e2c79216e939f/fx3_lrg.jpg","file-basename":"fx3","filename":"fx3_lrg.jpg","extension":"jpg","filesize":"76632","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx4_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx4/HIGHRES/image/jpeg/804ee5f754bd5f1bf34b705e7884ada9/fx4_lrg.jpg","file-basename":"fx4","filename":"fx4_lrg.jpg","extension":"jpg","filesize":"101614","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx5_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx5/HIGHRES/image/jpeg/4148dc898ff2b8420fc60e7f7dc948b7/fx5_lrg.jpg","file-basename":"fx5","filename":"fx5_lrg.jpg","extension":"jpg","filesize":"56484","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx6_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx6/HIGHRES/image/jpeg/f6f28242b60ee2c972eaabc9538731b9/fx6_lrg.jpg","file-basename":"fx6","filename":"fx6_lrg.jpg","extension":"jpg","filesize":"81038","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0925231220300783-fx7_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231220300783/fx7/HIGHRES/image/jpeg/ac50514826885617ea968301f9265ac0/fx7_lrg.jpg","file-basename":"fx7","filename":"fx7_lrg.jpg","extension":"jpg","filesize":"53647","pixel-height":"667","pixel-width":"500","attachment-type":"IMAGE-HIGH-RES"}]},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":true},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"class":"keyword","id":"keys0001","view":"all"},"$$":[{"#name":"section-title","$":{"id":"cesectitle0002"},"_":"Keywords"},{"#name":"keyword","$":{"id":"key0001"},"$$":[{"#name":"text","$":{"id":"cetext0002"},"_":"Facial expression recognition"}]},{"#name":"keyword","$":{"id":"key0002"},"$$":[{"#name":"text","$":{"id":"cetext0003"},"_":"Deep convolution network"}]},{"#name":"keyword","$":{"id":"key0003"},"$$":[{"#name":"text","$":{"id":"cetext0004"},"_":"Mental health care"}]},{"#name":"keyword","$":{"id":"key0004"},"$$":[{"#name":"text","$":{"id":"cetext0005"},"_":"Emotion analysis"}]}]}]},{"#name":"miscellaneous","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"m0001"},"_":"Communicated by Bo Shen"}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"isOpen":true},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}};
      </script>
      <noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1730203870494?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AREG_SHIBBOLETH&c1=ae%3A50401&c12=ae%3A71970787 />
    </noscript>
      <a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
      <div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container u-hide-from-print"><div id="gh-profile-cnt" class="u-flex-center-ver gh-move-to-spine"><div class="popover" id="gh-profile-dropdown"><div id="popover-trigger-gh-profile-dropdown"><input type="hidden"><button class="button-link gh-icon-btn gh-user-icon u-margin-l-left gh-truncate text-s button-link-secondary button-link-medium button-link-icon-left" title="Gergo Gyori" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-person"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="button-link-text-container"><span class="button-link-text">Gergo Gyori</span></span></button></div></div></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id="institution-popover"><div id="popover-trigger-institution-popover"><input type="hidden"><button class="button-link gh-icon-btn gh-has-institution gh-truncate text-s button-link-secondary u-margin-l-left button-link-medium button-link-icon-left" id="gh-inst-icon-btn" aria-expanded="false" aria-label="Institutional Access" title="IT University of Copenhagen" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-institution gh-inst-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="button-link-text-container"><span class="button-link-text">IT University of Copenhagen</span></span></button></div></div></div><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S0925231220300783/pdfft?md5=3c9834b28d4c58545e2c110a9251b0d7&amp;pid=1-s2.0-S0925231220300783-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="search" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#abs0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry" id="ai-components-toc-entry"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#keys0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction"><span class="anchor-text-container"><span class="anchor-text">1. Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0002" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Materials and methods"><span class="anchor-text-container"><span class="anchor-text">2. Materials and methods</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Results"><span class="anchor-text-container"><span class="anchor-text">3. Results</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0018" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Discussion"><span class="anchor-text-container"><span class="anchor-text">4. Discussion</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0019" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Conclusion"><span class="anchor-text-container"><span class="anchor-text">5. Conclusion</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0019a" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="CRediT authorship contribution statement"><span class="anchor-text-container"><span class="anchor-text">CRediT authorship contribution statement</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#coi0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Declaration of Competing Interest"><span class="anchor-text-container"><span class="anchor-text">Declaration of Competing Interest</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0021" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgements"><span class="anchor-text-container"><span class="anchor-text">Acknowledgements</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0022" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Ethical approval"><span class="anchor-text-container"><span class="anchor-text">Ethical approval</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#cebibl1" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#auth1Bio1" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Vitae"><span class="anchor-text-container"><span class="anchor-text">Vitae</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (111)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (9)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0001" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 1. Structure of the whole system [34]" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr1.sml" width="92px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0002" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 2. Structure of the AlexNet [34,61]" class="u-display-block" height="57px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr2.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0003" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 3. Using the T-SNE to visualize the features extracted by the AlexNet" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr3.sml" width="206px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0004" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 4. Recognition results using the proposed framework" class="u-display-block" height="157px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr4.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 5. Facial expression analysis for the video stimulus of the author own emotionalâ€¦" class="u-display-block" height="108px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr5.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0006" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 6. Comparison of accumulative cross-validation error rates using different methodsâ€¦" class="u-display-block" height="126px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr6.sml" width="219px"></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more figures</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (14)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0001" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison of cross-validation error rate using different methods for the JAFFE dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0002" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the JAFFE dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0003" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison cross-validation error rate using different method for the KDEF dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0004" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the KDEF dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 4</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison cross-validation error rate using different methods for the CK dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 5</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0006" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Comparison of recognition accuracy of each emotion and overall recognition accuracy using different method for the CK dataset."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 6</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-tables" type="button"><span class="button-link-text-container"><span class="button-link-text">Show all tables</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/neurocomputing" title="Go to Neurocomputing on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/neurocomputing" title="Go to Neurocomputing on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Neurocomputing</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/neurocomputing/vol/388/suppl/C" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 388</span></span></a>, <!-- -->7 May 2020<!-- -->, Pages 212-227</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/neurocomputing/vol/388/suppl/C"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220X00117-cov150h.gif" alt="Neurocomputing"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">Deep convolution network based emotion analysis towards mental health care</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0001" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Zixiang</span> <span class="text surname">Fei</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0002" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Erfu</span> <span class="text surname">Yang</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0003" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">David Day-Uei</span> <span class="text surname">Li</span> </span><span class="author-ref" id="baff0002"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0004" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Stephen</span> <span class="text surname">Butler</span> </span><span class="author-ref" id="baff0003"><sup>c</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Winifred</span> <span class="text surname">Ijomah</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0006" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Xia</span> <span class="text surname">Li</span> </span><span class="author-ref" id="baff0004"><sup>d</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0007" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Huiyu</span> <span class="text surname">Zhou</span> </span><span class="author-ref" id="baff0005"><sup>e</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.01.034" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.neucom.2020.01.034</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0925231220300783&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author" id="abs0001"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="abss0001"><div class="u-margin-s-bottom" id="spara024"><span>Facial expressions play an important role during communications, allowing information regarding the <a href="/topics/computer-science/emotional-state" title="Learn more about emotional state from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional state</a><span><span> of an individual to be conveyed and inferred. Research suggests that automatic <a href="/topics/biochemistry-genetics-and-molecular-biology/facial-recognition" title="Learn more about facial expression recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial expression recognition</a><span> is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual's mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based <a href="/topics/computer-science/emotion-analysis" title="Learn more about emotion analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotion analysis</a><span><span> framework to support mental state detection and diagnosis. The proposed system is able to process <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a> and interpret the </span><a href="/topics/earth-and-planetary-sciences/temporal-evolution" title="Learn more about temporal evolution from ScienceDirect's AI-generated Topic Pages" class="topic-link">temporal evolution</a> of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard </span></span></span><a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about Linear Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Linear Discriminant Analysis</a><span> Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, <a href="/topics/neuroscience/face" title="Learn more about KDEF from ScienceDirect's AI-generated Topic Pages" class="topic-link">KDEF</a>,CK+, and databases with the images obtained â€˜in the wildâ€™ such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art </span></span></span><a href="/topics/computer-science/deep-learning" title="Learn more about deep learning algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning algorithms</a><span> such as Vgg16, GoogleNet, <a href="/topics/computer-science/residual-neural-network" title="Learn more about ResNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ResNet</a> and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis.</span></div></div></div></div><div id="reading-assistant-main-body-section"></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" href="/science/article/pii/S0925231220300837"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S0925231220300722"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="keys0001" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="key0001" class="keyword"><span id="cetext0002">Facial expression recognition</span></div><div id="key0002" class="keyword"><span id="cetext0003">Deep convolution network</span></div><div id="key0003" class="keyword"><span id="cetext0004">Mental health care</span></div><div id="key0004" class="keyword"><span id="cetext0005">Emotion analysis</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="sec0001"><h2 id="cesectitle0003" class="u-h4 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div class="u-margin-s-bottom" id="para0008">Understanding people's emotions plays an important role in daily human communications. For advanced human-computer interaction in many emerging applications, recognizing usersâ€™ emotions is also vital. Currently, there are many approaches for automated emotional recognition, including the recognition of facial expression and analysis of voice tone <a class="anchor anchor-primary" href="#bib0001" name="bbib0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0001"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a><span><span>, which exist alongside more conventional physiological measures such as measurements of blood pressure, <a href="/topics/biochemistry-genetics-and-molecular-biology/pulse-rate" title="Learn more about pulse rate from ScienceDirect's AI-generated Topic Pages" class="topic-link">pulse rate</a> or </span><a href="/topics/biochemistry-genetics-and-molecular-biology/skin-conductance" title="Learn more about skin conductivity from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin conductivity</a>.</span></div><div class="u-margin-s-bottom" id="para0009"><span>Automated <a href="/topics/biochemistry-genetics-and-molecular-biology/facial-recognition" title="Learn more about facial expression recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial expression recognition</a> has found many practical applications such as in e-learning and health care systems </span><a class="anchor anchor-primary" href="#bib0002" name="bbib0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0002"><span class="anchor-text-container"><span class="anchor-text">[2</span></span></a>,<a class="anchor anchor-primary" href="#bib0003" name="bbib0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0003"><span class="anchor-text-container"><span class="anchor-text">3]</span></span></a>. For example, facial expressions are considered as an important feedback mechanism for teachers in terms of monitoring levels of studentsâ€™ understanding <a class="anchor anchor-primary" href="#bib0003" name="bbib0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0003"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>. Within this domain, Mau-Tsuen et&nbsp;al. proposed an automatic system to identify how well students were learning, by means of the analysis of videos taken from learners <a class="anchor anchor-primary" href="#bib0003" name="bbib0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0003"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, and demonstrated that such a system could help improving teaching effectiveness and efficiency. Additionally, the user interface developed in such a system could work as a cognitive tool <a class="anchor anchor-primary" href="#bib0004" name="bbib0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0004"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a><span> to support and understand the usersâ€™ mental state better when and where it was appropriate without external <a href="/topics/earth-and-planetary-sciences/actuation" title="Learn more about actuation from ScienceDirect's AI-generated Topic Pages" class="topic-link">actuation</a><span>. Further applications of facial expression analysis include its use in the fields of human-computer interaction and interface optimization. Within these domains, Bahr et&nbsp;al. proposed a novel method to analyze postural and facial expressions to guide interface <a href="/topics/engineering/actuation" title="Learn more about actuation from ScienceDirect's AI-generated Topic Pages" class="topic-link">actuation</a> and actions </span></span><a class="anchor anchor-primary" href="#bib0004" name="bbib0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0004"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, whilst Pedro et&nbsp;al. employed electromyogram (EMG) sensors to investigate the relationship between usersâ€™ facial expressions and adverse-event occurrences <a class="anchor anchor-primary" href="#bib0005" name="bbib0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0005"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a><span><span>. Moreover, health and medical applications of automated facial expression analysis are also being investigated, for example in the area of diagnostics related to <a href="/topics/pharmacology-toxicology-and-pharmaceutical-science/developmental-disorder" title="Learn more about developmental disorders from ScienceDirect's AI-generated Topic Pages" class="topic-link">developmental disorders</a> such as </span><a href="/topics/neuroscience/pervasive-developmental-disorder" title="Learn more about autism from ScienceDirect's AI-generated Topic Pages" class="topic-link">autism</a><span>. Here, promising work has been conducted by means of a system of facial expression analysis during <a href="/topics/neuroscience/social-interaction" title="Learn more about social interactions from ScienceDirect's AI-generated Topic Pages" class="topic-link">social interactions</a>, where many individuals with such disorders find challenging </span></span><a class="anchor anchor-primary" href="#bib0006" name="bbib0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0006"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0010"><span><span>Modern techniques in facial expression recognition systems play important roles in these practical applications. These techniques typically involve multiple components, including <a href="/topics/neuroscience/face" title="Learn more about face from ScienceDirect's AI-generated Topic Pages" class="topic-link">face</a><span> localization and facial component alignment, <a href="/topics/computer-science/facial-feature" title="Learn more about facial feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial feature</a> extraction and facial feature classification. As a simple </span></span><a href="/topics/earth-and-planetary-sciences/dichotomy" title="Learn more about dichotomy from ScienceDirect's AI-generated Topic Pages" class="topic-link">dichotomy</a><span>, facial expression analysis systems can be divided into two groups: those using static images and those using continuous video frames. Static algorithms enjoy the advantage of facial expression recognition from a single image or a video frame. In this paper, we describe an algorithm which has been tested for facial expression <a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a><span><span> from single images mainly acquired through webcams and <a href="/topics/earth-and-planetary-sciences/mobile-phone" title="Learn more about mobile phone from ScienceDirect's AI-generated Topic Pages" class="topic-link">mobile phone</a> cameras. We would argue that static approaches show promising, but require rigorous and various testing conditions for robust outcomes. Deepak et&nbsp;al., for example, proposed a convolution </span><a href="/topics/neuroscience/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> based algorithm to recognize facial expression with high accuracy </span></span></span><a class="anchor anchor-primary" href="#bib0007" name="bbib0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0007"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a><span>. However, their algorithm was tested on only two small facial expression datasets. Conversely, Jie et&nbsp;al. also proposed three novel convolution <a href="/topics/chemical-engineering/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> (CNN) models with different architectures </span><a class="anchor anchor-primary" href="#bib0008" name="bbib0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0008"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a><span>, and tested their algorithms on several datasets such as CK+ and FER2013 datasets. Within the three architectures they presented, the data suggested that the pertained CNN with 5 <a href="/topics/computer-science/convolution-layer" title="Learn more about convolution layers from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution layers</a> had the best performance. It should be noted however that not all researchers restrict their work to 2D images. For instance, Chenlei et&nbsp;al. very recently proposed a new 3D facial expression modeling method based on facial landmarks </span><a class="anchor anchor-primary" href="#bib0009" name="bbib0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0009"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a>. On the other hand, other researchers have moved beyond the analysis of static images and proposed facial expression recognition systems using continuous video frames. For instance, Zia, Lee and other researchers worked on facial expression recognition using temporal dynamics <a class="anchor anchor-primary" href="#bib0010" name="bbib0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0010"><span class="anchor-text-container"><span class="anchor-text">[10</span></span></a>,<a class="anchor anchor-primary" href="#bib0011" name="bbib0011" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0011"><span class="anchor-text-container"><span class="anchor-text">11]</span></span></a><span>. Their proposed system used Fisher <a href="/topics/engineering/independent-component-analysis" title="Learn more about Independent Component Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Independent Component Analysis</a><span> as a feature extractor and <a href="/topics/neuroscience/hidden-markov-model" title="Learn more about Hidden Markov Models from ScienceDirect's AI-generated Topic Pages" class="topic-link">Hidden Markov Models</a> to learn the features of six different expressions. The experiment results showed that the recognition rate was about 92.85%.</span></span></div><div class="u-margin-s-bottom" id="para0011"><span><span>Despite advances in the recognition accuracy, there remain significant issues to be addressed in the field of facial expression recognition systems. The first issue is related to the datasets employed in testing such systems. Some facial expression recognition systems may have <a href="/topics/computer-science/good-performance" title="Learn more about good performance from ScienceDirect's AI-generated Topic Pages" class="topic-link">good performance</a> in some image datasets, but perform poorly in the others. For instance, deep </span><a href="/topics/engineering/neural-network-approach" title="Learn more about CNN approaches from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN approaches</a> often need to determine large amount of weights in the training phase. Consequently it is observed that such approaches suffer from performance decrements when being trained on a small image dataset </span><a class="anchor anchor-primary" href="#bib0008" name="bbib0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0008"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a><span>. A second important issue is ecological validity, in that most of the existing systems use lab-posed facial expressions images. This is potentially problematic, as it ignores real-world problems such as <a href="/topics/computer-science/lighting-condition" title="Learn more about lighting conditions from ScienceDirect's AI-generated Topic Pages" class="topic-link">lighting conditions</a><span>, <a href="/topics/biochemistry-genetics-and-molecular-biology/image-quality" title="Learn more about image quality from ScienceDirect's AI-generated Topic Pages" class="topic-link">image quality</a> and background complexity. It is fatal to address such issues for real-world applications. Facial expression recognition in the wild is a challenging topic due to such issue as well as others such as variance in poses </span></span><a class="anchor anchor-primary" href="#bib0008" name="bbib0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0008"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a><span><span><span>. Thirdly, both traditional approaches and <a href="/topics/chemical-engineering/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> based approaches have inherent </span><a href="/topics/pharmacology-toxicology-and-pharmaceutical-science/weakness" title="Learn more about weaknesses from ScienceDirect's AI-generated Topic Pages" class="topic-link">weaknesses</a>. Traditional approaches such as </span><a href="/topics/engineering/local-binary-pattern" title="Learn more about Local Binary Pattern from ScienceDirect's AI-generated Topic Pages" class="topic-link">Local Binary Pattern</a><span><span> (LBP), <a href="/topics/computer-science/scale-invariant-feature-transform" title="Learn more about Scale Invariant Feature Transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Scale Invariant Feature Transform</a> (SIFT) and </span><a href="/topics/computer-science/support-vector-machine" title="Learn more about Support Vector Machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Support Vector Machine</a> (SVM) employ manually designed features, which can result in poor performance with unseen images. Deniz et&nbsp;al., for instance, observed that traditional approaches performed weakly when being presented with variations in pose, and instead employed a deep CNN based approach to recognize facial expressions, which they argued resulted in improved performance </span></span><a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a>. Approaches such as AlexNet, Vgg, and GoogleNet have long training and testing time as well as an enormous amount of memory resource <a class="anchor anchor-primary" href="#bib0013" name="bbib0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0013"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a><span><span> and require hardware incorporating <a href="/topics/engineering/graphics-processing-unit" title="Learn more about Graphics Processing Units from ScienceDirect's AI-generated Topic Pages" class="topic-link">Graphics Processing Units</a> (GPU's), whilst it is also essential in these approaches. Extensive literature provides a comprehensive review of the relative advantages and weaknesses of the existing facial </span><a href="/topics/biochemistry-genetics-and-molecular-biology/gene-expression-system" title="Learn more about expression systems from ScienceDirect's AI-generated Topic Pages" class="topic-link">expression systems</a>, we would recommend those seeking further information to consult one of the several good reviews of the area (see </span><a class="anchor anchor-primary" href="#bib0014" name="bbib0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0014"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>, <a class="anchor anchor-primary" href="#bib0015" name="bbib0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0015"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a>, <a class="anchor anchor-primary" href="#bib0016" name="bbib0016" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0016"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, <a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a>, <a class="anchor anchor-primary" href="#bib0018" name="bbib0018" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0018"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a><span>). Indeed, even more recently Byoung, reviewed approaches to facial emotion recognition including conventional facial expression recognition, <a href="/topics/computer-science/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> based facial expression recognition, well-known facial expression datasets and has also examined some common performance evaluation methods for automated facial expression recognition </span><a class="anchor anchor-primary" href="#bib0019" name="bbib0019" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0019"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0012"><span>Whilst there are many notable developments within the field of automatic face recognition systems, there remain urgent priorities to be addressed to allow these technologies to flourish within the field of mental health care. We would argue that there is vast untapped potential for the field in this area of healthcare. Globally, there are rising numbers of people suffering from <a href="/topics/pharmacology-toxicology-and-pharmaceutical-science/cognitive-defect" title="Learn more about cognitive impairment from ScienceDirect's AI-generated Topic Pages" class="topic-link">cognitive impairment</a>, and consequently there is an urgent need to develop and deploy user-friendly, low-cost and effective facial expression analysis systems for mental health care. Within mental health care, there is enormous potential for automatic facial expression recognition systems to assist clinicians working within mental health settings. Whilst it is uncontroversial to suggest that facial expressions can reflect people's mental states </span><a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>, it has also been reported that the patients with cognitive impairments may express abnormal facial expressions <a class="anchor anchor-primary" href="#bib0021" name="bbib0021" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0021"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a><span> that are quantitatively different from those of healthy elderly people. Burton and Kaszniak, for example, reported abnormal corrugator activity in individuals with <a href="/topics/neuroscience/cognitive-disorders" title="Learn more about cognitive impairment from ScienceDirect's AI-generated Topic Pages" class="topic-link">cognitive impairment</a> when compared to a control group, when these participants were exposed to image or video stimuli </span><a class="anchor anchor-primary" href="#bib0021" name="bbib0021" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0021"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a>. The forehead muscle is integral to the emitted emotions related to frowning <a class="anchor anchor-primary" href="#bib0022" name="bbib0022" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0022"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a><span>. Work in line with this was conducted by Henry et&nbsp;al., who invited 20 cognitive impaired people and 20 <a href="/topics/biochemistry-genetics-and-molecular-biology/normal-human" title="Learn more about healthy people from ScienceDirect's AI-generated Topic Pages" class="topic-link">healthy people</a> to watch videos in order to compare their facial reactions when viewing such stimuli, reporting that the group of participants with cognitive impairment demonstrated difficulty both in facial muscle control and in the amplification of expressed emotion </span><a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a><span>. Similarly, Smithfound cognitive impaired people demonstrated more <a href="/topics/engineering/negative-emotion" title="Learn more about negative emotions from ScienceDirect's AI-generated Topic Pages" class="topic-link">negative emotions</a> when they were exposed to negative image stimuli, which they argued was indicative of reduced emotional control </span><a class="anchor anchor-primary" href="#bib0024" name="bbib0024" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0024"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a><span>. Building upon such initial findings, this paper presents a proposed system that has the potential to be applied to the detection and monitoring of cognitive impairments such as dementia through the application of <a href="/topics/engineering/machine-learning-technique" title="Learn more about machine learning techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning techniques</a><span> to the <a href="/topics/biochemistry-genetics-and-molecular-biology/autoanalysis" title="Learn more about automatic analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">automatic analysis</a> of people's facial expressions. Our proposed smart system is able to label and quantify the usersâ€™ emotions automatically, through continuous focus upon the evolution of facial expressions over a period of time.</span></span></div><div class="u-margin-s-bottom" id="para0013"><span>In order to develop an efficient and effective framework for recognizing facial expression towards mental health care, this paper presents a facial expression recognition framework which effectively exploits the features extracted from the Fully Connected Layer 6 of AlexNet and the <a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about Linear Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Linear Discriminant Analysis</a><span><span> Classifier (LDA). We present the findings from a series of experiments where the performance of our proposed method are compared to that of traditional approaches such as SVM, LDA, the K-nearest <a href="/topics/engineering/neighbor-classifier" title="Learn more about Neighbor Classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">Neighbor Classifier</a> (KNN) and made further comparisons to </span><a href="/topics/engineering/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a><span> based approaches such as AlexNet, Vgg, GoogleNet and <a href="/topics/computer-science/residual-neural-network" title="Learn more about ResNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ResNet</a>. Within our comparisons, we have employed a broad range of well-known facial expression databases including the Karolinska Directed Emotional Faces (KDEF) Database </span></span></span><a class="anchor anchor-primary" href="#bib0025" name="bbib0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0025"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>, the Japanese Female Facial Expression (JAFFE) Database <a class="anchor anchor-primary" href="#bib0026" name="bbib0026" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0026"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a>, the extended Cohn-Kanade&nbsp;dataset&nbsp;(CK+) <a class="anchor anchor-primary" href="#bib0027" name="bbib0027" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0027"><span class="anchor-text-container"><span class="anchor-text">[27</span></span></a>,<a class="anchor anchor-primary" href="#bib0028" name="bbib0028" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0028"><span class="anchor-text-container"><span class="anchor-text">28]</span></span></a>, the Facial Expression Recognition Challenge (FER2013) <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a> and the AffectNet Database <a class="anchor anchor-primary" href="#bib0030" name="bbib0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0030"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a><span>. From our experiments it would suggest that exploiting the features extracted from the Fully Connected Layer 6 of AlexNet with the LDA classifier can achieve the best performance in all the five <a href="/topics/engineering/testing-database" title="Learn more about testing databases from ScienceDirect's AI-generated Topic Pages" class="topic-link">testing databases</a>.</span></div><div class="u-margin-s-bottom" id="para0014">The major contributions of this paper are as follows:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1.</span><span><div class="u-margin-s-bottom" id="para0015"><span>First, our novel framework for facial expression analysis to support mental health care is presented. Our proposed system extracts deep features from the Fully Connected Layer 6 of the AlexNet, and uses a standard Linear <a href="/topics/earth-and-planetary-sciences/discriminant-analysis" title="Learn more about Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">Discriminant Analysis</a> Classifier to train these deep features. As it is known that patients with cognitive impairments may express abnormal facial expressions when exposed to emotional </span><a href="/topics/biochemistry-genetics-and-molecular-biology/visual-stimulation" title="Learn more about visual stimuli from ScienceDirect's AI-generated Topic Pages" class="topic-link">visual stimuli</a>, we would argue that our proposed facial analysis system has excellent potential to detect cognitive impairment at the early stage.</div></span></li><li class="react-xocs-list-item"><span class="list-label">2.</span><span><div class="u-margin-s-bottom" id="para0016">We present the findings from the tests of our proposed framework against both across databases with small number of images such as the JAFFE, KDEF and CK+ databases, and databases with images obtained â€˜in the wildâ€™ such as the FER2013 and the AffectNet databases.</div></span></li><li class="react-xocs-list-item"><span class="list-label">3.</span><span><div class="u-margin-s-bottom" id="para0017">We present the findings from the comparisons of our proposed method with both traditional methods and state-of-the-art methods proposed by other researchers. It is observed that our method has better accuracy facial expression recognition. More importantly, we have also observed that our proposed method has much less computing time and lower device requirements than the other state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, and ResNet. We would argue that these characteristics support our view that the proposed method is both competent and suitable for a facial analysis system that can be employed within mental health care settings.</div></span></li><li class="react-xocs-list-item"><span class="list-label">4.</span><span><div class="u-margin-s-bottom" id="para0018">A system which can analyze facial expressions from video stimuli, and subsequently produce an accurate evaluation of the facial expressions detected in an automated system is presented.</div></span></li></ul></div><div class="u-margin-s-bottom" id="para0019">This paper is organized as follows. Following our general overview in <a class="anchor anchor-primary" href="#sec0001" name="bsec0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0001"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;1</span></span></a>, we present our proposed deep learning-based framework in <a class="anchor anchor-primary" href="#sec0002" name="bsec0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0002"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;2</span></span></a>. <a class="anchor anchor-primary" href="#sec0010" name="bsec0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0010"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;3</span></span></a> then introduces the experimental set-up for the evaluation of the proposed system to obtain the results. Our findings are then discussed within <a class="anchor anchor-primary" href="#sec0018" name="bsec0018" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0018"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;4</span></span></a>. Finally, <a class="anchor anchor-primary" href="#sec0019" name="bsec0019" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0019"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;5</span></span></a> provides our conclusions.</div></section><section id="sec0002"><h2 id="cesectitle0004" class="u-h4 u-margin-l-top u-margin-xs-bottom">2. Materials and methods</h2><section id="sec0003"><h3 id="cesectitle0005" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Overview of the whole system structure</h3><div class="u-margin-s-bottom"><div id="para0020"><span><span><span>In the current research, the inputs to the system are videos of people's frontal <a href="/topics/neuroscience/face" title="Learn more about faces from ScienceDirect's AI-generated Topic Pages" class="topic-link">faces</a>. Image pre-processing is then applied to the video streams. The analysis of the acquired videos is carried out using the deep convolution network AlexNet combined with traditional classifiers such as </span><a href="/topics/computer-science/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span>, <a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about LDA from ScienceDirect's AI-generated Topic Pages" class="topic-link">LDA</a> and </span></span><a href="/topics/computer-science/k-nearest-neighbor-classifier" title="Learn more about KNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN</a>. Finally, the system we present analyzes the emotions acquired in the videos and reports the evolutions of the emotions detected over a period of time. The general structure of the system is shown in </span><a class="anchor anchor-primary" href="#fig0001" name="bfig0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0001"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a>.</div><figure class="figure text-xs" id="fig0001"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr1.jpg" height="978" alt="Fig 1" aria-describedby="cap0001"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (960KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (960KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0001"><p id="spara001"><span class="label">Fig. 1</span>. Structure of the whole system <a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a>.</p></span></span></figure></div><div class="u-margin-s-bottom" id="para0021">In the system, the first stage is the system input. Here we take video frames of facial expressions from 120&nbsp;s of video with a rate of 30 frames per second. As a result, a 120-second video is thus converted to 3600 images. The image pre-processing technique can then be applied to such input image frames. This stage has two main operations: removal of unnecessary image parts such as background environmental aspects of the image, and removal of hair; and resizing of the images. Within our approach, firstly a face detector is used to locate the position of the face in the images using the standard Viola-Jones algorithm <a class="anchor anchor-primary" href="#bib0031" name="bbib0031" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0031"><span class="anchor-text-container"><span class="anchor-text">[31</span></span></a>,<a class="anchor anchor-primary" href="#bib0032" name="bbib0032" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0032"><span class="anchor-text-container"><span class="anchor-text">32]</span></span></a><span>. Next, the appropriate face part is cropped from the images. Finally, the cropped face part is resized to the required size, which is decided by the input of the <a href="/topics/chemical-engineering/deep-learning" title="Learn more about deep learning network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning network</a>. In our proposed system, we utilize well-established and reliable AlexNet to extract the deep features from the images which have been applied with the aforementioned image pre-processing techniques.</span></div><div class="u-margin-s-bottom" id="para0022">Finally, at this stage the image will be also converted into RGB format by concatenating the arrays <a class="anchor anchor-primary" href="#bib0033" name="bbib0033" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0033"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a><span> to meet the requirement of the <a href="/topics/computer-science/deep-learning" title="Learn more about deep learning network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning network</a><span> if the video frames are <a href="/topics/engineering/grayscale" title="Learn more about grayscale from ScienceDirect's AI-generated Topic Pages" class="topic-link">grayscale</a>.</span></span></div><div class="u-margin-s-bottom" id="para0023"><span>In the third stage, facial expression analysis techniques are applied to the pre-processed images. These techniques use a combination of deep learning networks i.e. AlexNet, and traditional classifiers such as <a href="/topics/engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span><span>, <a href="/topics/earth-and-planetary-sciences/discriminant-analysis" title="Learn more about LDA from ScienceDirect's AI-generated Topic Pages" class="topic-link">LDA</a> and </span><a href="/topics/biochemistry-genetics-and-molecular-biology/k-nearest-neighbor" title="Learn more about KNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN</a>, which will be introduced in detail in the next section. Facial expressions in the video stimuli are classified into five facial expression groups, namely neutral, happy, sad, angry and surprised. In addition, the probability of each facial expression category for every frame is established. By combining all the </span></span><a href="/topics/biochemistry-genetics-and-molecular-biology/facial-recognition" title="Learn more about facial expression recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial expression recognition</a> results in the image frames, we obtain the evolution of the probability of each type of facial expression over a period of time.</div></section><section id="sec0004"><h3 id="cesectitle0006" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Convolution neural networks and proposed framework</h3><section id="sec0005"><h4 id="cesectitle0007" class="u-margin-m-top u-margin-xs-bottom">2.2.1. Overview of convolution neural networks</h4><div class="u-margin-s-bottom" id="para0024"><span><span>Convolution Neural Networks (CNNs) are a type of <a href="/topics/engineering/deep-learning" title="Learn more about deep learning network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning network</a> that needs less image pre-processing compared to other traditional </span><a href="/topics/computer-science/image-classification" title="Learn more about image classification from ScienceDirect's AI-generated Topic Pages" class="topic-link">image classification</a> algorithms </span><a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a><span>. CNNs have an advantage in that they do not need prior knowledge and manually design the features. They have many applications in various domains including <a href="/topics/neuroscience/natural-language-processing" title="Learn more about natural language processing from ScienceDirect's AI-generated Topic Pages" class="topic-link">natural language processing</a><span> and <a href="/topics/engineering/computervision" title="Learn more about computer vision from ScienceDirect's AI-generated Topic Pages" class="topic-link">computer vision</a> </span></span><a class="anchor anchor-primary" href="#bib0036" name="bbib0036" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0036"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a><span>. A typical CNN has an input layer, an output layer, and hidden layers such as <a href="/topics/computer-science/convolution-layer" title="Learn more about convolution layers from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution layers</a>, pooling layers and fully connected layers.</span></div></section><section id="sec0006"><h4 id="cesectitle0008" class="u-margin-m-top u-margin-xs-bottom">2.2.2. AlexNet</h4><div class="u-margin-s-bottom"><div id="para0025">AlexNet is a type of CNN that was designed by Alex Krizhevsky in the SuperVision group <a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>. The AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge in 2012, and achieved a top-five error of 15.3%. It has eight major layers in total including five convolution layers and three fully connected layers. The detailed network structure is shown in <a class="anchor anchor-primary" href="#fig0002" name="bfig0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0002"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;2</span></span></a>. The original AlexNet was trained on a subset of the ImageNet database which contains more than one million images, and was able to classify images into 1000 object categories <a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a><span>. In the AlexNet, the input receives images and the output includes the label of the images and the probabilities for each of the object categories. In our experiments, the AlexNet is run in Matlab. Moreover, the <a href="/topics/biochemistry-genetics-and-molecular-biology/transfer-of-learning" title="Learn more about transfer learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">transfer learning</a> strategy is used to reduce the training time of the network </span><a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a><span>. By using the <a href="/topics/engineering/transfer-learning" title="Learn more about transfer learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">transfer learning</a><span>, a much smaller number of training images are needed. There are several steps needed for <a href="/topics/computer-science/transfer-learning" title="Learn more about transfer learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">transfer learning</a> in a pre-trained AlexNet.</span></span></div><figure class="figure text-xs" id="fig0002"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr2.jpg" height="210" alt="Fig 2" aria-describedby="cap0002"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (317KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (317KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0002"><p id="spara002"><span class="label">Fig. 2</span>. Structure of the AlexNet <a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34</span></span></a>,<a class="anchor anchor-primary" href="#bib0061" name="bbib0061" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0061"><span class="anchor-text-container"><span class="anchor-text">61]</span></span></a>.</p></span></span></figure></div><div class="u-margin-s-bottom" id="para0026"><span><span>There are 25 layers in the AlexNet, which begins with an image input layer. Next, there are five convolution layers to extract <a href="/topics/computer-science/facial-feature" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a>. The output of the </span><a href="/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages" class="topic-link">activation function</a> forms the neurons of the current layer. As a result, it will form the feature map of the current convolution layer. The calculation can be described in the following function </span><a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>, <a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>:<span class="display"><span id="eqn0001" class="formula"><span class="label">(1)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>l</mi></msubsup><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><mi is=&quot;true&quot;>F</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>&amp;#x2208;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>M</mi><mi is=&quot;true&quot;>j</mi></msub></mrow></munder><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>i</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>l</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></msubsup><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>*</mo><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mi is=&quot;true&quot;>j</mi></mrow><mi is=&quot;true&quot;>l</mi></msubsup><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>+</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mi is=&quot;true&quot;>b</mi></msub></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="35.359ex" height="4.625ex" viewBox="0 -1244 15223.9 1991.2" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"></g><g is="true" transform="translate(280,0)"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(1777,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true"></g><g is="true" transform="translate(3114,0)"><use xlink:href="#MJMATHI-46"></use></g><g is="true" transform="translate(4030,0)"><use xlink:href="#MJSZ2-28" is="true"></use><g is="true" transform="translate(597,0)"><g is="true"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true" transform="translate(716,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-4D"></use></g><g is="true" transform="translate(686,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-6A"></use></g></g></g></g><g is="true" transform="translate(3002,0)"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(211,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(761,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(5300,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(6022,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(716,-304)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g><g is="true" transform="translate(7597,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true"></g><g is="true" transform="translate(8878,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use></g></g></g><use xlink:href="#MJSZ2-29" is="true" x="10595" y="-1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mspace width="0.28em" is="true"></mspace><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><munder is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">âˆˆ</mo><msub is="true"><mi is="true">M</mi><mi is="true">j</mi></msub></mrow></munder><msubsup is="true"><mi is="true">X</mi><mi is="true">i</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo linebreak="badbreak" is="true">*</mo><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">w</mi><mi is="true">b</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-12"><math><mrow is="true"><mspace width="0.28em" is="true"></mspace><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><munder is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">âˆˆ</mo><msub is="true"><mi is="true">M</mi><mi is="true">j</mi></msub></mrow></munder><msubsup is="true"><mi is="true">X</mi><mi is="true">i</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo linebreak="badbreak" is="true">*</mo><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">w</mi><mi is="true">b</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span>Where, <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>l</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></msubsup></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.82ex" height="3.586ex" viewBox="0 -995.6 2075.2 1544.1" role="img" focusable="false" style="vertical-align: -1.274ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(211,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(761,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup></math></span></span><script type="math/mml" id="MathJax-Element-13"><math><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup></math></script></span> is the feature map of the output of the <span class="small-caps">L</span><span>-1 layer, * depicts the <a href="/topics/computer-science/convolution-operation" title="Learn more about convolution operation from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution operation</a>, </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mi is=&quot;true&quot;>j</mi></mrow><mi is=&quot;true&quot;>l</mi></msubsup></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.141ex" height="3.471ex" viewBox="0 -945.9 1352.5 1494.4" role="img" focusable="false" style="vertical-align: -1.274ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(716,-304)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup></math></span></span><script type="math/mml" id="MathJax-Element-14"><math><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup></math></script></span> and <em>w<sub>b</sub></em>represents weight and the bias, respectively. Each convolution layer is followed by the ReLU (Rectified Linear Units) layer in order to increase the nonlinear properties of the network. The ReLU is a half-wave rectifier function with the advantage of reducing the training time whilst preventing overfitting <a class="anchor anchor-primary" href="#bib0040" name="bbib0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0040"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a><span>. In addition, ReLU layers can prevent the gradient vanishing problem and are much faster than other <a href="/topics/engineering/logistic-function" title="Learn more about logistic function from ScienceDirect's AI-generated Topic Pages" class="topic-link">logistic function</a> </span><a class="anchor anchor-primary" href="#bib0041" name="bbib0041" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0041"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a>. The ReLU layers for input <em>x</em> can be described as <a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>, <a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>:<span class="display"><span id="eqn0002" class="formula"><span class="label">(2)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>F</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>x</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mi is=&quot;true&quot;>m</mi><mi is=&quot;true&quot;>a</mi><mi is=&quot;true&quot;>x</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>0</mn><mo is=&quot;true&quot;>,</mo><mi is=&quot;true&quot;>x</mi></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="18.688ex" height="2.779ex" viewBox="0 -846.5 8046.1 1196.3" role="img" focusable="false" style="vertical-align: -0.812ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-46"></use></g><g is="true" transform="translate(916,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(962,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2545,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3601,0)"><use xlink:href="#MJMATHI-6D"></use></g><g is="true" transform="translate(4480,0)"><use xlink:href="#MJMATHI-61"></use></g><g is="true" transform="translate(5009,0)"><use xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(5748,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMAIN-30"></use></g><g is="true" transform="translate(500,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(945,0)"><use xlink:href="#MJMATHI-78"></use></g></g><g is="true" transform="translate(1907,0)"><use xlink:href="#MJMAIN-29"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">x</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mi is="true">x</mi></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-15"><math><mrow is="true"><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">x</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mi is="true">m</mi><mi is="true">a</mi><mi is="true">x</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mi is="true">x</mi></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span></div><div class="u-margin-s-bottom" id="para0027">In addition, convolution layers are also followed by the max pooling layers. The max pooling layers are used for down-sampling. The number of feature maps will not' be changed by the down-sampling process. On the other hand, the down-sampling process removes unnecessary information and reduces the number of the parameters of the feature map. The down-sampling layer can be described by Krizhevsky et&nbsp;al. <a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>, <a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>:<span class="display"><span id="eqn0003" class="formula"><span class="label">(3)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>l</mi></msubsup><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><mi is=&quot;true&quot;>F</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>d</mi><mi is=&quot;true&quot;>o</mi><mi is=&quot;true&quot;>w</mi><mi is=&quot;true&quot;>n</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>l</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></msubsup><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>+</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><msub is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mi is=&quot;true&quot;>b</mi></msub></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="30.963ex" height="4.625ex" viewBox="0 -1244 13331.4 1991.2" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(1497,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true"></g><g is="true" transform="translate(2834,0)"><use xlink:href="#MJMATHI-46"></use></g><g is="true" transform="translate(3750,0)"><use xlink:href="#MJSZ2-28" is="true"></use><g is="true" transform="translate(597,0)"><g is="true"><use xlink:href="#MJMATHI-64"></use></g><g is="true" transform="translate(523,0)"><use xlink:href="#MJMATHI-6F"></use></g><g is="true" transform="translate(1009,0)"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(1725,0)"><use xlink:href="#MJMATHI-6E"></use></g><g is="true" transform="translate(2492,0)"><use xlink:href="#MJSZ2-28" is="true"></use><g is="true" transform="translate(597,0)"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(211,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(761,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><use xlink:href="#MJSZ2-29" is="true" x="2672" y="-1"></use></g><g is="true" transform="translate(5985,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true"></g><g is="true" transform="translate(7265,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use></g></g></g><use xlink:href="#MJSZ2-29" is="true" x="8983" y="-1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">d</mi><mi is="true">o</mi><mi is="true">w</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">w</mi><mi is="true">b</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-16"><math><mrow is="true"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">d</mi><mi is="true">o</mi><mi is="true">w</mi><mi is="true">n</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">w</mi><mi is="true">b</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span>where <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>l</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></msubsup></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.82ex" height="3.586ex" viewBox="0 -995.6 2075.2 1544.1" role="img" focusable="false" style="vertical-align: -1.274ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(211,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(761,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup></math></span></span><script type="math/mml" id="MathJax-Element-17"><math><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup></math></script></span> represents the <em>j</em> feature map of the pooling layer <em>l</em>, and <em>w<sub>b</sub></em> depicts the offset term of the down-sampling layer.</div><div class="u-margin-s-bottom" id="para0028"><span>Finally, in the AlexNet, after the five convolution layers, there are three fully connected layers: FC6, FC7 and FC8. The fully connected layers can be considered as a convolution layer. Also, its <a href="/topics/computer-science/convolution-kernel" title="Learn more about convolution kernel from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution kernel</a> size and the input data size should be consistent with those used in the convolution layer. The Fully Connected Layer can be described by Krizhevsky et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>, <a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>:<span class="display"><span id="eqn0004" class="formula"><span class="label">(4)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mi is=&quot;true&quot;>l</mi></msubsup><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><mi is=&quot;true&quot;>F</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mi is=&quot;true&quot;>i</mi></munder><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mi is=&quot;true&quot;>j</mi></mrow><mi is=&quot;true&quot;>l</mi></msubsup><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mi is=&quot;true&quot;>j</mi><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>l</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mn is=&quot;true&quot;>1</mn></mrow></msubsup><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>+</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><msubsup is=&quot;true&quot;><mi is=&quot;true&quot;>w</mi><mi is=&quot;true&quot;>b</mi><mi is=&quot;true&quot;>l</mi></msubsup><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="29.18ex" height="4.625ex" viewBox="0 -1244 12563.7 1991.2" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(1497,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true"></g><g is="true" transform="translate(2834,0)"><use xlink:href="#MJMATHI-46"></use></g><g is="true" transform="translate(3750,0)"><use xlink:href="#MJSZ2-28" is="true"></use><g is="true" transform="translate(597,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(2164,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(716,-304)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g></g><g is="true" transform="translate(3517,0)"><g is="true"><use xlink:href="#MJMATHI-58"></use></g><g is="true" transform="translate(859,403)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(211,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(761,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g><g is="true" transform="translate(828,-304)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6A"></use></g></g><g is="true" transform="translate(5814,0)"><use xlink:href="#MJMAIN-2B"></use></g><g is="true"></g><g is="true" transform="translate(7095,0)"><g is="true"><use xlink:href="#MJMATHI-77"></use></g><g is="true" transform="translate(716,353)"><use transform="scale(0.707)" xlink:href="#MJMATHI-6C"></use></g><g is="true" transform="translate(716,-327)"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use></g></g><use xlink:href="#MJSZ2-29" is="true" x="8215" y="-1"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><munder is="true"><mo is="true">âˆ‘</mo><mi is="true">i</mi></munder><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msubsup is="true"><mi is="true">w</mi><mi is="true">b</mi><mi is="true">l</mi></msubsup><mo stretchy="true" is="true">)</mo></mrow></mrow></math></span></span><script type="math/mml" id="MathJax-Element-18"><math><mrow is="true"><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mi is="true">l</mi></msubsup><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mi is="true">F</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><munder is="true"><mo is="true">âˆ‘</mo><mi is="true">i</mi></munder><msubsup is="true"><mi is="true">w</mi><mrow is="true"><mi is="true">i</mi><mi is="true">j</mi></mrow><mi is="true">l</mi></msubsup><msubsup is="true"><mi is="true">X</mi><mi is="true">j</mi><mrow is="true"><mi is="true">l</mi><mo is="true">âˆ’</mo><mn is="true">1</mn></mrow></msubsup><mo linebreak="badbreak" is="true">+</mo><mspace width="0.28em" is="true"></mspace><msubsup is="true"><mi is="true">w</mi><mi is="true">b</mi><mi is="true">l</mi></msubsup><mo stretchy="true" is="true">)</mo></mrow></mrow></math></script></span></span></span></div></section><section id="sec0007"><h4 id="cesectitle0009" class="u-margin-m-top u-margin-xs-bottom">2.2.3. Deep feature extraction</h4><div class="u-margin-s-bottom" id="para0029"><span>In this context, the AlexNet works as a deep feature extractor to extract <a href="/topics/computer-science/image-feature" title="Learn more about image features from ScienceDirect's AI-generated Topic Pages" class="topic-link">image features</a><span><span>. We then use these features to train traditional classifiers such as <a href="/topics/neuroscience/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span>, LDA and <a href="/topics/engineering/neighbor-classifier" title="Learn more about KNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN</a>. In spite of the possible improvement in </span></span><a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a>, feature extraction remains fast and relatively simple, using the representational power of the pre-trained deep networks </span></span><a class="anchor anchor-primary" href="#bib0042" name="bbib0042" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0042"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a>. In addition, as feature extraction only requires a single pass through the data, a CPU is also able to do this work. In this work, a pre-trained CNN works as the deep feature extractor. The advantages of using a pre-trained CNN to extract deep features, compared to the training of a new CNN, are: (1) Less computational power is needed and (2) less data is needed to achieve high recognition accuracy <a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>. The work we present used a pre-trained AlexNet, which was trained with more than a million images so that the network model has learned rich feature representations.</div><div class="u-margin-s-bottom"><div id="para0030">To demonstrate what features can be learned by the AlexNet, we need to visualize the deep features extracted by the network. In our work, we employed the T-SNE (Stochastic Neighbor Embedding) approach to visualize the features extracted by the AlexNet. The T-SNE - (TSNE) is an algorithm for dimensionality reduction <a class="anchor anchor-primary" href="#bib0044" name="bbib0044" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0044"><span class="anchor-text-container"><span class="anchor-text">[44</span></span></a>,<a class="anchor anchor-primary" href="#bib0045" name="bbib0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0045"><span class="anchor-text-container"><span class="anchor-text">45]</span></span></a><span>. This algorithm allows us to visualize the high-dimensional data of the <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a><span>. The T-SNE function will convert high-dimensional data into low <a href="/topics/computer-science/dimensional-data" title="Learn more about dimensional data from ScienceDirect's AI-generated Topic Pages" class="topic-link">dimensional data</a>. Generally, distant points in high-dimensional space will be converted into distant embedded low-dimensional points and nearby points in the high-dimensional space will be converted into nearby embedded low-dimensional points. As a result, we can visualize the low-dimensional points to find the clusters in the original high-dimensional data. The features extracted by the AlexNet which have transfer-learned facial images from the KDEF dataset are shown in </span></span><a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a>.</div><figure class="figure text-xs" id="fig0003"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr3.jpg" height="311" alt="Fig 3" aria-describedby="cap0003"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (238KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (238KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0003"><p id="spara003"><span class="label">Fig. 3</span>. Using the T-SNE to visualize the features extracted by the AlexNet.</p></span></span></figure></div></section><section id="sec0008"><h4 id="cesectitle0010" class="u-margin-m-top u-margin-xs-bottom">2.2.4. Layer selection</h4><div class="u-margin-s-bottom" id="para0031">In this study, we aimed to achieve the best performance by selecting the most appropriate layer to extract the features including the Fully Connected Layer 6 (FC6), Fully Connected Layer 7 (FC7) and Fully Connected Layer 8 (FC8) from the AlexNet. While extracting the features from deep CNNs, deeper layers contain higher level features and earlier layers produce lower-level features <a class="anchor anchor-primary" href="#bib0042" name="bbib0042" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0042"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a>. In order to obtain the feature representations of the training and testing images, we can use the activations&nbsp;on FC7 or we can obtain lower-level representations of the images from FC6. Meanwhile, the earlier layers may learn features like colors and edges <a class="anchor anchor-primary" href="#bib0046" name="bbib0046" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0046"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a>. However, in the deeper layers, the network may learn complicated features such as eyes. We will do experiments to explore, evaluate and compare which layer in the AlexNet is most appropriate to extract the deep features in our study.</div></section><section id="sec0009"><h4 id="cesectitle0011" class="u-margin-m-top u-margin-xs-bottom">2.2.5. Traditional classifiers</h4><div class="u-margin-s-bottom" id="para0032"><span>The deep features or â€˜activationsâ€™ can provide the input of traditional classifiers such as <a href="/topics/earth-and-planetary-sciences/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a><span><span>, LDA and KNN in order to further improve the recognition accuracy. Therefore, we also investigated which traditional <a href="/topics/computer-science/combined-classifier" title="Learn more about classifier combined from ScienceDirect's AI-generated Topic Pages" class="topic-link">classifier combined</a> with the AlexNet can achieve the best recognition accuracy. Each of the traditional classifiers has its own characteristics. For example, </span><a href="/topics/chemical-engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> can use kernels to transform many feature representations into a higher dimensional space in order to classify multiple classes </span></span><a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a><span><span>. In addition, <a href="/topics/biochemistry-genetics-and-molecular-biology/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> has </span><a href="/topics/computer-science/good-performance" title="Learn more about good performance from ScienceDirect's AI-generated Topic Pages" class="topic-link">good performance</a> in object classification and face detection applications.</span></div><div class="u-margin-s-bottom" id="para0033">On the other hand, the LDA approach is able to find the optimal transformation which can better separate different classes <a class="anchor anchor-primary" href="#bib0047" name="bbib0047" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0047"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a><span>. The LDA has wide applications such as face recognition and <a href="/topics/computer-science/image-retrieval" title="Learn more about image retrieval from ScienceDirect's AI-generated Topic Pages" class="topic-link">image retrieval</a> </span><a class="anchor anchor-primary" href="#bib0048" name="bbib0048" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0048"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a>. In the LDA, when <em>W</em> represents an optimal set of discriminant projection vectors <a class="anchor anchor-primary" href="#bib0048" name="bbib0048" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0048"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a>, the LDA can be represented as a function ofâ€‰<em>W</em>:<span class="display"><span id="eqn0005" class="formula"><span class="label">(5)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>J</mi><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>W</mi><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>|</mo><mrow is=&quot;true&quot;><msup is=&quot;true&quot;><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>T</mi></msup><msub is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mi is=&quot;true&quot;>B</mi></msub><mi is=&quot;true&quot;>W</mi></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>|</mo></mrow><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>|</mo><mrow is=&quot;true&quot;><msup is=&quot;true&quot;><mi is=&quot;true&quot;>W</mi><mi is=&quot;true&quot;>T</mi></msup><msub is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mi is=&quot;true&quot;>W</mi></msub><mi is=&quot;true&quot;>W</mi></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>|</mo></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="18.47ex" height="5.317ex" viewBox="0 -1393 7952.2 2289.2" role="img" focusable="false" style="vertical-align: -2.082ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4A"></use></g><g is="true" transform="translate(800,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(1438,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(2905,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true"></g><g is="true" transform="translate(3963,0)"><g transform="translate(397,0)"><rect stroke="none" width="3470" height="60" x="0" y="220"></rect><g is="true" transform="translate(132,679)"><g is="true" transform="translate(0,629)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-752"></use><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-1031"></use></g><g is="true" transform="translate(196,0)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(763,290)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g><g is="true" transform="translate(1186,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(433,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-42"></use></g></g><g is="true" transform="translate(2070,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-57"></use></g></g><g is="true" transform="translate(3009,629)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-752"></use><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-1031"></use></g></g><g is="true" transform="translate(60,-533)"><g is="true" transform="translate(0,629)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-752"></use><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-1031"></use></g><g is="true" transform="translate(196,0)"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(763,290)"><use transform="scale(0.5)" xlink:href="#MJMATHI-54"></use></g></g><g is="true" transform="translate(1186,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(433,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-57"></use></g></g><g is="true" transform="translate(2215,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-57"></use></g></g><g is="true" transform="translate(3153,629)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-752"></use><use transform="scale(0.707)" xlink:href="#MJMAIN-2223" x="0" y="-1031"></use></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">J</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">W</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mfrac is="true"><mrow is="true"><mo stretchy="true" is="true">|</mo><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><msub is="true"><mi is="true">S</mi><mi is="true">B</mi></msub><mi is="true">W</mi></mrow><mo stretchy="true" is="true">|</mo></mrow><mrow is="true"><mo stretchy="true" is="true">|</mo><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><msub is="true"><mi is="true">S</mi><mi is="true">W</mi></msub><mi is="true">W</mi></mrow><mo stretchy="true" is="true">|</mo></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-19"><math><mrow is="true"><mi is="true">J</mi><mrow is="true"><mo stretchy="true" is="true">(</mo><mi is="true">W</mi><mo stretchy="true" is="true">)</mo></mrow><mo linebreak="goodbreak" is="true">=</mo><mspace width="0.28em" is="true"></mspace><mfrac is="true"><mrow is="true"><mo stretchy="true" is="true">|</mo><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><msub is="true"><mi is="true">S</mi><mi is="true">B</mi></msub><mi is="true">W</mi></mrow><mo stretchy="true" is="true">|</mo></mrow><mrow is="true"><mo stretchy="true" is="true">|</mo><mrow is="true"><msup is="true"><mi is="true">W</mi><mi is="true">T</mi></msup><msub is="true"><mi is="true">S</mi><mi is="true">W</mi></msub><mi is="true">W</mi></mrow><mo stretchy="true" is="true">|</mo></mrow></mfrac></mrow></math></script></span></span></span></div><div class="u-margin-s-bottom" id="para0034">In <a class="anchor anchor-primary" href="#eqn0005" name="beqn0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="eqn0005"><span class="anchor-text-container"><span class="anchor-text">(5)</span></span></a>, <em>S<sub>B</sub></em> and <em>S<sub>W</sub></em><span> are between <a href="/topics/computer-science/class-scatter-matrix" title="Learn more about class scatter matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">class scatter matrix</a><span> and within <a href="/topics/engineering/class-scatter-matrix" title="Learn more about class scatter matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">class scatter matrix</a>, respectively. They are given by</span></span><span class="display"><span id="eqn0006" class="formula"><span class="label">(6)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mi is=&quot;true&quot;>b</mi></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><munderover is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>1</mn></mrow><mi is=&quot;true&quot;>N</mi></munderover><msub is=&quot;true&quot;><mi is=&quot;true&quot;>M</mi><mi is=&quot;true&quot;>i</mi></msub><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>m</mi><mi is=&quot;true&quot;>i</mi></msub><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>&amp;#x2212;</mo><mi is=&quot;true&quot;>m</mi></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><msup is=&quot;true&quot;><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>m</mi><mi is=&quot;true&quot;>i</mi></msub><mo is=&quot;true&quot;>&amp;#x2212;</mo><mi is=&quot;true&quot;>m</mi></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mi is=&quot;true&quot;>T</mi></msup></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="35.48ex" height="3.24ex" viewBox="0 -1045.3 15275.9 1395" role="img" focusable="false" style="vertical-align: -0.812ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(613,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use></g></g><g is="true" transform="translate(1294,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2351,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,477)"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(794,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(4823,0)"><g is="true"><use xlink:href="#MJMATHI-4D"></use></g><g is="true" transform="translate(970,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(6304,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-6D"></use></g><g is="true" transform="translate(878,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1445,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2445,0)"><use xlink:href="#MJMATHI-6D"></use></g></g><g is="true" transform="translate(3713,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(10574,0)"><g is="true"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-6D"></use></g><g is="true" transform="translate(878,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1445,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(2445,0)"><use xlink:href="#MJMATHI-6D"></use></g></g><g is="true" transform="translate(3713,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(4103,477)"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">S</mi><mi is="true">b</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></munderover><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">m</mi><mi is="true">i</mi></msub><mo linebreak="badbreak" is="true">âˆ’</mo><mi is="true">m</mi></mrow><mo stretchy="true" is="true">)</mo></mrow><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">m</mi><mi is="true">i</mi></msub><mo is="true">âˆ’</mo><mi is="true">m</mi></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-20"><math><mrow is="true"><msub is="true"><mi is="true">S</mi><mi is="true">b</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></munderover><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">m</mi><mi is="true">i</mi></msub><mo linebreak="badbreak" is="true">âˆ’</mo><mi is="true">m</mi></mrow><mo stretchy="true" is="true">)</mo></mrow><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><msub is="true"><mi is="true">m</mi><mi is="true">i</mi></msub><mo is="true">âˆ’</mo><mi is="true">m</mi></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup></mrow></math></script></span></span></span><span class="display"><span id="eqn0007" class="formula"><span class="label">(7)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>S</mi><mi is=&quot;true&quot;>W</mi></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><munderover is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>i</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>1</mn></mrow><mi is=&quot;true&quot;>N</mi></munderover><msub is=&quot;true&quot;><mi is=&quot;true&quot;>M</mi><mi is=&quot;true&quot;>i</mi></msub></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="15.272ex" height="3.24ex" viewBox="0 -1045.3 6575.6 1395" role="img" focusable="false" style="vertical-align: -0.812ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-53"></use></g><g is="true" transform="translate(613,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-57"></use></g></g><g is="true" transform="translate(1732,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2788,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,477)"><use transform="scale(0.707)" xlink:href="#MJMATHI-4E"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g><g is="true" transform="translate(244,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(794,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-31"></use></g></g></g><g is="true" transform="translate(5260,0)"><g is="true"><use xlink:href="#MJMATHI-4D"></use></g><g is="true" transform="translate(970,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">S</mi><mi is="true">W</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></munderover><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub></mrow></math></span></span><script type="math/mml" id="MathJax-Element-21"><math><mrow is="true"><msub is="true"><mi is="true">S</mi><mi is="true">W</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munderover is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">N</mi></munderover><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub></mrow></math></script></span></span></span>where<span class="display"><span id="eqn0008" class="formula"><span class="label">(8)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub is=&quot;true&quot;><mi is=&quot;true&quot;>M</mi><mi is=&quot;true&quot;>i</mi></msub><mo linebreak=&quot;goodbreak&quot; is=&quot;true&quot;>=</mo><munder is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>X</mi><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><mo is=&quot;true&quot;>&amp;#x2208;</mo><mspace width=&quot;0.28em&quot; is=&quot;true&quot; /><msub is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi><mi is=&quot;true&quot;>i</mi></msub></mrow></munder><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi><mo linebreak=&quot;badbreak&quot; is=&quot;true&quot;>&amp;#x2212;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mi is=&quot;true&quot;>i</mi></msub></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><msup is=&quot;true&quot;><mrow is=&quot;true&quot;><mo stretchy=&quot;true&quot; is=&quot;true&quot;>(</mo><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><msub is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mi is=&quot;true&quot;>i</mi></msub></mrow><mo stretchy=&quot;true&quot; is=&quot;true&quot;>)</mo></mrow><mi is=&quot;true&quot;>T</mi></msup></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="32.666ex" height="3.586ex" viewBox="0 -1045.3 14064.4 1544.1" role="img" focusable="false" style="vertical-align: -1.158ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-4D"></use></g><g is="true" transform="translate(970,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g><g is="true" transform="translate(1592,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2648,0)"><g is="true"><use xlink:href="#MJSZ1-2211"></use></g><g is="true" transform="translate(1056,-287)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-58"></use></g><g is="true"></g><g is="true" transform="translate(882,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2208"></use></g><g is="true"></g><g is="true" transform="translate(1634,0)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(404,-107)"><use transform="scale(0.5)" xlink:href="#MJMATHI-69"></use></g></g></g></g><g is="true" transform="translate(6255,0)"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(794,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(1795,0)"><g is="true"><use xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(603,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g></g><g is="true" transform="translate(3132,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(9944,0)"><g is="true"><g is="true"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(389,0)"><g is="true"><use xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(794,0)"><use xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(1795,0)"><g is="true"><use xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(603,-150)"><use transform="scale(0.707)" xlink:href="#MJMATHI-69"></use></g></g></g><g is="true" transform="translate(3132,0)"><use xlink:href="#MJMAIN-29"></use></g></g><g is="true" transform="translate(3522,477)"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">X</mi><mspace width="0.28em" is="true"></mspace><mo is="true">âˆˆ</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">x</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">âˆ’</mo><msub is="true"><mi is="true">Î¼</mi><mi is="true">i</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">x</mi><mo is="true">âˆ’</mo><msub is="true"><mi is="true">Î¼</mi><mi is="true">i</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup></mrow></math></span></span><script type="math/mml" id="MathJax-Element-22"><math><mrow is="true"><msub is="true"><mi is="true">M</mi><mi is="true">i</mi></msub><mo linebreak="goodbreak" is="true">=</mo><munder is="true"><mo is="true">âˆ‘</mo><mrow is="true"><mi is="true">X</mi><mspace width="0.28em" is="true"></mspace><mo is="true">âˆˆ</mo><mspace width="0.28em" is="true"></mspace><msub is="true"><mi is="true">x</mi><mi is="true">i</mi></msub></mrow></munder><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">x</mi><mo linebreak="badbreak" is="true">âˆ’</mo><msub is="true"><mi is="true">Î¼</mi><mi is="true">i</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><msup is="true"><mrow is="true"><mo stretchy="true" is="true">(</mo><mrow is="true"><mi is="true">x</mi><mo is="true">âˆ’</mo><msub is="true"><mi is="true">Î¼</mi><mi is="true">i</mi></msub></mrow><mo stretchy="true" is="true">)</mo></mrow><mi is="true">T</mi></msup></mrow></math></script></span></span></span></div><div class="u-margin-s-bottom" id="para0035">The comparison of these traditional classifiers are presented in the experimental results section.</div></section></section></section><section id="sec0010"><h2 id="cesectitle0012" class="u-h4 u-margin-l-top u-margin-xs-bottom">3. Results</h2><div class="u-margin-s-bottom" id="para0036"><span>In order to evaluate and identify the approach with the best recognition accuracy, different methods were tested in our experiments. To this end, we tested the pre-trained deep CNN AlexNet with the initial <a href="/topics/computer-science/learning-rate" title="Learn more about learning rate from ScienceDirect's AI-generated Topic Pages" class="topic-link">learning rate</a> 0.0003, minimum batch size 5 and maximum epochs 10 </span><a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a><span>. The test of the traditional classifiers included multiclass model for SVM, the LDA with linear Discriminant-Type and the KNN with <a href="/topics/computer-science/euclidean-distance" title="Learn more about Euclidean Distance from ScienceDirect's AI-generated Topic Pages" class="topic-link">Euclidean Distance</a><span> and 1 neighbor number. We also conducted some experiments to test the influence of the hyperparameters in the training stage. For the proposed method (AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA), we use the LDA classifier to train and classify the deep features extracted from the AlexNet. We have tried to use the â€˜OptimizeHyperparametersâ€™ function in Matlab to find the optimized hyperparameters. We found that the time cost outweighs the small <a href="/topics/computer-science/performance-improvement" title="Learn more about performance improvement from ScienceDirect's AI-generated Topic Pages" class="topic-link">performance improvement</a>. In the LDA, the function will try to optimize the performance by changing hyperparameters Delta and Gamma automatically. By using the OptimizeHyperparametersâ€™ function, we found the operating time is increased by 100 times and there is limited improvement in recognition accuracy in the JAFFE dataset for the LDA classifier. We found the â€˜OptimizeHyperparametersâ€™ function has the drawback of greatly increasing the operating time, which is not appropriate in our research. As a result, we use the default hyperparameters for the LDA classifier.</span></span></div><div class="u-margin-s-bottom" id="para0037">We also tested the combination of the AlexNet with traditional classifiers <a class="anchor anchor-primary" href="#bib0049" name="bbib0049" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0049"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a>. As the layer selected to extract the features affects the recognition result, we conducted the experiments using different layers to extract the deep features. The explored layers are the FC6, FC7, and FC8 of the AlexNet. We first tested the combination of the AlexNet and the LDA using FC6, FC7 and FC8 to investigate which layer may have the best recognition accuracy. We subsequently discovered that FC6 demonstrated the highest recognition accuracy. We then used FC6 to extract the deep features and experimentally tested which classifier demonstrated the highest recognition accuracy among SVM, LDA and KNN. We found that the LDA showed the best recognition performance. In summary, we tested the recognition accuracy of facial expressions using the following methods: deep convolution neural networks AlexNet, traditional classifiers SVM, LDA and KNN and the combination of the AlexNet and traditional classifiers using FC8 and LDA, FC7 and LDA, FC6 and LDA, FC6 and SVM, and FC6 and KNN, respectively.</div><div class="u-margin-s-bottom" id="para0038">Additionally, in order to compare the recognition accuracy among the nine methods, we conducted our experiments on different facial expression databases in this research. Specifically, we used JAFFE, KDEF, CK+, FER2013 and AffectNet Datasets. Moreover, we used the proposed system to quantify the evolution of facial expressions from a video of facial expressions emitted by the first author as the ground truth is known in this case. The detailed experimental outcomes of these comparisons are reported in the following sections.</div><section id="sec0011"><h3 id="cesectitle0013" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.1. Experiment using the online datasets</h3><section id="sec0012"><h4 id="cesectitle0014" class="u-margin-m-top u-margin-xs-bottom">3.1.1. JAFFE dataset experiments</h4><div class="u-margin-s-bottom"><div id="para0039"><span>To begin with, the JAFFE is a facial expression database with only 213 static images. By using the JAFFE Dataset, we aim to test the influence of small number of images in training the system using different methods. From the JAFFE Dataset, we selected 202 images that were all processed using the image <a href="/topics/computer-science/preprocessing-technique" title="Learn more about preprocessing techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">preprocessing techniques</a> mentioned above (it was observed that the JAFFE dataset included some incorrectly labeled facial expressions, which were removed </span><a class="anchor anchor-primary" href="#bib0026" name="bbib0026" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0026"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a>). There are 7 different facial expressions in this dataset: angry, happy, neutral, surprised, sad, afraid, and disgusted. In each test, 70% of the images were randomly selected as the training images, with the rest of the images serving as testing images. <a class="anchor anchor-primary" href="#tbl0001" name="btbl0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0001"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;1</span></span></a> compares the 5-fold Cross Validation Error Rate for the facial expressions using the JAFFE Dataset with each different method. The first column in the table shows the method used for facial expressions recognition. The second column shows the 5-fold Cross Validation Error Rate for each method.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0001"><span class="captions text-s"><span id="cap0010"><p id="spara010"><span class="label">Table 1</span>. Comparison of cross-validation error rate using different methods for the JAFFE dataset.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0001">Method</th><th scope="col" class="align-left valign-top" id="en0002">Error Rate (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0003">AlexNet</td><td class="align-left valign-top" id="en0004">24.8</td></tr><tr><td class="align-left valign-top" id="en0005">LDA</td><td class="align-left valign-top" id="en0006">26.7</td></tr><tr><td class="align-left valign-top" id="en0007">SVM</td><td class="align-left valign-top" id="en0008">24.8</td></tr><tr><td class="align-left valign-top" id="en0009">KNN</td><td class="align-left valign-top" id="en0010">39.6</td></tr><tr><td class="align-left valign-top" id="en0011">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0012">18.8</td></tr><tr><td class="align-left valign-top" id="en0013">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0014">9.4</td></tr><tr><td class="align-left valign-top" id="en0015">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0016">5.9</td></tr><tr><td class="align-left valign-top" id="en0017">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0018">10.9</td></tr><tr><td class="align-left valign-top" id="en0019">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0020">15.4</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0040">As the table shows, our proposed method (AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA) reaches the lowest error rate of 5.9%. As the number of images in the JAFFE Database is quite small, the deep CNN AlexNet does not demonstrate satisfactory performance. Also, the performance of the AlexNet appears to be quite near to that of the traditional classifiers such as LDA and SVM.</div><div class="u-margin-s-bottom"><div id="para0041">Furthermore, <a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a> also shows the recognition accuracy for each emotion and the overall recognition accuracy rates obtained when using each different method for the JAFFE Dataset. The first column in the table shows the method used for facial expressions recognition whilst the second to the eighth column shows the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces, respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. The method using AlexNet, FC6 and LDA has the highest overall recognition accuracy, but does not demonstrate good performance in recognizing â€˜surpriseâ€™. In general, â€˜happyâ€™ is the most easily recognized emotion by this method, while â€˜surprisedâ€™ is the most difficult emotion category to be recognized.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0002"><span class="captions text-s"><span id="cap0011"><p id="spara011"><span class="label">Table 2</span>. Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the JAFFE dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0021">Method</th><th scope="col" class="align-left valign-top" id="en0022">Angry (%)</th><th scope="col" class="align-left valign-top" id="en0023">Disgust (%)</th><th scope="col" class="align-left valign-top" id="en0024">Fear (%)</th><th scope="col" class="align-left valign-top" id="en0025">Happy (%)</th><th scope="col" class="align-left valign-top" id="en0026">Neutral (%)</th><th scope="col" class="align-left valign-top" id="en0027">Sad (%)</th><th scope="col" class="align-left valign-top" id="en0028">Surprise (%)</th><th scope="col" class="align-left valign-top" id="en0029">Overall Accuracy for Each Method (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0030">AlexNet</td><td class="align-left valign-top" id="en0031">88.9</td><td class="align-left valign-top" id="en0032">37.5</td><td class="align-left valign-top" id="en0033">33.3</td><td class="align-left valign-top" id="en0034">100</td><td class="align-left valign-top" id="en0035">77.8</td><td class="align-left valign-top" id="en0036">87.5</td><td class="align-left valign-top" id="en0037">87.5</td><td class="align-left valign-top" id="en0038">73.3</td></tr><tr><td class="align-left valign-top" id="en0039">LDA</td><td class="align-left valign-top" id="en0040">66.7</td><td class="align-left valign-top" id="en0041">62.5</td><td class="align-left valign-top" id="en0042">55.6</td><td class="align-left valign-top" id="en0043">88.9</td><td class="align-left valign-top" id="en0044">55.6</td><td class="align-left valign-top" id="en0045">37.5</td><td class="align-left valign-top" id="en0046">25.0</td><td class="align-left valign-top" id="en0047">56.7</td></tr><tr><td class="align-left valign-top" id="en0048">SVM</td><td class="align-left valign-top" id="en0049">88.9</td><td class="align-left valign-top" id="en0050">62.5</td><td class="align-left valign-top" id="en0051">66.7</td><td class="align-left valign-top" id="en0052">88.9</td><td class="align-left valign-top" id="en0053">100</td><td class="align-left valign-top" id="en0054">62.5</td><td class="align-left valign-top" id="en0055">25.0</td><td class="align-left valign-top" id="en0056">71.7</td></tr><tr><td class="align-left valign-top" id="en0057">KNN</td><td class="align-left valign-top" id="en0058">77.8</td><td class="align-left valign-top" id="en0059">62.5</td><td class="align-left valign-top" id="en0060">33.3</td><td class="align-left valign-top" id="en0061">77.8</td><td class="align-left valign-top" id="en0062">100</td><td class="align-left valign-top" id="en0063">75.0</td><td class="align-left valign-top" id="en0064">37.5</td><td class="align-left valign-top" id="en0065">66.7</td></tr><tr><td class="align-left valign-top" id="en0066">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0067">100</td><td class="align-left valign-top" id="en0068">87.5</td><td class="align-left valign-top" id="en0069">88.9</td><td class="align-left valign-top" id="en0070">88.9</td><td class="align-left valign-top" id="en0071">88.9</td><td class="align-left valign-top" id="en0072">75.0</td><td class="align-left valign-top" id="en0073">50.0</td><td class="align-left valign-top" id="en0074">83.3</td></tr><tr><td class="align-left valign-top" id="en0075">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0076">100</td><td class="align-left valign-top" id="en0077">87.5</td><td class="align-left valign-top" id="en0078">88.9</td><td class="align-left valign-top" id="en0079">100</td><td class="align-left valign-top" id="en0080">100</td><td class="align-left valign-top" id="en0081">87.5</td><td class="align-left valign-top" id="en0082">25.0</td><td class="align-left valign-top" id="en0083">85.0</td></tr><tr><td class="align-left valign-top" id="en0084">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0085">100</td><td class="align-left valign-top" id="en0086">87.5</td><td class="align-left valign-top" id="en0087">100</td><td class="align-left valign-top" id="en0088">100</td><td class="align-left valign-top" id="en0089">100</td><td class="align-left valign-top" id="en0090">100</td><td class="align-left valign-top" id="en0091">75.0</td><td class="align-left valign-top" id="en0092">95.0</td></tr><tr><td class="align-left valign-top" id="en0093">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0094">88.9</td><td class="align-left valign-top" id="en0095">87.5</td><td class="align-left valign-top" id="en0096">88.9</td><td class="align-left valign-top" id="en0097">100</td><td class="align-left valign-top" id="en0098">100</td><td class="align-left valign-top" id="en0099">87.5</td><td class="align-left valign-top" id="en0100">37.5</td><td class="align-left valign-top" id="en0101">85.0</td></tr><tr><td class="align-left valign-top" id="en0102">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0103">77.8</td><td class="align-left valign-top" id="en0104">87.5</td><td class="align-left valign-top" id="en0105">44.4</td><td class="align-left valign-top" id="en0106">88.9</td><td class="align-left valign-top" id="en0107">100</td><td class="align-left valign-top" id="en0108">87.5</td><td class="align-left valign-top" id="en0109">62.5</td><td class="align-left valign-top" id="en0110">78.3</td></tr><tr><td class="align-left valign-top" id="en0111">Average Accuracy for Each Emotion</td><td class="align-left valign-top" id="en0112">87.7</td><td class="align-left valign-top" id="en0113">73.6</td><td class="align-left valign-top" id="en0114">66.7</td><td class="align-left valign-top" id="en0115">92.6</td><td class="align-left valign-top" id="en0116">91.4</td><td class="align-left valign-top" id="en0117">77.8</td><td class="align-left valign-top" id="en0118">47.2</td><td class="align-left valign-top" id="en0119">77.2</td></tr></tbody></table></div></div></div></section><section id="sec0013"><h4 id="cesectitle0015" class="u-margin-m-top u-margin-xs-bottom">3.1.2. KDEF dataset experiments</h4><div class="u-margin-s-bottom"><div id="para0042">The KDEF Dataset contains 4900 facial expression images. As our system aims to quantify the evolution of emotion from front-view videos of facial expressions, we have only used front-view images. From the KDEF Dataset, we selected all 980 front-view images of facial expressions that were all processed using the images preprocessing techniques outlined above <a class="anchor anchor-primary" href="#bib0025" name="bbib0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0025"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>. There are seven different facial expressions in this dataset: angry, happy, neutral, surprised, sad, fearful, and disgusted. In the experiment, 70% of the images were again randomly selected as the training images, with the rest of the images used as the testing images. <a class="anchor anchor-primary" href="#tbl0003" name="btbl0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0003"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;3</span></span></a> compares the 5-fold Cross Validation Error Rate for the facial expressions using the KDEF Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0003"><span class="captions text-s"><span id="cap0012"><p id="spara012"><span class="label">Table 3</span>. Comparison cross-validation error rate using different method for the KDEF dataset.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0120">Method</th><th scope="col" class="align-left valign-top" id="en0121">Error Rate (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0122">AlexNet</td><td class="align-left valign-top" id="en0123">15.1</td></tr><tr><td class="align-left valign-top" id="en0124">LDA</td><td class="align-left valign-top" id="en0125">36.3</td></tr><tr><td class="align-left valign-top" id="en0126">SVM</td><td class="align-left valign-top" id="en0127">32.1</td></tr><tr><td class="align-left valign-top" id="en0128">KNN</td><td class="align-left valign-top" id="en0129">56.0</td></tr><tr><td class="align-left valign-top" id="en0130">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0131">17.2</td></tr><tr><td class="align-left valign-top" id="en0132">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0133">12.0</td></tr><tr><td class="align-left valign-top" id="en0134">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0135">11.6</td></tr><tr><td class="align-left valign-top" id="en0136">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0137">12.6</td></tr><tr><td class="align-left valign-top" id="en0138">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0139">32.6</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0043">As the table shows, our proposed method has the lowest error rate at 11.6%. However, as the number of images in the KDEF was greatly increased compared to the JAFFE Database, the performance of the AlexNet can be seen to be significantly improved, with an error rate now 4% higher than the proposed method. On the other hand, the traditional classifiers such as LDA and SVM did not show such a good performance when there were tested with such a large image database.</div><div class="u-margin-s-bottom"><div id="para0044">In addition, <a class="anchor anchor-primary" href="#tbl0004" name="btbl0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0004"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;4</span></span></a> shows the recognition accuracy for each emotion category, and the overall recognition accuracy when using each different method for the KDEF Dataset. The first column in the table shows the method used for facial expressions recognition whilst the second to eighth columns show the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. The proposed method of using AlexNet, FC6 and LDA clearly demonstrates the highest recognition accuracy. Moreover, we can observe that â€˜happyâ€™ is still the easiest emotion to be recognized, while â€˜sadâ€™ and â€˜angryâ€™ are the two most difficult emotions to be recognized.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0004"><span class="captions text-s"><span id="cap0013"><p id="spara013"><span class="label">Table 4</span>. Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the KDEF dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0140">Method</th><th scope="col" class="align-left valign-top" id="en0141">Angry (%)</th><th scope="col" class="align-left valign-top" id="en0142">Disgust (%)</th><th scope="col" class="align-left valign-top" id="en0143">Fear (%)</th><th scope="col" class="align-left valign-top" id="en0144">Happy (%)</th><th scope="col" class="align-left valign-top" id="en0145">Neutral (%)</th><th scope="col" class="align-left valign-top" id="en0146">Sad (%)</th><th scope="col" class="align-left valign-top" id="en0147">Surprise (%)</th><th scope="col" class="align-left valign-top" id="en0148">Overall Accuracy for Each Method (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0149">AlexNet</td><td class="align-left valign-top" id="en0150">76.2</td><td class="align-left valign-top" id="en0151">81.0</td><td class="align-left valign-top" id="en0152">92.9</td><td class="align-left valign-top" id="en0153">97.6</td><td class="align-left valign-top" id="en0154">88.1</td><td class="align-left valign-top" id="en0155">81.0</td><td class="align-left valign-top" id="en0156">76.2</td><td class="align-left valign-top" id="en0157">84.7</td></tr><tr><td class="align-left valign-top" id="en0158">LDA</td><td class="align-left valign-top" id="en0159">47.6</td><td class="align-left valign-top" id="en0160">69.0</td><td class="align-left valign-top" id="en0161">50.0</td><td class="align-left valign-top" id="en0162">83.3</td><td class="align-left valign-top" id="en0163">71.4</td><td class="align-left valign-top" id="en0164">50.0</td><td class="align-left valign-top" id="en0165">71.4</td><td class="align-left valign-top" id="en0166">63.3</td></tr><tr><td class="align-left valign-top" id="en0167">SVM</td><td class="align-left valign-top" id="en0168">57.1</td><td class="align-left valign-top" id="en0169">66.7</td><td class="align-left valign-top" id="en0170">66.7</td><td class="align-left valign-top" id="en0171">81.0</td><td class="align-left valign-top" id="en0172">54.8</td><td class="align-left valign-top" id="en0173">52.4</td><td class="align-left valign-top" id="en0174">76.2</td><td class="align-left valign-top" id="en0175">65.0</td></tr><tr><td class="align-left valign-top" id="en0176">KNN</td><td class="align-left valign-top" id="en0177">38.1</td><td class="align-left valign-top" id="en0178">35.7</td><td class="align-left valign-top" id="en0179">31.0</td><td class="align-left valign-top" id="en0180">47.6</td><td class="align-left valign-top" id="en0181">50.0</td><td class="align-left valign-top" id="en0182">35.7</td><td class="align-left valign-top" id="en0183">57.1</td><td class="align-left valign-top" id="en0184">42.2</td></tr><tr><td class="align-left valign-top" id="en0185">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0186">83.3</td><td class="align-left valign-top" id="en0187">69.0</td><td class="align-left valign-top" id="en0188">83.3</td><td class="align-left valign-top" id="en0189">97.6</td><td class="align-left valign-top" id="en0190">88.1</td><td class="align-left valign-top" id="en0191">73.8</td><td class="align-left valign-top" id="en0192">90.5</td><td class="align-left valign-top" id="en0193">83.7</td></tr><tr><td class="align-left valign-top" id="en0194">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0195">83.3</td><td class="align-left valign-top" id="en0196">81.0</td><td class="align-left valign-top" id="en0197">85.7</td><td class="align-left valign-top" id="en0198">100</td><td class="align-left valign-top" id="en0199">85.7</td><td class="align-left valign-top" id="en0200">78.6</td><td class="align-left valign-top" id="en0201">85.7</td><td class="align-left valign-top" id="en0202">85.7</td></tr><tr><td class="align-left valign-top" id="en0203">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0204">78.6</td><td class="align-left valign-top" id="en0205">85.7</td><td class="align-left valign-top" id="en0206">83.3</td><td class="align-left valign-top" id="en0207">100</td><td class="align-left valign-top" id="en0208">92.9</td><td class="align-left valign-top" id="en0209">83.3</td><td class="align-left valign-top" id="en0210">90.5</td><td class="align-left valign-top" id="en0211">87.8</td></tr><tr><td class="align-left valign-top" id="en0212">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0213">78.6</td><td class="align-left valign-top" id="en0214">83.3</td><td class="align-left valign-top" id="en0215">90.5</td><td class="align-left valign-top" id="en0216">97.6</td><td class="align-left valign-top" id="en0217">88.1</td><td class="align-left valign-top" id="en0218">78.6</td><td class="align-left valign-top" id="en0219">88.1</td><td class="align-left valign-top" id="en0220">86.4</td></tr><tr><td class="align-left valign-top" id="en0221">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0222">40.5</td><td class="align-left valign-top" id="en0223">64.3</td><td class="align-left valign-top" id="en0224">45.2</td><td class="align-left valign-top" id="en0225">88.1</td><td class="align-left valign-top" id="en0226">78.6</td><td class="align-left valign-top" id="en0227">54.8</td><td class="align-left valign-top" id="en0228">76.2</td><td class="align-left valign-top" id="en0229">64.0</td></tr><tr><td class="align-left valign-top" id="en0230">Average Accuracy for Each Emotion</td><td class="align-left valign-top" id="en0231">64.8</td><td class="align-left valign-top" id="en0232">70.6</td><td class="align-left valign-top" id="en0233">69.8</td><td class="align-left valign-top" id="en0234">88.1</td><td class="align-left valign-top" id="en0235">77.5</td><td class="align-left valign-top" id="en0236">65.4</td><td class="align-left valign-top" id="en0237">79.1</td><td class="align-left valign-top" id="en0238">73.6</td></tr></tbody></table></div></div></div></section><section id="sec0014"><h4 id="cesectitle0016" class="u-margin-m-top u-margin-xs-bottom">3.1.3. CK+ dataset experiments</h4><div class="u-margin-s-bottom"><div id="para0045">The CK+ Dataset consists of 593 sequences of facial expressions <a class="anchor anchor-primary" href="#bib0027" name="bbib0027" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0027"><span class="anchor-text-container"><span class="anchor-text">[27</span></span></a>,<a class="anchor anchor-primary" href="#bib0028" name="bbib0028" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0028"><span class="anchor-text-container"><span class="anchor-text">28]</span></span></a>. Each video sequences can be regarded as a few continuous video frames. As a result, this is a large database of around 10,000 images of facial expressions taken from 123 models. As these image sequences are continuous, there are many similar images. In our experiment, after removing similar images, 693 images were selected and processed using the image preprocessing techniques described. We selected images with seven different facial expressions in the dataset: angry, happy, neutral, surprised, sad, fearful, and disgusted. We again randomly selected 70% of the images as the training images, with the remainder employed as the testing images. <a class="anchor anchor-primary" href="#tbl0005" name="btbl0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0005"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;5</span></span></a> shows the 5-fold Cross Validation Error Rate for the facial expressions from the CK+ Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0005"><span class="captions text-s"><span id="cap0014"><p id="spara014"><span class="label">Table 5</span>. Comparison cross-validation error rate using different methods for the CK dataset.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0239">Method</th><th scope="col" class="align-left valign-top" id="en0240">Error Rate (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0241">AlexNet</td><td class="align-left valign-top" id="en0242">10.9</td></tr><tr><td class="align-left valign-top" id="en0243">LDA</td><td class="align-left valign-top" id="en0244">10.7</td></tr><tr><td class="align-left valign-top" id="en0245">SVM</td><td class="align-left valign-top" id="en0246">8.4</td></tr><tr><td class="align-left valign-top" id="en0247">KNN</td><td class="align-left valign-top" id="en0248">24.2</td></tr><tr><td class="align-left valign-top" id="en0249">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0250">8.0</td></tr><tr><td class="align-left valign-top" id="en0251">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0252">5.2</td></tr><tr><td class="align-left valign-top" id="en0253">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0254">3.6</td></tr><tr><td class="align-left valign-top" id="en0255">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0256">6.7</td></tr><tr><td class="align-left valign-top" id="en0257">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0258">21.6</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0046">As the table shows, our proposed method still obtains the lowest error rate at 3.6%. Generally, in the CK+ Dataset, two images are selected for each emotion for each subject, with one image being the frame when the emotion begins to be expressed whilst the other image is the frame in the image sequence when the emotion reaches the peak of its expression. All the methods appear to have good performance in this dataset.</div><div class="u-margin-s-bottom"><div id="para0047">Additionally, <a class="anchor anchor-primary" href="#tbl0006" name="btbl0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0006"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;6</span></span></a><span> shows the recognition accuracy for each emotion, and the overall recognition accuracy rate when employing each different method for the CK+ Dataset. The first column in the table shows the method used for facial expressions recognition, with the next seven columns showing the recognition accuracy for each method for angry, disgusted, fearful, happy, neutral, sad and surprised <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a> respectively. The ninth column shows the overall recognition accuracy of each method tested. Also, the last row shows the average recognition accuracy using all the 9 methods for each emotion. Within this dataset, â€˜sadâ€™ seems to be the most difficult emotion to be recognized, with several methods wrongly classifying the sad emotion as â€˜neutralâ€™. The difficulty of recognition of sad emotion will be discussed in the discussion part.</span></div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0006"><span class="captions text-s"><span id="cap0015"><p id="spara015"><span class="label">Table 6</span>. Comparison of recognition accuracy of each emotion and overall recognition accuracy using different method for the CK dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0259">Method</th><th scope="col" class="align-left valign-top" id="en0260">Angry (%)</th><th scope="col" class="align-left valign-top" id="en0261">Disgust (%)</th><th scope="col" class="align-left valign-top" id="en0262">Fear (%)</th><th scope="col" class="align-left valign-top" id="en0263">Happy (%)</th><th scope="col" class="align-left valign-top" id="en0264">Neutral (%)</th><th scope="col" class="align-left valign-top" id="en0265">Sad (%)</th><th scope="col" class="align-left valign-top" id="en0266">Surprise (%)</th><th scope="col" class="align-left valign-top" id="en0267">Overall Accuracy for Each Method (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0268">AlexNet</td><td class="align-left valign-top" id="en0269">95.7</td><td class="align-left valign-top" id="en0270">85.7</td><td class="align-left valign-top" id="en0271">81.8</td><td class="align-left valign-top" id="en0272">97.4</td><td class="align-left valign-top" id="en0273">76.2</td><td class="align-left valign-top" id="en0274">33.3</td><td class="align-left valign-top" id="en0275">97.8</td><td class="align-left valign-top" id="en0276">86.4</td></tr><tr><td class="align-left valign-top" id="en0277">LDA</td><td class="align-left valign-top" id="en0278">78.3</td><td class="align-left valign-top" id="en0279">88.6</td><td class="align-left valign-top" id="en0280">72.7</td><td class="align-left valign-top" id="en0281">97.4</td><td class="align-left valign-top" id="en0282">88.1</td><td class="align-left valign-top" id="en0283">33.3</td><td class="align-left valign-top" id="en0284">91.1</td><td class="align-left valign-top" id="en0285">85.4</td></tr><tr><td class="align-left valign-top" id="en0286">SVM</td><td class="align-left valign-top" id="en0287">73.9</td><td class="align-left valign-top" id="en0288">91.4</td><td class="align-left valign-top" id="en0289">81.8</td><td class="align-left valign-top" id="en0290">92.1</td><td class="align-left valign-top" id="en0291">95.2</td><td class="align-left valign-top" id="en0292">66.7</td><td class="align-left valign-top" id="en0293">88.9</td><td class="align-left valign-top" id="en0294">87.9</td></tr><tr><td class="align-left valign-top" id="en0295">KNN</td><td class="align-left valign-top" id="en0296">65.2</td><td class="align-left valign-top" id="en0297">77.1</td><td class="align-left valign-top" id="en0298">63.6</td><td class="align-left valign-top" id="en0299">65.8</td><td class="align-left valign-top" id="en0300">59.5</td><td class="align-left valign-top" id="en0301">33.3</td><td class="align-left valign-top" id="en0302">64.4</td><td class="align-left valign-top" id="en0303">64.1</td></tr><tr><td class="align-left valign-top" id="en0304">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0305">87.0</td><td class="align-left valign-top" id="en0306">77.1</td><td class="align-left valign-top" id="en0307">63.6</td><td class="align-left valign-top" id="en0308">97.4</td><td class="align-left valign-top" id="en0309">95.2</td><td class="align-left valign-top" id="en0310">33.3</td><td class="align-left valign-top" id="en0311">91.1</td><td class="align-left valign-top" id="en0312">85.4</td></tr><tr><td class="align-left valign-top" id="en0313">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0314">91.3</td><td class="align-left valign-top" id="en0315">82.9</td><td class="align-left valign-top" id="en0316">72.7</td><td class="align-left valign-top" id="en0317">97.4</td><td class="align-left valign-top" id="en0318">97.6</td><td class="align-left valign-top" id="en0319">33.3</td><td class="align-left valign-top" id="en0320">93.3</td><td class="align-left valign-top" id="en0321">88.3</td></tr><tr><td class="align-left valign-top" id="en0322">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0323">100</td><td class="align-left valign-top" id="en0324">91.4</td><td class="align-left valign-top" id="en0325">100</td><td class="align-left valign-top" id="en0326">100</td><td class="align-left valign-top" id="en0327">97.6</td><td class="align-left valign-top" id="en0328">58.3</td><td class="align-left valign-top" id="en0329">95.6</td><td class="align-left valign-top" id="en0330">94.7</td></tr><tr><td class="align-left valign-top" id="en0331">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0332">91.3</td><td class="align-left valign-top" id="en0333">80.0</td><td class="align-left valign-top" id="en0334">81.8</td><td class="align-left valign-top" id="en0335">97.4</td><td class="align-left valign-top" id="en0336">97.6</td><td class="align-left valign-top" id="en0337">41.7</td><td class="align-left valign-top" id="en0338">97.8</td><td class="align-left valign-top" id="en0339">89.8</td></tr><tr><td class="align-left valign-top" id="en0340">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0341">73.9</td><td class="align-left valign-top" id="en0342">85.7</td><td class="align-left valign-top" id="en0343">63.6</td><td class="align-left valign-top" id="en0344">68.4</td><td class="align-left valign-top" id="en0345">66.7</td><td class="align-left valign-top" id="en0346">33.3</td><td class="align-left valign-top" id="en0347">71.1</td><td class="align-left valign-top" id="en0348">69.9</td></tr><tr><td class="align-left valign-top" id="en0349">Average Accuracy for Each Emotion</td><td class="align-left valign-top" id="en0350">84.1</td><td class="align-left valign-top" id="en0351">84.4</td><td class="align-left valign-top" id="en0352">75.7</td><td class="align-left valign-top" id="en0353">90.4</td><td class="align-left valign-top" id="en0354">86.0</td><td class="align-left valign-top" id="en0355">40.7</td><td class="align-left valign-top" id="en0356">87.9</td><td class="align-left valign-top" id="en0357">83.5</td></tr></tbody></table></div></div></div></section><section id="sec0015"><h4 id="cesectitle0017" class="u-margin-m-top u-margin-xs-bottom">3.1.4. FER2013 dataset experiments</h4><div class="u-margin-s-bottom"><div id="para0048">The FER2013 is an online open dataset for facial expressions <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a><span><span>. This dataset consists of more than 30,000 48Ã—48&nbsp;pixel <a href="/topics/engineering/grayscale-image" title="Learn more about grayscale images from ScienceDirect's AI-generated Topic Pages" class="topic-link">grayscale images</a> of faces. There are also seven types of emotions in this dataset: angry, disgusted, fearful, happy, sad, surprised and neutral. However, unlike the JAFFE and KDEF datasets that are lab-posed facial expressions, the FER2013 dataset consists of facial images taken from the internet. In the JAFFE and KDEF databases, facial expression of the same </span><a href="/topics/computer-science/emotional-category" title="Learn more about emotional category from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional category</a> are similar to each other. However, in the FER2013 dataset, very large variations in facial expression can be observed, within the same category of emotion. In the current experiment, about 30,000 images were selected. Image preprocessing techniques were not applied to this dataset, as the original 48Ã—48&nbsp;pixel grayscale images are already quite small. We tested with all seven categories of facial emotion available in the database, again randomly selecting 70% of the images as the training set and using the remainder of the dataset as the testing images. </span><a class="anchor anchor-primary" href="#tbl0007" name="btbl0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0007"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;7</span></span></a> compares the 5-fold Cross Validation Error Rate for the facial expressions using the FER2013 Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method tested. As the table shows, the proposed method still obtains the lowest error rate at 43.5%. However, as the FER2013 dataset is primarily drawn from the internet, with a broad range of individual variation in terms of expression within the same category of emotion, we can see from the data that all approaches demonstrated difficulty in accurate classification, with no method demonstrating good levels of performance.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0007"><span class="captions text-s"><span id="cap0016"><p id="spara016"><span class="label">Table 7</span>. Comparison cross-validation error rate using different methods for the FER2013 dataset.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0358">Method</th><th scope="col" class="align-left valign-top" id="en0359">Error Rate (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0360">AlexNet</td><td class="align-left valign-top" id="en0361">44.6</td></tr><tr><td class="align-left valign-top" id="en0362">LDA</td><td class="align-left valign-top" id="en0363">68.6</td></tr><tr><td class="align-left valign-top" id="en0364">SVM</td><td class="align-left valign-top" id="en0365">73.4</td></tr><tr><td class="align-left valign-top" id="en0366">KNN</td><td class="align-left valign-top" id="en0367">61.7</td></tr><tr><td class="align-left valign-top" id="en0368">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0369">50.2</td></tr><tr><td class="align-left valign-top" id="en0370">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0371">46.5</td></tr><tr><td class="align-left valign-top" id="en0372">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0373">43.5</td></tr><tr><td class="align-left valign-top" id="en0374">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0375">48.2</td></tr><tr><td class="align-left valign-top" id="en0376">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0377">49.6</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="para0049"><a class="anchor anchor-primary" href="#tbl0008" name="btbl0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0008"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;8</span></span></a> shows the recognition accuracy for each emotion category, and the overall recognition accuracy using each particular method with the FER2013 Dataset. The first column in the table shows the method used for facial expressions recognition, whilst the next seven columns show the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. In this dataset, happy and surprised facial categories seem to be the simplest emotions to be recognized. However, even the method using the AlexNet, FC6 and LDA fails to show good performance in recognizing fear emotion.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0008"><span class="captions text-s"><span id="cap0017"><p id="spara017"><span class="label">Table 8</span>. Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the FER2013 dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0378">Method</th><th scope="col" class="align-left valign-top" id="en0379">Angry (%)</th><th scope="col" class="align-left valign-top" id="en0380">Disgust (%)</th><th scope="col" class="align-left valign-top" id="en0381">Fear (%)</th><th scope="col" class="align-left valign-top" id="en0382">Happy (%)</th><th scope="col" class="align-left valign-top" id="en0383">Neutral (%)</th><th scope="col" class="align-left valign-top" id="en0384">Sad (%)</th><th scope="col" class="align-left valign-top" id="en0385">Surprise (%)</th><th scope="col" class="align-left valign-top" id="en0386">Overall Accuracy for Each Method (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0387">AlexNet</td><td class="align-left valign-top" id="en0388">42.0</td><td class="align-left valign-top" id="en0389">43.8</td><td class="align-left valign-top" id="en0390">34.2</td><td class="align-left valign-top" id="en0391">81.3</td><td class="align-left valign-top" id="en0392">58.2</td><td class="align-left valign-top" id="en0393">43.0</td><td class="align-left valign-top" id="en0394">65.6</td><td class="align-left valign-top" id="en0395">56.3</td></tr><tr><td class="align-left valign-top" id="en0396">LDA</td><td class="align-left valign-top" id="en0397">18.7</td><td class="align-left valign-top" id="en0398">14.6</td><td class="align-left valign-top" id="en0399">18.4</td><td class="align-left valign-top" id="en0400">47.6</td><td class="align-left valign-top" id="en0401">28.9</td><td class="align-left valign-top" id="en0402">21.1</td><td class="align-left valign-top" id="en0403">43.9</td><td class="align-left valign-top" id="en0404">30.8</td></tr><tr><td class="align-left valign-top" id="en0405">SVM</td><td class="align-left valign-top" id="en0406">14.7</td><td class="align-left valign-top" id="en0407">27.0</td><td class="align-left valign-top" id="en0408">13.0</td><td class="align-left valign-top" id="en0409">38.0</td><td class="align-left valign-top" id="en0410">20.5</td><td class="align-left valign-top" id="en0411">25.6</td><td class="align-left valign-top" id="en0412">40.1</td><td class="align-left valign-top" id="en0413">26.2</td></tr><tr><td class="align-left valign-top" id="en0414">KNN</td><td class="align-left valign-top" id="en0415">28.0</td><td class="align-left valign-top" id="en0416">54.0</td><td class="align-left valign-top" id="en0417">31.7</td><td class="align-left valign-top" id="en0418">38.0</td><td class="align-left valign-top" id="en0419">38.8</td><td class="align-left valign-top" id="en0420">30.9</td><td class="align-left valign-top" id="en0421">52.4</td><td class="align-left valign-top" id="en0422">36.5</td></tr><tr><td class="align-left valign-top" id="en0423">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0424">37.5</td><td class="align-left valign-top" id="en0425">25.5</td><td class="align-left valign-top" id="en0426">24.6</td><td class="align-left valign-top" id="en0427">70.9</td><td class="align-left valign-top" id="en0428">50.1</td><td class="align-left valign-top" id="en0429">41.5</td><td class="align-left valign-top" id="en0430">65.3</td><td class="align-left valign-top" id="en0431">49.8</td></tr><tr><td class="align-left valign-top" id="en0432">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0433">42.3</td><td class="align-left valign-top" id="en0434">32.1</td><td class="align-left valign-top" id="en0435">28.5</td><td class="align-left valign-top" id="en0436">72.3</td><td class="align-left valign-top" id="en0437">55.8</td><td class="align-left valign-top" id="en0438">46.4</td><td class="align-left valign-top" id="en0439">67.2</td><td class="align-left valign-top" id="en0440">53.5</td></tr><tr><td class="align-left valign-top" id="en0441">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0442">43.8</td><td class="align-left valign-top" id="en0443">36.5</td><td class="align-left valign-top" id="en0444">30.7</td><td class="align-left valign-top" id="en0445">74.9</td><td class="align-left valign-top" id="en0446">59.9</td><td class="align-left valign-top" id="en0447">52.1</td><td class="align-left valign-top" id="en0448">67.5</td><td class="align-left valign-top" id="en0449">56.4</td></tr><tr><td class="align-left valign-top" id="en0450">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0451">42.2</td><td class="align-left valign-top" id="en0452">41.6</td><td class="align-left valign-top" id="en0453">36.9</td><td class="align-left valign-top" id="en0454">70.0</td><td class="align-left valign-top" id="en0455">45.6</td><td class="align-left valign-top" id="en0456">40.3</td><td class="align-left valign-top" id="en0457">69.5</td><td class="align-left valign-top" id="en0458">51.7</td></tr><tr><td class="align-left valign-top" id="en0459">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0460">40.1</td><td class="align-left valign-top" id="en0461">59.9</td><td class="align-left valign-top" id="en0462">42.1</td><td class="align-left valign-top" id="en0463">61.6</td><td class="align-left valign-top" id="en0464">43.0</td><td class="align-left valign-top" id="en0465">39.5</td><td class="align-left valign-top" id="en0466">66.9</td><td class="align-left valign-top" id="en0467">49.5</td></tr><tr><td class="align-left valign-top" id="en0468">Average Accuracy for Each Emotion</td><td class="align-left valign-top" id="en0469">34.5</td><td class="align-left valign-top" id="en0470">35.8</td><td class="align-left valign-top" id="en0471">28.9</td><td class="align-left valign-top" id="en0472">62.1</td><td class="align-left valign-top" id="en0473">44.3</td><td class="align-left valign-top" id="en0474">37.5</td><td class="align-left valign-top" id="en0475">60.9</td><td class="align-left valign-top" id="en0476">45.8</td></tr></tbody></table></div></div></div></section><section id="sec0016"><h4 id="cesectitle0018" class="u-margin-m-top u-margin-xs-bottom">3.1.5. AffectNet dataset experiments</h4><div class="u-margin-s-bottom"><div id="para0050">The AffectNet is also an online dataset which consists of about one million images of facial expressions which were again collected from the Internet, through three major search engines and 1250 emotion related keywords <a class="anchor anchor-primary" href="#bib0030" name="bbib0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0030"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a>. This dataset provides eleven emotion and non-emotion labels including: Neutral, Surprised, Happy, Sad, Fearful, Disgusted, and Angry, whilst additionally providing the categories of Contemptuous, None, Uncertain and No-Face. In line with the FER2013 Dataset, the AffectNet contains the images of facial expressions which are naturally occurring rather than containing the images posed within a lab. In the current study, about 16,000 images were selected. Image preprocessing techniques were not applied to this dataset. We selected the seven emotional expressions that were in line with the previous four datasets tested and again we randomly selected 70% of the images to serve as the training images, while the remaining images were again employed as testing images. <a class="anchor anchor-primary" href="#tbl0009" name="btbl0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0009"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;9</span></span></a> compares the 5-fold Cross Validation Error Rate for the facial expressions using the AffectNet Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0009"><span class="captions text-s"><span id="cap0018"><p id="spara018"><span class="label">Table 9</span>. Comparison cross-validation error rate using different methods for the AffectNet dataset.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0477">Method</th><th scope="col" class="align-left valign-top" id="en0478">Error Rate (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0479">AlexNet</td><td class="align-left valign-top" id="en0480">40.81</td></tr><tr><td class="align-left valign-top" id="en0481">LDA</td><td class="align-left valign-top" id="en0482">59.97</td></tr><tr><td class="align-left valign-top" id="en0483">SVM</td><td class="align-left valign-top" id="en0484">60.92</td></tr><tr><td class="align-left valign-top" id="en0485">KNN</td><td class="align-left valign-top" id="en0486">68.54</td></tr><tr><td class="align-left valign-top" id="en0487">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0488">48.02</td></tr><tr><td class="align-left valign-top" id="en0489">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0490">43.55</td></tr><tr><td class="align-left valign-top" id="en0491">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0492">39.43</td></tr><tr><td class="align-left valign-top" id="en0493">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0494">45.28</td></tr><tr><td class="align-left valign-top" id="en0495">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0496">59.81</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0051">As the table shows, the proposed method has the lowest error rate at 39.43%. As the AffectNet dataset is similar to the FER2013 dataset in employing images of facial expressions taken from the Internet, the huge variance within the same class of emotion can again be seen to result in low recognition accuracy rates.</div><div class="u-margin-s-bottom"><div id="para0052"><a class="anchor anchor-primary" href="#tbl0010" name="btbl0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0010"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;10</span></span></a> shows the recognition accuracy of each emotion category, and the overall recognition accuracy using each different method with the AffectNet Dataset. The first column in the table shows the method used for facial expressions recognition, whilst the second to the eighth columns show the recognition accuracy for each method tested for angry, disgusted, fearful, happy, neutral, sad and surprised emotional expressions respectively. The ninth column shows the overall recognition accuracy for each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. In general, it can be seen that these methods show their best performance in recognizing happy emotions, which is similar to the performance observed with the FER2013 dataset. On the other hand, each method demonstrates its worst performance in recognizing the emotion of surprise.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0010"><span class="captions text-s"><span id="cap0019"><p id="spara019"><span class="label">Table 10</span>. Comparison of recognition accuracy of each emotion and overall recognition accuracy using different methods for the AffectNet dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0497">Method</th><th scope="col" class="align-left valign-top" id="en0498">Angry (%)</th><th scope="col" class="align-left valign-top" id="en0499">Disgust (%)</th><th scope="col" class="align-left valign-top" id="en0500">Fear (%)</th><th scope="col" class="align-left valign-top" id="en0501">Happy (%)</th><th scope="col" class="align-left valign-top" id="en0502">Neutral (%)</th><th scope="col" class="align-left valign-top" id="en0503">Sad (%)</th><th scope="col" class="align-left valign-top" id="en0504">Surprise (%)</th><th scope="col" class="align-left valign-top" id="en0505">Overall Accuracy for Each Method (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0506">AlexNet</td><td class="align-left valign-top" id="en0507">58.2</td><td class="align-left valign-top" id="en0508">22.0</td><td class="align-left valign-top" id="en0509">14.3</td><td class="align-left valign-top" id="en0510">85.6</td><td class="align-left valign-top" id="en0511">50.5</td><td class="align-left valign-top" id="en0512">17.0</td><td class="align-left valign-top" id="en0513">36.3</td><td class="align-left valign-top" id="en0514">58.6</td></tr><tr><td class="align-left valign-top" id="en0515">LDA</td><td class="align-left valign-top" id="en0516">17.8</td><td class="align-left valign-top" id="en0517">4.3</td><td class="align-left valign-top" id="en0518">8.9</td><td class="align-left valign-top" id="en0519">60.8</td><td class="align-left valign-top" id="en0520">38.5</td><td class="align-left valign-top" id="en0521">14.3</td><td class="align-left valign-top" id="en0522">14.4</td><td class="align-left valign-top" id="en0523">38.9</td></tr><tr><td class="align-left valign-top" id="en0524">SVM</td><td class="align-left valign-top" id="en0525">20.8</td><td class="align-left valign-top" id="en0526">9.1</td><td class="align-left valign-top" id="en0527">12.5</td><td class="align-left valign-top" id="en0528">61.8</td><td class="align-left valign-top" id="en0529">37.3</td><td class="align-left valign-top" id="en0530">19.4</td><td class="align-left valign-top" id="en0531">16.1</td><td class="align-left valign-top" id="en0532">40.3</td></tr><tr><td class="align-left valign-top" id="en0533">KNN</td><td class="align-left valign-top" id="en0534">14.5</td><td class="align-left valign-top" id="en0535">8.1</td><td class="align-left valign-top" id="en0536">15.6</td><td class="align-left valign-top" id="en0537">43.1</td><td class="align-left valign-top" id="en0538">34.0</td><td class="align-left valign-top" id="en0539">19.4</td><td class="align-left valign-top" id="en0540">7.6</td><td class="align-left valign-top" id="en0541">30.7</td></tr><tr><td class="align-left valign-top" id="en0542">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0543">25.1</td><td class="align-left valign-top" id="en0544">5.4</td><td class="align-left valign-top" id="en0545">19.6</td><td class="align-left valign-top" id="en0546">78.8</td><td class="align-left valign-top" id="en0547">53.1</td><td class="align-left valign-top" id="en0548">17.0</td><td class="align-left valign-top" id="en0549">26.9</td><td class="align-left valign-top" id="en0550">52.1</td></tr><tr><td class="align-left valign-top" id="en0551">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0552">33.1</td><td class="align-left valign-top" id="en0553">11.8</td><td class="align-left valign-top" id="en0554">22.8</td><td class="align-left valign-top" id="en0555">78.6</td><td class="align-left valign-top" id="en0556">59.2</td><td class="align-left valign-top" id="en0557">25.1</td><td class="align-left valign-top" id="en0558">34.0</td><td class="align-left valign-top" id="en0559">56.0</td></tr><tr><td class="align-left valign-top" id="en0560">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0561">36.2</td><td class="align-left valign-top" id="en0562">12.4</td><td class="align-left valign-top" id="en0563">25.4</td><td class="align-left valign-top" id="en0564">83.2</td><td class="align-left valign-top" id="en0565">64.1</td><td class="align-left valign-top" id="en0566">32.7</td><td class="align-left valign-top" id="en0567">32.0</td><td class="align-left valign-top" id="en0568">60.1</td></tr><tr><td class="align-left valign-top" id="en0569">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0570">38.0</td><td class="align-left valign-top" id="en0571">15.6</td><td class="align-left valign-top" id="en0572">31.3</td><td class="align-left valign-top" id="en0573">77.2</td><td class="align-left valign-top" id="en0574">49.1</td><td class="align-left valign-top" id="en0575">32.3</td><td class="align-left valign-top" id="en0576">32.0</td><td class="align-left valign-top" id="en0577">54.6</td></tr><tr><td class="align-left valign-top" id="en0578">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0579">18.8</td><td class="align-left valign-top" id="en0580">8.6</td><td class="align-left valign-top" id="en0581">19.2</td><td class="align-left valign-top" id="en0582">59.4</td><td class="align-left valign-top" id="en0583">35.6</td><td class="align-left valign-top" id="en0584">21.4</td><td class="align-left valign-top" id="en0585">17.6</td><td class="align-left valign-top" id="en0586">39.2</td></tr><tr><td class="align-left valign-top" id="en0587">Average Accuracy for Each Emotion</td><td class="align-left valign-top" id="en0588">29.2</td><td class="align-left valign-top" id="en0589">10.8</td><td class="align-left valign-top" id="en0590">18.8</td><td class="align-left valign-top" id="en0591">69.8</td><td class="align-left valign-top" id="en0592">46.8</td><td class="align-left valign-top" id="en0593">22.1</td><td class="align-left valign-top" id="en0594">24.1</td><td class="align-left valign-top" id="en0595">47.8</td></tr></tbody></table></div></div></div></section></section><section id="sec0017"><h3 id="cesectitle0019" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.2. Experiment using the author's facial expressions</h3><div class="u-margin-s-bottom"><div id="para0053">In this experiment, our proposed framework was used to recognize facial expression taken from a video of facial expressions emitted by the first author as the ground truth of emotions are perfectly known in this case. Within the video stimulus there were 898 continuous frames, which were preprocessed by the techniques previously outlined. The framework mainly aimed to recognize 5 kinds of emotion in the video stimulus: angry, happy, neutral, surprised, and sad. The training images used 20% of the frames from the video, combined with another dataset emitted by the author containing about 2600 images, whilst the remaining 80% images from the video acted as the testing images in this case. In the subsequent testing, the proposed system successfully classified the images into five facial expression categories: angry, happy, neutral, sad and surprised. In this experiment, as <a class="anchor anchor-primary" href="#fig0004" name="bfig0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0004"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> illustrates, the accuracy was about 96.0%.</div><figure class="figure text-xs" id="fig0004"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr4.jpg" height="280" alt="Fig 4" aria-describedby="cap0004"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (195KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (195KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0004"><p id="spara004"><span class="label">Fig. 4</span>. Recognition results using the proposed framework.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="para0054"><a class="anchor anchor-primary" href="#fig0005" name="bfig0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a> shows the probability of each type of emotion, which is made up of predicted results for each frame in the video. The graph displays the evolution of facial expressions from the video and changes in facial expressions over time. In the graph, the probability of the five facial expression categories for every frame was predicted by the proposed framework. Each line represents the evolution of one emotion. In <a class="anchor anchor-primary" href="#fig0005" name="bfig0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a>, the five lines show the evolution of five emotions over a period of time. In addition, the plot was smoothed by calculating the average of the recent frames.</div><figure class="figure text-xs" id="fig0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr5.jpg" height="291" alt="Fig 5" aria-describedby="cap0005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (611KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (611KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0005"><p id="spara005"><span class="label">Fig. 5</span>. Facial expression analysis for the video stimulus of the author own emotional expressions.</p></span></span></figure></div><div class="u-margin-s-bottom" id="para0055">As an electrocardiogram can reflect the electrical activity of the&nbsp;heart, <a class="anchor anchor-primary" href="#fig0005" name="bfig0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a> reflects the mental state of the patient/ user over a period of time. As shown in <a class="anchor anchor-primary" href="#fig0005" name="bfig0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a><span><span>, around the 300th frame, there are three emotions: happy, neutral and sad. The evolutions of the three emotions at that time are also shown. In addition, this figure illustrates when the user was said to be happy, the duration of the emotion, and how quickly it reached the peak of the emotion. By data analysis, the plot also shows the relative percentage of time for each emotion over the period. As a result, in the area of mental health care, the proposed system clearly has the potential to identify the <a href="/topics/computer-science/emotional-state" title="Learn more about emotional state from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional state</a> of the user. As patients with severe </span><a href="/topics/neuroscience/cognitive-disorders" title="Learn more about cognitive impairments from ScienceDirect's AI-generated Topic Pages" class="topic-link">cognitive impairments</a> may express abnormal facial expressions </span><a class="anchor anchor-primary" href="#bib0021" name="bbib0021" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0021"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a>, the proposed system may have the potential to be used diagnostically in the future.</div></section></section><section id="sec0018"><h2 id="cesectitle0020" class="u-h4 u-margin-l-top u-margin-xs-bottom">4. Discussion</h2><div class="u-margin-s-bottom" id="para0056"><span>Facial expressions are an important component of human <a href="/topics/neuroscience/social-interaction" title="Learn more about social interaction from ScienceDirect's AI-generated Topic Pages" class="topic-link">social interaction</a>, which reflect people's emotions, attitudes, social relations and physiological state. The automated facial expression analysis can play an important role in human-computer interaction in many important applications. On the other hand, deep learning is a dynamic and vibrant topic, encompassing diverse and useful applications, such as the use of A DSAE-based deep learning framework for facial expression recognition and the use of a deep-belief-network-based particle Filter for Analysis of Gold Immunochromatographic Strips </span><a class="anchor anchor-primary" href="#bib0050" name="bbib0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0050"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a>, <a class="anchor anchor-primary" href="#bib0051" name="bbib0051" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0051"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a>, <a class="anchor anchor-primary" href="#bib0052" name="bbib0052" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0052"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a>. In this work, we have proposed a deep learning based facial expression recognition framework towards mental health care.</div><div class="u-margin-s-bottom"><div id="para0057">In the experiments presented here we have tested the facial expression recognition performance of nine methods, including deep convolution neural network AlexNet, the traditional classifiers SVM, LDA and KNN and the combination of the AlexNet and traditional classifiers against five datasets. The overall performance can be seen in <a class="anchor anchor-primary" href="#tbl0011" name="btbl0011" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0011"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;11</span></span></a>, which shows the 5-fold cross validation error rate of each method for the five datasets employed in our testing. The first column in the table shows the method used for facial expressions recognition, while the second column to the sixth columns show the error rate of each method for JAFFE, KDEF, CK+, FER2013 and AffectNet respectively.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0011"><span class="captions text-s"><span id="cap0020"><p id="spara020"><span class="label">Table 11</span>. Comparison cross-validation error rate using different methods over 5 databases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0596">Method</th><th scope="col" class="align-left valign-top" id="en0597">JAFFE</th><th scope="col" class="align-left valign-top" id="en0598">KDEF</th><th scope="col" class="align-left valign-top" id="en0599">CK+</th><th scope="col" class="align-left valign-top" id="en0600">FER2013</th><th scope="col" class="align-left valign-top" id="en0601">AffectNet</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0602">AlexNet</td><td class="align-left valign-top" id="en0603">24.8</td><td class="align-left valign-top" id="en0604">15.1</td><td class="align-left valign-top" id="en0605">10.9</td><td class="align-left valign-top" id="en0606">44.6</td><td class="align-left valign-top" id="en0607">40.8</td></tr><tr><td class="align-left valign-top" id="en0608">LDA</td><td class="align-left valign-top" id="en0609">26.7</td><td class="align-left valign-top" id="en0610">36.3</td><td class="align-left valign-top" id="en0611">10.7</td><td class="align-left valign-top" id="en0612">68.6</td><td class="align-left valign-top" id="en0613">60.0</td></tr><tr><td class="align-left valign-top" id="en0614">SVM</td><td class="align-left valign-top" id="en0615">24.8</td><td class="align-left valign-top" id="en0616">32.1</td><td class="align-left valign-top" id="en0617">8.4</td><td class="align-left valign-top" id="en0618">73.4</td><td class="align-left valign-top" id="en0619">60.9</td></tr><tr><td class="align-left valign-top" id="en0620">KNN</td><td class="align-left valign-top" id="en0621">39.6</td><td class="align-left valign-top" id="en0622">56.0</td><td class="align-left valign-top" id="en0623">24.2</td><td class="align-left valign-top" id="en0624">61.7</td><td class="align-left valign-top" id="en0625">68.5</td></tr><tr><td class="align-left valign-top" id="en0626">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0627">18.8</td><td class="align-left valign-top" id="en0628">17.2</td><td class="align-left valign-top" id="en0629">8.0</td><td class="align-left valign-top" id="en0630">50.2</td><td class="align-left valign-top" id="en0631">48.0</td></tr><tr><td class="align-left valign-top" id="en0632">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0633">9.4</td><td class="align-left valign-top" id="en0634">12.0</td><td class="align-left valign-top" id="en0635">5.2</td><td class="align-left valign-top" id="en0636">46.5</td><td class="align-left valign-top" id="en0637">43.6</td></tr><tr><td class="align-left valign-top" id="en0638">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0639">5.9</td><td class="align-left valign-top" id="en0640">11.6</td><td class="align-left valign-top" id="en0641">3.6</td><td class="align-left valign-top" id="en0642">43.5</td><td class="align-left valign-top" id="en0643">39.4</td></tr><tr><td class="align-left valign-top" id="en0644">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0645">10.9</td><td class="align-left valign-top" id="en0646">12.6</td><td class="align-left valign-top" id="en0647">6.7</td><td class="align-left valign-top" id="en0648">48.2</td><td class="align-left valign-top" id="en0649">45.3</td></tr><tr><td class="align-left valign-top" id="en0650">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0651">15.4</td><td class="align-left valign-top" id="en0652">32.6</td><td class="align-left valign-top" id="en0653">21.6</td><td class="align-left valign-top" id="en0654">49.6</td><td class="align-left valign-top" id="en0655">59.8</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0058">Our experiments demonstrate that in a small dataset like the JAFFE and CK+ datasets, the deep convolution neural network AlexNet and some traditional classifiers like SVM and LDA have similar facial expression recognition accuracy. However, by combining the AlexNet with traditional classifiers like SVM and LDA, the recognition accuracy increases, especially when we extract the deep features from FC6. The experiments show that the method that extracts features from FC6 has better performance than FC7 and FC8 in the five online datasets. We use the AlexNet which is pre-trained with one million natural images from ImageNet to extract features. The features extracted from FC7 are more in line with the classification attribute of the training set of natural images but less in accordance with the dataset of facial expressions <a class="anchor anchor-primary" href="#bib0053" name="bbib0053" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0053"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a>. As a result, the methods that extract the features from FC6 have better performance.</div><div class="u-margin-s-bottom"><div id="para0059"><span>Additionally, we have observed that using the LDA to classify the deep features results in better performance. When the number of the images in the training dataset increased, such as in the case of the KDEF dataset (which contains about 1000 images), the AlexNet seemed to have better recognition accuracy relatively to its performance when tested with a small image database. On the other hand, we have observed that classifying facial expressions with traditional classifiers did not show good overall performance. As a result, in the KDEF dataset, the recognition accuracy of the combination of the AlexNet and traditional classifiers only demonstrated slightly better performance than that of the AlexNet. In addition, it is noticed that the overall recognition accuracy for the FER2013 and AffectNet databases are lower than that of the JAFFE, KDEF and CK+ databases. We would argue that this is mainly as a result of the FER2013 and AffectNet databases containing the more challenging facial expressions sourced from the Internet, which have huge variance within a given class of emotion for example the difference in image sizes and lighting situations. These facial expressions are more natural and diverse, resulting in stimuli that are more difficult to recognize than the lab-based, controlled and actor-generated facial expressions. Although the recognition performance for the AffectNet is worse than the performance in the JAFFE, KDEF and CK+ databases, the proposed framework has a relatively good performance for the AffectNet database compared to the other state-of-art facial expression <a href="/topics/computer-science/recognition-algorithm" title="Learn more about recognition algorithms from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition algorithms</a> </span><a class="anchor anchor-primary" href="#bib0054" name="bbib0054" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0054"><span class="anchor-text-container"><span class="anchor-text">[54</span></span></a>,<a class="anchor anchor-primary" href="#bib0055" name="bbib0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0055"><span class="anchor-text-container"><span class="anchor-text">55]</span></span></a>. <a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> shows the accumulative recognition error rate for the nine methods over the five databases.</div><figure class="figure text-xs" id="fig0006"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr6.jpg" height="225" alt="Fig 6" aria-describedby="cap0006"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (160KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (160KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0006"><p id="spara006"><span class="label">Fig. 6</span>. Comparison of accumulative cross-validation error rates using different methods over 5 databases.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="para0060">In general, the method that extracts the deep features using FC6 from the AlexNet and classifies with LDA showed the best overall performance in the five databases tested using these nine methods. Our experiment result showed a facial expression recognition accuracy of 94.1% on the JAFFE database and 88.4% on the KDEF database, which is a relatively good performance compared to other facial expression recognition algorithms tested on the same databases, as shown in <a class="anchor anchor-primary" href="#tbl0012" name="btbl0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0012"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;12</span></span></a>. The recognition accuracy is calculated from the error rate.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0012"><span class="captions text-s"><span id="cap0021"><p id="spara021"><span class="label">Table 12</span>. Recognition accuracy from published papers on KDEF and JAFFE datasets.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0656">System</th><th scope="col" class="align-left valign-top" id="en0657">Accuracy for KDEF</th><th scope="col" class="align-left valign-top" id="en0658">Accuracy for JAFFE</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0659">DeepPCA <a class="anchor anchor-primary" href="#bib0056" name="bbib0056" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0056"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a></td><td class="align-left valign-top" id="en0660">83.0%</td><td class="align-left valign-top" id="en0661">/</td></tr><tr><td class="align-left valign-top" id="en0662">AAM+SVM <a class="anchor anchor-primary" href="#bib0057" name="bbib0057" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0057"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></td><td class="align-left valign-top" id="en0663">74.6%</td><td class="align-left valign-top" id="en0664">/</td></tr><tr><td class="align-left valign-top" id="en0665">Feature+SVM <a class="anchor anchor-primary" href="#bib0058" name="bbib0058" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0058"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></td><td class="align-left valign-top" id="en0666">82.4%</td><td class="align-left valign-top" id="en0667">/</td></tr><tr><td class="align-left valign-top" id="en0668">C+CNN <a class="anchor anchor-primary" href="#bib0059" name="bbib0059" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0059"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a></td><td class="align-left valign-top" id="en0669">/</td><td class="align-left valign-top" id="en0670">91.6%</td></tr><tr><td class="align-left valign-top" id="en0671">HF <a class="anchor anchor-primary" href="#bib0060" name="bbib0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0060"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a></td><td class="align-left valign-top" id="en0672">/</td><td class="align-left valign-top" id="en0673">87.1%</td></tr><tr><td class="align-left valign-top" id="en0674">Proposed Method (5-fold cross validation recognition accuracy)</td><td class="align-left valign-top" id="en0675">88.4%</td><td class="align-left valign-top" id="en0676">94.1%</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="para0061"><span>In order to further estimate the performance of the proposed method, we compared the proposed method (AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA) with some state-of-the-art deep CNN including pure AlexNet, VGG16, GoogleNet and <a href="/topics/computer-science/residual-neural-network" title="Learn more about ResNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ResNet</a>. We estimated the performance mainly with regard to the operating time of training the network, recognizing the facial expressions and in terms of the recognition accuracy of the facial expression categories. Three facial expression datasets including the JAFFE, KDEF and CK+ Datasets were used in this estimation. </span><a class="anchor anchor-primary" href="#fig0007" name="bfig0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0007"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;7</span></span></a> shows both the recognition accuracy and the operating time for each method with the JAFFE, KDEF and CK+ Datasets. In <a class="anchor anchor-primary" href="#fig0007" name="bfig0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0007"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;7</span></span></a><span>, the recognition accuracy is shown as clustered columns, with the operating time superimposed as a line chart. We can observe that the proposed method has high recognition accuracy compared to the other deep learning algorithms, but slightly lower than that of the ResNet. However, this should be considered in light of the clear reduction in operating time, as the operating time of the proposed method is around 100 times shorter than that of the ResNet. In addition, it is important to note that deep learning algorithms have high device requirements relating to <a href="/topics/engineering/graphics-processing-unit" title="Learn more about GPU from ScienceDirect's AI-generated Topic Pages" class="topic-link">GPU</a> resources and local dynamic random-access memory requirements. Indeed in the current assessment, the Vgg16 failed to produce a recognition result in the KDEF and CK+ datasets due to insufficiency in memory resource to complete the task. In general, the proposed method can be seen to have relatively good recognition accuracy, much shorter operating time and low device requirements compared to the state-of-art deep learning algorithms.</span></div><figure class="figure text-xs" id="fig0007"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr7.jpg" height="381" alt="Fig 7" aria-describedby="cap0007"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (521KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (521KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0007"><p id="spara007"><span class="label">Fig. 7</span>. Comparison of recognition accuracy and operating time for different methods in the JAFFE, KDEF and CK+ datasets.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="para0062"><span>Another important aspect is the ratio of training images to testing the images employed in assessments. This can enable us to determine the influence of training image ratios for different algorithms on the selected dataset. In the experiments presented, we selected 70% of the images randomly from the whole dataset as the training dataset and used the remaining images as the <a href="/topics/engineering/testing-dataset" title="Learn more about testing dataset from ScienceDirect's AI-generated Topic Pages" class="topic-link">testing dataset</a>. Here we choose to test the JAFFE Database. </span><a class="anchor anchor-primary" href="#fig0008" name="bfig0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0008"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;8</span></span></a> shows the different ratios of the training images with the different methods for the JAFFE Database and the resulting recognition accuracy. We observe that when the training images ratios increases, the recognition accuracy for all the methods is increased. The result also suggests that the proposed method using the AlexNet, FC6 and LDA demonstrates the best performance, regardless of the training images ratio examined. Indeed, when we select 90% of the images randomly from the JAFFE Database to act as the training images, and use the remaining images as the testing dataset, the recognition accuracy of the proposed method reaches 97.0%. We can assume that when the training images ratios increase, the recognition performance for other datasets will also be improved.</div><figure class="figure text-xs" id="fig0008"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr8.jpg" height="273" alt="Fig 8" aria-describedby="cap0008"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr8_lrg.jpg" target="_blank" download="" title="Download high-res image (301KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (301KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr8.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0008"><p id="spara008"><span class="label">Fig. 8</span>. The influence of the training images ratios on recognition accuracy with the different algorithms for the JAFFE database.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="para0063">In the experiment, the first phase of the proposed method involves image pre-processing, including aspects such as removing unnecessary environment aspects in the image, identification of the facial region in the image, and some additional essential pre-processing steps. To test the efficiency of this phase we also conducted experiments to compare the 5-fold cross validation error rate using each particular method, for both processed and unprocessed images from the CK+ dataset. The results of these experiments are shown in <a class="anchor anchor-primary" href="#tbl0013" name="btbl0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0013"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;13</span></span></a> below. The first column in the table shows the method used for facial expressions recognition, with the second and third columns showing the 5-fold Cross-Validation Error Rate of each method for both processed and unprocessed images, respectively.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0013"><span class="captions text-s"><span id="cap0022"><p id="spara022"><span class="label">Table 13</span>. Comparison cross-validation error rate using different methods for processed and unprocessed images from the CK+ dataset.</p></span></span><div class="groups"><table><thead class="valign-top"><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0677">Method</th><th scope="col" class="align-left valign-top" id="en0678">Error Rate (%) for Processed Data</th><th scope="col" class="align-left valign-top" id="en0679">Error Rate (%) for Unprocessed Data</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0680">AlexNet</td><td class="align-left valign-top" id="en0681">10.90</td><td class="align-left valign-top" id="en0682">12.72</td></tr><tr><td class="align-left valign-top" id="en0683">LDA</td><td class="align-left valign-top" id="en0684">10.74</td><td class="align-left valign-top" id="en0685">26.42</td></tr><tr><td class="align-left valign-top" id="en0686">SVM</td><td class="align-left valign-top" id="en0687">8.42</td><td class="align-left valign-top" id="en0688">23.80</td></tr><tr><td class="align-left valign-top" id="en0689">KNN</td><td class="align-left valign-top" id="en0690">24.24</td><td class="align-left valign-top" id="en0691">20.46</td></tr><tr><td class="align-left valign-top" id="en0692">AlexNet&nbsp;+&nbsp;FC8&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0693">7.98</td><td class="align-left valign-top" id="en0694">11.76</td></tr><tr><td class="align-left valign-top" id="en0695">AlexNet&nbsp;+&nbsp;FC7&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0696">5.22</td><td class="align-left valign-top" id="en0697">8.56</td></tr><tr><td class="align-left valign-top" id="en0698">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA</td><td class="align-left valign-top" id="en0699">3.63</td><td class="align-left valign-top" id="en0700">3.92</td></tr><tr><td class="align-left valign-top" id="en0701">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;SVM</td><td class="align-left valign-top" id="en0702">6.68</td><td class="align-left valign-top" id="en0703">8.85</td></tr><tr><td class="align-left valign-top" id="en0704">AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;KNN</td><td class="align-left valign-top" id="en0705">21.63</td><td class="align-left valign-top" id="en0706">19.74</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0064">As <a class="anchor anchor-primary" href="#tbl0013" name="btbl0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0013"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;13</span></span></a> shows, the error rate is decreased in all the algorithms for the processed dataset, which proves the efficiency of the image-preprocessing phase. Additionally, we can observe that the image pre-processing phase improves the error rate performance of the traditional classifier to a greater extent, but the combination of the AlexNet and the traditional classifiers seems less dependent on this phase.</div><div class="u-margin-s-bottom"><div id="para0065"><span>Finally, we report on the performance relating to the recognition of each kind of facial expression category. <a href="/topics/computer-science/confusion-matrix" title="Learn more about Confusion matrices from ScienceDirect's AI-generated Topic Pages" class="topic-link">Confusion matrices</a> (a), (b), (c), (d) and (e) in </span><a class="anchor anchor-primary" href="#fig0009" name="bfig0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0009"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;9</span></span></a><span><span> show the data relating to the AlexNet&nbsp;+&nbsp;FC6 LDA for the Datasets of JAFFE, KDEF, CK+, FER2013 and AffectNet respectively. Here, it can be observed that the <a href="/topics/computer-science/emotional-category" title="Learn more about emotional category from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional category</a> â€˜happyâ€™ appears to be the easiest emotion to be recognized. On the other hand, some emotional categories like â€˜sadâ€™ and â€˜disgustâ€™ seems to be much more difficult to be recognized. We would argue that the sad emotion is currently relatively hard to be recognized for the following two reasons. The first reason is due to the quantity of images depicting the sad emotion within the training dataset. We have observed that in most of the datasets, there are notably fewer images of sad emotions, which consequently increases the recognition difficulty. For example, within the CK+ dataset there are 693 images in total, whilst only 40 images depict sad expressions. Additionally, there are relatively small visual differences between sad emotions and neutral emotions, and consequently sad expressions may be recognized as neutral by the system without using sufficient </span><a href="/topics/computer-science/training-data" title="Learn more about training data from ScienceDirect's AI-generated Topic Pages" class="topic-link">training data</a>.</span></div><figure class="figure text-xs" id="fig0009"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr9.jpg" height="589" alt="Fig 9" aria-describedby="cap0009"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr9_lrg.jpg" target="_blank" download="" title="Download high-res image (2MB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (2MB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-gr9.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0009"><p id="spara009"><span class="label">Fig. 9</span>. The <a href="/topics/computer-science/confusion-matrix" title="Learn more about confusion matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">confusion matrix</a> using the AlexNet&nbsp;+&nbsp;FC6 LDA for the datasets of JAFFE, KDEF, CK+, FER2013 and AffectNet are shown in (a), (b), (c), (d) and (e).</p></span></span></figure></div><div class="u-margin-s-bottom" id="para0066">Furthermore, we notice that in the datasets using the images from the Internet, such as the FER2013 and AffectNet datasets, the number of images for each emotion category is uneven. The predominant emotion type contained within these two datasets are happy images, which consequently reduces the difficulty of recognition of the happy emotion. However, in the case of the other databases, which contain a more balanced range of emotions, the emotional category with the lowest error rate varies.</div><div class="u-margin-s-bottom"><div id="para0067">As the developed facial expression recognition method aims to benefit the mental health care, we further test the proposed method with the datasets that contain the images of facial expressions from the patients with cognitive impairment. We tested the proposed method in the following three datasets: a dataset with facial expressions from the patient with cognitive impairment, a dataset that combines the images from the JAFFE dataset and the images from patients and a dataset that combines the images from the KDEF dataset and the images form patients. We used 70% of images as the training dataset and 30% of images as the testing dataset. <a class="anchor anchor-primary" href="#tbl0014" name="btbl0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0014"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;14</span></span></a> shows the recognition accuracy using the proposed method in the three datasets. The experimental result shows that the proposed method also has a good performance for the facial expressions from the patients with cognitive impairment.</div><div class="tables colsep-0 frame-topbot rowsep-0" id="tbl0014"><span class="captions text-s"><span id="cap0023"><p id="spara023"><span class="label">Table 14</span>. The recognition accuracy using the proposed algorithm (AlexNet&nbsp;+&nbsp;FC6&nbsp;+&nbsp;LDA) in the datasets containing facial expressions from patient with cognitive impairment.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0707">Dataset</th><th scope="col" class="align-left valign-top" id="en0708">Recognition Accuracy (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0709">Patient with Cognitive Impairment Dataset</td><td class="align-left valign-top" id="en0710">85.13</td></tr><tr><td class="align-left valign-top" id="en0711">JAFFE&nbsp;+&nbsp;Patient Dataset</td><td class="align-left valign-top" id="en0712">89.90</td></tr><tr><td class="align-left valign-top" id="en0713">KDEF&nbsp;+&nbsp;Patient Dataset</td><td class="align-left valign-top" id="en0714">89.33</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0068">In our examination of the data relating to recognition when employing the first author's facial expressions, drawn from a series of continuous video frames, the accuracy rate was approximately 96.0%.We would argue that the system achieved strong performance in recognizing facial expressions taken from videos in this context. It should be noted that one factor that contributed to the high accuracy rate may be that some of the training images and the testing images were selected from a video that had the similar <a href="/topics/computer-science/lighting-condition" title="Learn more about lighting condition from ScienceDirect's AI-generated Topic Pages" class="topic-link">lighting condition</a> and viewpoint. In addition, there was only one participant and one viewpoint. However, the analysis of facial expressions from the videos in our testing appears to be able to evaluate the evolution of the facial expression over a period of time, with changes of the expression emitted, by recognizing facial expressions for each frame in the video. Additionally, the system was also able to quantify the extent of the facial expression. However, there remain some practical issues to be considered. First, it is noticed that the differences in facial expression for happy and neutral, neutral and sad, sad and angry are small. Moreover, for the video of the facial expressions emitted by the first author, the facial expression is currently labelled manually, and there may be potential problems in labelling some frames during a change of expressions. In addition, as stated previously, it was noted that in the JAFFE dataset, some facial expressions appeared to be labelled incorrectly and were removed prior to testing. In the experiment, 202 images of facial expressions were used from the JAFFE, while the original JAFFE dataset has about 213 images. Finally, there are some practical problems for the proposed facial expression recognition system that remain to be addressed in future, including a need for enhanced stability with regard to how the system crops the head area in the images.</div><div class="u-margin-s-bottom" id="para0069">From the perspective of clinical practice, in order to use the system within elderly people to detect <a href="/topics/computer-science/mental-health-issue" title="Learn more about mental health issues from ScienceDirect's AI-generated Topic Pages" class="topic-link">mental health issues</a><span> like cognitive impairment, the framework needs to be trained with large samples of natural facial expressions taken from elderly people, in order to achieve <a href="/topics/engineering/optimal-performance" title="Learn more about optimal performance from ScienceDirect's AI-generated Topic Pages" class="topic-link">optimal performance</a>. In recent work, we have collected circa 100, 000 images of facial expressions from elderly people. The recognition of naturally occurring facial expression is more challenging. We should also note that work to complete the processing of these images is ongoing, and we will report in a future work regarding satisfactory verification of these images as a suitable training set for our network.</span></div></section><section id="sec0019"><h2 id="cesectitle0021" class="u-h4 u-margin-l-top u-margin-xs-bottom">5. Conclusion</h2><div class="u-margin-s-bottom" id="para0070"><span>This paper has presented a novel <a href="/topics/computer-science/emotion-analysis" title="Learn more about emotion analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotion analysis</a> framework that is able to understand and automatically recognize usersâ€™ emotions by analyzing the usersâ€™ facial expression from their facial images. The system consists of three parts: input of the videos of facial expressions, the image pre-processing technique, and automatic facial expression analysis. The system is able to successfully conduct image pre-processing and facial expression analysis. Additionally, after facial expression analysis has been undertaken, it is able to understand the evolution of facial expression over a period of time and can quantify the extent of the emotions detected from a facial video. As facial expressions reflect people's mental </span><a href="/topics/pharmacology-toxicology-and-pharmaceutical-science/health-status" title="Learn more about health state from ScienceDirect's AI-generated Topic Pages" class="topic-link">health state</a>, the proposed framework has great potential to be employed within mental health care.</div><div class="u-margin-s-bottom" id="para0071">For the facial expression analysis, this paper has also proposed a new solution that extracts the deep features from the FC6 of the AlexNet whilst the standard LDA is exploited to train these deep features. The proposed solution shows promising and has stable performance on all the five tested datasets for the nine methods studied. Additionally, we would argue that the proposed method has relatively good recognition accuracy, much less operating time and lower device requirements compared to the other current state-of-the-art deep learning algorithms. Furthermore, the analysis of facial expressions when taken from the videos demonstrated that our proposed approach is able to report the evolution of facial expression over a period of time, and reliably detect the changes of expression by means of the recognition of facial expression within each frame in the video.</div></section><section id="sec0019a"><h2 id="cesectitle0021a" class="u-h4 u-margin-l-top u-margin-xs-bottom">CRediT authorship contribution statement</h2><div class="u-margin-s-bottom" id="para0071a"><strong>Zixiang Fei:</strong><span> Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Resources, <a href="/topics/computer-science/data-curation" title="Learn more about Data curation from ScienceDirect's AI-generated Topic Pages" class="topic-link">Data curation</a>, Writing - original draft, Writing - review &amp; editing, Visualization. </span><strong>Erfu Yang:</strong> Conceptualization, Methodology, Investigation, Resources, Writing - review &amp; editing, Supervision, Funding acquisition. <strong>David Day-Uei Li:</strong> Writing - review &amp; editing, Supervision. <strong>Stephen Butler:</strong> Writing - review &amp; editing, Supervision. <strong>Winifred Ijomah:</strong> Writing - review &amp; editing, Supervision. <strong>Xia Li:</strong> Investigation, Resources, Supervision. <strong>Huiyu Zhou:</strong> Writing - review &amp; editing, Supervision.</div></section></div><section id="coi0001"><h2 id="cesectitle0023" class="u-h4 u-margin-l-top u-margin-xs-bottom">Declaration of Competing Interest</h2><div class="u-margin-s-bottom" id="para0073">The authors declare no conflict of interest.</div></section><section id="sec0021"><h2 id="cesectitle0024" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgements</h2><div class="u-margin-s-bottom" id="para0074">The research work is funded by Strathclyde's Strategic Technology Partnership (STP) Programme with CAPITA (2016â€“2019). We thank Dr. Neil Mackin and Miss Angela Anderson for their support. The contents in the paper are those of the authors alone and don't stand for the views of CAPITA plc. Huiyu Zhou was partly funded by UK <span id="gs0001">EPSRC</span> under Grant <a class="anchor anchor-primary" href="#gs0001"><span class="anchor-text-container"><span class="anchor-text">EP/N011074/1</span></span></a>, and <span id="gs0002">Royal Society in Newton Advanced Fellowship</span> under Grant <a class="anchor anchor-primary" href="#gs0002"><span class="anchor-text-container"><span class="anchor-text">NA160342</span></span></a>. Winifred Ijomah is supported under <span id="gs0003">EPSRC</span> (Grant no. <a class="anchor anchor-primary" href="#gs0003"><span class="anchor-text-container"><span class="anchor-text">EP/N018427/1</span></span></a>). The authors thank Shanghai Mental Health Center for their help and support. We also thank Dr. Fei Gao from Beihang University, China for his kind support and comment.</div></section><section id="sec0022"><h2 id="cesectitle0025" class="u-h4 u-margin-l-top u-margin-xs-bottom">Ethical approval</h2><div class="u-margin-s-bottom" id="para0075">This article does not contain any studies with human participants or animals performed by any of the authors without the proper ethical approval.</div></section></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="cebibl1"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="cebibsec1"><ol class="references" id="reference-links-cebibsec1"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0001" id="ref-id-bib0001" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a></span><span class="reference" id="sbref0001"><div class="contribution"><div class="authors u-font-sans">N. Yildirm, A. Varol</div><div id="ref-id-sbref0001" class="title text-m">A research on estimation of emotion using EEG signals and brain computer interfaces</div></div><div class="host u-font-sans">Proceedings of the 2nd International Conference on Computer Science and Engineering UBMK'20 (2017), pp. 1132-1136, <a class="anchor anchor-primary" href="https://doi.org/10.1109/UBMK.2017.8093523" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/UBMK.2017.8093523</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85040538297&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0001"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20research%20on%20estimation%20of%20emotion%20using%20EEG%20signals%20and%20brain%20computer%20interfaces&amp;publication_year=2017&amp;author=N.%20Yildirm&amp;author=A.%20Varol" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0001"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0002" id="ref-id-bib0002" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a></span><span class="reference" id="sbref0002"><div class="contribution"><div class="authors u-font-sans">A. Oliveira, C. Pinho, S. Monteiro, A. Marcos, A. Marques</div><div id="ref-id-sbref0002" class="title text-m">Usability testing of a respiratory interface using computer screen and facial expressions videos</div></div><div class="host u-font-sans">Comput. Biol. Med., 43 (2013), pp. 2205-2213, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.compbiomed.2013.10.010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.compbiomed.2013.10.010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0010482513002898/pdfft?md5=c2d19d2fa2a0472d4102e7e32a6a1eb9&amp;pid=1-s2.0-S0010482513002898-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0002"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0010482513002898" aria-describedby="ref-id-sbref0002"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84887247877&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0002"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Usability%20testing%20of%20a%20respiratory%20interface%20using%20computer%20screen%20and%20facial%20expressions%20videos&amp;publication_year=2013&amp;author=A.%20Oliveira&amp;author=C.%20Pinho&amp;author=S.%20Monteiro&amp;author=A.%20Marcos&amp;author=A.%20Marques" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0002"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0003" id="ref-id-bib0003" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></span><span class="reference" id="othref0001"><div class="other-ref"><span>M.-T. Yang, Y.-J. Cheng, Y.-C. Shih, Facial expression recognition for learning status analysis, 2011. doi:<a class="anchor anchor-primary" href="http://dx.doi.org/10.1007/978-3-642-21619-0_18" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-21619-0_18</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.-T.%20Yang%2C%20Y.-J.%20Cheng%2C%20Y.-C.%20Shih%2C%20Facial%20expression%20recognition%20for%20learning%20status%20analysis%2C%202011.%20doi%3A10.1007%2F978-3-642-21619-0_18." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0004" id="ref-id-bib0004" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a></span><span class="reference" id="othref0002"><div class="other-ref"><span>G.S. Bahr, C. Balaban, M. Milanova, H. Choe, Nonverbally smart user interfaces: postural and facial expression data in human computer interaction, 2007.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=G.S.%20Bahr%2C%20C.%20Balaban%2C%20M.%20Milanova%2C%20H.%20Choe%2C%20Nonverbally%20smart%20user%20interfaces%3A%20postural%20and%20facial%20expression%20data%20in%20human%20computer%20interaction%2C%202007." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0005" id="ref-id-bib0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a></span><span class="reference" id="sbref0003"><div class="contribution"><div class="authors u-font-sans">P. Branco, P. Firth, L.M. EncarnaÃ§Ã£o, P. Bonato</div><div id="ref-id-sbref0003" class="title text-m">Faces of emotion in human-computer interaction</div></div><div class="host u-font-sans">Proceedings of the Conference on Human Factors in Computing Systems (2005), pp. 1236-1239, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1056808.1056885" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1056808.1056885</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84869143369&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0003"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Faces%20of%20emotion%20in%20human-computer%20interaction&amp;publication_year=2005&amp;author=P.%20Branco&amp;author=P.%20Firth&amp;author=L.M.%20Encarna%C3%A7%C3%A3o&amp;author=P.%20Bonato" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0003"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0006" id="ref-id-bib0006" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a></span><span class="reference" id="othref0003"><div class="other-ref"><span>O. Grynszpan, J.-C. Martin, J. Nadel, Using facial expressions depicting emotions in a human-computer interface intended for people with autism, 2005. doi:<a class="anchor anchor-primary" href="http://dx.doi.org/10.1007/11550617_41" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/11550617_41</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=O.%20Grynszpan%2C%20J.-C.%20Martin%2C%20J.%20Nadel%2C%20Using%20facial%20expressions%20depicting%20emotions%20in%20a%20human-computer%20interface%20intended%20for%20people%20with%20autism%2C%202005.%20doi%3A10.1007%2F11550617_41." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0007" id="ref-id-bib0007" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></span><span class="reference" id="sbref0004"><div class="contribution"><div class="authors u-font-sans">D.K. Jain, P. Shamsolmoali, P. Sehdev</div><div id="ref-id-sbref0004" class="title text-m">Extended deep neural network for facial emotion recognition</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 120 (2019), pp. 69-74, <a class="anchor anchor-primary" href="https://doi.org/10.1016/J.PATREC.2019.01.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/J.PATREC.2019.01.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S016786551930008X/pdfft?md5=4fe7f1fdf6a320ef2e0efcef551ef975&amp;pid=1-s2.0-S016786551930008X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0004"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S016786551930008X" aria-describedby="ref-id-sbref0004"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060285318&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0004"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Extended%20deep%20neural%20network%20for%20facial%20emotion%20recognition&amp;publication_year=2019&amp;author=D.K.%20Jain&amp;author=P.%20Shamsolmoali&amp;author=P.%20Sehdev" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0004"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0008" id="ref-id-bib0008" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></span><span class="reference" id="sbref0005"><div class="contribution"><div class="authors u-font-sans">J. Shao, Y. Qian</div><div id="ref-id-sbref0005" class="title text-m">Three convolutional neural network models for facial expression recognition in the wild</div></div><div class="host u-font-sans">Neurocomputing, 355 (2019), pp. 82-92, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2019.05.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2019.05.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231219306137/pdfft?md5=b31bdcbb1e65c9378c37c8bc3bd861d0&amp;pid=1-s2.0-S0925231219306137-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0005"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231219306137" aria-describedby="ref-id-sbref0005"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85065595874&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0005"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Three%20convolutional%20neural%20network%20models%20for%20facial%20expression%20recognition%20in%20the%20wild&amp;publication_year=2019&amp;author=J.%20Shao&amp;author=Y.%20Qian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0005"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0009" id="ref-id-bib0009" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a></span><span class="reference" id="sbref0006"><div class="contribution"><div class="authors u-font-sans">C. Lv, Z. Wu, X. Wang, M. Zhou</div><div id="ref-id-sbref0006" class="title text-m">3D facial expression modeling based on facial landmarks in single image</div></div><div class="host u-font-sans">Neurocomputing, 355 (2019), pp. 155-167, <a class="anchor anchor-primary" href="https://doi.org/10.1016/J.NEUCOM.2019.04.050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/J.NEUCOM.2019.04.050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231219306356/pdfft?md5=9a3bce0654bdf2af10e4228199e4625f&amp;pid=1-s2.0-S0925231219306356-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0006"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231219306356" aria-describedby="ref-id-sbref0006"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85065603846&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0006"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3D%20facial%20expression%20modeling%20based%20on%20facial%20landmarks%20in%20single%20image&amp;publication_year=2019&amp;author=C.%20Lv&amp;author=Z.%20Wu&amp;author=X.%20Wang&amp;author=M.%20Zhou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0006"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0010" id="ref-id-bib0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a></span><span class="reference" id="sbref0007"><div class="contribution"><div class="authors u-font-sans">M.Z. Uddin, J.J. Lee, T.-S. Kim</div><div id="ref-id-sbref0007" class="title text-m">An enhanced independent component-based human facial expression recognition from video</div></div><div class="host u-font-sans">IEEE Trans. Consum. Electron., 55 (2009), pp. 2216-2224, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCE.2009.5373791" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCE.2009.5373791</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-75449116730&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0007"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20enhanced%20independent%20component-based%20human%20facial%20expression%20recognition%20from%20video&amp;publication_year=2009&amp;author=M.Z.%20Uddin&amp;author=J.J.%20Lee&amp;author=T.-S.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0007"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0011" id="ref-id-bib0011" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a></span><span class="reference" id="sbref0008"><div class="contribution"><div class="authors u-font-sans">J.J. Lee, Z. Uddin, T.-S. Kim</div><div id="ref-id-sbref0008" class="title text-m">Spatiotemporal human facial expression recognition using fisher independent component analysis and Hidden Markov Model</div></div><div class="host u-font-sans">Proceedings of the 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (2008), pp. 2546-2549</div><div class="comment">EMBSâ€™08 - "Personalized Healthc. through Technol</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1109/IEMBS.2008.4649719" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0008"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-61849128425&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0008"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatiotemporal%20human%20facial%20expression%20recognition%20using%20fisher%20independent%20component%20analysis%20and%20Hidden%20Markov%20Model&amp;publication_year=2008&amp;author=J.J.%20Lee&amp;author=Z.%20Uddin&amp;author=T.-S.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0008"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0012" id="ref-id-bib0012" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></span><span class="reference" id="sbref0016a"><div class="contribution"><div class="authors u-font-sans">D. Engin, C. Ecabert, H.K. Ekenel, J.P. Thiran</div><div id="ref-id-sbref0016a" class="title text-m">Face frontalization for cross-pose facial expression recognition</div></div><div class="host u-font-sans">Eur. Signal Process. Conf., European Signal Processing Conference, EUSIPCO (2018), pp. 1795-1799, <a class="anchor anchor-primary" href="https://doi.org/10.23919/EUSIPCO.2018.8553087" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.23919/EUSIPCO.2018.8553087</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Face%20frontalization%20for%20cross-pose%20facial%20expression%20recognition&amp;publication_year=2018&amp;author=D.%20Engin&amp;author=C.%20Ecabert&amp;author=H.K.%20Ekenel&amp;author=J.P.%20Thiran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0016a"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0013" id="ref-id-bib0013" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></span><span class="reference" id="sbref0009"><div class="contribution"><div class="authors u-font-sans">W. Sun, H. Zhao, Z. Jin</div><div id="ref-id-sbref0009" class="title text-m">An efficient unconstrained facial expression recognition algorithm based on Stack Binarized Auto-Encoders and Binarized Neural Networks</div></div><div class="host u-font-sans">Neurocomputing, 267 (2017), pp. 385-395, <a class="anchor anchor-primary" href="https://doi.org/10.1016/J.NEUCOM.2017.06.050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/J.NEUCOM.2017.06.050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231217311785/pdfft?md5=7fbe0b1c4dc1cca0c398830dccaaac06&amp;pid=1-s2.0-S0925231217311785-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0009"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231217311785" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021789014&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20efficient%20unconstrained%20facial%20expression%20recognition%20algorithm%20based%20on%20Stack%20Binarized%20Auto-Encoders%20and%20Binarized%20Neural%20Networks&amp;publication_year=2017&amp;author=W.%20Sun&amp;author=H.%20Zhao&amp;author=Z.%20Jin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0014" id="ref-id-bib0014" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a></span><span class="reference" id="sbref0010"><div class="contribution"><div class="authors u-font-sans">A. Samal, P.A. Iyengar</div><div id="ref-id-sbref0010" class="title text-m">Automatic recognition and analysis of human faces and facial expressions: a survey</div></div><div class="host u-font-sans">Pattern Recognit. (1992), p. 25, <a class="anchor anchor-primary" href="https://doi.org/10.1016/0031-3203(92)90007-6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/0031-3203(92)90007-6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20recognition%20and%20analysis%20of%20human%20faces%20and%20facial%20expressions%3A%20a%20survey&amp;publication_year=1992&amp;author=A.%20Samal&amp;author=P.A.%20Iyengar" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0010"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0015" id="ref-id-bib0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a></span><span class="reference" id="sbref0011"><div class="contribution"><div class="authors u-font-sans">B. Fasel, J. Luettin</div><div id="ref-id-sbref0011" class="title text-m">Automatic facial expression analysis: a survey</div></div><div class="host u-font-sans">Pattern Recognit., 36 (2003), <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0031-3203(02)00052-3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0031-3203(02)00052-3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20analysis%3A%20a%20survey&amp;publication_year=2003&amp;author=B.%20Fasel&amp;author=J.%20Luettin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0011"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0016" id="ref-id-bib0016" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></span><span class="reference" id="sbref0012"><div class="contribution"><div class="authors u-font-sans">G. Sandbach, S. Zafeiriou, M. Pantic, L. Yin</div><div id="ref-id-sbref0012" class="title text-m">Static and dynamic 3D facial expression recognition: a comprehensive survey</div></div><div class="host u-font-sans">Image Vis. Comput., 30 (2012), <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.imavis.2012.06.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.imavis.2012.06.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Static%20and%20dynamic%203D%20facial%20expression%20recognition%3A%20a%20comprehensive%20survey&amp;publication_year=2012&amp;author=G.%20Sandbach&amp;author=S.%20Zafeiriou&amp;author=M.%20Pantic&amp;author=L.%20Yin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0012"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0017" id="ref-id-bib0017" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a></span><span class="reference" id="sbref0013"><div class="contribution"><div class="authors u-font-sans">Z. Zeng, M. Pantic, G.I. Roisman, T.S. Huang</div><div id="ref-id-sbref0013" class="title text-m">A survey of affect recognition methods: audio, visual, and spontaneous expressions</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 31 (2009), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2008.52" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2008.52</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20affect%20recognition%20methods%3A%20audio%2C%20visual%2C%20and%20spontaneous%20expressions&amp;publication_year=2009&amp;author=Z.%20Zeng&amp;author=M.%20Pantic&amp;author=G.I.%20Roisman&amp;author=T.S.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0013"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0018" id="ref-id-bib0018" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a></span><span class="reference" id="sbref0014"><div class="contribution"><div class="authors u-font-sans">E. Sariyanidi, H. Gunes, A. Cavallaro</div><div id="ref-id-sbref0014" class="title text-m">Automatic analysis of facial affect: a survey of registration, representation, and recognition</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 37 (2015), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2014.2366127" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2014.2366127</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20analysis%20of%20facial%20affect%3A%20a%20survey%20of%20registration%2C%20representation%2C%20and%20recognition&amp;publication_year=2015&amp;author=E.%20Sariyanidi&amp;author=H.%20Gunes&amp;author=A.%20Cavallaro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0014"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0019" id="ref-id-bib0019" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a></span><span class="reference" id="sbref0015"><div class="contribution"><div class="authors u-font-sans">B. Ko, B.Chul Ko</div><div id="ref-id-sbref0015" class="title text-m">A brief review of facial emotion recognition based on visual information</div></div><div class="host u-font-sans">Sensors, 18 (2018), p. 401, <a class="anchor anchor-primary" href="https://doi.org/10.3390/s18020401" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/s18020401</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85041431671&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0015"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20brief%20review%20of%20facial%20emotion%20recognition%20based%20on%20visual%20information&amp;publication_year=2018&amp;author=B.%20Ko&amp;author=B.Chul%20Ko" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0015"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0020" id="ref-id-bib0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a></span><span class="reference" id="sbref0016"><div class="contribution"><div class="authors u-font-sans">S. Baron-Cohen, A. Riviere, M. Fukushima, D. French, J. Hadwin, P. Cross, C. Bryant, M. Sotillo</div><div id="ref-id-sbref0016" class="title text-m">Reading the mind in the face: a cross-cultural and developmental study</div></div><div class="host u-font-sans">Vis. Cogn., 3 (1996), pp. 39-59</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.1080/713756728" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0016"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0001087434&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0016"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Reading%20the%20mind%20in%20the%20face%3A%20a%20cross-cultural%20and%20developmental%20study&amp;publication_year=1996&amp;author=S.%20Baron-Cohen&amp;author=A.%20Riviere&amp;author=M.%20Fukushima&amp;author=D.%20French&amp;author=J.%20Hadwin&amp;author=P.%20Cross&amp;author=C.%20Bryant&amp;author=M.%20Sotillo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0016"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0021" id="ref-id-bib0021" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a></span><span class="reference" id="sbref0017"><div class="contribution"><div class="authors u-font-sans">K. Burton, A. Kaszniak</div><div id="ref-id-sbref0017" class="title text-m">Emotional experience and facial expression in Alzheimer's disease</div></div><div class="host u-font-sans">Aging, Neuropsychol. Cogn., 13 (2006), <a class="anchor anchor-primary" href="https://doi.org/10.1080/13825580600735085" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1080/13825580600735085</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotional%20experience%20and%20facial%20expression%20in%20Alzheimers%20disease&amp;publication_year=2006&amp;author=K.%20Burton&amp;author=A.%20Kaszniak" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0017"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0022" id="ref-id-bib0022" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></span><span class="reference" id="sbref0018"><div class="contribution"><div class="authors u-font-sans">S. Passardi, P. Peyk, M. Rufer, T.S.H. Wingenbach, M.C. Pfaltz</div><div id="ref-id-sbref0018" class="title text-m">Facial mimicry, facial emotion recognition and alexithymia in post-traumatic stress disorder</div></div><div class="host u-font-sans">Behav. Res. Ther., 122 (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.brat.2019.103436" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.brat.2019.103436</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20mimicry%2C%20facial%20emotion%20recognition%20and%20alexithymia%20in%20post-traumatic%20stress%20disorder&amp;publication_year=2019&amp;author=S.%20Passardi&amp;author=P.%20Peyk&amp;author=M.%20Rufer&amp;author=T.S.H.%20Wingenbach&amp;author=M.C.%20Pfaltz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0018"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0023" id="ref-id-bib0023" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></span><span class="reference" id="sbref0019"><div class="contribution"><div class="authors u-font-sans">J.D. Henry, P.G. Rendell, A. Scicluna, M. Jackson, L.H. Phillips</div><div id="ref-id-sbref0019" class="title text-m">Emotion experience, expression, and regulation in Alzheimer's disease</div></div><div class="host u-font-sans">Psychol. Aging., 24 (2009), <a class="anchor anchor-primary" href="https://doi.org/10.1037/a0014001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/a0014001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20experience%2C%20expression%2C%20and%20regulation%20in%20Alzheimers%20disease&amp;publication_year=2009&amp;author=J.D.%20Henry&amp;author=P.G.%20Rendell&amp;author=A.%20Scicluna&amp;author=M.%20Jackson&amp;author=L.H.%20Phillips" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0019"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0024" id="ref-id-bib0024" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a></span><span class="reference" id="sbref0020"><div class="contribution"><div class="authors u-font-sans">M.C. Smith</div><div id="ref-id-sbref0020" class="title text-m">Facial expression in mild dementia of the Alzheimer type</div></div><div class="host u-font-sans">Behav. Neurol., 8 (1995), pp. 149-156</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0029598421&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0020"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20in%20mild%20dementia%20of%20the%20Alzheimer%20type&amp;publication_year=1995&amp;author=M.C.%20Smith" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0020"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0025" id="ref-id-bib0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a></span><span class="reference" id="othref0005"><div class="other-ref"><span>A. Lundqvist, D. Flykt, A. Ã–hman, The Karolinska Directed Emotional Faces KDEF, CD ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet, 1998.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Lundqvist%2C%20D.%20Flykt%2C%20A.%20%C3%96hman%2C%20The%20Karolinska%20Directed%20Emotional%20Faces%20KDEF%2C%20CD%20ROM%20from%20Department%20of%20Clinical%20Neuroscience%2C%20Psychology%20section%2C%20Karolinska%20Institutet%2C%201998." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0026" id="ref-id-bib0026" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a></span><span class="reference" id="othref0006"><div class="other-ref"><span>M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, Coding facial expressions with Gabor wavelets, in: Proceedings of the 3rd IEEE International Conference on Automatic Face and Gesture Recognition, IEEE Computer Society, n.d.: pp. 200â€“205. doi:<a class="anchor anchor-primary" href="http://dx.doi.org/10.1109/AFGR.1998.670949" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.1998.670949</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Lyons%2C%20S.%20Akamatsu%2C%20M.%20Kamachi%2C%20J.%20Gyoba%2C%20Coding%20facial%20expressions%20with%20Gabor%20wavelets%2C%20in%3A%20Proceedings%20of%20the%203rd%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%2C%20IEEE%20Computer%20Society%2C%20n.d.%3A%20pp.%20200%E2%80%93205.%20doi%3A10.1109%2FAFGR.1998.670949." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0027" id="ref-id-bib0027" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></span><span class="reference" id="othref0007"><div class="other-ref"><span>T. Kanade, J.F. Cohn, Yingli Tian, Comprehensive database for facial expression analysis, in: Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), IEEE Computer Society, n.d.: pp. 46â€“53. doi:<a class="anchor anchor-primary" href="http://dx.doi.org/10.1109/AFGR.2000.840611" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.2000.840611</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=T.%20Kanade%2C%20J.F.%20Cohn%2C%20Yingli%20Tian%2C%20Comprehensive%20database%20for%20facial%20expression%20analysis%2C%20in%3A%20Proceedings%20of%20the%20Fourth%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20(Cat.%20No.%20PR00580)%2C%20IEEE%20Computer%20Society%2C%20n.d.%3A%20pp.%2046%E2%80%9353.%20doi%3A10.1109%2FAFGR.2000.840611." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0028" id="ref-id-bib0028" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a></span><span class="reference" id="sbref0021"><div class="contribution"><div class="authors u-font-sans">P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews</div><div id="ref-id-sbref0021" class="title text-m">The Extended Cohnâ€“Kanade Dataset (CK+): a complete dataset for action unit and emotion-specified expression</div></div><div class="host u-font-sans">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, IEEE (2010), pp. 94-101, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPRW.2010.5543262" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPRW.2010.5543262</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77956509035&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0021"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Extended%20CohnKanade%20Dataset%20%3A%20a%20complete%20dataset%20for%20action%20unit%20and%20emotion-specified%20expression&amp;publication_year=2010&amp;author=P.%20Lucey&amp;author=J.F.%20Cohn&amp;author=T.%20Kanade&amp;author=J.%20Saragih&amp;author=Z.%20Ambadar&amp;author=I.%20Matthews" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0021"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0029" id="ref-id-bib0029" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a></span><span class="reference" id="othref0008"><div class="other-ref"><span>Challenges in representation learning: facial expression recognition challenge | Kaggle, (n.d.). <a class="anchor anchor-primary" href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/overview" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/overview</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed April 15, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Challenges%20in%20representation%20learning%3A%20facial%20expression%20recognition%20challenge%20%7C%20Kaggle%2C%20(n.d.).%20https%3A%2F%2Fwww.kaggle.com%2Fc%2Fchallenges-in-representation-learning-facial-expression-recognition-challenge%2Foverview(accessed%20April%2015%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0030" id="ref-id-bib0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a></span><span class="reference" id="sbref0022"><div class="contribution"><div class="authors u-font-sans">A. Mollahosseini, B. Hasani, M.H. Mahoor</div><div id="ref-id-sbref0022" class="title text-m">AffectNet: a database for facial expression, valence, and arousal computing in the wild</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 10 (2019), pp. 18-31, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2740923" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2740923</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85028454548&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0022"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AffectNet%3A%20a%20database%20for%20facial%20expression%2C%20valence%2C%20and%20arousal%20computing%20in%20the%20wild&amp;publication_year=2019&amp;author=A.%20Mollahosseini&amp;author=B.%20Hasani&amp;author=M.H.%20Mahoor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0022"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0031" id="ref-id-bib0031" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a></span><span class="reference" id="othref0009"><div class="other-ref"><span>P. Viola, M. Jones, Rapid object detection using a boosted cascade of simple features, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, n.d.: p. I-511-I-518. doi:<a class="anchor anchor-primary" href="http://dx.doi.org/10.1109/CVPR.2001.990517" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2001.990517</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Viola%2C%20M.%20Jones%2C%20Rapid%20object%20detection%20using%20a%20boosted%20cascade%20of%20simple%20features%2C%20in%3A%20Proceedings%20of%20the%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition.%20IEEE%20Computer%20Society%2C%20n.d.%3A%20p.%20I-511-I-518.%20doi%3A10.1109%2FCVPR.2001.990517." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0032" id="ref-id-bib0032" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a></span><span class="reference" id="othref0010"><div class="other-ref"><span>Detect objects using the Viola-Jones algorithm - MATLAB - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/vision/ref/vision.cascadeobjectdetector-system-object.html" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/vision/ref/vision.cascadeobjectdetector-system-object.html</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed January 29, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Detect%20objects%20using%20the%20Viola-Jones%20algorithm%20-%20MATLAB%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fvision%2Fref%2Fvision.cascadeobjectdetector-system-object.html(accessed%20January%2029%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0033" id="ref-id-bib0033" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a></span><span class="reference" id="othref0011"><div class="other-ref"><span>Concatenate arrays - MATLAB cat - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/matlab/ref/cat.html" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/matlab/ref/cat.html</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed May 7, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Concatenate%20arrays%20-%20MATLAB%20cat%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fmatlab%2Fref%2Fcat.html(accessed%20May%207%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0034" id="ref-id-bib0034" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></span><span class="reference" id="othref0012"><div class="other-ref"><span>Pretrained AlexNet convolutional neural network - MATLAB alexnet - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/deeplearning/ref/alexnet.html;jsessionid=c4669357f290858d36ed1bcbf8cf" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/deeplearning/ref/alexnet.html;jsessionid=c4669357f290858d36ed1bcbf8cf</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed September 21, 2018).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Pretrained%20AlexNet%20convolutional%20neural%20network%20-%20MATLAB%20alexnet%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fdeeplearning%2Fref%2Falexnet.html%3Bjsessionid%3Dc4669357f290858d36ed1bcbf8cf(accessed%20September%2021%2C%202018)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0035" id="ref-id-bib0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></span><span class="reference" id="sbref0023"><div class="contribution"><div class="authors u-font-sans">M. Matsugu, K. Mori, Y. Mitari, Y. Kaneda</div><div id="ref-id-sbref0023" class="title text-m">Subject independent facial expression recognition with robust face detection using a convolutional neural network</div></div><div class="host u-font-sans">Neural Netw., 16 (2003), pp. 555-559, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0893-6080(03)00115-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0893-6080(03)00115-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0893608003001151/pdfft?md5=fe80ce0a82acc47c9ce0b687973c438f&amp;pid=1-s2.0-S0893608003001151-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0023"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0893608003001151" aria-describedby="ref-id-sbref0023"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037707305&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0023"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Subject%20independent%20facial%20expression%20recognition%20with%20robust%20face%20detection%20using%20a%20convolutional%20neural%20network&amp;publication_year=2003&amp;author=M.%20Matsugu&amp;author=K.%20Mori&amp;author=Y.%20Mitari&amp;author=Y.%20Kaneda" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0023"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0036" id="ref-id-bib0036" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></span><span class="reference" id="sbref0024"><div class="contribution"><div class="authors u-font-sans">R. Collobert, J. Weston</div><div id="ref-id-sbref0024" class="title text-m">A unified architecture for natural language processing</div></div><div class="host u-font-sans">Proceedings of the 25th International Conference on Machine Learning - ICML â€™08, New York, New York, USA, ACM Press (2008), pp. 160-167, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1390156.1390177" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1390156.1390177</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20unified%20architecture%20for%20natural%20language%20processing&amp;publication_year=2008&amp;author=R.%20Collobert&amp;author=J.%20Weston" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0024"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0037" id="ref-id-bib0037" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a></span><span class="reference" id="othref0013"><div class="other-ref"><span>A. Krizhevsky, I. Sutskever, G.E. Hinton, ImageNet classification with deep convolutional neural networks, n.d.<a class="anchor anchor-primary" href="http://code.google.com/p/cuda-convnet/" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://code.google.com/p/cuda-convnet/</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed September 21, 2018).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Krizhevsky%2C%20I.%20Sutskever%2C%20G.E.%20Hinton%2C%20ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%2C%20n.d.http%3A%2F%2Fcode.google.com%2Fp%2Fcuda-convnet%2F(accessed%20September%2021%2C%202018)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0038" id="ref-id-bib0038" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a></span><span class="reference" id="sbref0025"><div class="contribution"><div class="authors u-font-sans">X. Chen, X. Yang, M. Wang, J. Zou</div><div id="ref-id-sbref0025" class="title text-m">Convolution neural network for automatic facial expression recognition</div></div><div class="host u-font-sans">Proceedings of the International Conference Applied System Innovation, IEEE (2017), pp. 814-817, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASI.2017.7988558" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASI.2017.7988558</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85028572014&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0025"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Convolution%20neural%20network%20for%20automatic%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=X.%20Chen&amp;author=X.%20Yang&amp;author=M.%20Wang&amp;author=J.%20Zou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0025"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0039" id="ref-id-bib0039" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a></span><span class="reference" id="sbref0026"><div class="contribution"><div class="authors u-font-sans">K. Shan, J. Guo, W. You, D. Lu, R. Bie</div><div id="ref-id-sbref0026" class="title text-m">Automatic facial expression recognition based on a deep convolutional-neural-network structure</div></div><div class="host u-font-sans">Proceedings of the IEEE 15th International Conference on Software Engineering Research, Management and Application, IEEE (2017), pp. 123-128, <a class="anchor anchor-primary" href="https://doi.org/10.1109/SERA.2017.7965717" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/SERA.2017.7965717</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85026642686&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0026"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20recognition%20based%20on%20a%20deep%20convolutional-neural-network%20structure&amp;publication_year=2017&amp;author=K.%20Shan&amp;author=J.%20Guo&amp;author=W.%20You&amp;author=D.%20Lu&amp;author=R.%20Bie" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0026"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0040" id="ref-id-bib0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></span><span class="reference" id="sbref0027"><div class="contribution"><div class="authors u-font-sans">X. Han, Y. Zhong, L. Cao, L. Zhang</div><div id="ref-id-sbref0027" class="title text-m">Pre-trained alexnet architecture with pyramid pooling and supervision for high spatial resolution remote sensing image scene classification</div></div><div class="host u-font-sans">Remote Sens. (2017), p. 9, <a class="anchor anchor-primary" href="https://doi.org/10.3390/rs9080848" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/rs9080848</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84995653305&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0027"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Pre-trained%20alexnet%20architecture%20with%20pyramid%20pooling%20and%20supervision%20for%20high%20spatial%20resolution%20remote%20sensing%20image%20scene%20classification&amp;publication_year=2017&amp;author=X.%20Han&amp;author=Y.%20Zhong&amp;author=L.%20Cao&amp;author=L.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0027"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0041" id="ref-id-bib0041" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></span><span class="reference" id="sbref0028"><div class="contribution"><div class="authors u-font-sans">U. Chavan, D. Kulkarni</div><div id="ref-id-sbref0028" class="title text-m">Optimizing deep convolutional neural network for facial expression recognitions</div></div><div class="host u-font-sans">Advances in Intelligent Systems and Computing, Springer Verlag (2019), pp. 185-196, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-13-1402-5_14" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-13-1402-5_14</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85052384485&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0028"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Optimizing%20deep%20convolutional%20neural%20network%20for%20facial%20expression%20recognitions&amp;publication_year=2019&amp;author=U.%20Chavan&amp;author=D.%20Kulkarni" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0028"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0042" id="ref-id-bib0042" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></span><span class="reference" id="othref0014"><div class="other-ref"><span>Feature Extraction Using AlexNet - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/deeplearning/examples/feature-extraction-using-alexnet.html;jsessionid=a9dd0dd508fd2b96854cd6f11d5c" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/deeplearning/examples/feature-extraction-using-alexnet.html;jsessionid=a9dd0dd508fd2b96854cd6f11d5c</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed January 23, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Feature%20Extraction%20Using%20AlexNet%20-%20MATLAB%20%26%20Simulink%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fdeeplearning%2Fexamples%2Ffeature-extraction-using-alexnet.html%3Bjsessionid%3Da9dd0dd508fd2b96854cd6f11d5c(accessed%20January%2023%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0043" id="ref-id-bib0043" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></span><span class="reference" id="sbref0029"><div class="contribution"><div class="authors u-font-sans">P. McAllister, H. Zheng, R. Bond, A. Moorhead</div><div id="ref-id-sbref0029" class="title text-m">Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets</div></div><div class="host u-font-sans">Comput. Biol. Med., 95 (2018), pp. 217-233, <a class="anchor anchor-primary" href="https://doi.org/10.1016/J.COMPBIOMED.2018.02.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/J.COMPBIOMED.2018.02.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0010482518300386/pdfft?md5=e278c8c70b0cf34c1989c1a5ec24ccb8&amp;pid=1-s2.0-S0010482518300386-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0029"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0010482518300386" aria-describedby="ref-id-sbref0029"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044474907&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0029"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Combining%20deep%20residual%20neural%20network%20features%20with%20supervised%20machine%20learning%20algorithms%20to%20classify%20diverse%20food%20image%20datasets&amp;publication_year=2018&amp;author=P.%20McAllister&amp;author=H.%20Zheng&amp;author=R.%20Bond&amp;author=A.%20Moorhead" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0029"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0044" id="ref-id-bib0044" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></span><span class="reference" id="othref0015"><div class="other-ref"><span>Visualize High-Dimensional Data Using t-SNE - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/stats/visualize-high-dimensional-data-using-t-sne.html" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/stats/visualize-high-dimensional-data-using-t-sne.html</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed April 15, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Visualize%20High-Dimensional%20Data%20Using%20t-SNE%20-%20MATLAB%20%26%20Simulink%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fstats%2Fvisualize-high-dimensional-data-using-t-sne.html(accessed%20April%2015%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0045" id="ref-id-bib0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></span><span class="reference" id="othref0016"><div class="other-ref"><span>L. van der Maaten, Barnes-Hut-SNE, (2013). <a class="anchor anchor-primary" href="http://arxiv.org/abs/1301.3342" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/1301.3342</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed April 15, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=L.%20van%20der%20Maaten%2C%20Barnes-Hut-SNE%2C%20(2013).%20http%3A%2F%2Farxiv.org%2Fabs%2F1301.3342(accessed%20April%2015%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0046" id="ref-id-bib0046" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a></span><span class="reference" id="othref0017"><div class="other-ref"><span>Visualize Activations of a Convolutional Neural Network - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/deeplearning/examples/visualize-activations-of-a-convolutional-neural-network.html" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/deeplearning/examples/visualize-activations-of-a-convolutional-neural-network.html</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed January 23, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Visualize%20Activations%20of%20a%20Convolutional%20Neural%20Network%20-%20MATLAB%20%26%20Simulink%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fdeeplearning%2Fexamples%2Fvisualize-activations-of-a-convolutional-neural-network.html(accessed%20January%2023%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0047" id="ref-id-bib0047" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></span><span class="reference" id="sbref0030"><div class="contribution"><div class="authors u-font-sans">Q. Ye, N. Ye, T. Yin</div><div id="ref-id-sbref0030" class="title text-m">Fast orthogonal linear discriminant analysis with application to image classification</div></div><div class="host u-font-sans">Neurocomputing, 158 (2015), pp. 216-224, <a class="anchor anchor-primary" href="https://doi.org/10.1016/J.NEUCOM.2015.01.045" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/J.NEUCOM.2015.01.045</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231215000909/pdfft?md5=98824327409d7f2b889be5a815182654&amp;pid=1-s2.0-S0925231215000909-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0030"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231215000909" aria-describedby="ref-id-sbref0030"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84926527852&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fast%20orthogonal%20linear%20discriminant%20analysis%20with%20application%20to%20image%20classification&amp;publication_year=2015&amp;author=Q.%20Ye&amp;author=N.%20Ye&amp;author=T.%20Yin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0048" id="ref-id-bib0048" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a></span><span class="reference" id="sbref0031"><div class="contribution"><div class="authors u-font-sans">N.A.A. Shashoa, N.A. Salem, I.N. Jleta, O. Abusaeeda</div><div id="ref-id-sbref0031" class="title text-m">Classification depend on linear discriminant analysis using desired outputs</div></div><div class="host u-font-sans">Proceedings of the 17th International Conference Science Technology and Automation Control Computing Engineering, IEEE (2016), pp. 328-332, <a class="anchor anchor-primary" href="https://doi.org/10.1109/STA.2016.7952041" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/STA.2016.7952041</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85024369700&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0031"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Classification%20depend%20on%20linear%20discriminant%20analysis%20using%20desired%20outputs&amp;publication_year=2016&amp;author=N.A.A.%20Shashoa&amp;author=N.A.%20Salem&amp;author=I.N.%20Jleta&amp;author=O.%20Abusaeeda" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0031"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0049" id="ref-id-bib0049" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></span><span class="reference" id="othref0018"><div class="other-ref"><span>Compute convolutional neural network layer activations - MATLAB activations - MathWorks United Kingdom, (n.d.). <a class="anchor anchor-primary" href="https://uk.mathworks.com/help/deeplearning/ref/activations.html" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://uk.mathworks.com/help/deeplearning/ref/activations.html</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>(accessed January 30, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Compute%20convolutional%20neural%20network%20layer%20activations%20-%20MATLAB%20activations%20-%20MathWorks%20United%20Kingdom%2C%20(n.d.).%20https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fdeeplearning%2Fref%2Factivations.html(accessed%20January%2030%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0050" id="ref-id-bib0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></span><span class="reference" id="sbref0032"><div class="contribution"><div class="authors u-font-sans">N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, A.M. Dobaie</div><div id="ref-id-sbref0032" class="title text-m">Facial expression recognition via learning deep sparse autoencoders</div></div><div class="host u-font-sans">Neurocomputing, 273 (2018), pp. 643-649, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2017.08.043" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2017.08.043</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231217314649/pdfft?md5=fdf1b5dff70b49ece7fe836cd258b5c2&amp;pid=1-s2.0-S0925231217314649-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0032"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231217314649" aria-describedby="ref-id-sbref0032"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85029156229&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0032"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20via%20learning%20deep%20sparse%20autoencoders&amp;publication_year=2018&amp;author=N.%20Zeng&amp;author=H.%20Zhang&amp;author=B.%20Song&amp;author=W.%20Liu&amp;author=Y.%20Li&amp;author=A.M.%20Dobaie" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0032"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0051" id="ref-id-bib0051" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></span><span class="reference" id="sbref0033"><div class="contribution"><div class="authors u-font-sans">N. Zeng, Z. Wang, H. Zhang, K.-E. Kim, Y. Li, X. Liu</div><div id="ref-id-sbref0033" class="title text-m">An improved particle filter with a novel hybrid proposal distribution for quantitative analysis of gold immunochromatographic strips</div></div><div class="host u-font-sans">IEEE Trans. Nanotechnol., 18 (2019), pp. 819-829, <a class="anchor anchor-primary" href="https://doi.org/10.1109/tnano.2019.2932271" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/tnano.2019.2932271</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85070979566&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0033"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20improved%20particle%20filter%20with%20a%20novel%20hybrid%20proposal%20distribution%20for%20quantitative%20analysis%20of%20gold%20immunochromatographic%20strips&amp;publication_year=2019&amp;author=N.%20Zeng&amp;author=Z.%20Wang&amp;author=H.%20Zhang&amp;author=K.-E.%20Kim&amp;author=Y.%20Li&amp;author=X.%20Liu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0033"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0052" id="ref-id-bib0052" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a></span><span class="reference" id="sbref0034"><div class="contribution"><div class="authors u-font-sans">N. Zeng, Z. Wang, H. Zhang, W. Liu, F.E. Alsaadi</div><div id="ref-id-sbref0034" class="title text-m">Deep belief networks for quantitative analysis of a gold immunochromatographic strip</div></div><div class="host u-font-sans">Cognit. Comput., 8 (2016), pp. 684-692, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12559-016-9404-x" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12559-016-9404-x</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84964598999&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0034"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20belief%20networks%20for%20quantitative%20analysis%20of%20a%20gold%20immunochromatographic%20strip&amp;publication_year=2016&amp;author=N.%20Zeng&amp;author=Z.%20Wang&amp;author=H.%20Zhang&amp;author=W.%20Liu&amp;author=F.E.%20Alsaadi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0034"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0053" id="ref-id-bib0053" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a></span><span class="reference" id="sbref0035"><div class="contribution"><div class="authors u-font-sans">L. Ding, H. Li, C. Hu, W. Zhang, S. Wang</div><div id="ref-id-sbref0035" class="title text-m">Alexnet feature extraction and multi-kernel learning for object-oriented classification</div></div><div class="host u-font-sans">Proceedings of the International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences- ISPRS Archives., International Society for Photogrammetry and Remote Sensing (2018), pp. 277-281, <a class="anchor anchor-primary" href="https://doi.org/10.5194/isprs-archives-XLII-3-277-2018" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.5194/isprs-archives-XLII-3-277-2018</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85046944752&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0035"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Alexnet%20feature%20extraction%20and%20multi-kernel%20learning%20for%20object-oriented%20classification&amp;publication_year=2018&amp;author=L.%20Ding&amp;author=H.%20Li&amp;author=C.%20Hu&amp;author=W.%20Zhang&amp;author=S.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0035"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0054" id="ref-id-bib0054" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a></span><span class="reference" id="sbref0036"><div class="contribution"><div class="authors u-font-sans">Y. Li, J. Zeng, S. Shan, X. Chen</div><div id="ref-id-sbref0036" class="title text-m">Occlusion aware facial expression recognition using CNN with attention mechanism</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 28 (2019), pp. 2439-2450, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2018.2886767" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2018.2886767</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85058877186&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0036"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Occlusion%20aware%20facial%20expression%20recognition%20using%20CNN%20with%20attention%20mechanism&amp;publication_year=2019&amp;author=Y.%20Li&amp;author=J.%20Zeng&amp;author=S.%20Shan&amp;author=X.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0036"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0055" id="ref-id-bib0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a></span><span class="reference" id="sbref0037"><div class="contribution"><div class="authors u-font-sans">S. Jyoti, G. Sharma, A. Dhall</div><div id="ref-id-sbref0037" class="title text-m">A single hierarchical network for face, action unit and emotion detection</div></div><div class="host u-font-sans">Proceedings of the Digital Image Computing: Techniques and Applications, IEEE (2018), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/DICTA.2018.8615852" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/DICTA.2018.8615852</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20single%20hierarchical%20network%20for%20face%2C%20action%20unit%20and%20emotion%20detection&amp;publication_year=2018&amp;author=S.%20Jyoti&amp;author=G.%20Sharma&amp;author=A.%20Dhall" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0037"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0056" id="ref-id-bib0056" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a></span><span class="reference" id="sbref0038"><div class="contribution"><div class="authors u-font-sans">K. Rujirakul, C. So-In</div><div id="ref-id-sbref0038" class="title text-m">Histogram equalized deep pca with ELM classification for expressive face recognition</div></div><div class="host u-font-sans">Proceedings of the International Workshop on Advanced Image Technology, IEEE (2018), pp. 1-4, <a class="anchor anchor-primary" href="https://doi.org/10.1109/IWAIT.2018.8369725" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/IWAIT.2018.8369725</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048820068&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0038"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Histogram%20equalized%20deep%20pca%20with%20ELM%20classification%20for%20expressive%20face%20recognition&amp;publication_year=2018&amp;author=K.%20Rujirakul&amp;author=C.%20So-In" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0038"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0057" id="ref-id-bib0057" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></span><span class="reference" id="sbref0039"><div class="contribution"><div class="authors u-font-sans">P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews</div><div id="ref-id-sbref0039" class="title text-m">The extended Cohnâ€“Kanade dataset (CK+): a complete dataset for action unit and emotion-specified expression</div></div><div class="host u-font-sans">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops CVPRW (2010), <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPRW.2010.5543262" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPRW.2010.5543262</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20extended%20CohnKanade%20dataset%20%3A%20a%20complete%20dataset%20for%20action%20unit%20and%20emotion-specified%20expression&amp;publication_year=2010&amp;author=P.%20Lucey&amp;author=J.F.%20Cohn&amp;author=T.%20Kanade&amp;author=J.%20Saragih&amp;author=Z.%20Ambadar&amp;author=I.%20Matthews" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0039"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0058" id="ref-id-bib0058" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></span><span class="reference" id="sbref0040"><div class="contribution"><div class="authors u-font-sans">T. Jabid, M.H. Kabir, O. Chae</div><div id="ref-id-sbref0040" class="title text-m">Robust facial expression recognition based on local directional pattern</div></div><div class="host u-font-sans">ETRI J., 32 (2010), pp. 784-794, <a class="anchor anchor-primary" href="https://doi.org/10.4218/etrij.10.1510.0132" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.4218/etrij.10.1510.0132</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-78049351030&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0040"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Robust%20facial%20expression%20recognition%20based%20on%20local%20directional%20pattern&amp;publication_year=2010&amp;author=T.%20Jabid&amp;author=M.H.%20Kabir&amp;author=O.%20Chae" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0040"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0059" id="ref-id-bib0059" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a></span><span class="reference" id="sbref0041"><div class="contribution"><div class="authors u-font-sans">A.T. Lopes, E. de Aguiar, A.F. De Souza, T. Oliveira-Santos</div><div id="ref-id-sbref0041" class="title text-m">Facial expression recognition with convolutional neural networks: coping with few data and the training sample order</div></div><div class="host u-font-sans">Pattern Recognit. (2017), p. 61, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2016.07.026" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2016.07.026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20with%20convolutional%20neural%20networks%3A%20coping%20with%20few%20data%20and%20the%20training%20sample%20order&amp;publication_year=2017&amp;author=A.T.%20Lopes&amp;author=E.%20de%20Aguiar&amp;author=A.F.%20De%20Souza&amp;author=T.%20Oliveira-Santos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0041"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0060" id="ref-id-bib0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a></span><span class="reference" id="sbref0042"><div class="contribution"><div class="authors u-font-sans">G. Fanelli, A. Yao, P.-L. Noel, J. Gall, L. Van Gool</div><div id="ref-id-sbref0042" class="title text-m">Hough Forest-Based Facial Expression Recognition from Video Sequences</div></div><div class="host u-font-sans">Springer, Berlin, Heidelberg (2012), pp. 195-206, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-35749-7_15" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-35749-7_15</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84871143691&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0042"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Hough%20Forest-Based%20Facial%20Expression%20Recognition%20from%20Video%20Sequences&amp;publication_year=2012&amp;author=G.%20Fanelli&amp;author=A.%20Yao&amp;author=P.-L.%20Noel&amp;author=J.%20Gall&amp;author=L.%20Van%20Gool" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0042"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0061" id="ref-id-bib0061" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></span><span class="reference" id="othref0002a"><div class="other-ref"><span>NN SVG. n.d.. [accessed 14 December 2019].</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=NN%20SVG.%20n.d..%20%5Baccessed%2014%20December%202019%5D." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (111)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S0031320322006367"><span class="anchor-text-container"><span class="anchor-text">A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition</span></span></a></h3><div>2023, Pattern Recognition</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="spara018">Considering most deep learning-based methods heavily depend on huge labels, it is still a challenging issue for facial expression recognition to extract discriminative features of training samples with limited labels. Given above, we propose a discriminatively deep fusion (DDF) approach based on an improved conditional generative adversarial network (im-cGAN) to learn abstract representation of facial expressions. First, we employ facial images with action units (AUs) to train the im-cGAN to generate more labeled expression samples. Subsequently, we utilize global features learned by the global-based module and the local features learned by the region-based module to obtain the fused feature representation. Finally, we design the discriminative loss function (D-loss) that expands the inter-class variations while minimizing the intra-class distances to enhance the discrimination of fused features. Experimental results on JAFFE, CK+, Oulu-CASIA, and KDEF datasets demonstrate the proposed approach is superior to some state-of-the-art methods.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S0925231222012656"><span class="anchor-text-container"><span class="anchor-text">In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study</span></span></a></h3><div>2022, Neurocomputing</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp005">Many researchers have been seeking robust emotion recognition system for already last two decades. It would advance computer systems to a new level of interaction, providing much more natural feedback during humanâ€“computer interaction due to analysis of user affect state. However, one of the key problems in this domain is a lack of generalization ability: we observe dramatic degradation of model performance when it was trained on one corpus and evaluated on another one. Although some studies were done in this direction, visual modality still remains under-investigated. Therefore, we introduce the visual cross-corpus study conducted with the utilization of eight corpora, which differ in recording conditions, participantsâ€™ appearance characteristics, and complexity of data processing. We propose a visual-based end-to-end emotion recognition framework, which consists of the robust pre-trained backbone model and temporal sub-system in order to model temporal dependencies across many video frames. In addition, a detailed analysis of mistakes and advantages of the backbone model is provided, demonstrating its high ability of generalization. Our results show that the backbone model has achieved the accuracy of 66.4% on the AffectNet dataset, outperforming all the state-of-the-art results. Moreover, the CNN-LSTM model has demonstrated a decent efficacy on dynamic visual datasets during cross-corpus experiments, achieving comparable with state-of-the-art results. In addition, we provide backbone and CNN-LSTM models for future researchers: they can be accessed via GitHub.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="/science/article/pii/S0957417421006977"><span class="anchor-text-container"><span class="anchor-text">Negative emotions detection on online mental-health related patients texts using the deep learning with MHA-BCNN model</span></span></a></h3><div>2021, Expert Systems with Applications</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-2-title" aria-controls="citing-articles-article-2" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp010">Mining the emotions in the text related to mental health-care oriented is a challenging aspect, especially dealing with a long-text sequence of data. The extraction of emotions depends upon the various psychological depression factors like negative and ambiguity. Identifying these factors is the most perplexing task for every psychiatrist to treat their patients. Our study includes the deep learning (DL) models with global vector representations (GloVe) embeddings to capture the text sequence of data. We proposed a model multi-head attention with bidirectional long short-term memory and convolutional neural network (MHA-BCNN) is a pre-eminent mechanism that outperforms better than past research works for capturing the negative text-based emotions. In this paper, by using DL extracted the various negative mental-health emotions like addiction, anxiety, depression, insomnia, stress, and obsessive cleaning disorder (OCD). By using the GloVe embeddings and handled the ambiguity factors like multiple emotion words in a certain sequence. As we proposed a vigorous appliance in our research to capture and hoard the long-term dependencies. We extracted the questions related to mental health issues were posted by the patients in an online mental healthcare-oriented platform. We efficaciously handled both negative and ambiguity factors at the document level. Our suggested exemplary MHA-BCNN surmounts various aspects from preceding research works and ensued preeminent performance. Experimental results show that our proposed framework MHA-BCNN outperformed than the erstwhile research works.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/JSEN.2020.3028075" target="_blank"><span class="anchor-text-container"><span class="anchor-text">GA-SVM-Based Facial Emotion Recognition Using Facial Geometric Features</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2021, IEEE Sensors Journal</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.30880/jscdm.2021.02.01.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Facial Expression Recognition Based on Deep Learning Convolution Neural Network: A Review</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2021, Journal of Soft Computing and Data Mining</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.1007/s10489-020-01888-w" target="_blank"><span class="anchor-text-container"><span class="anchor-text">COVIDetectioNet: COVID-19 diagnosis system based on X-ray images using features selected from pre-learned deep features ensemble</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2021, Applied Intelligence</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-85078252482&amp;md5=4a9f8d373e892922c4fb5fb1c2a17e" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><div class="article-biography article-biography-has-image" id="auth1Bio1"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx1.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara025"><strong>Zixiang Fei</strong> received his Bachelor degree in Liverpool John Moores University and Master Degree in University of York. He is currently a Ph.D. student in University of Strathclyde. His major research interests include computer vision, machine learning, object recognition and deep learning.</div></div></div><div class="article-biography article-biography-has-image" id="auth2Bio2"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx2.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara026"><strong>Erfu Yang</strong> is a Lecturer in University of Strathclyde under Strathclyde Chancellor's Fellowship Scheme. In 2008, he received his Ph.D. degree in robotics in the interdisciplinary area of robotics and autonomous Systems from the University of Essex. His main research interests include robotics, autonomous systems, computer vision, image/signal processing, mechatronics, data analytics, etc.</div></div></div><div class="article-biography article-biography-has-image" id="auth3Bio3"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx3.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara027"><strong>David Day-Uei Li</strong> received the Ph.D. degree in electrical engineering from the National Taiwan University, Taipei, Taiwan, in 2001. He then joined the Industrial Technology Research Institute, Taiwan, working on CMOS communication chipsets. From 2007 to 2011 he worked at the University of Edinburgh before he took the lectureship in biomedical engineering at the University of Sussex. In 2014 he joined the University of Strathclyde, Glasgow, as a Senior Lecturer. His research interests include mixed signal circuits, CMOS sensors and systems, embedded systems, FLIM systems &amp; analysis, and optical communications.</div></div></div><div class="article-biography article-biography-has-image" id="auth4Bio4"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx4.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara028"><strong>Stephen Butler</strong> received an MA and Ph.D. at the University of Glasgow. He is the director of the Strathclyde Oculomotor Lab. His research interests, employing eye tracking technology lie in the cognitive neuroscience of attention and the neuropsychology of eye movements. &nbsp;He has applied his research skills to theoretical work in areas including face processing, addiction, biases in attention and brain injury, and has a broad range of experience in applying his research skills in applied fields from shipping to interface design.</div></div></div><div class="article-biography article-biography-has-image" id="auth5Bio5"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx5.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara029"><strong>Winifred Ijomah</strong> is&nbsp;a Reader in University of Strathclyde. She gained a Ph.D. in Remanufacturing from the University of Plymouth in 2002. Her research focuses on sustainable design and manufacturing, and to date has focused on product end-of-life, particularly on remanufacturing.</div></div></div><div class="article-biography article-biography-has-image" id="auth6Bio6"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx6.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara030"><strong>Xia Li</strong> is a psychiatry specialist for older adults in Shanghai Mental Health Center, Shanghai Jiaotong University, school of medicine. She has been engaged in psychogeriatric clinical practice and research for 16 years. Her interest mainly includes Dementia, behavioral problems and Depression in the elderly.</div></div></div><div class="article-biography article-biography-has-image" id="auth7Bio7"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0925231220300783-fx7.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="spara031"><strong>Huiyu Zhou</strong> received a Ph.D. in Computer Vision from Heriot-Watt University. He is a Reader in University of Leicester. He currently heads the Applied Algorithm and AI (AAAI) Theme and is leading the Biomedical Image Processing Lab at University of Leicester. His research interests includes machine learning, computer vision and artificial intelligence.</div></div></div><a class="anchor abstract-link anchor-primary" href="/science/article/abs/pii/S0925231220300783"><span class="anchor-text-container"><span class="anchor-text">View Abstract</span></span></a><div class="Copyright"><span class="copyright-line">Â© 2020 Published by Elsevier B.V.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1568494619305058" title="Recognizing learning emotion based on convolutional neural networks and transfer learning"><span class="anchor-text-container"><span class="anchor-text"><span>Recognizing learning emotion based on convolutional neural networks and transfer learning</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Applied Soft Computing, Volume 84, 2019, Article 105724</div></div><div class="authors"><span>Jason C.</span> <span>Hung</span>, â€¦, <span>Nian-Xiang</span> <span>Lai</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494619305058/pdfft?md5=a5e1d0a84d019738eedad5f13d1415d5&amp;pid=1-s2.0-S1568494619305058-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article0-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865521000489" title="Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 145, 2021, pp. 58-66</div></div><div class="authors"><span>Darshan</span> <span>Gera</span>, <span>S</span> <span>Balasubramanian</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865521000489/pdfft?md5=4744a0def560a5dd8ab3d07f57599987&amp;pid=1-s2.0-S0167865521000489-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article1-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865517303902" title="Deep spatial-temporal feature fusion for facial expression recognition in static images"><span class="anchor-text-container"><span class="anchor-text"><span>Deep spatial-temporal feature fusion for facial expression recognition in static images</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 119, 2019, pp. 49-61</div></div><div class="authors"><span>Ning</span> <span>Sun</span>, â€¦, <span>Guang</span> <span>Han</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865517303902/pdfft?md5=7d06ffd4ab371ac33b9e8cd3a5e7d5b3&amp;pid=1-s2.0-S0167865517303902-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0923596520300540" title="Human emotion recognition by optimally fusing facial expression and speech feature"><span class="anchor-text-container"><span class="anchor-text"><span>Human emotion recognition by optimally fusing facial expression and speech feature</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Signal Processing: Image Communication, Volume 84, 2020, Article 115831</div></div><div class="authors"><span>Xusheng</span> <span>Wang</span>, â€¦, <span>Congjun</span> <span>Cao</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0923596520300540/pdfft?md5=b82a17bfa07384bca018e7f11b6b1516&amp;pid=1-s2.0-S0923596520300540-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article3-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231215001605" title="AU-inspired Deep Networks for Facial Expression Feature Learning"><span class="anchor-text-container"><span class="anchor-text"><span>AU-inspired Deep Networks for Facial Expression Feature Learning</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 159, 2015, pp. 126-136</div></div><div class="authors"><span>Mengyi</span> <span>Liu</span>, â€¦, <span>Xilin</span> <span>Chen</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231215001605/pdfft?md5=88aa205d563f728728330b60e4c22aec&amp;pid=1-s2.0-S0925231215001605-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article4-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1876201822002611" title="Automatic recognition of schizophrenia from facial videos using 3D convolutional neural network"><span class="anchor-text-container"><span class="anchor-text"><span>Automatic recognition of schizophrenia from facial videos using 3D convolutional neural network</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Asian Journal of Psychiatry, Volume 77, 2022, Article 103263</div></div><div class="authors"><span>Jie</span> <span>Huang</span>, â€¦, <span>Shuping</span> <span>Tan</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1876201822002611/pdfft?md5=33114bb91fb2b76dabdcddb8dda9755d&amp;pid=1-s2.0-S1876201822002611-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article5-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plum-sciencedirect-theme"><div class="PlumX-Summary"><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-inverse-logo.png" class="plx-logo"></div><div class="pps-cols"><div class="pps-col plx-citation"><div class="plx-citation"><div class="pps-title">Citations</div><ul><li class="plx-citation"><span class="pps-label">Citation Indexes: </span><span class="pps-count">111</span></li></ul></div></div><div class="pps-col plx-capture"><div class="plx-capture"><div class="pps-title">Captures</div><ul><li class="plx-capture"><span class="pps-label">Readers: </span><span class="pps-count">136</span></li></ul></div></div><div class="pps-col plx-socialMedia"><div class="plx-socialMedia"><div class="pps-title">Social Media</div><ul><li class="plx-socialMedia"><span class="pps-label">Shares, Likes &amp; Comments: </span><span class="pps-count">1</span></li></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plx-logo"></div><a target="_blank" href="https://plu.mx/plum/a/?doi=10.1016/j.neucom.2020.01.034&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class="pps-seemore" title="PlumX Metrics Detail Page">View details<svg fill="currentColor" tabindex="-1" focusable="false" width="16" height="16" viewBox="0 0 16 16" class="svg-arrow"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0925231220300783" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLt_qTVss9so_a_5APBuClac*" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright Â© <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
      <div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
      
      <script async="" src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="text/javascript"></script>
      
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S0925231220300783","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0925-2312","volumeNumber":"388","suppl":"C","provider":"elsevier","entitlementType":"package"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1730203870444,"loadTime":""},"visitor":{"accessType":"ae:REG_SHIBBOLETH","accountId":"ae:50401","accountName":"ae:IT University of Copenhagen","loginStatus":"logged in","userId":"ae:71970787","ipAddress":"130.225.244.206","appSessionId":"d803021f-cf0c-4303-a9ac-edd1111de6ef"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
      <script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
      <script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
      <script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/562c52f0a29e84041e12a6c3286c1d8a5b89ba0b/arp.js" type="text/javascript"></script>
      <script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:REG_SHIBBOLETH","countryCode":"DK"},"account":{"id":"ae:50401","name":"ae:IT University of Copenhagen"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
      <span id="pendo-answer-rating"></span>
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
      <script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
      <script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
      <script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'false' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>
    
  <script>
var pageDataTracker = {
    eventCookieName: 'eventTrack',
    debugCookie: 'els-aa-debugmode',
    debugCounter: 1,
    warnings: [],
    measures: {},
    timeoffset: 0

    ,trackPageLoad: function(data) {
        if (window.pageData && ((pageData.page && pageData.page.noTracking == 'true') || window.pageData_isLoaded)) {
            return false;
        }

        this.updatePageData(data);

        this.initWarnings();
        if(!(window.pageData && pageData.page && pageData.page.name)) {
            console.error('pageDataTracker.trackPageLoad() called without pageData.page.name being defined!');
            return;
        }

        this.processIdPlusData(window.pageData);

        if(window.pageData && pageData.page && !pageData.page.loadTime) {
          pageData.page.loadTime = performance ? Math.round((performance.now())).toString() : '';
        }

        if(window.pageData && pageData.page) {
            var localTime = new Date().getTime();
            if(pageData.page.loadTimestamp) {
                // calculate timeoffset
                var serverTime = parseInt(pageData.page.loadTimestamp);
                if(!isNaN(serverTime)) {
                    this.timeoffset = pageData.page.loadTimestamp - localTime;
                }
            } else {
                pageData.page.loadTimestamp = localTime;
            }
        }

        this.validateData(window.pageData);

        try {
            var cookieTest = 'aa-cookie-test';
            this.setCookie(cookieTest, cookieTest);
            if(this.getCookie(cookieTest) != cookieTest) {
                this.warnings.push('dtm5');
            }
            this.deleteCookie(cookieTest);
        } catch(e){
            this.warnings.push('dtm5');
        }

        this.registerCallbacks();
        this.setAnalyticsData();

        // handle any cookied event data
        this.getEvents();

        window.pageData_isLoaded = true;

        this.debugMessage('Init - trackPageLoad()', window.pageData);

        _satellite.track('eventDispatcher', JSON.stringify({
          eventName: 'newPage',
          eventData: {eventName: 'newPage'},
          pageData: window.pageData
        }));
    }

    ,trackEvent: function(event, data, callback) {
        if (window.pageData && pageData.page && pageData.page.noTracking == 'true') {
            return false;
        }
        
        if(!window.pageData_isLoaded) {
            if(this.isDebugEnabled()) {
                console.log('[AA] pageDataTracker.trackEvent() called without calling trackPageLoad() first.');
            }
            return false;
        }

        if (event) {
            this.initWarnings();
            if(event === 'newPage') {
                // auto fillings
                if(data && data.page && !data.page.loadTimestamp) {
                    data.page.loadTimestamp = ''+(new Date().getTime() + this.timeoffset);
                }
                this.processIdPlusData(data);
            }

            window.eventData = data ? data : {};
            window.eventData.eventName = event;
            if(!_satellite.getVar('blacklisted')) {
                this.handleEventData(event, data);
    
                if(event === 'newPage') {
                    this.validateData(window.pageData);
                }
                this.debugMessage('Event: ' + event, data);
    
                _satellite.track('eventDispatcher', JSON.stringify({
                  eventName: event,
                  eventData: window.eventData,
                  pageData: window.pageData
                }));
            } else {
                this.debugMessage('!! Blocked Event: ' + event, data);
            }
        }

        if (typeof(callback) == 'function') {
            callback.call();
        }
    }

    ,processIdPlusData: function(data) {
        if(data && data.visitor && data.visitor.idPlusData) {
            var idPlusFields = ['userId', 'accessType', 'accountId', 'accountName'];
            for(var i=0; i < idPlusFields.length; i++) {
                if(typeof data.visitor.idPlusData[idPlusFields[i]] !== 'undefined') {
                    data.visitor[idPlusFields[i]] = data.visitor.idPlusData[idPlusFields[i]];
                }
            }
            data.visitor.idPlusData = undefined;
        }
    }

    ,validateData: function(data) {
        if(!data) {
            this.warnings.push('dv0');
            return;
        }

        // top 5
        if(!(data.visitor && data.visitor.accessType)) {
            this.warnings.push('dv1');
        }
        if(data.visitor && (data.visitor.accountId || data.visitor.accountName)) {
            if(!data.visitor.accountName) {
                this.warnings.push('dv2');
            }
            if(!data.visitor.accountId) {
                this.warnings.push('dv3');
            }
        }
        if(!(data.page && data.page.productName)) {
            this.warnings.push('dv4');
        }
        if(!(data.page && data.page.businessUnit)) {
            this.warnings.push('dv5');
        }
        if(!(data.page && data.page.name)) {
            this.warnings.push('dv6');
        }

        // rp mandatory
        if(data.page && data.page.businessUnit && (data.page.businessUnit.toLowerCase().indexOf('els:rp:') !== -1 || data.page.businessUnit.toLowerCase().indexOf('els:rap:') !== -1)) {
            if(!(data.page && data.page.loadTimestamp)) {
                this.warnings.push('dv7');
            }
            if(!(data.page && data.page.loadTime)) {
                this.warnings.push('dv8');
            }
            if(!(data.visitor && data.visitor.ipAddress)) {
                this.warnings.push('dv9');
            }
            if(!(data.page && data.page.type)) {
                this.warnings.push('dv10');
            }
            if(!(data.page && data.page.language)) {
                this.warnings.push('dv11');
            }
        }

        // other
        if(data.page && data.page.environment) {
            var env = data.page.environment.toLowerCase();
            if(!(env === 'dev' || env === 'cert' || env === 'prod')) {
                this.warnings.push('dv12');
            }
        }
        if(data.content && data.content.constructor !== Array) {
            this.warnings.push('dv13');
        }

        if(data.visitor && data.visitor.accountId && data.visitor.accountId.indexOf(':') == -1) {
            this.warnings.push('dv14');
            data.visitor.accountId = "data violation"
        }
    }

    ,initWarnings: function() {
        this.warnings = [];
        try {
            var hdn = document.head.childNodes;
            var libf = false;
            for(var i=0; i<hdn.length; i++) {
                if(hdn[i].src && (hdn[i].src.indexOf('satelliteLib') !== -1 || hdn[i].src.indexOf('launch') !== -1)) {
                    libf = true;
                    break;
                }
            }
            if(!libf) {
                this.warnings.push('dtm1');
            }
        } catch(e) {}

        try {
            for (let element of document.querySelectorAll('*')) {
                // Check if the element has a src attribute and if it meets the criteria
                if (element.src && element.src.includes('assets.adobedtm.com') && element.src.includes('launch')) {
                    if (element.tagName.toLowerCase() !== 'script') {
                        this.warnings.push('dtm5');
                    }
                    if (!element.hasAttribute('async')) {
                        this.warnings.push('dtm4');
                    }
                }
            }
        } catch (e) { }
    }

    ,getMessages: function() {
        return ['v1'].concat(this.warnings).join('|');
    }
    ,addMessage: function(message) {
        this.warnings.push(message);
    }

    ,getPerformance: function() {
        var copy = {};
        for (var attr in this.measures) {
            if(this.measures.hasOwnProperty(attr)) {
                copy[attr] = this.measures[attr];
            }
        }

        this.measures = {};
        return copy;
    }

    ,dtmCodeDesc: {
        dtm1: 'satellite-lib must be placed in the <head> section',
        dtm2: 'trackPageLoad() must be placed and called before the closing </body> tag',
        dtm3: 'trackEvent() must be called at a stage where Document.readyState=complete (e.g. on the load event or a user event)',
        dtm4: 'Embed codes need to be loaded in async mode',
        dtm5: 'Embed codes not in type script',
        dv1: 'visitor.accessType not set but mandatory',
        dv2: 'visitor.accountName not set but mandatory',
        dv3: 'visitor.accountId not set but mandatory',
        dv4: 'page.productName not set but mandatory',
        dv5: 'page.businessUnit not set but mandatory',
        dv6: 'page.name not set but mandatory',
        dv7: 'page.loadTimestamp not set but mandatory',
        dv8: 'page.loadTime not set but mandatory',
        dv9: 'visitor.ipAddress not set but mandatory',
        dv10: 'page.type not set but mandatory',
        dv11: 'page.language not set but mandatory',
        dv12: 'page.environment must be set to \'prod\', \'cert\' or \'dev\'',
        dv13: 'content must be of type array of objects',
        dv14: 'account number must contain at least one \':\', e.g. \'ae:12345\''
    }

    ,debugMessage: function(event, data) {
        if(this.isDebugEnabled()) {
            console.log('[AA] --------- [' + (this.debugCounter++) + '] Web Analytics Data ---------');
            console.log('[AA] ' + event);
            console.groupCollapsed("[AA] AA Data: ");
            if(window.eventData) {
                console.log("[AA] eventData:\n" + JSON.stringify(window.eventData, true, 2));
            }
            if(window.pageData) {
                console.log("[AA] pageData:\n" + JSON.stringify(window.pageData, true, 2));
            }
            console.groupEnd();
            if(this.warnings.length > 0) {
                console.groupCollapsed("[AA] Warnings ("+this.warnings.length+"): ");
                for(var i=0; i<this.warnings.length; i++) {
                    var error = this.dtmCodeDesc[this.warnings[i]] ? this.dtmCodeDesc[this.warnings[i]] : 'Error Code: ' + this.warnings[i];
                    console.log('[AA] ' + error);
                }
                console.log('[AA] More can be found here: https://confluence.cbsels.com/display/AA/AA+Error+Catalog');
                console.groupEnd();
            }
            console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        }
    }

    ,getTrackingCode: function() {
      var campaign = _satellite.getVar('Campaign - ID');
      if(!campaign) {
        campaign = window.sessionStorage ? sessionStorage.getItem('dgcid') : '';
      }
      return campaign;
    }

    ,isDebugEnabled: function() {
        if(typeof this.debug === 'undefined') {
            this.debug = (document.cookie.indexOf(this.debugCookie) !== -1) || (window.pageData && pageData.page && pageData.page.environment && pageData.page.environment.toLowerCase() === 'dev');
            //this.debug = (document.cookie.indexOf(this.debugCookie) !== -1);
        }
        return this.debug;
    }

    ,enableDebug: function(expire) {
        if (typeof expire === 'undefined') {
            expire = 86400;
        }
        console.log('You just enabled debug mode for Adobe Analytics tracking. This mode will persist for 24h.');
        console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        this.setCookie(this.debugCookie, 'true', expire, document.location.hostname);
        this.debug = true;
    }

    ,disableDebug: function() {
        console.log('Debug mode is now disabled.');
        this.deleteCookie(this.debugCookie);
        this.debug = false;
    }

    ,setAnalyticsData: function() {
        if(!(window.pageData && pageData.page && pageData.page.productName && pageData.page.name)) {
            return;
        }
        pageData.page.analyticsPagename = pageData.page.productName + ':' + pageData.page.name;

        var pageEls = pageData.page.name.indexOf(':') > -1 ? pageData.page.name.split(':') : [pageData.page.name];
        pageData.page.sectionName = pageData.page.productName + ':' + pageEls[0];
    }

    ,getEvents: function() {
        pageData.savedEvents = {};
        pageData.eventList = [];

        var val = this.getCookie(this.eventCookieName);
        if (val) {
            pageData.savedEvents = val;
        }

        this.deleteCookie(this.eventCookieName);
    }

    ,updatePageData(data) {
        window.pageData = window.pageData || {};
        if (data && typeof(data) === 'object') {
            for (var x in data) {
                if(data.hasOwnProperty(x) && data[x] instanceof Array) {
                    pageData[x] = data[x];
                } else if(data.hasOwnProperty(x) && typeof(data[x]) === 'object') {
                    if(!pageData[x]) {
                        pageData[x] = {};
                    }
                    for (var y in data[x]) {
                        if(data[x].hasOwnProperty(y)) {
                            pageData[x][y] = data[x][y];
                        }
                    }
                }
            }
        }
    }

    ,handleEventData: function(event, data) {
        var val;
        switch(event) {
            case 'newPage':
                this.updatePageData(data);
                this.setAnalyticsData();
            case 'saveSearch':
            case 'searchResultsUpdated':
                if (data) {
                    // overwrite page-load object
                    if (data.search && typeof(data.search) == 'object') {
                        window.eventData.search.resultsPosition = '';
                        pageData.search = pageData.search || {};
                        var fields = ['advancedCriteria', 'criteria', 'currentPage', 'dataFormCriteria', 'facets', 'resultsByType', 'resultsPerPage', 'sortType', 'totalResults', 'type', 'database',
                        'suggestedClickPosition','suggestedLetterCount','suggestedResultCount', 'autoSuggestCategory', 'autoSuggestDetails','typedTerm','selectedTerm', 'channel',
                        'facetOperation', 'details'];
                        for (var i=0; i<fields.length; i++) {
                            if (data.search[fields[i]]) {
                                pageData.search[fields[i]] = data.search[fields[i]];
                            }
                        }
                    }
                }
                this.setAnalyticsData();
                break;
            case 'navigationClick':
                if (data && data.link) {
                    window.eventData.navigationLink = {
                        name: ((data.link.location || 'no location') + ':' + (data.link.name || 'no name'))
                    };
                }
                break;
            case 'autoSuggestClick':
                if (data && data.search) {
                    val = {
                        autoSuggestSearchData: (
                            'letterct:' + (data.search.suggestedLetterCount || 'none') +
                            '|resultct:' + (data.search.suggestedResultCount || 'none') +
                            '|clickpos:' + (data.search.suggestedClickPosition || 'none')
                        ).toLowerCase(),
                        autoSuggestSearchTerm: (data.search.typedTerm || ''),
                        autoSuggestTypedTerm: (data.search.typedTerm || ''),
                        autoSuggestSelectedTerm: (data.search.selectedTerm || ''),
                        autoSuggestCategory: (data.search.autoSuggestCategory || ''),
                        autoSuggestDetails: (data.search.autoSuggestDetails || '')
                    };
                }
                break;
            case 'linkOut':
                if (data && data.content && data.content.length > 0) {
                    window.eventData.linkOut = data.content[0].linkOut;
                    window.eventData.referringProduct = _satellite.getVar('Page - Product Name') + ':' + data.content[0].id;
                }
                break;
            case 'socialShare':
                if (data && data.social) {
                    window.eventData.sharePlatform = data.social.sharePlatform || '';
                }
                break;
            case 'contentInteraction':
                if (data && data.action) {
                    window.eventData.action.name = pageData.page.productName + ':' + data.action.name;
                }
                break;
            case 'searchWithinContent':
                if (data && data.search) {
                    window.pageData.search = window.pageData.search || {};
                    pageData.search.withinContentCriteria = data.search.withinContentCriteria;
                }
                break;
            case 'contentShare':
                if (data && data.content) {
                    window.eventData.sharePlatform = data.content[0].sharePlatform;
                }
                break;
            case 'contentLinkClick':
                if (data && data.link) {
                    window.eventData.action = { name: pageData.page.productName + ':' + (data.link.type || 'no link type') + ':' + (data.link.name || 'no link name') };
                }
                break;
            case 'contentWindowLoad':
            case 'contentTabClick':
                if (data && data.content) {
                    window.eventData.tabName = data.content[0].tabName || '';
                    window.eventData.windowName = data.content[0].windowName || '';
                }
                break;
            case 'userProfileUpdate':
                if (data && data.user) {
                    if (Object.prototype.toString.call(data.user) === "[object Array]") {
                        window.eventData.user = data.user[0];
                    }
                }
                break;
            case 'videoStart':
                if (data.video) {
                    data.video.length = parseFloat(data.video.length || '0');
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.open(data.video.id, data.video.length, s.Media.playerName);
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoPlay':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoStop':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                }
                break;
            case 'videoComplete':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                    s.Media.close(data.video.id);
                }
                break;
            case 'addWebsiteExtension':
                if(data && data.page) {
                    val = {
                        wx: data.page.websiteExtension
                    }
                }
                break;
        }

        if (val) {
            this.setCookie(this.eventCookieName, val);
        }
    }

    ,registerCallbacks: function() {
        var self = this;
        if(window.usabilla_live) {
            window.usabilla_live('setEventCallback', function(category, action, label, value) {
                if(action == 'Campaign:Open') {
                    self.trackEvent('ctaImpression', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                } else if(action == 'Campaign:Success') {
                    self.trackEvent('ctaClick', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                }
            });
        }
    }

    ,getConsortiumAccountId: function() {
        var id = '';
        if (window.pageData && pageData.visitor && (pageData.visitor.consortiumId || pageData.visitor.accountId)) {
            id = (pageData.visitor.consortiumId || 'no consortium ID') + '|' + (pageData.visitor.accountId || 'no account ID');
        }

        return id;
    }

    ,getSearchClickPosition: function() {
        if (window.eventData && eventData.search && eventData.search.resultsPosition) {
            var pos = parseInt(eventData.search.resultsPosition), clickPos;
            if (!isNaN(pos)) {
                var page = pageData.search.currentPage ? parseInt(pageData.search.currentPage) : '', perPage = pageData.search.resultsPerPage ? parseInt(pageData.search.resultsPerPage) : '';
                if (!isNaN(page) && !isNaN(perPage)) {
                    clickPos = pos + ((page - 1) * perPage);
                }
            }
            return clickPos ? clickPos.toString() : eventData.search.resultsPosition;
        }
        return '';
    }

    ,getSearchFacets: function() {
        var facetList = '';
        if (window.pageData && pageData.search && pageData.search.facets) {
            if (typeof(pageData.search.facets) == 'object') {
                for (var i=0; i<pageData.search.facets.length; i++) {
                    var f = pageData.search.facets[i];
                    facetList += (facetList ? '|' : '') + f.name + '=' + f.values.join('^');
                }
            }
        }
        return facetList;
    }

    ,getSearchResultsByType: function() {
        var resultTypes = '';
        if (window.pageData && pageData.search && pageData.search.resultsByType) {
            for (var i=0; i<pageData.search.resultsByType.length; i++) {
                var r = pageData.search.resultsByType[i];
                resultTypes += (resultTypes ? '|' : '') + r.name + (r.results || r.values ? '=' + (r.results || r.values) : '');
            }
        }
        return resultTypes;
    }

    ,getJournalInfo: function() {
        var info = '';
        if (window.pageData && pageData.journal && (pageData.journal.name || pageData.journal.specialty || pageData.journal.section || pageData.journal.issn || pageData.journal.issueNumber || pageData.journal.volumeNumber || pageData.journal.family || pageData.journal.publisher)) {
            var journal = pageData.journal;
            info = (journal.name || 'no name') 
            + '|' + (journal.specialty || 'no specialty') 
            + '|' + (journal.section || 'no section') 
            + '|' + (journal.issn || 'no issn') 
            + '|' + (journal.issueNumber || 'no issue #') 
            + '|' + (journal.volumeNumber || 'no volume #')
            + '|' + (journal.family || 'no family')
            + '|' + (journal.publisher || 'no publisher');

        }
        return info;
    }

    ,getBibliographicInfo: function(doc) {
        if (!doc || !(doc.publisher || doc.indexTerms || doc.publicationType || doc.publicationRights || doc.volumeNumber || doc.issueNumber || doc.subjectAreas || doc.isbn)) {
            return '';
        }

        var terms = doc.indexTerms ? doc.indexTerms.split('+') : '';
        if (terms) {
            terms = terms.slice(0, 5).join('+');
            terms = terms.length > 100 ? terms.substring(0, 100) : terms;
        }

        var areas = doc.subjectAreas ? doc.subjectAreas.split('>') : '';
        if (areas) {
            areas = areas.slice(0, 5).join('>');
            areas = areas.length > 100 ? areas.substring(0, 100) : areas;
        }

        var biblio	= (doc.publisher || 'none')
            + '^' + (doc.publicationType || 'none')
            + '^' + (doc.publicationRights || 'none')
            + '^' + (terms || 'none')
            + '^' + (doc.volumeNumber || 'none')
            + '^' + (doc.issueNumber || 'none')
            + '^' + (areas || 'none')
            + '^' + (doc.isbn || 'none');

        return this.stripProductDelimiters(biblio).toLowerCase();
    }

    ,getContentItem: function() {
        var docs = window.eventData && eventData.content ? eventData.content : pageData.content;
        if (docs && docs.length > 0) {
            return docs[0];
        }
    }

    ,getFormattedDate: function(ts) {
        if (!ts) {
            return '';
        }

        var d = new Date(parseInt(ts) * 1000);

        // now do formatting
        var year = d.getFullYear()
            ,month = ((d.getMonth() + 1) < 10 ? '0' : '') + (d.getMonth() + 1)
            ,date = (d.getDate() < 10 ? '0' : '') + d.getDate()
            ,hours = d.getHours() > 12 ? d.getHours() - 12 : d.getHours()
            ,mins = (d.getMinutes() < 10 ? '0' : '') + d.getMinutes()
            ,ampm = d.getHours() > 12 ? 'pm' : 'am';

        hours = (hours < 10 ? '0' : '') + hours;
        return year + '-' + month + '-' + date;
    }

    ,getVisitorId: function() {
        var orgId = '4D6368F454EC41940A4C98A6@AdobeOrg';
        if(Visitor && Visitor.getInstance(orgId)) {
            return Visitor.getInstance(orgId).getMarketingCloudVisitorID();
        } else {
            return ''
        }
    }

    ,setProductsVariable: function() {
        var prodList = window.eventData && eventData.content ? eventData.content : pageData.content
            ,prods = [];
        if (prodList) {
            for (var i=0; i<prodList.length; i++) {
                if (prodList[i].id || prodList[i].type || prodList[i].publishDate || prodList[i].onlineDate) {
                    if (!prodList[i].id) {
                        prodList[i].id = 'no id';
                    }
                    var prodName = (pageData.page.productName || 'xx').toLowerCase();
                    if (prodList[i].id.indexOf(prodName + ':') != 0) {
                        prodList[i].id = prodName + ':' + prodList[i].id;
                    }
                    prodList[i].id = this.stripProductDelimiters(prodList[i].id);
                    var merch = [];
                    if (prodList[i].format) {
                        merch.push('evar17=' + this.stripProductDelimiters(prodList[i].format.toLowerCase()));
                    }
                    if (prodList[i].type) {
                        var type = prodList[i].type;
                        if (prodList[i].accessType) {
                            type += ':' + prodList[i].accessType;
                        }
                        merch.push('evar20=' + this.stripProductDelimiters(type.toLowerCase()));

                        if(type.indexOf(':manuscript') > 0) {
                            /*
                            var regex = /[a-z]+:manuscript:id:([a-z]+-[a-z]-[0-9]+-[0-9]+)/gmi;
                            var m = regex.exec(prodList[i].id);
                            if(m) {
                                merch.push('evar200=' + m[1]);
                            }
                            merch.push('evar200=' + prodList[i].id);
                            */
                            a = prodList[i].id.lastIndexOf(':');
                            if(a>0) {
                                merch.push('evar200=' + prodList[i].id.substring(a+1).toUpperCase());
                            }
                        } else if(type.indexOf(':submission') > 0) {
                            merch.push('evar200=' + prodList[i].id);
                        }
                    }
                    if(!prodList[i].title) {
                        prodList[i].title = prodList[i].name;
                    }
                    if (prodList[i].title) {
                        merch.push('evar75=' + this.stripProductDelimiters(prodList[i].title.toLowerCase()));
                    }
                    if (prodList[i].breadcrumb) {
                        merch.push('evar63=' + this.stripProductDelimiters(prodList[i].breadcrumb).toLowerCase());
                    }
                    var nowTs = new Date().getTime()/1000;
                    if (prodList[i].onlineDate && !isNaN(prodList[i].onlineDate)) {
                        if(prodList[i].onlineDate > 32503680000) {
                            prodList[i].onlineDate = prodList[i].onlineDate/1000;
                        }
                        merch.push('evar122=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate)));
                        var onlineAge = Math.floor((nowTs - prodList[i].onlineDate) / 86400);
                        onlineAge = (onlineAge === 0) ? 'zero' : onlineAge;
                        merch.push('evar128=' + onlineAge);
                    }
                    if (prodList[i].publishDate && !isNaN(prodList[i].publishDate)) {
                        if(prodList[i].publishDate > 32503680000) {
                            prodList[i].publishDate = prodList[i].publishDate/1000;
                        }
                        merch.push('evar123=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                        var publishAge = Math.floor((nowTs - prodList[i].publishDate) / 86400);
                        publishAge = (publishAge === 0) ? 'zero' : publishAge;
                        merch.push('evar127=' + publishAge);
                    }
                    if (prodList[i].onlineDate && prodList[i].publishDate) {
                        merch.push('evar38=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate) + '^' + pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                    }
                    if (prodList[i].mapId) {
                        merch.push('evar70=' + this.stripProductDelimiters(prodList[i].mapId));
                    }
					if (prodList[i].relevancyScore) {
						merch.push('evar71=' + this.stripProductDelimiters(prodList[i].relevancyScore));
					}
                    if (prodList[i].status) {
                        merch.push('evar73=' + this.stripProductDelimiters(prodList[i].status));
                    }
                    if (prodList[i].previousStatus) {
                        merch.push('evar111=' + this.stripProductDelimiters(prodList[i].previousStatus));
                    }
                    if (prodList[i].entitlementType) {
                        merch.push('evar80=' + this.stripProductDelimiters(prodList[i].entitlementType));
                    }
                    if (prodList[i].recordType) {
                        merch.push('evar93=' + this.stripProductDelimiters(prodList[i].recordType));
                    }
                    if (prodList[i].exportType) {
                        merch.push('evar99=' + this.stripProductDelimiters(prodList[i].exportType));
                    }
                    if (prodList[i].importType) {
                        merch.push('evar142=' + this.stripProductDelimiters(prodList[i].importType));
                    }
                    if (prodList[i].section) {
                        merch.push('evar100=' + this.stripProductDelimiters(prodList[i].section));
                    }
                    if (prodList[i].detail) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].detail.toLowerCase()));
                    } else if(prodList[i].details) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].details.toLowerCase()));
                    }
                    if (prodList[i].position) {
                        merch.push('evar116=' + this.stripProductDelimiters(prodList[i].position));
                    }
                    if (prodList[i].publicationTitle) {
                        merch.push('evar129=' + this.stripProductDelimiters(prodList[i].publicationTitle));
                    }
                    if (prodList[i].specialIssueTitle) {
                        merch.push('evar130=' + this.stripProductDelimiters(prodList[i].specialIssueTitle));
                    }
                    if (prodList[i].specialIssueNumber) {
                        merch.push('evar131=' + this.stripProductDelimiters(prodList[i].specialIssueNumber));
                    }
                    if (prodList[i].referenceModuleTitle) {
                        merch.push('evar139=' + this.stripProductDelimiters(prodList[i].referenceModuleTitle));
                    }
                    if (prodList[i].referenceModuleISBN) {
                        merch.push('evar140=' + this.stripProductDelimiters(prodList[i].referenceModuleISBN));
                    }
                    if (prodList[i].volumeTitle) {
                        merch.push('evar132=' + this.stripProductDelimiters(prodList[i].volumeTitle));
                    }
                    if (prodList[i].publicationSection) {
                        merch.push('evar133=' + this.stripProductDelimiters(prodList[i].publicationSection));
                    }
                    if (prodList[i].publicationSpecialty) {
                        merch.push('evar134=' + this.stripProductDelimiters(prodList[i].publicationSpecialty));
                    }
                    if (prodList[i].issn) {
                        merch.push('evar135=' + this.stripProductDelimiters(prodList[i].issn));
                    }
                    if (prodList[i].id2) {
                        merch.push('evar159=' + this.stripProductDelimiters(prodList[i].id2));
                    }
                    if (prodList[i].id3) {
                        merch.push('evar160=' + this.stripProductDelimiters(prodList[i].id3));
                    }
                    if (prodList[i].provider) {
                        merch.push('evar164=' + this.stripProductDelimiters(prodList[i].provider));
                    }
                    if (prodList[i].citationStyle) {
                        merch.push('evar170=' + this.stripProductDelimiters(prodList[i].citationStyle));
                    }

                    var biblio = this.getBibliographicInfo(prodList[i]);
                    if (biblio) {
                        merch.push('evar28=' + biblio);
                    }

                    if (prodList[i].turnawayId) {
                        pageData.eventList.push('product turnaway');
                    }

                    var price = prodList[i].price || '', qty = prodList[i].quantity || '', evts = [];
                    if (price && qty) {
                        qty = parseInt(qty || '1');
                        price = parseFloat(price || '0');
                        price = (price * qty).toFixed(2);

                        if (window.eventData && eventData.eventName && eventData.eventName == 'cartAdd') {
                            evts.push('event20=' + price);
                        }
                    }

                    var type = window.pageData && pageData.page && pageData.page.type ? pageData.page.type : '', evt = window.eventData && eventData.eventName ? eventData.eventName : '';
                    if (type.match(/^CP\-/gi) !== null && (!evt || evt == 'newPage' || evt == 'contentView')) {
                        evts.push('event181=1');
                    }
                    if (evt == 'contentDownload' || type.match(/^CP\-DL/gi) !== null) {
                        evts.push('event182=1');
                    }
                    if (evt == 'contentDownloadRequest') {
                        evts.push('event319=1');
                    }
                    if (evt == 'contentExport') {
                        evts.push('event184=1');
                    }
                    if (this.eventFires('recommendationViews')) {
                        evts.push('event264=1');
                    }

                    if(prodList[i].datapoints) {
                        evts.push('event239=' + prodList[i].datapoints);
                    }
                    if(prodList[i].documents) {
                        evts.push('event240=' + prodList[i].documents);
                    }
                    if(prodList[i].size) {
                        evts.push('event335=' + prodList[i].size);
                        evts.push('event336=1')
                    }

                    prods.push([
                        ''					// empty category
                        ,prodList[i].id		// id
                        ,qty				// qty
                        ,price				// price
                        ,evts.join('|')		// events
                        ,merch.join('|')	// merchandising eVars
                    ].join(';'));
                }
            }
        }

        return prods.join(',');
    }
    ,eventFires: function(eventName) {
      var evt = window.eventData && eventData.eventName ? eventData.eventName : '';
      if(evt == eventName) {
        return true;
      }
      // initial pageload and new pages
      if((!window.eventData || evt == 'newPage') && window.pageData && window.pageData.trackEvents) {
        var tEvents = window.pageData.trackEvents;
        for(var i=0; i<tEvents.length; i++) {
          if(tEvents[i] == eventName) {
            return true;
          }
        }
      }
      return false;
    }

    ,md5: function(s){function L(k,d){return(k<<d)|(k>>>(32-d))}function K(G,k){var I,d,F,H,x;F=(G&2147483648);H=(k&2147483648);I=(G&1073741824);d=(k&1073741824);x=(G&1073741823)+(k&1073741823);if(I&d){return(x^2147483648^F^H)}if(I|d){if(x&1073741824){return(x^3221225472^F^H)}else{return(x^1073741824^F^H)}}else{return(x^F^H)}}function r(d,F,k){return(d&F)|((~d)&k)}function q(d,F,k){return(d&k)|(F&(~k))}function p(d,F,k){return(d^F^k)}function n(d,F,k){return(F^(d|(~k)))}function u(G,F,aa,Z,k,H,I){G=K(G,K(K(r(F,aa,Z),k),I));return K(L(G,H),F)}function f(G,F,aa,Z,k,H,I){G=K(G,K(K(q(F,aa,Z),k),I));return K(L(G,H),F)}function D(G,F,aa,Z,k,H,I){G=K(G,K(K(p(F,aa,Z),k),I));return K(L(G,H),F)}function t(G,F,aa,Z,k,H,I){G=K(G,K(K(n(F,aa,Z),k),I));return K(L(G,H),F)}function e(G){var Z;var F=G.length;var x=F+8;var k=(x-(x%64))/64;var I=(k+1)*16;var aa=Array(I-1);var d=0;var H=0;while(H<F){Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=(aa[Z]| (G.charCodeAt(H)<<d));H++}Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=aa[Z]|(128<<d);aa[I-2]=F<<3;aa[I-1]=F>>>29;return aa}function B(x){var k="",F="",G,d;for(d=0;d<=3;d++){G=(x>>>(d*8))&255;F="0"+G.toString(16);k=k+F.substr(F.length-2,2)}return k}function J(k){k=k.replace(/rn/g,"n");var d="";for(var F=0;F<k.length;F++){var x=k.charCodeAt(F);if(x<128){d+=String.fromCharCode(x)}else{if((x>127)&&(x<2048)){d+=String.fromCharCode((x>>6)|192);d+=String.fromCharCode((x&63)|128)}else{d+=String.fromCharCode((x>>12)|224);d+=String.fromCharCode(((x>>6)&63)|128);d+=String.fromCharCode((x&63)|128)}}}return d}var C=Array();var P,h,E,v,g,Y,X,W,V;var S=7,Q=12,N=17,M=22;var A=5,z=9,y=14,w=20;var o=4,m=11,l=16,j=23;var U=6,T=10,R=15,O=21;s=J(s);C=e(s);Y=1732584193;X=4023233417;W=2562383102;V=271733878;for(P=0;P<C.length;P+=16){h=Y;E=X;v=W;g=V;Y=u(Y,X,W,V,C[P+0],S,3614090360);V=u(V,Y,X,W,C[P+1],Q,3905402710);W=u(W,V,Y,X,C[P+2],N,606105819);X=u(X,W,V,Y,C[P+3],M,3250441966);Y=u(Y,X,W,V,C[P+4],S,4118548399);V=u(V,Y,X,W,C[P+5],Q,1200080426);W=u(W,V,Y,X,C[P+6],N,2821735955);X=u(X,W,V,Y,C[P+7],M,4249261313);Y=u(Y,X,W,V,C[P+8],S,1770035416);V=u(V,Y,X,W,C[P+9],Q,2336552879);W=u(W,V,Y,X,C[P+10],N,4294925233);X=u(X,W,V,Y,C[P+11],M,2304563134);Y=u(Y,X,W,V,C[P+12],S,1804603682);V=u(V,Y,X,W,C[P+13],Q,4254626195);W=u(W,V,Y,X,C[P+14],N,2792965006);X=u(X,W,V,Y,C[P+15],M,1236535329);Y=f(Y,X,W,V,C[P+1],A,4129170786);V=f(V,Y,X,W,C[P+6],z,3225465664);W=f(W,V,Y,X,C[P+11],y,643717713);X=f(X,W,V,Y,C[P+0],w,3921069994);Y=f(Y,X,W,V,C[P+5],A,3593408605);V=f(V,Y,X,W,C[P+10],z,38016083);W=f(W,V,Y,X,C[P+15],y,3634488961);X=f(X,W,V,Y,C[P+4],w,3889429448);Y=f(Y,X,W,V,C[P+9],A,568446438);V=f(V,Y,X,W,C[P+14],z,3275163606);W=f(W,V,Y,X,C[P+3],y,4107603335);X=f(X,W,V,Y,C[P+8],w,1163531501);Y=f(Y,X,W,V,C[P+13],A,2850285829);V=f(V,Y,X,W,C[P+2],z,4243563512);W=f(W,V,Y,X,C[P+7],y,1735328473);X=f(X,W,V,Y,C[P+12],w,2368359562);Y=D(Y,X,W,V,C[P+5],o,4294588738);V=D(V,Y,X,W,C[P+8],m,2272392833);W=D(W,V,Y,X,C[P+11],l,1839030562);X=D(X,W,V,Y,C[P+14],j,4259657740);Y=D(Y,X,W,V,C[P+1],o,2763975236);V=D(V,Y,X,W,C[P+4],m,1272893353);W=D(W,V,Y,X,C[P+7],l,4139469664);X=D(X,W,V,Y,C[P+10],j,3200236656);Y=D(Y,X,W,V,C[P+13],o,681279174);V=D(V,Y,X,W,C[P+0],m,3936430074);W=D(W,V,Y,X,C[P+3],l,3572445317);X=D(X,W,V,Y,C[P+6],j,76029189);Y=D(Y,X,W,V,C[P+9],o,3654602809);V=D(V,Y,X,W,C[P+12],m,3873151461);W=D(W,V,Y,X,C[P+15],l,530742520);X=D(X,W,V,Y,C[P+2],j,3299628645);Y=t(Y,X,W,V,C[P+0],U,4096336452);V=t(V,Y,X,W,C[P+7],T,1126891415);W=t(W,V,Y,X,C[P+14],R,2878612391);X=t(X,W,V,Y,C[P+5],O,4237533241);Y=t(Y,X,W,V,C[P+12],U,1700485571);V=t(V,Y,X,W,C[P+3],T,2399980690);W=t(W,V,Y,X,C[P+10],R,4293915773);X=t(X,W,V,Y,C[P+1],O,2240044497);Y=t(Y,X,W,V,C[P+8],U,1873313359);V=t(V,Y,X,W,C[P+15],T,4264355552);W=t(W,V,Y,X,C[P+6],R,2734768916);X=t(X,W,V,Y,C[P+13],O,1309151649);Y=t(Y,X,W,V,C[P+4],U,4149444226);V=t(V,Y,X,W,C[P+11],T,3174756917);W=t(W,V,Y,X,C[P+2],R,718787259);X=t(X,W,V,Y,C[P+9],O,3951481745);Y=K(Y,h);X=K(X,E);W=K(W,v);V=K(V,g)}var i=B(Y)+B(X)+B(W)+B(V);return i.toLowerCase()}
    ,stripProductDelimiters: function(val) {
        if (val) {
            return val.replace(/\;|\||\,/gi, '-');
        }
    }

    ,setCookie: function(name, value, seconds, domain) {
        domain = document.location.hostname;
        var expires = '';
        var expiresNow = '';
        var date = new Date();
        date.setTime(date.getTime() + (-1 * 1000));
        expiresNow = "; expires=" + date.toGMTString();

        if (typeof(seconds) != 'undefined') {
            date.setTime(date.getTime() + (seconds * 1000));
            expires = '; expires=' + date.toGMTString();
        }

        var type = typeof(value);
        type = type.toLowerCase();
        if (type != 'undefined' && type != 'string') {
            value = JSON.stringify(value);
        }

        // fix scoping issues
        // keep writing the old cookie, but make it expire
        document.cookie = name + '=' + value + expiresNow + '; path=/';

        // now just set the right one
        document.cookie = name + '=' + value + expires + '; path=/; domain=' + domain;
    }

    ,getCookie: function(name) {
        name = name + '=';
        var carray = document.cookie.split(';'), value;

        for (var i=0; i<carray.length; i++) {
            var c = carray[i];
            while (c.charAt(0) == ' ') {
                c = c.substring(1, c.length);
            }
            if (c.indexOf(name) == 0) {
                value = c.substring(name.length, c.length);
                try {
                    value = JSON.parse(value);
                } catch(ex) {}

                return value;
            }
        }

        return null;
    }

    ,deleteCookie: function(name) {
        this.setCookie(name, '', -1);
        this.setCookie(name, '', -1, document.location.hostname);
    }

    ,mapAdobeVars: function(s) {
        var vars = {
            pageName		: 'Page - Analytics Pagename'
            ,channel		: 'Page - Section Name'
            ,campaign		: 'Campaign - ID'
            ,currencyCode	: 'Page - Currency Code'
            ,purchaseID		: 'Order - ID'
            ,prop1			: 'Visitor - Account ID'
            ,prop2			: 'Page - Product Name'
            ,prop4			: 'Page - Type'
            ,prop6			: 'Search - Type'
            ,prop7			: 'Search - Facet List'
            ,prop8			: 'Search - Feature Used'
            ,prop12			: 'Visitor - User ID'
            ,prop13			: 'Search - Sort Type'
            ,prop14			: 'Page - Load Time'
            ,prop15         : 'Support - Topic Name'
            ,prop16			: 'Page - Business Unit'
            ,prop21			: 'Search - Criteria'
            ,prop24			: 'Page - Language'
            ,prop25			: 'Page - Product Feature'
            ,prop28         : 'Support - Search Criteria'
            ,prop30			: 'Visitor - IP Address'
            ,prop33         : 'Page - Product Application Version'
            ,prop34         : 'Page - Website Extensions'
            ,prop60			: 'Search - Data Form Criteria'
            ,prop63			: 'Page - Extended Page Name'
            ,prop65         : 'Page - Online State'
            ,prop67         : 'Research Networks'
            ,prop40: 'Page - UX Properties'

            ,eVar3			: 'Search - Total Results'
            ,eVar7			: 'Visitor - Account Name'
            ,eVar15			: 'Event - Search Results Click Position'
            ,eVar19			: 'Search - Advanced Criteria'
            ,eVar21			: 'Promo - Clicked ID'
            ,eVar22			: 'Page - Test ID'
            ,eVar27			: 'Event - AutoSuggest Search Data'
            ,eVar157		: 'Event - AutoSuggest Search Typed Term'
            ,eVar156		: 'Event - AutoSuggest Search Selected Term'
            ,eVar162		: 'Event - AutoSuggest Search Category'
            ,eVar163		: 'Event - AutoSuggest Search Details'
            ,eVar33			: 'Visitor - Access Type'
            ,eVar34			: 'Order - Promo Code'
            ,eVar39			: 'Order - Payment Method'
            ,eVar41			: 'Visitor - Industry'
            ,eVar42			: 'Visitor - SIS ID'
            ,eVar43			: 'Page - Error Type'
            ,eVar44			: 'Event - Updated User Fields'
            ,eVar48			: 'Email - Recipient ID'
            ,eVar51			: 'Email - Message ID'
            ,eVar52			: 'Visitor - Department ID'
            ,eVar53			: 'Visitor - Department Name'
            ,eVar60			: 'Search - Within Content Criteria'
            ,eVar61			: 'Search - Within Results Criteria'
            ,eVar62			: 'Search - Result Types'
            ,eVar74			: 'Page - Journal Info'
            ,eVar59			: 'Page - Journal Publisher'
            ,eVar76			: 'Email - Broadlog ID'
            ,eVar78			: 'Visitor - Details'
            ,eVar80         : 'Visitor - Usage Path Info'
            ,eVar102		: 'Form - Name'
            ,eVar103        : 'Event - Conversion Driver'
            ,eVar105        : 'Search - Current Page'
            ,eVar106        : 'Visitor - App Session ID'
            ,eVar107        : 'Page - Secondary Product Name'
            ,eVar117        : 'Search - Database'
            ,eVar126        : 'Page - Environment'
            ,eVar141        : 'Search - Criteria Original'
            ,eVar143        : 'Page - Tabs'
            ,eVar161        : 'Search - Channel'
            ,eVar169        : 'Search - Facet Operation'
            ,eVar173        : 'Search - Details'
            ,eVar174        : 'Campaign - Spredfast ID'
            ,eVar175        : 'Visitor - TMX Device ID'
            ,eVar176        : 'Visitor - TMX Request ID'
            ,eVar148        : 'Visitor - Platform Name'
            ,eVar149        : 'Visitor - Platform ID'
            ,eVar152        : 'Visitor - Product ID'
            ,eVar153        : 'Visitor - Superaccount ID'
            ,eVar154        : 'Visitor - Superaccount Name'
            ,eVar177        : 'Page - Context Domain'
            ,eVar189    : 'Page - Experimentation User Id'
            ,eVar190    : 'Page - Identity User'
            ,eVar199    : 'Page - ID+ Parameters'

            ,list2			: 'Page - Widget Names'
            ,list3			: 'Promo - IDs'
        };

        for (var i in vars) {
            s[i] = s[i] ? s[i] : _satellite.getVar(vars[i]);
        }
    }
};

// async support fallback
(function(w) {
	var eventBuffer = [];
	if(w.appData) {
		if(Array.isArray(w.appData)) {
			eventBuffer = w.appData;
		} else {
			console.error('Elsevier DataLayer "window.appData" must be specified as array');
			return;
		}
    }

	w.appData = [];

	var oldPush = w.appData.push;

	var appDataPush = function() {
        oldPush.apply(w.appData, arguments);
        for(var i=0; i<arguments.length; i++) {
            var data = arguments[i];
            if(data.event) {
                if(data.event == 'pageLoad') {
                    w.pageDataTracker.trackPageLoad(data);
                } else {
                    w.pageDataTracker.trackEvent(data.event, data);
                }
            }
        }
	};

	w.appData.push = appDataPush;
	for(var i=0; i<eventBuffer.length; i++) {
	    var data = eventBuffer[i];
	    w.appData.push(data);
	}
})(window);

</script><script>_satellite["_runScript1"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><script src="https://cdn.plu.mx/widget-summary.js" async=""></script><script>_satellite["_runScript2"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-hide ot-fade-in"></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" checked="" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" checked="" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" checked="" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details Listâ€Ž</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Searchâ€¦" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><button aria-label="Feedback" type="button" id="_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E" data-layout="badgeBlank" class="_pendo-badge _pendo-badge_" style="z-index: 19000; margin: 0px; height: 32px; width: 128px; font-size: 0px; background: rgba(255, 255, 255, 0); padding: 0px; line-height: 1; min-width: auto; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; border: 0px; float: none; vertical-align: baseline; cursor: pointer; position: absolute; top: 50765px; left: 912px;"><img id="pendo-image-badge-19b66351" src="https://pendo-static-5661679399600128.storage.googleapis.com/D_T2uHq_M1r-XQq8htU6Z3GjHfE/guide-media-75af8ddc-3c43-49fd-8836-cfc7e2c3ea60" alt="Feedback" data-_pendo-image-1="" class="_pendo-image _pendo-badge-image" style="display: block; height: 32px; width: 128px; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; float: none; vertical-align: baseline;"></button></body></html>