{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "https://ieeexplore.ieee.org/document/4563052\n",
      "https://sci-hub.se/10.1109/CVPRW.2008.4563052\n",
      "https://ieeexplore.ieee.org/document/1640921\n",
      "https://sci-hub.se/10.1109/CVPR.2006.14\n",
      "https://ieeexplore.ieee.org/document/4813304\n",
      "https://sci-hub.se/10.1109/AFGR.2008.4813304\n",
      "https://ieeexplore.ieee.org/document/1613022\n",
      "https://sci-hub.se/10.1109/FGR.2006.6\n",
      "https://ieeexplore.ieee.org/document/5975141\n",
      "https://sci-hub.se/10.1109/T-AFFC.2011.25\n",
      "https://ieeexplore.ieee.org/document/5523955\n",
      "https://sci-hub.se/10.1109/TMM.2010.2060716\n",
      "https://ieeexplore.ieee.org/document/5647898\n",
      "https://sci-hub.se/10.1109/CISP.2010.5647898\n",
      "http://ieeexplore.ieee.org/document/7775081\n",
      "https://sci-hub.se/10.1109/TIFS.2016.2636093\n",
      "https://ieeexplore.ieee.org/document/9727163\n",
      "https://sci-hub.se/10.1109/ACCESS.2022.3156598\n",
      "http://ieeexplore.ieee.org/document/8014813\n",
      "https://sci-hub.se/10.1109/CVPRW.2017.79\n",
      "https://ieeexplore.ieee.org/document/8291717\n",
      "https://sci-hub.se/10.1109/ACCESS.2018.2805861\n",
      "https://ieeexplore.ieee.org/abstract/document/8025197\n",
      "https://sci-hub.se/10.1109/ARSO.2017.8025197\n",
      "https://ieeexplore.ieee.org/document/8014982/\n",
      "https://sci-hub.se/10.1109/CVPRW.2017.248\n",
      "https://ieeexplore.ieee.org/document/6595975\n",
      "https://sci-hub.se/10.1109/CVPRW.2013.130\n",
      "https://ieeexplore.ieee.org/document/6553734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(3, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(95, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(43, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(5, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(56, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(7, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(97, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(43, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(65, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(135, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(6, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(98, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(74, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(43, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(135, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(74, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(40, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(56, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(6, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(97, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(3, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(98, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(74, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(98, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(97, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(3, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(56, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(40, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(135, 0, 1478170596432)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sci-hub.se/10.1109/FG.2013.6553734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(5, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(43, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(7, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(56, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(97, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(135, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(70, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(43, 0, 1478170596432)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(65, 0, 1478170596432)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ieeexplore.ieee.org/abstract/document/7965717/\n",
      "https://sci-hub.se/10.1109/SERA.2017.7965717\n",
      "https://ieeexplore.ieee.org/document/7747479\n",
      "https://sci-hub.se/10.1109/TCYB.2016.2625419\n",
      "https://ieeexplore.ieee.org/document/6998925/\n",
      "https://sci-hub.se/10.1109/TAFFC.2014.2386334\n",
      "https://ieeexplore.ieee.org/document/7451244\n",
      "https://sci-hub.se/10.1109/TAFFC.2016.2553038\n",
      "https://ieeexplore.ieee.org/abstract/document/4539275/\n",
      "https://sci-hub.se/10.1109/TIFS.2008.924598\n",
      "https://ieeexplore.ieee.org/document/9815154\n",
      "https://sci-hub.se/10.1109/TAFFC.2022.3188390\n",
      "https://ieeexplore.ieee.org/document/6200254\n",
      "https://sci-hub.se/10.1109/MMUL.2012.26\n",
      "https://ieeexplore.ieee.org/document/4804691\n",
      "https://sci-hub.se/10.1109/TSMCB.2009.2014245\n",
      "https://ieeexplore.ieee.org/iel5/83/4358840/06020798.pdf\n",
      "https://sci-hub.se/10.1109/TIP.2011.2168413\n",
      "https://ieeexplore.ieee.org/document/7284873\n",
      "https://sci-hub.se/10.1109/FG.2015.7284873\n",
      "https://ieeexplore.ieee.org/document/7574736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(117, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(121, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(125, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(129, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(141, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(57, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(129, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(117, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(61, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(52, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(66, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(141, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(61, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(121, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(55, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(141, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(66, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(66, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(121, 0, 1477622116752)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sci-hub.se/10.1109/ICMEW.2016.7574736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(137, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(133, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(57, 0, 1477622116752)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(113, 0, 1477622116752)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ieeexplore.ieee.org/document/5674019\n",
      "https://sci-hub.se/10.1109/T-AFFC.2010.16\n",
      "https://ieeexplore.ieee.org/document/7780969\n",
      "https://sci-hub.se/10.1109/CVPR.2016.600\n",
      "https://ieeexplore.ieee.org/document/840614\n",
      "https://sci-hub.se/10.1109/AFGR.2000.840614\n",
      "https://ieeexplore.ieee.org/document/7280539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(118, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(136, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(30, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(54, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(17, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(118, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(54, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(54, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sci-hub.se/10.1109/IJCNN.2015.7280539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(54, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(54, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(30, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(49, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(34, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(64, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(81, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(30, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(93, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(69, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(81, 0, 1478193422224)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(105, 0, 1478193422224)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ieeexplore.ieee.org/document/1176132\n",
      "https://sci-hub.se/10.1109/TNN.2002.806629\n",
      "https://ieeexplore.ieee.org/document/1000134\n",
      "https://sci-hub.se/10.1109/TNN.2002.1000134\n",
      "https://ieeexplore.ieee.org/document/554195\n",
      "https://sci-hub.se/10.1109/72.554195\n",
      "https://ieeexplore.ieee.org/document/9154121\n",
      "https://sci-hub.se/10.1109/INCET49848.2020.9154121\n",
      "http://ieeexplore.ieee.org/document/8723728/\n",
      "https://sci-hub.se/10.1109/ICOASE.2019.8723728\n",
      "http://ieeexplore.ieee.org/document/5551215/\n",
      "https://sci-hub.se/10.1109/TNN.2010.2064176\n",
      "https://ieeexplore.ieee.org/document/4032815\n",
      "https://sci-hub.se/10.1109/TIP.2006.884954\n",
      "https://ieeexplore.ieee.org/document/7518582\n",
      "https://sci-hub.se/10.1109/TAFFC.2016.2593719\n",
      "https://ieeexplore.ieee.org/document/8119447\n",
      "https://sci-hub.se/10.1109/KSE.2017.8119447\n",
      "https://ieeexplore.ieee.org/document/5871583\n",
      "https://sci-hub.se/10.1109/T-AFFC.2011.13\n",
      "https://ieeexplore.ieee.org/document/8371638\n",
      "https://sci-hub.se/10.1109/TMM.2018.2844085\n",
      "https://ieeexplore.ieee.org/document/1593706\n",
      "https://sci-hub.se/10.1109/TNN.2005.860849\n",
      "https://ieeexplore.ieee.org/document/9226437\n",
      "https://sci-hub.se/10.1109/TIM.2020.3031835\n",
      "https://ieeexplore.ieee.org/document/8308363\n",
      "https://sci-hub.se/10.1109/AICCSA.2017.124\n",
      "https://ieeexplore.ieee.org/document/8528894\n",
      "https://sci-hub.se/10.1109/TAFFC.2018.2880201\n",
      "https://ieeexplore.ieee.org/document/8734943\n",
      "https://sci-hub.se/10.1109/KBEI.2019.8734943\n",
      "http://ieeexplore.ieee.org/abstract/document/7789677/\n",
      "https://sci-hub.se/10.1109/CVPRW.2016.187\n",
      "https://ieeexplore.ieee.org/document/6131215\n",
      "https://sci-hub.se/10.1109/EMS.2011.20\n",
      "http://ieeexplore.ieee.org/document/8373843/\n",
      "https://sci-hub.se/10.1109/FG.2018.00050\n",
      "https://ieeexplore.ieee.org/document/7961791\n",
      "https://sci-hub.se/10.1109/FG.2017.140\n",
      "https://ieeexplore.ieee.org/document/8227443\n",
      "https://sci-hub.se/10.1109/DICTA.2017.8227443\n",
      "https://ieeexplore.ieee.org/document/9674818\n",
      "https://sci-hub.se/10.1109/TII.2022.3141400\n",
      "https://ieeexplore.ieee.org/document/6874505\n",
      "https://sci-hub.se/10.1109/TAFFC.2014.2346515\n",
      "https://ieeexplore.ieee.org/document/7410698\n",
      "https://sci-hub.se/10.1109/ICCV.2015.341\n",
      "https://ieeexplore.ieee.org/document/7956190\n",
      "https://sci-hub.se/10.1109/TCSVT.2017.2719043\n",
      "http://ieeexplore.ieee.org/document/8658192/\n",
      "https://sci-hub.se/10.1109/ACCESS.2019.2901521\n",
      "https://ieeexplore.ieee.org/document/9474949\n",
      "https://sci-hub.se/10.1109/TIP.2021.3093397\n",
      "https://ieeexplore.ieee.org/document/8080244\n",
      "https://sci-hub.se/10.1109/TIP.2017.2765830\n",
      "https://ieeexplore.ieee.org/document/7006757\n",
      "https://sci-hub.se/10.1109/TIP.2015.2390959\n",
      "https://ieeexplore.ieee.org/document/4813445\n",
      "https://sci-hub.se/10.1109/AFGR.2008.4813445\n",
      "https://ieeexplore.ieee.org/document/7944639\n",
      "https://sci-hub.se/10.1109/TMM.2017.2713408\n",
      "https://ieeexplore.ieee.org/document/8576656\n",
      "https://sci-hub.se/10.1109/TIP.2018.2886767\n",
      "https://ieeexplore.ieee.org/document/8545853/\n",
      "https://sci-hub.se/10.1109/ICPR.2018.8545853\n",
      "http://ieeexplore.ieee.org/document/9143068/\n",
      "https://sci-hub.se/10.1109/ACCESS.2020.3010018\n",
      "https://ieeexplore.ieee.org/document/8895215\n",
      "https://sci-hub.se/10.1109/TIPTEKNO.2019.8895215\n",
      "https://ieeexplore.ieee.org/document/8014981\n",
      "https://sci-hub.se/10.1109/CVPRW.2017.247\n",
      "https://ieeexplore.ieee.org/document/908962\n",
      "https://sci-hub.se/10.1109/34.908962\n",
      "https://ieeexplore.ieee.org/document/1467492/1000\n",
      "https://sci-hub.se/10.1109/CVPR.2005.297\n",
      "https://ieeexplore.ieee.org/document/8099760\n",
      "https://sci-hub.se/10.1109/CVPR.2017.277\n",
      "https://ieeexplore.ieee.org/document/7124463\n",
      "https://sci-hub.se/10.1109/TIFS.2015.2446438\n",
      "https://ieeexplore.ieee.org/document/7961822\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/FG.2017.99\n",
      "https://ieeexplore.ieee.org/document/6130508\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/ICCVW.2011.6130508\n",
      "https://ieeexplore.ieee.org/document/5543262\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/CVPRW.2010.5543262\n",
      "https://ieeexplore.ieee.org/document/5771374\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/FG.2011.5771374\n",
      "https://ieeexplore.ieee.org/document/4459336\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/TPAMI.2008.48\n",
      "https://ieeexplore.ieee.org/document/5653653\n",
      "PDF link not found.\n",
      "https://sci-hub.se/10.1109/ICIP.2010.5653653\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "def get_doi_from_ieee_page(url):\n",
    "    # Set up the Firefox WebDriver options\n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    gecko_driver_path =os.getcwd()+'/geckodriver/geckodriver.exe'\n",
    "    # Specify the path to the Firefox binary if it's in a non-standard location\n",
    "    firefox_options.binary_location = 'C:/Program Files/Mozilla Firefox/firefox.exe'  # <-- Update this to your Firefox path\n",
    "    service = Service(gecko_driver_path)\n",
    "    # Use webdriver-manager to automatically download and manage GeckoDriver\n",
    "    driver = webdriver.Firefox(service=service, options=firefox_options)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the IEEE Xplore paper page URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load fully\n",
    "        time.sleep(5)  # Adjust based on your connection speed\n",
    "        \n",
    "        # Get the page source after JavaScript execution\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # Find the DOI on the page (DOIs are typically found in a meta tag or a specific link)\n",
    "        \n",
    "        text = soup.prettify()\n",
    "        pattern = r'doi.org/(.*?)(?=\")'\n",
    "        match = re.search(pattern, text)\n",
    "        doi = match.group(1)\n",
    "        return doi\n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "    finally:\n",
    "        # Close the browser after scraping\n",
    "        driver.quit()\n",
    "def download_pdf(sci_hub_url):\n",
    "    # Fetch the Sci-Hub page\n",
    "    response = requests.get(sci_hub_url)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the download button and extract the URL from its onclick attribute\n",
    "    pdf_link = None\n",
    "    pdf_link = None\n",
    "    button = soup.find('button', onclick=re.compile(r'location\\.href='))\n",
    "\n",
    "    if button:\n",
    "        # Extract the link from the onclick attribute\n",
    "        onclick_value = button['onclick']\n",
    "        match = re.search(r\"location\\.href='(.*?)'\", onclick_value)\n",
    "        if match:\n",
    "            pdf_link = match.group(1)\n",
    "\n",
    "    if pdf_link is None:\n",
    "        print(\"PDF link not found.\")\n",
    "        return None\n",
    "\n",
    "    # Make the PDF link absolute\n",
    "    pdf_link = requests.compat.urljoin(sci_hub_url, pdf_link)\n",
    "\n",
    "    # Download the PDF\n",
    "    pdf_response = requests.get(pdf_link)\n",
    "    return pdf_response.content\n",
    "\n",
    "def read_pdf(pdf_content):\n",
    "    # Read the PDF content\n",
    "    pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))\n",
    "    full_text = ''\n",
    "    \n",
    "    for page in pdf_reader.pages:\n",
    "        full_text += page.extract_text() + '\\n'\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "df = pd.read_csv('Scrapes_ALL.csv')\n",
    "errors = []\n",
    "all_links = [i for i in list(df['URL']) if 'ieee' in i]\n",
    "full_texts = []\n",
    "print(len(all_links))\n",
    "for i in all_links:\n",
    "    try:\n",
    "        url = i\n",
    "        print(url)\n",
    "        doi = get_doi_from_ieee_page(url)\n",
    "        sci_link = 'https://sci-hub.se/'+doi\n",
    "        pdf_content = download_pdf(sci_link)\n",
    "        print(sci_link)\n",
    "        if pdf_content:\n",
    "            full_text = read_pdf(pdf_content)\n",
    "            full_texts.append(full_text)\n",
    "        else:\n",
    "            full_texts.append('Nothing found')\n",
    "    except:\n",
    "        full_texts.append('Error')\n",
    "        errors.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"3D Facial Expression Recognition Based on Automatically Selected Features\\nHao Tang and Thomas S. Huang\\nUniversity of Illinois at Urbana-Champaign, Urbana, IL 61801\\nfhaotang2,huangg@ifp.uiuc.edu\\nAbstract\\nIn this paper, the problem of person-independent fa-\\ncial expression recognition from 3D facial shapes is inves-\\ntigated. We propose a novel automatic feature selection\\nmethod based on maximizing the average relative entropy\\nof marginalized class-conditional feature distributions and\\napply it to a complete pool of candidate features composed\\nof normalized Euclidean distances between 83 facial fea-\\nture points in the 3D space. Using a regularized multi-class\\nAdaBoost classi\\x02cation algorithm, we achieve a 95.1% av-\\nerage recognition rate for six universal facial expressions\\non the publicly available 3D facial expression database BU-\\n3DFE [1], with a highest average recognition rate of 99.2%\\nfor the recognition of surprise. We compare these results\\nwith the results based on a set of manually devised features\\nand demonstrate that the auto features yield better results\\nthan the manual features. Our results outperform the re-\\nsults presented in the previous work [2] and [3], namely\\naverage recognition rates of 83.6% and 91.3% on the same\\ndatabase, respectively.\\n1. Introduction\\n3D facial shapes, which are de\\x02ned by the 3D geometry\\nof the faces of human beings, contain important information\\nabout human's facial expressions. Such information is in-\\nvariant to pose and lighting conditions, which have imposed\\nserious hurdles on many 2D facial analysis tasks such as\\n2D facial expression recognition. In light of this, the explo-\\nration and exploitation of 3D facial geometric information\\nin tackling various facial analysis problems have emerged\\nin the research community. Thanks to the fast-developing\\nand increasingly mature 3D range scanning products and\\ncomputer vision based techniques for 3D face reconstruc-\\ntion from one or more 2D images, the 3D facial geometric\\ninformation has been relatively extensively utilized in the\\nstudy of face recognition [4, 5]. However, unlike in the\\ncase of face recognition, far fewer work has been done on\\nfacial expression recognition which takes advantage of the\\n3D facial geometric information. The most obvious reasonfor this embarrassing situation is that there has long been a\\nlack of a publicly available 3D facial expression database to\\nserve as a common platform for the researchers in the \\x02eld.\\nWithout such a database, neither can the researchers start\\nwith problem investigation nor can they end in comparing\\ntheir results. Now the good news is, Yin et. al. at Bing-\\nhamton University have recently constructed a 3D facial\\nexpression database for facial behavior research and made\\nit publicly available [1]. This is de\\x02nitely the \\x02rst attempt\\nto make a publicly available 3D facial expression database\\nfor the research community. It is believed that the existence\\nof this database will have a great impact on the facial\\nbehavior relevant research in the next a few years to come.\\nWang et. al. [2] performed the very \\x02rst work of 3D\\nfacial expression recognition on the BU-3DFE database.\\nThey reported that the highest average recognition rate\\nthey had obtained on this database was 83.6% by using\\nelaborately extracted primitive facial surface features\\nand an LDA classi\\x02er. They also reported that the facial\\nexpressions of happiness and surprise were well identi\\x02ed\\nwith accuracies of 95% and 90.8%, respectively. They\\ncompared their results with the results obtained by two\\n2D appearance feature based methods, namely the well-\\nknown Gabor-wavelet (GW) approach and the Topographic\\nContext (TC) approach. In both cases, they showed that\\ntheir results outperformed those for the 2D methods. They\\ndemonstrated that by incorporating 3D facial geometric\\ninformation the facial expression recognition problem\\ncan be better solved. This is highly expectable because\\n3D facial shapes contain rich information about facial\\nexpressions and are more robust than 2D appearances.\\nSoyel and Demirel [3] also conducted 3D facial expres-\\nsion recognition work on the BU-3DFE database. They\\napplied a carefully tuned neural network architecture to \\x02ve\\ncharacteristic facial feature distances that represent the eye\\nopening, eyebrow height, mouth opening, mouth height,\\nand lip stretching. They reported that the average recogni-\\ntion rate of their system was 91.3% and the highest average\\nrecognition rate reached to 98.3% in the recognition of\\nsurprise. As of the writing of this paper, to the best of our\\nknowledge, this is the only work that claims better results\\n978-1-4244-2340-8/08/$25.00 ©2008 IEEE\\nFigure 1. Some examples in the BU-3DFE database showing six universal facial expressions with four levels of intensities.\\nthan Wang et. al.'s work on the same database.\\nIn this paper, we further investigate the problem of\\nperson-independent facial expression recognition from 3D\\nfacial shapes based on the BU-3DFE database. We propose\\na novel automatic feature selection method based on max-\\nimizing the average relative entropy of marginalized class-\\nconditional feature distributions and apply it to a complete\\npool of candidate features composed of normalized Eu-\\nclidean distances between 83 facial feature points in the 3D\\nspace. Using a regularized multi-class AdaBoost classi\\x02ca-\\ntion algorithm with three different weak classi\\x02ers, namely\\nNearest Neighbor (NN), Naive Bayes (NB), and LDA, we\\nachieve a 95.1% average recognition rate for six universal\\nfacial expressions, namely anger, disgust, fear, happiness,\\nsadness, and surprise. The highest average recognition rate\\nthat we obtain is 99.2% for the recognition of surprise. We\\ncompare these results with the results based on a set of man-\\nually devised features and demonstrate that the auto features\\nyield better results than the manual features. In addition, we\\nshow that our results outperform the results presented in the\\nprevious work [2] and [3], namely average recognition rates\\nof 83.6% and 91.3% on the same database, respectively.\\n2. Database description\\nThe BU-3DFE database was recently developed by Yin\\net. al. at Binghamton University. It was designed to sample\\n3D facial behaviors with different prototypical emotional\\nstates at various levels of intensities. There are a total of\\n100 subjects in the database. Among these subjects, 56\\nare female and 44 are male. The subjects are well dis-tributed across different ethnic or racial ancestries, includ-\\ning White, Black, East-Asian, Middle-East Asian, Hispanic\\nLatino, and others. While being recorded, each subject was\\nasked to perform the neutral facial expression as well as\\nsix universal facial expressions, namely anger (AN), dis-\\ngust (DI), fear (FE), happiness (HA), sadness (SA), and sur-\\nprise (SU). Each facial expression has four levels of inten-\\nsities (low, middle, high, highest or 01-04), except that the\\nneutral facial expression has only one intensity level (00).\\nThus, there are 25 3D facial expression models for each\\nsubject, resulting in 2500 3D facial expression models in\\nthe database.\\nAssociated with a 3D facial expression model are a raw\\n3D face mesh model, a cropped 3D face mesh model, a pair\\nof texture images with two-angle views (about +45oand -\\n45oaway from the face frontal normal), a frontal-view tex-\\nture image, a set of 83 facial feature points, and a facial pose\\nvector. These data give a complete 3D description of a face\\nunder a speci\\x02c facial expression. A detailed description\\nof the BU-3DFE database can be found in [1]. In this pa-\\nper, we only use the 83 facial feature points marked on the\\ncropped 3D face mesh model, as shown in Figure 2 (a). We\\nrefer to a cropped 3D face mesh model as a 3D facial ex-\\npression model. Some examples of the 3D facial expression\\nmodels in the BU-3DFE database are illustrated in Figure 1.\\n3. Feature extraction\\nIn this paper, we manually devise a set of features (here-\\nafter referred to as the manual features, as compared to the\\nautomatically selected features, namely the auto features)\\nbased on the normalized Euclidean distances between the\\nfacial feature points on the 3D facial expression models.\\nAccording to Ekman [6, 7], the six universal facial expres-\\nsions are coded by combinations of action units, which are\\ncaused by the movements of particular facial feature points.\\nSimilarly, the MPEG-4 standard [8] speci\\x02es that the six\\nuniversal facial expressions correspond to six unique fa-\\ncial con\\x02gurations, which can be obtained by adjusting the\\nweights of certain facial animation parameters (FAPs). The\\nFAPs will change the relative position of the facial feature\\npoints, resulting in distinct collocation patterns of facial fea-\\nture points. In light of this, we believe that the distances be-\\ntween certain facial feature points offer a sparse, compact,\\nyet information-rich representation of the 3D facial shape.Such representation is invariant to translations and rotations\\nof the 3D facial geometry.\\nWe extract a set of 24 features composed of the nor-\\nmalized distances between a subset of the 83 facial feature\\npoints. These distances are believed to play an important\\nrole in determining the various facial expressions. The 24\\nfeatures are shown in Figure 2 (b) and their textual descrip-\\ntions are given in Table 1.\\nSince the faces of different people tend to have differ-\\nent sizes and proportions, in order to make the features\\nperson-independent, we normalize the distances by facial\\nanimation parameter units (FAPUs), as guided by theMPEG-4 standard. In MPEG-4, the FAPUs serve to scale\\nthe FAPs in order that a single set of FAPs can be used with\\narbitrary face models. The FAPUs are de\\x02ned as fractions\\nof distances between certain feature points on a face model\\nin its neutral state and allow us to interpret the FAPs on\\narbitrary face models in a consistent way. The \\x02ve FAPUs\\nare shown in Figure 3. The distances are then divided by\\nthe corresponding FAPUs, given as the unit in Table 1.\\n4. Automatic feature selection\\nIn a typical pattern classi\\x02cation system, the features\\nare generally devised by experts from the domain of the\\nproblem. While expert-derived features are often compact\\nand useful, to derive them requires domain knowledge and\\nmany trials and errors. An attractive scheme is that we have\\na large pool of candidate features, which are relatively easy\\nto collect, and we want a computer algorithm to be able to\\nautomatically select the \\x93best\\x94 features from this candidate\\nfeature pool.\\nIn order to perform automatic feature selection, one has\\nto tell the computer algorithm in what sense a feature is\\nconsidered \\x93the best\\x94. Since our goal is to classify patterns,\\nthe discrimination power comes in naturally. In general, the\\nmore discrimination power a feature possesses, the more\\nit will contribute to the classi\\x02cation accuracy. Thus, intu-\\nitively and naturally, we want an automatic feature selection\\nalgorithm to select from the candidate pool those features\\n11\\n13\\n1214\\n23246\\n41 2\\n7 8\\n15169101920212218 173\\n5\\n(a) (b)\\nFigure 2. (a) 83 facial feature points marked on the 3D facial ex-\\npression model displayed in the texture mode. (b) 24 manually\\ndevised features de\\x02ned by the normalized Euclidean distances\\nbetween certain facial feature points on the 3D facial expression\\nmodel displayed in the shade mode.\\nFigure 3. A face model in its neutral state on which the FAPUs are\\nde\\x02ned by the fractions of distances between the marked key facial\\nfeature points. The \\x02ve FAPUs are IRISD0 - Iris diameter, ES0 -\\nEye separation, ENS0 - Eye-nose separation, MNS0 - Mouth-nose\\nseparation, and MW0 - Mouth width, respectively [8].\\nwhich have the highest discrimination power, denoted by\\nDP(x), where xis a feature set or a feature vector.\\nFrom a Bayesian point of view, the discrimination power\\nof a feature vector DP(x)depends on how much discrimi-\\nnative information lies in the class-conditional distributions\\nofx. The more differences that the class-conditional distri-\\nbutions exhibit, the more discriminative information there\\nis in x. A good \\x93metric\\x94 that measures the differences be-\\ntween two probability distributions is the relative entropy, or\\nKullback-Leibler divergence (KLD) [9]. Given two proba-\\nFeature Textual description Unit\\n1 the length of the right eyebrow ES0\\n2 the length of the left eyebrow ES0\\n3 the distance between the left and right eyebrows ES0\\n4 the distance between the left and right eyes ES0\\n5 the distance between the right inner eyebrow and the right inner eye corner ENS0\\n6 the distance between the left inner eyebrow and the left inner eye corner ENS0\\n7 the distance between the right outer eyebrow and the right outer eye corner ENS0\\n8 the distance between the left outer eyebrow and the left outer eye corner ENS0\\n9 the distance between the right inner eye corner and the right nose wing ES0\\n10 the distance between the left inner eye corner and the left nose wing ES0\\n11 the width of the right eye ES0\\n12 the width of the left eye ES0\\n13 the height of the right eye IRISD0\\n14 the height of the left eye IRISD0\\n15 the distance between right outer eye corner and the right outer mouth corner ENS0\\n16 the distance between the left outer eye corner and the left outer mouth corner ENS0\\n17 the distance between the right outer mouth corner and the right jaw root MNS0\\n18 the distance between the left outer mouth corner and the left jaw root MNS0\\n19 the distance between the right nostril and the upper mid lip MNS0\\n20 the distance between the left nostril and the upper mid lip MNS0\\n21 the distance between the right nostril and the chin MNS0\\n22 the distance between the left nostril and the chin MNS0\\n23 the width of the mouth MW0\\n24 the height of the mouth MNS0\\nTable 1. The normalized Euclidean distances between certain facial feature points on the 3D facial expression model comprise 24 manual\\nfeatures. The units are given by the MPEG-4 facial animation parameter units (FAPUs).\\nbility distributions f(x) and g(x), the KLD is given by\\nD(f(x)kg(x)) =Z\\nxf(x)logf(x)\\ng(x)(1)\\nThe properties of the KLD states that D\\x150andD= 0\\nif and only if f(x) =g(x). In addition, the KLD is not\\nsymmetric: D(f(x)kg(x))6=D(f(x)kg(x)). A commonly\\nused symmetric metric is given by\\nDs(f(x)kg(x)) =1\\n2(D(f(x)kg(x)) +D(g(x)kf(x)))\\n(2)\\nLetx= [x 1;x2;:::;x D]Tdenote a set of Dcandidate\\nfeatures. We \\x02rst linearly transform xsuch that\\ny=U(x\\x00m) (3)\\nwhere mis the sample mean vector of x,U=\\n[u1;u2;:::;uK]Tis an orthogonal matrix with each row be-\\ning an eigenvector of the sample covariance matrix of x,\\nandK=min(N;D )withNbeing the number of train-\\ning examples. By Equation 3, the components of xare\\ndecorrelated, resulting in a new feature vector ywith sta-\\ntistically independent components y1;y2;:::;y Kunder thejointly Gaussian assumption. Suppose there are Cclasses.\\nThe discrimination power of the feature set yis given by\\nDP(y) =CX\\ni=1CX\\nj=i+1Ds(fi(y)kfj(y))\\n=1\\n2CX\\ni=1CX\\nj=1D(fi(y)kfj(y)) (4)\\nwhere fi(y)andfj(y)are the class-conditional probability\\ndistributions of the feature vector y, or the class-conditional\\njoint probability distributions of the features y1;y2;:::;y K.\\nSincey1;y2;:::;y Kare statistically independent, we have\\nD(fi(y)kfj(y))\\n=Z\\ny1;y2;:::;y KKY\\nk=1fi(yk)logQK\\nk=1fi(yk)\\nQK\\nk=1fj(yk)dy1;y2;:::;y K\\n=KX\\nk=1Z\\nykfi(yk)logfi(yk)\\nfj(yk)dyk\\n=KX\\nk=1D(fi(yk)kfj(yk)) (5)\\nEquation 5 indicates that given statistically independent\\nfeatures, the KLD between the class-conditional joint dis-\\ntributions is equal to the sum of the class-conditional distri-\\nbutions marginalized over all but one feature. Thus, from\\nEquation 4, we have\\nDP(y) =1\\n2CX\\ni=1CX\\nj=1KX\\nk=1D(fi(yk)kfj(yk))\\n=KX\\nk=1DP(yk) (6)\\nEquation 6 states that the discrimination power of a set\\nof independent features yis the sum of the discrimination\\npower of the individual features y1;y2;:::;y K. The intu-\\nition of this statement is that once we are given a new, in-\\ndependent feature, we at least gain some new information\\nin discriminating between the classes. However, due to the\\nlack of suf\\x02cient training data, we cannot keep all the fea-\\ntures, even if the total discrimination power increases, as\\ndeterred by the curse of dimensionality. From the additive\\nrelationship in Equation 6, one reasonable way of doing fea-\\nture selection is to keep only those features that have the\\nhighest discrimination power and that contribute the most\\nto the total discrimination power of the selected feature set.\\nThus, one can \\x02rst sort the features y1;y2;:::;y Kby their\\ndiscrimination power DP(yk)in the descending order and\\nthen keep only the \\x02rst K0< K features. As we will see\\nin the experiment section, the discrimination power of the\\nfeatures drops rather sharply, which means keeping only a\\nsmall number of the features would be suf\\x02cient.\\nAssuming that the class-conditional marginal feature dis-\\ntributions are Gaussian, namely\\nfi(yk) =1p\\n2\\x19\\x1bie\\x00(yk\\x00\\x16i)2\\n2\\x1b2\\ni i= 1;2;:::;C (7)\\nwhere \\x16iand\\x1biare the mean and standard deviation of the\\nclassi. The discrimination power of the feature ykis then\\nDP(yk) =1\\n2CX\\ni=1CX\\nj=1D(fi(yk)kfj(yk))\\n=1\\n2CX\\ni=1CX\\nj=1flog\\x1bj\\n\\x1bi+\\x1b2\\ni\\n2\\x1b2\\nj+(\\x16i\\x00\\x16j)2\\n2\\x1b2\\nj\\x001\\n2g(8)\\n5. Regularized AdaBoost classi\\x02cation\\nThe AdaBoost algorithm, originally proposed by Yoav\\nFreund and Robert Schapire [10], is a proven, effective clas-\\nsi\\x02cation algorithm. AdaBoost calls a weak classi\\x02er re-\\npeatedly in t= 1;2;:::;T rounds while at each round a\\ndifferent distribution over the training examples is providedAlgorithm 1 AdaBoost for binary classi\\x02cation.\\n1:Input: Ntraining examples fxi;yig, where xi2X,\\nyi2 f\\x001; 1g,i= 1;2;:::;N ; a weak classi\\x02er Cweak ;\\nthe number of iterations T.\\n2:Initialize D1(i) =1\\nN,i= 1;2;:::;N .\\n3:Fort= 1;2;:::;T , do\\n\\x0fcallCweak to generate a hypothesis ht:X!\\nf\\x001;1gthat minimizes the training error with re-\\nspect to the distribution Dt:ht= argmin\\nhj2H\\x0fj\\nwhere \\x0fj=PN\\ni=1Dt(i)[y i6=hj(xi)].\\n\\x0fif\\x0ft\\x150:5then stop.\\n\\x0fset\\x0bt=1\\n2log1\\x00\\x0f t\\n\\x0ft.\\n\\x0fupdate Dt+1(i) =1\\nZtDt(i)e\\x00\\x0btyiht(xi),i=\\n1;2;:::;N , where Ztis a normalization factor that\\nmakes Dt+1a valid probability distribution.\\n4:Output: H(x) = sign(PT\\nt=1\\x0btht(x)).\\nto the weak classi\\x02er. This distribution is updated to in-\\ndicate the importance of the examples in the training data\\nset for classi\\x02cation. That is, at each round, the weights\\nof every incorrectly classi\\x02ed examples are increased while\\nthe weights of every correctly classi\\x02ed examples are de-\\ncreased. After Trounds, the hypotheses of each round are\\nweighted and summed to produce a \\x02nal strong classi\\x02er.\\nThese procedures are presented in Algorithm 1.\\nThe AdaBoost algorithm presented in Algorithm 1 aims\\nto solve a binary classi\\x02cation problem. It can be readily\\nextended to solve multi-class classi\\x02cation problems [10,\\n11] by slightly changing the output \\x02nal hypothesis to\\nH(x) = argmax\\ny2YTX\\nt=1Dt(i)[y=ht(x)] (9)\\nwhere Y=fy1;y2;:::;y Cgrepresent the labels of the C\\nclasses.\\nThe basic idea of the AdaBoost algorithm is that it tries\\nto focus more on the dif\\x02cult training examples than theeasy ones. At each round, the distribution over the training\\nexamples is updated in a way such that the weak classi\\x02er is\\ntweaked in favor of those examples misclassi\\x02ed at the pre-\\nvious round in the hope that the misclassi\\x02cation error can\\nbe reduced overall. While being known that it is less sus-\\npectable to the over\\x02tting problem, the AdaBoost algorithmis sensitive to noise and outliers. Since the distribution is\\nadjusted at each round to give more weights to the dif\\x02cult\\nexamples, this distribution tends to become sharply peaked\\nat the outliers if they exist (as they are hard to be classi\\x02ed).\\nSuch a distribution would wrongly guide the weak classi\\x02er\\nAlgorithm 2 Regularized multi-class AdaBoost.\\n1:Input: Ntraining examples fxi;yig, where xi2X,\\nyi2 fy1;y2;:::;y Cg,i= 1;2;:::;N ; a weak classi\\x02er\\nCweak ; the number of iterations T; constants mandb.\\n2:Initialize D1(i) =1\\nN,i= 1;2;:::;N .\\n3:Initialize E(i) = 0, i= 1;2;:::;N .\\n4:Fort= 1;2;:::;T , do\\n\\x0fcallCweak to generate a hypothesis ht:X!\\nfy1;y2;:::;y Cgthat minimizes the training er-\\nror with respect to the distribution Dt:ht=\\nargmin\\nhj2H\\x0fjwhere \\x0fj=PN\\ni=1Dt(i)[y i6=hj(xi)].\\n\\x0fif\\x0ft\\x150:5then stop.\\n\\x0fsetE(i) = [E (i) + 1][y i6=ht(xi)],i=\\n1;2;:::;N .\\n\\x0fset\\x0bt=1\\n2log1\\x00\\x0f t\\n\\x0ft.\\n\\x0fupdate\\nDt+1(i) =Dt(i)\\nZt8\\n<\\n:e\\x00\\x0btifyi=ht(xi)\\n0 ifE(i)\\x15m; D t(i)> b\\ne\\x0bt ifyi6=ht(xi)\\ni= 1;2;:::;N , where Ztis a normalization factor\\nthat makes Dt+1a valid probability distribution.\\n5:Output: H(x) = argmax\\ny2YPT\\nt=1Dt(i)[y=ht(x)].\\nto focus on the outliers. It would be advantageous to discard\\nthe outliers and force the weak classi\\x02er to focus on the\\nmeaningful data. To this end, we regularize the AdaBoost\\nalgorithm in the following way: 1. If a training example is\\nincorrectly classi\\x02ed for a consecutive mrounds, then we\\nconsider it as an outlier. 2. If the current weight of this out-\\nlier exceeds a limit b(upper bound), we decide to discard\\nthis outlier by setting its weight to zero. The regularized\\nAdaBoost algorithm is given in Algorithm 2.\\n6. Experiments\\nWe conducted extensive experiments on the BU-3DFE\\ndatabase using the same setup as that in the previous work\\n[2] and [3]. In our experiments, we used the data of 60\\nsubjects with two high-intensity models for each of the six\\nuniversal facial expressions. For the purpose of person and\\ngender independency, half (30) of the 60 subjects were fe-\\nmale and half (30) were male. The subjects we chose were\\nwell distributed across the various races, as illustrated in\\nFigure 1. We randomly divided the 60 subjects into two\\nsets. The training set contained 54 subjects and the testset contained 6 subjects. The features we used were solely\\nbased on the 83 facial feature points given for every 3D\\nfacial expression model in the database. The experiments\\nwere carried out in the follow phases.\\nFirst, we extracted the manual features for each 3D facial\\nexpression model as described in Section 3. Each facial ex-\\npression model was then represented by a 24 dimensional\\nfeature vector. In this work, the neutral facial expression\\nis not classi\\x02ed. Rather, as a preprocessing step its fea-\\ntures serve as \\x02ducial measures that are subtracted from the\\nfeatures of the six universal facial expressions of the corre-\\nsponding subject. We applied the regularized AdaBoost al-\\ngorithm (Algorithm 2) with three weak classi\\x02ers, namely\\nNearest Neighbor (NN), Naive Bayes (NB), and LDA, to\\nthe manual features, and achieved average recognition rates\\nof 93.6%, 93.8%, and 91.8%, respectively. Note that these\\nnumbers were obtained by averaging the results of 10 in-\\ndependent experiments run on 10 random partitions of the\\ntraining and test data sets. We believe that they represent\\na generalization of the recognition rates. The average con-\\nfusion matrices of the experiments are given in Tables 2, 3,\\n4, respectively. From these confusion matrices, the high-\\nest average recognition rates in each of the experiments are\\n98.3%, 97.5%, and 95%, respectively, all for the recognition\\nof surprise.\\n\\x01% AN DI FE HA SA SU\\nAN 86.7 3.3 2.5 4.2 1.7 1.7\\nDI 0.8 94.2 1.7 0 3.3 0\\nFE 2.5 3.3 88.3 3.3 1.7 0.8\\nHA 0 0.8 0 98.3 0 0.8\\nSA 0 1.7 1.7 0.8 95.8 0\\nSU 0 0.8 0 0 0.8 98.3\\nTable 2. Average confusion matrix (manual features, regularized\\nAdaBoost with a nearest neighbor weak classi\\x02er).\\n\\x01% AN DI FE HA SA SU\\nAN 91.7 1.7 2.5 0 1.7 2.5\\nDI 1.7 90 3.3 0.8 2.5 1.7\\nFE 4.2 3.3 75.8 7.5 3.3 5.8\\nHA 0.8 0.8 2.5 90.8 2.5 2.5\\nSA 0 4.2 8.3 5 80 2.5\\nSU 1.7 0 0.8 0 0 97.5\\nTable 3. Average confusion matrix (manual features, regularized\\nAdaBoost with a naive Bayes weak classi\\x02er).\\nNext, we applied our automatic feature selection method\\nto a complete pool of normalized distances of facial fea-\\nture points. The 83 facial feature points on a 3D facial ex-\\npression model produced C2\\n83unique pairs of facial feature\\npoints and the distance of each pair was \\x02rst normalized\\n\\x01% AN DI FE HA SA SU\\nAN 80.8 7.5 6.7 2.5 2.5 0\\nDI 0.8 90.8 3.3 2.5 1.7 0.8\\nFE 2.5 5 80.8 5 5.8 0.8\\nHA 0 5 1.7 89.2 2.5 1.7\\nSA 4.2 3.3 4.2 5 81.7 1.7\\nSU 1.7 1.7 0 0.8 0.8 95\\nTable 4. Average confusion matrix (manual features, regularized\\nAdaBoost with an LDA weak classi\\x02er).\\n0 100 200 300 400 500 600 700051015202530\\nFeatur eDiscrimination power\\nFigure 4. The discrimination power (DP) of the individual features\\nsorted in descending order. The DP curve drops sharply.\\nby the distance between two outer eye corners of the same\\n3D facial expression model in order to make the features\\nscale-invariant. After the same \\x02ducial subtraction as done\\nfor the manual features, the C2\\n83normalized distances com-\\nprised a candidate feature pool, from which the automatic\\nfeature selection method selected the features with the high-\\nest discrimination power. We performed principal compo-\\nnent analysis (PCA) on the training data set. While the orig-inal feature dimension was C\\n2\\n83, since the training data set\\nonly contained 54\\x022\\x026 = 648 examples, there were only\\n648 eigenvectors of the sample covariance matrix associ-ated with non-zero eigenvalues. The auto features were then\\nselected from the 648 PCA transformed features. Figure 4\\nshows the discrimination power of the transformed features,\\nsorted in the descending order. We observe that the discrim-\\nination power of the features drops rather sharply. It is rea-\\nsonable to keep only a few features with dramatically large\\ndiscrimination power while discarding the rest. Empirically,\\n10 to 30 features are suf\\x02cient to yield good classi\\x02cation\\nresults. The class-conditional marginal distributions of the\\nfour top features with the highest discrimination power are\\nshown in Figure 5.\\nFinally, based on the auto features, we applied the regu-\\nlarized AdaBoost algorithm with the same three weak clas-\\nsi\\x02ers (NN, NB, LDA) and achieved average recognition\\nrates of 94.8%, 90.8%, and 95.1%, respectively. The corre-\\nsponding average confusion matrices are given in Tables 5,\\nFigure 5. The class-conditional distributions of the four top fea-\\ntures with the largest discrimination power: 27.74, 20.46, 8.35,\\nand 7.89. In each subplot, the green dots are the data points, the\\nblue bars the histogram, and the red curve the \\x02tted Gaussian.\\n6, 7, respectively. A typical run of the regularized AdaBoost\\nalgorithm is shown in Figure 6.\\n\\x01% AN DI FE HA SA SU\\nAN 94.2 1.7 2.5 0 0.8 0.8\\nDI 1.7 94.2 0.8 0.8 2.5 0\\nFE 0.8 3.3 87.5 4.2 0 4.2\\nHA 0 0 1.7 98.3 0 0\\nSA 0 1.7 3.3 0 95 0\\nSU 0 0.8 0 0 0 99.2\\nTable 5. Average confusion matrix (auto features, regularized Ad-\\naBoost with a nearest neighbor weak classi\\x02er).\\n\\x01% AN DI FE HA SA SU\\nAN 80 10 2.5 3.3 4.2 0\\nDI 2.5 89.2 3.3 2.5 1.7 0.8\\nFE 6.7 9.2 73.3 5.8 2.5 2.5\\nHA 2.5 2.5 1.7 88.3 3.3 1.7\\nSA 0.8 4.2 3.3 1.7 89.2 0.8\\nSU 0.8 1.7 2.5 0.8 0.8 93.3\\nTable 6. Average confusion matrix (auto features, regularized Ad-\\naBoost with a naive Bayes weak classi\\x02er).\\nA comparison of our results and the results reported in\\nthe previous work [2] and [3] is shown in Figure 7. On\\naverage, our results on the auto features are better than our\\n\\x01% AN DI FE HA SA SU\\nAN 85.8 7.5 3.3 2.5 0 0.8\\nDI 5 86.7 0.8 3.3 2.5 1.7\\nFE 1.7 5 79.2 3.3 8.3 2.5\\nHA 3.3 0.8 1.7 94.2 0 0\\nSA 4.2 0 2.5 1.7 90.8 0.8\\nSU 0.8 0.8 0 0 0.8 97.5\\nTable 7. Average confusion matrix (auto features, regularized Ad-\\naBoost with an LDA weak classi\\x02er.\\n2 4 6 8 10 12 14 16 18 2000.10.20.30.40.50.60.70.80.91AdaBoost with different weak classifiers\\nIteration numberRecognition rate\\n  \\nAdaBoost with Nearest Neighbor\\nAdaBoost with Naive Bayes\\nAdaBoost with LDA\\nFigure 6. A typical run of the regularized multi-class AdaBoost\\nalgorithm with three weak classi\\x02ers. The curves represent the\\nrecognition rate on the test set vs. the number of training iterations.\\n7580859095100Average recognition rate (%)Performance comparison\\n  \\nManual features, AdaBoost.NN\\nManual features, AdaBoost.NB\\nManual features, AdaBoost.LDA\\nAuto features, AdaBoost.NN\\nAuto features, AdaBoost.NB\\nAuto features, AdaBoost.LDA\\nWang et. al.'s work\\nSoyel and Demirel's work\\nFigure 7. Comparison of the results.\\nresults on the manual features. All our results outperform\\nthose of the previous work, except that in the case of auto\\nfeatures and regularized AdaBoost with a naive Bayes weak\\nclassi\\x02er, our result is slightly worse than that of [3].\\n7. Conclusion\\nIn this paper, we investigate the problem of person-\\nindependent facial expression recognition from 3D facial\\nshapes. We propose a novel automatic feature selection\\nmethod based on maximizing the average relative entropyof marginalized class-conditional feature distributions\\nand apply it to a complete pool of candidate features\\ncomposed of normalized Euclidean distances between 83\\nfacial feature points in the 3D space. Using a regularized\\nmulti-class AdaBoost classi\\x02cation algorithm, we achieve\\na 95.1% average recognition rate for six universal facial\\nexpressions on the publicly available 3D facial expression\\ndatabase BU-3DFE, with a highest average recognition rate\\nof 99.2% for the recognition of surprise. We compare these\\nresults with the results based on a set of manually devised\\nfeatures and demonstrate that the auto features yield better\\nresults than the manual features. Our results outperform the\\nresults presented in the previous work [2] and [3], namely\\naverage recognition rates of 83.6% and 91.3% on the same\\ndatabase, respectively.\\nReferences\\n[1] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and Matthew\\nRosato, \\x93A 3D Facial Expression Database For Facial Behav-\\nior Research\\x94, 7th International Conference on Automatic\\nFace and Gesture Recognition (FG2006), pp. 211 - 216.\\n[2] Jun Wang, Lijun Yin, Xiaozhou Wei, and Yi Sun, \\x933D Facial\\nExpression Recognition Based on Primitive Surface Feature\\nDistribution\\x94, IEEE International Conference on Computer\\nVision and Pattern Recognition (CVPR 2006).\\n[3] Soyel, H. and Demirel, H., \\x93Facial Expression Recognition\\nUsing 3D Facial Feature Distances\\x94, ICIAR07, pp. 831-838.\\n[4] Bronstein, A. M.; Bronstein, M.M, and Kimmel, R., \\x93Three-\\ndimensional face recognition\\x94, International Journal of Com-\\nputer Vision (IJCV) 64 (1): 5-30, 2005.\\n[5] Kakadiaris, I. A.; Passalis, G. and Toderici, G. and Mur-\\ntuza, N. and Karampatziakis, N. and Theoharis, T., \\x933D face\\nrecognition in the presence of facial expressions: an anno-\\ntated deformable model approach\\x94, Transactions on Pattern\\nAnalysis and Machine Intelligence (PAMI) 13 (12), 2007.\\n[6] P. Ekman and W. Friesen, \\x93Facial Action Coding System: A\\nTechnique for the Measurement of Facial Movement\\x94, Con-\\nsulting Psychologists Press, Palo Alto, 1978.\\n[7] Ekman, P., Huang, T., Sejnowski, T., and Hager, J., \\x93Final\\nReport to NSF of the Planning Workshop on Facial Expres-\\nsion Understanding\\x94, Available from HIL-0984, UCSF, San\\nFrancisco, CA 94143.\\n[8] Igor S. Pandzic, Robert Forchheimer (Eds), MPEG-4 Facial\\nAnimation: The Standard, Implementation and Applications,\\nJohn Wiley & Sons, Inc., 2002.\\n[9] S. Kullback, \\x93The Kullback-Leibler distance\\x94, The Ameri-\\ncan Statistician 41:340-341, 1987.\\n[10] Y . Freund and R. Schapire, \\x93A decision-theoretic general-\\nization of on-line learning and an application to boosting\\x94,\\nJournal of Computer and System Sciences, 55(1):119\\x96139,\\nAugust, 1997.\\n[11] R. Schapire and Y . Singer, \\x93Improved Boosting Algorithms\\nUsing Con\\x02dence-rated Predictions\\x94, Machine Learning\\n37(3): 297-336, December, 1999.\\n\",\n",
       " '3D Facial Expression Recognition Based on Primitive Surface Feature\\nDistribution\\nJun Wang, Lijun Yin, Xiaozhou Wei and Yi Sun\\nDepartment of Computer Science\\nState University of New Y ork at Binghamton, NY , 13902, USA\\nAbstract\\nThe creation of facial range models by 3D imaging sys-\\ntems has led to extensive work on 3D face recognition [19].\\nHowever, little work has been done to study the usefulness\\nof such data for recognizing and understanding facial ex-pressions. Psychological research shows that the shape of ahuman face, a highly mobile facial surface, is critical to fa-cial expression perception. In this paper, we investigate theimportance and usefulness of 3D facial geometric shapes to\\nrepresent and recognize facial expressions using 3D facial\\nexpression range data. We propose a novel approach to ex-\\ntract primitive 3D facial expression features, and then apply\\nthe feature distribution to classify the prototypic facial ex-\\npressions. In order to validate our proposed approach, we\\nhave conducted experiments for person-independent facial\\nexpression recognition using our newly created 3D facial\\nexpression database. We also demonstrate the advantages\\nof our 3D geometric based approach over 2D texture based\\napproaches in terms of various head poses.\\n1. Introduction\\nThere is a long history of interest in the problem of rec-\\nognizing human emotion from facial expressions, as well\\nas extensive studies on face perception over the last threedecades [8, 17]. Analyzing the emotional expression of ahuman face requires a number of preprocessing steps whichattempt to detect and locate characteristic facial regions, ex-\\ntract facial expression features, and model facial gestures\\nusing anatomic information about the face. Although all\\nthese steps are equally important, current research mostlyconcentrates on the facial expression feature detection anddescription, which is also the focus of this paper.\\nFacial expression features are mainly represented by\\nthree categories: (1) static versus dynamic (or temporal);(2) global versus local (or analytic); and (3) 2D versus\\n3D. Most research over the past thirty years has been di-\\nrected towards static/dynamic, analytic, 2D feature extrac-tion [9], focusing primarily on two types of features: 2D\\ngeometric features and appearance features from 2D static\\nimages [16, 18] or video sequences [4, 3, 31]. Geomet-ric features are described by a set of facial feature pointsused to derive facial organs’ shapes or expressive regions.Appearance features refer to the features exhibited on the\\nskin (e.g., frowns or wrinkles) [22] or action units [8, 6].\\nOver the past decade, a number of techniques have been\\nsuccessfully developed for facial expression recognition, in-cluding optical ﬂow [28, 10] and Gabor wavelets [16], andFACS-feature based techniques [22, 1, 6, 31]. The excel-lent review of recent advances in this ﬁeld can be found in\\n[5, 12, 17, 32]. All recent advances have been based on\\n2D images or videos, and most were primarily concerned\\nwith extracting prototypic expressions from frontal or near-frontal views of a face.\\nRecently, additional work has been done to improve the\\nperformance of facial expression recognition under vari-\\nous imaging conditions (e.g., pose, lighting, etc). Some\\nresearchers have successfully explored partial 3D informa-\\ntion for facial expression recognition, such as multiple-view\\nbased [18] and 3D model-based techniques [2, 13, 27, 30].\\nThese methods are still based on 2D data. Because the facial\\nexpression actuated by the facial muscle movement resultsin the facial skin shape variation, it is ideal to model thefacial expressions explicitly in a 3D space.\\nDue to the limitations in describing facial surface defor-\\nmation when 3D features are evaluated in 2D space, 2D\\nimages with a group of feature units may not accuratelyreﬂect complex and authentic facial expressions. More im-portantly, head pose and posture, which are precious cues inconjunction with facial action, reﬂect a person’s real emo-tion. Since people rarely express emotions without head\\nmotion or posture spontaneity, the assumption of frontalimages of faces under good illumination is to be unreal-istic. Therefore, there is a high demand to represent and\\nrecognize facial expressions in 3D space. In this paper, we\\naddress the issue regarding 3D global features on the 3Df a -\\ncial surface in order to mitigate the problems posted by 2Dbased facial expression analysis.\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\nThe3D surface features, reﬂecting the facial skin\\n“wave”, represent the intrinsic facial surface structure as-\\nsociated with the speciﬁc facial expressions. Motivated bythis fact, we propose a novel geometric feature based facialexpression descriptor in the 3-dimensional Euclidean space.\\nBased on the principal curvature information estimated onthe3D triangle mesh model, we apply a surface labeling\\napproach to classify the 3D primitive surface features into\\ntwelve basic categories. In order to classify the speciﬁc ex-\\npression, we partition the face surface into a number of ex-pressive regions, and conduct the statistics of the surface\\nprimitive label distribution on each region separately. The\\nstatistic histograms of the surface labels of all these regionsare combined to construct the speciﬁc facial expression fea-ture.\\nFinally, we validate our approach by the recognition ex-\\nperiments on our newly constructed 3D facial expression\\ndatabase. We conduct the experiments in the following\\nseveral aspects: the performance investigation of our 3D\\napproach; the comparison study with 2D appearance fea-\\nture based methods (Gabor-Wavelet method and our Topo-graphic Context method), including the front view case and\\nthe angle-view case. The experiments are executed in a\\nperson-independent manner, which means that the subject\\nbeing tested has never appeared in the training set. We haveselected four classiﬁers for the classiﬁcation experiments.Since the paper focuses on 3D facial expression descrip-\\ntion, the classiﬁcation algorithm design is not expatiated onin this paper.\\nThe remainder of this paper is organized as follows: In\\nSection 2, we introduce the surface primitive feature anal-ysis based on the computational geometry method. In Sec-tion 3, we describe the facial expression feature extractionfrom the primitive surface features distribution. Section 4reports the experimental results of the 3D facial expression\\nrecognition, followed by a comparison with the 2D appear-\\nance feature based approaches in Section 5. Finally, con-cluding remarks and discussion are given in Section 6.\\n2.3D Primitive Feature Analysis\\nThe surface feature analysis is based on the triangle\\nmeshes of faces, which are created by a 3D imaging system\\n[15]. The system captures a three dimensional point cloudand generates a meshed surface that models the face as 3D\\ntriangle mesh geometry. The 3D surface data points are less\\nthan300 microns apart, providing the capability of distin-\\nguishing between small differences regardless of lighting\\nor orientation during the original scan. Figure 1 shows an\\nexample of 3D facial expression range models with six pro-\\ntotypic facial expressions. In the following sub-sections, we\\nwill give a detailed description of the primitive facial feature\\nestimation and labeling of the range models.\\nFigure 1. An example of 3D facial range mod-\\nels showing six prototypic expressions, from\\nleft to right: Anger, Disgust, Fear, Happiness, Sad-\\nness, and Surprise . The textured models are\\nshown in the upper row, and the correspond-ing shaded models are in the lower row.\\n2.1. Principal Curvature Analysis by Local\\nSurface Fitting\\nThe shape information of a surface is “encoded” in its\\nprimitive geometric features (such as ridge, ravine, peak,pit, saddle, concave hill, convex hill, etc.), which may be\\nviewed as a digital signature of a facial expression. Thesefeatures are determined by the surface curvatures and theirprincipal directions. Curvature estimation techniques fortriangle meshes could be based on the mesh itself or al-\\nternatively on a local smooth approximation. In general,there are broadly two categories: discrete and continuous[11]. The ﬁrst refers to approximating curvatures by formu-\\nlating a closed form for differential geometry operators thatwork directly on the discrete representation of the underly-ing surface. The latter involves ﬁtting a surface locally, then\\ncomputing the curvatures by interrogating the ﬁtted surface.\\nOur experiment shows that curvature estimates derived by\\nlocally approximating the surface with a smooth polynomialfunction give better results than the discrete versions.\\nIn order to derive the geometry features of facial sur-\\nface, we give an analytic form of the regression functionfor smoothly ﬁtting the triangle mesh surface. Note thatalthough the triangle meshes are deﬁned in a global coordi-nate system, for easy computation, the ﬁtting procedure isperformed in a local coordinate system, which is centered\\nat the vertex to be examined. The local coordinate systemfor the surface ﬁtting is deﬁned as follows: let pbe a vertex\\non a 3D model M, andn\\np=(a,b, c)Tbe the unit nor-\\nmal at vertex p. A set of vertices that are adjacent to pis\\n{qi=(xi,yi,zi)T},i=1,2...,m . A local coordinate sys-\\ntem is deﬁned by taking the vertex pas an origin and ( nx,\\nny,np) as the three axes. nxandnyare two orthogonal\\naxes, which are arbitrarily selected to form a tangent planeperpendicular to the normal vector n\\npat crossing point p\\n(see Figure 2 for an example). The normal vector can be\\nsimply estimated by the mean normal, which is obtained by\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\nnp\\nv1\\nv2nx\\nny\\nxy\\nzp\\nFigure 2. An example of a local coordinate\\nsystem (nx,ny,np)deﬁned on a nose surface\\npatch. v1,v2are the principal directions at\\npointp.\\nthe average of normal values of the polygons sharing the\\ncommon vertex p.\\nTo ﬁt a smooth polynomial patch onto the local surface\\nand estimate the principal curvatures, we transform the ver-\\ntices of the local region to the local coordinate system. Theresulting vertices ˜q\\niand the corresponding normal vectors\\n˜n˜qiare expressed as:\\n˜qi=RT·(qi−p)\\n˜n˜qi=RT·nqi (1)\\nwhere the rotation matrix is obtained by R=[nxnynp].\\nThe vertex pand its normal npare transformed to the origin\\n˜p=( 0,0,0)Tand the unit vector ˜np=( 0,0,1)Talong the\\npositive zaxis, respectively.\\nThe approximating polynomial surface is at least sec-\\nond order. Considering the ﬁtting accuracy and the compu-tation complexity, we choose a cubic-order approximationmethod. Similar to the method in [14], we deﬁne the ﬁttingfunction in the form of\\nz(˜x,˜y)=U·X (2)\\nwhere the variable Uand the coefﬁcient Xare deﬁned as:\\nU=(1\\n2˜x2˜x˜y1\\n2˜y2˜x3˜x2˜y˜x˜y2˜y3)\\nX=(ABCDEFG )T(3)\\nGiven a set of neighbor vertices {˜qi=( ˜xi,˜yi,˜zi)T},\\n(i=1,2,..., m )and their corresponding normals {˜n˜qi=(ai,bi,ci)T}, we can establish 3mequations to solve the\\nseven parameters of X. When the vertex phas more than\\nthree neighbor vertices, the parameters Xcan be approx-\\nimated using the least-square ﬁtting method. With the re-\\ngressed local ﬁtting function z(˜x,˜y), the Weingarten matrix\\nfor the surface patch becomes:\\nW=/bracketleftBigg∂2z(˜x,˜y)\\n∂˜x2∂2z(˜x,˜y)\\n∂˜x∂˜y\\n∂2z(˜x,˜y)\\n∂˜x∂˜y∂2z(˜x,˜y)\\n∂˜y2/bracketrightBigg\\n=/bracketleftbiggAB\\nBC/bracketrightbigg\\n(4)\\nAfter the eigenvalue decomposition, the principal direc-\\ntions in the local coordinates ˜ v1and˜ v2can be estimated.\\nW=(˜ v1˜ v2)·diag(λ1λ2)·(˜ v1˜ v2)T(5)\\nwhere λ1andλ2are the eigenvalues and ˜ v1and˜ v2are the\\northogonal eigenvectors. If |λ1|>|λ2|,˜v1and˜v2are in the\\ndirections with the maximum curvature and the minimum\\ncurvature, respectively. Figure 3(a)(b) shows an example ofthe principal direction estimation on a facial range model.\\nSince the principal directions are represented in the local\\ncoordinate system, to obtain a global view of the principal\\ndirections, ˜ v\\n1and˜ v2must be rotated back to the global\\ncoordinate system, as formulated by\\nv1=R·˜ v1,v2=R·˜ v2 (6)\\nIt is worth noting that our surface labeling criteria rely\\non the surface principal curvatures, the principal directions\\nas well as the surface gradient. After the transformation\\nfrom the local coordinate system to the original global co-ordinate system, we can derive the gradient using the nor-mal direction of each surface point. Let z(x,y)be the ﬁt-\\nting function of a surface patch centered at p, the normal\\ndirection n\\np=(a,b, c)Tcan be written in the form of\\n(−∂z(x,y)\\n∂x,−∂z(x,y)\\n∂y,−1)T. The gradient magnitude /bardbl∇z(x,y)/bardbl\\natpis then calculated as:\\n/bardbl∇z(x,y)/bardbl=/radicalBig\\n[∂z(x,y)\\n∂x]2+[∂z(x,y)\\n∂x]2=/radicalBig\\n[−a\\nc]2+[−b\\nc]2(7)\\nThe principal curvature analysis produced a set of at-\\ntributes {/bardbl∇z/bardbl,v1,v2,λ1,λ2}, which describes the sur-\\nface property at each vertex. Every vertex can be classiﬁed\\naccording to a primitive feature classiﬁcation rule, which\\nwill be explained in the next sub-section.\\n2.2. Primitive 3D Surface Feature Labeling\\nThe principal curvatures λ1,λ2represent the maximum\\nand the minimum degrees of bending of a surface, v1and\\nv2indicate the surface principal directions, and /bardbl∇z/bardblre-\\nﬂects the steepness of the surface. Using these geometric\\nattributes, we are able to classify every vertex into one of\\nthe primitive categories. In other words, we can symbolize\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\n(a)\\n (b) (c)\\nFigure 3. From left to right: minimum curva-\\nture direction map, maximum curvature di-rection map and primitive label map.\\nthe geometric surface by classifying and labeling the primi-tive features. This symbolization process can be realized bymapping the primitive surface features from a 3D geometric\\nspace to a discrete label space, which is formulated as:\\nQ:{/bardbl∇z/bardbl,v\\n1,v2,λ1,λ2}=⇒{Lm},m=1,2,..., M\\n(8)\\nwhere Qis a pre-deﬁned classiﬁcation rule and {Lm}is\\na set of surface categories with a total of Mtypes to be\\nidentiﬁed.\\nThere are several existing rules for primitive surface fea-\\nture classiﬁcation. For example, (1) the shape index basedclassiﬁcation [7]; (2) Tanaka’s method [21], which catego-\\nrized eight distinct features to describe local shapes accord-\\ning to the sign of two principal curvatures; (3) the topo-graphic classiﬁcation method [23, 26], which has been used\\nfor analyzing the 3D topographic surfaces of gray level im-\\nages. Depending on the classiﬁcation ﬁneness, the maximaltwelve distinct primitive features can be deﬁned. They are\\npeak, pit, ﬂat, ravine, ridge, saddle (including ridge saddle\\nand ravine saddle) and hill (including convex hill, concavehill, concave saddle hill, convex saddle hill and slope hill).\\nIn order to scrutinize the facial expression surface de-\\ntails, we use the twelve distinct primitive surface features torepresent the facial expressions. Similar to the classiﬁcation\\nrule used in [23], we extend the method to the application\\nfor the real 3D facial surface labeling. The labeling process\\nis based on the feature values of {/bardbl∇z/bardbl,v\\n1,v2,λ1,λ2}in\\nthe global coordinate system. To do so, two thresholds, TG\\nandTλ, are deﬁned. They are used to evaluate whether the\\ngradient magnitude and the principal curvatures are trivial\\nenough to be ignored as zero. The thresholds are calculated\\nbased on the mixed error criteria [26]:\\nTG= max[ /epsilon1,/epsilon1·z(x,y)]\\nTλ= max[ /epsilon1,/epsilon1·/bardblW/bardbl∞] (9)\\nwhere /bardblW/bardbl∞= max[ |A|+|B|,|B|+|C|](see Equation4) and /epsilon1is a user-speciﬁed parameter. In our method, we\\nuse/epsilon1=0.001·s, where sis the average distance from each\\nvertex to the center of the individual 3D facial model.\\nThe classiﬁcation rule for twelve primitive surface labels\\nis expounded in Table 1. In general, if /bardbl∇z/bardbl<TGor there\\nis a zero crossing in the direction of the maximum curva-ture, one of the non-hillside labels is assigned; otherwise,\\none of the hill-side labels is assigned using the rule deﬁnedin the table. Figure 3(c) shows an example of the labeling\\nresult from a 3D facial expression range model.\\nλ1 λ2 Hillside Label Non-Hillside Label\\n|λ1|<Tλ |λ2|<Tλ ﬂat slope hill\\nλ1<−Tλ λ2<−Tλ peak convex hill\\nλ1<−Tλ |λ2|<Tλ ridge convex hill\\nλ1<−Tλ λ2>Tλ ridge saddle convex saddle hill\\nλ1>Tλ λ2<−Tλ ravine saddle concave hill\\nλ1>Tλ |λ2|<Tλ ravine convex hill\\nλ1>Tλ λ2>Tλ pit\\nλ1>Tλ λ2<−Tλ concave saddle hill\\nTable 1. Classiﬁcation rule of primitive 3D\\nsurface labels.\\n3. 3D Expression Description Based on Primi-\\ntive Label Distribution\\nAfter the labeling process, the facial expressions can be\\ndescribed by the distribution of the labels over the local orentire facial region. Intuitively, every facial expression is a\\nresult of facial muscle actuation, reﬂecting the facial surface\\nvariation. Such a variation results in the different distribu-tions of primitive surface labels. This fact suggests that the\\nprimitive label distribution could directly link to a distinct\\nfacial expression. In other words, the same type of facial\\nexpression is expected to share the similar primitive label\\ndistribution with a certain robustness, given a sufﬁcient res-olution of the range model.\\nTo ﬁnd an explicit representation of the fundamental\\nstructure of facial surface details, we investigate the statisti-cal distributions of the primitive surface labels in seven ex-\\npressive facial regions, which are deﬁned according to the\\nneuro-anatomy knowledge of conﬁguration of facial mus-\\ncles and their dynamics [20]. As shown in Figure 4, sixty-\\nfour ﬁducial points are deﬁned on the facial surface, and ac-cordingly, the seven expressive local regions are constructed\\nbased on these key points. Note that the interiors of mouth\\nand eyes are currently not included in the seven local re-\\ngions. The reasons are twofold: (1) the interiors of mouth\\nand eyes are isolated from the facial skin. They can be\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\n(b) (a)\\nFigure 4. 64facial ﬁducial points and 7se-\\nlected facial regions.\\ntreated as three separated objects. These “holes” are com-\\nplementary areas of the facial skin area. The change of their\\nshapes is directly reﬂected on the change of the surroundingskins which have been included in the seven expressive re-gions; (2) although tongues and eye-balls can be viewed asthe signatures of facial expressions, their appearances may\\nnot be as stable as the facial skin due to the dark hole effect\\nexisting in the current 3D imaging systems.\\nIn short, the selected seven local regions cover the most\\nexpressive areas on a human face. In each selected region,\\nwe identify the expression signature by calculating the his-togram distribution of primitive labels. Such a distributionis described as follows:\\nr\\ni=/bracketleftbiggni1\\nni,···,nim\\nni,···,niM\\nni/bracketrightbigg\\n(10)\\nwhere nimis the number of vertices which are labeled\\nby the label type Lm, andniis the total number of vertices\\nin the ith local region ( ni=M/summationtext\\nm=1nim).M=1 2 is the number\\nof the primitive label categories.\\nThe combination of seven histogram distributions of the\\nselected entire regions generates a unique expression de-\\nscriptor for a speciﬁc expression. As a result, the expres-\\nsion is described by the primitive surface feature distribu-tion (PSFD), which is expressed by\\nE=[r\\n1,···,ri,···,rK] (11)\\nwhere Kis the number of expressive regions ( K=7in our\\nexperiment).\\n4. Recognition Experiments\\n4.1. Database\\nWe constructed a 3D facial expression database for our\\nexperiment. The 3D range data is scanned by a 3DMD static\\ndigitizer [15], which uses a random light pattern projectionin the speckle projection ﬂash environment . The modelresolution is in the range of 20,000 polygons to 35,000\\npolygons, depending on the size of the face being scanned.The facial expressions with six universal emotional states,\\nAnger, Disgust, Fear, Happiness, Sadness and Surprise , are\\nsampled in four different levels of intensity (e.g., from lesspronounced to more pronounced). In our experiment, we\\nused the data captured from 60subjects with two high-\\nintensity models for each expression. The test is based onthe six prototypic expressions.\\nEach range facial model consists of a meshed surface\\nmodel and an associated texture image. The head pose\\ncan be estimated by a triangle plane determined by the in-\\nner corners of two eyes and a nose tip. Given the head\\npose, the original scan is processed by rotating the model\\nto the frontal view. This pre-processing results in a work-\\ning model , on which we manually labeled 64ﬁdicual ver-\\ntices for the expression recognition experiments. Table 2\\ngives a summary of the data set that we used for our exper-\\niments. A detail description of the database construction,\\npost-processing, and organization can be found in [29].\\n# of subjects # of expres-\\nsion types# of samples for each\\nsubject of each ex-\\npressiontotal # of sam-\\nples\\n60 6 2 720\\nTable 2. Summary of the 3D facial expression\\ndata set used in our experiment.\\n4.2. Recognition Results\\nOur facial expression recognition experiments are car-\\nried out in a person-independent manner, which is believed\\nto be more challenging than a person-dependent approach\\n[17]. We randomly partitioned the 60subjects into two sub-\\nsets: one with 54subjects for training and the other with\\n6subjects for test. The experimental paradigm guarantees\\nthat any subject used for testing does not appear in the train-ing set because the random partition is based on the subjectsrather than the individual expression. Four popular clas-\\nsiﬁers: Quadratic Discriminant Classiﬁer (QDC), Linear\\nDiscriminant Analysis (LDA), Naive Bayesian Classiﬁer\\n(NBC), and Support V ector Classiﬁer (SVC) with RBF ker-\\nnel are used in the experiments. The tests are executed 20\\ntimes on each classiﬁer with different partitions to achieve\\na stable generalization recognition rate. The entire process\\nguarantees that every subject is tested at least once for each\\nclassiﬁer. For each round of the test, all the classiﬁers are\\nreset and re-trained from the initial state.\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\nClassiﬁer QDC LDA NBC SVC\\nRecognition rate 74.5% 83.6% 71.7% 77.8%\\nTable 3. Results of person-independent ex-\\npression classiﬁcation using the 3D-PSFDmethod.\\nTable 3 shows the average correct recognition rates of\\nthe four classiﬁers. The LDA classiﬁer achieves the high-\\nest correct recognition rate with an accuracy of 83.6%. The\\nconfusion matrix of the average case for the LDA classi-\\nﬁer is shown in Table 4. The expressions of Happiness\\nand Surprise are well identiﬁed with accuracies of 95.0%\\nand90.8%, respectively. Anger ,Sadness ,Fear and Disgust\\nhave comparatively lower recognition rates. The misclassi-\\nﬁcation rate between Anger and Sadness is around 19.6%\\n(=8.3%+11.3%), while that between Fear with Happiness\\nis around 16.3%.\\nInput\\\\Output Anger Disgust Fear Happiness Sadness Surprise\\nAnger 80.0% 1.7% 6.3% 0.0% 11.3% 0.8%\\nDisgust 4.6% 80.4% 4.2% 3.8% 6.7% 0.4%\\nFear 0.0% 2.5% 75.0% 12.5% 7.9% 2.1%\\nHappiness 0.0% 0.8% 3.8% 95.0% 0.4% 0.0%\\nSadness 8.3% 2.5% 2.9% 0.0% 80.4% 5.8%\\nSurprise 1.7% 0.8% 1.2% 0.0% 5.4% 90.8%\\nTable 4. Confusion Matrix of the average case\\nof LDA classiﬁer for person-independent ex-\\npression recognition.\\n5. Comparison Study\\nIn this section, we compare the proposed 3D primitive\\nfeature distribution method (3D-PSFD) with two 2D ap-\\npearance feature based methods. One is the well-knownGabor-wavelet (GW) approach [16] and the other is our re-\\ncently developed Topographic Context (TC) approach [24].\\n(1) In the Gabor-wavelet (GW) based approach, a set of\\nmulti-scale and multi-orientation coefﬁcients are calculated\\nto describe the appearance variations in the facial region.\\nWe applied 6×3complex Gabor-wavelet ( 6orientations and\\n3spatial resolutions) on 34ﬁducial points. The coefﬁcients\\nof the real and imaginary parts can be computed as:\\nG+(x,y;ω,θ)=ω2\\nσ2·e−ω2(x2+y2)\\n2σ2·{cos[ω(xcosθ+ysinθ)]−e−σ2\\n2}\\nG−(x,y;ω,θ)=ω2\\nσ2·e−ω2(x2+y2)\\n2σ2·sin[ω(xcosθ+ysinθ)] (12)where we let σ=π. The three selected spatial scales\\nare{π/4,π/8,π/16}and the 6orientations selected are\\n{0,π/6,π/3,π/2,2π/3,5π/6}. The feature extracted at\\npixel(x,y)by a certain Gabor-wavelet kernel with parame-\\nters{ω,θ}is the amplitude of the real and imaginary coef-\\nﬁcients G=/radicalBig\\nG2\\n++G2−. Therefore in total, we extracted\\n18×34 = 612 wavelet features for each image.\\n(2) In our existing work, we developed a Topographic\\nContext (TC) based approach for facial expression recog-\\nnition based on 2D static images. The TC expression fea-\\ntures are the topographic primal sketch features inherent inthe2D facial images. We use such appearance features to\\ndescribe the distinct facial expressions. Similar to our 3D\\nbased approach, we used the feature statistics to representthe2D facial expressions. It has proved to be robust to fa-\\ncial landmark detection as a result of its intrinsic statisticsproperty (see details in [24]).\\nThe comparison study with the above two methods is\\nconducted under various head pose conditions, including\\ntwo cases: frontal view and non-frontal views.\\n5.1. Case 1: Frontal View\\nOur frontal-view images are generated from the texture-\\nmapped working models. All these images are normalized\\nto256×256 pixels. Several examples are shown in the\\ntop row of Figure 1. There are 60subjects with a total of\\n720 frontal-view facial images used for the test. The experi-\\nments are executed in a person-independent manner similar\\nto the strategy used in Section 4.2.\\nTable 5 reports the correct recognition rates using these\\ntwo methods. We found the Gabor-wavelet approach per-\\nforms poorly when using the SVC classiﬁer, which is not\\ncomparable to the results from the other three classiﬁers.\\nMethod \\\\Classiﬁer QDC LDA NBC\\nTopographic context method 73.8% 79.2% 70.9%\\nGabor-wavelets method 72.3% 74.1% 62.1%\\nTable 5. Results of person-independent ex-\\npression classiﬁcation using GW and TC.\\nComparing to the performance shown in Table 3, the\\n3D-PSFD method is superior to the 2D appearance feature\\nbased methods when classifying the six prototypic facial ex-\\npressions.\\n5.2. Case 2: Non-frontal View\\nIn this section, we compare the performance of our 3D\\ngeometric based approach (PSFD) with the 2D appearance\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\nfeature based approaches under different head pose condi-\\ntions. We obtained the head pose information from the orig-\\ninal face scans by estimating the triangle face-plane formed\\nby the inner corners of two eyes and the tip of nose. Know-\\ning the 3D head pose, we are able to rotate the 3D model to a\\nfrontal view position or arbitrary pose positions without los-\\ning any geometric feature information. However, if viewedin the 2D projection plane, the appearance of 2D facial im-\\nages varies dramatically when the head pose is arbitrarilychanged. The difﬁculty to recover the missing appearanceinformation from 2D images makes 2D based facial expres-\\nsion recognition sensitive to head pose variations.\\nUsing our face range models, we have generated facial\\nexpression images under different views, corresponding todifferent head poses. This is done by rotating the models to\\na certain degree and generating the face images in that view\\nby texture-mapping. We generated the face images underviews of ±10\\n◦,±20◦,±30◦, and±40◦for each orienta-\\ntion (pitch and yaw rotation), resulting in 720 images for\\neach view. Figure 5 shows the examples of different facial\\nappearances under different views.\\nFigure 5. Facial expression images with dif-\\nferent head rotation. Top row is yaw rotation\\nand bottom row is pitch rotation. From left to\\nright, the rotation angle is −40◦,−30◦,−20◦,\\n20◦,30◦and40◦.\\nWe choose to use the LDA classiﬁer to evaluate the\\nexpression recognition performance under different head\\nposes because in most cases LDA outputs the best recog-\\nnition result. For all three algorithms (PSFD, GW, and TC),\\nthe LDA classiﬁer is trained using the front view face data.\\nThe recognition results with respect to different poses\\n(e.g., pitch and yaw rotations) are shown in Figure 6. From\\nthe ﬁgure, we can see that the average recognition rates\\nof the 2D appearance feature based methods degrade con-\\nsiderably as head rotation increases, especially the Gabor-wavelet method. The extraction of Gabor-wavelet coefﬁ-\\ncients is based on the selected ﬁducial points. The posechange alters the distribution of these ﬁducial points, and re-\\nsults in signiﬁcant distortion of Gabor-wavelet features. On\\nthe contrary, the 3D-PSFD based approach makes the 3D\\nprimitive feature distribution invariant to the pose variations\\nbecause the 3D geometric features are view-independent.\\nFigure 6. Comparison of the recognition per-\\nformance under different head orientations.\\nleft: pitch; right: yaw.\\n6. Concluding Remarks\\nIn this paper, we investigated the issue of 3D facial ex-\\npression representation and recognition. To the best of our\\nknowledge, this is the ﬁrst attempt to recognize facial ex-\\npressions using range data in a complete 3D space. We\\nhave proposed to extract and label the primitive 3D surface\\nfeatures, and derive their statistical distributions to repre-\\nsent the distinct prototypic facial expressions. We used the\\nprimitive surface feature distribution (PSFD) as the signa-\\nture to distinguish facial expressions, and conducted expres-sion recognition experiments using the person-independentstrategy. Compared to the existing 2D static image based\\napproaches (e.g., GW and TC methods), our 3D range data\\nbased approach shows superior performance as a result ofthe lighting and orientation invariance of 3D geometric fea-\\ntures. The experiments show encouraging results in recog-nizing six prototypic facial expressions under various head-\\npose conditions.\\nThere are some limitations in the current work:\\n(1)Our current data set contains only static expression\\nmodels. A database including dynamic 3D facial expres-\\nsion sequences is needed in order to study the subtle skin\\nmovement associated with facial expressions in 3D space.\\nWith the emergence of dynamic 3D imaging systems [25],\\nit is possible to investigate the feasibility of tracking action\\nunits in 3D space to further enhance the current FACS based\\nfacial expression recognition technique. Because the dis-\\ncriminability of 3D facial expressions is dependent on the\\nresolution of the 3D mesh, higher resolution models are re-\\nquired in order to improve the recognition performance.\\n(2) The current work involves the pre-processing of\\nrange data which requires manual selection of the surfaceﬁducial points. In order to realize an automatic system for3D facial expression analysis, algorithms for automatic de-tection of 3D surface features must be developed. The exist-\\ning approaches (e.g., free-form based [25], etc.) are promis-\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\ning for this purpose.\\nThe above issues give rise to our future research direc-\\ntions in order to improve facial expression analysis in the\\n3D space. In addition, we will investigate integrating 3D\\ngeometric shape and 2D texture information to improve our\\ncurrent approach.\\n7. Acknowledgement\\nThis material is based upon the work supported in part by\\nthe National Science Foundation under grants IIS-0541044,IIS-0414029, and the NYSTAR’s James D. Watson Investi-gator Program.\\nReferences\\n[1] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I.Fasel,\\nand J. Movellan. Recognizing facial expressions: machine\\nlearning and application to spontaneous behavior. In IEEE\\nCVPR2005 , San Diego, CA. 2005.\\n[2] B. Braathen, M. Bartlett, G. Littlewort, E. Smith, and\\nJ. Movellan. An approach to automatic recognition of spon-taneous facial actions. In Proc. of Int. Conf. on FGR , 2002.\\n[3] Y . Chang, C. Hu, and M. Turk. Probabilistic expression anal-\\nysis on manifolds. In CVPR , Washington DC, USA, 2004.\\n[4] I. Cohen, N. Sebe, A. Garg, L. S. Chen, and T. S. Huang. Fa-\\ncial expression recognition from video sequences: temporal\\nand static modeling. CVIU , 91:160–187, 2003.\\n[5] R. Cowie, , E. Douglas-Cowie, N. Tsapatsoulis, G. V otsis,\\nS. Kollias, W. Fellenz, and J. Taylor. Emotion recognition in\\nhuman computer interaction. IEEE Signal Processing Mag-\\nazine , 18(1):32–80, 2001.\\n[6] G. Donato, M. Bartlett, J. Hager, P . Ekman, and T. Se-\\njnowski. Classifying facial actions. IEEE Trans. on PAMI ,\\n21:974–989, 1999.\\n[7] C. Dorai and A. Jain. Cosmos - a representation scheme for\\n3d free-form objects. IEEE Trans. on PAMI , 19(10):1115–\\n1130, 1997.\\n[8] P . Ekman and W. Friesen, editors. The facial action coding\\nsystem: a technique for the measurement of facial move-ment . Consulting Psychologists Press, San Francisco, 1978.\\n[9] P . Ekman, T. Huang, T. Sejnowski, and J. Hager. Final re-\\nport to NSF of the planning workshop on facial expressionunderstanding . Human Interaction Lab., UC at San Fran-\\ncisco, 1993.\\n[10] I. Essa and A. Pentland. Coding, analysis, interpretation,\\nand recognition of facial expressions. IEEE Trans. on PAMI ,\\n19:757–763, 1997.\\n[11] G. Farin. Curves and Surfaces for Computer Aided Geomet-\\nric Design . 5th ed., Morgan-Kaufmann, 2001.\\n[12] B. Fasel and J. Luttin. Automatic facial expression analysis:\\nSurvey. Pattern Recognition , 36:259–275, 2003.\\n[13] S. Gokturk, J. Bouguet, C. Tomasi, and B. Girod. Model-\\nbased face tracking for view-independent facial expressionrecognition. In Proc. of Int. Conf. on FGR , 2002.[14] J. Goldfeather and V . Interrante. A novel cubic-order algo-\\nrithm for approximating principal direction vectors. ACM\\nTrans. on Graphics , 23:45–63, 2004.\\n[15] Inc.3dMd. http://www.3q.com . 2005.\\n[16] M. Lyons, J. Budynek, and S. Akamatsu. Automatic clas-\\nsiﬁcation of single facial images. IEEE Trans. on PAMI ,\\n21:1357–1362, 1999.\\n[17] M. Pantic and L. Rothkrantz. Automatic analysis of facial\\nexpressions: the state of the art. IEEE Trans. on PAMI ,\\n22:1424–1445, 2000.\\n[18] M. Pantic and L. Rothkrantz. Facial action recognition for\\nfacial expression analysis from static face images. IEEE\\nTrans. on SMC-Part B: Cybernetics , 34:1449–1461, 2004.\\n[19] P . Phillips, P . Flynn, T. Scruggs, K. Bowyer, J. Chang,\\nK. Hoffman, J. Marques, J. Min, and W. Worek. Overviewof the face recognition grand challenge. In IEEE Conf. on\\nCVPR, San Diego, CA , 2005.\\n[20] W. Rinn. The neuropsychology of facial expression: A\\nreview of the neurological and psychological mechanismsfor producing facial expressions. Psychological Bulletin ,\\n95:52–77, 1984.\\n[21] H. Tanaka, M. Ikeda, and H. Chiaki. Curvature-based face\\nsurface recognition using spherical correlation. In IEEE\\nConf. on FGR , pages 372–377, 1998.\\n[22] Y . Tian, T. Kanade, and J. Cohn. Recognizing action units\\nfor facial expression analysis. IEEE Trans. on PAMI , 23:1–\\n9, 2001.\\n[23] O. Trier, T. Taxt, and A. Jain. Data capture from maps based\\non gray scale topographic analysis. In The Third Interna-\\ntional Conference on Document Analysis and Recognition ,\\nMontreal, Canada, 1995.\\n[24] J. Wang and L. Yin. Facial expression representation and\\nrecognition from static images using topographic context. InTechnical Report, Department of Computer Science, SUNYat Binghamton , Nov., 2005.\\n[25] Y . Wang, X. Huang, C. Lee, S. Zhang, Z. Li, D. Samaras,\\nD. Metaxas, A. Elgammal, and P . Huang. High resolutionacquisition, learning and transfer of dynamic 3d facial ex-pressions. In EUROGRAPHICS 2004 , 2004.\\n[26] L. T. Watson, T. J. Laffey, and R. M. Haralick. Topographic\\nclassiﬁcation of digital image intensity surfaces using gener-\\nalized splines and the discrete cosine transform. Computer\\nVision, Graphics and Image Processing , 29:143–167, 1985.\\n[27] Z. Wen and T. Huang. Capturing subtle facial motions in 3d\\nface tracking. In ICCV , 2003.\\n[28] Y . Yacoob and L. Davis. Recognizing human facial expres-\\nsion from long image sequences using optical ﬂow. IEEE\\nTrans. on PAMI , 16:636–642, 1996.\\n[29] L. Yin, X. Wei, Y . Sun, J. Wang, and M. Rosato. A 3d facial\\nexpression database for facial behavior research. In 7th Int.\\nConf. on FGR , Southampton, UK, 2006.\\n[30] L. Zalewski and S. Gong. Synthesis and recognition of facial\\nexpressions in virtual 3d views. In IEEE 6th Inter. Conf. on\\nFGR , 2004.\\n[31] Y . Zhang and Q. Ji. Active and dynamic information fusion\\nfor facial expression understanding from image sequences.\\nIEEE Trans. on PAMI , 27(5):699–714, May 2005.\\n[32] W. Zhao, R. Chellappa, P . Phillips, and A. Rosenfeld. Face\\nrecognition: A literature survey. ACM Computing Surveys ,\\n35(4), Dec. 2003.\\nProceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06) \\n0-7695-2597-0/06 $20.00 © 2006  IEEE \\n\\n',\n",
       " \"3DFacial Expr ession Recognition Based onProperties ofLine Segments\\nConnecting Facial Featur ePoints\\nHao TangandThomas S.Huang\\x03\\nBeckman Institute forAdvanced Science andTechnology\\nCoordinated Science Laboratory\\nDepartment ofElectrical andComputer Engineering\\nUniversity ofIllinois atUrbana-Champaign, Urbana, IL61801 U.S.A.\\nfhaotang2,huang g@ifp.uiuc.edu\\nAbstract\\nThe 3Dfacial geometry contains ample information\\nabout human facial expressions. Suchinformation isinvari-\\nanttopose and lighting conditions, whic hhave imposed\\nserious hurdles onmany 2Dfacial analysis problems. In\\nthispaper ,weperform personandgender independent fa-\\ncialexpression recognition based onproperties oftheline\\nsegments connecting certain 3Dfacial featur epoints. The\\nnormalized distances andslopes ofthese linesegments com-\\nprise asetof96distinguishing featur esforrecognizing six\\nuniver salfacial expressions, namely anger,disgust, fear,\\nhappiness, sadness, andsurprise .Using amulti-class sup-\\nport vector machine (SVM) classi\\x02er ,an87.1% average\\nrecognition rateisachievedonthepublicly available 3D\\nfacial expression database BU-3DFE [1]. Thehighest av-\\neragerecognition rateobtained inourexperiments is99.2%\\nfortherecognition ofsurprise .Our result outperforms the\\nresult reported intheprior work [2],whic huses elabor ately\\nextracted primitive facial surface featur esandanLDAclas-\\nsi\\x02er andwhic hyields anaveragerecognition rateof83.6%\\nonthesame database .\\n1.Introduction\\nThe3Dgeometry ofthefaceplays animportant rolein\\nrecognizing theidentity andexpression ofaperson [1\\x965].\\nThe3Dfacial geometry contains ample information about\\nhuman facial expressions. Such information isinvariant to\\npose andlighting conditions, which haveimposed serious\\nhurdles onmany2Dfacial analysis problems including the\\nfacial expression recognition problem [6\\x968].Itisthus ad-\\nvantageous toexploit theavailable 3Dfacial geometry in-\\n\\x03This workwassupported inpartbytheU.S. Government VACEPro-\\ngram.formation totackle thevarious problems offacial analy-\\nsis. While the3Dfacial geometry hasbeen relati velyex-\\ntensivelyexplored andutilized inthestudy offacerecog-\\nnition [9,10]thanks tothefast-de veloping andincreasingly\\nmature 3Dscanning products and3Dfacereconstruction\\ntechniques, itishoweverfarlessused forfacial expression\\nrecognition. The most obvious reason forthisembarrass-\\ningsituation isclearly thatthere haslong been alack ofa\\npublicly available 3Dfacial expression database toserveas\\nacommon platform fortheresearchers inthisarea. With-\\noutsuch adatabase, neither cantheresearchers start with\\nproblem investigation norcantheyendincomparing their\\nresults. Nowthegood newsis,Yinet.al.atBinghamton\\nUniversity haverecently constructed a3Dfacial expression\\ndatabase forfacial beha vior research andmade itpublicly\\navailable [1]. This isde\\x02nitely the\\x02rstattempt tomakea\\npublicly available 3Dfacial expression database forthere-\\nsearch community .Itisbelie vedthatthisdatabase willun-\\ndoubtedly haveagreat impact onthefacial beha viorrelated\\nresearch inthenextafewyears.\\nWanget.al.[2]conducted the\\x02rst workof3Dfacial\\nexpression recognition ontheBU-3DFE database. They\\nreported thatthehighest average recognition ratetheyob-\\ntained onthisdatabase was83.6% byusing elaborately ex-\\ntracted primiti vefacial surfacefeatures andanLDAclas-\\nsi\\x02er .Theyalso reported that thefacial expressions of\\nhappiness andsurprise were well identi\\x02ed with accuracies\\nof95% and90.8%, respecti vely.Theycompared their re-\\nsults with theresults obtained bytwo2Dappearance feature\\nbased methods, namely theGabor -wavelet(GW) approach\\nandtheTopographic Conte xt(TC) approach. Inboth cases,\\ntheyshowedthattheir results outperformed those forthe2D\\nmethods. This ishighly expectable because 3Dfacial geom-\\netrycontains richinformation about facial expressions and\\nismore robustthan 2Dappearances.\\nInthispaper ,wefurther perform person andgender in-\\n978-1-4244-215 4-1/08/$25.00c\\r2008 IEEE\\nFigure 1.Some examples inBU-3DFE database.\\ndependent facial expression recognition ontheBU-3DFE\\ndatabase. Ourmethod isbased onproperties ofthelineseg-\\nments connecting certain 3Dfacial feature points. Thenor-\\nmalized distances andslopes ofthese linesegments com-\\nprise asetof96distinguishing features forrecognizing six\\nuniversal facial expressions, namely anger ,disgust, fear,\\nhappiness, sadness, andsurprise. Using amulti-class sup-\\nportvector machine (SVM) classi\\x02er ,weachie vean87.1%\\naverage recognition rate. The highest average recognition\\nrateobtained is99.2% fortherecognition ofsurprise.\\n2.Database description\\nTheBU-3DFE database wasrecently developed byYin\\net.al.atBinghamton University .Itwasdesigned tosample\\n3Dfacial beha viors with different prototypical emotional\\nstates atvarious levelsofintensities. Inthedatabase, there\\nareatotal of100subjects, among which 56arefemale and\\n44aremale. These subjects arewell distrib uted across dif-\\nferent ethnic orracial ancestries, including White, Black,\\nEast-Asian, Middle-East Asian, Hispanic Latino, andoth-\\ners. While being recorded, each subject wasaskedtoper-\\nform theneutral facial expression aswell assixuniversalfacial expressions, namely anger (AN), disgust (DI), fear\\n(FE), happiness (HA), sadness (SA), and surprise (SU).\\nEach facial expression hasfour different levelsofintensi-\\nties(low,middle, high, highest), except thattheneutral fa-\\ncialexpression hasonly oneintensity level.Thus, there are\\n253Dfacial expression models foreach subject, resulting\\nin2500 3Dfacial expression models inthedatabase.\\nAssociated with each 3Dfacial expression model area\\nraw3Dfacemesh model, acropped 3Dfacemesh model,\\napairoftexture images with two-angle views(about +45o\\nand-45oawayfrom thefacefrontal normal), afrontal-vie w\\ntexture image, asetof83facial feature points, andafa-\\ncialpose vector .These data giveacomplete 3Ddescription\\nofafaceunder aspeci\\x02c facial expression. Adetailed de-\\nscription oftheBU-3DFE database canbefound in[1].In\\nthispaper ,webase ourmethod solely onthe83facial fea-\\nturepoints mark edonthecropped 3Dfacemesh model, as\\nshowninFigure 2.Wehereafter refer toacropped 3Dface\\nmesh model asa3Dfacial expression model. Some exam-\\nples ofthe3Dfacial expression models intheBU-3DFE\\ndatabase areillustrated inFigure 1.\\nFigure 2.The83facial feature points.\\n3.Featur eextraction\\nInthispaper ,wecarefully devise asetoffeatures based\\nonproperties ofthelinesegments connecting certain facial\\nfeature points onthe3Dfacial expression models. Accord-\\ningtoEkman' sfacial action coding system (FACS)[11,12],\\nthesixuniversal facial expressions canbeencoded byac-\\ntion units, which describe themovements ofparticular fa-\\ncialfeature points. Similarly ,intheMPEG-4 standard [13],\\nthesixfacial expressions arede\\x02ned byfacial animation\\nparameters (FAPs), which specify precisely howmuch the\\nfacial feature points havetobemoved.Inlight ofthis, we\\nbelie vethatcertain properties ofthelinesegments connect-\\ningfacial feature points offerasparse, compact, anduseful\\nrepresentation ofthe3Dfacial geometry .\\nWetherefore extract asetof96features, which consist\\nofthenormalized distances andslopes ofthelinesegments\\nconnecting asubset ofthe83facial feature points. These\\nfeatures arebelie vedtoplay animportant roleindetermin-\\ningthevarious facial expressions. The features arepicto-\\nrially showninFigure 3andtheir textual descriptions are\\ngiveninTable 1.\\nSince thefaces ofdifferent people havedifferent sizes\\nand proportions, inorder tomakethefeatures person-\\nindependent, wenormalize thedistance features byfa-\\ncialanimation parameter units (FAPUs), asguided bythe\\nMPEG-4 standard. InMPEG-4, theFAPUs servetoscale\\ntheFAPs inorder thatasingle setofFAPs canbeused with\\narbitrary facemodels. TheFAPUs arede\\x02ned asfractions\\nofdistances between certain feature points onafacemodel\\ninitsneutral state andallowtointerpret theFAPs onar-\\nbitrary facemodels inaconsistent way.The \\x02veFAPUs\\nareshowninFigure 4.The distance features aredivided\\nbythecorresponding FAPUs, givenastheunitinTable 1.\\nInaddition, theslope features (each ofwhich being a3-\\ndimensional vector) aredivided bytheir respecti venorms,\\nresulting inunitvectors.\\n11\\n13\\n1214\\n23246\\n41 2\\n7 8\\n15169101920212218 173\\n5\\n2825 26\\n2930\\n3940\\n46 4527\\n3132333435363837\\n4142\\n4344\\n4748\\n(a) (b)\\nFigure 3.(a)Thedistance features. (b)Theslope features.\\nFigure 4.Afacemodel initsneutral state onwhich theFAPUs are\\nde\\x02ned bythefractions ofdistances between themark edkeyfacial\\nfeature points. The\\x02veFAPUs areIRISD0 -Irisdiameter ,ES0 -\\nEyeseparation, ENS0 -Eye-nose separation, MNS0 -Mouth-nose\\nseparation, andMW0 -Mouth width, respecti vely[13].\\n4.Multi-class SVM classi\\x02cation\\nThe support vector machine (SVM) isasuccessful su-\\npervised learning technique forclassi\\x02cation andregression\\n[14,15]. Inabinary classi\\x02cation scenario, givenasetof\\nNtraining feature vectors fxngN\\nn=1andtheir correspond-\\ningclass labelsyT=[y1;y2;:::;yN]where yn2f\\x001;1g,\\nn=1;2;:::;N,theSVM makespredictions based ona\\nfunction oftheform\\ny(x;w)=NX\\nn=1wnK(x;xn)+b (1)\\nwherewT=[w1;w2;:::;wN]aretheweights andK(\\x01;\\x01)\\nisakernel function. The weights aredetermined bymin-\\nFeature Textual description Property Number ofvalues Unit\\n1 length ofright eyebro w distance 1 ES0\\n2 length oflefteyebro w distance 1 ES0\\n3 distance between leftandright eyebro ws distance 1 ES0\\n4 distance between leftandright eyes distance 1 ES0\\n5 distance between right inner eyebro wandright inner eyecorner distance 1 ENS0\\n6 distance between leftinner eyebro wandleftinner eyecorner distance 1 ENS0\\n7 distance between right outer eyebro wandright outer eyecorner distance 1 ENS0\\n8 distance between leftouter eyebro wandleftouter eyecorner distance 1 ENS0\\n9 distance between right inner eyecorner andright nose wing distance 1 ES0\\n10 distance between leftinner eyecorner andleftnose wing distance 1 ES0\\n11 width ofright eye distance 1 ES0\\n12 width oflefteye distance 1 ES0\\n13 height ofright eye distance 1 IRISD0\\n14 height oflefteye distance 1 IRISD0\\n15 distance between right outer eyecorner andright outer mouth corner distance 1 ENS0\\n16 distance between leftouter eyecorner andleftouter mouth corner distance 1 ENS0\\n17 distance between right outer mouth corner andright jawroot distance 1 MNS0\\n18 distance between leftouter mouth corner andleftjawroot distance 1 MNS0\\n19 distance between right nostril andupper midlip distance 1 MNS0\\n20 distance between leftnostril andupper midlip distance 1 MNS0\\n21 distance between right nostril andchin distance 1 MNS0\\n22 distance between leftnostril andchin distance 1 MNS0\\n23 width ofmouth distance 1 MW0\\n24 height ofmouth distance 1 MNS0\\n25 slope ofright eyebro w slope 3 -\\n26 slope oflefteyebro w slope 3 -\\n27 slope between right inner eyebro wandright inner eyecorner slope 3 -\\n28 slope between leftinner eyebro wandleftinner eyecorner slope 3 -\\n29 slope between right outer eyebro wandright outer eyecorner slope 3 -\\n30 slope between leftouter eyebro wandleftouter eyecorner slope 3 -\\n31 slope between right outer eyecorner andright upper mideyelid slope 3 -\\n32 slope between right inner eyecorner andright upper mideyelid slope 3 -\\n33 slope between leftouter eyecorner andleftupper mideyelid slope 3 -\\n34 slope between leftinner eyecorner andleftupper mideyelid slope 3 -\\n35 slope between right outer eyecorner andright lowermideyelid slope 3 -\\n36 slope between right inner eyecorner andright lowermideyelid slope 3 -\\n37 slope between leftouter eyecorner andleftlowermideyelid slope 3 -\\n38 slope between leftinner eyecorner andleftlowermideyelid slope 3 -\\n39 slope between right outer eyecorner andright outer mouth corner slope 3 -\\n40 slope between leftouter eyecorner andleftouter mouth corner slope 3 -\\n41 slope between right outer mouth corner andupper midlip slope 3 -\\n42 slope between leftouter mouth corner andupper midlip slope 3 -\\n43 slope between right outer mouth corner andlowermidlip slope 3 -\\n44 slope between leftouter mouth corner andlowermidlip slope 3 -\\n45 slope between right outer mouth corner andright jawroot slope 3 -\\n46 slope between leftouter mouth corner andleftjawroot slope 3 -\\n47 slope between right jawrootandchin slope 3 -\\n48 slope between leftjawrootandchin slope 3 -\\nTable 1.The normalized distances andslopes between certain facial feature points onthe3Dfacial expression model comprise a96-\\ndimension feature vector .Theunits ofthedistance features aregivenbytheMPEG-4 facial animation parameter units (FAPUs). Theslope\\nfeatures arenormalized bytheir respecti vevector norms.\\nimizing theerror onthetraining setwhile maximizing the\\nmarginbetween thetwoclasses inthefeature space implic-\\nitlyde\\x02ned bythekernel function. The optional constant\\ntermbindicates theuseofabias intheformulation. The\\nkeyfeature andnice property oftheSVM isthatasanat-\\nuralresult oftheeffective\\x93prior\\x94 (maximizing themargin)\\nmanyoftheweights inEquation 1turnouttobezero. This\\nleads toasparse model represented bythetraining feature\\nvectors corresponding tothenon-zero weights (those lieon\\nthemarginoronthewrong side ofit).These training fea-\\nturevectors arecalled thesupport vectors. Thesparseness\\nofamodel isanappealing feature thatisextremely helpful\\nforcontrolling themodel comple xity toavoidover-\\x02tting\\nandtoreduce thecomputational time.\\nThe SVM training seeks to\\x02nd thesolution tothefol-\\nlowing optimization problem (primal form):\\nmin\\nw;b;\\x181\\n2wTw+CPN\\nn=1\\x18n (2)\\nsubject to yn(wT\\x1e(xn)+b)\\x151\\x00\\x18n\\n\\x18n\\x150;n=1;2;:::;N\\nwhose dual is\\nmin\\x0b1\\n2\\x0bTQ\\x0b\\x00eT\\x0b (3)\\nsubject to yT\\x0b=0\\n0\\x14\\x0bn\\x14C;n=1;2;:::;N\\nwhereeT=[1;1;:::;1],C>0istheerror/mar gintrade-\\noffparameter ,QisanN\\x02Nmatrix with elements Qij=\\nyiyjK(xi;xj),andK(xi;xj)=\\x1e(xi)T\\x1e(xj)isthe\\nkernel function. Through thekernel function, thetraining\\nfeature vectorsxnaremapped into ahigher orin\\x02nite di-\\nmensional space inwhich theSVM seeks to\\x02nd alinear\\nseparating hyperplane with themaximum marginbetween\\nthetwoclasses. Averyuseful kernel function istheradial\\nbasis function (RBF)\\nK(xi;xj)=e\\x00\\rkxi\\x00xjk2(4)\\nwhere \\risthekernel parameter .\\nThedecision function ofSVM classi\\x02cation isgivenby\\ny(x)=sgn NX\\nn=1yn\\x0bnK(x;xn)+b!\\n(5)\\nTheSVM initsbasic form (Equation 5)isabinary clas-\\nsi\\x02er .Weconstruct amulti-class SVM classi\\x02er using the\\n\\x93one-against-one\\x94 approach. Inthecase ofK-class classi\\x02-\\ncation, anSVM model islearned foreach ofthe1\\n2K(K\\x001)\\nunique pairs ofclasses. Givenanovelinput feature vectorx\\ntobeclassi\\x02ed, wetestitagainst allthelearned SVM mod-\\nelsyij(x),i;j=1;2;:::;K,i6=j.Thefollo wing votingstrate gyisadopted\\nvote(i)=vote(i)+1 ifyij(x)\\x140 (6)\\nvote(j)=vote(j)+1ifyij(x)>0\\nTheclass which gains themajority voteisthen thepre-\\ndicted class label^k:\\n^k=argmax\\n0\\x14k\\x14Kv(k) (7)\\n5.Experiments\\nWeperformed extensi vefacial expression recognition\\nexperiments ontheBU-3DFE database. The setup ofour\\nexperiments isasfollo ws.First, werandomly chose asub-\\nsetofthedatabase containing 60subjects ofwhich 30are\\nfemale and30aremale. The purpose ofthisistoensure\\ngender independenc y.Then, werandomly partitioned the\\n60subjects intotwoseparate sets. Thetraining setcontains\\n54subjects andthetestsetcontains 6subjects. Inallexper-\\niments, weused thedata captured from these 60subjects\\nwith twohigh intensity models foreach facial expression.\\nForeach facial expression model inthetraining andtest\\nsets, weextracted a96dimensional feature vector asde-\\nscribed inSection 3.Inthiswork,theneutral facial expres-\\nsion isnotclassi\\x02ed. Rather ,asapreprocessing stepitsfea-\\ntures serveas\\x02ducial measures thataresubtracted from the\\nfeatures ofthesixuniversal facial expressions ofthecorre-\\nsponding subject. Thetraining feature vectors were used to\\ntrain amulti-class SVM classi\\x02er with aC++ implementa-\\ntionoftheSVM: theLibSVM softw are[16]. Thetestfea-\\nture vectors were evaluated against thetrained SVM clas-\\nsi\\x02er toproduce theclassi\\x02cation results. Byrunning the\\nexperiment independently for10times on10random par-\\ntitions ofthetraining andtestsets, weachie vedanaverage\\nrecognition rateof87.1% forthesixuniversal facial expres-\\nsions, which isbelie vedtobeageneralization oftherecog-\\nnition rate. Anumerical comparison ofourresult with the\\nresults reported in[2]isgiveninTable 2.Weseethatthere\\nisa3.5% absolute increase intheaverage recognition rate\\ninourworkascompared tothehighest average recognition\\nrateobtained in[2],which used elaborately extracted prim-\\nitivefacial surfacefeatures andanLDAclassi\\x02er .Table 3\\ngivestheaverage confusion matrix ofourresults andTa-\\nble4givestheaverage confusion matrix ofthebest results\\nin[2](with anLDAclassi\\x02er). Figure 5givesapictorial\\ncomparison oftheexpression-speci\\x02c average recognition\\nrates. From Figure 5,itcanbeseen thatallourresults are\\nbetter than theresults in[2]except forthefear (FE) case.\\nEspecially ,forthecase ofsurprise (SU), ouraverage recog-\\nnition ratereaches to99.2%. Inthecase offear (FE), our\\nresult isslightly worse than thatin[2]-a0.8% decrease.\\nTheresults in[2] Ourresult\\nQDC LDA NBC SVC SVM\\n74.5% 83.6% 71.7% 77.8% 87.1%\\nTable 2.Comparison oftheaverage recognition rates.\\n\\x01% AN DI FE HA SA SU\\nAN 86.7 1.7 2.5 0 9.2 0\\nDI 3.3 84.2 5.8 3.3 0.8 2.5\\nFE 5.8 5 74.2 5.8 6.7 2.5\\nHA 0 0 4.2 95.8 0 0\\nSA 12.5 0 5 0 82.5 0\\nSU 0 0 0.8 0 0 99.2\\nTable 3.Average confusion matrix ofSVM classi\\x02cation.\\n\\x01% AN DI FE HA SA SU\\nAN 80 1.7 6.3 0 11.3 0.8\\nDI 4.6 80.4 4.2 3.8 6.7 0.4\\nFE 0 2.5 75 12.5 7.9 2.1\\nHA 0 0.8 3.8 95 0.4 0\\nSA 8.3 2.5 2.9 0 80.4 5.8\\nSU 1.7 0.8 1.2 0 5.4 90.8\\nTable 4.Average confusion matrix ofLDAclassi\\x02cation in[2].\\n65 70 75 80 85 90 95 100ANDIFEHASASU\\nAver age recognition rate (% )Expressio nCompari son of expression!specific reco gnition rate s\\n  \\nWan g et. al.'s result s\\nOur re sults\\nFigure 5.Comparison ofexpression-speci\\x02c recognition rates.\\n6.Conclusion\\nInthis paper ,person and gender independent facial\\nexpression recognition isperformed ontheBU-3DFE\\ndatabase. Wepropose a3Dfacial expression recognition\\nmethod based onproperties ofthelinesegments connect-\\ningcertain facial feature points. Thenormalized distances\\nandslopes ofthese linesegments comprise asetof96dis-\\ntinguishing features forrecognizing sixuniversal facial ex-pressions. Using amulti-class SVM classi\\x02er ,weachie ve\\nan87.1% average recognition rate. The highest average\\nrecognition rate obtained is99.2% fortherecognition of\\nsurprise. Ourresult yields a3.5% absolute increase inthe\\naverage recognition rate ascompared tothat intheprior\\nwork[2].\\nRefer ences\\n[1]Lijun Yin,Xiaozhou Wei,YiSun, JunWang, andMatthe w\\nRosato, \\x93A3DFacial Expression Database ForFacial Be-\\nhaviorResearch\\x94, FG2006, pp.211-216.\\n[2]JunWang, Lijun Yin,Xiaozhou Wei,andYiSun, \\x933D Facial\\nExpression Recognition Based onPrimiti veSurfaceFeature\\nDistrib ution\\x94, CVPR 2006.\\n[3]P.Jonathon Phillips, Patrick J.Flynn, ToddScruggs, Kevin\\nW.Bowyer ,JinChang, KevinHoffman, JoeMarques, Jaesik\\nMin, andWilliam Worek, \\x93Overvie woftheFaceRecogni-\\ntionGrand Challenge\\x94, CVPR 2005, pp.947-954.\\n[4]Lijun Yinand Xiaozhou Wei,\\x93Multi-Scale Primal Fea-\\ntureBased Facial Expression Modeling andIdenti\\x02cation\\x94,\\nFG2006, pp.603-608.\\n[5]L.Yin,X.Wei,P.Longo, andA.Bhuv anesh, \\x93Analyzing Fa-\\ncialExpressions Using Intensity-V ariant 3DData forHuman\\nComputer Interaction, \\x94ICPR 2006, pp.1248-1251.\\n[6]M.Pantic andL.J.M. Rothkrantz, \\x93Automatic analysis offa-\\ncialexpressions: thestate oftheart\\x94, T-PAMI, 22(12):1424-\\n1445, 2000.\\n[7]Cowie, R.,Douglas-Co wie, E.,Tsapatsoulis, N.,Votsis, G.,\\nKollias, S.,Fellenz, W.,&Taylor ,J.Emotion recognition in\\nhuman-computer interaction. IEEE Signal Processing Mag-\\nazine: 18(1),32-80, 2001.\\n[8]Fasel, B.,Luettin, J.,Automatic facial expression analysis:\\nAsurvey.Pattern Recognition 36(2003) 259-275\\n[9]Bronstein, A.M.,Bronstein, M.M, andKimmel, R.,\\x93Three-\\ndimensional facerecognition\\x94, IJCV ,64(1):5-30, 2005.\\n[10] Kakadiaris, I.A., Passalis, G.,Toderici, G.,Murtuza, N.,\\nKarampatziakis, N.,andTheoharis, T.,\\x933D facerecogni-\\ntioninthepresence offacial expressions: anannotated de-\\nformable model approach\\x94, T-PAMI, 13(12), 2007.\\n[11] P.Ekman andW.Friesen, \\x93Facial Action Coding System: A\\nTechnique fortheMeasurement ofFacial Movement\\x94, Con-\\nsulting Psychologists Press, PaloAlto, 1978.\\n[12] Ekman, P.,Huang, T.,Sejno wski, T.,andHager ,J.,\\x93Final\\nReport toNSF ofthePlanning Workshop onFacial Expres-\\nsion Understanding\\x94, Human Interaction Lab, University of\\nCalifornia, SanFrancisco, CA.\\n[13] Igor S.Pandzic, Robert Forchheimer (Eds), MPEG-4 Facial\\nAnimation: TheStandard, Implementation andApplications,\\nJohn Wiley&Sons, Inc., 2002.\\n[14] B.E.Boser ,I.M.Guyon, andV.N.Vapnik. Atraining algo-\\nrithm foroptimal marginclassi\\x02ers. InD.Haussler ,editor ,\\n5thAnnual ACMWorkshop onCOL T,pp.144-152, 1992.\\n[15] Corinna Cortes andV.Vapnik, \\x94Support-V ector Netw orks,\\nMachine Learning, 20,1995\\n[16] Chih-Chung Chang andChih-Jen Lin, LIBSVM :alibrary\\nforsupport vector machines, 2001. Softw areavailable at\\nhttp://www .csie.ntu.edu.tw/ cjlin/libsvm.\\n\",\n",
       " \"A 3D Facial Expression Database  For Facial Beh avior Research \\nLijun Yin      Xiaozhou Wei      Yi Sun      Jun Wang      Matthew J. Rosato  \\nDepartment of Computer Scie nce, State University of New York at Binghamton \\n \\n \\nAbstract \\n \\nTraditionally, human facial expressions have been \\nstudied using either 2D static images or 2D video \\nsequences. The 2D-based analysis is incapable of handing \\nlarge pose variations. Although 3D modeling techniques \\nhave been extensively used fo r 3D face recognition and 3D \\nface animation, barely any research on 3D facial expression recognition using 3D range data has been \\nreported. A primary factor fo r preventing such research is \\nthe lack of a publicly available 3D facial expression \\ndatabase. In this paper, we present a newly developed 3D \\nfacial expression database, which includes both \\nprototypical 3D facial expression shapes and 2D facial \\ntextures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database \\navailable for the research community, with the ultimate \\ngoal of fostering the research on affective computing and \\nincreasing the general understanding of facial behavior \\nand the fine 3D structure inherent in human facial \\nexpressions. The new database can be a valuable resource \\nfor algorithm assessment, comparison and evaluation.  \\n \\n1. Introduction \\n \\nComputer facial expression analysis would be highly \\nbeneficial for many fields including those as diverse as \\nhuman computer interaction, security, medicine, behavior \\nscience, communication, and education. Currently, all \\nexisting face expression analysis and recognition systems rely \\nprimarily on static images or dynamic videos from many 2D \\nfacial expression databases (e.g., [19] and Table 1). Although \\nsome systems have been successful, the performance \\ndegradation remains when handling expressions with large \\nhead rotation, subtle skin movement, and/or lighting change with varying postures. In order to mitigate the problems \\ninherent in the 2D based analysis, we propose to establish a \\nnew 3D facial expression database, and conduct facial \\nexpression analysis in a 3D space by exploring the surface \\ninformation, which is beyond the availability from the 2D \\nplane. In the following section, we will review the existing \\nwork, identify the critical issues to show why analyzing facial expression in a fully 3D space is necessary. \\n \\n \\n1.1 The State of The Art \\n \\n     Research on automatic techniques for analyzing human \\nfacial behavior has been conducted for over three decades [11, 32, 34]. There are two general approaches which have been developed relying on eith er 2D information or partial \\n3D information.  \\n       The conventional methods for facial expression \\nrecognition focuses on extracting the expression data needed to describe the change of facial features, such as Action Units (AUs) which are defined in the Facial Action Coding System (FACS) [11]. A number of techniques were successfully developed using 2D static images or video \\nsequences, including machine vision techniques [44, 12, 10, \\n21, 4, 41, 42] and machine learning techniques [1, 20, 5, 6, \\n45]. The excellent review of recent advances in this field can be found in [27, 42, 46, 28, 15].        Recently, some researchers have noticed the \\nimportance of exploring 3D information to improve facial \\nexpression recognition. Some have successfully used partial 3D information, such as multiple views [29] or 3D models for facial expression analysis [2, 17, 43, 26]. These methods are based on 2D images. They can alleviate the problems caused by different head poses to a certain degree with the \\nassistance of a 3D model or with multiple views of the face. \\nHowever, since no complete 3D individual facial geometric shapes are employed, the abilit y to handle large head pose \\nvariation and the ability to differentiate subtle expressions \\nis inherently limited.        To the best of our knowledge, little investigation  \\nhas been conducted on analyzing facial behavior in a \\ncomplete 3D space even though it is believed to be a better reflection of facial behavior. In the following section, we summarize several critical issues and limitations of the existing facial expression recognition systems, and show \\nthe advantage of 3D facial expression analysis.  \\n \\n1.2 Why 3D: Critical Issues and Limitations of 2D \\n \\n(1) 3D surface features exhib ited in facial expressions  \\n     The common theme in the current research on face expression recognition is that the face is a flat pattern, like a 2D geometric shape associating with certain textures. This \\nview has the consequences that expression variations is \\nconsidered only in terms of measurements made on the \\npicture plane. However, the common feature of faces is the three-dimensional surface rather than a two-dimensional \\npattern. Understanding the face as a mobile, bump surface \\ninstead of a flat pattern may have a theoretical implication as \\nwell as practical applications. Psychological research shows \\nthat the human visual system can perceive and understand \\nembedded features contained in the 3D facial surface even when such features are not exhibited in corresponding 2D \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\nplane images. It is possible that  the viewer actually \\nrepresents the surface shape of the face when constructing \\nrepresentations for recognition  [3]. This explains why \\nhuman recognition of 2D facial expressions is presently so \\nmuch better than machine recognition.         The facial expression is an entire facial behavior. \\nMulti-\\ndimensional expression space better characterizes this \\ncomplexity [37]. Many expressions in this space exhibit subtle \\nin-depth skin motion. For example, the skin extrusion in the \\nareas of the cheek, forehead, glabella (in between eyebrows), \\nnasolabial (in-between nose-side and mouth corners), crow-\\nfeet (out-corners of eyes), chin or mouth exhibits these subtle \\nmotions. These areas contain a high number of precious \\nsurface features (e.g., convex, concave, or other 3D primitive \\nfeatures), and could play a critical role in distinguishing subtle \\nfacial expressions. However, the 2D based approaches are hard \\nto detect 3D surface features and in-depth motions (e.g., wrinkles) although they are good at detecting high-contrast \\nfeatures in salient organ areas (such as eye, nose, mouth).  \\n       Due to the limitations in describing facial surface \\ndeformation when 3D features are evaluated in 2D space, 2D images with a handful of feature units may not accurately reflect the authentic facial expressions. \\nTherefore, there is a great demand for representing facial \\nexpressions in a 3D space in order to scrutinize the facial \\nbehavior at the level of subtlety explored between human-\\nhuman interactions. Such a 3D-based analysis approach \\ncould allow us to examine the fine structure change for \\nuniversal and complex expressions.   \\n(2) Pos e / Posture:  \\n     People rarely express emotions without head motion or \\nposture spontaneity. Nevertheless, current research on facial \\nexpression analysis primarily focuses on the frontal view of face images, with very limited head motion or posture change. The assumption of frontal view expressions is not \\nonly unrealistic, but also jeopardizes the accurate \\nexpression analysis because head pose and posture are \\nimportant cues, which, in conjunction with facial action, \\nreflect a person's real emotion [28]. \\nLarge head pose change \\nwill cause an illumination change on the face which may cause \\npart of the face to become invisible. The head motion and \\nresulting occlusion increase the difficulty to track facial \\nfeatures and pose (e.g., in-depth direction) accurately and \\nreliably in the 2D plane, jeopardizing the robust detection of \\nAUs. Capturing 3D head orientation and analyzing facial \\nexpressions in 3D space has the potential to alleviate these \\nproblems related to pose and posture.  \\n \\n(3) B enchmark 3D facial ex pression databas e \\n     A common testing resource is essential for research on  \\nfacial expression analysis. Although there are a number of  \\npopular 2D face expression databases accessible for facial \\nexpression research, as of yet, no readily accessible \\ndatabase of test materials for 3D expression analysis has \\nbeen established (see Table 1). The lack of an accredited common database (like the FERET and FRGC databases for face recognition) and evaluation methodology makes it difficult to compare, validate, resolve, and extend the issues concerned with 3D facial expression analysis. Currently, a \\nnumber of standard  face databases , containing both 2D and \\n3D data (e.g., [30, 13, 14]), are available to the face \\nrecognition community. However, these databases were not designed systematically for the purpose of  facial expression recognition. They do not include either a whole set of prototypic expression data or 3D face expressions at \\nvarious levels of intensity, therefore are not sufficient for \\n3D face expression research.   \\nData- \\nBase          Face \\n    Recognition   Face Expression \\n    Recognition  \\n2D FERET [31], FRGC [30],\\nCMU-PIE [39], BioID[50]\\nAR [48],  Yale [47],   \\nxm2vtsdb [18]\\nUT-Dallas [25, 24],\\nMany others [46], ...Cohn-Kanade [19], \\nJAFFE [21], MMI [8], \\nRU-FACS-1 [9], \\nEkman-Hager [16,10] \\nUSC-IMSC [23], \\nUT-Dallas [25, 24], \\nUA-UIUC [38],  \\nQU [33], PICS[49],...  \\n3D 3D FRGC [30, 22], \\nDARPA-HumanID [14], \\nPRISM-ASU [13],  \\nxm2vtsdb [18], …  None \\n \\nTable 1. Survey of existing databases for research on face recognition and face expression recognition \\n \\n       In short, the establishment of a 3D facial expression \\ndatabase is crucial to enhancing facial behavior research. \\nThe lack of such essentials has impeded research in this area. In the following sections, we will introduce the database creation process and the organization of the database. We will also describe the process for data validation and assessment. Finall y, the limitation and future \\nextension of the current database are addressed.\\n \\n \\n2. Creation of 3D Facial  Expression Database  \\n \\n2.1 Capturing 3D Facial Expressions \\n    The development of our database was designed to sample \\nfacial behaviors with seven universal emotional states. Each expression is represented by multiple intensities which reflect different levels of spontaneity.   \\n \\n \\nFigure 1. 3D face imaging system setup \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\n(1) 3D face digitizer  \\n     The 3D facial range data is captured with a 3D face  \\nimaging system (3DMD digitizer, Figure 1) [36] using a \\nrandom light pattern projection in the speckle projection \\nflash environment. The system projects a random light \\npattern onto the subject and captures his/her shape using the \\nprecisely synchronized digital cameras which are set at \\nvarious angles in an optimum configuration. The six digital cameras and two light pattern projectors are positioned on \\ntwo sides (three cameras and one projector on each side). \\nThe system automatically merges all six synchronized \\ncameras’ viewpoints data and produces a single 3D polygon \\nsurface mesh. Using the stereo photogrammetry technique, the 3D face surface geometry and surface texture are acquired. Each instant shot (l ess than 2 milliseconds capture \\ntime) outputs a set of data, including a pair of textures with two angle views and a wire-frame model. The texture size of the two-views image is around 1300 by 900 pixels. The model resolution is in the range from 20,000 polygons to 35,000 polygons, depending on the size of subject’s face.  \\n                    \\n(2) Expression scanning at work  \\n     Each subject is instructed to sit in front of the 3D face capture system. They are requested to perform seven universal expressions, i.e., neutral, happiness, surprise, fear, \\nsadness, disgust, and angry . Although the 3D capture \\nsystem does not capture facial expressions dynamically, we \\nrequire the subjects to perform each expression for a short \\nperiod of time. The scan fires four instant shots to capture \\nthe four different degrees of the expression. The intensity ranges from low, middle, high , and highest , and the \\ncapturing at approximately ten-second intervals.      Ideally, a video clip could be used for eliciting the \\nauthentic expressions refl ecting naturally occurring \\nemotions. However, it is difficult to elicit a wide range of \\ntrue emotions from a short video clip, especially for sadness and fear [38]. It is worth noting that archetypal emotions are \\na rare phenomenon. As quoted by Cowie et al [33], displays \\nof intense emotion or “pure” primary emotions rarely \\nhappened.  \\n     The true emotion could be developed over a long time of \\ninvolvement in special activ ities or events. The best \\nelicitation could be from scenarios such as those shown in the reality TV shows, “Fear Factor”, “Survivors”, and “The \\nApprentice”. However, it is not feasible to set up a lab \\nenvironment to obtain such authentic and spontaneous \\nexpressions associated with true emotions. In everyday life, most people are likely to exhibit spontaneous emotions in a very light (low)  intensity without exaggerated appearances. \\nThis common observation is similar to the scenario \\nexhibited in the initial stage of the expression action. With \\nthis consideration, the subjects were asked to perform the light (low) intensity of each expression to simulate the \\nspontaneity of the emotional state.      We requested each subject to perform four stages of expressions, ranging from low intensity, middle, high, and highest intensity of a specific expression. It was up to the subject to post four stages of expressions with his/her own \\nstyle. Upon completing the face scanning, the data were \\nannotated for archival as a ground truth .  \\n \\n(3) Statistics of participan ts and expression data \\n       There were 100 subjects who participated in face scans, \\nincluding undergraduates, graduates and faculty from our institute’s departments of Psychology, Arts, and \\nEngineering (Computer Science, Electrical Engineering and Mechanical Engineering). The majority of participants were undergraduates from the Psychology Department. The resulting database consists of  about 60% female and 40% \\nmale subjects with a variety of ethnic/racial ancestries, \\nincluding White, Black, East-Asian, Middle-east Asian, \\nHispanic Latino, and others.      Each subject performed seven expressions. With the exception of the neutral expression, each of the six \\nprototypic expressions ( happiness, disgust, fear, angry, \\nsurprise and sadness ) includes four levels of intensity. \\nTherefore, there are 25 instant 3D expression models for \\neach subject, resulting in a total of 2,500 3D facial \\nexpression models in the database. Associated with each expression shape model, is a corresponding facial texture \\nimage captured at two views (about +45 ° and -45 °). As a \\nresult, the database consists of 2,500 two-view’s texture \\nimages and 2,500 geometric shape models. \\n \\n2.2 Expression Data Description and Management \\n \\n     The expression data includes the 3D model, texture, and \\nenrollment information. Along with the raw model data, \\nadditional semantic and surface feature data are also archived. Figure 2 shows the data structure for archival. By \\nquery, the data is searchable by gender, ethnicity, \\nexpression (emotion state), and intensity.   \\nGender Race\\n3D scans mesh     TexturesExpression Data3D Face Expression\\nSubject#\\nFeatures EP1EP2EP3 EP7 .......\\n.... \\n \\nFigure 2. Data structure of 3D facial expressions for archival \\n \\n(1) Data processing \\n     In order to make the database useful for assessing and \\ncomparing algorithms using 2D-based and 3D-based facial \\nexpression recognition techniques, we provide both facial texture images and facial shape models as the raw data in the database.       Since the raw geometric models contain the unprocessed \\nhead-shoulder boundaries including necks and clothing, \\nwhich are not “clean”, further processing was performed to \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\nmake the data easier to use. The original raw data was \\nprocessed by truncating the boundary to generate a face \\nmodel with the pure face region. The cropped face region \\ncontains about 13,000 - 21,000 polygons. In addition, a \\nfrontal view texture (512 by 512 pixels) is generated using \\nour 3D face shape processing and warping tool. Therefore, in total, the database is composed of 2,500 raw 3D expression models, 2,500 raw textures in two-views’ faces, \\n2,500 cropped models and 2,500 frontal view textures of \\nthe face regions.  \\n     In addition to the geomet ric data and texture data, a set \\nof associated descriptors is also generated as an optional \\ndata set. \\n \\n(2) Associated Optional Descriptor  \\n(a) Feature point set:  We picked 83 feature vertices on  \\neach facial model (Figure 4 (row 1)). Given the set of \\nfeature points on the face model labeled, the feature regions \\non the face surface can be easily determined. These features could be used as a ground truth to assess algorithms for 3D \\nmodel segmentation and 3D feature detection. \\n(b) 3D face pose:  The obtained models contain various \\nposes. We provide the model orientation using a normal \\nvector with respect to the frontal projection plane. Given \\nthree vertices picked from two eye corners and a nose \\ncenter, a triangle plane is formed. The norm of this plane \\nrepresents the original face pose. The database includes such data for pose-related algorithm assessment.   \\n \\nRaw data (Figure 3) Produced data (Figure 4) \\n2,500 face shape models 2,500 cropped face  \\nregional  shape models  \\n2,500 face textures  \\n(two views)  2,500 frontal texture of  facial regions\\n \\n 2,500 data sets of  \\nfacial feature points  \\n 2,500 data sets of the  \\noriginal facial poses   \\n \\nTable 2. Summary of the archived data including the raw data and the processed data.  \\n    \\n   In summary, the amount of 3D facial expression data \\narchived in the database is list ed in the Table 2. Note that \\nsince the database is designed to be available to public \\nresearch, researchers in different areas can test their \\nalgorithms against the database and update or expand the dataset by adding new features in the future.  \\n \\n3. Validation and Evaluation of the Database  \\n \\n      The quality of the 3D face expression database is \\nevaluated through the validation experiments. The \\nvalidation study addresses th e question of whether the \\ninterpretations by machines are equal to those given by \\nobservers or performers. To do so, we conducted an analysis and test against our 3D expression database. Each expression data set was analyzed three times. Firstly, by the subject who performed the expression (as ground truth). \\nSecondly, by observers from the Psychology Department \\nwho are experts in interpreting facial expressions (as expert votes). Thirdly, using machines via our facial expression recognizer (as machine votes). The following sub-sections report the statistical results of the expert evaluation and computer recognition.  \\n \\n3.1 Subjective votes by observers \\n \\nAs described in Section 2, the subjects provided the \\nvalidation results for each expression with four intensities. Given such ground truth data, we compare the results by the \\nsubjective votes from two psychologists of Psychology \\nDepartment. The confusion matrix is reported in the Table 3. \\nThe average expert recognition rate is 94.1% for low \\nintensity expressions, 95.7% for middle intensity, 96.8% for high intensity, and 98.1% for highest intensity expressions. The most likely confused expressions were sad-fear and \\ndisgust -angry, even for experts .  \\n \\nIn/Out Ang  Dis Fea Hap Sad Sur Neu \\nAnger 94.9 2.5 1.2 0 0.3 0.2 0.9 \\nDisgust 2.6 95.4 0.9 0 0.9 0 0.2 \\nFear 0.1 0.5 96.4 0 2.4 0.1 0.5 \\nHappy 0.1 0 0.1 99.4 0 0.4 0 \\nSad 1.0 0.2 2.4 0 96.2 0 0.2 \\nSurprise 0.4 0 0.2 0.4 0 99.0 0 \\nNeutral 0.8 0 0.2 0 0.3 0 98.7 \\n \\nTable 3. Confusion matrix of expert voting averagely for four intensities of expressions (%). \\n \\n3.2 Objective votes by machine classification \\n \\n   To validate the created 3D facial expression database, we \\nconducted experiments on face expression recognition \\nusing our newly developed 3D face expression \\nrepresentation and classification algorithm. The basic algorithm is outlined as follows: (details in the report [7]).  \\n     Given the set of expression range models, in order to \\nbetter characterize 3D features of the facial surface, each \\nvertex on the individual model is labeled by one of the \\ntwelve primitive surface. Our labeling approach is based on the estimation of principal curvatures of the facial surface. It is believed that  the curvature information is a good \\nreflection of local shape of the facial surface [40]. \\n     In order to classify the facial expressions based on the \\n3D facial expression data, we segment the 3D face surface into seven local expressive re gions (excluding interiors of \\nmouth, interior of eyes and nose bridge), and conduct the histogram statistics on each region in terms of the twelve primitive surface label distribution. Each expressive region \\nforms a twelve-dimension feature vector, in which each \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\nelement is defined as a ratio of the number of vertices with \\na specific label type to the number of vertices in the local region. As such, an 84-dimension feature vector is \\nconstructed on the entire facial region. The facial \\nexpression surface labels exhibit different patterns which correspond to different facial expressions. Such feature \\nvectors are used for expression classification. \\n     We conducted facial expression recognition using pure 3D geometric shape models from our 3D facial expression \\ndatabase. The experiment is person-independent, which \\nmeans the query subject has never appeared in the training set. We applied linear discriminant analysis (LDA) classifier to classify the prototypic facial expressions of \\nsixty subjects. The correct recognition rate is about 83.6%.  \\n     \\na\\n \\nb \\nc  \\na’  \\nb’  \\nc’   \\n \\nFigure 3: Sample expressions: Left four (happiness) and \\nright four (surprise) with four levels of intensity. a-a’ are \\nraw models; b-b’ are cropped shape models in face \\nregions. c-c’ are two views’ textures.  \\n \\n4. Limitation and Development of Database \\n \\n    There are several limitations in  the current version of the \\ndatabase in terms of dynamics, FACS-related coding and \\nthe expression variety in the expression space. Limited by \\nthe speed of 3D imaging capture system and post \\nprocessing load, no 3D dynamic expressions are captured in \\nthe current version. The number of expression types is still limited to the prototypic expression space, more spontaneous expressions need to be included for naturally occurring emotion analysis. Our future work will focus on the following aspects:  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 4: Four sample subjects showing seven \\nexpressions (neutral, angry, disgust, fear, happiness, \\nsadness, and surprise). The facial shape model and \\nfrontal-view textures are produced. The first row shows \\na sample set of the picked feature points. \\n \\n(1) Dynamics:  We will extend the database to include \\ndynamic 3D facial expression sequences using a real-time dynamic range system, with a super-high resolution model representation. As such, the 3D action units coding and \\nlabeling could be further explored.  \\n (2) Expression space:  We will include more spontaneous \\n3D expression data with more affects states (such as \\nboredom, skepticism, shame, etc.) through eliciting \\nchildren/adults emotion response with the experiments \\ndesigned and guided by our collaborated psychologists.  \\n \\n (3) Applications in medical and psychological research:  \\nWe will be interested in the study of the clinically interested \\ndata for diagnosis purpose. For example, reading pain \\nexpressions when the self-report is not possible for people like non-\\ncommunicative adults, developmentally delayed \\nchildren or newborns. We will also extend the 3D facial \\nexpression database to the emerging field of applications, such as using 3D expression models as a source of stimuli for the \\npsychological research to diagnose, assess and rehabilitate \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\npatients with brain or psychological disorders (e.g., alzheimer, \\netc.) [35].  \\n \\n5. Conclusion \\n \\n    We have developed a new 3D facial expression database \\nfor the scientific research community. This is the first \\nattempt to foster the research on analyzing the facial \\nbehavior in a complete 3D space, targeting the \\nidentification of more detail and subtle facial behavior. The future challenge is to fu rther develop the 3D facial \\nexpression database using the dynamic and spontaneous 3D \\nhigh-resolution expression data in order to move closer to \\ndeveloping a naturally occurring facial behavior analysis \\nsystem.  \\n  \\nAcknowledgement  \\n \\n        This material is based upon the work supported in \\npart by the National Science Foundation under grants IIS-\\n0541044, IIS-0414029, and the NYSTAR’s James D. \\nWatson Investigator Program. We would like thank Gina \\nShroff, Peter Gerhardstein, Joseph Morrissey of the Department of Psychology, Lee Serversky and Ben Myerson of the Computer Science Department for the help during the process of creating the database.  \\n     \\nReferences \\n \\n[1] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. Measuring facial \\nexpressions by computer image analysis. Psychophysiology, 36, 1999.  [2] B. Braathen, M. Bartlett, G. Littlewort, et al. An approach to automatic \\nrecognition of spontaneous facial actions. FGR 2002. \\n[3] V Bruce, M. Burton, and T. Doyle. Faces as surfaces. Processing \\nImages of Faces, 1992. \\n[4] Y Chang, C. Hu, and M. Turk. Probabilistic expression analysis on \\nmanifolds. CVPR’04, Washington DC, 2004.  \\n[5] I. Cohen, F. Cozman, N. Sebe, M. Cirelo, and T. Huang. Semi-\\nsupervised learning of classiﬁers: Theory, algorithms for Bayesian network \\nclassiﬁers and application to human-computer interaction. IEEE Trans. \\nPAMI, 26(12), 2004.  \\n[6] I. Cohen, N. Sebe, A. Garg, L. Chen, and T. Huang. Facial expression \\nrecognition from video sequences: temporal and static modeling. CVIU, \\n91(1), 2003.  \\n[7] J. Wang, L. Yin, et al, “3D facial expression recognition based on \\nprimitive surface feature distribution”, Tech. Report, Binghamton U, 2006.   \\n[8] Man machine interaction group. http://www.mmifacedb.com/. Delft University of Technology, 2005.  \\n[9] RU-FACS-1 Database. http://mplab.ucsd.edu/databases/databases.html.  \\n[10] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski. \\nClassifying facial actions. IEEE Trans. PAMI, 21(10):974-989, 1999.  \\n[11] P. Ekman and W. Friesen. Facial Action Coding System. New York: \\nConsulting Psychologists Press, 1977.  \\n[12] I. Essa and A. Pentland. Coding, analysis, interpretation, and \\nrecognition of facial expressions. IEEE Trans. PAMI, 19(7), 1997.  \\n[13]  PRISM-ASU. http://prism.asu.edu/3dface/default.asp .  \\n[14]  USF DARPA Human ID 3D face database. Courtesy of Prof. Sudeep \\nSarkar, University of South Florida, Tampa, FL.  \\n[15]  B. Fasel and J. Luettin. Automatic facial expression analysis: A \\nsurvey. Pattern Recognition, 36(1), 2003.  \\n[16]  W. Friesen and P. Ekman. Dictionary - Interpretation of FACS Scoring. Unpublished manuscript, UC San Francisco, 1987.  [17]  S. Gokturk, J. Bouguet, C. Tomasi, and B. Girod. Model-based face tracking for view-independent facial expression recognition. In FGR 2002. \\n[18] K Messer, J Matas, J Kittler, et al. Xm2vtsdb: The extended m2vts \\ndatabase. In International Conference of AVBPA, March 1999. \\nhttp://www.ee.surrey.ac.uk/Research/VSSP/xm2vtsdb/   \\n[19] T. Kanade, J.Cohn, and Y. Tian. Comprehensive database for facial \\nexpression analysis. FGR’00, France, 2000.  \\n[20]  G. Littlewort, M. Bartlett, I. Fasel, J. Susskind, and J. Movellan. \\nDynamics of facial expression extracted automatically from video. In \\nCVPR Workshop on FPIV'04, 2004.  \\n[21]  M. Lyons, et al. Automatic classiﬁcation of single facial images. \\nIEEE Trans. PAMI, 21(12):1357-1362, 1999.  \\n[22] K. Chang and K. Bowyer and P. Flynn. An evaluation of multimodal \\n2D+3D face biometrics, IEEE Trans. on PAMI. 27(4): 619-624. 2005. \\n[23]  U. Neumann. Facial expression analysis and synthesis (NSF report). \\nhttp://imsc.usc.edu/research/project/facialexp/ . \\n[24]  A. O'Toole. Psychological and neural perspectives in human face \\nrecognition. In The Handbook of Face Recognition, 2004, Springer-Verlag. \\nEditors: S. Li and A. Jain.  \\n[25]  A. O'Toole, J. Harms, et al. A video database of moving faces and \\npeople. IEEE Trans. PAMI, 27(5), 2005.  [26]  L. Zalewski and S. Gong. Synthesis and recognition of facial \\nexpressions in virtual 3D views. In FGR’04, 2004.  \\n[27]  M. Pantic and L. Rothkrantz. Automatic analysis of facial \\nexpressions: the state of the art. IEEE Trans. PAMI, 22(12), 2000. \\n[28]  M. Pantic and L. Rothkrantz. Toward an affect-sensitive multimodal \\nhuman-computer interaction. Proceedings of IEEE, 91(9):1370-1390, 2003.  \\n[29]  M. Pantic and L. Rothkrantz. Facial action recognition for facial \\nexpression analysis from static face images. IEEE Trans. on SMC Part B: \\nCybernetics, 34(3):1449-1461, 2004.  \\n[30]  P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman, \\nJ. Marques, J. Min, and W. Worek. Overview of the face recognition grand \\nchallenge. CVPR05, San Diego, CA, 2005.  \\n[31]  P. Phillips, H. Moon, P. Rauss, et al., The FERET evaluation metho-\\ndology for face recognition algorithm. IEEE Trans. PAMI, 22 (10), 2000.  \\n[32]  R. Picard. Affective computing: challenges. Inter. Journal of Human \\nComputer Studies, 59(1-2):55-64, 2003.  \\n[33] E. Douglas-Cowie, R. Cowie and M. Schroder. A new emotion \\ndatabase: considerations, sources and scope. Proc. of the ISCA ITRW on Speech and Emotion, Newcastle, 2000, pp. 39-44. \\n[34] R.Cowie, E. Douglas-Cowie, et al.  Emotion Recognition in Human \\nComputer Interaction. IEEE Signal Processing Magazine 18 (1). 2001. \\n[35] A. Rizzo. Virtual reality and disability: emergence and challenge. \\nDisability and Rehabilitation, 24(11), 2002.  \\n[36] 3DMD Inc., http://www.3dmd.com , 2005. \\n[37] J. Russell. Is there universal recognition of emotion from facial \\nexpression? Psychological Bulletin, 115(1):102-141, 1994.  \\n[38]  N. Sebe, M. Lew, I. Cohen, Y Sun, T. Gevers, and T. Huang. \\nAuthentic facial expression analysis. In FGR 2004. \\n[39] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination and \\nexpression database. IEEE Trans. PAMI, 25(12), 2003.  \\n[40]  H. Tanaka, M. Ikeda, and H. Chiaki. Curvature-based face surface \\nrecognition using spherical correlation. In FGR’1998.  \\n[41] F. Bettinger and T.F.Cootes. A Model of Facial Behavior. In FGR’04. \\n[42] Y Tian, T. Kanade, and J. Cohn. Recognizing action units for facial \\nexpression analysis. IEEE Trans. on PAMI, 23(2), 2001.  \\n[43]  Z. Wen and T. Huang. Capturing subtle facial motions in 3D face \\ntracking. In IEEE Inter. Conf. on Computer Vision, 2003.  \\n[44]  Y. Yacoob and L. Davis. Recognizing human facial expressions from \\nlong image sequences using optical ﬂow. IEEE Trans. PAMI, 18 (6), 1996.  \\n[45]  Y Zhang and Q. Ji. Active and dynamic information fusion for facial \\nexpression understanding from image sequences. IEEE Trans. PAMI, 27 \\n(5):699-714, May 2005.  \\n[46]  W. Zhao, R. Chellappa, P. Phillips, and A. Rosenfeld. Face \\nrecognition: A literature survey. ACM Computing Surveys, 35(4), 2003.  \\n[47] http://cvc.yale.edu/projects/yalefaces/yalefaces.html  \\n[48] http://rvl1.ecn.purdue.edu/~aleix/aleix_face_DB.html  \\n[49] PICS database, http://pics.psych.stir.ac.uk/index.html  \\n[50]http://www.humanscan.de/support/downloads/facedb.php  \\nProceedings of the 7th International Conference on  Automatic Face and Gesture Recognition (FGR’06) \\n0-7695-2503-2/06 $20.00 © 2006  IEEE \\n\\n\",\n",
       " 'A Multimodal Database for\\nAffect Recognition and Implicit Tagging\\nMohammad Soleymani, Member ,IEEE , Jeroen Lichtenauer,\\nThierry Pun, Member ,IEEE , and Maja Pantic, Fellow ,IEEE\\nAbstract —MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and\\nimplicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data,\\nand peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural\\nbackgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt\\nemotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos\\nand images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed\\ntags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The\\ndatabase is made available to the academic community via a web-based system. The collected data were analyzed and single\\nmodality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the\\npotential uses of the recorded modalities and the significance of the emotion elicitation protocol.\\nIndex Terms —Emotion recognition, EEG, physiological signals, facial expressions, eye gaze, implicit tagging, pattern classification,\\naffective computing.\\nÇ\\n1I NTRODUCTION\\nALTHOUGH the human emotional experience plays a\\ncentral part in our lives, our scientific knowledge about\\nhuman emotions is still very limited. Progress in the field ofaffective sciences is crucial for the development ofpsychology as a scientific discipline or application that\\nhas anything to do with humans as emotional beings. More\\nspecifically, the application of human-computer interactionrelies on knowledge about the human emotional experi-ence, as well as on knowledge about the relation betweenemotional experience and affective expression.\\nAn area of commerce that could obviously benefit from\\nan automatic understanding of human emotional experi-ence is the multimedia sector. Media items such as movies\\nand songs are often primarily valued for the way in which\\nthey stimulate a certain emotional experience. While itmight often be the affective experience that a person islooking for, media items are currently primarily tagged by\\ntheir genre, subject or their factual content. Implicit affectivetagging through automatic understanding of an individual’s\\nresponse to media items would make it possible to rapidly\\ntag large quantities of media, on a detailed level and in a\\nway that would be more meaningful to understand how\\npeople experience the affective aspects of media content [1].\\nThis allows more effective content retrieval, required to\\nmanage the ever-increasing quantity of shared media.\\nTo study human emotional experience and expression in\\nmore detail and on a scientific level, and to develop andbenchmark methods for automatic recognition, researchers\\nare in need of rich sets of data of repeatable experiments [2].\\nSuch corpora should include high-quality measurements ofimportant cues that relate to the human emotional experi-ence and expression. The richness of the human emotionalexpressiveness poses both a technological as well as a\\nresearch challenge. This is recognized and represented by\\nan increasing interest into pattern recognition methods forhuman behavior analysis that can deal with the fusion ofmeasurements from different sensor modalities [2]. How-ever, obtaining multimodal sensor data is a challenge in\\nitself. Different modalities of measurement require different\\nequipment, developed and manufactured by differentcompanies, and different expertise to set up and operate.The need for interdisciplinary knowledge as well astechnological solutions to combine measurement data from\\na diversity of sensor equipment is probably the main reason\\nfor the current lack of multimodal databases of recordingsdedicated to human emotional experiences.\\nTo contribute to this need for emotional databases and\\naffective tagging, we have recorded a database of multi-modal recordings of participants in their response to\\naffectively stimulating excerpts from movies and images\\nand videos with correct or incorrect tags associated withhuman actions. The database is freely available to the42 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\n.M. Soleymani and T. Pun are with the Computer Vision and Multimedia\\nLaboratory, Computer Science Department, University of Geneva, BattelleCampus, Building A, Rte. de Drize 7, Carouge (GE) CH-1227, Switzer-land. E-mail: {mohammad.soleymani, thierry.pun}@unige.ch.\\n.J. Lichtenauer is with the Department of Computing, Imperial College\\nLondon, 180 Queen’s Gate, London SW7 2AZ, United Kingdom.E-mail: j.lichtenauer@imperial.ac.uk.\\n.M. Pantic is with the Department of Computing, Imperial College London,\\n180 Queen’s Gate, London SW7 2AZ, United Kingdom, and the Faculty ofElectrical Engineering, Mathematics and Computer Science (EEMCS),University of Twente, Drienerlolaan 5, Enschede 7522 NB, The Nether-\\nlands. E-mail: m.pantic@imperial.ac.uk.\\nManuscript received 12 Nov. 2010; revised 1 July 2011; accepted 6 July 2011;\\npublished online 28 July 2011.Recommended for acceptance by B. Schuller, E. Douglas-Cowie, and A. Batliner.\\nFor information on obtaining reprints of this article, please send e-mail to:\\ntaffc@computer.org, and reference IEEECS Log NumberTAFFCSI-2010-11-0112.Digital Object Identifier no. 10.1109/T-AFFC.2011.25.\\n1949-3045/12/$31.00 /C2232012 IEEE Published by the IEEE Computer Society\\nacademic community, and is easily accessible through a\\nweb-interface.1The recordings for all excerpts are anno-\\ntated through an affective feedback form, filled in by the\\nparticipants immediately after each excerpt. A summary of\\nthe MAHNOB-HCI database characteristics is given inTable 1. The recordings of this database are preciselysynchronized and its multimodality permits researchers to\\nstudy the simultaneous emotional responses using different\\nchannels. Two typical sets including responses to emotionalvideos and implicit tagging or agreement with displayedtags can be used for both emotion recognition as well asmultimedia tagging studies. Emotion recognition and\\nimplicit tagging baseline results are given for researchers\\nwho are going to use the database. The baseline results set atarget for the researchers to reach.\\nIn Section 2, we give an overview of existing affective\\ndatabases, followed by descriptions of the modalities wehave recorded in our database in Section 3. Section 4explains the experimental setup. The first experimentparadigm, some statistics and results of classifications ofemotions are presented in Section 5 and for the secondexperiment in Section 6. A discussion on the use of thedatabase and recommendations for recordings of suchdatabases are given in Section 7, followed by our conclu-sions in Section 8.\\n2B ACKGROUND\\nCreating affective databases is an important step in emotion\\nrecognition studies. Recent advances in emotion recognition\\nhave motivated the creation of novel databases containing\\nemotional expressions. These databases mostly includespeech, visual, or audio-visual data [5], [6], [7], [8].The visual modality of the emotional databases includesface and/or body gestures. The audio modality carries\\nacted or genuine emotional speech in different languages.\\nIn the last decade, most of the databases consisted only of\\nacted or deliberately expressed emotions. More recently,\\nresearchers have begun sharing spontaneous and naturalemotional databases such as [6], [7], [9]. We only review the\\npublicly available spontaneous or naturalistic databases\\nand refer the reader to the following review [2] for posed,\\naudio, and audio-visual databases.\\nPantic et al. created the MMI web-based emotional\\ndatabase of posed and spontaneous facial expressions withboth static images and videos [5], [10]. The MMI database\\nconsists of images and videos captured from both frontal and\\nprofile view. The MMI database includes data from 61 adults\\nacting different basic emotions and 25 adults reacting to\\nemotional videos. This web-based database gives an option of\\nsearching in the corpus and is downloadable.\\n2\\nOne notable database with spontaneous reactions is the\\nBelfast database (BE) created by Cowie et al. [11]. The BE\\ndatabase includes spontaneous reactions in TV talk shows.\\nAlthough the database is very rich in body gestures and\\nfacial expressions, the variety in the background makes the\\ndata a challenging data set of automated emotion recogni-\\ntion. The BE database was later included in a much largerensemble of databases in the HUMAINE database [6]. The\\nHUMAINE database consists of three naturalistic and six\\ninduced reaction databases. Databases vary in size from 8 to\\n125 participants and in modalities, from only audio-visual\\nto peripheral physiological signals. These databases were\\ndeveloped independently at different sites and collected\\nunder the HUMAINE project.\\nThe “Vera am Mittag” (VAM) audio-visual database [7]\\nis another example of using spontaneous naturalistic\\nreactions during a talk show to develop a database. Twelve\\nhours of audio-visual recordings from a German talk show,\\n“Vera am Mittag,” were segmented and annotated. The\\nsegments were annotated using valence, activation, anddominance. The audio-visual signals consist of the video\\nand utterances from 104 different speakers.\\nCompared to audio-visual databases, there are fewer\\npublicly available affective physiological databases. Healeyand Picard recorded one of the first affective physiologicaldata sets at MIT, which has reactions of 17 drivers underdifferent levels of stress [4]. Their recordings include\\nelectrocardiogram (ECG), galvanic skin response (GSR)\\nrecorded from hands and feet, electromyogram (EMG) fromthe right trapezius, as well as the respiration pattern. Thedatabase of stress recognition in drivers is publicly availablefrom Physionet.\\n3\\nThe Database for Emotion Analysis using Physiological\\nSignals (DEAP) [9] is a recent database that includes\\nperipheral and central nervous system physiological signals\\nin addition to face videos from 32 participants. The face\\nvideos were only recorded from 22 participants. EEGsignals were recorded from 32 active electrodes. Peripheral\\nnervous system physiological signals were EMG, electro-\\nocologram (EOG), blood volume pulse (BVP) using plethys-\\nmograph, skin temperature, and GSR. The spontaneousSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 43\\n1. http://mahnob-db.eu.TABLE 1\\nMAHNOB-HCI Database Content Summary\\n2. http://www.mmifacedb.com/.\\n3. http://www.physionet.org/pn3/drivedb/.\\nreactions of participants were recorded in response to music\\nvideo clips. This database is publicly available on theInternet.\\n4The characteristics of the reviewed databases are\\nsummarized in Table 2.\\n3M ODALITIES AND APPARATUS\\n3.1 Stimuli and Video Selection\\nAlthough the most straightforward way to represent an\\nemotion is to use discrete labels such as fear or joy, label-based representations have some disadvantages. Specifi-cally, labels are not cross-lingual: Emotions do not haveexact translations in different languages, e.g., “disgust”does not have an exact translation in Polish [12]. Psychol-\\nogists therefore often represent emotions or feelings in an\\nn-dimensional space (generally 2 or 3D). The most famoussuch space, which is used in the present study andoriginates from cognitive theory, is the 3D valence-arousal-dominance or pleasure-arousal-dominance (PAD)\\nspace [13]. The valence scale ranges from unpleasant to\\npleasant. The arousal scale ranges from passive to active orexcited. The dominance scale ranges from submissive (or“without control”) to dominant (or “in control, empow-ered”). Fontaine et al. [14] proposed adding a predictabilitydimension to PAD dimensions. Predictability level de-\\nscribes to what extent the sequence of events is predictable\\nor surprising for a viewer.\\nIn a preliminary study, 155 video clips containing movie\\nscenes manually selected from 21 commercially producedmovies were shown to more than 50 participants; each\\nvideo clip received 10 annotations on average [15]. The\\npreliminary study was con ducted utilizing an online\\naffective annotation system in which the participantsreported their emotions in response to the videos playedby a web-based video player.\\nIn the preliminary study, the participants were thus\\nasked to self-assess their emotion by reporting the feltarousal (ranging from calm to excited/activated) andvalence (ranging from unpleasant to pleasant) on ninepoints scales. SAM Manikins were shown to facilitate theself-assessments of valence and arousal [16]. Fourteen video\\nclips were chosen based on the preliminary study from the\\nclips which received the highest number of tags in differentemotion classes, e.g., the clip with the highest number ofsad tags was selected to induce sadness. Three otherpopular video clips from online resources were added to\\nthis set (two for joy and one for disgust). Three past weather\\nforecast reports (retrieved from youtube.com) were alsoused as neutral emotion clips. The videos from online\\nresources were added to the data set to enable us to\\ndistribute some of the emotional video samples with themultimodal database described below. The full list ofvideos is given in Table 3.\\nUltimately, 20 videos were selected to be shown which\\nwere between 34.9 and 117 s long ( M¼81:4s;SD ¼22:5s).\\nPsychologists recommended videos from 1 to 10 minuteslong for elicitation of a single emotion [17], [18]. Here, the\\nvideo clips were kept as short as possible to avoid multiple\\nemotions or habituation to the stimuli while keeping themlong enough to observe the effect.\\n3.2 Facial Expressions and Audio Signals\\nOne of the most well-studied emotional expression chan-\\nnels is facial expressions. A human being uses facial\\nexpressions as a natural mean of emotional communication.\\nEmotional expressions are also used in human-human\\ncommunication to clarify and stress what is said, to signal\\ncomprehension, disagreement, and intentions, in brief, to\\nregulate interactions with the environment and other\\npersons in the vicinity [19], [20]. Automatic analysis offacial expression is an interesting topic from both scientific\\nand practical point of view. It has attracted the interest of\\nmany researchers since such systems will have numerous\\napplications in behavioral science, medicine, security, and\\nhuman-computer interaction. To develop and evaluate suchapplications, large collections of training and test data are\\nneeded [21], [22]. In the current database, we are interested\\nin studying the spontaneous responses of participants while44 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\nTABLE 2\\nThe Summary of the Characteristics of the Emotional Databases Reviewed\\nTABLE 3\\nThe Video Clips Listed with Their Sources\\nThe listed emotional keywords were chosen by polling over participants’\\nself-reports in the preliminary study. 4. http://www.eecs.qmul.ac.uk/mmv/data sets/deap/.\\nwatching video clips. This can be used later for emotional\\nimplicit tagging of multimedia content.\\nFig. 1 shows the synchronized views from the six different\\ncameras. Two types of cameras have been used in the\\nrecordings: one Allied Vision Stingray F-046C, color camera(C1), and five Allied Vision Stingray F-046B, monochrome\\ncameras (BW1 to BW5). All cameras recorded with a\\nresolution of 780/C2580pixels at 60 frames per second. The\\ntwo close up cameras above the screen give a near-frontal\\nview of the face in color Fig. 1a or monochrome Fig. 1b. The\\nmonochrome views have a better sharpness and less motionblur than the color camera. The two views from the bottom of\\nthe screen, Figs. 1c and 1d, give a close up view that may be\\nmore useful during down-facing head poses, and make it\\npossible to apply passive stereo imaging. For this, the\\nintrinsic and extrinsic parameters of all cameras have beencalibrated. Linear polarizing filters were applied with the\\ntwo bottom cameras in order to reduce the reflection of the\\ncomputer screen in eyes and glasses. The profile view Fig. 1ecan be used to extract backward-forward head/body move-\\nments or to aid the extraction of facial expressions, together\\nwith the other cameras. The wide-angle view Fig. 1f capturesthe upper body, arms and hands, which can also carry\\nimportant information about a person’s affective state.\\nAlthough we did not explicitly ask the participants to\\nexpress or talk during the experiments, we expected some\\nnatural utterances and laughter in the recorded audio signals.\\nThe audio was recorded for its potential to be used for videotagging, e.g., it has been used to measure the hilarity of videos\\nby analyzing a user’s laughter [23]. However, the amount of\\nlaughter and audio responses in the database from partici-pants is not enough for such studies and therefore the audio\\nsignals were not analyzed. The recorded audio contains two\\nchannels. Channel one (or “left” if interpreted as a stereostream) contains the audio signal from a AKG C 1000 S MkIII\\nroom microphone, which includes the room noise as well asthe sound of the video stimuli. Channel two contains the\\naudio signal from an AKG HC 577 L head-worn microphone.\\n3.3 Eye Gaze Data\\nThe Tobii X1205eye gaze tracker provides the position of\\nthe projected eye gaze on the screen, the pupil diameter, the\\nmoments when the eyes were closed, and the instantaneousdistance of the participant’s eyes to the gaze tracker device.The eye gaze data were sampled at 60 Hz due to instabilityof the eye gaze tracker system at 120 Hz. The blinkingmoments are also extractable from eye gaze data by finding\\nthe moments in the eye gaze responses where the\\ncoordinates are equal to /C01. Pupil diameter has been\\nshown to change in different emotional states [24], [25].Examples of eye gaze responses are shown in Fig. 2.\\n3.4 Physiological Signals\\nPhysiological responses (ECG, GSR, respiration amplitude,and skin temperature) were recorded with a 1,024 Hzsampling rate and later downsampled to 256 Hz to reducethe memory and processing costs. The trend of the ECG and\\nGSR signals was removed by subtracting the temporal low\\nfrequency drift. The low frequency drift was computed bysmoothing the signals on each ECG and GSR channels witha 256 points moving average.\\nGSR provides a measure of the resistance of the skin by\\npositioning two electrodes on the distal phalanges of themiddle and index fingers and passing a negligible currentthrough the body. This resistance decreases due to an\\nincrease of perspiration, which usually occurs when one is\\nexperiencing emotions such as stress or surprise. Moreover,Lang et al. discovered that the mean value of the GSR isrelated to the level of arousal [26].\\nECG signals were recorded using three sensors attached\\non the participants’ body. Two of the electrodes wereplaced on the chest’s upper right and left corners below theclavicle bones and the third electrode was placed on theabdomen below the last rib for setup simplicity. This setup\\nallows precise identification of heart beats and conse-\\nquently to compute heart rate (HR).\\nSkin temperature was recorded by a temperature sensor\\nplaced participant’s little finger. The respiration amplitudewas measured by tying a respiration belt around theabdomen of the participant.\\nPsychological studies regarding the relations between\\nemotions and the brain are uncovering the strong implica-tion of cognitive processes in emotions [27]. As a result, theEEG signals carry valuable information about the partici-\\npants’ felt emotions. EEG signals were recorded using\\nactive AgCl electrodes placed according to the international10-20 system. Examples of peripheral physiological re-sponses are shown in Fig. 2.\\n4E XPERIMENTAL SETUP\\n4.1 Experimental Protocol\\nAs explained above, we set up an apparatus to record facial\\nvideos, audio and vocal expressions, eye gaze, and\\nphysiological signals simultaneously. The experiment wasSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 45\\nFig. 1. Snapshots of videos captured from six cameras recording facial\\nexpressions and head pose.\\n5. http://www.tobii.com.\\ncontrolled by the Tobii studio software. The Biosemi active\\nII system6with active electrodes was used for physiological\\nsignals acquisition. Physiological signals including ECG,\\nEEG (32 channels), respiration amplitude, and skin\\ntemperature were recorded while the videos were shown\\nto the participants. In the first experiment, five multiple\\nchoice questions were asked during the self-report for each\\nvideo. For the second experiment, where the feedback was\\nlimited to yes and no, two big colored buttons (red andgreen) were provided.\\nThirty participants with different cultural backgrounds\\nvolunteered to participate in response to a campus wide call\\nfor volunteers at Imperial College, London. Out of the\\n30 young healthy adult participants, 17 were female and 13\\nwere male; ages varied between 19 to 40 years old\\n(M¼26:06;SD ¼4:39). Participants had different educa-\\ntional background, from undergraduate students to post-\\ndoctoral fellows, with different English proficiency from\\nintermediate to native speakers. The data recorded from\\nthree participants (P9, P12, P15) were not analyzed due to\\ntechnical problems and unfinished data collection. Hence,\\nthe analysis results of this paper are only based on the\\nresponses recorded from 27 participants.\\n4.2 Synchronized Setup\\nAn overview of the synchronization in the recording setup\\nis shown in Fig. 3. To synchronize between sensors, wecentrally monitored the timings of all sensors, using aMOTU 8pre\\n7audio interface (“c” in Fig. 3) that can sampleup to eight analog inputs simultaneously. This allowed the\\nderivation of the exact temporal relations between events ineach of the eight channels. By recording the external cameratrigger pulse signal (“b” in Fig. 3) in a parallel audio track\\n(see the fifth signal in Fig. 4), each recorded video frame\\ncould be related to the recorded audio with an uncertaintybelow 25/C22s. More details about the data synchronization\\ncan be found in [28].\\nThe gaze tracking data and physiological signals were\\nrecorded with separated capture systems. Because neither of46 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\nFig. 3. Overview of our synchronized multisensor data capture system,\\nconsisting of (a) a physiological measurement device, (b) videocameras, (c) a multichannel A/D converter, (d) an A/V capture PC,(e) microphones, (f) an eye gaze capture PC, (g) an eye gaze tracker,and (h) a photo diode to capture the pulsed IR-illumination from the eye\\ngaze tracker. Camera trigger was recorded as audio and physiological\\nchannels for synchronization.\\n6. http://www.biosemi.com.\\n7. http://www.motu.com/products/motuaudio/8pre.\\nFig. 2. Natural expressions to a fearful (on the left) and disgusting (on the right) video. The snapshots of the stimuli videos with eye gaze overlaid\\nand without eye gaze overlaid, frontal captured video, raw physiological signals, and raw eye gaze data are shown. In the first row, the red circlesshow the fixation points and their radius indicates the time spent in each fixation point. The red lines indicate the moments where each of thesnapshots was captured.\\nthem allowed the recording of the actual sensor trigger\\nsignals, they required alternative synchronization strategies.\\nThe physiological data were captured with a multichannel\\nA/D converter (“a” in Fig. 3) that allowed recording one\\nbinary input signal alongside the data. This input was used to\\nconnect the camera trigger signal. Since the accurate timing\\nof each camera frame is known, this allowed synchronizing\\nthe physiological data with all the other modalities.\\nThe eye gaze tracker (“g” in Fig. 3) synchronized with\\nthe CPU cycle counter of its dedicated capture PC (“f”) with\\nan accuracy of approximately one millisecond. To synchro-\\nnize the respective CPU cycle counter to the audio interface,\\nwe developed an application that periodically (twice per\\nsecond) outputs binary time stamp signals with the current\\ntime, through the serial port output (see the third signal in\\nFig. 4), with an error below 10/C22s. To get a more accurate\\ntiming accuracy than the 1 ms accuracy of the time stamps\\nof the gaze tracking data, the infrared strobe illumination of\\nthe gaze tracker was recorded using a photo diode (“h” inFig. 3 and the fourth signal in Fig. 4). This allowed the\\ncorrection of the gaze data time stamps with the temporal\\nresolution as high as 10 microseconds.\\nThe start moments of the stimuli data were time stamped\\nusing the same synchronized CPU cycle counter as the eye-gaze data. An uncertainty in timing of the stimuli data is\\nintroduced by the video player software, as well as by the\\nlatency of the audio system, graphics card, and the screen.\\nFurthermore, the accuracy of the time codes of the\\nfragments may introduce further errors in synchronizing\\nthe recorded data with the actual stimuli. The room\\nmicrophone was placed close to the speaker that produced\\nthe stimuli sound. Therefore, the recorded ambient sound\\nprovides an implicit synchronization, as it includes the\\nsound of the stimuli.\\n4.3 Practical Considerations\\nAlthough the protocol and setup was done carefully,\\nproblems arose during recordings. The data recorded from\\nparticipants 9 and 15 are not complete due to technical\\nproblems. The physiological responses of participant 12 aremissing due to recording difficulties. The physiological\\nresponses to each stimuli were recorded each in a separate\\nfile in Biosemi data format (BDF) which is an extension of\\nEuropean data format (EDF) and easily readable in different\\nplatforms. For each trial, the response to the 15 seconds\\nneutral video is stored separately. All the files containingphysiological signals include the signals recorded 30 s\\nbefore the start and after the end of their stimuli. In\\naccordance with the Biosemi recording methodology, we\\ndid not record a reference electrode with EEG signals.Therefore, EEG signals need rereferencing to a virtual\\nreference, for example, the average reference. The stimuli\\nvideos were all encoded in MPEG-4 Xvid format and MPEGlayer three format with 44,100 Hz sampling frequency in an\\naudio video interleave container (AVI). The frames were\\nencoded in 1;280/C3800to match our display resolution.\\n5E MOTION RECOGNITION EXPERIMENT\\nIn this section, we present the emotion recognition experi-\\nmental paradigm, analysis methods, and experimentalresults. Three modalities, including peripheral and central\\nnervous system physiological signals and information\\ncaptured by eye gaze tracker, were used to recognize\\nemotions from participants’ responses.\\n5.1 Emotion Experiment Paradigm\\nThe participants were informed about the experiment, and\\ntheir rights, in a verbal introduction, by e-mail, and through\\na consent form. Participants were trained to use the\\ninterface before the experiment and during the setup time.The participants were also introduced to the meaning of\\narousal, valence, dominance, and predictability in the self-\\nassessment procedure, and to the nature of the videocontent. The five questions which were asked during self-\\nreporting were\\n1.emotional label/tag,\\n2.arousal,\\n3.valence,\\n4.dominance,\\n5.predictability [14].\\nThe emotional labels included neutral, anxiety, amusement,\\nsadness, joy, disgust, anger, surprise, and fear. To simplify\\nthe interface for the first experiment a keyboard wasprovided with only nine numerical keys and the partici-\\npants answered each question by pressing one of the keys.\\nQuestions 2 to 5 were on a nine point scale.\\nIn emotional-affective experiments the bias from the\\nemotional state needs to be reduced. For this purpose, a\\nshort neutral clip was shown to the participants before eachemotional video. The neutral clip was randomly selected\\nfrom the clips provided by the Stanford psychophysiology\\nlaboratory [18]. The 20 emotional video clips were played\\nfrom the data set in random order. After watching a video\\nclip, the participant filled in the self-assessment form. Intotal, the time interval between the start of a trial and the\\nend of the self-reporting phase was approximately two and\\nhalf minutes. This interval included playing the neutral clip,playing the emotional clip, and performing the self-\\nassessment. Running of the whole protocol took, on\\naverage, 50 minutes, in addition to 30 minutes set up time.\\n5.2 Emotional Features\\n5.2.1 EEG and Physiological Signals\\nThe following peripheral nervous system signals were\\nrecorded: GSR, respiration amplitude, skin temperature,SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 47\\nFig. 4. Five tracks recorded in parallel by MOTU 8pre audio interface.\\nFrom top to bottom: 1) room microphone, 2) head microphone,\\n3) serial port time stamp output (transmitted at 9,600 bps), showing\\n2 time stamp signals, 4) measured infrared light in front of eye tracker,5) camera trigger.\\nand ECG. Most of the current theories of emotion [29] agree\\nthat physiological activity is an important component of an\\nemotion. Heart rate and heart rate variability (HRV)\\ncorrelate with emotional changes. Pleasantness of stimulican increase peak heart rate response [26], and HRVdecreases with fear, sadnes s, and happiness [30]. In\\naddition to the HR and HRV features, spectral featuresderived from HRV were shown to be a useful feature in\\nemotion assessment [31]. Skin temperature was also\\nrecorded since it changes in different emotional states[32]. Regarding the respiration amplitude, slow respirationis linked to relaxation, while irregular rhythm, quickvariations, and cessation of respiration correspond to morearoused emotions like anger or fear [33], [30]. In total,102 features were extracted from peripheral physiological\\nresponses based on the proposed features in the literature\\n[34], [33], [30].\\nIn addition to the peripheral nervous system re-\\nsponses, electroencephalogram signals were acquired.The power spectral features were extracted from EEGsignals. The logarithms of the spectral power from theta(4H z <f< 8H z), slow alpha ( 8H z <f< 10 Hz ), alpha\\n(8H z <f< 12 Hz ), beta ( 12 Hz <f< 30 Hz ), and gamma\\n(30 Hz <f) bands were extracted from all 32 electrodes as\\nfeatures. In addition to power spectral features, thedifference between the spectral power of all the symme-trical pairs of electrodes on the right and left hemisphereswas extracted to measure the possible asymmetry in thebrain activities due to the valence of perceived emotion\\n[35], [36]. The asymmetry features were extracted from all\\nmentioned bands except slow alpha. The total number ofEEG features of a trial for 32 electrodes is 14/C24þ32/C2\\n5¼216features. The total number of EEG features of a\\ntrial for 32 electrodes is 216 features. (Table 4 lists thefeatures extracted from the physiological signals.)\\n5.2.2 Eye Gaze Data\\nAfter removing the linear trend, the power spectrum of thepupil diameter variation was computed. Standard deviationand spectral features were extracted from the pupil\\ndiameter. The Hippus effect is the small oscillations of the\\neye pupil diameter between 0.05 and 0.3 Hz and withamplitude of 1 mm [37], [38]. The Hippus effect has beenshown to be present when one is relaxed or passive. In thepresence of mental activity the effect will disappear. TheHippus effect is extracted by the first two power spectral\\nfeatures which are covering up to 0.4 Hz.\\nEye blinking was extracted by counting the number of\\ntimes when eye gaze data were not available, i.e., themoments when eyes were closed. The rate of eye blinking isshown to be correlated with anxiety. From the eye blinks,the eye blinking rate, the average and maximum blinkduration were extracted as features. In addition to the eyeblinking features the amount of time each participant spent\\nwith his/her eyes closed was also used as a feature to detect\\npossible eye closing due to unpleasant emotions.\\nAlthough participants were asked not to move during\\nexperiments, there were small head movements whichmanifested themselves in the distance between participants’eyes and the eye gaze tracker. The distance of participantsto the screen and its changes provide valuable information\\nabout participants’ posture. The total change in the distanceof a participant to the gaze tracker, gaze distance, was\\ncalculated to measure the possible approach and avoidance.\\nThe amount of time participants spent per trial getting closeto or far from the screen was computed as well. Thesefeatures were named approach and avoidance ratio to\\nrepresent the amount of time participants spent getting\\nclose to or going far from the screen. The frequency of theparticipants’ movement toward the screen during each trial,\\napproach rate, was also extracted. Approach and with-\\ndrawal are closely related to emotional experiences [39].\\nThe statistical features were extracted from eye gaze\\ncoordinates along the horizontal and vertical axes, namely,the standard deviation, Kurtosis, and skewness of horizon-tal and vertical projected eye gaze. Moreover, the power\\nspectral density in different bands was extracted to\\nrepresent different oscillations the eye gaze pattern (seeTable 5). These bands were empirically chosen based on the\\nspectral changes in eye gaze movements. Ultimately,\\n38 features were extracted from the eye gaze data. Thefeatures extracted from eye gaze data are listed in Table 5.\\n5.3 Rating Analysis\\nRegarding the self-reports, we computed the multiraterCohen’s kappa for different annotations. A fair agreementwas found on emotional keywords with the average\\n/C20¼0:32. For arousal and valence rating, the cross correlation\\nvalues were computed which was ( M¼0:40;SD ¼0:26) for48 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\nTABLE 4\\nThis Table Lists All 102 Features\\nExtracted from Physiological Signals\\nNumber of features extracted from each channel is given in brackets.\\narousal and ( M¼0:71;SD ¼0:12) for valence. The key-\\nword-based feedbacks were used to generate each partici-\\npant’s ground truth. The histograms of emotional self-reports’ keywords and ratings given to all videos are shownin Fig. 5. In Fig. 5, it is visible that the emotions which were\\nnot initially targeted (see Table 3) have the least frequencies.\\n5.4 Emotion Recognition Results\\nIn order to give the reader some baseline classification results,\\nemotion recognition results from three modalities and fusionof ebest modalities are presented. Two classification schemeswere defined: first, along the arousal dimension, three classesof calm, medium aroused, and excited, and second along the\\nvalence dimension, unpleasant, neutral valence and pleasant.\\nThe mapping between emotional keyword and classes whichare based on [14] and are given in Table 6.\\nA participant independent approach was taken to check\\nwhether we can estimate a new participant’s felt emotion\\nbased on others. For each video from the data set, the groundtruth was thus defined by the feedback given by each\\nparticipant individually. The keyword-based feedback was\\nthen translated into the defined classes. According to this\\ndefinition, we can name th ese classes calm, medium\\naroused, and excited/activated for arousal and unpleasant,\\nneutral valence, and pleasant for valence (see Table 6).\\nTo reduce the between participant differences, it is\\nnecessary to normalize the features. Each feature was\\nseparately normalized by mapping to the range ½0;1/C138on\\neach participant’s signals. In this normalization the mini-\\nmum value for any given feature is subtracted from the same\\nfeature of a participant and the results were divided by the\\ndifference between the maximum and minimum values.\\nA leave-one-participant-out cross validation technique\\nwas used to validate the user independent classification\\nperformance. At each step of cross validation, the samples of\\none participant were taken out as test set and the classifier\\nwas trained on the samples from the rest of the participants.\\nThis process was repeated for all participants’ data. An\\nimplementation of the SVM classifier from libSVM [40] withRBF kernel was employed to classify the samples using\\nfeatures from each of the three modalities. For the SVM\\nclassifier, the size of the kernel, /C13, was selected between\\n½0:01;10/C138, based on the average F1 score using a 20-fold cross\\nvalidation on the training set. The Cparameter that regulates\\nthe tradeoff between error minimization and margin\\nmaximization is empirically set to 1. Prior to classification,\\na feature selection was used to select discriminative features\\nas follows: First, a one-way ANOVA test was done on the\\ntraining set for each feature with the class as the independent\\nvariable. Then, any feature for which the ANOVA test was\\nnot significant ( p> 0:05) was rejected.\\nHere, we used three modalities which are peripheral\\nphysiological signals, EEG, and eye gaze data. From these\\nthree modalities, the results of the classification over the two\\nbest modalities were fused to obtain the multimodal fusion\\nresults. If the classifiers provide confidence measures on their\\ndecisions, combining decisions of classifiers can be doneusing a summation rule. The confidence measure summation\\nfusion was used due to its simplicity and its proven\\nperformance for emotion recognition according to [34].\\nThe data from the 27 participants which had enough\\ncompleted trials was used. Five hundred thirty-two samples\\nof physiological responses and gaze responses were\\ngathered over a potential data set of 27/C220¼540samples;SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 49\\nFig. 5. This bar chart shows the frequency of the emotional keywords\\nassigned to all videos.TABLE 6\\nThe Emotional Keywords Are Mapped into Three Classes\\non Arousal and ValenceTABLE 5\\nThis Table Lists the Features Extracted\\nfrom Eye Gaze Data for Emotion Recognition\\nNumber of features extracted from each channel is given in brackets.\\nthe eight missing ones were unavailable due to not having\\nenough valid samples in eye gaze data.\\nThe F1 scores and recognition rates for the classification\\nin different modalities are given in the Table 7 and Fig. 6.\\nIn Table 7, two random level results are also reported. Thefirst random level results represent a random classifier\\nwith uniform distribution, whereas the second random\\nclassifier (weighted) uses the training set distribution torandomly choose the class. The confusion matrices for eachmodality and their fusion show how they performed on\\neach emotion class (Table 8). In these confusion matrices\\nthe row represents the classified label and each columnrepresents the ground truth for those samples. For allcases, classification on gaze data performed better than\\nEEG and peripheral signals. This is due to the fact that eye\\ngaze is more correlated with the shown content and similarvisual features induce similar emotions. The peripheral\\nphysiological responses have a high variance between\\ndifferent participants which makes interparticipant classi-fication difficult to perform. Therefore, the classificationusing peripheral physiological features gave the worst\\nresults among these three modalities. This can be reduced\\nin future studies by using better methods to reduce thebetween participants’ variance. The high arousal, “acti-vated,” class was the most challenging class. While EEG\\nand peripheral physiological modalities were completely\\nunable to classify the samples, eye gaze also did not obtainits superior accuracy for this class (see Fig. 5). This might\\nhave been caused by the lower number of responses for the\\nemotions assigned to this class. The fusion of the two bestmodalities, eye gaze and EEG, ultimately outperformed all\\nsingle modalities.\\n6I MPLICIT TAGGING EXPERIMENT\\nIn this section, we describe the implicit tagging experiment\\nparadigm, analysis methods, and experimental results. Two\\nmodalities were used to predict the correctness of displayed\\ntags, namely, facial expression (captured by a camera) and\\nthe eye gaze location on the screen (captured by an eye gaze\\ntracker). The results presented in this section are limited tothe static images only. The sequences with tagged videos\\nwere not processed. In total, we analyzed 756 data\\nsequences of 27 participants with the goal of recovering\\nthe correctness of the displayed tags. The average sequence\\nlength was 5s. The method utilized for facial expression\\nanalysis and its results were previously published in [41].\\n6.1 Implicit Tagging Experiment Paradigm\\nIn this second experiment, 28 images and 14 video fragments\\nwere subsequently shown on their own and accompanied by\\na word tag. Once using a correct tag and once using an\\nincorrect tag. The videos were chosen from the Hollywood\\nhuman actions database (HOHA) [3] and were between 12\\nand 22 seconds long ( M¼17:6s;SD ¼2:2s). For each trial,\\nthe following procedure was taken:\\n1.Untagged Stimulus: The untagged stimulus was\\ndisplayed (still images were shown during 5 sec-onds). This allowed the participant to get to know thecontent of the image/video.\\n2.Tagged Stimulus: The same stimulus was displayed\\nwith tag (still images were shown during 5 seconds).The participants’ behavior in this period containedtheir reaction to the displayed tag.50 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\nFig. 6. This bar chart shows the F1 score for classification results of\\neach class from different modalities.TABLE 7\\nThe Recognition Rate and F1 Scores of\\nEmotion Recognition for Different Modalities and\\nFusion of the Two Best Modalities, EEG and Eye Gaze\\nTABLE 8\\nConfusion Matrices of Different Classification Schemes (Row: Classified Label; Column: Ground Truth)\\nThe numbers on the first row and the first column of tables (a), (b), (c), and (d) represent: 1. calm, 2. medium aroused, 3. activated, and for tables (e),\\n(f), (g), and (h) represent: 1. unpleasant 2. neutral valence 3. pleasant. The confusion matrices relate to classification using (a), (e) peripheralphysiological signals, (b), (f) EEG signals, (c), (g) eye gaze data, (d), (h) EEG and eye gaze decision-level fusion.\\n3.Question: A question was displayed on the screen to\\nask whether the participant agreed with the suggestedtag. Agreement or disagreement was expressed bypressing a green or a red button, respectively.\\nOnly the color video capturing the frontal view of partici-\\npants’ faces was used in the analysis (see Fig. 1a). The length\\nof each trial was about 11 seconds for images, and slightlymore than double the stimulus’ length in case of videos. Therecording of each participant was segmented in three sets of28 small clips, according to the order in which the images/\\nvideos were presented. Each fragment corresponds to the\\nperiod between the time point when the stimulus appearsand the point when the participant has given his/herfeedback. Running of the second experiment’s protocol took\\nin average 20 minutes, excluding the setup time.\\n6.2 Facial Expression Analysis\\nTo extract facial features, the Patras-Pantic particle filter [42]\\nwas employed to track 19 facial points. The initial positions\\nof the 19 tracked points were manually labeled for each\\nvideo and then automatically tracked for the rest of thesequence. After tracking, each frame of the video wasrepresented as a vector of the facial points’ 2D coordinates.\\nFor each frame, geometric features, f1 to f20, were then\\nextracted based on the positions of the facial points. Theextracted features are:\\n.Eyebrows : Angles between the horizontal line\\nconnecting the inner corners of the eyes and theline that connects inner and outer eyebrow (f1, f2),the vertical distances from the outer eyebrows to theline that connects the inner corners of the eyes (f3, f4)(see Fig. 7a).\\n.Eyes : Distances between the outer eyes’ corner and\\ntheir upper eyelids (f5, f9), distances between theinner eyes’ corner and their upper eyelid (f6, f10),distances between the outer eyes’ corner and their\\nlower eyelids (f8, f12), distances between the inner\\neyes’ corner and their lower eyelids (f7, f11), verticaldistances between the upper eyelids and the lowereyelids (f13, f14) (see Fig. 7b).\\n.Mouth : Distances between the upper lip and mouth\\ncorners (f15, f16), distances between the lower lipand mouth corners (f18, f18), distances between themouth corners (f19), vertical distance between theupper and the lower lip (f20) (see Fig. 7c).\\nThe line that connects the inner eye corners was used as a\\nreference line since the inner eye corners are stable facial\\npoints, i.e., changes in facial expression do not induce any\\nchanges in the position of these points. They are also thetwo most accurately tracked points. For each sequence,\\nthe listed 20 features were extracted for all the frames. Thedifference of these 20 features with their values in a neutral\\nframe was used in further processing.\\n6.3 Analysis of Eye Gaze\\nGaze fixations are the coordinates of the points on the\\ndisplay on which the eye gaze stayed fixed for a certainperiod of time. Each fixation is composed of its duration as\\nwell as the two-dimensional coordinates of the projection of\\nthe eye gaze on the screen. An example of an eye gazepattern and fixations points on an image is shown in Fig. 8.The features extracted from eye gaze are listed in Table 9.\\n6.4 Classification Methods\\nWe have chosen Hidden Markov Models (HMMs) toclassify the facial expression sequences according to thecorrectness of the displayed tags. HMMs have beencommonly used for modeling dynamic sequences. For a\\nmore detailed description of the utilized HMM framework\\nfor facial expression analysis, see the early work on facial-expression-based implicit tagging [41].\\nAs shown in [43], a temporal facial movement consists of\\nfour states:\\n1.neutral—there are no signs of muscular activation;\\n2.onset—the muscular contraction begins and in-\\ncreases in intensity;\\n3.apex—a plateau where the intensity reaches a stable\\nlevel;\\n4.offset—the relaxation of muscular action.\\nBased on these states, the number of modeled states in the\\nHMM was set to four. We chose the ergodic topology, inSOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 51\\nFig. 8. An example of displayed images is shown with eye gaze fixation\\nand scan path overlaid. The size of the circles represents the time spentstaring at each fixation point.\\nTABLE 9\\nThis Table Lists the 19 Features Extracted\\nfrom Eye Gaze Data for Implicit Tagging\\nFig. 7. The tracked points and features.\\nwhich all the states are connected with each other. This\\nmeans that it is possible to jump from any of the four statesto any other. The initial state and transition probabilitieswere randomly chosen. The emission probabilities were\\nmodeled by a mixture of two Gaussians with diagonal\\ncovariance matrices. For the implementation of the utilizedHMM, the HMM toolbox for MATLAB was used. For eachparticipant, two HMMs were trained: one for the partici-pant’s facial responses when the image with a correct tagwas displayed and the other for when the incorrect tag was\\nshown. The correctness of the displayed tags was predicted\\nby comparing the likelihood of these two HMMS. Theperformance of the classification was investigated by a10-fold cross validation.\\nAdaboost was employed for the classification of eye gaze\\ndata. The general idea of Adaboost is to combine a group of\\nweak classifiers to form a strong classifier. A set of classifierswere trained sequentially and then combined [44]. The latergenerated classifiers focused more on the mistakes of theearlier classifiers. Let ðx\\n1;y1Þ;ðx2;y2Þ;...;ðxn;ynÞdenote the\\ninstances we have. For the gaze data, each ~xiis a\\n19-dimensional feature vector and yi¼/C0 1;þ1is its asso-\\nciated label. The weak classifiers used in this paper aredecision stumps in the following form:\\ncðx; i; p; /C21 Þ¼1i f px\\ni<p/C21\\n/C01 otherwise ;/C26\\nð1Þ\\nin which i; p; /C21 are the parameters of a decision stump. iis the\\nfeature chosen, pis a polarity flag with value 1 or /C01,/C21is\\nthe decision threshold, xiis an instance, and xiis the value of\\ntheith feature of x. The decision stump simply chooses a\\nfeature from the instance and then compares its value to a\\nthreshold. The algorithm takes two parameters: the data set\\nand the number of iterations T. It generates a sequence of\\nweak classifiers and combines them with weights\\nHðxÞ¼X\\nt¼1/C11thtðxÞ: ð2Þ\\nAt each iteration, Adaboost chooses the decision stump\\nthat minimizes the weighted classification error, which is\\nequivalent to choosing the most discriminative feature.\\nThe weight for the weak classifier /C11t¼1\\n2lnð1/C0/C15t\\n/C15tÞdecreases\\nwhen the error increases. The weights are then updatedbased on the classification result. A larger weight will beassigned to the misclassified samples at each iteration.\\nThis increases the importance of the misclassified sample\\nin the next iteration. For the gaze data, 19 features werechosen to model the gaze pattern.\\n6.5 Facial Expression Results\\nAt the single participant level, the best predication rate was\\nnot better than 62 percent. This led to using a strategy to\\npredict the correctness of the displayed tag by the weightedsum of the predictions made from the behavior of differentparticipants. In this combination, the prediction rate on thetraining set was used as the combining weight of the\\nprediction based on a single participant. This strategy\\nattenuates the negative effect of participants with lessconsistent behavior in the measured features.The results of the fusion of the participants’ responses\\nare presented in Table 10. Nis the number of classifiers, of\\nwhich each was trained on a single participant’s responses.The classifiers whose weights are among the top Nare\\nfused using our proposed method.\\nThe best result is achieved by using only one classifier,\\nwhich means only using one participant’s data. Note thatthe results of N¼1andN¼2should always be the same\\nbecause the classifier with the second largest weight cannotchange a binary decision made by the top 1 classifier. Thebest result of 58 percent is worse than the best predictionrate on a single participant, which is 62 percent. The\\nprediction rate decreases as the number of participants\\nincreases, which implies that the number of effectiveparticipants is very limited.\\n6.6 Eye Gaze Results\\nUsing the gaze responses at the single participant level,\\nthe best prediction rate using Adaboost was 66 percent.Here, the same combination strategy was employed tocombine multiple participants’ responses to detect thecorrectness of tags.\\nThe result of combining Adaboosts from different\\nparticipants is presented in Table 10. Nis again the\\nnumber of participants used. When Nis equal to 1 or 2,\\nthe results are slightly lower than the best result on the\\nsingle participant. This might be due to the selection ofoverfitted participants from the training set. The perfor-mance of those ineffective participants can be regarded asnearly random guess.\\nAs noted previously, the results for N¼1,N¼2are the\\nsame. As Ngoes to 4, the prediction rate increases to\\n73 percent. Unlike the facial data, combining differentparticipants’ gaze responses improves the overall predic-tion rate. When Nis larger than 4, the result starts to\\ndeteriorate. This indicates that the number of effectiveparticipants is also limited here.\\n6.7 Modality Fusion\\nAfter combining the responses from different participants,the combined classifiers of facial expression and eye gazewere fused at the decision level using a weighted sum ofconfidence measures. The assigned weights were found\\nbased on the performance of fusion on the training set.\\nSince better results were obtained with gaze features, theweight of gaze confidence was set to one and the weight offacial analysis confidence was between ½0;1/C138.\\nThe prediction rate improved from 73.2 to 75 percent by\\ncombining the facial and gaze results. The best result wasachieved when the facial analysis confidence weight wasbetween 0.1 and 0.35, which gives us an estimate of therelative importance of the two modalities. These results52 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\nTABLE 10\\nPrediction Rate of Fusing HMM for Facial Expression\\nAnalysis and Fusing Adaboost for Eye Gaze Analysis\\nNis the number of combined classifiers.\\nshow that a participant’s facial and eye gaze responses\\nconvey information about the correctness of tags associated\\nwith multimedia content.\\n7D ISCUSSIONS AND RECOMMENDATIONS\\nTo our knowledge, MAHNOB-HCI is the first databasewhich has five modalities precisely synchronized, namely,\\neye gaze data, video, audio, and peripheral and centralnervous system physiological signals. This database can be\\nof interest to researchers in different fields, from psychol-\\nogy and affective computing to multimedia. In addition to\\nemotion recognition from a single modality or multiple\\nmodalities, the relations between simultaneous emotion-\\nrelated activity and behavior can be studied. The high\\naccuracy of synchronization of this database allows study-\\ning the simultaneous effects of emotions on EEG and other\\nmodalities, and fusing them in any desired way, fromdecision-level fusion (DLF) down to combined processing at\\nthe signal level. As the results reflect, not all recorded\\nmodalities are as correlated with the stimuli as the others.\\nFor example, there are not enough audio events and the\\nperipheral physiological signals do not give the same\\nemotion recognition results comparing to the eye gaze\\nand EEG signals.\\nIn any emotional experiment, having more trials gives\\nthe opportunity for single participant studies. At the same\\ntime, longer sessions make participants tired and unable to\\nfeel the stimuli emotions. Considering this tradeoff, we\\nfound that an upper limit of 20 video fragments was\\nacceptable. Although 20 videos are enough to compute\\nsignificant correlations, this number of samples is notsufficient for single-participant emotion recognition.\\nInducing emotions and recording affective reactions is a\\nchallenging task. Special attention needs to be paid to several\\ncrucial factors, such as stimuli that are used, the laboratory\\nenvironment, as well as the recruitment of participants. Our\\nexperience can be distilled into a list of recommendations\\nthat will enable the development of additional corpora to\\nproceed smoothly.\\nThe experiment environment in the laboratory should be\\nkept isolated from the outside environment. The partici-\\npants should not be able to see the examiners or hear noise\\nfrom the outside. The light and temperature should be\\ncontrolled to avoid variation in physiological reactions due\\nto uncontrolled parameters.\\nChoosing the right stimuli material is an important factor\\nin any affective study. They should be long enough to\\ninduce emotions and short enough to prevent boredom.\\nFurthermore, to be sure variation in stimuli length does notintroduce variance in the measurements between emotional\\nand nonemotional stimuli, we suggest the stimuli durations\\nbe equal. The mixture of contradicting emotions can make\\nproblems for self-assessments. We recommend using videos\\nwhich do not induce multiple emotions. A correct partici-\\npant recruitment can make a big difference in the results.\\nDue to the nature of affective experiments, a motivated\\nparticipant with the right knowledge for filling the\\nquestionnaire in is desirable. The rewards can make theparticipants more motivated and responsible. However,\\ncash compensation might attract participants who are notmotivated or lack desired communication skills. Therefore,\\nrewarded recruitment should be done by carefully con-\\nsidering the desired qualifications. Contact lenses usually\\ncause participants to blink more, which introduces a higher\\nlevel of artifacts on EEG signals. Therefore, participants\\nwith visual correction should avoid using contact lenses as\\nmuch as possible. Thick glasses affect the eye gaze trackerperformance. In the experiments in which both these\\nmodalities are recorded, recruiting participants with no\\nvisual correction is advised. Properly attending to partici-\\npants takes an important part of one’s attention, which can\\neasily lead to forgetting parts of complicated technical\\nprotocols. Therefore, operation of the recording equipment\\nduring the experiments should be made as simple as\\npossible (preferably just by pressing a single button).\\nAlternatively, the tasks of controlling and monitoring\\ncorrect data collection and attending to participants can\\nbe divided between multiple laboratory assistants with\\ncarefully defined procedures.\\n8C ONCLUSIONS\\nA multimodal affective database has been recorded and\\nmade available to the affective computing community. Thelarge collection of modalities recorded (multicamera video\\nof face, head, speech, eye gaze, pupil size, ECG, GSR,\\nrespiration amplitude, and skin temperature) and the highsynchronization accuracy between them makes this data-base a valuable contribution to the ongoing development\\nand benchmarking of emotion-related algorithms that\\nexploit data fusion, as well as to studies on humanemotion and emotional expression. Emotion recognitionand implicit tagging results from different modalities set a\\nbaseline result for researchers who are going to use the\\ndatabase in the future.\\nACKNOWLEDGMENTS\\nThe work of Soleymani and Pun was supported in part by\\nthe Swiss National Science Foundation and in part by the\\nEuropean Community’s Seventh Framework Programme\\n(FP7/2007-2011) under grant agreement Petamedia\\nno 216444. The data acquisition part of this work and the\\nwork of Pantic and Lichtenauer were supported by the\\nEuropean Research Council under the ERC Starting Grant\\nagreement no. ERC-2007-StG-203143 (MAHNOB). The\\nauthors would like to thank J. Dobo /C20s, Prof. D. Grandjean,\\nand Dr. G. Chanel for their valuable scientific contributions\\nto the experiments’ protocol. They also acknowledge the\\ncontributions of J. Jiao for the analysis on the implicit\\ntagging.\\nREFERENCES\\n[1] M. Pantic and A. Vinciarelli, “Implicit Human-Centered Tagging,”\\nIEEE Signal Processing Magazine, vol. 26, no. 6, pp. 173-180, Nov.\\n2009.\\n[2] Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang, “A Survey of\\nAffect Recognition Methods: Audio, Visual, and SpontaneousExpressions,” IEEE Trans. Pattern Analysis and Machine Intelligence,\\nvol. 31, no. 1, pp. 39-58, Mar. 2009.\\n[3] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld, “Learning\\nRealistic Human Actions from Movies,” Proc. IEEE Conf. Computer\\nVision and Pattern Recognition, pp. 1-8, June 2008.SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 53\\n[4] J.A. Healey and R.W. Picard, “Detecting Stress during Real-World\\nDriving Tasks Using Physiological Sensors,” IEEE Trans. Intelligent\\nTransportation Systems, vol. 6, no. 2, pp. 156-166, June 2005.\\n[5] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, “Web-Based\\nDatabase for Facial Expression Analysis,” Proc. IEEE Int’l Conf.\\nMultimedia and Expo, pp. 317-321, 2005.\\n[6] E. Douglas-Cowie, R. Cowie, I. Sneddon, C. Cox, O. Lowry, M.\\nMcRorie, J.-C. Martin, L. Devillers, S. Abrilian, A. Batliner, N.\\nAmir, and K. Karpouzis, “The HUMAINE Database: Addressingthe Collection and Annotation of Naturalistic and InducedEmotional Data,” Proc. Second Int’l Conf. Affective Computing and\\nIntelligent Interaction, A. Paiva et al., pp. 488-500, 2007.\\n[7] M. Grimm, K. Kroschel, and S. Narayanan, “The Vera am Mittag\\nGerman Audio-Visual Emotional Speech Database,” Proc. IEEE\\nInt’l Conf. Multimedia and Expo, pp. 865-868, Apr. 2008.\\n[8] G. McKeown, M.F. Valstar, R. Cowie, and M. Pantic, “The\\nSEMAINE Corpus of Emotionally Coloured Character Interac-tions,” Proc. IEEE Int’l Conf. Multimedia and Expo, pp. 1079-1084,\\nJuly 2010.\\n[9] S. Koelstra, C. Mu ¨hl, M. Soleymani, A. Yazdani, J.-S. Lee, T.\\nEbrahimi, T. Pun, A. Nijholt, and I. Patras, “DEAP: A Database forEmotion Analysis Using Physiological Signals,” IEEE Trans.\\nAffective Computing, vol. 3, no. 1, pp. 18-31, Jan.-Mar. 2012.\\n[10] M.F. Valstar and M. Pantic, “Induced Disgust, Happiness and\\nSurprise: An Addition to the MMI Facial Expression Database,”Proc. Int’l Conf. Language Resources and Evaluation, WorkshopEMOTION, pp. 65-70, May 2010.\\n[11] E. Douglas-cowie, R. Cowie, and M. Schro ¨der, “A New Emotion\\nDatabase: Considerations, Sources and Scope,” Proc. ISCA Int’l\\nTechnical Research Workshop Speech and Emotion, pp. 39-44, 2000.\\n[12] J.A. Russell, “Culture and the Categorization of Emotions,”\\nPsychological Bull., vol. 110, no. 3, pp. 426-450, 1991.\\n[13] J.A. Russell and A. Mehrabian, “Evidence for a Three-Factor\\nTheory of Emotions,” J. Research in Personality, vol. 11, no. 3,\\npp. 273-294, Sept. 1977.\\n[14] J.R.J. Fontaine, K.R. Scherer, E.B. Roesch, and P.C. Ellsworth, “The\\nWorld of Emotions Is Not Two-Dimensional,” Psychological\\nScience, vol. 18, no. 12, pp. 1050-1057, 2007.\\n[15] M. Soleymani, J. Davis, and T. Pun, “A Collaborative Personalized\\nAffective Video Retrieval System,” Proc. Third Int’l Conf. Affective\\nComputing and Intelligent Interaction and Workshops, Sept. 2009.\\n[16] J.D. Morris, “Observations: Sam: The Self-Assessment Manikin;\\nAn Efficient Cross-Cultural Measurement of Emotional Re-sponse,” J. Advertising Research, vol. 35, no. 8, pp. 63-38, 1995.\\n[17] A. Schaefer, F. Nils, X. Sanchez, and P. Philippot, “Assessing the\\nEffectiveness of a Large Database of Emotion-Eliciting Films: A\\nNew Tool for Emotion Researchers,” Cognition and Emotion,\\nvol. 24, no. 7, pp. 1153-1172, 2010.\\n[18] J. Rottenberg, R.D. Ray, and J.J. Gross, “Emotion Elicitation Using\\nFilms,” Handbook of Emotion Elicitation and Assessment, series in\\naffective science, pp. 9-28, Oxford Univ. Press, 2007.\\n[19] The Psychology of Facial Expression, J. Russell and J. Fernandez-Dols,\\neds. Cambridge Univ. Press, 1997.\\n[20] D. Keltner and P. Ekman, Facial Expression of Emotion, second ed.,\\npp. 236-249. Guilford Publications, 2000.\\n[21] T. Kanade, J.F. Cohn, and T. Yingli, “Comprehensive Database for\\nFacial Expression Analysis,” Proc. IEEE Fourth Int’l Conf. Automatic\\nFace and Gesture Recognition, pp. 46-53, 2000.\\n[22] M. Pantic and L.J.M. Rothkrantz, “Toward an Affect-Sensitive\\nMultimodal Human-Computer Interaction,” Proc. IEEE, vol. 91,\\nno. 9, pp. 1370-1390, Sept. 2003.\\n[23] S. Petridis and M. Pantic, “Is This Joke Really Funny? Judging the\\nMirth by Audiovisual Laughter Analysis,” Proc. IEEE Int’l Conf.\\nMultimedia and Expo, pp. 1444-1447, 2009.\\n[24] M.M. Bradley, Miccoli, Laura, Escrig, A. Miguel, Lang, and J.\\nPeter, “The Pupil as a Measure of Emotional Arousal andAutonomic Activation,” Psychophysiology, vol. 45, no. 4, pp. 602-\\n607, July 2008.\\n[25] T. Partala and V. Surakka, “Pupil Size Variation as an Indication of\\nAffective Processing,” Int’l J. Human-Computer Studies, vol. 59,\\nnos. 1/2, pp. 185-198, 2003.\\n[26] P.J. Lang, M.K. Greenwald, M.M. Bradley, and A.O. Hamm,\\n“Looking at Pictures: Affective, Facial, Visceral, and BehavioralReactions,” Psychophysiology, vol. 30, no. 3, pp. 261-273, 1993.\\n[27] R. Adolphs, D. Tranel, and A.R. Damasio, “Dissociable Neural\\nSystems for Recognizing Emotions,” Brain and Cognition, vol. 52,\\nno. 1, pp. 61-69, June 2003.[28] J. Lichtenauer, J. Shen, M. Valstar, and M. Pantic, “Cost-Effective\\nSolution to Synchronised Audio-Visual Data Capture UsingMultiple Sensors,” technical report, Imperial College London,2010.\\n[29] D. Sander, D. Grandjean, and K.R. Scherer, “A Systems Approach\\nto Appraisal Mechanisms in Emotion,” Neural Networks, vol. 18,\\nno. 4, pp. 317-352, 2005.\\n[30] P. Rainville, A. Bechara, N. Naqvi, and A.R. Damasio, “Basic\\nEmotions Are Associated with Distinct Patterns of Cardiorespira-tory Activity,” Int’l J. Psychophysiology, vol. 61, no. 1, pp. 5-18, July\\n2006.\\n[31] R. McCraty, M. Atkinson, W.A. Tiller, G. Rein, and A.D. Watkins,\\n“The Effects of Emotions on Short-Term Power Spectrum Analysisof Heart Rate Variability,” The Am. J. Cardiology, vol. 76, no. 14,\\npp. 1089-1093, 1995.\\n[32] R.A. McFarland, “Relationship of Skin Temperature Changes to\\nthe Emotions Accompanying Music,” Applied Psychophysiology and\\nBiofeedback, vol. 10, pp. 255-267, 1985.\\n[33] J. Kim and E. Andre ´, “Emotion Recognition Based on Physiolo-\\ngical Changes in Music Listening,” IEEE Trans. Pattern Analysis\\nand Machine Intelligence, vol. 30, no. 12, pp. 2067-2083, Dec. 2008.\\n[34] G. Chanel, J.J.M. Kierkels, M. Soleymani, and T. Pun, “Short-Term\\nEmotion Assessment in a Recall Paradigm,” Int’l J. Human-\\nComputer Studies, vol. 67, no. 8, pp. 607-627, Aug. 2009.\\n[35] S.K. Sutton and R.J. Davidson, “Prefrontal Brain Asymmetry: A\\nBiological Substrate of the Behavioral Approach and InhibitionSystems,” Psychological Science, vol. 8, no. 3, pp. 204-210, 1997.\\n[36] R.J. Davidson, “Affective Neuroscience and Psychophysiology:\\nToward a Synthesis,” Psychophysiology, vol. 40, no. 5, pp. 655-665,\\nSept. 2003.\\n[37] V.F. Pamplona, M.M. Oliveira, and G.V.G. Baranoski, “Photo-\\nrealistic Models for Pupil Light Reflex and Iridal Pattern\\nDeformation,” ACM Trans. Graphics, vol. 28, no. 4, pp. 1-12, 2009.\\n[38] H. Bouma and L.C.J. Baghuis, “Hippus of the Pupil: Periods of\\nSlow Oscillations of Unknown Origin,” Vision Research,\\nvol. 11,\\nno. 11, pp. 1345-1351, 1971.\\n[39] R.J. Davidson, P. Ekman, C.D. Saron, J.A. Senulis, and W.V.\\nFriesen, “Approach-Withdrawal and Cerebral Asymmetry: Emo-\\ntional Expression and Brain Physiology I,” J. Personality and Social\\nPsychology, vol. 58, no. 2, pp. 330-341, 1990.\\n[40] C.-C. Chang and C.-J. Lin, “LIBSVM: A Library for Support Vector\\nMachines,” Science, vol. 2, pp. 1-39, 2001.\\n[41] J. Jiao and M. Pantic, “Implicit Image Tagging via Facial\\nInformation,” Proc. Second Int’l Workshop Social Signal Processing,\\npp. 59-64, 2010.\\n[42] I. Patras and M. Pantic, “Particle Filtering with Factorized\\nLikelihoods for Tracking Facial Features,” Proc. IEEE Sixth Int’l\\nConf. Automatic Face and Gesture Recognition, pp. 97-102, May 2004.\\n[43] S. Petridis, H. Gunes, S. Kaltwang, and M. Pantic, “Static vs.\\nDynamic Modeling of Human Nonverbal Behavior from\\nMultiple Cues and Modalities,” Proc. Int’l Conf. Multimodal\\nInterfaces, pp. 23-30, 2009.\\n[44] Y. Freund and R.E. Schapire, “A Decision-Theoretic General-\\nization of On-Line Learning and an Application to Boosting,” Proc.\\nEuropean Conf. Computational Learning Theory, pp. 23-37, 1995.\\nMohammad Soleymani received both the\\nBSc and MSc degrees from the Departmentof Electrical and Computer Engineering, Uni-\\nversity of Tehran, Iran. He is now a doctoral\\nstudent and research assistant in the Compu-ter Vision and Multimedia Laboratory, Compu-ter Science Department, University of Geneva,Switzerland. His research interests includeaffective computing and multimedia informationretrieval. He is a member of the IEEE and the\\nHUMAINE association.54 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 3, NO. 1, JANUARY-MARCH 2012\\n\\nJeroen Lichtenauer received the MSc and PhD\\ndegrees in electrical engineering from DelftUniversity of Technology, Delft, The Nether-lands, in 2003 and 2009, respectively. He is aresearch associate with the Intelligent Behaviour\\nUnderstanding Group, Department of Comput-\\ning, Imperial College London, London, UnitedKingdom. His research interests include real-timesignal processing, real-time computer vision.\\nThierry Pun received the EE Engineering\\ndegree in 1979 and the PhD degree in 1982.\\nHe is a full professor and the head of the\\nComputer Vision and Multimedia Laboratory,Computer Science Department, University ofGeneva, Switzerland. He has authored orcoauthored more than 300 full papers as wellas eight patents in various aspects of multimediaprocessing; his current research interests lie in\\naffective computing and multimodal interaction.\\nHe is a member of the IEEE.Maja Pantic is a professor in affective and\\nbehavioural computing at Imperial College Lon-don, Department of Computing, United King-dom, and at the University of Twente,Department of Computer Science, The Nether-\\nlands. She has received various awards for her\\nwork on automatic analysis of human behavior,including the European Research Council Start-ing Grant Fellowship and the Roger NeedhamAward 2011. She currently serves as the editor\\nin chief of Image and Vision Computing Journal and as an associate\\neditor for both the IEEE Transactions on Systems, Man, and\\nCybernetics Part B and the IEEE Transactions on Pattern Analysis\\nand Machine Intelligence . She is a fellow of the IEEE.\\n.For more information on this or any other computing topic,\\nplease visit our Digital Library at www.computer.org/publications/dlib.SOLEYMANI ET AL.: A MULTIMODAL DATABASE FOR AFFECT RECOGNITION AND IMPLICIT TAGGING 55\\n\\n',\n",
       " '682 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010\\nA Natural Visible and Infrared Facial\\nExpression Database for Expression\\nRecognition and Emotion Inference\\nShangfei Wang , Member, IEEE , Zhilei Liu, Siliang Lv, Yanpeng Lv, Guobing Wu, Peng Peng, Fei Chen, and\\nXufa Wang\\nAbstract— To date, most facial expression analysis has been\\nbased on visible and posed expression databases. Visible images,\\nhowever, are easily affected by illumination variations, while\\nposed expressions differ in appearance and timing from naturalones. In this paper, we propose and establish a natural visibleand infrared facial expression database, which contains bothspontaneous and posed expressions of more than 100 subjects,recorded simultaneously by a visible and an infrared thermalcamera, with illumination provided from three different direc-tions. The posed database includes the apex expressional imageswith and without glasses. As an elementary assessment of theusability of our spontaneous database for expression recogni-tion and emotion inference, we conduct visible facial expressionrecognition using four typical methods, including the eigenfaceapproach [principle component analysis (PCA)], the ﬁsherfaceapproach [PCA + linear discriminant analysis (LDA)], the ActiveAppearance Model (AAM), and the AAM-based + LDA. We alsouse PCA and PCA+LDA to recognize expressions from infraredthermal images. In addition, we analyze the relationship betweenfacial temperature and emotion through statistical analysis. Ourdatabase is available for research purposes.\\nIndex Terms— Emotion inference, expression recognition, facial\\nexpression, infrared image, spontaneous database, visible image.\\nI. I NTRODUCTION\\nFACIAL expression is a convenient way for humans to\\ncommunicate emotion. As a result, research on expres-\\nsion recognition has become a key focus area of personalized\\nhuman-computer interaction [1], [2]. Most current research\\nfocuses on visible images or videos and good performance hasbeen achieved in this regard. Whereas varying light exposure\\ncan hinder visible expression recognition, infrared thermal\\nimages, recording the temperature distribution formed by face\\nvein branches, are not sensitive to imaging conditions. Thus,\\nthermal expression recognition is a useful and necessary com-plement to visible expression recognition [3]. Besides, a change\\nManuscript received December 13, 2009; revised March 24, 2010 and June\\n23, 2010; accepted June 24, 2010. Date of publication July 26, 2010; date of\\ncurrent version October 15, 2010. This paper is supported in part by National\\n863 Program (2008AA01Z122), in part by Anhui Provincial Natural Science\\nFoundation (No.070412056), and in part bySRF for ROCS, SEM. The associate\\neditor coordinating the review of this manuscript and approving it for publica-\\ntion was Dr. Caifeng Shan.\\nThe authors are with the School of Computer Science and Technology,\\nUniversity of Science and Technology of China, Hefei 230027, China (e-mail:\\nsfwang@ustc.edu.cn; leivo@mail.ustc.edu.cn; lsliang@mail.ustc.edu.cn;\\nlvyp@mail.ustc.edu.cn; guobing@mail.ustc.edu.cn; dbpeng@mail.ustc.\\nedu.cn; feichen@mail.ustc.edu.cn; xfwang@ustc.edu.cn).\\nDigital Object Identiﬁer 10.1109/TMM.2010.2060716in facial temperature is a clue that can prove helpful in emotion\\ninference [4], [5]. Furthermore, most existing research has beenbased on posed expression databases, which are elicited by\\nasking subjects to perform a series of emotional expressions\\nin front of a camera. These artiﬁcial expressions are usually\\nexaggerated. Spontaneous expressions, on the other hand, may\\nbe subtle and differ from posed ones both in appearance andtiming. It is, therefore, most important to establish a natural\\ndatabase to allow research to move from artiﬁcial to natural\\nexpression recognition, ultimately leading to more practical\\napplications thereof.\\nThis paper proposes and establishes a natural visible and in-\\nfrared facial expression database (NVIE) for expression recog-\\nnition and emotion inference. First, we describe in detail the de-\\nsign, collection, and annotation of the NVIE database. In addi-\\ntion, we conduct facial expression analysis on spontaneous vis-\\nible images with front lighting using several typical methods,including the eigenface approach [principle component anal-\\nysis (PCA)], the ﬁsherface approach [PCA + linear discriminant\\nanalysis (LDA)], the Active Appearance Model (AAM), and\\nthe combined AAM-based + LDA (referred to as AAM+LDA).\\nThereafter, we use PCA and PCA +LDA to recognize expres-\\nsions from spontaneous infrared thermal images. In addition, we\\nanalyze the relationship between facial temperature and emo-tion through an analysis of variance (ANOV A). The evaluation\\nresults verify the effectiveness of our spontaneous database for\\nexpression recognition and emotion inference.\\nII. B\\nRIEF REVIEW OF EXISTING NATURAL\\nAND INFRARED DATABASES\\nThere are many existing databases dealing with facial ex-\\npressions, an exhaustive survey of which is given in [1] and [6].\\nHere, we only focus on natural and infrared facial expressiondatabases. Due to the difﬁculty of eliciting affective displaysand the time-consuming manual labeling of spontaneous ex-\\npressions, only a few natural visible expression databases\\nexist. These are listed in Table I, together with details of size,elicitation method, illumination, expression descriptions, and\\nmodality [visual (V) or audiovisual (A V)]. From Table I, we\\ncan see that researchers use one of three possible approaches toobtain spontaneous affective behavior, i.e., human-human con-versation, human-computer interaction, or emotion-inducing\\nvideos [22]. Since this paper only focuses on facial expressions,\\nand not speech or language, using emotion-inducing videos is\\n1520-9210/$26.00 © 2010 IEEE\\nWANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 683\\nTABLE I\\nDATABASES OF NATURAL FACIAL EXPRESSIONS\\nTABLE II\\nDATABASES OF INFRARED FACIAL EXPRESSIONS\\na suitable approach, especially as the datasets will not include\\nany facial changes caused by speech.\\nBecause expression recognition in the thermal infrared\\ndomain has received relatively little attention compared withrecognition in visible-light imagery, no thermal expressiondatabases exist. There are, however, a few thermal face\\ndatabases (listed in Table II) that include some posed thermal\\nexpression images. For each database, we provide informa-tion on the size, wave band, elicitation method, illumination,thermal information, and expression descriptions.\\nBased on Tables I and II, it is obvious that current natural\\nexpression databases focus only on visible spectrum imagery,while the existing infrared databases consist only of posedimages. Therefore, we propose and establish a natural visible\\nand infrared facial expression database, enriching the existing\\ndatabases for expression recognition and emotion inference.\\nIII. I\\nMAGE ACQUISITION SETUP\\nTo set up the database, we ﬁrst built a photographic room.\\nSince videos are used to induce the subjects’ emotions, we chosea quiet room as the experiment environment to ensure that the ef-fect of the screened videos was not compromised. The room was\\n9.6 m*4.0 m*3.0 m, with a single door and two windows. The\\nfacial expression recording system included a camera system,illumination system, glasses, and thermometer. The details ofthe system, depicted in Fig. 1, are described below.\\nA. Camera Setup\\nTo record both visible and infrared videos, we used two\\ncameras: a DZ-GX25M visible camera capturing 30 frames\\nper second, with resolution 704*480, and a SAT-HY6850 in-\\nfrared camera capturing 25 frames per second, with resolution320*240 and wave band 8–14\\n. The lenses of the two cam-\\neras were placed 0.75 m in front of the subject, with a distance\\nof 0.1 m between the two cameras. The cameras were placed\\nat a height of 1.2 m above the ground. To begin the process,both the video and the two cameras were turned on at the sametime. Then, a sun visor was placed in front of both cameras,\\nFig. 1. Design of the photographic room.\\nand removed very quickly from above. Thus, the removal of thesun visor was recorded in both the visible and thermal videos.Before segmenting the emotional frames, frames containingevidence of the sun visor were carefully compared to ﬁnd\\ntwo frames in which the position of the sun visor was almost\\nthe same. Appropriate time stamps were calculated for each,allowing the remaining parts of the visible and infrared videosto start simultaneously.\\nTo prevent any discomfort and to guarantee that the emotion\\nwas spontaneous, we did not require subjects to keep their headsﬁxed in one position. Once the screening had started, the exper-imenter stayed in the room to supervise the whole process.\\nTemperature information recorded by the infrared camera\\ngradually becomes inaccurate after a long period of recordingdue to the increased heat of the sensor. Thus, timely calibration\\nis needed. To minimize any disruption to the subject and to\\nprevent any loss of frames, the experimenters manually invokedthe auto-calibration before each experiment and while neutralvideos (depicted in Fig. 2) were being screened.\\nB. Illumination Setup\\nThis system controls the indoor light and the different direc-\\ntions of the light source. The indoor lighting was controlled bydaylight lamps on the ceiling. Both the door and the curtains\\n684 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010\\nFig. 2. Video clips.\\nwere kept closed during the screening of the videos, keeping\\nthe lighting difference between day and night as small as pos-sible. Another three ﬂoor lamps were used to create the dif-ferent illumination conditions. The distance between the lamps\\nand the subject was one meter. As shown in Fig. 1, the front\\nlamp was placed in front of the subject, while the left and rightlamps were at 45 degree angles to the front lamp. Bearing inmind that bright lights may cause the subjects some discomfort,\\n36-W lamp bulbs were used. To create front illumination, all\\nthree lamps were turned on, whereas for left or right illumina-tion, the front lamp, together with either the left or right lamp,respectively, was turned on.\\nC. Glasses\\nWhen performing facial expression recognition, the wearing\\nof glasses can disguise important information. To make theNVIE database more practical, we added facial expressionswith glasses. In the spontaneous experiment, subjects were not\\nspeciﬁcally requested to wear glasses, but could do so if they\\nchose to. In the posed experiment, all subjects were required topose for two complete sets of expressions under each lightingcondition. In the ﬁrst set, subjects wore glasses, whereas in\\nthe other, they posed without glasses. Subjects could choose to\\nwear their own glasses or the pair we provided.\\nD. Environmental Temperature\\nAlthough thermal emissivity from the facial surface is rela-\\ntively stable under illumination variations, it is sensitive to the\\ntemperature of the environment. We, therefore, recorded the\\ntemperature of the room during the experiments. Room temper-ature ranged between 18 and\\n, with a mean of\\n and\\na standard deviation of\\n .\\nIV . D ATAACQUISITION\\nA. Subjects\\nA total of 215 healthy students (157 males and 58 females),\\nranging in age from 17 to 31, participated in our experiments.\\nAll of the subjects had normal auditory acuity and were men-\\ntally stable. Each signed an informed consent before the exper-iment and received compensation for participating after com-pleting the experiment. Each subject participated in three spon-\\ntaneous experiments, for each of the three illumination condi-\\ntions, and one posed experiment. However, some of the subjectscould only express two or fewer expressions and some thermal\\nand visible videos were lost. Ultimately, for the spontaneous\\ndatabase, we obtained images of 105 subjects under front illu-mination, 111 subjects under left illumination, and 112 subjectsunder right illumination, while 108 subjects contributed to the\\nposed database.B. Stimuli\\nIn our experiments, we induced the subjects’ emotions by\\nscreening deliberately selected emotional videos. All the videos\\nwere obtained from the Internet, including 13 happy, 8 angry,\\n45 disgusted, 6 fearful, 7 sad, 7 surprised, and 32 emotionallyneutral videos, as judged by the authors. Each emotional videowas about 3–4 min long, while neutral videos were about 1–2\\nmin long.\\nC. Experiment Procedure\\nTwo kinds of facial expressions were recorded during our ex-\\nperiments: spontaneous expressions induced by the ﬁlm clips\\nand posed ones obtained by asking the subjects to perform a se-ries of expressions in front of the cameras. Each kind of facialexpression was recorded under three different illumination con-\\nditions, namely left, front, and right illumination.\\nDetails of this procedure are given below.First, the subjects were given an introduction to the experi-\\nmental procedure, the meaning of arousal and valence, and how\\nto self-assess their emotions.\\nSecond, the subjects seated themselves comfortably, and then\\nwe moved their chairs forward or backwards to ensure a distanceof 0.5 m between the chair and the screen.\\nThird, a 19-inch LCD screen was used to display the video\\nclips, each of which contained six segments (as depicted inFig. 2), corresponding to the six types of emotional videos. Ineach segment, a particular type of emotional video was played\\nand the subject’s expressions recorded as synchronous videos.\\nTo reduce the interaction of different emotions induced by thedifferent emotional video clips, neutral clips were shown be-\\ntween segments. Meanwhile, subjects were also asked to re-\\nport the real emotion experienced by considering emotional va-lence, arousal, the basic emotion category, and its intensity on aﬁve-point scale. These self-reported data were used as the emo-\\ntion label for the recorded videos.\\nFourth, once the video had terminated, each subject was asked\\nto display six expressions, namely happiness, sadness, surprise,fear, anger, and disgust, both with and without glasses.\\nV. D\\nESIGN OF THE NVIE D ATABASE\\nFirst, we manually ﬁnd the onset and apex of an expression in\\nthe visible facial videos. Then both visible and thermal videos\\nduring these periods are segmented into frames. Thus, our NVIEdatabase includes two sub-databases: a spontaneous databaseconsisting of image sequences from onset to apex and a posed\\ndatabase consisting of apex images, with each containing both\\nthe visible and infrared images that were recorded simultane-ously and under three different illumination conditions, namely,illumination from the left, front, and right. The posed database\\nalso includes expression images with and without glasses.\\nBecause it is difﬁcult to determine automatically what kind\\nof facial expression will be induced by a certain emotional ﬁlm,\\nﬁve students in our lab manually labeled all the visible apex\\nfacial images in the spontaneous database according to the in-tensity of the six categories (happiness, sadness, surprise, fear,anger, and disgust), arousal, and valence on a three-point scale.\\nThe category with the highest average intensity was used as the\\nWANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 685\\nFig. 3. Example images of a subject showing posed and spontaneous anger.\\nFig. 4. Visual and infrared images of a subject expressing anger.\\nFig. 5. Example images of a subject expressing happiness with illumination\\nfrom the left, front, and right.\\nlabel for the visible and thermal apex facial images. The average\\narousal and valence were also adopted as labels. The kappa co-efﬁcient of the labeling was 0.65, indicating a good consistency.\\nAs we know, expression is not emotion. For example, we may\\nfeel sad, but our expression can be neutral. The data collectedfrom the self assessment reports, described in Section IV-C,were used to provide an emotional annotation of the corre-\\nsponding image sequences.\\nThe following sections describe the variations in our database\\nand present some example images.\\nA. Posed versus Spontaneous\\nBoth posed and spontaneous expressions were recorded in\\nour database, allowing researchers to investigate differences fur-ther. Fig. 3 shows an example of a posed and spontaneous facial\\nexpression in the database.\\nB. Infrared versus Visible\\nBoth visible and infrared images were collected. Fig. 4 shows\\nan expression of anger in both a visible and infrared image.\\nC. Illumination Direction\\nUsing the light system described in Section III-B, we captured\\nthe facial expressions with illumination from three different di-rections. Fig. 5 shows the images captured under the different\\nillumination conditions.\\nFig. 6. Example images of a subject with and without glasses, expressing hap-\\npiness.\\nFig. 7. Images of a subject showing the six expressions (happiness, disgust,fear, surprise, sadness, and anger).\\nD. Glasses\\nImages of a subject with and without glasses are shown in\\nFig. 6.\\nE. Six Facial Expressions\\nIn the posed database, each subject displayed six facial ex-\\npressions. In the spontaneous emotion database, the expressionswere evaluated by ﬁve experimenters. Fig. 7 shows example im-ages of the six spontaneous facial expressions.\\nVI. V\\nALIDATION AND EV ALUATION OF THE DATABASE\\nThe main aim of this section is to present an elementary as-\\nsessment of the usability of the spontaneous sub-database with\\nfront lighting for expression recognition and emotion inference,\\nand to provide reference evaluation results for researchers usingthe database. We conduct visible facial expression recognitionusing four typical methods, namely the PCA, PCA+LDA,\\nAAM, and AAM+LDA. We also use PCA and PCA +LDA to\\nrecognize expressions from infrared thermal images. In addi-tion, we analyze the relationship between facial temperatureand emotion through statistical analysis.\\nNot all subjects displayed all six types of emotion we aimed\\nto elicit. Thus, only three expressions (i.e., disgust, fear, andhappiness), which were induced successfully in most cases,are used for expression recognition. Furthermore, since we did\\nnot restrict head movement in our data acquisition experiment,\\n686 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010\\nFig. 8. Face labeling (a) with and (b) without face conﬁguration.\\nthereby ensuring more natural expressions, some sequences\\nwith non-frontal facial images were discarded. Ultimately,236 apex images of 84 subjects were selected for visible and\\nthermal expression recognition, including 83 images depicting\\ndisgust, 62 images depicting fear, and 91 images depictinghappiness. In addition, the expressions were divided into fourclasses in the arousal-valence space: arousal positive/valence\\n, arousal positive/valence\\n ,\\narousal negative/valence\\n , and arousal nega-\\ntive/valence\\n . Of the 236 images, 100 images\\nwere classiﬁed as\\n , 132 images as\\n , 0 images as\\n, and 4 images as\\n .\\nAlthough some subjects did not display sad, angry, and sur-\\nprised expressions, they really did feel these, which is why allsix types of emotion for 20 participants (13 male and 7 female)\\nfrom the NVIE spontaneous sub-database are used for emotion\\ninference based on facial temperature as described below.\\nA. Expression Recognition From Visible Images\\n1) Analysis Methods: Before feature extraction, prepro-\\ncessing work activities including manual eye location, angle\\nrectiﬁcation, and image zoom were performed to normalize\\nthe apex images to\\nrectangles. Then, four baseline\\nalgorithms are used to extract image features, namely the PCA[23], PCA+LDA [24], AAM [26], and AAM+LDA [24], [26].\\nDetails of these algorithms can be found in [23], [24], and [26],\\nrespectively. For the PCA and PCA+LDA methods,\\nand\\nare both set as 64 to reduce the computational complexity,\\nyet retain sufﬁcient information of the images. Furthermore,\\nfour methods are used to normalize the grey-level values of\\nthe images, that is, histogram equalization (HE), regionalhistogram equalization (RHE), gamma transformation (GT),and regional gamma transformation (RGT) [25]. Then, the\\nnon-facial region was removed using an elliptic mask. For\\neach image of a speciﬁc person in the selected 236 images, weapplied PCA with the training set containing all the remainingimages, except the other images of different expressions for the\\nsame person. The grey-level values of all pixels of images are\\nthe feature sets used for PCA and LDA.\\nFor the AAM and AAM+LDA methods, W and H are both\\nset as 400 to allow the facial features to be located relatively\\nprecisely. We consider two labeling options: with face conﬁg-\\nuration points (WFC: 61 points in total) and without (NFC: 52points in total), as shown in Fig. 8. From the 236 images, weTABLE III\\nCONFUSION MATRIX OF CATEGORICAL EXPRESSION\\nRECOGNITION ON VISIBLE IMAGES (%)\\nselected 72 images, which included 24 images for each expres-\\nsion, to build the appearance model. We applied this model tothe 236 images to obtain their appearance parameters.\\nAfter feature extraction,\\n-nearest neighbors was used as the\\nclassiﬁer. Euclidean distance and leave-one-subject-out cross-validation was adopted, and\\nwas set to 1 here.\\n2) Experimental Results and Analysis: Tables III and IV\\nshow the performance of these algorithms with respect to our\\ndatabase, including confusion matrices and average recognitionrates.\\nThe following observations are evident from Tables III and\\nIV.\\n• With respect to categorical expressions, happiness has a\\nhigher recognition rate than disgust and fear. Accordingto the confusion matrices, very few instances of happiness\\nare incorrectly recognized as disgust or fear, and vice versa.\\nOn the other hand, instances of disgust and fear are morelikely to be incorrectly identiﬁed as the other. Different ex-pressions are displayed using different facial characteris-\\ntics, especially in the mouth and eye regions. In our exper-\\niments, when most subjects smiled, they lifted the cornersof their mouths and kept their eyes half-closed. However,when the subjects expressed disgust or fear, some closed\\ntheir mouths, while others opened their mouths slightly\\nor even widely. Moreover, some closed their eyes, whileothers kept their eyes wide open. In other words, facialmovements of the subjects are similar in expressing happi-\\nness, but they differ from person to person when expressing\\ndisgust or fear. This may explain why happiness is moreeasily recognized.\\nWANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 687\\nTABLE IV\\nCONFUSION MATRIX OF DISCRETE DIMENSIONAL\\nEXPRESSION RECOGNITION ON VISIBLE IMAGES (%)\\n• With respect to discrete dimensional expressions, it is hard\\nto say whether\\n or\\n is more recognizable.\\nAmong the incorrect cases,\\n and\\n are mostly\\nrecognized as one another. However, all\\n instances\\nare incorrectly recognized as\\n or\\n . The main\\nreason for this is that most instances belong to\\n or\\n, and only four instances belong to\\n .\\n• With respect to the processing algorithms without LDA,\\nAAM performs better than PCA. The reason may be that\\nPCA uses grey-level values of each image as features,while AAM uses geometric characteristics of each image,that is, shape and texture information, which better repre-\\nsents facial movements. LDA improves recognition rates\\nin all PCA cases, although it has the opposite effect in mostAAM cases. This shows that the effect of LDA depends\\non the property of the data processed [15]. The differ-\\nence in recognition rates for different feature extractionalgorithms is larger than that for different preprocessingalgorithms within one processing algorithm, which means\\nthat feature extraction algorithms have a greater inﬂuence\\non the results than preprocessing algorithms. In general,NFC+AAM+KNN is the best combination of prepro-cessing and processing algorithms.\\nB. Expression Recognition From Infrared Thermal Images\\n1) Analysis Methods: The preprocessing procedure for IR\\nimages is similar to that for visible images, except that theTABLE V\\nCONFUSION MATRIX OF CATEGORICAL EXPRESSION\\nRECOGNITION ON INFRARED IMAGES (%)\\nTABLE VI\\nCONFUSION MATRIX OF DISCRETE DIMENSIONAL\\nEXPRESSION RECOGNITION ON INFRARED IMAGES (%)\\nmanual eye location is less accurate than in visible images,\\nbecause the eyes in IR images are not as clear as those in visible\\nimages. Because some subjects wore glasses, we removed the\\neye region using a 64x15 rectangular mask for the infraredfacial images. After preprocessing, the PCA and PCA+LDAmethods are used to extract features of the images. Then,\\nK-nearest neighbors is used as the classiﬁer. Euclidean distance\\nand leave-one-subject-out cross validation is adopted, and K isset to 1 here.\\n2) Experimental Results and Analysis: Both apex images and\\ndifference images (Dif.), which are created from the difference\\nbetween the grey-level of the corresponding pixels of apex andneutral images, are used in the experiments. Tables V and VI\\nshow the results.\\n• With respect to categorical expressions, it is hard to say\\nwhich one is the best discriminator in the thermal infrareddomain. However, with respect to discrete dimensional ex-\\npressions,\\nis more recognizable than\\n , and\\nall\\n instances are incorrectly recognized. The main\\nreason for this is that most instances belong to\\n or\\n, and only four instances belong to\\n .\\n• With respect to the processing algorithms, LDA improves\\nrecognition rates in discrete dimension expression recogni-tion, but causes the performance to deteriorate in categoryrecognition. The reason may be similar to that given above\\n[15]. The effect of different source images is negligible.\\nHowever, since the overall recognition rates are not very high,\\nnew methods that are suitable for thermal infrared images needto be developed.\\nC. Emotion Inference From Thermal Images\\n1) Analysis Methods: Physiological changes due to auto-\\nnomic nervous system activity can impact the temperature pat-\\n688 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010\\nFig. 9. Face segmentation.\\nterns of the face, which may be used to predict rated emotions\\n[4], [5], [27]. In this section, we analyze the relationship be-tween emotion and facial temperature using infrared thermaldifference images. First, in order to retain the original tempera-\\nture data for analysis, we manually segmented the difference in-\\nfrared original images into ﬁve regions, namely forehead, eyes,nose, mouth, and cheeks, as shown in Fig. 9, and ensured thatthe facial segmentation size and ratio of each participant’s neu-\\ntral and emotional infrared images were consistent. Then, for\\neach facial sub-region, we extracted the temperature data as thedifference between the neutral and emotional infrared facial im-ages, and calculated the ﬁve statistical parameters given below\\nto reﬂect the temperature variance. As most of the participants\\nwore glasses, thereby masking the thermal features of the eyeregion, the eye regions were not taken into account in this anal-ysis.\\n• MEAN—the mean of each difference temperature matrix\\n• ABS—the mean of the absolute values of each difference\\ntemperature matrix\\n• ADDP—the mean of the positive values of each difference\\ntemperature matrix\\n• ADDN—the mean of the negative values of each differ-\\nence temperature matrix\\n• V AR—the variance of each difference temperature matrix\\nHaving obtained the ﬁve statistical parameters, three ANOV A\\nanalyses were conducted. First, an ANOV A was conductedto ascertain which statistical parameter is most useful forreﬂecting temperature change associated with changes in emo-\\ntion. Second, an ANOV A was applied to the facial regions with\\ndifferent emotional states to ascertain in which facial regionsthe change in temperature due to the different emotions isthe greatest. Third, an ANOV A analysis was applied to the\\nemotional states in different facial regions to analyze which\\nemotional state differs most in each facial sub-region.\\n2) Experimental Results and Analysis:\\nExperiment for Parameter Selection: The factors in this\\nanalysis are the six emotional states (i.e., sadness, anger, sur-\\nprise, fear, happiness, and disgust) and the four facial sub-re-gions (forehead, nose, mouth, and cheeks). Using a multiple\\ncomparison of the post hoc test, the signiﬁcant differences be-\\ntween the emotional states for the different statistical parametersare given in Table VII.\\nFrom Table VII, the following observations can be made.TABLE VII\\nSIGNIFICANT DIFFERENCES BETWEEN DIFFERENT EMOTION\\nSTATES UNDER DIFFERENT STATIC PARAMETERS\\n• When the statistical parameter V AR is used, the signiﬁ-\\ncance of the differences between the six emotional statesis relatively higher than for all the other statistical parame-ters at the 0.05 level. Therefore, in the subsequent analyses,\\nwe only consider the V AR statistic.\\n• When V AR is used, the differential validities of the six\\nemotional states are sorted as follows: Sadness-Fear,Anger-Fear, Fear-Disgust, Sadness-Surprise, Fear-Hap-\\npiness, and Anger- Surprise. Changes in the thermal\\ndistribution on the face for different emotional states arecaused by facial muscle movements, in addition to thetransition of emotional states and/or other physiological\\nchanges [29]. However, for some emotional states, the\\nthermal distribution on the face is not sufﬁciently uniqueto be distinguished from that of another state. This may becaused by ambiguity or overlap of the emotional states.\\nANOVA Analysis of the Different Facial Regions With Dif-\\nferent Emotion States Using VAR: The factors in this analysis\\nare the facial sub-regions (forehead, nose, mouth, and cheeks),while the dependent variable is V AR. The impact is measured\\nas the mean V AR values for different facial sub-regions, shown\\nin Fig. 10 and Table VIII.\\n• From Fig. 10, we can conclude that, when using the V AR\\nstatistic, the degree of impact of the forehead and cheek\\nregions is more signiﬁcant than the other two regions for\\nmost emotional states, e.g., surprise, fear and happiness.Furthermore, the impact of the mouth region on all emo-tional states is the smallest compared with other regions.\\nThis means that when the emotion changes, the tempera-\\nture variance in the forehead and cheek regions is muchlarger than that in the other regions, while the temperaturevariance in the mouth region is the smallest. The impor-\\ntance of the forehead region in the analysis of emotions\\nusing thermal images coincides with the research results ofSugimoto and Jenkins [30], [31]. The V AR values for the\\ncheek region are relatively large for most emotional states,\\nthereby reﬂecting the uniformity of this region’s tempera-ture change, while the results for the nose and mouth re-gions are opposite, thereby reﬂecting the non-uniformity\\nWANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 689\\nFig. 10. Impact of different facial regions on different emotional states.\\nTABLE VIII\\nMATRIX SHOWING SIGNIFICANT DIFFERENCES BETWEEN\\nFACIAL REGIONS WITHDIFFERENT EMOTION STATES\\nof the temperature change in these regions. This result\\nis vastly different to that obtained when using visible fa-\\ncial expressions to reﬂect the various emotions, in that the\\nchange in forehead and cheek is not signiﬁcant when thefacial expression changes. This result is, therefore, mean-ingful in the research of emotion inference using changes\\nin facial temperature.\\n• From Table VIII, only one facial sub-region pair, Fore-\\nhead-Mouth, is signiﬁcantly different for the emotion fear.We can conclude that the differences between different fa-\\ncial sub-regions are not signiﬁcant for any of the emotional\\nstates when using the V AR statistics. This indicates thatthe overall change in the facial temperature occurs duringthe emotional state transfer. In some individual cases, for\\ncertain emotional states, a change in temperature may be\\ncaused by the combined inﬂuence of psychological factorsor physiological factors related to facial muscular move-ments. This will be studied in depth in our future research.\\nANOVA Analysis of the Different Emotion States in Dif-\\nferent Facial Regions Using VAR: In this ANOV A analysis, the\\nfactors are the emotion states, with the dependent variable beingV AR. The degree of impact is measured as the mean of the V AR\\nvalues for different emotional states in each facial sub-region.\\nThe analysis results are given in Fig. 11 and Table IX.\\n• From Fig. 11, we can conclude that, when using the V AR\\nstatistic, the sorted degree of impact of these emotional\\nstates, i.e., Fear-Surprise-Happiness, is similar in the fore-\\nhead and cheek regions, but not in the nose and mouth re-gions. This means that the degree of impact of the emo-tional states fear, surprise, and happiness in most of the\\nfacial sub-regions is more signiﬁcant than that of the other\\nemotional states. The exception of the nose and mouth re-gions may be due to a change in the breathing rate or anon-stationary physiological signal for different emotional\\nstates [27], [28]. Besides, muscular movement in the mouth\\nFig. 11. Impact of the different emotional states in the different facial sub-\\nregions.\\nTABLE IX\\nMATRIX SHOWING SIGNIFICANT DIFFERENCES BETWEEN DIFFERENT\\nEMOTIONAL STATES FOR EACH FACIAL SUB-REGION\\nregion for different facial expressions may also be a con-\\ntributing factor [29].\\n• From Table IX, we can conclude that when using the\\nV AR statistic, three emotional state pairs are signiﬁ-\\ncantly different in the forehead region (Sadness-Fear,\\nAnger-Fear, and Fear-Disgust), two emotional state pairsin the mouth region (Sadness-Surprise and Anger-Sur-prise), and two emotional state pairs in the cheek regions\\n(Sadness-Surprise and Sadness-Fear) at the 0.05 level. All\\nthese differences may be caused by the combined inﬂu-ence of the emotional state change, the factor of the facialmuscular movements, and various other psychological\\nfactors [29].\\nVII. C\\nONCLUSION\\nThe NVIE database developed in this study for expression\\nrecognition and emotion inference has four main characteris-tics. 1) To the best of our knowledge, it is the ﬁrst natural ex-\\npression database containing both visible and infrared videos.\\nAs such, it can be used for visible, infrared, or multi-spectralnatural expression analysis. 2) It contains the facial temperatureof subjects, thereby providing the potential for emotion recog-\\nnition. 3) Both posed and spontaneous expressions by the same\\nsubject are recorded in the database, thus supplying a valuableresource for future research on their differences. 4) Both lightingand glasses variations are considered in the database. This is\\nuseful for algorithm assessment, comparison, and evaluation.\\n690 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 12, NO. 7, NOVEMBER 2010\\nWe carried out an elementary assessment of the usability of\\nthe spontaneous sub-database, using four baseline algorithmsfor visible expression recognition and two baseline algorithms\\nfor infrared expression recognition, and provided reference\\nevaluation results for researchers using the database. We alsoanalyzed the relationship between facial temperature and emo-tion, and provided useful clues for emotion inference from\\nthermal images.\\nA\\nCKNOWLEDGMENT\\nThe authors would like to thank all the subjects who partici-\\npated in the experiments. The authors also would like to thankthe Editor and the anonymous reviewers for their insightful\\ncomments.\\nR\\nEFERENCES\\n[1] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, “A survey of af-\\nfect recognition methods: Audio, visual, and spontaneous expressions,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 1, pp. 39–58, Jan.\\n2009.\\n[2] B. Fasel and J. Luettin, “Automatic facial expression analysis: A\\nsurvey,” Pattern Recognit. , vol. 36, pp. 259–275, 2003.\\n[3] M. M. Khan, R. D. Ward, and M. Ingleby, “Classifying pretended and\\nevoked facial expressions of positive and negative affective states using\\ninfrared measurement of skin temperature,” Trans. Appl. Percept. , vol.\\n6, no. 1, pp. 1–22, 2009.\\n[4] M. M. Khan, M. Ingleby, and R. D. Ward, “Automated facial expres-\\nsion classiﬁcation and affect interpretation using infrared measurement\\nof facial skin temperature variations,” ACM Trans. Autonom. Adapt.\\nSyst. , vol. 1, pp. 91–113, 2006.\\n[5] A. T. Krzywicki, G. He, and B. L. O’Kane, “Analysis of facial\\nthermal variations in response to emotion—eliciting ﬁlm clips,” Proce.\\nSPIE—Int. Soc. Optic. Eng. , vol. 7343, no. 12, pp. 1–11, 2009.\\n[6] [Online]. Available: http://emotion-research.net/wiki/Databases.\\n[7] M. Pantic and M. Stewart Bartlett , Machine Analysis of Facial Expres-\\nsions, in Face Recognition , K. D. a. M. Grgic, Ed. Vienna, Austria:\\nI-Tech Education and Publishing, 2007, pp. 377–416.\\n[8] A. J. O’Toole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pappas, J. H.\\nAyyad, and H. Abdi, “A video database of moving faces and people,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 27, no. 5, pp. 812–816,\\nMay 2005.\\n[9] N. Sebe, M. S. Lew, I. Cohen, Y. Sun, T. Gevers, and T. S. Huang,\\n“Authentic facial expression analysis,” in Proc. 6th IEEE Int. Conf.\\nAutomatic Face and Gesture Recognition , 2004.\\n[10] E. Douglas-Cowie, R. Cowie, and M. Schroeder, “The description of\\nnaturally occurring emotional speech,” in Proc. 15th Int. Conf. Pho-\\nnetic Sciences , Barcelona, Spain, 2003, pp. 2877–2880.\\n[11] G. I. Roisman, J. L. Tsai, and K. S. Chiang, “The emotional integration\\nof childhood experience: Physiological, facial expressive, and self-re-\\nported emotional response during the adult attachment interview,” De-\\nvelopment. Psychol. , vol. 40, no. 5, pp. 776–789, 2004.\\n[12] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J.\\nMovellan, “Recognizing facial expression: Machine learning and ap-\\nplication to spontaneous behavior,” in Proc. IEEE Int. Conf. Computer\\nVision and Pattern Recognition (CVPR ’05) , 2005, pp. 568–573.\\n[13] R. Gajsek, V. Struc, F. Mihelic, A. Podlesek, L. Komidar, G. Socan,\\nand B. Bajec, “Multi-modal emotional database: AvID,” Informatica\\n33, pp. 101–106, 2009.\\n[14] K. R. Scherer and G. Ceschi, “Lost luggage emotion: A ﬁeld study of\\nemotion-antecedent appraisal,” Motivation and Emotion , vol. 21, pp.\\n211–235, 1997.\\n[15] K. Delac, M. Grgic, and S. Grgic, “Independent comparative study\\nof PCA, ICA, and LDA on the FERET data set,” Int. J. Imag. Syst.\\nTechnol. , vol. 15, no. 5, pp. 252–260, 2005.\\n[16] [Online]. Available: http://www.image.ntua.gr/ermis/.\\n[17] [Online]. Available: http://emotion-research.net/download/vam.\\n[18] D5i: Final Report on WP5, IST FP6 Contract no. 507422, 2008.[19] [Online]. Available: http://www.equinoxsensors.com/prod-\\nucts/HID.html.\\n[20] [Online]. Available: http://www.cse.ohio-state.edu/OTCBVS-\\nBENCH/Data/02/download.html.\\n[21] [Online]. Available: http://www.terravic.com/research/facial.htm.[22] J. Rottenberg, R. R. Ray, J. J. Gross, J. A. Coan, and J. J. B. Allen ,\\nHandbook of Emotion Elicitation and Assessment . New York: Ox-\\nford Univ. Press, 2007.\\n[23] M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,” in\\nProc. IEEE Int. Conf. Computer Vision Pattern Recognition , 1991, pp.\\n586–591.\\n[24] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\\nFisherfaces: Recognition using class speciﬁc linear projection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711–720, Jul.\\n1997.\\n[25] S. Shan, W. Gao, B. Cao, and D. Zhao, “Illumination normalization for\\nrobust face recognition against varying lighting conditions,” in Proc.\\nIEEE Int. Workshop Analysis and Modeling of Faces and Gestures\\n,\\n2003, pp. 157–164.\\n[26] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance\\nmodels,” IEEE Trans. Pattern Analysis Mach. Intell. , vol. 23, no. 6,\\npp. 681–685, Jun. 2001.\\n[27] B. R. Nhan and T. Chau, “Infrared thermal imaging as a physiological\\naccess pathway: A study of the baseline characteristics of facial skin\\ntemperatures,” Physiol. Measure. , vol. 30, no. 4, pp. N23–N35, 2009.\\n[28] H. Tanaka, H. Ide, and Y. Nagashuma, “An attempt of feeling analysis\\nby the nasal temperature change model,” in Proc. 2000 IEEE Int. Conf.\\nSystems, Man, and Cybernetics , 2000, vol. 1, pp. 1265–1270.\\n[29] Y. Sugimoto, Y. Yoshilomi, and S. Tomita, “A method for detecting\\ntransitions of emotional states using a thermal facial image based on a\\nsynthesis of facial expressions,” Robot. Autonomous Syst. , vol. 3I, no.\\n3, 2000.\\n[30] A. Merla and G. L. Romani, “Thermal signatures of emotional arousal:\\nA functional infrared imaging study,” in Proc. IEEE 29th Annu. Int.\\nConf. , 2007, pp. 247–249.\\n[31] S. Jenkins, R. Brown, and N. Rutterford, “Comparing thermographic,\\nEEG, and subjective measures of affective experience during simulated\\nproduct interactions,” Int. J. Design , vol. 3, no. 2, pp. 53–65, 2009.\\n[32] S.-J. Chung, “Lexpression et la perception de l.emotion extraite de la\\nparole spontanee: Evidences du coreen et de l’anglais,” , Univ. Sor-\\nbonne Nouvelle, Paris, France, 2000.\\nShangfei Wang (M’02) received the M.S. degree in\\ncircuits and systems and the Ph.D. degree in signaland information processing from the University of\\nScience and Technology of China (USTC), Hefei,\\nAnhui, China, in 1999 and 2002.\\nFrom 2004 to 2005, she was a postdoctoral\\nresearch fellow with Kyushu University, Fukuoka,\\nJapan. She is currently an Associate Professor with\\nthe School of Computer Science and Technology,\\nUSTC. Her research interests cover computation\\nintelligence, affective computing, multimedia\\ncomputing, information retrieval, and artiﬁcial environment design. She has\\nauthored or coauthored over 40 publications.\\nZhilei Liu received the B.S. degree in information\\nand computing science from Shandong University of\\nTechnology, Zibo, Shandong, China, in 2008. He is\\npursuing the M.S. degree in the School of Computer\\nScience and Technology of the University of Science\\nand Technology of China, Hefei, Anhui, China.\\nHis research interest is affective computing.\\nSiliang Lv received the B.S. degree in computer sci-\\nence and technology from the University of Science\\nand Technology of China (USTC), Hefei, Anhui,\\nChina, in 2008. He is pursuing the M.S. degree in\\nthe School of Computer Science and Technology of\\nUSTC. His major is affective computing.\\nWANG et al. : A NATURAL VISIBLE AND INFRARED FACIAL EXPRESSION DATABASE FOR EXPRESSION RECOGNITION AND EMOTION INFERENCE 691\\nYanpeng Lv received the B.S. degree in software en-\\ngineering from Shandong University, Jinan, China,\\nin 2008. He is currently pursuing the M.S. degree\\nin computer science in the University of Science and\\nTechnology of China, Hefei, Anhui, China.\\nHis research interest is affective computing.\\nGuobing Wu received the B.S. degree in computer\\nscience and technology from Anhui University,\\nHefei, Anhui, China. He is pursuing the M.S. degreein the School of Computer Science and Technology\\nof the University of Science and Technology of\\nChina, Hefei, Anhui, China.\\nHis research interest is affective computing.\\nPeng Peng received the B.S. degree in computer sci-\\nence and technology from the University of Science\\nand Technology of China, Hefei, Anhui, China, in\\n2010. He is pursuing the M.S. degree in the School\\nof Computing Science at Simon Frazer University,\\nBurnaby, BC, Canada.\\nFei Chen received the B.S. degree in computer sci-\\nence and technology from the University of Science\\nand Technology of China, Hefei, Anhui, China, in\\n2010. He is pursuing the M.S. degree in the School of\\nComputing Science at the University of Hong Kong.\\nXufa Wang received the B.S. degree in radio elec-\\ntronics from University of Science and Technology\\nof China, Hefei, China, in 1970.\\nHe is currently a Professor of the School of Com-\\nputer Science and Technology, University of Science\\nand Technology of China, Hefei, Anhui, China, and\\nthe Director of the Key Lab of Computing and Com-\\nmunicating Software of Anhui Province. He has pub-\\nlished ﬁve books and over 100 technical articles in\\njournals and proceedings in the areas of computation\\nintelligence, pattern recognition, signal processing,\\nand computer networks.\\nProf. Wang is an Editorial Board Member of the Chinese Journal of Elec-\\ntronic , the Journal of Chinese Computer Systems , and the International Journal\\nof Information Acquisition .\\n',\n",
       " '                                 978-1-4244-6516-3/10/$26.00 ©2010 IEEE                                 1750\\n2010 3rd International Congress on Image and Signal Processing (CISP2010)                       \\nA New Method For Facial Expression Recognition \\nBased On Sparse Representation Plus LBP  \\n \\nMing-Wei Huang \\nSchool of Information Engineering \\nWUYI University  \\nJiangmen, China Zhe-wei Wang ， Zi-Lu Ying \\n \\n \\n \\n \\nAbstract —Sparse Representation-based Classification (SRC) is a \\nnewly introduced algorithm for face recognition, notable for its robust performance to occlusions and corruptions. Local Binary Patterns (LBP) is a very powerful method to describe the texture \\nand shape of images. In this paper, we propose a novel method \\nfor facial expression recognition based on sparse representation of LBP features. Extensive experiments on Japanese Female Facial Expression (JAFFE) database are conducted. The \\nexperiment results show that the new method has a better \\nperformance than using Sparse Representation-based Classification solely on facial recognition, and is also better than those traditional algorithms such as Principal Component Analysis (PCA) and Linear discriminant analysis (LDA). \\nKeywords-Facial expression recognition; Sparse representation; \\nLBP; SRC; PCA; LDA  \\nI. INTRODUCTION  \\nFacial expressions imply much information about human \\nemotions and play an important role in human communications. With the advance of information technology, the wide spread use of Internet, mobile telephony and other \\ninexpensive computer equipment has made human computer \\ninteraction (HCI) a usual activity in everyday life [1]. In order to facilitate a more intelligent and smart human machine interface of multimedia products [1], the automatic facial expression recognition (FER), which had been studied world wide in the last twenty years, has become a hot spot in the computer vision and pattern recognition community. Despite much effort had been made on, and many algorithms, such as PCA and LDA, have been introduced to, FER remains a tough problem, for its relative low discriminative rate and weak robustness to occlusions and corruptions. \\nIn this paper we propose a new approach to facial \\nexpression recognition based on Sparse Representation and LBP. Sparse Representations a newly developed signal processing tool [2]. Sparse Representation-based Classification is a new algorithm for digital image classification based on sparse representation theory [3]. SRC is notable for its robustness to corruption and occlusion in face recognition. Local Binary Patterns (LBP) [4, 5] is a very powerful method to describe the texture and shapes of an image. Therefore it appeared to be suitable for feature extraction in facial expression recognition. In this paper we use LBP to extract the features of expression images and then we use SRC to classify images into categories of expressions. This algorithm can attain \\na recognition rate of 62.86% for FER, better than using SRC solely, and also better than some traditional algorithms, such as PCA and LDA. \\nIn this paper, SRC+LBP is used for facial expression \\nrecognition. Traditional algorithms for FER such as PCA and LDA are used to compare with SRC+LBP algorithm. The robustness of SRC algorithm for FER to noise is studied. The rest of the paper is organized as follows. In section 2, the fundamental theory of SRC is introduced. LBP is described in section 3. The new proposed algorithm LBP+SRC is introduced in section 4. Section 5 describes extensive experiments of LBP+SRC algorithm on FER. And some conclusions are given in section 6. \\nII. S\\nPARSE REPRESENTATION -BASED CLASSIFICATION  \\nThe original goal of Sparse Representation algorithm was \\nnot for classification itself, but rather for representation or compression of signals. Sparse representation of signals potentially uses much lower sampling rates than the Shannon-Nyquist bound. The Sparse representation theory has shown that sparse signals can be exactly reconstructed from a small number of linear measurements [6, 7]. Therefore, the algorithm performance was measured in terms of sparsity of the representation and fidelity to the original signals. In SRC the \\nmeasurements are the training samples themselves. Given\\nn \\ntraining samples: 12, ,...,nvv v . We can construct the matrix \\n[]12, ,..., ,mn\\nnAv v v×=∈ \\\\  the test samplemy∈\\\\ can be \\nlinearly represented by all training samples as \\nmyA x=∈ \\\\  (1) \\nThe equation yA x= is usually over-determined. We can \\nfind the sparest solution by choosing the minimum 0A-norm \\nsolution: \\n0 0x = argmin x subject to Axy= (2) \\nwhere x is 0A-norm of x. If , mn>  however, the problem \\nof finding the solution of (2) is NP-hard. To solve the NP-hard \\nequation, Donoho [8] and some other scholars [9] proved that if \\n                                                                                                                                          1751\\nthe solution0xis sparse enough, the solution of the 0A-\\nminimization problem (2) is equivalent to the following 1A-\\nminimization problem: \\n11arg min subject to xx A x y==\\x03 (3) \\nIf the solution is sparse and has t nonzero entries, it can be \\nefficiently solved by homotopy algorithms  in 3()Ot n +  time, \\nlinear in the size of the training set [10].  \\nIn the case of dealing data with noise, the Sparse  \\nRepresentation model (1) can be modified to account for small possibly dense \\nnoise  by writing  \\nyzAx=+  (4) \\nwhere  mz∈\\\\is a noise factor with bounded energy  \\n2z ε<. \\nThe Sparse x can still be approximately recovered by solving \\nthe following stable  1A-minimization problem: \\n1\\nsA() : 112arg min subject to - xx A x y ε =≤\\x03 (5) \\nThis convex optimization problem can be solved via \\nsecond-order cone programming [11]. The solution of 1\\nsA()  is \\nguaranteed to approximately recovery sparse solutions in \\nensembles of random matrices A.   \\nGiven a test sample y from one of the classes in the \\ntraining set, we first calculate its sparse representation l\\n1xvia \\n(3) or (5). The nonzero entries in the estimate l\\n1xwill all be \\nassociated with the columns of A from a single object class i, \\nand then we can assign the test sample y to that class.  \\nHowever, noise and modeling error may lead to small \\nnonzero entries associated with multiple object classes. In this \\ncase, we instead classify y based on how well the coefficients \\nassociated with all training samples of each object reproduce y.  \\nFor each class i, letiδ:nn→\\\\\\\\  be the characteristic \\nmapping that selects the coefficients associated with the thi \\nclass. For1nx∈\\\\, \\ninxδ ∈\\\\（）  is a new vector only whose only \\nnonzero entries are the entries in x that are associated with \\nclass i.  \\nUsing only the coefficients associated with the thi class, \\none can approximate the given test sample yasl\\x03\\n1()iiyA x δ= . \\nWe then classify y based on these approximations by \\nassigning it to the object class that minimizes the residual \\nbetween y and\\x03\\niy: \\n1 2min ( ) ( )iiiry y A x δ−\\x18  (6) \\nIII. THE PRINCIPLE OF LBP   \\nLBP is a very powerful method to describe the texture and \\nshape of a digital image. Ojala et al. [4] first introduced LBP operator and showed its high discriminative power for texture \\nclassification. At a given pixel position (),ccxy, LBP is \\ndefined as an ordered set of binary comparisons of pixel \\nintensities between the center pixel and its eight surrounding pixels (Fig. 1). The decimal form of the resulting 8-bit word (LBP code) can be expressed as follows: \\n7\\n0(, ) ( ) 2n\\ncc n c\\nnLBP x y s i i\\n==−∑ (7) \\nwhere ci corresponds to the grey value of the center pixel \\n(),ccxy, in to the grey values of the 8 surrounding pixels, and \\nfunction s(x) is defined as:  \\n10()00if xsxif x≥ ⎧=⎨< ⎩ (8) \\n \\nFigure 1.  The example of basic LBP operator.  \\nBy definition, the LBP operator is unaffected by any \\nmonotonic gray-scale transformation which preserves the pixel intensity order in a local neighborhood. Note that each bit of the LBP code has the same significance level and that two successive bit values may have a totally different meaning. Actually, The LBP code may be interpreted as a kernel structure index. \\nLater the LBP operator was extended to use neighborhood \\nof different sizes [5]. Their \\n,PRLBP  notation refers to P equally \\nspaced pixels on a circle of radius R. In this paper, we use the \\n,PRLBP  operator which is illustrated in Fig. 2.  \\n \\nFigure 2.  Circularly neighbour-sets for three different values of P and R \\nAnother extension to the original operator is the definition \\nof so called uniform patterns. A local binary pattern is called uniform if the binary pattern contains at most two bitwise transitions from 0 to 1 or vice versa when the bit pattern is considered circular. For example, the patterns 00000000 (0 transitions) and 11001111 (2 transitions) are uniform whereas the patterns 11001001 (4 transitions) and 01010011 (6 transitions) are not. In the computation of the LBP histogram, uniform patterns are used so that the histogram has a separate bin for every uniform pattern and all non-uniform patterns are \\nassigned to a single bin. For uniform patterns with\\nP sampling \\npoints and radius Rthe notion 2\\n,u\\nPRLBP  is used. The superscript \\n                                                                                                                                          1752\\n2u stands for using only uniform patterns. Ojala et al. noticed \\nthat in their experiments with texture images, uniform patterns \\naccount for a bit less than 90 % of all patterns when using the (8, 1) neighborhood and for around 70 % in the (16, 2) neighborhood. The image after LBP uniform pattern is illustrated in Fig. 3. \\n \\nFigure 3.  The instance of 2\\n,u\\nPRLBP  processing of facial expression image. \\nTo extract the features of the facial expression, the images \\nare divided into local regions01 1,m RRR−\"  and texture \\ndescriptors are extracted from each region independently. For \\nevery region a histogram with all possible labels is constructed. This means that every bin in a histogram represents a pattern and contains the number of its appearance in the region. A \\nhistogram of the labeled image \\n),(yxfl can be defined as: \\n{ }\\n,(, ) { (, ) }\\n,0 , ,1 , 0 , , 1 .il j\\nxyH T f xy i T xy R\\nin jm== ∈\\n=− = −∑\\n\"\" (9) \\nWhere, n is referred to the number of label generated by LBP \\noperator. The m is refereed to the local region number of the \\ndivided image. The function of ( ) TB can be defined as: \\n1\\n()\\n0Bis true\\nTB\\nBis false=⎧⎨⎩  (10) \\nThe feature vector is then constructed by concatenating the \\nregional histograms to a one big histogram (Fig. 4).  \\n \\nFigure 4.  The example of regional histograms and global histograms \\nconstruction. \\nIV. SRC+LBP  ALGORITHM FOR FER \\nIn this section, we introduce the new method for facial \\nexpression recognition.  The proposed algorithm involves two vital portions: facial representation and classifier design. Facial representation is to derive a set of features from original face images to effectively represent faces.  The optimal features should minimize within-class variations while maximize between-class variations [12]. In our proposed algorithm, the first portion is using LBP to extract image feature, and the \\nsecond part is using Sparse Representation as classifier for FER.  \\nThe flow of the algorithm is shown in Fig. 5. First, n facial \\nexpression images were inputted as training samples. The next step is features extraction. After the processing of LBP operator, the images yield regional histograms (features). Then, each of the facial expression images constructs their feature vectors by combining the histograms of all blocks of each sub-image into one histogram. One benefit of feature extraction, which carries over to the proposed SRC algorithm is to reduce data dimension and computational cost. For raw facial expression \\nimages, the corresponding linear system \\nyA x= is very large. \\nFor example, if the images are given at the typical resolution, \\n64×64 pixels, the dimension m is in order of 4096, but after \\nthe LBP preprocessing, the dimension is reduced to 885. These \\nhistograms are feature vectors12,n vv v\" . \\n \\n \\nFigure 5.  The FEA  algorithm flow \\nSince the most feature transformations involve only linear \\noperations, the projection from image space to feature space \\ncan be represented as a matrix dmR×∈\\\\ with dm\\x13 . Applying \\nR to both sides of (1) yields ： \\n \\x04\\n0dy Ry RAx =∈\\x18\\\\  (11) \\nIn practice, the dimension d of the feature space is chosen \\nto be much smaller than n, which is the number of training \\nimages. In this case, the system of equations \\x04dyR x=∈\\\\is \\nunderdetermined. Nevertheless, as the desired solution 0xis \\nsparse, we can recover it by solving the following reduced 1A-\\nminimization problem ： \\n\\x03\\n1 12arg min x x subject to RAx y ε =− ≤\\x04 (12) \\nfor a given error tolerance  0ε>. Thus, in Algorithm of SRC, \\nthe matrix Aof training images is now replaced by the matrix \\ndnRA×∈\\\\of -dimensionald features; the test image yis replaced \\nby its features \\x04y. And rigorously follow the step of the above-\\nmentioned SRC algorithm; we can get the result of the \\nclassification of the sample \\x04y. Image normalizatio n Input JAFFE database  \\nFeature extraction \\nbased on LBP \\nSRC classifie r \\nEnd \\n                                                                                                                                          1753\\nV. EXPERIMENTS  \\nThe experiments of the proposed algorithm on facial \\nexpression recognition are carried out on JAFFE database. The database contains 213 images in which ten persons expressing three or four times the seven basic expressions (anger, disgust, fear, joy, neutral, sadness and surprise). We select 3 copies of the image of each individual per expression (210 images in all) for our experiments.  \\nBefore extracting features, all images were preprocessed: \\naligned the position of face images using eye coordinates and \\ncropped with a mask to exclude non-fac\\ne area. Then, images \\nare resized to 6 4×64 pixels. Some sample images are shown in \\nFig. 6. For FER experiments, we divide the database into ten \\nequally sized sets (each set is consist of only one person’s facial expression image): nine sets are used for training and the remaining one for testing. The above process is repeated so that each of the ten roughly equally sized sets is used once as the test set. The average result over all ten cycles is considered as the recognition rate of one trial.\\n  \\n \\n  \\n  \\n  \\n    \\nFigure 6.  The samples of JAFFE images after preprocessing  \\nWhen solving equation (12), we find that the error tolerance \\nε is a very important undetermined parameter. It determines \\nthe accuracy of the classification algorithms.  We did an \\nexperiment on different value of ε, the results are shown in \\nTABLE I.  \\nTABLE I.   FACIAL EXPRESSION RECOGNITION RATES ON DIFFERENT \\nVALUE OF ε \\nε 0.01 0.005 0.003 0.0001 \\nAccuracy 60.95% 62.86% 62.38% 55.24% \\n \\nWe can see that, the recognition rate improved  with the \\nerror tolerance εdecreasing at first, but then the recognition \\nrate turned bad when the error tolerance ε become extremely \\nsmall. The best recognition rate can be obtained when the error \\ntolerance ε is 0.005. \\nFinally, we compare the performance of our proposed \\nalgorithm to SRC, Principle Component Analysis (PCA), Kernel PCA (KPCA), Linear Discriminant Analysis (LDA) [13], and Gabor Histogram Feature+MVBoost [14]. All those four methods use SVM as classifiers. The results are shown in TABLE II. \\nTABLE II.  THE FACIAL EXPRESSION RECOGNITION RATE COMPARISON \\nOF SEVERAL ALGORITHM ABOUT FEATURE FORMATION AND SELECTION  \\nSRC+LBP SRC PCA \\n[13] LDA \\n[13] KPC A \\n[13] Gabor Histogram \\nFeature+MVBoost \\n[14] \\n62.9% 61.4% 53.8% 55.7% 53.6% 58.7%\\n The best recognition rate of SRC+LBP reaches 62.9%, \\nhigher than SRC, also higher than traditional algorithms, such as PCA, LDA  and some freshly introduced algorithms, such as those based on Gabor Histogram Feature and MVBoost. \\nVI.\\n CONCLUTIONS  \\nA Novel approach for facial expression recognition based \\non Sparse Representation and LBP was proposed. Tested on JAFFE database, the proposed algorithm obtained satisfactory results. The performance of the proposed algorithm is better than that of the traditional algorithms such as PCA, KPCA and LDA etc, and also superior to some freshly introduced algorithms, such as FER based on Gabor Histogram Feature and MVBoost.   \\nFER, which is filled of excitement and challenge, is \\nthriving in the pattern recognition and computer vision community. Many scientific workers and scholars in the world are devoted to it with the prospect of applying it to commerce. After decades of development, FER arrival to the commercial application era, not only happens in lab. Remarkable examples are Sony’s T200 camera and Hanson Robotics’ apish robot. In our experiment, LBP as a way of feature extraction combined \\nwith Sparse Compressing Classification has a great success on \\nfacial expression recognition. However, the average performance of our proposed algorithm is still relatively low for commercial appliance. There are a lot of research works to do with FER.   \\nA\\nCKNOWLEDGMENT  \\nThis paper was supported by Guangdong NSF (032356), \\nGuangdong NSF (07010869), Open Project Fund of National \\nKey Laboratory on Machine Perception, Peking University (0505) and Open Project Fund of National Key Laboratory on CAD &CG, Zhejiang University (A0703)  \\nR\\nEFERENCES  \\n[1] George Votsis, Nikolaos Doulamis, Anastasios Doulais, \\nNicolasTsapatsoulis and Steanos Kollias, \"A Neural-Network-based Approach to Adaptive Human Computer Interaction\" Lecture Notes in Computer Science, Spinger-Verlang Press, Vol. 2130, pp. 1054-1059 \\n2001. \\n[2] Donoho. D.L, “Compressed sensing”, IEEE Transaction on Information \\nTheory, Vol 52, No. 4, pp. 1289-1306, Apr. 2006. \\n[3] Wright. J, Yang. A.Y, Ganesh. A, Sastry. S.S, and Ma.Y, “Robust Face \\nRecognition via Sparse Representation”, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 31, No. 2, pp. 210-227, \\nFeb.2009. \\n[4] T. Ojala, M. Pietik¨ainen and D. Harwood. “A comparative study of \\ntexture measures with classification based on feature distributions”,J. \\nPatternRecognition vol. 29, No.1 pp. 51-59,  1996\\n.. \\n[5] Timo Ahonen, Abdenour Hadid, and Matti Pietikainen, “Face \\nRecognition with Local Binary Patterns”M.  Lecture Notes in Computer \\nScience,  Vol. 3021, pp.469-474,May.2004. \\n[6] Candes. E.J, “Compressive sampling”, International Congress of \\nMathematicians, Aug. 2006. \\n[7] Candes. E.J, Romberg. J, and Tao.T, “Robust uncertainty principles: \\nExact signal reconstruction from highly incomplete frequency information”, IEEE Transactions on Information Theory, Vol 52, No. 2, \\npp. 489-509, Feb. 2006. \\n[8] Donoho. D.L, “For Most Large Underdetermined Systems of Linear \\nEquations the Minimal \\n1A-Norm Solution Is Also the Sparest Solution”, \\n                                                                                                                                          1754\\nCommunication on Pure and Applied math, Vol 59, No. 6, pp. 797-829, \\nMay. 2006. \\n[9] Candes. E.J, and Tao. T, “Near-Optimal Signal Recovery from Random \\nProjections: Universal Encoding Strategies?”, IEEE Transaction on \\nInformation Theory, Vol 52, No. 12, pp. 5406-5426, Dec. 2006. \\n[10] Donoho. D.L, and Tsaig. Y, “Fast Solution of 1A-Norm Minimization \\nProblems When the Solution May Be Sparse”, \\nhttp://www.stanford.edu/tsaig/research.html.  \\n[11] S. Chen, D. Donoho, and M. Saunders, “Atomic Decomposition by \\nBasis Pursuit,” SIAM Rev., vol. 43, no. 1, pp. 129-159, 2001. \\n[12] Caifeng shan, Shaogang Gong, Peter W. McOwan, “Facial expression \\nrecognition based on Local Binary Patterns: A comprhensive study”, J. \\nImage and Vision Computing, Vol. 27, No 6, pp 803-816, 2009. \\n[13] Ying Zilu, Zhang Guoyi, “Facial Expression Recognition Based on \\nNMF and SVM”, International Forum on Information Technology and \\nApplications, Chengdu, China, pp. 612-615, 2009. \\n[14] Liu Xiaomin, Zhang Yujin, “Facial Expression Recognition Based on \\nGabor Histogram Feature and MVBoost”, J.Journal of Computer \\nResearch and Development, Vol. 44, No.7, pp. 1089-1096, 2007. \\n',\n",
       " '1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 1 \\n  \\n \\nAbstract—Accura te biometric  identification under  real \\nenvironments is one of the most critical and chall enging tasks to \\nmeet growing demand for higher security. This paper proposes a \\nnew framework to efficiently and accurately match periocular \\nimages that are automatically acquired under less -constrained \\nenvironments. Our framework, referred to as semantics -assisted \\nconvolutional neural networks (SCNN) in this paper, incorporates \\nexplicit semantic information to automatically recover \\ncomprehensive periocular features. This strategy enables superior \\nmatc hing accuracy with the usage of  relatively smaller  number  of \\ntraining samples which is often an issue with s everal biometrics. \\nOur reproduci ble experimental results on four different publicly \\navailable databases suggest that  the SCNN based  periocular \\nrecognition approach  can achieve outperforming results, both i n \\nachievable accuracy and matching time, for less -constrained \\nperiocular matching. Additional experimental results presented in \\nthis paper also indicate that the effectiveness of proposed SCNN \\narchitecture is not only limited to periocular recognition but it can \\nalso be useful for generalized image classification.  Without \\nincreasing the volume of training data, the SCNN is able to \\nautomatically extract more discriminative features from the input \\ndata than a single CNN, therefore can consistently improve the  \\nrecognition performance . The experimental results in this paper \\nvalidate such an approach to enable faster and more accurate \\nperiocular recognition  under less constrained environments.  \\n \\nIndex Terms —Periocular recognition, deep learning, \\nconvolution neural  network , training data augmentation.  \\nI. INTRODUCTION  \\nERIOCULAR  recognition is an emerging biometric \\nmodality that has attracted noticeable  interest in recent \\nyears and a lot of research effort ha ve been devoted to advance \\naccuracy from the automated algori thms. The periocular region \\nusually refers to the region around the eye, although there is no \\nstrict definition  or standard fro m research bodies like NIST [41]. \\nPeriocular recognition is believed to be u seful when accurate \\niris recognition cannot be ensured , such as under visible \\nillumination [8], unconstrain ed environment [9] or when the \\nwhole face is not available , as illustrated  from some real -life \\nsamp les in Figure 1. It has also been shown that the periocular \\nregion is more resistant to expression variation s [10] and aging \\n[11] as compared  with the face. In addition to serving  as an \\nindependent biometric modality, periocular information  can \\nalso be simultaneously combined with iris [2], [13] and/or face \\n[15] to improve the overall recognition performa nce. However  \\nmatching  periocular images, particularly under less constrained \\nenvironment , is a challenging problem as this  region itself \\ncontains less information than the entire  face and often \\naccompanied by high intra -class variations along with occlusio ns like from glasses, hair, etc. \\nIn recent years, Convolutional Neural Network (CNN) has \\ngained popular ity for its strong ability to extract comprehensive \\nfeature s from the input data, especially for visual pattern s. It \\nhas demonstrated its robustness to the real -life intra -class \\nspatial variation s. The CNN has many successful applications \\nlike hand -written character recognition [6], object detection \\n[16], large -scale image classification [17] and face recognition \\n[18]-[19], where CNN has significantly outperform ed \\ntraditional methods using handcrafted features or other learning \\nbased approaches. Therefore we have b een motivated to use \\nCNN to achieve better performance  for the challenging \\nperiocular recognition problem.   \\nA. Our Wor k and Contributions  \\nAutomated periocular recognition under less constrained \\nenvironment has shown promising performa nce and underlined \\nthe ne ed for further research. Several databases, acquired under \\nvisible and near -infrared illuminations, have been introduced in \\nthe public domain [30], [34]-[36] and it can be observed that \\nresearchers require/use training samples from respective \\ndatabases, primarily to select  or learn  best set of parameters.  \\nThe performance achieved on these less -constrained databases \\nis encouraging but requires further work. This paper atte mpts to \\naddress these two limitations for the automated periocular \\nrecognition.  \\n      In addition to  successfully investigating the strengths of \\nCNN for the less -constrained periocular recognition , this paper \\nintroduc es the Semantics -Assisted CNN (SCNN) a rchitecture \\nto fully exploit the discriminative information within limited Accurate Periocular Recognition under Less Constrained Environment  \\nUsing Semantics -Assisted Co nvolutional Neural Network  \\nZijing Zhao, Ajay Kumar  \\nP    \\n(a) \\n   \\n \\n \\n \\n \\n \\n \\n  \\n(b)                                     (c)                                 (d) \\n \\nFigure 1: Periocular recognition is useful when (a) iris texture is degraded or  \\nwhen the faces are covered for (b) protection from environment, (c) during \\nsickness or (d) during demonstrations or riot s [42]. \\n\\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 2 \\nnumber of training samples. The key contributions of our work \\ncan be summarized as in the following.  \\nOur approach for periocular recognition using SCNN does \\nnot require training sam ples from target datasets , while \\nachieving outperforming results, which is a key advantage over \\nstate-of-the-art approaches [2] and [10]. In our experiments, t he \\nSCNN is trained with one database an d tested on totally \\nindependent/separate databases. The testing and training sets \\nhave mutually exclusive subjects and hi ghly different image \\nquality as well as imaging condition s and/or equipment’s . The \\nSCNN architecture  can also enable recovery of  more \\ncomprehensive periocular features from the limited training \\nsamples . Another key advantage of  proposed method in this \\npaper is its computational simplicity, i.e., our trained model \\nrequires much less computational time for feature extraction \\nand matching c ompared with other methods . Unlike earlier \\nworks, the trained models and executable files of our work are \\nmade publicly available [40] so that other researchers can easily \\nreproduce our results or evaluate on new data bases . Finally, t he \\nuse of  SCNN is not only limited to the periocular recognition \\nbut can also be useful for  general image classification  task. By \\nattaching branch CNN(s) that are trained with semantic \\nsupervision from the training data, the SCNN architecture can \\nbe easily used to extend and improve existing CNN based \\napproaches  while limiting the general requirement of increase \\nin training data for such performance improvement. The SCNN \\nenables the deep neural network to fully learn the training data \\nin conjunction with the semantical correlation and therefore can \\nbenefit  the final classification  task, especially when the size of \\ntraining data is limited to build a very deep network. The \\nstructure of SCNN is easy to implement, and semantic \\nannotation of the training samples is often included with the \\nrelease of many public databases . \\nB. Related Work  \\nIn 2009, Park et al. [8] have investigated the feasibility of using \\nperiocular region for human recognition under various \\nconditions. Bharadwaj  et al. [22] also support the usefulness of \\nperiocular recognition when iris recognition fails. There is also \\nresearch work focusing on cross -spectrum periocular matching \\n[5], where techniques of neur al network have been used. \\nState -of-the art  work for periocular recognition includes [2], \\nwhere good performance is obtained by fusing periocular and \\niris features/scores together . However, DSIFT feature  \\nextraction and the K -means clustering  used by this w ork for the \\nperiocular region are highly  time consuming . Another \\nstate-of-the-art approach by Smereka  et al. [10] proposes  the \\nPeriocular Probabilistic Deformation Model (PPDM), which is \\na variant o f their previous work, and promising performance \\nhas been reported. The PPDM uses a probabilistic inference \\nmodel to evaluate the matching scores from correlation filters \\non patches of the image pair. However, this patch -based \\nmatching scheme is sensitive to scale variance among samples, \\nwhich often exists in the challenging forensic  and security  \\nscenario s. More importantly, approaches in both [2] and [10] \\nemploy some samples in the target dataset fo r training or \\nselection of parameter s, while our objective has been to \\ndevelop a more effective approach , that does not require any \\nsamples from target datasets for training , that can deliver outperforming results  and is computationally simpler . \\nAs for the  development of CNN, LeCu n’s early work [6] for \\nhandwritten character recognition is one of the most typical \\napplications of CNN in computer vision. Gradient based \\nlearning was used in that work so that CNN can learn from the \\ntraining data effectively. In recent years  CNN becomes very \\npopular due to its powerful feature extraction ability for visual \\npattern and robustness for challenging scenarios (typically for \\nlarge intra -class variance ), and CNN based methods hold \\nstate-of-the-art performance for many computer vision tasks, \\nsuch as image classification [17], object detection [16], etc. In \\n2014, Sun et al. [18] and Taigman  et al. [19] have presented \\nsuccessful application of CNN on face recognition, which \\nshowed superior results even compared with human \\nperformance. Above two approaches have shown great \\npotential of using CNN on biometrics, which is the primary \\nmotivation for us to develop the proposed CNN based method \\nfor the highly challenging periocular recognition problem.  \\nHowever, most of the existing CNN based methods require \\nhuge amount of data for training, which is the major bottleneck \\nfor its quick use for  many other computer vision tasks. This has \\nmotivated us to explore alternate strategies to considerably \\ncompensate lack of large dataset often required for the training.  \\nII. METHODOLOGY  \\nAs discussed earlier , we were motivated to incorporate  CNN \\nfor the challe nging periocular recognition problem due to its \\nknown ability to extract comprehensive feature from image.  In \\nthis section we will first introduce the theoretical background of \\nCNN and the practical architecture of our SCNN model in \\nSection II.A, followed by detailing the application for the \\nperiocular recognition proble m in Section II.B and II.C.  \\nA. Semantics -Assisted Convolution Neural Network (SCNN)  \\n1) Basic Introduction to  CNN  \\nCNN is a biologically -inspired variants of multilayer  \\nperceptron  (MLP) and well-known  as one of typical deep \\nlearning architectures. CNN has shown strong ability to learn \\neffective feature representation from input data especially for \\nimage/video unde rstanding tasks, such as handwritten character \\nrecognition [6], large -scale image classification [17], face \\nrecognition [18]-[19], etc. In t he following, we will briefly \\nintroduce the basic knowledge of a typical CNN architecture \\nthat is used in our and many other work.   \\nCNN is usually composed of convolution layers, pooling \\nlayers and fully connected  (FC) layers. At the output of each \\nlayer, there is often a nonlinear activate function, such as  \\nsigmoid, ReLU [1], etc. In our work, we adopt the basic CNN \\nstructure similar to AlexNet [7] and is shown in Figure 2 (say \\nthe periocular recognition problem as an example). The input \\nimage is passed through several convolutional units and then a \\nfew fully connected layers. The output of the last FC layer with \\nN (number of classes) nodes would represent probabilistic \\nprediction to the class labels.  \\nEach of the convolutio n unit s is composed of three \\ncomponents - a convolution layer, a max -pooling layer and a \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 3 \\nReLU (Rectified Linear Unit) activation function, as shown in  \\nFigure 2. For the convolutio nal layer, each channel of its output \\nis computed as:  \\n \\n( ) ( ) ( )( * )i ij ij j\\nj\\uf03d\\uf02b\\uf0e5 y b k x  (1) \\nwhere \\n()iy  is the i-th channel of the output map, \\n()jx  is the j-th \\nchannel of the input map, \\n()ijb  is called the bias term, \\n()ijk  is \\nthe convolution  kernel between \\n()iy  and \\n()jx , and * denote s \\nthe 2D convolution operation. \\n()ijb  and \\n()ijk  will be learned by \\nback -propagation so that the convolution kernels are trained  to \\nextract most useful features that are discriminative among \\ndifferent subjects.  \\nThe pooling layer extracts one maximum or average value \\nfrom each patch of the input channel. In our application, we use \\nmax-pooling with non-overlapping patches . As a resul t, the \\ninput maps, after convolution, are down -sampled with a scale \\ndetermined by the pooling kernel. The pooling operation \\naggregates low -level features from the input to high -level \\nrepresentation and thus could achieve spatial invariance  among \\ndifferent samples.   \\nAt the output of each pooling layer and the first FC layer (e.g., \\nL7 in Figure 2), we choose the ReLU ( Rectified  Linear Unit)  [1] \\nas the activation function:  \\n \\nmax( ,0)iiyy\\uf0a2\\uf03d   (2) \\nThe ReLU activation ensures the nonlinearity of the feature \\nextraction process and is more efficient for training, compared \\nwith the traditi onal activation functions like sigmoid or t anh \\nemployed in other approaches [14]. \\nThe FC layers process the input as in conventional  neural \\nnetworks:   \\ni i j ij\\njy b x w\\uf03d \\uf02b \\uf0d7\\uf0e5   (3) \\nwhere \\njx  is the j-th element of the vectorized  input map to th e \\ncurrent  layer, \\niy  is the i-th element of the output map, which is \\nalso a vector. \\nib  and \\nijw  are elements of the bias and weights \\nto be learned through training. The last FC layer, as usually \\nconfigured in classific ation problem, is not followed by ReLU \\nbut a softmax  function:  \\n \\ni\\njy\\ni y\\njey\\ne\\uf0a2\\uf0a2\\uf03d\\uf0e5  (4) \\nThe use of softmax  function  in the final output of the network \\nresults in a \\n1N\\uf0b4  vector with positive elements which are \\nsummed up to one. Each element then is treated as the \\nprobabilistic prediction of the class  label. The cross -entropy \\nloss function is to be minimized, which is formulated as:  \\n \\n( ) logt Ly\\uf0a2\\uf0a2 \\uf0a2\\uf0a2\\uf03d\\uf02dy   (5) \\nwhere t is the ground truth label of the training sample. The loss \\nfunction is minimized via back -propagation so that the \\npredictions of the ground truth class of the training samples will \\napproach to unity .  \\n2) Limitation of Contemporary CNN Based Approach  \\nIn order t o achieve superior performance using CNN based \\nmethod s, a common way is  to add more  layers to make the \\nnetwor k deeper and more comprehensive, and/or devote more \\nlabeled training data because CNN is usually trained in a \\nsupervised  manner . For instance, the famous CNN architecture \\nGoogLeNet [17] has 22 layers and later comes the Microso ft’s \\ndeep network with 152 layers [21]. Apparently , common \\nresearchers or companies could hardly afford to train such deep \\nnetworks due to the lack of enough computational power. Also, \\nas the network goes deeper, the need for t raining data grows \\naccordingly, while in many research areas, it is difficult to \\nacquire  enough labeled training samples like ImageNet [23]. \\nTable 1 provides examples of several typical deep learning \\nbased approaches  and their employed train ing data. In reference \\n[18] in Table 1, for instance, where the developed CNN is no t \\nvery deep (nine layers) , a total of ~200,000 face images from \\nmore than 10,0 00 people were used for training to achieve \\n10 x 226 x 226\\nL1\\nConvolution10 x 113 x 113\\nL2\\nPooling\\n+\\nReLU20 x 103 x 103\\nL3\\nConvolution20 x 52 x 52\\nL4\\nPooling\\n+\\nReLU40 x 46 x 46\\nL5\\nConvolution40 x 23 x 23\\nL6\\nPooling\\n+\\nReLU1 x 250\\nL7: Features\\nFully connected\\n+\\nReLU1 x N\\nL8: Prediction\\nFully connected\\n+\\nsoftmax15\\n1522\\n11\\n112\\n27\\n72\\n2 \\nFigure 2: Structure of the employed deep convolutional neuron network.  \\n \\n \\n Table 1: Examples of several deep learning based approaches and their required  \\nnumber of training images.  \\nApproach  Task  Size of Training Data  \\nNo. of Classes  No. of Samples  \\nCVPR [17] Image classification  1,000 1,281,167  \\nICML [24]  Handwritten digits \\nrecognition  10 60,000  \\nT-PAMI [25]  Object detection  200 456,567  \\nCVPR [18] Face recognition  10,177  202,599  \\nCVPR [19] Face recognition  4,030  ~ 4,400,000  \\n \\n \\n \\n \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 4 \\nsuperior performance . However for other  popular biometrics \\nmodalities like iris or periocular, in the best of our knowledge, \\nthere is currently no single public database with that many \\nimages .  \\nTherefore, we are motivated to improve the performance of \\nexisting CNN based architecture in another way - to enhance \\nCNN with supervision from explicit semantic information. \\nWhen human recognizes  objects , for example  while \\nrecognizing a f ace image, one would analyze  not on ly the \\noverall visual pattern  but also the semantic information, such as \\ngender, ethnicity , age, etc., to judge whether the face image \\nbelongs to a  certain known person. Therefore, it is reasonable to \\nbelieve that semantic information is helpful for the visual \\nidentification task. For a CNN that is trained with the identity \\nlabel only, it is possible that the network is already capable of \\nacquiring  semantic information . For instance , for the \\nwell-known deep learning model for face recognition, \\nDeepID2+ , researchers discovered that although the network \\nwas trained using subject identities, certain neurons turn out to \\nexhibit selectiveness to attributes like gender, ethnicity, age, etc. \\nThese semantic attributes contribute to discriminating identities  \\n[43]. However , such useful  semantic information is expected to \\nbe implicit ly learned by the CNN . It is not easy to answer the \\nfollowing questions : \\n(1) How man y types of semantic information can be acquired ? \\nSince the discriminative capacit y of a certain CNN is \\nlimited, we c annot guarantee that all the semantic \\ninformation we prefer to have has already been included.  \\n(2) To what exten t the semantic information c an be analyzed by \\nthe trained CNN? Does it really help in the final \\nidentification ta sk, or could it be further improved?  \\nAbove problems arise  due to the nature of training popularly \\nemployed for the CNN , i.e., the loss function is usually only \\nrelated to the class labels, therefore it is hard to reveal how the \\nsemantic information c an be implicitly  acquired . In order t o \\naddress this issue, we propose to empower the CNN with the \\nability to analyze semantic information explic itly. The idea is \\nvery simple and illustrated in Figure 3.  \\n3) Semantics -Assisted CNN  \\nAs illustrated in Figure 3, we simply add a branch, which is also \\na CNN, to the existing CNN. The attached CNN is not trained \\nusing the identity of the training data but the semantic groups. \\nFor example, we could train CNN2 using the gend er \\ninformation of the training sample, i.e., let the CNN2 be able to \\nestimate the gender instead of identity, and train CNN3 using \\nthe ethnicity information. After the CNNs are trained, we can \\ncombine the output of each CNN in the way of feature fusion. \\nWe refer to such extended structure of the CNN as \\nSemantics -Assisted CNN  (SCNN  for sh ort). Despite the \\nsimplicity of t his idea , it can inherently improve the original \\nCNN by adding more discriminative power to it, w hich has \\nbeen shown from  the experiments de scribed in Section 3. \\nTheoretically , the SCNN has  the following benefits:  \\n\\uf06c Instead of letting the semantic information be learned \\nfrom the identities by the CNN in an unpredictable and \\nuncontrollable way, SCNN allows us to explicitly  recover  the preferred s emantic information that can be  helpful for  \\nthe identification t ask. As a result, the feature \\nrepresentation  from the SCNN  is accompanied by more \\nreliable semantic information that is closer to mechanism \\nin human visual system.   \\n\\uf06c The training s cheme for SCN N can reuse the same set of \\ntraining data but just labeled in another way than the \\nsimple identities. Since the labeling scheme is variable, \\nthe branches of SCNN learn the training data from \\ndifferent points of view, which is equivalent to increasing \\nthe d ata volume without really adding the number of \\ntraining samples. This c an relax the constraints on the \\nrequirements of  enormous training data for deep neural \\nnetworks to som e ext ent, i.e., instead of purs uing for \\nsuperior performance from a single CNN, we enhance the \\njoint performance of branches of CNNs with fewer \\namounts of training dat a. \\n\\uf06c The SCNN architecture and training scheme is naturally \\ncompatible for most of the existing CNN based \\napproaches . What we need is just to train some \\nindependent CNNs with  semantic grouping labels and \\njudiciously combine the features from multiple CNNs  to \\nbenefit from such training , as the semantic annotations of \\ntraining samples are also available for many public \\ndatabases. In a ddition, the architecture of SCNN is highly \\nfriendly for paralle l computing platforms.  \\nB. Application for Periocular Recognition  \\nAs discussed earlier , CNN has been successfully used for the \\nface recognition in sever al state -of-the-art approaches [18]-[19]. \\nConsidering t hat the periocular region is actually a part of face  \\nand also presents  some  structural information (eyebrow, \\neyelids, eyeball, etc.), it is reasonable to expect that CNN  can \\nbe effective  for the  periocular recognition  problem. Howev er, \\nas compared with such related  work, we are constrained by lack  \\nof large -scale periocular databases that are usually required to \\nsufficiently train a deep neural network. Therefore  we \\ndeveloped and investigated SCNN for the periocular \\nrecognition proble m. \\n1) Network Structure and Supervision Information  \\nThe detailed SCNN structure used for the periocular \\nrecognition is shown in Figure 4. In order t o examine the \\nCNN n – Trained by Semantic InformationCNN 2 – Trained by Semantic InformationCNN 1 – Trained by Identity\\nJoint \\nFeature or \\nPrediction... \\n \\nFigure 3: Structure of the proposed Semantics -Assisted CNN (SCNN). While \\nfirst branch is trained by the label of the intended tasks, other branch CNNs \\nare trained using different semantic information, t hen the branches are joint in \\nthe end to get a comprehensive feature representation or perform score fusion.  \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 5 \\nimpact of adding  branch to an existing CNN, we simply \\ndesigned one bran ch that is trained with semantic information, \\ndenoted  as CNN2 in  Figure 4. While CNN 1 is like the ones \\ncommo nly tra ined with the subject identities from  the training \\nsamples , CNN2 is designated to be train ed with the side (left or  \\nright)  and the gender information. More specifically, we \\nlabeled the training data as follows , also shown in Figure 5:  \\n\\uf0ec\\n\\uf0ed\\n\\uf0ee\\n 0 - Left and Male,  \\n1 - Right and Male,  \\n2 - Left and Female,  \\n3 - Right and Female.  \\nThe reason fo r using left/right and gender information is that \\nhuman s also tend to incorporate such judgment by visuall y \\ninspecting the presented periocular image s, although such \\naccuracy may not reach cent percent level. Therefore there is \\nsome scientif ic basis to believe that CNN can learn to \\ndistinguish above semantic information from the periocular  \\npattern s and assist in the identification task. Another reason for \\nusing gender information is that the genders of subjects are \\noften  included i n the metad ata of  many publicly available \\ndataset s, such as UBIpr [36]. Therefore we can directly use \\nthose labels to train CNN2. Other possible and useful semantic \\ninformation include iris color (light/dark), ethnicity , shape of \\neyebrow,  etc.  \\n Using such additional semantic information to supervise the network  make s the overall architecture and learning process of \\nSCNN similar to multi -label learning [44] to some extent. \\nHowever, the principal difference is t hat, the introduction of \\nsemantic labeling in our model aims to assist/supplement the \\nprediction of subject identity labels, i.e., they are inequally \\nimportant, while in traditional multi -label learning, the multiple \\nlabels are usually in equal positions. In addition, the learning \\nprocesses of identities and other semantic information are \\nseparately undertaken to maximally ensure the explicitness of \\nsemantic learning and compatibility to other CNN based model, \\nwhile in general multi -label learning, features  are usually \\njointly learned for predicting different lables. Nevert heless, in \\nspite of the diffrent iation between the identity labels and other \\nsupportive  labels, the  s emantic learning process ( e.g., CNN2 \\nitself) can also be conducted in the manner of mu lti-label \\nlearning alternatively.  \\n2) Training Protocol and Data Augmentation  \\nAmong the original training samples, the last sample of each \\nsubject is selected to form the validation set, which is tested in \\nevery certain amount of iterations to observe whether  the \\ntraining process is converging i n a right  direction or not.  \\nFurthermore, it is observed that the periocular images from \\nthe training set  are well aligned and scaled to a similar level, \\nwhile the samples from independent test datasets and real \\napplica tions may have misalignments and scale  variations . \\nSuch inconsistency can also be observed from the image \\nsamples in Figure 6.  \\nIf the deep network is trained with the well aligned and \\nscaled images, it may not be effectively gene ralized to other \\ndatasets or d ata acquired  by real applications.  In order to  \\naddress such problem s, we firstly augmented the training data \\nwith a different scale to simulate scale inconsistency in the test  \\nenvironment. Then we applied random cropping durin g the \\ntraining process to ensure that the network can accommodate  \\nspatial variations  among the periocular images.  The scale \\naugmentation and random cropping process is also  illustrated in \\nFigure 7. As illustrated in this, each ori ginal of the image in \\nCNN 2CNN 1\\nFeature or \\nScore Fusion10 x 226 x 226\\nL1\\nConvolution10 x 113 x 113\\nL2\\nPooling\\n+\\nReLU20 x 103 x 103\\nL3\\nConvolution20 x 52 x 52\\nL4\\nPooling\\n+\\nReLU40 x 46 x 46\\nL5\\nConvolution40 x 23 x 23\\nL6\\nPooling\\n+\\nReLU1 x 250\\nL7: Features\\nFully connected\\n+\\nReLU15\\n1522\\n11\\n112\\n27\\n72\\n2\\n10 x 226 x 226\\nL1\\nConvolution10 x 113 x 113\\nL2\\nPooling\\n+\\nReLU20 x 103 x 103\\nL3\\nConvolution22\\n11\\n112\\n2\\n1 x 200\\nL5: Features\\nFully connected\\n+\\nReLU20 x 52 x 52\\nL4\\nPooling\\n+\\nReLU \\nFigure 4: Structure of the employed SCNN for the periocular recognition.  \\n \\n \\n \\nSemantical Annotation\\nLeft Right\\nMale Female Male Female\\nLabel: 0                       1                       2                       3\\n \\nFigure 5: Semantical labeling used in our implantation  to train CNN2.  \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 6 \\ntraining set is automatically  cropped from its center with a size \\nof \\n0.6 0.6wh\\uf0b4 , where w and h are its original width and height \\nrespectively. The original images and its cropped patch are \\nresized to\\n300 240\\uf0b4 , then padded with symmetric  edges filled \\nwith zeroes  to a size of \\n300 300\\uf0b4 . So far one original \\nperiocular image could generate two training samples.  As a \\nresult , we have 6,270 samples for training and 448 samples for \\nvalidation w hile training for each side of the periocular  image s. \\nFurthermore, during the training process, each training sample \\nwould be cropped by a \\n240 240\\uf0b4  window randomly placed \\nwithin the image region before entering the first layer of the \\nnetw ork. Such  random ized cropping  process  from one training \\nsample could produce abundant samples that have random ized \\nmisalignments with others. In this way,  the network can be \\nenforced to learn to extract features that are robust to the \\nmisalignment s.  \\n3) Visualization of Trained SCNN  \\nOnce the networks have been trained, CNN1  is expected to \\nlock-into features that are directly relevant to the subject \\nidentities, while CNN2 is expected to analyz e the features that \\nare more related to side and the gender differe nce. In order to \\nobserve the difference among f eatures extract ed by the two \\nCNNs , we have visualize d the filter kernels from the first two \\nconvolutional layers  of trained CNN1 and C NN2  in Figure 8.  \\nWe can visuall y observe from Figure 8 that: 1) Overall both \\nCNNs were not trained sufficiently. Compared with \\nconvolutional kernels trained with large amount  of samples (e.g., those in [7]), a number  of kernels here remain flat or noisy, \\nfor which it is less likely to extract useful information.  \\nInsufficiently trained network parameters usually results in \\ncertain levels of over -fitting.  2) Despite the over -fitting concern , \\nthe convolutional filter ker nels of CNN1 and CNN2 are quite \\ndifferent. Critical  kernels in CNN2 are sharper and present \\nmore visual salience , therefore might  be more sensitive to small \\ntexture,  edges  or corners  than the filters in CNN1. This \\nindicates CNN2 can provide complementary i nformation that \\nCNN1 was not able to learn due to lack of  sufficient training \\ndata. Although the features extracted by CNN2 are not directly \\nrelated to the subject identities, it is reasonable to expect that \\nthose visual features could assist  CNN1 to form a more \\ncomprehensive visual representation of the periocular image, \\ntherefore help to distinguish different subject s finally.  \\nOriginal Imagewh\\n0.6w0.6h\\n300\\n300240\\n300 240\\nBi-scaling\\n240\\n240\\n240\\n240Training\\nTrainingPre-processed In real-time\\nRandom cropping \\nFigure 7: Illustrat ion of scale augmentation and random cropping. Each \\noriginal image is augmented to two samples with different scales, and each \\naugmented sample would be cropped by a smaller window that is randomly \\nplaced before entering the network for each epoch  of the t raining process.  \\n    \\n(a) UBIpr (training)  \\n   \\n(b) UBIRIS.v2 (testing)  \\n   \\n(c) FRGC (testing)  \\n   \\n(d) FOCS (training and testing)  \\n   \\n(e) CASIA.v4 -dista nce (testing)  \\nFigure 6: Sample images from the databases we used in the experiments. \\nScale variance and misalignment are common in the testing environment.  \\n \\nCNN1:  \\nFirst convolutional layer (L1)  Second  convolutional layer ( L3) \\n \\n \\n \\nCNN2:  \\nFirst convolutional layer (L1)  Second  convolutional layer ( L3) \\n \\n \\n \\nFigure 8: Visualization of the filter kernels from the fir st two convolutional \\nlayers of trained CNN1 and CNN2 respectively . \\n \\n \\n \\n\\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 7 \\nC. Featur e Vector  and Verification Score  Generation  \\nThe CNNs we use are trained in a classification protocol, i.e., \\nthe category  or identity of the input data is known and fixed. \\nTherefore  this network can be directly used in some \\nclassification or id entification tasks. However, in biometrics, \\none-to-one matching for probably unseen subj ects is the key \\nproblem and needs to be evaluated.  Therefore, we need to \\ngeneralize the trained model to separated subjects that are not \\nincluded in the training set, and formulate  one-to-one matching \\nscheme.  \\nSimilar to [18], we use the output of second last layer (L7 in \\nCNN1  and L5 in CNN2) as the feature representation of the \\ninput data. While the last layer represents the class prediction \\nduring the training process, the second last layer should contain \\nthe most relevant and aggregated information that can \\ncontribute to dis tinguishing the classes or identities. Therefore, \\nit is reasonable to use the output of the second last layer as the \\nfeature representation and generalize the model to unseen \\nsubje cts. Once we get the layer output vect ors, we first \\nnormalize  them by l2 norm, then apply  PCA to reduce the \\ndimensionality of the vector.  For the SCNN architecture, we \\nsimply concatenate the two independently normalized output \\nvectors to form a longer vector before PCA. In our experiments, \\nthe dimension of output vectors after PCA  is set to 80, for both \\nthe single CNN and SCNN cases. Then the joint Bayesian \\nscheme [33] is utilized to  predict the similarity between a pair \\nof feature vectors. The joint Bayesian is primarily designed for \\nface verification,  in which a face (equivalent to the periocular \\nfeature vector here) is represented by:  \\n \\nf=μ+ε  (6) \\nwhere \\nf  is the observation, in this paper the feature vector  \\nafter PCA , \\nμ is the identity of the subject, \\nε  is the intra -class \\nvariation. \\nμ  and \\nε  are assumed to be two independent \\nGaussian variables following  \\n( , )N\\uf06d0S  and \\n( , )N\\uf0650S  \\nrespectively , then the covariance of two observation  is: \\n \\n1 2 1 2 1 2 cov( , ) cov( , ) cov( , ) \\uf03d\\uf02b ff μ μ ε ε  (7) \\nThe joint distribution of a pair of observations \\n12{ , }ff  is \\nconsidered . Let \\nIH  denote the intra -person hypothesis \\nindicating that two observations are from the same person, and \\nEH\\n the extra -person hypothesis. Under \\nIH , since \\n1μ  and \\n2μ  \\nare the same, \\n1ε and \\n2ε  are independent, the covariance matrix \\nof the  distribution \\n12( , | )I PHff  is: \\n \\n+\\n+I\\uf06d \\uf065 \\uf06d\\n\\uf06d \\uf06d \\uf065\\uf0e9\\uf0f9\\uf03d\\uf0ea\\uf0fa\\n\\uf0eb\\uf0fbS S S\\nΣS S S  (8) \\nOn the other hand, under \\nEH , \\n1μ and \\n2μ  are also independent, \\ntherefore the covariance matrix has become:  \\n \\n+\\n+I\\uf06d\\uf065\\n\\uf06d\\uf065\\uf0e9\\uf0f9\\uf03d\\uf0ea\\uf0fa\\n\\uf0eb\\uf0fb0\\n0SS\\nΣSS  (9)  With above conditional joint probabilities, the log likelihood \\nratio which tells the difference between intra - and extra -person \\nprobabilities can be obtained in a closed form:  \\n \\nT T T 12\\n1 2 1 1 2 2 1 2\\n12( , | )( , ) 2( , | )I\\nEPHrPH\\uf03d \\uf03d \\uf02b \\uf02dfff f fΑffΑf f Gfff  (10) \\nwhere  \\n \\n( ) ( )\\uf06d\\uf065\\uf03d \\uf02b \\uf02d \\uf02bΑ S S F G  (11) \\n \\n1\\n\\uf06d \\uf065 \\uf06d\\n\\uf06d \\uf06d \\uf065\\uf02d\\uf02b \\uf02b \\uf0e9\\uf0f9 \\uf0e9\\uf0f9\\uf03d\\uf0ea\\uf0fa \\uf0ea\\uf0fa\\uf02b \\uf02b \\uf0eb\\uf0fb \\uf0eb\\uf0fbS S S F G G\\nS S S G F G  (12) \\n The covariance matrix \\n\\uf06dS  and \\n\\uf065S  can be estimated using \\nan EM based algorithm as detailed in [33], and the log \\nlikelihood ratio \\n12( , )rff  is used as the similarity score in our \\none-to-one matching scenario.  \\nIII. EXPERIMENTS  AND RESULTS  \\nIn this section we provide the details on the experiments and \\nanalyze the results. The experimental de tails on the periocular \\nidentification are firstly provided and this is followed by details \\non supporting experiments for the image classification.  \\nA. Periocular Recognition  \\n1) Training and Testing Datasets and Protocol  \\nWe use follo wing publicly available datab ases for the \\nexperiments . Two different databases were employed for \\ntraining the deep neural networks and three separate databases \\nwere employed for the testing . \\n \\n\\uf06c UBIpr [36] - for training  \\nWe employed UBIpr periocular database [26] for training the \\nSCNN  for the visible spectrum . This database originally \\ncontains 5,126 images for each of left and right perioculars \\nfrom 344 subjects. However, we are also employing a subset of \\nUBIRIS.v2 database  [4] for separate test experiments , which \\nhas some overlapping subjects with the UBIpr database. In \\norder to ensure that subjects of training set and testing set are \\nmutually exclusive,  we removed these overlapping subjects \\nfrom UBIpr database before we perform training on the \\nnetwork. As a result, we only have 3359 periocular images \\nfrom  each of the two sides of 224 subjects. Such a scale is \\nrelatively small as compared w ith those in  the training \\nprotocols in other typical deep learnin g work like ImageNet [27] \\nor LFW [28]. Therefore, the application scenario is good for \\nvalidating the ability of SCNN f or learning comprehensive  \\ninformation from limited size of training data.  \\n \\n\\uf06c UBIR IS.v2 [4] \\nThe UBIRIS.v2 database is prima rily released for evaluation of \\nat-a-distance iris segmentation and recognition algorithms \\nunder visible illumination and challenging imaging \\nenvironment. Since the eye images in this da tabase contain \\nsurrounding regions of the eye, it is possible to perform  \\nperiocular recogn ition on the UBIRIS.v2 database. Similar to \\nas in [2], we use a subset of 1,000 images from this database \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 8 \\nthat is released in NICE.I comp etition [29]. This subset \\ncontains left and right eye images together from 161 subjects \\nthat are captured from 3m to 8m, bringing serious scale \\ninconsistency. Some images only contain the eye region \\nwithout eyebrow and other su rrounding texture which makes \\nthe task of perioc ular recognition  highly challenging . Some \\nsample images are shown in Figure 6(b). \\n \\n\\uf06c FRGC [30] \\nThe dataset of Face Recognition Grand Challenge (FRGC) is \\nreleased by the National Institute of Standards and Technology \\n(NIST) and has been primarily for the evaluation of new \\nalgorithms for the automated face recognition. Similar to  as in  \\n[2], we automatically extrac ted the periocula r region from the  \\noriginal face images of FRGC using publicly available face and \\neye detector [31]-[32]. A subset of 540 right eye images  from \\n163 subjects , same as also the ones used in [2], were employed \\nin the experiments.  Some sample images are reproduced in \\nFigure 6(c).  \\n \\n\\uf06c FOCS [34] - for training  and testing  \\nThe Face and Ocular Challenge Series  (FOCS) dataset is also \\nreleased by NIST and contains face, ocular images and videos. \\nWe employ ed the “OcularStillChallenge1 ” section , which  \\nconsist s of 4,792 left and 4,789 right periocular images from \\n136 subjects that are cropped from face video clips acquired \\nunder near -infrared (NIR) spectrum. The periocular samples \\nfrom thi s dataset, as shown in Figure 6, suffer from serious \\nillumination inconsistency and misalignment s, therefore this \\ndataset is considered as highly challenging.  We used 3,262 left \\nand 3,259 right periocular images of the first 80 subjects to train \\nthe CNNs and used the remaining images from 56 subjects for \\ntesting. Again, such a scale of training samples and subjects is \\nsmall compared with other typical deep learning tasks.  \\n \\n\\uf06c CASIA.v4 -distance [35] \\nCASIA.v4 is t he first publicly available long -range iris  and face \\ndatabase acquired u nder NIR illumination , which is released by \\nthe Center for Biometrics and Security Research (CBSR) from \\nthe Chinese Acade my of Sciences (CASIA). The full database \\ncontains  2,567 images from 142 subjects in single session. The \\nstandoff distance of the subjects to the camera is from 3 meters \\naway. Similar to FRGC, w e used publicly available eye \\ndetector [31]-[32] to automatically segment left periocular  \\nimages which are used in our experiments. The first eight \\nsamples of each subject , excluding a few badly segmented \\nimages , were used for the periocular matching experimen t.  \\nAbove  datasets were selected  for evaluation  because of the \\navailability of periocular images acquired under less constrained environments that are close to  real world scenarios. \\nThe selected subsets from FRGC and UBIRIS.v2 contain \\nmulti -session data an d exhibit obvious scale/illumunation \\nvariation. Samples in FOCS database suffer from sig nificant \\nillumination degradation  and misalignment. Images from  \\nCASIA.v4 -distance are  more consistent than the other three \\ndatabases, but were acquired at a distance an d some contain \\nartifacts like glasses and/or hair, therefore also represent less \\nconstrained scenari os. In addition, networks for visible and \\nNIR spectrums were trained separately due to the significant \\ndifference between the image properties.  \\nIt is import ant to clarify that during our (reproducible [40]) \\nexperiments, the SCNN is tested in totally cross -database  \\nmanner , i.e., not only the su bjects from the training and test  set \\nsets are totally separated, the databases themselves are \\nindependent  from traini ng for three sets  of experiments. \\nHowever, the methods we are going to compare  with, [2] and \\n[10], both requir e some  samples of the target databases for the \\ntraining. In order to compare with the be st performance of [2] \\nand [10] as well as to ensure the fairness in such comparison, \\nwe still divide the target datasets into training and testing sets, \\nas summarized  in Table 2. For example, 96 samples of the first \\n19 subjects in UBIRIS.v2 were used to train the models [2] and \\n[10], the remaining were used for test as in [2], [10] and also for \\nour method. Such a configuration is highly disadvantageous to \\nour methods because the inter -database variance is always a \\nkey factor for the performance of all learning based methods. \\nHowever, our method has still been ab le to achieve \\noutperforming results as detailed later.  \\nWe perform periocular mat ching using  the all -to-all protocol, \\ni.e., every image is matched to al l the other images in the testing \\nset, and all the generated matching scores are taken into \\ncalculation o f the receiver operating characteristic (ROC) curve.  \\nSuch a protocol is considered to be highly challenging because \\none bad sample may result in several poor genuine scores, \\nwhich drops the overall matching performance.  \\n2) Effectiveness of SCNN  \\nWe firstly ex amine the i mpact of the added branch that has been \\ntrained with the semantic information. We have compared the \\nperformance of a single CNN, i.e., only CNN1 in Figure 3, with \\nthe performance of the extended SCNN. The  results from t he \\nverification  experiments are illustrated i n Figure 9.  \\nWe can observe from Figure 9 that the SCNN consistently  \\nachieves better performance than that of original or single CNN. \\nThis observation suggests  that the newly added CNN2 which is \\ntrained with semantic supervision has been successful in \\ncontributing to some useful information that is not reinforced in \\nCNN1, and therefore improving the overall discriminative Table 2: Summary of the employed databases for training and testing.  \\nSpectrum  Visible  Near Infrared (NIR)  \\nDivision  Training set  Testing set  Training set  Testing  set \\nDataset  UBIpr  UBIRIS.v2  FRGC  FOCS  FOCS  CASIA.v4 -distance  \\nStandoff  distance  4 – 8m 3 - 8m N/A N/A N/A ≥3m \\nNo. of subjects * 224 171(19/152)  163 (13/150)  80 56 141 (10/131)  \\nNo. of image s* left: 3,359  \\nright: 3,359  1,000 (96/904)  540 (40/500)  left: 3,2 62 \\nright: 3,259  1,530  1,077 (79/998)  \\n* In the bracket ( a/b) means a subjects  or images  were used for training for methods [2] and [10] (not for our method) , remaining  b subjects  or images  were \\nused for testing.  \\n \\n \\n \\n \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 9 \\npower of the network. In theory, we can add more branches that \\nare trained with different semantic information ( e.g., iris color) \\nto further improve the final recognition accuracy. However, the \\nneed for computational power would also increase  and the \\ntrade -off may need to be made according to the applications. In \\nour example, since CNN2 shown in Figure 4 has a relatively \\nsimpl ified structure, the additional training cost is minor .  \\n3) Comparison with Earlier  Work on Periocular Recognition  \\nWe also compared the performance o f our approach with \\nstate-of-the-art approaches [2], [10] on the periocular \\nrecognition problem. While [2] is our previous work, w e have \\ncaref ully implemented the method s in [10] with the help of the \\noriginal  authors. The t est protocol s were  kept exactly  the same \\nfor different approaches during the experimental process and \\ntherefore the comparis ons of ROC /CMC curves are  fair. \\nHowever  several  factors can be firstly clarified here to ensure \\nclarity in understanding the experimental c omparisons.   1) For U BIRIS.v2, we use the 1,000 image set that was \\nemployed f or the NICE.I  competition . This subset is  the same \\nas was used in [2] but different  from the one in [10]. In [10], test \\nimages were gathered from the full dataset, but only those \\nacquired from 6 -8 meters were used, while the 1,000 image set \\nin [2] included samples acquired from 3 -8 meters. Due to the \\nrelatively consistent imaging distance, the subset used in [10] \\ninvolves much less scale variance than those  in [2] and also in \\nthis paper . As a result , the performance from our experiment \\nusing exact method in [10] is not reproduced as good as what \\nappears to be in [10] and this is reasonable  due to the difference \\nin selection of image s as explained above . \\n2) For FRGC, we also used the same subset as in [2] but \\ndifferent from the one  used in [10]. As described before , the \\nsubset we used contains 540 periocular images which were  \\nautomatically  segmented  from the original fa ce images and \\ntherefore may suffer from some misalignment. Moreover, \\nimages in this subset were acquired from various sessions  with \\ncertain time lapse  and different imaging environments , which \\nincreases the difficulty for accurate recognition. However,  the   \\n(a) UBIRIS.v2                                                                                 (b) FRGC  \\n \\n(c) FOCS                                                                           (d) CASIA.v4 -distance  \\nFigure 9: ROC curves of the periocular verification using SCNN and comparison with single CNN and other state -of-the-art methods for different databases.  \\n10-310-210-110000.10.20.30.40.50.60.70.80.91\\nFalse Accept RateVerification Rate\\n  \\nSCNN (EER: 10.98%)\\nCNN1 (EER: 13.76%)\\nTIFS\\'15 (EER: 34.96%)\\nTIP\\'13 (EER: 24.49%)\\n10-310-210-110000.10.20.30.40.50.60.70.80.91\\nFalse Accept RateVerification Rate\\n  \\nSCNN (EER: 10.93%)\\nCNN1 (EER: 11.88%)\\nTIFS - EER: 27.55%\\nTIP (EER: 19.46%)\\n10-310-210-110000.10.20.30.40.50.60.70.80.91\\nFalse Accept RateVerification Rate\\n  \\nSCNN (EER: 10.47%)\\nCNN1 (EER: 11.79%)\\nTIFS\\'15 [10] (EER: 20.09%)\\nTIP\\'13 [2] (EER: 23.90%)\\n10-310-210-11000.50.60.70.80.91\\nFalse Accept RateVerification Rate\\n  \\nSCNN (EER: 6.61%)\\nCNN1 (EER: 7.27%)\\nTIFS\\'15 (EER: 10.46%)\\nTIP\\'13 (EER: 8.27%)\\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 10 \\nsubset used in  [10] only consist s of images captured in \\nconsistent  illumination and background in single session , and \\nthe periocular regions were manually  segmented. Therefore, it \\nis also a reasonable explanation for the drop  in performance in \\nour reproduced results, over the ones shown in [10] using \\nmanual segmentation.   \\n3) For FOCS, we used fixed division of training and testing sets \\nas shown in Table 2, while the original setup in [10] used 5-fold \\ncross validation for  the entire  dataset. Although the  subsets \\nused in our experiment  and their original experiment  are not \\nexactly the same, the quality of images is observed to be quite \\nsimilar . Therefore our reproduced resul t is very close to those \\nappearing in reference [10]. \\nThe verification  results  (ROC)  for above comparison s are \\nalso shown in  Figure 9, while the identification results (CMC) \\nare shown in  Figure 10. It can be observed from the \\nexperimental results in these two figures that the proposed \\napproach using SCNN consistently outperforms  the two  \\nstate-of-the-art approaches.   \\nIn order to ascertain statistical significance of the \\nimprove ments, we have conducted the significance test for the \\nROC curves usi ng the method described in [12], which judges Table 3: Results of significance test for comparison of ROCs using method  [12]. \\np-value indicates the probability of the null hypothesis that two methods have no \\ndifference statistically.  \\nComparison  p-value  \\nUBIRIS.v2  FRGC  CASIA.v4\\n-distance  FOCS  \\nSCNN &  TIP’13 [2] < 1e-4 < 1e-4 < 1e-4 < 1e-4 \\nSCNN &  TIFS ’15 [10] < 1e-4 < 1e-4 < 1e-4 < 1e-4 \\n*  The computed z-statistics are too large that the corresponding p-values \\nexceed double precision, therefore expressed as  < 1e-4. \\n \\n \\n \\n \\n Table 4: Comparison of time required to match two periocular images by \\ndifferent approaches, f rom Matlab im plementation running on a computer with \\nLinux OS, 16 GB RAM, 3.4 GHz Intel i7-4770  CPU (4 cores) and NVIDIA \\nGeForce GTX 670  GPU.  \\nApproach  Major Time Consuming \\nOperations  Matching Time (s)  \\nGPU  CPU  \\nproposed  convolution, matrix \\nmultiplication  0.013  0.183  \\nTIP’13 [2] DSIFT feature extraction , \\nK-means clustering  / 15.478  \\nTIFS ’15 \\n[10] Gabor feature extraction , \\ncorrelation filter matching  / 1.441  \\n \\n \\n \\n \\n  \\n(a) UBIRIS.v2                                                                                 (b) FRGC  \\n \\n(c) FOCS                                                                            (d) CASIA.v4 -distance  \\nFigure 10: CMC curves of the periocular verification using SCNN and comparison with state -of-the-art methods for different databases.  \\n1001011020.70.750.80.850.90.951\\nRankRecognition Rate\\n  \\nSCNN (Rank 1 = 82.43%)\\nTIP\\'13 [2] (Rank1 = 74.56%)\\nTIFS\\'13 [10] (Rank1 = 71.08%)\\n1001011020.70.750.80.850.90.951\\nRankRecognition Rate\\n  \\nSCNN (Rank 1 = 91.13%)\\nTIP\\'13 [2] (Rank 1 = 85.36%)\\nTIFS\\'15 [10] (Rank 1 = 78.34%)\\n1001010.70.750.80.850.90.951\\nRankRecognition Rate\\n  \\nSCNN (Rank 1 = 96.93%)\\nTIFS\\'15 [10] (Rank 1 = 91.83%)\\nTIP\\'13 [2] (Rank 1 = 81.16%)\\n1001011020.970.9750.980.9850.990.9951\\nRankRecognition Rate\\n  \\nSCNN (Rank 1 = 98.90%)\\nTIP\\'13 [2] (Rank 1 = 98.82%)\\nTIFS\\'15 [10] (Rank 1 = 97.61%)\\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 11 \\nfrom the area under the curve (AUC). Table 3 shows the \\nsignificance level  (p-value)  of the difference of the SCNN \\nbased method over the comparative methods [2] and [10]. The \\nresults indicate  that, by the commonly used confidence level  of \\n95%, our approach significantly outperforms th ese two \\nmethods ( p-value < 0.05) on all the employed datasets.  \\nIt may be noted that [10] perform ed poorly on the \\nUBIRIS.v2 set because it ad opts the patch based matching \\nscheme while , as explained above,  the 1,000 -image set of \\nUBIRIS.v2 used in our experiment suffers from serious scale  \\nvaria tions among the samples, which results in significant loss \\nof patch correspondence.  The approach from [2] which uses \\nDSIFT features is more robust to scale varian ce, however the \\nextraction of DSIFT fea ture is esp ecially time consuming. In \\ncontrast, our approach not only performs better than both of  the \\nbaseline approaches on different  databases, but is also \\ncomputationally simpler for the deployment using the train ed \\nnetwork . Table 4 presents  the summary of the average time \\nrequired for the feature extraction for the considered \\nstate-of-art approaches. These tests were performed usi ng the \\nMatlab  wrapper and C++ implementation  running on a \\ncomputer with Linux OS, 16 GB RAM, 3.4 GHz Intel® Core™ \\ni7-4770  CPU (4 cores) and NVIDIA ® GeForce GTX 670  GPU.  \\nIt can be observed that the proposed approa ch is much faster  \\ndue to the  straightforward architecture and the use of GPU \\ncould further reduce the c omputational time.   \\nB. Image Classification  \\nIn order  to examine that the proposed SCNN architecture is not \\nonly effective  for the periocular recognition but can also be \\nuseful f or more general problems, we performed experiment for \\nimage classification on the CIFAR -10 dataset  [37].  \\nThe CIFAR -10 dataset contains 60,000 32 ×32 color images \\nfrom 10  classes. Among these  images, 50,000 images are for \\ntraining and 10,000 are for testing.  Figure 11 shows some \\nrandomly selected samples from each class.  As we can see from  \\nFigure 11, although the number of classes is not large, the \\nintra-class variation  is significant and the resolution is also \\nsmall er, which brings certain challenge for cla ssifying those \\nimages. The CIFAR -10 has therefore emerged as  a popular \\ndataset for evaluating image cl assification algori thms along \\nwith others like ImageNet a nd CIFAR -100, etc.  \\nSince the SCNN is devel oped to enhance existing CNN \\nbased approaches, we select a baseline  CNN to  ascertain the \\nimprovement . We adopt the CNN originated from \\nKrizhevsky ’s cuda -convnet  [38], re -implemented and  \\nintroduced in the Caffe tutorial [39]. Although the selected \\nCNN i s not the state-of-the-art for CIFAR -10 in terms of \\nperformance, we chose  it because this model  is publicly \\navailable under Caffe, the deep learning frame work employed  \\nin the paper, and it is also quick to train. For simple annotation, \\nwe refer to this net work as cuda -convnet . By following the \\ntutorial, we can quickly get an accuracy of about 75% on the \\nCIFAR -10 test set. The n we trained a branch CNN to learn the \\nsemantic features of the images in CIFAR -10 in order to build \\nthe SCNN a rchitecture. We define one possible groups of \\nsemantic inform ation for the classes in the CIFAR -10 dataset as \\nfollows , also shown in  Figure 12. \\n\\uf0ec\\n\\uf0ed\\n\\uf0ee artificial  \\n\\uf07b rectangular , has wheel: (aut omobile, truck)  \\nno/invisible wheel: (airplane, ship)  \\nNatural  \\n\\uf07b round , short: ( cat, dog, bird , frog ) \\nslim, long : (deer, horse)  \\n \\nWith above division, the entire  dataset is grouped into four \\nsemantical classes. It may be n oted that this is not the unique or \\nthe optimal division, but it is an easy -to-understand scheme  to \\nstart with.   In order to  obtain  a branch CNN that was trained to \\nacquire  above semantic features, we simply duplicate the \\nstructure of the base cuda -convne t but replace the last fully \\nconnected layer having 10 neurons with a new fully connected \\nlayer with four neurons, since the task now is to recognize the \\nfour semantic groups. We then just repeat, as described in Caffe \\ntutorial , but train the new network w ith newly labeled data . We \\nrefer to this new CNN as cuda -convnet -s. Again, above \\nconfiguration is made because of the ease to execute and one airplane  \\n auto- \\nmobile  \\nbird \\ncat \\ndeer \\ndog \\nfrog \\nhorse  \\nship \\ntruck  \\n \\nFigure 11: Sample images from each class of CIFAR -10 dataset.  \\n \\nSemantical Annotation\\nArtificial Natural\\nRetangular,\\nHas wheelNo/invisible\\nwheelRound , \\nShortSlim, \\nLong\\nLabel: 0                       1                       2                       3\\n \\nFigure 12: The semantical group label ing used in our experiment to train \\ncuda -convnet -s. \\n \\n \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 12 \\nhas many choice s for actual applications. We then built an \\nSCN N with the architecture as in  \\nFigure 13. As shown in th is figure, we combine the branch CNN \\nand the original one to obtain  an extended structure. The \\ncomponents highlighted in red are retrained after the \\ncombination to aggregate the long concatenated features, and \\nthis process can be considered as a kind of finetuning. Since the \\nnumber of layers to be retrained is small, the finetuning is very \\nfast.  Table 5 shows the classification results on the test set \\nusing the original  cuda -convnet and the extended SCNN.   \\n We c an observe from the results that the proposed SCNN can \\nachieve an improvement of 2.11% over the original result. \\nAlthough this may not be considered as  a very large \\nimprovement, the achieved results reinforce the motivation for \\nSCNN is to mak e solid and consistent enhancement on existing \\nCNN based approaches, especially for the scen ario when  the \\ntraining data may not be enough to feed a complex n etwork. In \\nthe CIFAR -10 dataset, the number of images per class is \\nactually quite large a nd therefore the effect of SCNN is not \\nsignificant, but it still offers a noticable improvement with \\nminor addition in the complexity. Moreover, as discussd  above, \\nthe experimental setup  is reproducible and made  to execute in a  \\nstraigh tforward  manner. Therefore  it is reasonable to expect  \\ncertain space for further  improve ment .  \\nIV. CONCLUSIONS  \\nThis paper has presented automated periocular recognition \\nusing CNN with outperforming results and significantly \\nsmaller complexity. In particular,  we propose d a robust and \\nmore accurate framework for the periocular recognition  using  \\nthe semantics -assisted convolutional neural network (SCNN). \\nBy training on e or more branches of CNNs with semantical \\ninformation corresponding to training data, the SCNN  is \\ncapable of recovering more comprehensive features from the \\nimage s and therefore achiev e superior performance. Our \\nexperimental results on four publicly available databases \\nsuggest that t he proposed approach  can achieve outperforming \\nresults while requi ring much smaller computational time for the \\nmatching process.  The SCNN architecture can also be generalized for other image classification tasks, which can \\nimprove the performance over the single CNN based \\napproach es. The source and executable  files of ou r approach \\nare made publicly available [40] t o encourage other researchers \\nto easily reproduce our results  and further advance research on \\naccurate periocular recognition . \\n It may be noted that a t the current stage , we decouple the \\nidentity supervision and  other semantic supervision , in order to \\nensure h igh level of explicitness of  semantic learning  and \\ncompatibility to existing CNN based approach es. However, it is \\nbelieved that a well -designed network structure may explicitly \\nincorporate semantic informati on itself and facilitate  efficient \\ntraining in an end -to-end training manner. It will be our future \\nwork to investigate improved architecture which enables joint \\nlearning of semantic information explicitly as well as \\npreserving the network integrity.  \\nACKNOWLEDGEMENT  \\nAuthors thankfully acknowledge the support and help received \\nfrom the authors of reference [10] in reproducing results from \\ntheir approach  employed for comparisons in this paper.  \\nREFERENCES  \\n[1] V. Nair and G. Hinton, \"Rectified linear units imp rove restricted \\nboltzmann machines\", Proc .  27th Int l. Conf . Machine Learning  (ICML) , \\n2010, pp. 807 -814, 2010. . \\n[2] C. W. Tan and A. Kumar, \"Towards Online Iris and Periocular \\nRecognition Under Relaxed Imaging Constraints\", IEEE Transactions on \\nImage Processin g, vol. 22, no. 10, pp. 3751 -3765, 2013. . \\n[3] L. Nie, A. Kumar and S. Zhan, \"Periocular Recognition Using \\nUnsupervised Convolutional RBM Feature Learning\", Proc.  22nd In tl. \\nConf. on Pattern Recognition , ICPR 2014, Stockholm, pp. 399 -404, 2014  \\n[4] H. Proenca, S. Fi lipe, R. Santos, J. Oliveira and L. Alexandre, \"The \\nUBIRIS.v2: A Database of Visible Wavelength Iris Images Captured \\nOn-the-Move and At -a-Distance\", IEEE Transactions on Pattern \\nAnalysis and Machine Intelligence , vol. 32, no. 8, pp. 1529 -1535, 2010.   \\n[5] A. Sh arma, S. Verma, M. Vatsa and R. Singh, \"On cross spectral \\nperiocular recognition\", Proc. Intl. Conf.  Image  Processing , ICIP 2014, \\npp. 5007 -5011.  2014.  \\n[6] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient -based learning \\napplied to document recognition\" , Proc . IEEE , vol. 86, no. 11, pp. \\n2278 -2324, 1998 . \\n[7] A. Krizhevsky , I. Sutskever, and  G. Hinton,  “Imagenet classification with \\ndeep convolutional neural networks ”, In Advances in neural information \\nprocessing systems , 2012, pp. 1097 -1105 . \\n[8] U. Park, A. Ross, and A . K. Jain , \"Periocular biometrics in the visibl e \\nspectrum: A feasibility study \", in Biometrics: Theory, Applications, and \\nSystems  (BTAS) , 2009 . IEEE 3rd International Conference on . 2009 , pp. \\n1-6. \\n[9] G. Santos  and H. Proenca , \"Periocular biometrics: An e merging \\ntechnol ogy for unconstrained scenarios \", in Computational Intelligence \\nin Biometrics and Identity Management (CIBIM), 2013 IEEE Workshop \\non, 2013, pp. 14 -21. \\n[10] J. Smereka, V. Boddeti and B. Vijaya Kumar, \"Probabilistic Deformation \\nModels for Challeng ing Periocular Image Verification\", IEEE \\nTrans.Inform.Forensic Secur. , vol. 10, no. 9, pp. 1875 -1890, 2015.  \\n[11] F. Juefei -Xu, K. Luu,  M. Savvides, T . D. Bui  and C . Y. Suen , \\n\"Investigating age invariant face recognition based on periocular \\nbiometrics.\" In Biome trics (IJCB), 2011 International Joint Conference \\non, 2011, pp. 1 -7. \\n[12] E. DeLong, D. DeLong and D. Clarke -Pearson, \"Comparing the Areas \\nunder Two or More Correlated Receiver Operating Characteristic Curves: \\nA Nonparametric Approach\", Biometrics , vol. 44, no.  3, p. 837, 1988.  \\n[13] D. L. Woodard , S. Pundlik , P. Miller , R. Jillela, and A. Ross , \"On the \\nfusion of periocular and iris b iometrics in non -ideal imagery \", Pattern \\nRecognition (ICPR), 2010 20th IEEE International Conference on , 2010 , \\npp. 201-204. \\ncuda-convnet\\nL1 L2 L3 L4 L5 L6\\n...\\nL7\\n...\\n...\\nL8 Prediction\\nto 10 \\nclasses\\ncuda-convnet-s\\nL1 L2 L3 L4 L5 L6\\n...\\nL7 \\n \\nFigure 13: The structure of SCNN used in the experiment for CIFAR -10 \\ndataset . The cuda -convnet  is from the original Caffe tutorial, and the \\ncuda -convn et-s is newly trained by the semantic information.  \\n \\n \\n Table 5: Results of classification on the CIFAR -10 testing set using original \\nexisting cuda -convnet  and the proposed SCNN enhancement on the \\ncuda -convnet . \\nApproach  Accuracy  \\ncuda-convnet  74.95%  \\ncuda -convnet -SCNN  77.06%  \\n \\n \\n \\n \\n \\n1556-6013 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2016.2636093, IEEE\\nTransactions on Information Forensics and Security\\n \\n 13 \\n[14] A. Kumar, \"N eural network based defection of local textile defects,\" \\nPattern Recognition,  vol. 36, pp. 1645 -1659, Jul y 2003  \\n[15] R. Jillela and A . Ross. \"Mitigating effects of plastic surgery: Fu sing face \\nand ocular biometrics \", in Biometrics: Theory, Applications and Syst ems \\n(BTAS), 2012 IEEE Fifth International Conference on , 2012 , pp. \\n402-411. \\n[16] R. Girshick , J. Donahue , T. Darrell  and J.  Malik , \"Rich feature hierarchies \\nfor accurate object dete ction and semantic segmentation \", in Computer \\nVision and Pattern Recognition (CV PR), 2014 IEEE Conference on , 2014 , \\npp. 580 -587. \\n[17] C. Szegedy , W. Liu, Y . Jia, P . Sermanet, S . Reed, D . Anguelov, D. Erhan, \\nV. Vanhoucke, and A . Rabinovich , \"Going deeper with c onvolutions \", in \\nComputer Vision and Pattern Recognition (CVPR), 201 5 IEEE \\nConfer ence on , 2015, pp. 1 -9. \\n[18] Y. Sun, X . Wang, and X . Tang , \"Deep learning face representation  from \\npredicting 10,000 classes \", in Computer Vision and Pattern Recognition \\n(CVPR), 2014 IEEE Conference on , 2014 , pp. 1891 -1898.  \\n[19] Y. Taigman , M. Yang , M. A. Ranzato  and L. Wolf , \"Deepface: Closing \\nthe gap to human -level p erformance in face verification \", in Computer \\nVision and Pattern Recognition (CVPR), 2014 IEEE Conference on , 2014 , \\npp. 1701 -1708.  \\n[20] N. Srivastava , G. Hinton , A. Krizhevsky , I. Sutskever  and R.  \\nSalakhutdi nov, \"Dropout: A simple way to prevent neural networks fro m \\noverfitting \", The Journal of Machine Learning Research , vol. 15, no. 1 pp. \\n1929 -1958 , 2014 . \\n[21] K. He, X. Zhang , S. Ren and J.  Sun, \"Deep Residual  Learning for Image \\nRecognition \", The IEEE Conference on Computer Vision and Pattern \\nRecognition (CVPR) , June, 2016 . \\n[22] S. Bharadwaj , H. S. Bhatt , M. Vatsa and R. Singh,  \"Periocular biometrics: \\nWhen iris recogn ition fails \", in Biometrics: Theory, Applications, and \\nSystems  (BTAS) , 2010 Fourth IEEE International C onference on , 2010 , \\npp. 1 -6. \\n[23] J. Deng , W. Dong , R. Socher , L. J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A \\nlarge -scale hierarchical image database \", in Computer Visi on and Pattern \\nRecognition  ( CVPR ), 2009  IEEE Conference on , 2009 , pp. 248 -255. \\n[24] L. Wan , M. Zeiler, S. Zhang , Y. Lec un, and R. Fergus , “Regularization of \\nneural networks using dropconnect ”, Proc . 30th Int l. Conf. on Machine \\nLearning , ICML 2013, 1058 -1066 , 2013 . \\n[25] K. He, X. Zhang, S. Ren and J. Sun, \"Spatial Pyramid Pooling in Deep \\nConvolutional Networ ks for Visual Recognition\", IEEE Transactions on \\nPattern Analysis and Machine Intelligence , vol. 37, no. 9, pp. 1904 -1916, \\n2015.  \\n[26] C. N. Padole and H. Proenca. \"Periocular recognition: Analysis of \\nperformance degradation factors.\"  in Biometrics (ICB), 2012 5 th IAPR \\nInternational Conference on . IEEE, 2012 , pp. 439 -445. \\n[27] ImageNet Large Scale Visual Recognition Challenge (ILSVRC) : \\nhttp://www.image -net.org/challenges/LSVRC/  \\n[28] G. B. Huang, M. Ramesh,  T. Berg and E. Learned -Miller , Labeled faces \\nin the wild: A database for studying face recognition in unconstrained \\nenvironments . vol. 1. no. 2, Technical Report 07 -49, University of \\nMassachusetts, Amherst, 2007.  \\n[29] H. Proença and L . A. Alexandre , \"The NICE. I: Noisy  iris challenge \\nevaluation -part I \", in Biometrics: Theory, A pplications, and Systems  \\n(BTAS ) 2007 First IEEE International Conference on , 2007 , pp. 1 -4. \\n[30] \"FRGC dataset\", 2016. [Online]. Available: \\nhttp:/ /www.nist.gov/itl/iad/ig/frgc.cfm . [Accessed: 29 - Mar- 2016].   \\n[31] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software \\nTools, 2000.  \\n[32] P. Viola and M. Jones, “Rapid object detection using a boo sted cascade of \\nsimple features ”, in Computer Vision and  Pattern Recognition (CVPR), \\nProceedings of the 2001 IEEE Computer Society Conference on , 2001, \\nvol. 1, pp. I -511. \\n[33] D. Chen , X. Cao, L. Wang , F. Wen and J.  Sun, \"Bayesian face  revisited: A \\njoint formulation \", in Computer Vision -ECCV 2012 , Springer Berlin \\nHeidelberg, 2012 , pp. 5 66-579. \\n[34] \"FOCS  dataset\", 2016. [Online]. Available: \\nhttp://www.nist.gov/itl/iad/ig/focs.cfm . [Accessed: 29 - Mar- 2016] . \\n[35] \"CASIA.v4  dataset\", 2016. [Online]. Available: \\nhttp://biometrics.idealtest.org/dbDetailForUser.do?id=4 . [Accessed: 29 - \\nMar- 2016] .  \\n[36] \"UBIpr  dataset\", 2016. [Online]. Available: \\nhttp://socia -lab.d i.ubi.pt/~ubipr/ . [Accessed: 29 - Mar- 2016] .  \\n[37] \"CIFAR -10 dataset \", 2016. [Online]. Available: \\nhttps://www.cs.toronto.edu/~kriz/cifar.html . [Accessed: 29 - Mar- 2016] .  [38] \"cuda -convnet -   High -perfor mance C++/CUDA implementation of \\nconvolutional neural networks - Google Project Hosting\", \\nCode.google.com, 2016. [Online]. Available: \\nhttps://code.google.com/p/cuda -convnet/ . [Accessed: 29 - Mar- 2016] .  \\n[39] \"Caffe | CIFAR -10 tutorial\", Caffe.berkeleyvision.org, 2016. [Online]. \\nAvailable: \\nhttp://caffe.berkeleyvision.org/gathered/examples/cifar10.html . \\n[Accessed: 29 - Mar- 2016].   \\n[40] Weblink to download codes for methods in this paper, \\nhttp://www.comp.polyu.edu.hk/~csajaykr/scnn.rar  \\n[41] \"Biometric Evaluations Homepage\", Nist.gov , 2016. [Online]. Available: \\nhttp://www.nist.gov/itl/iad/ig/biometric_evaluations.cfm . [Accessed: 29 - \\nMay- 2016].  \\n[42] \"Stock Photo - epa01034158 Demonstrators cover their faces during \\nclashes at the end of the far leftist \\'No Bush -No Wa r\\' rally\",  Alamy , 2016. \\n[Online]. Available: \\nhttp://www.alamy.com/stock -photo -epa01034158 -demonstrators -cover -t\\nheir-faces -during -clashes -at-the-97583185.html . [Accessed: 30 -May- \\n2016].  \\n[43] Y. Sun , X. Wang and X. Tang,  “Deeply learned face representations ar e \\nsparse, selective, and robust,” in Computer Vision and Pattern \\nRecognition, 2015 IEEE conference on , pp. 2892 -2900 , 2015 . \\n[44] G. Tso umakas and I. Katakis, \"Multi -Label Classification: An \\nOverview\", International Journal of Data Warehousing and Mining , vol. \\n3, no. 3, pp. 1 -13, 2007.  \\n \\n \\n \\n \\n \\n \\n',\n",
       " 'Error',\n",
       " ' \\n \\n  \\nAbstract  \\n \\nA key challenge of facial expression recognition (FER) \\nis to develop effective representations to balance the \\ncomplex distribution of intra- and inter- class variations. \\nThe latest deep convolutional networks proposed for FER \\nare trained by penalizing the misclassification of images via \\nthe softmax loss. In this paper, we show that better FER \\nperformance can be achieved by combining the deep metric \\nloss and softmax loss in a unified two fully connected layer \\nbranches framework via joint optimization. A generalized \\nadaptive (N+M )-tuplet clusters loss function together with \\nthe identity-aware hard-negative mining and online \\npositive mining scheme are proposed for identity-invariant \\nFER. It reduces the computational burden of deep metric \\nlearning, and alleviates the difficulty of threshold \\nvalidation and anchor selection. Extensive evaluations \\ndemonstrate that our method outperforms many state-of-art \\napproaches on the posed as well as spontaneous facial \\nexpression databases. \\n \\n1. Introduction \\nFacial expression is one of the most expressive nonverbal \\ncommunication channels for humans to convey their \\nemotional state [5]. Therefore, automatic facial expression \\nrecognition (FER) is important in a wide range of \\napplications including human-computer interaction (HCI), \\ndigital entertainment, health care and intelligent robot \\nsystems [20]. \\nResearchers have achieved great progress in recognizing \\nthe posed facial expressions collected under tightly \\ncontrolled environment. Since the most promising face-\\nrelated applications occur in more natural conditions, it is \\nour goal to develop a robust system that can operate well in \\nthe real word. Despite the significant efforts, FER remains \\na challenge in the presence of pose and illumination \\nvariations as well as inter-subject variations (i.e., identity-\\nspecific attributes) [42]. These identity-specific factors \\ndegrade the FER performance of new identities unseen in \\nthe training data. Since spontaneous expressions only \\ninvolve subtle facial muscle movements, the extracted expression-related information from different classes can \\nbe dominated by the sharp-contrast identity-speci ¿c \\ngeometric or appearance features which are not useful for \\nFER. As shown in Fig. 1, example x1 and  x3 are of happy \\nfaces whereas x2 and x4 are not of happy faces. ݂ሺݔ\\u0bdcሻ\\x03are the \\nimage representations using the extracted features. For \\nFER, we desire that two face images with the same \\nexpression label are close to each other in the feature space, \\nwhile face images with different expressions are farther \\napart from each other, i.e., the distance D2 between \\nexamples x1 and x3 should be smaller than D1 and D3, as in \\nFig. 1(b). However, the learned expression representations \\nmay contain irrelevant identity information as illustrated in \\nFig. 1(a). Due to large inter-identity variations, D2 usually \\nhas a large value while the D1 and D3 are relatively small.  \\nTo further improve the discriminating power of the ex-\\npression feature representations, and address the large intra-\\nsubject variation in FER, a potential solution is to incorpo-\\nrate the deep metric learning scheme within a convolutional \\nneural network (CNN) framework. The fundamental phi-\\nlosophy behind the widely-used triplet loss function [7] is \\nto require one positive example closer to the anchor exam-\\nple than one negative example with a fixed gap Ĳ. Thus, dur-\\ning one iteration, the triplet loss ignores the negative exam-\\nples from the rest of classes. Moreover, one of the two ex-\\namples from the same class in the triplets can be chosen as  \\nAdaptive Deep Metric Learning for Identity-Aware Facial Expression Recognition\\n \\nXiaofeng Liu1,2,4*, B.V.K Vijaya Kumar1, Jane You3, Ping Jia2 \\n1Department of Electrical and Computer Engineerin g, Carnegie Mellon University, Pittsburgh, PA \\n2 Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Science \\n3 Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China \\n4 University of Chinese Academy of Sciences, Beijing, China  \\nliuxiaofeng@cmu.edu, kumar@ece.cmu.edu, csyjia@comp.polyu.edu.hk, jiap@ciomp.ac.cn  \\nFigure 1.  Illustration of representations in feature space learned by\\n(a) existing methods, and (b) the proposed method. \\n2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\nUnrecognized Copyright Information\\nDOI 10.1109/CVPRW.2017.79522\\n2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n2160-7516/17 $31.00 © 2017 IEEE\\nDOI 10.1109/CVPRW.2017.79522\\n\\n \\n \\n the anchor point. However, there exist some special cases \\nthat the triplet loss function with impropriate anchor may \\njudge falsely, as illustrated in Fig. 3(a). This means the per-\\nformance is quite sensitive to the anchor selection in the tri-\\nplets input. We adapted the idea from the ( N+1)-tuplet loss \\n[37] and  coupled clusters loss (CCL) [22] to design a \\n(N+M )-tuplet clusters loss function which incorporates a \\nnegative set with N examples and a positive set with M ex-\\namples in a mini-batch. A reference distance T is introduced \\nto force the negative examples to move away from the cen-\\nter of positive examples and for the positive examples to \\nsimultaneously map into a small cluster around their  center  \\ncା. The circles of radius ሺܶ\\u0d45ഓ\\nమሻ andሺܶെഓ\\nమሻ centered at the \\ncାform the boundary of the negative set and positive set re-\\nspectively, as shown in Fig. 3(d). By doing this, our ap-\\nproach can handle complex distribution of intra- and inter-\\nclass variations, and free the anchor selection trouble in \\nconventional deep metric learning methods. Furthermore, \\nthe reference distance T and the margin Ĳ can be learned \\nadaptively via the propagation in the CNN instead of the \\nmanually-set hyper-parameters. We also propose a simple \\nand efficient mini-batch construction scheme that uses dif-\\nferent expression images with the same identity as the neg-\\native set to avoid the expensive hard-negative example \\nsearching, while mining the positive set online. Then, the \\n(N+M )-tuplet clusters loss guarantees all the discriminating \\nnegative samples are efficiently used per update to achieve \\nan identity-invariant FER. \\nWe jointly optimize the softmax loss and ( N+M )-tuplet \\nclusters loss to explore the potential of both the expression \\nlabels and identity labels information. Considering the dif-\\nferent characteristics of each loss function and their tasks, \\nwe design two branches of fully connected (FC) layers, and \\na connecting layer to balance them. The features extracted \\nby the expression classification branch can be fed to the fol-\\nlowing metric learning processing. This enables each \\nbranch to focus better on their own task without embedding \\nmuch information of the other. As shown in Fig. 2, the in-\\nputs are two facial expression image set: one positive set \\n(images of the same expression from different subjects) and \\none negative set (images of other expressions with the same identity of the query example). The deep features and dis-\\ntance metrics are learned simultaneously in a network.  \\nThe three major contributions in this paper are: 1) We \\npropose a generalized ( N+M )-tuplet clusters loss function \\nwith adaptively learned reference threshold which can be \\nseamlessly factorized into a linear-fully connected layer for \\nan end-to-end learning. 2)  With the identity-aware negative \\nmining and online positive mining scheme, we learn dis-\\ntance metrics with fewer input passes and distance calcula-\\ntions, without sacrificing the performance for identity-in-\\nvariant FER. 3) We optimize the softmax loss and ( N+M )-\\ntuplet clusters loss jointly in a unified two-branch FC layer \\nmetric learning CNN framework based on their character-\\nistics and tasks. In experiments, we demonstrate that the \\nproposed method achieves promising results not only out-\\nperforming several state-of-art approaches in posed facial \\nexpression dataset (e.g., CK+, MMI), but also in spontane-\\nous facial expression dataset (namely, SFEW). \\n2. Related work \\nFER focus on the classification of seven basic facial ex-\\npressions which are considered to be common among hu-\\nmans [40]. Much progress has been made on extracting a \\nset of features to represent the facial images [13]. Geomet-\\nric representations utilize the shape or relationship between \\nfacial landmarks. However, they are sensitive to the facial \\nlandmark misalignments [35]. On the other hand, appear-\\nance features, such as Gabor filters, Scale Invariant Feature \\nTransform (SIFT), Local Binary Patterns (LBP), Local \\nPhase Quantization (LPQ), Histogram of Oriented Gradi-\\nents (HOG) and the combination of these features via mul-\\ntiple kernel learning are usually used for representing facial \\ntextures [3, 15, 49, 51]. Some methods such as active ap-\\npearance models (AAM) [41] combine the geometric and \\nappearance representations to provide better spatial infor-\\nmation. For a comprehensive survey, we refer readers to \\n[34]. Due to the limitations of handcrafted filters, extracting \\npurely expression-related features is difficult.  \\n    The developments in deep learning, especially the suc-\\ncess of CNN, have made high-accuracy image classification \\npossible in recent years. It has also been shown that care-\\nfully designed neural network architectures perform well in \\nFER [29]. Despite its popularity, current softmax loss-\\nbased network does not explicitly encourage intra-class \\ncompactness and inter-class separation. The emerging deep \\nmetric learning methods have been investigated for person \\nrecognition and vehicle re-identification problems with \\nlarge intra-class variations, which suggests that deep metric \\nlearning may offer more pertinent representations for FER. \\nCompared to traditional distance metric learning, deep met-\\nric learning learns a nonlinear embedding of the data using \\nthe deep neural networks.  The initial work is to train a Si-\\namese network with contrastive loss function [4]. The pair-\\nwise examples are fed into two symmetric sub-networks to \\npredict whether they are from the same class. Without the \\nPositive  examples\\nNegative  examples\\nTwo-branch FC layer\\nJoint Metric  Learning  Network\\n(N+M )-tuplet cluster loss &Softmax lossPositive Set\\nNegative Set\\nFigure 2. Frame work of our facial expression recognition model \\nused for training. The deep convolutional network aims to map the \\noriginal expression images into a feature space that the images o f \\nthe same expression tend to form a cluster while other images tend \\nto locate far away. \\n523\\n523\\n  \\n interactions of positive pairs and negative pairs, the Sia-\\nmese network may fail to learn effective metrics in the pres-\\nence of large intra- and inter-class variations. One improve-\\nment is the triplet loss approach [7], which achieved prom-\\nising performance in both re-identification and face recog-\\nnition problems. The inputs are triplets, each consisting of \\na query, a positive example and a negative example. Spe-\\ncifically, it forces the difference of the distance from the \\nanchor point to the positive example and from the anchor \\npoint to the negative example to be larger than a fixed mar-\\ngin ߬. Recently, some of its variations with faster and stable \\nconvergence have been developed. The most similar model \\nof our proposed method is the ( N+1)-tuplet loss [37]. We \\nuse ݔାand ݔିto denote the positive and negative examples \\nof a query example ݔ ,meaning that ݔାis the same class of \\nݔ ,while ݔିis not. Considering ( N+1) tuplet which includes \\nݔ ,ݔା\\x03and N-1 negative examples \\x03\\x03ሼݔ\\u0bddିሽ\\u0bddୀଵேିଵ, the loss is: \\n \\nܮቀݔǡݔାǡ൛ݔ\\u0bddିൟ\\u0bddୀଵேିଵǢ݂ቁൌ  \\n݃\\u074b݈൫ͳ\\u0d45σ\\u0bddୀଵேିଵ\\x87\\x9a\\x92ሺܦሺ݂ǡ݂ାሻ\\u0d45ɒെܦሺ݂ǡ݂\\u0bddିሻሻ൯          (1) \\n \\nwhere\\x03݂ሺ\\u0d49ሻ  is an embedding kernel defined by the CNN, \\nwhich takes ݔ and generates an embedding vector ݂ሺݔሻ . \\nWe write it as ݂ for simplicity, with \\x03݂ inheriting all \\nsuperscripts and subscripts. \\x03\\x07ሺ\\u0d49ǡ\\u0d49ሻis defined as the \\nMahalanobis or Euclidean distance according to different \\nimplementations. The philosophy in this paper also shares \\ncommonality with the coupled clusters loss [22], in which \\nthe positive example center cା is set as the anchor. By \\ncomparing each example with this center instead of each \\nother mutually, the evaluation times in a mini-batch are \\nlargely reduced.  \\n    Despite their wide use, the above-mentioned frameworks still suffer from the expensive example mining to provide \\nnontrivial pairs or triplets, and poor local optima. In \\npractice, generating all possible pairs or triplets would \\nresult in quadratic and cubic complexity, respectively and \\nthe most of these pairs or triplets are less valuable in the \\ntraining phase. Also, the online or offline traditional mini-\\nbatch sample selection is a large additional burden. \\nMoreover, as shown in Fig. 3(a), (b) and (c), all of them are \\nsensitive to the anchor point selection when the intra- and \\ninter-class variations are large. The triplet loss, ( N+1)-tuplet \\nloss and CCL are 0, since the distances between the anchor \\nand positive examples are indeed smaller than the distance \\nbetween the anchor and negative examples for a margin ɒ. \\nThis means the loss function will neglect these cases during \\nthe back propagation. We need much more input passes \\nwith properly selected anchors to correct it. The fixed \\nthreshold in the contrastive loss was also proven to be sub-\\noptimal for it failed to adapt to the local structure of data. \\nLi et al. proposed [21] to address this issue by learning a \\nlinear SVM in a new feature space. Some works [9, 43] \\nused shrinkage-expansion adaptive constraints for pair-\\nwise input, which optimized by alternating between SVM \\ntraining and projection on the cone of all positive \\nsemidefinite (PSD) matrices, but their mechanism cannot \\nbe implemented directly in deep learning. \\n    A recent study presented objective comparisons between \\nthe softmax loss and deep metric learning loss and showed \\nthat they could be complementary to each other [12]. \\nTherefore, an intuitive approach for improvement is \\ncombining the classification and similarity constraints to \\nform a joint CNN learning framework. For example, [39, \\n47] combining the contrastive and softmax losses together \\nto achieve a better performance, while [52] proposed to \\ncombine triplet and softmax loss via joint optimization. \\nThese models improve traditional CNN with softmax loss \\nbecause similarity constraints might augment the \\ninformation for training the network. The difficult learning \\nobjective can also effectively avoid overfitting. However, \\nall these strategies apply the similarity as well as \\nclassi¿cation constraints directly on the last FC layer, so \\nthat harder tasks cannot be assigned to deeper layers, (i.e., \\nmore weights) and interactions between constraints are \\nimplicit and uncontrollable. Normally, the softmax loss \\nconverges much faster than the deep metric learning loss in \\nmulti-task networks. This situation has motivated us to \\nconstruct a unified CNN framework to learn this two loss \\nfunction simultaneously in a more reasonable way. \\n3.  (N+M )-tuplet clusters loss \\nWe give a simple description of our intuition to introduce \\na reference distance T to control the relative boundary ( Tെ\\x03\\nɒ\\nʹ\\x03) and ሺܶ\\u0d45ഓ\\nమሻ for the positive and negative examples \\nrespectively, as shown in Fig. 3(d). We rewrite the ( N+1)-\\ntuplet loss function in Eq.(1) as follows: \\n Figure 3. Failed case of (a) triplet loss, (b) ( N+1)-tuplet loss, an d\\n(c) Coupled clusters loss. The proposed ( N+M )-tuplet clusters loss\\nis illustrated in (d). \\n524\\n524\\n \\n \\n &5\\x0f5?\\x0f5X@\\nXA;N@;\\x100n \\n241\\x16j\\x18XA;N@;\\t\\r\\x0co#0\\x0f0?jk)j7\\n\\x17j)j\\x017\\n\\x17k#o0\\x0f0X@pp \\nn241\\x16j\\x18XA;N@;\\t\\r\\x0c#0\\x0f0?k)j\\x017\\n\\x17\\x1b\\t\\r\\x0c\\x01o)j\\x017\\n\\x17k#o0\\x0f0X@pp     \\n(2) \\nIndeed, the \\t\\r\\x0c\\x030\\x0f0?\\x12\\x05jd\\nb term used to pull the \\npositive example together and the \\x01\\x01\\t\\r\\x0c\\x05\\x12d\\nbj\\x03o0\\x0f0X\\x12p \\nterm used to push the negative examples away have an \\n“OR” relationship. The relatively large negative distance \\nwill make the loss function ignore the large absolute \\npositive distance. One way to alleviate large intra-class \\nvariations is to construct an “AND” function for these two \\nterms.  \\nWe also extend the trip let loss to incorporate N negative \\nexamples and M negative examples. Considering a multi-\\nclassification problem, the triplet loss and CCL only \\ncompare the query example with one negative example, \\nwhich only guarantees the embedding vector of the query \\none to be far from a selected negative class instead of every \\nclass. The expectation of these methods is that the final \\ndistance metrics will be balan ced after sufficient number of \\niterations. However, towards the end of the training, \\nindividual iteration may exhibit zero errors due to the lack \\nof discriminative negative ex amples causing the iterations \\nto be unstable or slow in convergence.  \\nThe identity labels in FER database largely facilitate the \\nhard-negative mining to alleviate the effect of the inter-\\nsubject variations. In practi ce, for a query example, we \\ncompose its negative set with all the different expression \\nimages of the same person. Moreover, randomly choosing \\none or a group of positive examples is a paradigm of the \\nconventional deep metric methods, but some extremely \\nhard positive examples may distort the manifold and force \\nthe model to be over-fitting. In the case of spontaneous \\nFER, the expression label may erroneously be assigned due \\nto the subjectivity or varied expertise of the annotators [2, \\n50]. Thus, an efficient online mining for M randomly-\\nchosen positive examples should be designed for large \\nintra-class variation datasets. We find the nearest negative \\nexample and ignore those positive examples with a larger \\ndistance. Algorithm 1 shows the detail. In summary, the \\nnew loss function is expressed as follows: \\n \\n&5W?\\nWA;M\\x0f5X@\\nXA;N\\x100n;\\nM\\x1b\\x18WA;M\\x1b\\n\\x06\\ro\\x15\\x0f#0?\\x0f\\x07jk)j\\x01_\\n<p \\n                         \\x01ja\\nf\\x01\\x18XA;N\\n\\x06\\ro\\x15\\x0f)j \\x017\\n\\x17k#o0X@\\x0f\\x07jppp           (3)  \\n \\nThe simplified geometric inte rpretation is illustrated in \\nFig. 3(d). Only if the distances from online mined positive \\nexamples to the updated c? smaller than o)ki\\nbp and the  \\ndistances to the updated c+ than o)ji\\nbp, the loss can get a \\nzero value. This is much more consistent with the principle \\nused by many data cluster and discriminative analysis \\nmethods. One can see that the conventional triplet loss and its variations become the special cases of the ( N+M)-tuplet \\nclusters loss under our framework. \\n \\nFor a batch consisting of X queries, the input passes \\nrequired to evaluate the necessary embedding feature \\nvectors in our application are X, and the total number of \\ndistance calculations can be \\x17(j\\'\\x1b+. Normally, the \\nN and M are much smaller than X. In contrast, triplet loss \\nrequires \\x02Q= passes and \\x17\\x02Q=\\x01times calculations, ( N+1)-tuplet \\nloss requires +j\\x16\\x1b+ passes and +j\\x16\\x1b+< times \\ncalculations. Even for a dataset with a moderate size, it is \\nintractable to load all possible meaningful triplets into the \\nlimited memory for model training. \\n    By assigning different values for T and 7, we define a \\nflexible learning task with adjustable difficulty for the \\nnetwork. However, the two hyper-parameters need manual \\ntuning and validation. In the spirit of adaptive metric \\nlearning for SVM [21], we form ulate the reference distance \\nto be a function \\x01\\x05m\\x0fm\\x01related with each example instead of \\na constant. Since the Mahalanobis distance matrix M in \\nEq.(4) itself is quadratic, and can be calculated \\nautomatically via a linear fu lly connected layer as in  [36], \\nwe assume \\x050;\\x0f0< as a simple quadratic form, i.e., \\nT0;,0<=a\\nb6^!6j8^\\x0ej- , where 6^n0;^0<^\\x1a\\x19<S, \\n!n!UaUa!UaUb\\n!UbUa!UbUb\\x1a\\x19<Sl<S\\x0f 8^n8Ua^8Ub^\\x1a\\x19<S, -\\x1a\\x19 , \\n0;\\x01\\x06\\x0b\\x08\\x010<\\x1a\\x19<S are the representations of two images in \\nthe feature space. \\n \\n           D(0;,0<)=0;k0<M<n0;k0<O\\x1fo0;k0<p           (4) \\n \\nDue to the symmetry property with respect to 0; and 0<, we \\ncan rewrite T0;,0< as follows: \\n \\n     T0;,0<=\\x01a\\nb0;^\\x1c0;ja\\nb0<^\\x1c3j0;^\\x1d0<j.^0;j0<j-    (5) \\n Algorithm 1  Online positive mining \\nInput \\nquery example and its randomly selected  \\npositive set 5W?\\nWA;M, and negative set q5X@rXA;N \\n  1. map examples to feature plane with CNN to get: \\n0W?\\nWA;\\x01M,3/\\x01q0X@rXA;N \\n  2. calculate the positive cluster center c?=a\\ne\\x18WA;M0W? \\n  3. calculate the distance from c? to each  \\n      positive and negative example  Dfi\\x01\\x01?,c?, #fj\\x01\\x01@,c? \\n  4. search for the nearest negative distance: \\n#\\x01o5[TR\\\\]^@\\x0fc?p \\n  5. ignore those positive examples satisfying: \\n Dfi\\x01\\x01?,c?\\x01#\\x01o5[TR\\\\]^@\\x0fc?p \\n  6. update c?=a\\ne\\x1b\\x18WA;M\\x1b0W? \\nOutput \\nOnline mined M* positive examples and updated c? \\n525\\n525\\n \\n \\nwhere \\x1c= !UaUa=\\x01!UbUb\\x01and \\x1d = !UaUb= !UbUa are both the \\n/l/  real symmetric matrices (not necessarily positive \\nsemi-de ﬁnite), c = \\x14Ua = \\x14Ub is a d-dimensional vector, and \\nbis the bias term. Then, a new quadratic formula \\n\\x04(0\\x16,0\\x17)=To0\\x16,0\\x17pkD(0\\x16,0\\x17)is defined to combine the \\nreference distance function an d distance metric function. \\nSubstituting Eq.(4) and Eq.(5) to H(0;,0<), we get: \\nH(0;,0<)=\\x01;\\n<0;^\\x1ck\\x17\\x1f0;j;\\n<0<^\\x1ck\\x17\\x1f0< \\nj0;^o\\x1dj\\x17\\x1fp0<j.^0;j0<j- (6)\\n     \\n  H(0;,0<)=\\x01a\\nb0;^\\x1c0;ja\\nb0<^\\x1c0<j0;^\\x1d0<j.^0;j0<j-     (7)   \\nwhere A=\\x1ck\\x17\\x1f  and B=\\x1dj\\x17\\x1f\\x11 Suppose A is positive \\nsemi-definite (PSD) and B is negative semi-definite (NSD), \\nA and B can be factorized as \\x1eBO\\x1eB and\\x01\\x1eCO\\x1eC. Then \\n\\x04o0;\\x0f0<p can be formulated as follows: \\x01 \\nH(0;,0<)=\\x01;\\n<0;^\\x1eKO\\x1eB0;j;\\n<3^\\x1eBO\\x1eB0<j0;^\\x1eCO\\x1eC0< \\nj.^0;j0<j- \\n \\n=\\x01;\\n<\\x1eB0;^\\x1eB0;j;\\n<\\x1eB0<^\\x1eB0<j\\x1eC0;^\\x1eC0< \\nj.^0;j.^0<j-                                 (8)                                    \\nMotivated by the above, we propose a general, \\ncomputational feasible loss function. Following the \\nnotations in the preliminaries and denote o\\x1eB\\x0f\\x1eC\\x0f.pO as W, \\nwe have: \\n&*\\x0f5W?\\nWA;M\\x0f5X@\\nXA;N\\x100n \\n;\\nM\\x1b\\x18WA;M\\x1b\\n\\x06\\ro\\x15\\x0f\\x01 Ho0ij,cj)+_\\n<pj;\\nN\\x01\\x18XA;N\\n\\x06\\ro\\x15\\x0f%o0X@\\x0f.jpj_\\n<p \\n (9) \\nGiven the mined N+M*  training examples in a mini-batch, \\n2mis a label function. If the example 5Yis from the \\npositive set, 25Ynk\\x16, otherwise, \\x0125Yn1. Moreover, \\nwe simplify the  i\\nb\\x01\\x01to be the constant 1, and changing it to \\nany other positive value results only in the matrices being \\nmultiplied by corresponding factors. Our hinge-loss like \\nfunction is: &*\\x0f5W?\\nWA;M\\x0f5X@\\nXA;N\\x100n \\n              \\x01\\x01\\x01a\\nfce\\x1b\\x18YA;N?M\\x1b\\n\\x06\\r\\x15\\x0f25Y*H(0Y,c?)+\\x16           (10) \\n \\nWe optimize Eq.(12) using the standard stochastic gradient \\ndescent with momentum. The desired partial derivatives of \\neach example are computed as: \\n \\n                      `L\\n`Phna\\nfce\\x1b\\x18Yn;NjM\\x1b`L\\n`Qgh`Qgh\\n`Ph                   (11) \\n \\n                            `L\\n`Qghn`L\\n`Qghca`Qghca\\n`Qgh                            (12) \\n \\nwhere  +YZ represents the feature map of the example 5Y at \\nthe 2^V\\x01layer. Eq.(11) shows that the overall gradient is the \\nsum of the example-based gradie nts. Eq.(12) shows that the \\npartial derivative of each ex ample with respect to the \\nfeature maps can be calculated recursively. So, the \\ngradients of network parameters can be obtained with back \\npropagation algorithm.  \\n    In fact, as a straightforward generalization of \\nconventional deep metric learning methods, the ( N+M )-\\ntuplet clusters loss can be easily used as a drop-in \\nreplacement for the triplet loss and its variations, as well as \\nused in tandem with other performance-boosting \\napproaches and modules, in cluding modified network \\narchitectures, pooling functions, data augmentations or \\nactivation functions. \\n4.\\x01Network architecture \\nThe proposed two-branch FC layer joint metric learning \\narchitecture with softmax loss and ( N+M )-tuplet clusters \\nloss, denoted as 2B( N+M )Softmax, is illustrated in Fig. 4. \\nThe convolutional groups of our network are based on the \\ninception FER network presented in [28]. We adopt the \\nparametric rectified linear unit (PReLU) to replace the \\nconventional ReLU for its good performance and \\ngeneralization ability when given limited training data. In \\naddition to providing the sparsity to gain benefits discussed \\nin Arora et al. [1], the inception layer also allows for \\nFigure 4. The proposed network structure. In the testing phase, only the convolutional groups and expression classification bra nch with \\nsoftmax are used to recognize a single facial expression image. \\t\\x14\\r\\x18\\x12\\n\\x1b\\n\\x11\\x14\\x17\\x17\\x04\\x03\"\\x04\\x03\\n\\x04\\x03#\\x03\\x14\\x13\\x13\\x0c\\x0b\\x18\\x10\\x13\\x0e\\x04\\x03 \\x1e\\x07$\\x06\\x1f\\x1d\\x18\\x19\\x15\\x11\\x0c\\n\\x0b\\x11\\x19\\x17\\x18\\x0c\\x16\\x17\\n\\x11\\x14\\x17\\x17\\x07\\x0c\\x0e\\n\\x18\\x10\\x1a\\x0c\\t\\x0c\\x18\\n\\x08\\x14\\x17\\x10\\x18\\x10\\x1a\\x0c \\t\\x0c\\x18\\x05\\x13\\x0b\\x0c\\x15\\x18\\x10\\x14\\x13 \\x17\\x18\\x1c\\x11\\x0c \\x0b\\x14\\x13\\x1a\\x14\\x11\\x19\\x18\\x10\\x14\\x13\\n\\x11 \\x0e\\x16\\x14\\x19\\x15\\x17\\x05\\x13\\x0b\\x0c\\x15\\x18\\x10\\x14\\x13 \\x17\\x18\\x1c\\x11\\x0c\\x0b\\x14\\x13\\x1a\\x14\\x11\\x19\\x18\\x10\\x14\\x13\\n\\x11 \\x0e\\x16\\x14\\x19\\x15\\x17\\x02\\x0c\\x0b\\x11 \\x17\\x12\\x17\\x12\\x13\\n\\x03\\x06\\x12\\x01\\r\\x0c\\x0c\\n\\x15\\x12\\x15\\x12\\x14\\n\\x02\\x0c\\x0b\\x11 \\x15\\x12\\x15\\x12\\x13\\n\\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x16\\x12\\x16\\x0e\\t\\x08\\x10\\x07\\t \\x15\\x12\\x15\\x01\\x04\\x0c\\x0c\\n \\x15\\x12\\x15\\x0e\\t\\x08\\x10\\x07\\t\\n\\x16\\x12\\x16\\x02\\x0c\\x0b\\x11 \\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x15\\x12\\x15\\x02\\x0c\\x0b\\x11\\n\\x02\\x0c\\x0b\\x07\\x06\\x0f\\t\\x0b\\x06\\x0f\\t\\n\\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x16\\x12\\x16\\x0e\\t\\x08\\x10\\x07\\t \\x15\\x12\\x15\\x01\\x04\\x0c\\x0c\\n \\x15\\x12\\x15\\x0e\\t\\x08\\x10\\x07\\t\\n\\x16\\x12\\x16\\x02\\x0c\\x0b\\x11 \\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x15\\x12\\x15\\x02\\x0c\\x0b\\x11\\n\\x02\\x0c\\x0b\\x07\\x06\\x0f\\t\\x0b\\x06\\x0f\\t\\n\\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x16\\x12\\x16\\x0e\\t\\x08\\x10\\x07\\t \\x15\\x12\\x15\\x05\\x01\\x04\\x0c\\x0c\\n \\x15\\x12\\x15\\x0e\\t\\x08\\x10\\x07\\t\\n\\x16\\x12\\x16\\x02\\x0c\\x0b\\x11 \\x13\\x12\\x13\\x02\\x0c\\x0b\\x11 \\x15\\x12\\x15\\x02\\x0c\\x0b\\x11\\n\\x02\\x0c\\x0b\\x07\\x06\\x0f\\t\\x0b\\x06\\x0f\\t\\x03\\x06\\x12\\x01\\r\\x0c\\x0c\\n\\x15\\x12\\x15\\x12\\x14\\n\\x03\\x06\\x12\\x01\\r\\x0c\\x0c\\n\\x15\\x12\\x15\\x12\\x14\\x03\\x06\\x12\\x01\\r\\x0c\\x0c\\n\\x15\\x12\\x15\\x12\\x14\\n\\x02\\x1b\\x15\\x16\\x0c\\x17\\x17\\x10\\x14\\x13\\x01 \\x01\\x11\\n\\x17\\x17\\x10\\r\\x10\\x0b\\n\\x18\\x10\\x14\\x13\\x01\\x02\\x16\\n\\x13\\x0b\\x0f\\x04\\x0c\\x18\\x16\\x10\\x0b\\x01\\x03\\x0c\\n\\x16\\x13\\x10\\x13\\x0e\\x01\\x02\\x16\\n\\x13\\x0b\\x0f\\n\\x04\\x03!\\n  Expression \\nlabelIdentity \\nlabelҁML҂\\nҁEC҂\\n526\\n526\\n \\n \\n improved recognition of local features. The locally applied \\nsmaller convolution filters seem to align the way that \\nhuman process emotions with  the deformation of local \\nmuscles. Note that we did not specifically search for the \\narchitectures that obtain the absolute best accuracies on \\nsome datasets. Our goal is to confirm our generalized metric \\nlearning loss function and the unified two-branch FC layer \\njoint learning framework perform well.   \\n    Combing the ( N+M )-tuplet clusters loss and softmax loss \\nis an intuitive improvement to reach a better performance. \\nHowever, conducting them directly on the last FC layer is \\nsub-optimal. The basic idea of  building a two-branch FC \\nlayers after the deep convolution groups is combining two \\nlosses  in different level of tasks. We learn the detailed \\nfeatures shared between the sa me expression class with the \\nexpression classification (EC)  branch, while exploiting \\nsemantic representations via the metric learning (ML) \\nbranch to handle the significant appearance changes from \\ndifferent subjects. The connecting layer embeds the \\ninformation learned from the expression label-based detail \\ntask to the identity label- based semantical task, and \\nbalances the scale of weights in two task streams. This type \\nof combination can effectively alleviate the interference of \\nidentity-specific attributes. The inputs of connecting layer \\nare the output vectors of the former FC layers- FC 2 and FC 3, \\nwhich have the same dimension denoted as Dinput. The \\noutput of the connecting layer, denoted as FC 4 with \\ndimension Douput, is the feature vector fed into the second \\nlayer of the ML branch. The connecting layer concatenates \\ntwo input feature vectors into a larger vector and maps it \\ninto a Doutput dimension space: \\n \\n                    $\">n D$\"<\\x01\\x10$\"=n 9D$\"<j :D$\"=        (13) \\n \\nwhere P is a \\x17o#EFHJIl#GJIHJIp matrix, P1 and P2 are \\n#EFHJIl#GJIHJI\\x01matrices. \\n    Regarding the sampling strategy, every training image is \\nused as a query example in an epoch. In practice, the \\nsoftmax loss will only be calculated for the query example. \\nThe importance of two loss functions is balanced by a \\nweight α. During the testing stage, this framework takes one \\nfacial image as input, and gene rates the classification result \\nthrough the EC branch with the softmax loss function. \\n5.\\x01Experiments and analysis \\n5.1.\\x01Preprocessing \\n     For a raw image in the database, face registration is a \\ncrucial step for good performance. The bidirectional \\nwarping of Active Appearance Model (AAM) [30] and a \\nSupervised Descent Method (SDM) called IntraFace model \\n[45] are used to locate the 49 facial landmarks. Then, face \\nalignment is done to reduce in-plane rotation and crop the \\nregion of interest based on the coordinates of these landmarks to a size of 60 l60. The limited images of FER \\ndatasets is a bottleneck of deep model implementation. \\nThus, an augmentation procedure is employed to increase \\nthe volume of training data and alleviate the chance of over-\\nﬁtting. We randomly crop the 48 l48 size patches, flip them \\nhorizontally and transfer them to grayscale images. All the \\nimages are processed with the standard histogram \\nequalization and linear plane fitting to remove unbalanced \\nillumination. Finally, we normalize them to a zero mean \\nand unit variance vector. In the testing phase, a single center \\ncrop with the size of 48 l48 is used as input data. \\n5.2.\\x01Implementation Details \\nFollowing the experimental pr otocol in [28,48], we pre-\\ntrain our convolutional groups a nd EC branch FC layers on \\nthe FER2013 database [9] fo r 300 epochs, optimizing the \\nsoftmax loss using stochastic gradient decent with a \\nmomentum of 0.9. The initial network learning rate, batch \\nsize, and weight decay paramete r are set to 0.1, 128, 0.0001, \\nrespectively. If the training loss increased more than 25% \\nor the validation accuracy does not improve for ten epochs, \\nthe learning rate is halved a nd the previous network with \\nthe best loss is reloaded. Then the ML branch is added and \\nthe whole network is trained by 204,156 frontal viewpoints \\n(-45\\x02 to 45\\x02) face images selected from the CMU Multi-pie \\n[10] dataset. There contains  337 people displaying disgust, \\nhappy, surprise and neutron. The size of both the positive \\nand negative set are fixed to 3 images. The weights of two \\nloss functions are set equally. We select the highest \\naccuracy training epoch as our pre-trained model.  \\n    In the fine-tuning stage, the positive and negative set size \\nare fixed to 6 images (for CK+ and SFEW) or 5 images (for \\nMMI). For a query example, the random searching is \\nemployed to select the othe r 6 (or 5) same expression \\nimages to form the positive set. Identity labels are required \\nfor negative mining in our method. CK+ and MMI have the \\nsubject IDs while the SFEW need manually label. In \\npractice, an off-the-shelf face recognition method can be \\nused to produce this informa tion. When the query example \\nlacks some expression images from the same subject, the \\ncorresponding expression images sharing the same ID with \\nthe any other positive examples are used. The tuplet-size is \\nset to 12, which means 12 l(6+6) =144 (or 12 l(5+5) =120) \\nimages are fed in each training iteration. We use Adam [19] \\nfor stochastic optimization and other hyper-parameters \\nsuch as learning rate are tuned accordingly via cross-\\nvalidation. All the CNN archit ectures are implemented with \\nthe widely used deep learning tool “Caffe [14].” \\n5.3.\\x01Experimental Evaluations \\nTo evaluate the effectiveness of the proposed method, \\nextensive experiments have been conducted on three well-\\nknown publicly available facial e xpression databases: CK+,  \\nMMI and SFEW. For the fair comparison, we follow the \\n527\\n527\\n \\n \\nprotocol used by previous wo rks [28,48]. Three baseline \\nmethods are employed to demons trate the superiority of the \\nnovel metric learning loss and two-branch FC layer network \\nrespectively, i.e., adding the ( N+M )-tuplet clusters loss or \\n(N+1)-tuplet loss with softmax loss after the EC branch, \\ndenoted as 1B( N+1)Softmax or 1B( N+M )Softmax, and \\ncombining the ( N+1)-tuplet loss with softmax loss via the \\ntwo-branch FC layer structure, as 2B( N+1)Softmax. We do \\nnot compare with the triplet loss here, because the number \\nof triplets grows cubically with the number of images, \\nwhich makes it impractical and inef ﬁcient. With randomly \\nselected triplets, the loss failed to converge during the \\ntraining phase. \\nThe extended Cohn-Kanade database (CK+) [26] \\nincludes 327 sequences collected from 118 subjects, \\nranging from 7 different expressi ons (i.e., anger, contempt, \\ndisgust, fear, happiness, sadness, and surprise). The label is \\nonly provided for the last frame (peak frame) of each \\nsequence. We select and label the last three images, and \\nobtain 921 images (without neutral). The ﬁnal sequence-\\nlevel predictions are made by selecting the class with the \\nhighest possibility of the three images. We split the CK+ \\ndatabase to 8 subsets in a stri ct subject independent manner, \\nand an 8-fold cross-validation is employed. Data from 6 \\nsubsets is used for training and the others are used for \\nvalidation and testing. The confusions matrix of the \\nproposed method evaluated on the CK+ dataset is reported \\nin Table 1. It can be obser ved that the disgust and happy \\nexpressions are perfectly recognized while the contempt \\nexpression is relatively harder for the network because of \\nthe limited training examples and subtle muscular \\nmovements. As shown in Table 3, the proposed \\n2B(N+M )Softmax outperforms the human-crafted feature-\\nbased methods, sparse coding-based methods and the other \\ndeep learning methods in comparison. Among them, the \\n3DCNN-DAP, STM-Explet and DTAGN utilized temporal \\ninformation extracted from se quences. Not surprisingly, it \\nalso beats the baseline methods obviously benefit from the \\ncombination of novel deep metric learning loss and two-\\nbranch architecture. \\nThe MMI database [32] includes 31 subjects with frontal-\\nview faces among 213 image sequences which contain a full temporal pattern of expressions, i.e., from neutral to one \\nof six basic expressions as tim e goes on, and then released. \\nIt is especially favored by the video-based methods to \\nexploit temporal information. We collect three frames in the \\nmiddle of each image sequence and associate them with the \\nlabels, which results in 624 images in our experiments. We \\ndivide MMI dataset into 10 subsets for person-independent \\nten-fold cross validation. The sequence-level predictions \\nare obtained by choosing the class with the highest average \\nscore of the three images. The confusion matrix of the \\nproposed method on the MMI database is reported in Table \\n2. As shown in Table 3, th e performance improvements in \\nthis small database without causing overfitting are \\nimpressive. The proposed method outperforms other works \\nthat also use static image-based features and can achieve \\ncomparable and even better results than those video-based \\napproaches. \\n \\nTable 3. Recognition accuracy comparison on the CK+ database \\n[26] in terms of seven expressions, and MMI database [32] in \\nterms of six expressions. \\n \\nMethods CK+  MMI  \\nMSR [33] 91.4% N/A \\nITBN [44] 91.44% 59.7% \\nBNBN [25] 96.7% N/A \\nIB-CNN [11] 95.1% N/A \\n3DCNN-DAP [23] 92.4% 63.4% \\nSTM-Ex plet [24] 94.19% 75.12% \\nDTAGN [16] 97.25% 70.2% \\nInception [28] 93.2% 77.6% \\n1B(N+1)Softmax 93.21% 77.72% \\n2B(N+1)Softmax 94.3% 78.04% \\n1B(N+M)Softmax 96.55% 77.88% \\n2B(N+M )Softmax 97.1% 78.53% \\n \\nThe static facial expressions in the wild (SFEW) database \\n[6] is created by extracting frame s from the film clips in the \\nAFEW data corpus. There are 1766 well-labeled images \\n(i.e., 958 for training, 436 for validation and 372 for testing) \\nbeing assigned to be one of the 7 expressions. Different \\nfrom the previous two datasets, it targets for unconstrained \\nfacial expressions, which ha s large variations reflecting \\nreal-world conditions. The conf usion matrix of our method \\non the SFEW validation set is reported in Table 4. The \\nTable 1.  Average confusion matrix obtained from proposed\\nmethod on the CK+ database [26]. \\nTable 2.  Average confusion matrix obtained from proposed\\nmethod on the MMI database [32]. \\n \\n Predict \\nAN CO DI FE HA SA SU \\n AN 91.1% 0% 0% 1.1% 0% 7.8% 0% \\nCO 5.6% 90.3% 0% 2.7% 0% 5.6% 0% \\nDI 0% 0% 100% 0% 0% 0% 0% \\nFE 0% 4% 0% 98% 2% 0% 8% \\nHA 0% 0% 0% 0% 100% 0% 0% \\nSA 3.6 0% 0% 1.8% 0% 94.6% 0% \\nSU 0% 1.2% 0% 0% 0% 0% 98.8% \\nCO\\nD\\nFE\\nHA\\nActual  Predict \\nAN DI FE HA SA SU \\n AN 81.8% 3% 3% 1.5% 10.6% 0% \\nDI 10.9% 71.9% 3.1% 4.7% 9.4% 6% \\nFE 5.4% 8.9% 41.4% 7.1% 7.1% 30.4% \\nHA 1.1% 3.6% 0% 92.9% 2.4% 0% \\nSA 17.2% 7.8% 0% 1.6% 73.4% 0% \\nSU 7.3% 0% 14.6% 0% 0% 79.6% \\nActual \\n528\\n528\\n \\n \\nrecognition accuracy of disgus t and fear are much lower \\nthan the others, which is also observed in other works. As \\nillustrated in Table 5, the CNN-based methods dominate the ranking list. With the augmentation of deep metric learning and two-branch FC layer network, the proposed method works well in the real world environment setting. Note that Kim et al. [18] employed 216 AlexNet-like CNNs with \\ndifferent architectures to boost the ﬁnal performance. Our \\nnetwork performs about 25M operations, almost four times \\nfewer than a single AlexNet. With the smaller size, the \\nevaluation time in testing phase takes only 5ms using a \\nTitan X GPU, which makes it applicable for real-time applications. \\nOverall, we can see that joint optimizing the metric \\nlearning loss and softmax loss can successfully capture \\nmore discriminative expre ssion-related features and \\ntranslate them into the significant improvement of FER \\naccuracy. The (N+M )-tuplet clusters loss not only inherits \\nmerits of conventional deep metric learning methods, but \\nalso learns features in a more efficient and stable way. The \\ntwo-branch FC layer can further give a boost in \\nperformance. Some nice properties of the proposed method \\nare verified by Fig. 5, where the training loss of2B(N+M )Softmax converges after about 40 epochs with a \\nmore steady decline and reaches a lower value than those \\nbaseline methods as we expect. As Fig. 6 illustrates, the \\nproposed method and the baseline methods achieve better performance in terms of th e validation accuracy on the \\ntraining phase. Table 5. Recognition accuracy comparison on the SFEW database \\n[6] in terms of seven expressions. \\n  \\nMethods Validation  \\nKim et al. [18] 53.9% \\nYu et al. [48] 55.96%\\nNg et al. [31] 48.5% \\nYao et al. [46] 43.58% \\nSun et al. [38] 51.02% \\nZong et al.  [53] N/A \\nKaya et al.[17] 53.06% \\nMao et al.[27] 44.7% \\nMollahosseini [28] 47.7% \\n1B(N+1)Softmax 49.77% \\n2B(N+1)Softmax 50.75% \\n1B(N+M )Softmax 53.36% \\n2B(N+M )Softmax 54.19% \\n6.\\x01Conclusion \\nWe derive the ( N+M )-tuplet clusters loss and combine it \\nwith softmax loss in a unified two-branch FC layer joint \\nmetric learning CNN architecture to alleviate the attribute variations introduced by different identities on FER. The efficient identity-aware negative-mining and online positive-mining scheme are employed. After evaluating performance on the posed and spontaneous FER dataset, we show that the proposed met hod outperforms the previous \\nsoftmax loss-based deep learning approaches in its ability \\nto extract expression-related features. More appealing, the \\n(N+M )-tuplet clusters loss function has clear intuition and \\ngeometric interpretation for gene ric applications. In future \\nwork, we intend to explore the use of it to the person or vehicle re-identifications. \\n \\nAcknowledgements  The funding support from Youth Innovation Promotion Association, CAS (2017264), Innovative Foundation of CIOMP, CAS (Y586320150) and Hong Kong Government \\nGeneral Research Fund GRF (Ref. No.152202/14E) is \\ngreatly appreciated. \\nTable 4. Average confusion matrix obtained from proposed \\nmethod on the SFEW validation set [6]. \\n Predict \\nAN DI FE HA NE SA SU \\n AN 66.24% 1.3% 0% 6.94% 9.09% 5.19% 10.69% \\nDI 21.74% 4.35% 4.35% 30.34% 13.04% 4.35% 21.74% \\nFE 27.66% 0% 6.38% 8.51% 10.64% 19.15% 27.66% \\nHA 0% 0% 0% 87.67% 6.85% 1.37% 4.11% \\nNE 5.48% 0% 2.74% 1.37% 57.53% 5.48% 27.4% \\nSA 22.81% 0% 1.75% 7.02% 8.77% 40.35% 19.3% \\nSU 1.16% 0% 2.33% 5.81% 17.44% 0% 73.26% \\n \\nF\\nH\\nN\\nActual \\nFigure 5. The training loss of different methods on SFEW validation set. \\n Figure 6. The validation accuracies of different methods on \\nSFEW validation set. \\n  #\\x1c&#\\x1c&(#\\x1c\\'#\\x1c\\'(#\\x1c(#\\x1c((#\\x1c)\\n# $ #% #& #\\' #( #) #\\x07\\x08\\x11\\x10\\x0b\\x08\\x18\\x10\\x14\\x13 \\x08\\n\\n\\x19\\x16\\x08\\n\\x1b\\n\\x04\\x19\\x12\\t\\x0c\\x16 \\x14\\r \\x0c\\x15\\x14\\n\\x0f\\x05\\x14\\r\\x18\\x12\\x08\\x1a\\x01\\x11\\x14\\x17\\x17\\x01\\x14\\x13\\x11\\x1b\\n\\x02\\x02\\x1f\\x1d\\x04*$\\x1e!\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\x01\\n\\x03\\x02\\x1f\\x1d\\x04*$\\x1e!\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\x01\\n$\\x02\\x1f\\x1d\\x04*\\x03\\x1e!\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\n%\\x02\\x1f\\x1d\\x04*\\x03\\x1e!\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\n#$%&\\'()\\n# $ #% #& #\\' #( #) #\\x06\\x16\\x08\\x10\\x13\\x0e \\x11\\x14\\x17\\x17\\n\\x04\\x19\\x12\\t\\x0c\\x16 \\x14\\r \\x0c\\x15\\x14\\n\\x0f\\x05\\x14\\r\\x18\\x12\\x08\\x1a\\x01\\x11\\x14\\x17\\x17\\x01\\x14\\x13\\x11\\x1b\\n$\\x02\\x1f\\x1d\\x04*$\\x1e\\x01!\\x01\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\n%\\x02\\x1f\\x1d\\x04*$\\x1e\\x01!\\x01\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\n\\x02\\x02\\x1f\\x1d\\x04*\\x03\\x1e\\x01!\\x01\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\x01\\n\\x03\\x02\\x1f\\x1d\\x04*\\x03\\x1e\\x01!\\x01\\x05\\x14\\r\\x18\\x12\\x08\\x1a \\x01\\n529\\n529\\n \\n \\n References \\n[1]\\x01S. Arora, A. Bhaskara, R. Ge  and T. Ma. Provable bounds for \\nlearning some deep representations. arXiv preprint \\narXiv:1310.6343 , 2013. \\n[2]\\x01E. Barsoum, C. Zhang, C. C. Ferrer and Z. Zhang. Training \\ndeep networks for facial expression recognition with crowd-\\nsourced label distribution. In ICMI , pages 279-283, 2016. \\n[3]\\x01T. Baltrusaitis, M. Mahmoud and P. Robinson. Cross-dataset \\nlearning and person-speci ﬁc normalisation for automatic \\naction unit detection. In Automatic Face and Gesture \\nRecognition (FG) , pages 6:1-6, 2015. \\n[4]\\x01S. Chopra, R. Hadsell and Y. LeCun. Learning a similarity \\nmetric discriminatively, with application to face veri ﬁcation. \\nIn CVPR , 2005. \\n[5]\\x01J. F. Cohn and P. Ekman. Measuring facial action. The new \\nhandbook of methods in nonverbal behavior research , pages \\n9–64, 2005. \\n[6]\\x01A. Dhall, Abhinav, O. V. R Murthy, R. Goecke, J. Joshi and \\nT. Gedeon. Video and image based emotion recognition \\nchallenges in the wild: Emotiw 2015. In ICMI , pages 423–\\n426, 2015. \\n[7]\\x01S. Ding, L. lin, G Wang and H.  Chao. Deep Feature Learning \\nwith Relative Distance Comparison for Person Re \\nidentification. Pattern Recognition , 48: 2993-3003, 2015. \\n[8]\\x01Y. Dong, B Du, L. Zhang, L.  Zhang and D. Tao. LAM3L: \\nLocally adaptive maximum margin metric learning for visual \\ndata classification.  Neurocomputing, 235:1–9, 2017.  \\n[9]\\x01I. J. Goodfellow, D. Erhan, P.  L. Carrier, A. Courville, M. \\nMirza, B. Hamner, W. Cukiersk i, Y. Tang, D. Thaler, D Lee, \\nY Zhou, C Ramaiah, F Feng, R Li and X Wang. Challenges \\nin representation learning: A report on three machine \\nlearning contests. In  International Conference on Neural \\nInformation Processing , page 117–124, 2013. \\n[10]\\x01R. Gross, I. Matthews, J. C ohn, T. Kanade and S. Baker. \\nMulti-pie. Image and Vision Computing , 28(5):807–813, \\n2010. \\n[11]\\x01S. Han, Z. Meng, A. S. Khan and Y. Tong, Incremental \\nBoosting Convolutional Neural Network for Facial Action \\nUnit Recognition, In NIPS , pages 109–117, 2016. \\n[12]\\x01S. Horiguchi, D. Ikami and K. Aizawa. Significance of \\nsoftmax-based features over metric learning-based features. \\nIn ICLR , 2017. \\n[13]\\x01S. Jain, C. Hu and J.K. Aggarwal. Facial expression \\nrecognition with temporal modeling of shapes. In CVPRW , \\npages 1642–1649, 2011. \\n[14]\\x01Y. Jia, E. Shelhamer, J. Dona hue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama and T. Darrell. Caffe: \\nConvolutional architecture for fast feature embedding. arXiv \\npreprint arXiv:1408.5093 , 2014. \\n[15]\\x01B. Jiang, M. Valstar and M. Pantic. Action unit detection \\nusing sparse appearance descriptors in space-time video \\nvolumes. In Automatic Face and Gesture Recognition (FG) , \\npages 314-321, 2011. \\n[16]\\x01H. Jung, S. Lee, J. Yim, S. Park and J. Kim. Joint fine-tuning \\nin deep neural networks for facial expression recognition. \\nIn ICCV , pages 2983–2991, 2015. \\n[17]\\x01H. Kaya and A. Salah. Combining modality-specific extreme \\nlearning machines for emoti on recognition in the wild. \\nJournal on Multimodal User Interfaces  10(2):139-149, 2016. \\n[18]\\x01B.K. Kim, J. Roh, S.Y. Le e and J. Roh. Hierarchical \\ncommittee of deep convolutional neural networks for robust facial expression recognition. Journal on Multimodal User \\nInterfaces , 10(2): 173-189, 2016. \\n[19]\\x01D. Kingma and J. Ba. Adam: A method for stochastic \\noptimization. In ICLR , 2015. \\n[20]\\x01I. Kotsia, S. Zafeiriou and S. Fotopoulos. Affective gaming: \\nA comprehensive survey. In CVPRW , pages 663–670, 2013. \\n[21]\\x01Z. Li, S Chang, F Liang, T.S.  Huang, L Cao and J R. Smith. \\nLearning locally-adaptive decision functions for person \\nverification. In  CVPR , pages 3610-3617. 2013. \\n[22]\\x01H. Liu, Y. Tian, Y. Yang, L.  Pang and T. Huang. Deep \\nrelative distance learning: Tell the difference between similar \\nvehicles. In CVPR , pages 2167-2175, 2016. \\n[23]\\x01M. Liu, S. Li, S. Shan, R.  Wang and X. Chen. Deeply \\nlearning deformable facial action parts model for dynamic \\nexpression analysis. In  ACCV , pages 143–157, 2014. \\n[24]\\x01M. Liu, S. Shan, R. Wa ng and X. Chen. Learning \\nexpressionlets on spatio-temporal manifold for dynamic \\nfacial expression recognition. In  CVPR , pages 1749–1756, \\n2014. \\n[25]\\x01P. Liu, S. Han, Z. Meng a nd Y. Tong. Facial expression \\nrecognition via a boosted deep belief network. In CVPR , \\npages 1805–1812, 2014. \\n[26]\\x01P. Lucey, J.F. Cohn, T. Kanade , J. Saragih, Z. Ambadar and \\nI. Matthews. The extended cohn-kanade dataset (ck+): A \\ncomplete expression dataset for action unit and emotion-\\nspeciﬁed expression. In CVPRW , pages. 94-101, 2010. \\n[27]\\x01Q. Mao, Q. Rao, Y. Yu and M.  Dong. Hierarchical Bayesian \\nTheme Models for Multi-pose Facial Expression Recognition. \\nIEEE Transactions on Multimedia , 19(4): 861 – 873, 2017. \\n[28]\\x01A. Mollahosseini, D. Chan a nd M.H. Mahoor. Going deeper \\nin facial expression recognition using deep neural networks. \\nIn WACV,  pages 1–10, 2016. \\n[29]\\x01A. Mollahosseini, B. Hassani, M.J. Salvador, H. Abdollahi, \\nD. Chan and M.H. Mahoor. Facial expression recognition in \\nthe world wild web. In CVPR,  pages 58-65, 2016. \\n[30]\\x01A. Mollahosseini and M. H. Mahoor. Bidirectional warping \\nof active appearance model. In CVPRW , pages 875–880, \\n2013. \\n[31]\\x01H.W. Ng, V.D. Nguyen, V. V onikakis and S Winkler. Deep \\nlearning for emotion recognition on small datasets using \\ntransfer learning. In ICMI , pages 443–449, 2015.  \\n[32]\\x01M. Pantic, M Valstar, R Rademaker, and L Maat. Web-based \\ndatabase for facial expression analysis. In  ICME, pages5,  \\n2005.  \\n[33]\\x01S. Rifai, Y Bengio, A. Courville, P. Vincent and M Mirza. \\nDisentangling factors of variation for facial expression \\nrecognition. In  ECCV , pages 808–822, 2012. \\n[34]\\x01E. Sariyanidi, H. Gunes and A. Cavallaro. Automatic \\nanalysis of facial affect: A survey of registration, \\nrepresentation and recognition. Pattern Analysis and \\nMachine Intelligence, IEEE Transactions on,  37:1113–1133, \\n2015. \\n[35]\\x01J. Shen, S. Zafeiriou, G. G.  Chrysos, J. Kossaifi, G. \\nTzimiropoulos and M. Pantic. The first facial landmark \\ntracking in-the-wild challenge: Benchmark and results. In \\nICCVW , pages 1003-1011, 2015. \\n[36]\\x01H. Shi, Y. Yang, X. Zhu, L. Zhen, W. Zheng and S.Z. Li. \\nEmbedding deep metric for person re-identification: a study \\nagainst large variations. In ECCV , pages 732-748, 2016. \\n[37]\\x01K. Sohn. Improved deep metric learning with multi-class n-\\npair loss objective. In NIPS , pages 1849-1857, 2016. \\n530\\n530\\n \\n \\n [36] H. Shi, Y. Yang, X. Zhu, L. Zhen, W. Zheng and S.Z. Li. \\nEmbedding deep metric for person re-identification: a study \\nagainst large variations. In ECCV , pages 732-748, 2016. \\n[37] K. Sohn. Improved deep metric learning with multi-class n-\\npair loss objective. In NIPS , pages 1849-1857, 2016. \\n[38] B. Sun, L. Li, G. Zhou, X. Wu, J. He, L. Yu, D. Li and Q. \\nWei. Combining multimodal features within a fusion \\nnetwork for emotion recognition in the wild. In ICMI , pages \\n497–502, 2015. \\n[39] Y. Sun, Y. Chen, X. Wang and X. Tang, Deep learning face \\nrepresentation by joint identi ¿cation-veri ¿cation. In NIPS , \\npages 1988–1996, 2014. \\n[40] Y. Tian, T. Kanade and J. F. Cohn. Facial expression \\nanalysis. In Handbook of face recognition , pages 247–275, \\n2005. \\n[41] G. Tzimiropoulos and M. Pantic. Optimization problems for \\nfast aam fitting in-the-wild. In  ICCV , pages 593-600, 2013. \\n[42] M.F. Valstar, M. Mehu, B. Jiang, M. Pantic and K. Scherer. \\nMeta-analysis of the first facial expression recognition \\nchallenge.   IEEE Trans. Syst., Man Cybern. B, \\nCybern,  42(4): 966–979, 2012. \\n[43] Q. Wang, W. Zuo, L. Zhang and P. Li. Shrinkage expansion \\nadaptive metric learning. In  ECCV , pages 456–471, 2014. \\n[44] Z. Wang, S Wang and Q Ji. Capturing complex spatio-\\ntemporal relations among facial  muscles for facial expression \\nrecognition. In CVPR , pages 3422–3429, 2013. \\n[45] X. Xiong and F. De la Torre. Supervised descent method and \\nits applications to face alignment. In CVPR , pages 532–539, \\n2013. \\n[46] A. Yao, J. Shao, N. Ma and Y. Chen. Capturing au-aware \\nfacial features and their latent relations for emotion \\nrecognition in the wild. In ICMI , pages 451–458, 2015. \\n[47] D. Yi, Z. Lei, S. Liao and S.Z. Li, Learning face \\nrepresentation from scratch. arXiv preprint arXiv:1411.7923 , \\n2014. \\n[48] Z. Yu and C. Zhang. Image based static facial expression \\nrecognition with multiple deep network learning. In ICMI , \\npages 435–442, 2015. \\n[49] A. Yüce, H. Gao and J. P. Thiran. Discriminant multi-label \\nmanifold embedding for facial action unit detection. \\nIn Automatic Face and Gesture Recognition (FG), pages 9:1-\\n6, 2015. \\n[50] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou and G. \\nZhao. Facial Affect “In-the-Wild”: A Survey and a New \\nDatabase. In CVPRW , pages 1487-1498, 2016. \\n[51] L. Zhang, D. Tjondronegoro and V. Chandran. Random \\nGabor based templates for facial expression recognition in \\nimages with facial occlusion.  Neurocomputing, 145:451-464, \\n2014. \\n[52] X. Zhang, F. Zhou, Y. Lin and S. Zhang, Embedding label \\nstructures for ¿ne-grained feature representation. In CVPR , \\npages 1114–1123, 2016. \\n[53] Y. Zong, W. Zheng, X. Huang, J. Yan, and T. Zhang. \\nTransductive transfer lda with riesz-based volume lbp for \\nemotion recognition in the wild. In ICMI , pages 491–496, \\n2015. \\n531\\n531\\n',\n",
       " '2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 1 2169 -3536 ©  201 7 IEEE. Translations and content mining are permitted for academic research only.  \\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.  \\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for m ore information.  Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. \\nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number \\nAdaptive Feature Mapping for Customizing \\nDeep Learning Based Facial Expression \\nRecognition Model \\nBing-Fei Wu 1, Fellow, IEEE and Chun-Hsien Lin1, Student Member, IEEE \\n1National Chiao Tung University , Hsinchu, 30010 Taiwan  \\nCorresponding author: Chun-Hsien Lin (e-mail: clifflin@cssp.cn.nctu.edu.tw ). \\nThis work was supported by the Ministry of Science Technology under Grant no. MOST 106-2622-E-009-009 -CC2.  \\nABSTRACT  Automated facial expression recognition can greatly improve the human-machine interface. \\nThe machine can provide better and more personalized services when it knows the human\\'s emotion. This \\nkind of improvement is an important progress in this artificial intelligence era. Many deep learning \\napproaches have been applied in recent years due to their outstanding recognition accuracy after training \\nwith large amounts of data. The performance is limited, however, by the specific environmental conditions \\nand variations in different persons involved. Hence, this paper addresses the issue of how to customize the \\ngeneric model without label information from the testing samples. Weighted Center Regression Adaptive \\nFeature Mapping (W- CR-AFM) is mainly proposed to transform the feature distribution of testing samples \\ninto that of trained samples. By means of minimizing the error between each feature of testing sample and \\nthe center of the most relevant category , W-CR-AFM can bring the features of testing samples around the \\ndecision boundary to the centers of expression categories; therefore, their predicted labels can be corrected. \\nWhen the model which is tuned by W- CR-AFM is tested on extended Cohn-Kanade (CK+) [2], Radboud \\nFaces Database (RaFD) [3], and Amsterdam Dynamic Facial Expression Set (ADFES) [4], our approach \\ncan improve the recognition accuracy by about 3.01%, 0.49%, and 5.33% respectively. Compared to the \\ncompeting deep learning architectures with the same training data, our approach shows the better \\nperformance. \\nINDEX TERMS  Cross domain adaption, Facial expression recognition, Computer vision, Pattern \\nrecognition, Image processing \\nI. INTRODUCTION \\nFacial expression recognition plays a vital role in the \\nartificial intelligen ce era. According to the human’s emotion  \\ninformation, machines can provide personalized services. \\nMany applications, such as virtual reality, personalized \\nrecommendations, customer satisfaction, and so on, depend \\non an efficient and reliable way to recognize the facial \\nexpressions. This topic has attracted many researchers for \\nyears, but it is still a challenging topic since expression \\nfeatures vary greatly with the head poses, environments, and \\nvariations in the different persons involved .  \\nTo mitigate these variations, some approaches modified \\nthe handcraft ed features to gain the better performance, like \\n[32] and [33]. Qirong Mao et al.  [34] make a Bayesian model \\nby means of multiple head poses to conquer the feature \\nvariation caused by head poses. However, the handcraft ed features have shown their limitations in practical applications, \\nso deep learning methods are utilized to make the models \\nlearn to extract the complicated features from large amounts \\nof facial expression data [5][6][7][8][9]. Most of the standard \\ndatabase for facial expression recognition are not candid \\nsince they are built under the controlled environment with \\ncoached expressions. Therefore, [5][8][9] apply data mining \\ntechnique to search for the facial images on the internet to \\nmake the model more realistic. For deep learning neural \\nnetworks, there is no clear rule to determine the architecture \\nand learning parameters, so image pre-processing is often \\nadopted to improve the neural network’s  performance. Andre \\net al.  [10] apply the spatial normalization, local intensity \\nnormalization, and facial image cropping to the \\nConvolutional Neural Network (CNN). Mapped binary \\npattern method is utilized in [11]. They all have the better \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 result after applying the pre-processing. In addition, some \\nother approaches combine common machine learning models \\nto gain the robustness and higher performance. Duc et al.  [12] \\ntake the second- to-last output layer as the encoded features, \\nand utilize Support Vector Machine (SVM) to be the label \\npredictor. Dennis et al.  [13] propose a 2-channel CNN, and \\nthe first convolutional layer in one of the channels is trained \\nby Convolutional Auto-Encoder (CAE) to learn the better \\ncapability in order to extract better features. In order to \\nmitigate the effect of head pose, a CNN learns the pose-\\nrobust features by regressing the features extracted from the \\nPrincipal Component Analysis Network (PCANet) which has \\nbeen trained by the frontal facial images with various \\nexpressions [14]. Different from traditional learning \\nalgorithms in CNN, the model in [15] learns the correlations \\namong the training data. To mitigate the person-specific \\ndifferences, Zibo Meng et al.  [35] and Chongsheng Zhang et \\nal. [36] propose a way to train an identity-aware structure to \\nextract the person-specific features for recognizing the facial \\nexpressions. Rather than trying to recognize a single image, \\n[16][17][18] predict the expressions by passing a video, \\nwhich seems more reasonable in practice; nevertheless, \\nlabeling video data is more labor intensive. \\nThe approaches mentioned are all static after the learning \\nprocedure. If applying enough data for training, they can do \\nwell in general cases while the performance would be \\nrelatively low in the specific testing, like the experiment \\nresults in [9]. Moreover, CNNs are also weak with cross-\\ndomain data [6]. Consequently, adaptive learning may be a \\npossible solution to tailor the generic model in specific cases. \\nFeature adaption proposed in [19] tries to find a mapping in \\nthe universal Reproducing Kernel Hilbert Space (RKHS). \\nWith the mapping, the features of samples can be transferred \\ninto a new space where the feature distributions of testing \\nand training samples are similar. Sinno  et al.  [20] realize the \\ndomain adaption by transfer component analysis while Tzu-\\nMing et al.  [21] aim to use closest common space learning \\nfor associating cross-domain data. Besides, Wen-Sheng et al.  \\n[22] train the generic SVM by re-weighting the trained \\nsamples which are most relevant to the testing data. These \\nmethods have to calculate the relations between both training \\nand testing data sets, so the computational complexity is so \\nhigh that it is difficult to apply them to deep learning models.  \\nIn this research, three types of Adaptive Feature Mapping \\n(AFM) are proposed to transfer the feature space of testing \\nsamples to that of training samples as close ly as possible. \\nSince AFM learns the data sequentially, it can be deployed to \\ndeep learning models easily, and does improve the \\nperformance. \\nThere are two main contributions in this paper. First, a \\nnovel pre-processing method is proposed for the general \\nfacial image processing, and it does improve the \\nperformance. Second, the domain adaption methods, AFMs, \\nfor deep learning models with large number of training data \\nis proposed, which can fine-tune the parameters efficiently and gain better recognition accuracy in the specific \\napplications. \\nThis paper is organized as follows. Section II introduces \\nthe preparation of testing and training data. Section III and \\nIV address the method of image pre-processing and the CNN \\narchitecture. Section V explains how AFMs work and their \\ndesign principles. Section VI shows the experiments and the \\ndiscussions. Section VII is the conclusion. \\nII.  FACIAL EXPRESSION DATABASE \\nThis section addresses the usage of the facial expression \\ndatabase and the process of preparing the training and testing \\ndata. Only seven common facial expressions, anger, disgust, \\nfear, happiness, sadness, surprise, and neutral, are considered \\nin this paper. Other expressions are ignored even if they are \\ncollected in the public domain database. \\nA. EXTENDED COHN-KANADE \\nExtended Cohn-Kanade (CK+), [2], has been widely utilized \\nto research facial expression recognition for years. In each \\nexpression category for a person, there are about 15 images \\nin a sequence, and the expression intensity changes from low \\nto high. The first one or two images are regarded as the \\nneutral expression while the last one or two images are \\nselected as the expressions in full effect. Consequently, there \\nare 630 images from CK+. The samples are shown in Fig. 1. \\nB. RADBOUD FACES DATABASE \\nThe Radboud Faces Database (RaFD), [3], is a high quality \\ndatabase of faces, which contains pictures of 8 emotional \\nexpressions, including Caucasian males and females, \\n(a) (b) (c) (d) (e) (f) (g)\\n \\nFIGURE  1.  Samples in CK+. (a) Anger. (b) Disg ust. (c) Fear. (d) \\nHappiness. (e) Sadness. (f) Surprise. (g) Neutral.  \\n  \\n(a) (b) (c) (d) (e) (f) (g)\\n \\nFIGURE  2.  Samples in RaFD . (a) Anger. (b) Disgust. (c) Fear. (d) \\nHappiness. (e) Sadness. (f) Surprise. (g) Neutral.  \\n  \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 Caucasian children, both boys and girls, and Moroccan \\nDutch males. Head poses vary from left side to right, and \\neach pose is shot with three eye gazing directions. Compared \\nto CK+, RaFD is more challenging to the recognition model. \\nThe samples are shown in Fig. 2. \\nC. AMSTERDAM DYNAMIC FACIAL EXPRESSION SET \\nAround 10 emotional expressions are collected in the \\nAmsterdam Dynamic Facial Expression Set (ADFES), [4]. \\nMost of them are videos with head pose variations, and the \\nexpression intensity also changes from low to high, like in \\nCK+. The facial images are captured with fixed time steps \\nwhen the expressions start to become obvious. The samples \\nare shown in Fig. 3. \\nD. PROPRIETARY DATABASE \\nTo make the deep model more robust and general, a home-\\ngrown/proprietary database is built to train the model. 372 \\nvideos are downloaded from YouTube, including movies, \\nfilm reviews, variety shows, and some short videos. After \\nthat, a face detection method proposed by D. E. King, [25], is \\nemployed to capture the face images with the time intervals \\nset to 1, 2, or 3 second(s) to avoid repeating the images with \\nsimilar expressions for one person. Then, 100,000 facial \\nimages are produced. Only the images that represent their \\ncorresponding categories are manually picked to be the \\ntraining and testing samples. This database ended up with \\n17,655 images. Some samples are shown in Fig. 4. \\nE. TRAINING AND TESTING DATA REARRANGEMENT Since CK+ is a well-known benchmark in facial expression \\nrecognition and the number of images is small, it will not be \\nplaced in the training set but instead in the testing set only in \\norder to objectively show the performance of the proposed \\napproach. Out of RaFD and ADFES, 10 and 4 person s’ \\nimages are chosen to be the testing data respectively; \\ntherefore, people in the training and the testing sets are \\ndefinitely different. Altogether, the number of testing images \\nis 630 from CK+, 616 from RaFD, and 562 from ADFES \\nwhile the number of training data is 23,591 including 17,655 \\nfrom the proprietary database, 3,377 from RaFD and 2,559 \\nfrom ADFES. The configuration of the testing and training \\ndata is listed in TABLE I. \\nTo balance the image count throughout all categories, the \\ncategory with less images is supplemented by copying the \\nrandomly selected images before combining the training data. \\nIn this way, the total number of images in every category will \\nbe the same.  \\nResearches show that if the data is augmented in a \\nreasonable way, the model can perform much better, [30]. \\nThus, the training set is mirrored and also augmented by two \\nGamma transformation, three Gaussian blur, and three \\nsharpening filter, so one image is extended to 42 images. As \\na result, the total number of training data is increased to \\n2,315,544, and the resolution is set to \\n6 4 6 4\\uf0b4  pixels in \\ngrayscales. TABLE  I \\nTHE CONFIGURATION OF THE TESTING AND TRAINING DATA \\nType  Database  Anger  Disgust  Fear  Happiness  Sadness  Surprise  Neutral  Total  \\nTesting CK+  63 91 40 130 42 150 114 630 \\nRaFD  88 90 87 90 89 83 89 616 \\nADFES  80 80 80 80 82 80 80 562 \\nTrain ing RaFD  494 494 466 493 485 452 493 3,377 \\nADFES  367 373 367 367 380 343 362 2,559  \\nProprietary  407 244 411 6,545  2,105  941 7,002  17,655  \\n \\n(a) (b) (c) (d) (e) (f) (g)\\n \\nFIGURE  3.  Samples in ADFES . (a) Anger. (b) Disgust. (c) Fe ar. (d) \\nHappiness. (e) Sadness. (f) Surprise. (g) Neutral.  \\n  \\n(a) (b) (c) (d) (e) (f) (g)\\n \\nFIGURE  4.  Samples in proprietary  database  (a) Anger. (b) Disgust. (c) \\nFear. (d) Happiness. (e) Sadness. (f) Surprise. (g) Neutral.  \\n  \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 III. IMAGE PRE-PROCESSING \\nPrevious researches, [10] and [11], have shown that if the \\nimage is pre-processed appropriately, the recognition \\nperformance can be improved. In this chapter, a proposed \\npre-processing method which contains spatial normalization \\nand feature enhancement is introduced. \\nA. SPATIAL NORMALIZATION \\nThe purpose of spatial normalization is to adjust the \\nalignment of the position and rotation angle of the detected \\nfacial images. An example is shown in Fig. 5. \\nA face alignment algorithm [23] is utilized to detect some \\nlandmarks on the face. The tip of the nose will be shifted to \\nthe center of the image so that the placement offset can be \\nmitigated. \\nB. FEATURE ENHANCEMENT \\nLocal Binary Pattern (LBP) [30] may be an efficient way to \\nextract the features from images. Nonetheless, it may lose a \\nlot of intrinsic information. Jiwen et al. [24] try to fix this \\nproblem by finding a mapping from the Neighbor-Center \\nDifference Vector (NCDV) into the binary space so that the \\npatterns can better represent the images in the original \\ndatabase, but it needs more computing effort. \\nNeighbor-Center Difference Image (NCDI) is presented to \\nenhance the edges efficiently and retain the original \\ninformation. The concept is the same as NCDV. NCDIs are \\nextracted by subtracting the center pixel from the \\nneighboring pixels, so the pixel values fall in the range from \\n255\\uf02d\\n to \\n255 . An NCDI collects the subtraction results of \\nthe selected channel from all patch es to reconstruct the image. \\nThus, eight images which have been sharpened in eight \\ndifferent directions are produced if the 8-channel NCDI is \\napplied, Fig. 6. \\nAfter enhancing the edges, the facial contour and \\nbackground become sharper, but they have nothing to do \\nwith facial expressions. Hence, facial image cropping should \\nbe applied. Since the facial contour is often confused with the background, the detected landmarks may drift between the \\nfacial contour and background. It is not recommended to \\ncrop the facial image by connecti ng the landmarks, which is \\nconsidered as polygon cropping. Except for the contour, \\nother landmarks are more stable. An elliptical region which \\nregresses the suitable landmarks, as shown in Fig. 7, is the \\nbetter way to crop the facial image effectively. The ellipse \\nfunction is \\n \\n\\uf028 \\uf02922\\n1 2 3 4 5 6, f x y a x a x y a y a x a y a \\uf03d \\uf02b \\uf02b \\uf02b \\uf02b \\uf02ba , (1) \\nwhere \\n\\uf05b \\uf05d1 2 6T\\na a a\\uf03da\\n  and \\n2\\n1 3 240a a a\\uf02d\\uf03e . To \\nregress these landmarks, the cost function is defined as \\n \\n\\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf02922\\n1 3 2 1 3\\n1,\\n1\\n,4\\n22N\\nnn\\nnf x y a a a a a\\nN\\uf064\\n\\uf03d\\uf057\\uf03d\\n\\uf02d \\uf02d \\uf02b \\uf02b \\uf0e5a x y\\na ,  (2) \\nwhere \\n\\uf05b \\uf05d12T\\nNx x x\\uf03dx\\n  and \\n\\uf05b \\uf05d12T\\nNy y y\\uf03dy\\n  \\nare the selected positions of the landmarks, \\nN  is the sample \\nnumber, and \\n\\uf064  is the hyper parameter used to regularize the \\noptimization. By setting the gradient of the cost function to \\nzero, \\n\\uf028 \\uf029 ,0 \\uf0d1\\uf057 \\uf03d a x y , the equation becomes \\n \\n\\uf028 \\uf029\\uf02d\\uf03dDΨaΛ , (3) \\n \\n4 3 2 2 3 2 2\\n3 2 2 3 2 2\\n2 2 3 4 2 3 2\\n3 2 2 2\\n1\\n2 2 3 2\\n221i i i i i i i i i\\ni i i i i i i i i i i i\\nN\\ni i i i i i i i i\\ni i i i i i i i i i\\ni i i i i i i i i\\ni i i i i ix x y x y x x y x\\nx y x y x y x y x y x y\\nx y x y y x y y y\\nx x y x y x x y x\\nx y x y y x y y y\\nx x y y x y\\uf03d\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf03d\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fb\\uf0e5D , (4) \\n \\n2\\n0 0 0 0 0\\n0 0 0 0 0\\n2\\n0 0 0 0 0\\n0 0 0 0 0 0\\n0 0 0 0 0 0\\n0 0 0 0 0 0N\\nN\\nN\\uf064\\n\\uf064\\n\\uf064\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf02d\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf03d\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0eb\\uf0fbΨ , (5) \\n \\n0 0 0 0\\n22TNN\\uf064\\uf064\\uf0e9\\uf0f9\\uf03d\\uf0ea\\uf0fa\\uf0eb\\uf0fbΛ , (6) \\n \\nFIGURE  5.  Spatial normalization in image pre -processing . \\n  \\n \\nFIGURE 7.  Ellipse regression. The red landmarks are the suitable \\npoints used to find the cropping boundary, the blue curve, by ellipse \\nregression.  \\n \\n \\nFIGURE 6.  Neighbor -center difference images (NCDIs).  \\n  \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 where \\n\\uf02dDΨ  is a symmetric matrix, so \\n\\uf028 \\uf0291\\uf02d\\uf02dDΨ  exists if it \\nis of full rank. Through \\n\\uf028 \\uf0291\\uf02d\\uf03d\\uf02daD Ψ Λ , there is an \\nanalytical solution to find the ellipse. \\nThe schematic diagram is shown in Fig. 7. Only the pixels \\nin the ellipse and those lower than the highest landmarks of \\nthe eyebrows are kept, as Fig. 8 shows. The differences \\nbetween ellipse cropping and polygon cropping are shown in \\nFig. 9. \\nThe pre-processing procedure follows the steps below . \\nDetect the face and find the bounding box [25]. Resize the \\nfacial image into \\n6 4 6 4\\uf0b4  pixels. Then, extract the landmarks \\non the face, and perform the spatial normalization and feature \\nenhancement last. \\nIV. DEEP CONVOLUTIONAL NEURAL NETWORK \\nBased on Caffe framework [26], a CNN model is designed \\nfrom the concept of [6] and [27]. There are parallel \\nstructures in the network to extract the features using \\ndifferent sizes of windows. The model consists of nine \\nconvolutional layers, two max pooling layers, one mean \\npooling layer, three fully connected layers, and a Local \\nResponse Normalization (LRN) layer. The activation \\nfunctions are all set to rectified linear functions. Other \\ndetails and the configuration of the model are shown in Fig. \\n10. \\nLike [12], the output of Full connection layer (L12) is \\nregarded as the encoded features. The combination of Full \\nconnection layer (L13) and Softmax output layer (L14) is \\nregarded as a classifier. Except for the classifier, the whole \\nstructure is a feature extractor for the input image. The \\nConvolutional Feature Extractor (CFE) is defined as from \\nConvolution layer (L1) to Mean pooling layer (L10) while \\nthe Fully Connected Feature Extractor (FCFE) is defined as \\nfrom Full connection layer (L11) to Full connection layer \\n(L12). \\n \\nFIGURE 10.  Convolutional neural network architecture. C, H, W are the \\noutput channel, height, and weight respectively. K, S, P stand for the  \\nkernel size, stride, and patch of convolution or pooling. N is the local \\nsize while A and B are the scale and the exponent parameters of LRN. \\nD is the dropout ratio.  \\n \\nFIGURE 8.  Ellipse cropping of NCDIs.  \\n \\n \\nFIGURE 9.  Comparison between ellipse cropping and polygon cropping. \\nThe images in the top row are the results of ellipse cropping NCDIs, and \\nthe images in the bottom row are the results of polygon cropping NCDIs.  \\n \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 V. ADAPTIVE FEATURE MAPPING \\nThis section address es the design principle and the \\nmechanism of AFM. In the following description, the trained \\nand testing data sets are denoted as \\n12ss s s s\\nN\\uf0e9\\uf0f9\\uf03d\\uf0eb\\uf0fbX x x x\\n and \\n12tt t t t\\nN\\uf0e9\\uf0f9\\uf03d\\uf0eb\\uf0fbX x x x\\n  \\nrespectively, and \\nsN  is the number of trained samples while \\ntN\\n is the batch size of the testing samples. The feature \\nextractor consists of CFE and FCFE, and it is denoted as \\n\\uf028 \\uf029h x W\\n. \\nW  is the parameter set of the whole feature \\nextractor while \\nx  is the input sample. In this research, \\nx  is \\nthe 8-channel NCDIs. \\nA. COST FUNCTION \\nThe main purpose of AFM is to tune the parameters of the \\nfeature extractor for the testing samples so that the tuned \\nfeature extractor can make the feature distribution of the \\ntesting samples similar to that of the trained samples. See Fig. \\n11. That is, \\n\\uf028 \\uf029\\uf028\\uf029\\uf028 \\uf029\\uf028 \\uf029tspp \\uf0bb h x W h x W\\n , where \\nW  is the \\ngeneric parameter set, and \\nW\\n  is the new parameter set for \\nthe testing samples. To accomplish this, the discrepancy \\nbetween the means of the trained and testing samples must be \\nminimized. According to [19][20][22], the cost function can \\nbe written as \\n \\n\\uf028 \\uf029\\n\\uf028 \\uf029\\uf028\\uf029 \\uf028 \\uf029\\uf028 \\uf0292\\n11,\\n11tsts\\nNN\\nts\\nij\\nijtsE\\nNN\\uf06a\\uf06a\\n\\uf03d\\uf03d\\uf048\\uf03d\\n\\uf02d \\uf0e5\\uf0e5W X X\\nh x W h x W\\n , (7) \\nwhere \\n\\uf048  stands for RKHS which can be defined by a kernel, \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\uf028 \\uf029\\uf028 \\uf029 \\uf028 \\uf029\\uf028\\uf029 ,T\\nt s t s\\ni j i jk \\uf06a\\uf06a\\uf03d h x W h x W h x W h x W\\n, to \\ndescribe the relation between \\n\\uf028 \\uf029t\\nih x W  and \\n\\uf028 \\uf029s\\njh x W . The \\nlinear kernel is chosen for simplicity, so the kernel is \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\uf028 \\uf029 \\uf028 \\uf0292\\n,t s t s\\ni j i jk \\uf03d\\uf02d h x W h x W h x W h x W\\n. Based on \\n[20], the cost function shall be \\n \\n\\uf028 \\uf029\\uf028 \\uf029 ,tsE T r\\uf03d W X X K L\\n ,  (8) \\n \\n,,\\n,,s s s t\\nt s t t\\uf0e9\\uf0f9\\n\\uf03d\\uf0ea\\uf0fa\\n\\uf0eb\\uf0fbKK\\nK\\nKK , (9)  \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf0291 1 1 2 1\\n2 1 2 2 2\\n,\\n12, , ,\\n, , ,\\n, , ,s\\ns\\ns s s ss s s s s s\\nN\\ns s s s s s\\nN\\nss\\ns s s s s s\\nN N N Nk k k\\nk k k\\nk k k\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf03d\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fbh h h h h h\\nh h h h h h\\nK\\nh h h h h h\\n , (10) \\n \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf0291 1 1 2 1\\n2 1 2 2 2\\n,\\n12, , ,\\n, , ,\\n, , ,t\\nt\\nt t t tt t t t t t\\nN\\nt t t t t t\\nN\\ntt\\nt t t t t t\\nN N N Nk k k\\nk k k\\nk k k\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf03d\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fbh h h h h h\\nh h h h h h\\nK\\nh h h h h h\\n , (11) \\n \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf0291 1 1 2 1\\n2 1 2 2 2\\n,\\n12, , ,\\n, , ,\\n, , ,t\\nt\\ns s s ts t s t s t\\nN\\ns t s t s t\\nN\\nst\\ns t s t s t\\nN N N Nk k k\\nk k k\\nk k k\\uf0e9\\uf0f9\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf03d\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\n\\uf0ea\\uf0fa\\uf0eb\\uf0fbh h h h h h\\nh h h h h h\\nK\\nh h h h h h\\n , (12) \\n \\n,,T\\nt s s t\\uf03d KK , (13) \\nwhere \\nt\\nih  and \\ns\\njh  are the abbreviations of \\n\\uf028 \\uf029t\\nih x W\\n  and \\n\\uf028 \\uf029s\\njh x W\\n respectively. The elements of the matrix \\nL  is \\n21\\nsN\\uf02d\\n if \\n,s\\nij\\uf0ce x x X , else \\n21\\ntN\\uf02d  if \\n,t\\nij\\uf0ce x x X , otherwise, \\n1\\nstNN\\n. Since we are only concerned with the cross relations \\nbetween the features of trained and testing samples, other \\nterms can be eliminated. Therefore, the cost function can be \\nmodified as \\n \\n\\uf028 \\uf029\\n\\uf028 \\uf029 \\uf028 \\uf0292\\n,\\n11,\\ntsts\\nNN\\nij ts\\nij\\nij stE\\nNN\\uf061\\n\\uf03d\\uf03d\\uf03d\\n\\uf02d \\uf0e5\\uf0e5W X X\\nh x W h x W\\n , (14) \\nwhere \\n,ij\\uf061  are the weights to represent the relevance \\nbetween \\n\\uf028 \\uf029t\\nih x W   and \\n\\uf028 \\uf029s\\njh x W . A large \\n,ij\\uf061  value is \\nrequired when the features of trained and testing samples are \\nrelevant, i.e. error becomes small. On the contrary, a smaller \\n,ij\\uf061\\n value is required when the features of trained and testing \\nsamples are less relevant, i.e. error becomes larger. If \\n,ij\\uf061  is \\nnot appropriate, the cost function will oscillate drastically and \\n \\nFIGURE 11.  The purpose of AFM. There are red and blue categories in the example. The crosses and circles are the trained data. The triangles are the \\nnew testing subjects, and the color on each subject represents the ground truth.  \\n \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 may not converge during the training process. Moreover, the \\ncomputational complexity will increase as the number of \\ntrained samples increases. The winner-take-all strategy, \\ntherefore, is considered, so the cost function can be re-written \\nas \\n \\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf0292\\n11\\n,\\n2tN\\nt s t s\\nir\\nitE\\nN\\uf03d\\uf03d\\uf02d\\uf0e5W X X h x W h x W\\n , (15) \\n \\n\\uf028 \\uf029 \\uf028 \\uf0292\\na rg m ints\\nirr\\uf0e6\\uf0f6\\uf03d\\uf02d \\uf0e7\\uf0f7\\n\\uf0e8\\uf0f8h x W h x W\\n , (16) \\nwhere \\nr  is the index of the trained sample which is most \\nrelevant to the \\nthi  new sample in distance measure, and \\n\\uf05b \\uf05d1,srN\\uf0ce\\n. By minimizing the nearest trained sample, the \\noscillation is damped, and the computational complexity is \\nreduced. \\nSince some bad samples may limit the performance of \\nAFM , the probability distribution on the output of the \\nclassifier can be taken into consideration to regularize the \\ncost function. By simply multiplying the prediction \\nconfidence, the cost function can be written as \\n \\n\\uf028 \\uf029\\n\\uf028 \\uf029\\uf028 \\uf029\\uf028 \\uf029 \\uf028 \\uf0292\\n1,,\\n1\\n2t\\ns\\nrt s s\\nN\\ns t s\\nr i ry\\nitE\\nf\\nN\\uf03d\\uf03d\\n\\uf0d7\\uf02d \\uf0e5W X X y\\nh x W h x W h x W\\n , (17) \\nwhere the entries of \\n12sTs s s s\\nNy y y\\uf0e9\\uf0f9\\uf03d\\uf0eb\\uf0fby\\n  are the \\nlabels of the trained samples. \\n\\uf028 \\uf02912kT\\nNf f f\\uf0e9\\uf0f9\\uf03d\\uf0eb\\uf0fbf\\n  is \\nthe classifier, and \\nkN  is the number of categories. This \\nform of AFM is defined as Weighted Adaptive Feature \\nMapping (W-AFM). \\nThe classifier in CNN is a linear transformation. After a \\nCNN model is well trained, the features extracted by the \\nmodel must be linearly separable. Therefore, there must be \\na unique center in each category. If the centers of the \\ncategories are considered, the person-specific bias can be \\nmitigated further. The cost function, then, can be modified \\nas \\n \\n\\uf028 \\uf029\\n\\uf028 \\uf029\\uf028 \\uf029\\uf028 \\uf0292\\n1,,\\n1\\n2t\\nss\\nrrt s s\\nN\\ns t c\\nriyy\\nitE\\nf\\nN\\uf03d\\uf03d\\n\\uf0d7\\uf02d \\uf0e5W X X y\\nh x W h x W h\\n , ( 18) \\nwhere \\ns\\nrc\\nyh  stands for the feature center of category \\ns\\nry , and \\nsuch form of AFM is regarded as Weighted Center \\nRegression Adaptive Feature Mapping (W- CR-AFM). \\nThese cost functions can be easily solved by the \\nstochastic gradient descent which is written as follow: \\n \\n\\uf028 \\uf0291,,k k t s s kE\\uf068\\uf06c\\uf02b\\uf0e9\\uf0f9 \\uf03d \\uf02d \\uf0d7 \\uf0d1 \\uf02b\\uf0eb\\uf0fbW W W X X y W\\n ,  (19) \\nwhere \\n\\uf068  is the learning rate, and \\n\\uf06c  is the regularizing \\nfactor to prevent the parameters from going out of bound. \\n\\uf028 \\uf029 ,,t s sE\\uf0d1 W X X y\\n is the gradient of the cost function. B. SYSTEM OPERATION \\nAfter training the CNN model, the extracted features of \\ntraining samples shall be stored as the feature database. In the \\ntesting phase, AFM can tune the weights based on the \\nrelationship between features of testing samples and the \\nfeature database in order to transform the features of testing \\nsamples into a new space so that its distribution can be \\nsimilar to that of the feature database. Most of the parameters \\nare distributed in the fully connected layers, so AFM is only \\napplied to tune FCFE for higher efficiency. See Fig. 12. The \\npremise of AFM is that the feature distribution of the testing \\nsamples is assumed to be similar to that of the training \\nsamples. The features around the decision boundary, \\ntherefore, shall be moved to the centers of categories. This \\nway, the misclassified labels can be corrected. In addition, \\nmisclassified trained samples must be removed in advance so \\nthat the newly mapped features can be better. To make it \\nmore reliable, the testing samples with lower confidence of \\nprediction can be ignored. \\nVI. EXPERIMENTS \\nA.  IMAGE PRE-PROCESSING COMPARISON \\nTABLE II shows the results of the proposed model with \\ndifferent pre-processing methods. As can be seen in TABLE \\nII, the spatial normalization does not always seem to help the \\nrecognition accuracy since the edges of bounding box may \\nappear and become the main feature of the image after spatial \\nnormalization is applied, which impairs the recognition \\nfunction, causing the accuracy to be lower . Also, the model \\nhas been trained with many candid images from YouTube, so \\nit can extract some features that are not affected by rotation. \\nCFE ClassifierImage \\npre-processing\\nFeature databaseAFMFCFE\\nCFEImage \\npre-processing\\nHappinessClassifier FCFE\\nLossTesting phase\\nTraining phase \\nFIGURE 12.  System architecture.  \\nTABLE  II \\nIMAGE PRE-PROCESSING COMPARISON  \\nPre-processing  Accuracy (%)  \\nCK+ RaFD  ADFES  \\nNone  82.22  90.26  85.59  \\nSN 81.75  94.16  83.10  \\nSN + FE  86.83  95.78  87.37  \\nSN stands for spatial normalization while FE sta nds for feature \\nenhancement.  \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 Thus, the recognition accuracy can be higher than when \\nspatial normalization is applied. These may be the reasons \\nwhy the spatial normalization appears ineffective in Table II . \\nThe feature enhancement operation not only makes the facial \\nedges more distinct but also removes the areas that are irrelevant to facial expressions, so the accuracy can be \\nincreased by about 4.61% in CK+, 5.52% in RaFD, and \\n1.78% in ADFES. These results demonstrate that the \\nproposed pre-processing method is really effective. \\n7072747678808284868890\\n16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)\\nBatch SizeAFM\\nW-AFM\\nW-CR-AFM\\nGM \\nFIGURE 13.   The improved accuracy of CK+ in different batch size.  \\n \\n9394959697\\n16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)\\nBatch SizeAFM\\nW-AFM\\nW-CR-AFM\\nGM\\n \\nFIGURE 14.   The i mproved accuracy of RaFD in different batch size.  \\n \\n858687888990919293\\n16 32 48 64 80 96 112 128 144 160 176 192 208 224 240 256 512Accuracy (%)\\nBatch SizeAFM\\nW-AFM\\nW-CR-AFM\\nGM\\n \\nFIGURE 15.   The improved accuracy of ADFES in different batch size.  \\n \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 B. THE EFFECT OF ADAPTIVE FEATURE MAPPING \\nThe proposed CNN model with spatial normalization and \\nfeature enhancement is regard ed as our Generic Model (GM). \\nAs for AFM, the learning rate \\n\\uf068  is set to 0.001 while the \\nregularizing factor \\n\\uf06c  is set to 0.0005. The training iteration \\nis set to 1000. The batch size ranges from 16 to 5 12. The \\ntrained samples and testing samples are mirrored, and the \\ntrained samples which are misclassified  should be removed \\nin advance while the testing samples whose prediction \\nconfidence is lower than 90% are not taken into \\nconsideration when tuning the model with AFM . The results \\nare shown in Fig. 13, Fig. 14, and Fig. 15. In most cases, W-\\nAFM performs better than AFM, and W- CR-AFM is the best \\nof all. The performance will be more stable when the batch \\nsize is large enough, otherwise the effect may be limited.  \\nAccording to the experiments shown in Fig. 13, Fig. 14, \\nand Fig. 15, the best result of each AFM is listed in TABLE \\nIII. Based on the result of GM, the improved recognition \\naccuracy is in TABLE IV.  The category of happiness can \\nalways be predicted correctly in these three databases \\nbecause its feature is obvious and its training data is ample. \\nAfter applying AFM, most predicted labels are corrected. \\nCompared to other categories, the number of images in \\nanger, disgust and fear is less, so the ability of extracting \\nfeatures for these expressions is poor; hence, most features \\nof testing samples do not fall into the center of the category, \\nand will be drawn to other categories. Besides, the \\nexpression of anger is usually not explicit, so it is \\nsometimes confused with neutral expression even if AFM \\nor W-AFM is utilized. The main feature of surprise is the \\nexaggerated mouth while the minor feature is the eyes, but \\nthe feature of eyes is difficult to extract properly because it varies greatly with the person. Hamid et al.  [29] have \\nproven that the mouth is the primary feature for facial \\nexpressions. However, some surprised faces do not clearly \\nexpress the feature on the mouth in ADFES, so they are \\nmisclassified into neutral expression if AFM or W-AFM is \\napplied.  \\nFor W- CR-AFM, since it minimizes the distance \\nbetween the feature of the testing sample and the center of \\nthe most relevant category rather than the most relevant \\nfeature of the trained sample, the person-specific bias can \\nbe mitigated much more. Moreover, the feature distribution \\nof neutral expression contains the largest area in the featur e \\nspace, so the features of neutral expression that are far away \\nfrom the center of neutral expression category will be \\nbrought to other categories. That is why the recognition \\naccuracy of neutral expression is reduced after applying W-\\nCR-AFM while accuracy of other expressions are raised. \\nAccording to the experiments, all three types of AFM \\ncan assist in improving the performance of a model in \\nspecific cases. For the overall recognition accuracy, W- CR-\\nAFM works the best. \\nC. BENCHMARK COMPARISON \\nSome other deep learning approaches in facial expression \\nrecognition are introduced and compared with ours. They \\nare trained with our training data for fair comparison.   \\nTo make GoogLeNet [27] and AlexNet [28] perform better , \\nthey have been trained with ImageNet previously. The \\nsecond- to-last layer of the trained AlexNet is utilized to \\ntrain a SVM [12]. To present the original performance of \\nthe competing models, the architectures and training \\nparameters are set based on the original works. Since CK+ \\n[2] is not included in the training data, it is reasonable that TABLE  IV \\nIMPROVEMENT IN EACH DATABASE  \\nDatabase  \\nCategory  CK+  (%) RaFD  (%) ADFES  (%) \\nAFM  W-AFM  W-CR-AFM  AFM  W-AFM  W-CR-AFM  AFM  W-AFM  W-CR-AFM  \\nAnger  -7.94 -4.76 +3.17 +1.14 +1.14 +1.14 +7.50 +8.75 +10.00  \\nDisgust  +1.10 +4.40 +21.98  +1.11 +1.11 +2.22 0.00 +3.75 +11.25  \\nFear  +10.00  +10.00  +35.00  -2.30 -2.30 +1.15 -2.50 -2.50 +1.25 \\nHappiness  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 \\nSadness  +2.38 +2.38 0.00 +3.37 +3.37 +6.75 0.00 0.00 +3.66 \\nSurprise  +2.00 +2.00 +5.33 0.00 0.00 +1.20 -7.50 -5.00 +2.50 \\nNeutral  +1.75 0.00 -21.93  0.00 -1.13 -8.99 +25.00  +25.00  +8.75 \\nTotal  +0.95 +1.42 +3.01 +0.49 +0.32 +0.49 +3.20 +4.27 +5.33 \\n TABLE  III \\nACCURACY IN EACH DATABASE  \\nDatabase  \\nCategory  CK+  (%) RaFD  (%) ADFES  (%) \\nGM AFM  W-AFM  W-CR-AFM  GM AFM  W-AFM  W-CR-AFM  GM AFM  W-AFM  W-CR-AFM  \\nAnger  77.78  69.84  73.02  80.95  98.86  100.00  100.00  100.00  88.75  96.25  97.50  98.75  \\nDisgust  69.23  70.33  73.63  91.21  97.78 98.89  98.89  100.00  88.75  88.75  92.50  100.00  \\nFear  42.50  52.50  52.50  77.50  87.36  85.06  85.06  88.51  71.25  68.75  68.75  72.50  \\nHappiness  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  100.00  \\nSadness  83.33  85.71  85.71  83.33  88.76 92.13  92.13  95.51  93.90  93.90  93.90  97.56  \\nSurprise  94.67  96.67  96.67  100.00  98.80  98.80  98.80  100.00  97.50  90.00  92.50  100.00  \\nNeutral  97.37  99.12  97.37  75.44  98.88  98.88  97.75  89.89  71.25  96.25  96.25  80.00  \\nTotal  86.83  87.78  88.25  89.84  95.78  96.27  96.10  96.27  87.37  90.57  91.64  92.70  \\n \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 the recognition accuracy in TABLE V is lower than the \\nresults of state- of-the-arts. If the models are trained by CK+ \\n[2], the recognition accuracy is expected to be much higher. \\nThe results in TABLE V show that our approach \\nperforms better than others. The parameter quantities of \\nGoogLeNet, AlexNet and the CNN designed by Heechul et \\nal. [5] are around 40MB, 222MB, and 5MB respectively. \\nAlthough our parameter quantity, around 3.5MB, which is \\nmuch lower than others, the performance can be \\ncomparable to these state- of-the-art architectures through \\nthe use of proposed pre-processing method. Besides, AFMs \\ncan adapt the testing samples so that the model can perform \\nbetter than other approaches. \\nVII. CONCLUSION \\nTwo main contributions are presented in this paper. One \\ncontribution is that the proposed pre-processing method can \\nassist the CNN model to gain the higher accuracy rate in the \\napplications of facial image processing. The other \\ncontribution is that three types of AFMs can reformulate the \\nfeatures of new samples which do not have label information \\nso that some misclassified samples can be corrected, which \\nmeans it can tune a generic model to adapt to a specific \\ncondition. Moreover, AFMs can be deployed to real-time \\nsystems since it learns batch by batch rather than calculating \\nall of the training and testing data in one batch. The concept \\ndrift problem is restrained because AFMs map the features of \\nthe testing samples to a static feature distribution. With the \\npre-processing and AFMs, a light CNN can outperform the \\nstate- of-the-art architectures. \\nACKNOWLEDGMENT \\nThis work was supported by the Ministry of Science \\nTechnology under Grant no. MOST 106-2622-E- 009-009 -\\nCC2. \\nREFERENCES \\n[1] R. E. Jack, O. G. B. Garrod, H. Yu, R. Caldara, and P. G. Schyns, \\n“Facial expressions of emotion are not culturally universal,” Proc. \\nNational Academy of Sci. of the United States of America , vol. 109, \\nno. 19, pp. 7241 –7244, 2012. \\n[2] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. \\nMatthews, “The extended cohn -kanade dataset (CK+): A complete \\ndataset for action unit and emotion- specified expression,” In Comput. \\nVision and Pattern Recognition (CVPR) Workshops , 2010. [3] O. Langner, R. Dotsch, G. Bijlstra, D. H. J. Wigboldus, S. T. Hawk, \\nand A. van Knippenberg, “Presentation and validation of the \\nRadboud Faces Database,“ Cognition and Emotion , vol. 24, no. 8, pp. \\n1377 –1388, 2010. \\n[4] J. van der Schalk, S. T. Hawk, A. H. Fischer, and B. J. Doosje, \\n“Moving faces, looking places : The Amsterdam Dynamic Facial \\nExpressions Set (ADFES),” Emotion , vol. 11, no. 4, pp. 907 –920, \\n2011. \\n[5] W. Li, M. Li, and Z. Su, “A deep -learning approach to facial \\nexpression recognition with candid images,” MVA2015 IAPR Int. \\nConf. Mach. Vision Appl. , May 18 –22, 2015, Tokyo, Japan. \\n[6] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper in \\nfacial expression recognition using deep neural networks,”  IEEE \\nWinter Conf. Appl. of Comput. Vision (WACV) , pp. 1 –10, 2016. \\n[7] H. Jung, S. Lee, S. Park, B. Kim, J. Kim, I. Lee, and C. Ahn, \\n“Development of deep learning -based facial expression recognition \\nsystem,” 21st Korea-Japan Joint Workshop on Frontiers of Comput. \\nVision (FCV) , pp. 1 –4, 2015. \\n[8] A. Mollahosseini, B. Hassani, M. J. Salvador, H. Abdollahi, D. Chan, \\nand M. H. Mahoor, “Facial expression recognition from world wild \\nweb,” IEEE Conf. on Comput. Vision and Pattern Recognition \\n(CVPR)  Workshops, pp. 59 –65, 2016. \\n[9] X. Peng, Z. Xia, L. Li, and X. Feng, “Towards facial expression \\nrecognition in the wild: a new database and deep recognition \\nsystem,” IEEE Conf. Comput. Vision and Pattern Recognition \\n(CVPR)  Workshops, pp. 93 –99, 2016. \\n[10] A. T. Lopes, E. de Aguiar, and T. Oliveira- Santos, “A facial \\nexpression recognition system using convolutional networks,” 28th \\nSIBGRAPI Conf. Graphics, Patterns and Images , pp. 273 –280, 2015. \\n[11] G. Levi and T. Hassner, “Emotion recognition in the wild via \\nconvolutional neural networks and mapped binary patterns,” Proc. \\n2015 ACM Int. Conf. Multimodal Interaction , pp. 503 –510, 2015. \\n[12] D. M. Vo and T. H. Le, “Deep generic features and SVM for facial \\nexpression recognition,” 3rd National Foundation for Science and \\nTechnology Development Conf. Inf. and Comput. Sci. (NICS) , pp. \\n80–84, 2016. \\n[13] D. Hamester, P. Barros, and S. Wermter, “Face expression \\nrecognition with a 2- channel convolutional neural network,” Int. \\nJoint Conf. Neural Netw. (IJCNN) , pp. 1 –8, 2015. \\n[14] F. Zhang, Y. Yu, Q. Mao, J. Gou, and Y. Zhan, “Pose -robust feature \\nlearning for facial expression recognition,” J. Frontiers of Comput. \\nSci.: Selected Publications from Chinese Universities , vol. 10, no. 5, \\npp. 832 –844, 2016. \\n[15] Y. Guo, D. Tao, J. Yu, H. Xiong, Y. Li, and D. Tao, “Deep neural \\nnetworks with relativity learning for facial expression recognition,” \\nIEEE Int. Conf. Multimedia & Expo Workshops (ICMEW) , pp. 1 –6, \\n2016. \\n[16] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, “A deep \\nneural network driven feature learning method for multi-view facial \\nexpression recognition,” IEEE Trans. Multimedia , vol. 18, no. 12, pp. \\n2528 –2536, 2016. \\n[17] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, “Joint fine -tuning in \\ndeep neural networks for facial expression recognition,” IEEE Int. \\nConf. Comput. Vision (ICCV) , pp. 2983 –2991, 2015. \\n[18] Y. H. Byeon and K. C. Kwak, “Facial expression recognition using \\n3D convolutional neura l network,” Int. J. Advanced Comput. Sci. and \\nAppl. , vol. 5, no. 12, pp. 107– 112, 2014. \\n[19] R. Zhu, G. Sang, and Q. Zhao, “Discriminative feature adaptation for \\ncross- domain facial expression recognition,” Int. Conf. Biometrics \\n(ICB), pp. 1 –7, 2016. \\n[20] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation \\nvia transfer component analysis,” IEEE Trans. Neural Netw. , vol. 22, \\nno. 2, pp. 199 –210, 2011. \\n[21] T. M. H. Hsu, W. Y. Chen, C. A. Hou, Y. H. H. Tsai, Y. R. Yeh, and \\nY. C. F. Wang, “Unsupervised domain adaptation with imbalanced \\ncross- domain data,“ IEEE Int. Conf. Comput. Vision (ICCV) , pp. \\n4121 –4129, 2015. \\n[22] W. S. Chu, F. D. la Torre, and J. F. Cohn, “Selective transfer \\nmachine for personalized facial expression analysis,” IEEE Trans. \\nPattern Anal. Mach. Intell. , vol. 6, no. 1, pp. 1 –15, 2016. TABLE  V \\nBENCHMARK COMPARISON  \\nApproach  Accuracy (%)   \\nCK+  RaFD  ADFES   \\nGoogLeNet  [27] 85.71  95.45  86.48   \\nAlexNet  [28] 85.87  95.29  84.01   \\nAlexNet + SVM  [12] 86.83  95.13 88.43  \\nCNN  [7] 80.16  94.16  87.01   \\nOur GM  86.83  95.78  87.37   \\nOur GM +  AFM  87.78  96.27  90.57   \\nOur GM + W-AFM  88.25  96.10  91.64   \\nOur GM + W-CR-AFM  89.84  96.27  92.70   \\n \\n2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/ACCESS.2018.2805861, IEEE Access\\n \\nVOLUME XX, 2017 9 [23] V. Kazemi, J. Sullivan, “One millisecond face alignment with an \\nensemble of regression trees,” IEEE Conf. Comput. Vision and \\nPattern Recognition (CVPR) , pp. 1867 –1874, 2014. \\n[24] J. Lu, V. E. Liong, and J. Zhou, “Cost -sensitive local binary feature \\nlearning for facial age estimation,”  IEEE Trans. Image Process. , vol. \\n24, no. 12, pp. 5356 –5368, 2015. \\n[25] D. E. King, “Max -margin object detection,” Comput. Vision and \\nPattern Recognition (cs.CV) , arXiv:1502.00046, 2015. \\n[26] Y. Q. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. \\nGirshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional \\nArchitecture for Fast Feature Embedding,” J. arXiv preprint \\narXiv:1408.5093 , 2014. \\n[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. \\nErhan, V. Vanhoucke, A. Rabinovich, “Going deeper with \\nconvolutions,” IEEE Conf. Comput. Vision and Pattern Recognition \\n(CVPR) , pp. 1 –9, 2015. \\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet \\nclassification with deep convolutional neural net works,” Advances in \\nNeural Inf. Process. Syst. , pp. 1097 –1105, 2012. \\n[29] H. Sadeghi, A. A. Raie, M. R. Mohammadi, “Facial expression \\nrecognition using geometric normalization and appearance \\nrepresentation,” 8th Iranian Conf. Mach. Vision and Image Process. \\n(MVIP) , pp. 159 –163, 2013. \\n[30] P. Simard, D. Steinkraus, and J. C. Platt, “Best practices for \\nconvolutional neural networks applied to visual document analysis,” \\nin Seventh Int. Conf. Document Analysis and Recognition , pp. 958 –\\n963, 2003. \\n[31] D. C. He and L. Wang, \"Texture unit, texture spectrum, and texture \\nanalysis,\"  IEEE Trans. Geosci. Remote Sens. , vol. 28, no. 4, pp. 509 –\\n512, 1990. \\n[32] Y. Ding, Q. Zhao, B. Li, and X. Yuan, “Facial expression recognition \\nfrom image sequence based on LBP and Taylor expansion,” IEEE \\nAcce ss, vol. 5, pp. 19409 –19419, 2017. \\n[33] B. Ryu, A. R. Rivera, J. Kim, and O. Chae, “Local directional ternary \\npattern for facial expression recognition,” IEEE Trans. Image \\nProcess. , vol. 26, no. 12, pp. 6006– 6018, Dec. 2017. \\n[34] Q. Mao, Q. Rao, Y. Yu, and M. Dong, “Hierarchical Bayesian theme \\nmodels for multipose facial expression recognition,” IEEE Trans. \\nMultimedia , vol. 19, no. 4, pp. 861 –873, Apr. 2017. \\n[35] Z. Meng, P. Liu, J. Cai, S. Han, and Y. Tong, “Identity -aware \\nconvolutional neural network for facial expressi on recognition,” \\nIEEE Int. Conf. Automatic Face & Gesture Recognition, pp. 558 –\\n565, 2017. \\n[36] C. Zhang, P. Wang, K. Chen, and J. K. Kämäräinen , “Identity -aware \\nconvolutional neural networks for facial expression recognition,” J. \\nof Syst. Eng. and Electron. , vol. 28, no. 4, pp. 784-792, Aug. 2017. \\nBing-Fei Wu  (M’92 -SM’02 -F\\'12) received the \\nB.S. and M.S. degrees in control engineering from National Chiao Tung \\nUniversity (NCTU), Hsinchu, Taiwan, in 1981 and 1983, respectively, and \\nthe Ph.D. degree in electrical engineering from the University of Southern \\nCalifornia, Los Angeles, USA, in 1992. Since 1992, he has been with the \\nDepartment of Electrical and Computer Engineering, where he was \\npromoted to be a Professor in 1998 and Distinguished Professor in 2010, \\nrespectively. He serves as the Director of the Institute of Electrical and \\nControl Engineering, NCTU in 2011. He founded and served as the Chair of Taipei Chapter of IEEE Systems, Man and Cybernetics Society (SMCS) \\nin 2003, and is also the Chair of the Technical Committee on Intelligent \\nTransportation Systems of IEEE SMCS since 2011. Currently, he is an \\nAssociate Editor of the IEEE Transactions on Systems, Man and \\nCybernetics: Systems and Editor- in-Chief of International Journal of \\nComputer Science and Artificial Intelligence. Dr. Wu is an IEEE Fellow, \\nIET Fellow and CACS Fellow. His research interests include image \\nrecognition, vehicle driving safety and control, intelligent robotic systems, \\nintelligent transportation systems, and multimedia signal analysis. \\nDr. Wu receives many research honors, including the Outstanding \\nResearch Award of  Ministry of Science and Technology, Taiwan, in 2015; \\nthe Technology Invention Award of Y. Z. Hsu Scientific Award from Y. Z. \\nHsu Foundation in 2014; the National Invention and Creation Award of \\nMinistry of Economic Affairs, Taiwan, in 2012 and 2013, respectively; the \\nOutstanding Research Award of Pan Wen Yuan Foundation in 2012; the \\nBest Paper Award in The 12th International Conference on ITS \\nTelecommunications, 2012; the Best Technology Transfer Contribution \\nAward from National Science Council, Taiwan, in 2012; and the \\nOutstanding Automatic Control Engineering Award from the Chinese \\nAutomatic Control Society in 2007. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nChun-Hsien Lin (S’15) was born in Taipei, \\nTaiwan. He has received his B.S. degree in Electrical and Computer \\nEngineering in 2015 and M.S. degree in Electrical and Control \\nEngineering in 2017 from National Chiao Tung University, Taiwan. \\nCurrently, he is earning his Ph.D. degree at NCTU. His research interests \\ninclude computer vision, machine learning, neural networks, and control \\nsystems. \\n \\n\\n',\n",
       " 'Addressing Bias in Machine Learning Algorithms: A Pilot Study on Emotion Recognition for Intelligent Systems Ayanna Howard1*, Cha Zhang2, Eric Horvitz2 1School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, GA∗  2Microsoft Research Redmond, WA  Abstract: Recently, there has been an explosion of cloud-based services that enable developers to include a spectrum of recognition services, such as emotion recognition, in their applications.  The recognition of emotions is a challenging problem, and research has been done on building classifiers to recognize emotion in the open world.  Often, learned emotion models are trained on data sets that may not sufficiently represent a target population of interest. For example, many of these on-line services have focused on training and testing using a majority representation of adults and thus are tuned to the dynamics of mature faces. For applications designed to serve an older or younger age demographic, using the outputs from these pre-defined models may result in lower performance rates than when using a specialized classifier. Similar challenges with biases in performance arise in other situations where datasets in these large-scale on-line services have a non-representative ratio of the desired class of interest.  We consider the challenge of providing application developers with the power to utilize pre-constructed cloud-based services in their applications while still ensuring satisfactory performance for their unique workload of cases.  We focus on biases in emotion recognition as a representative scenario to evaluate an approach to improving recognition rates when an on-line pre-trained classifier is used for recognition of a class that may have a minority representation in the training set. We discuss a hierarchical classification approach to address this challenge and show that the average recognition rate associated with the most difficult emotion for the minority class increases by 41.5% and the overall recognition rate for all classes increases by 17.3% when using this approach. I.!INTRODUCTION Poor representation of people of different ages and skin colors in training data can lead to performance problems and biases for real-world classification tasks involving the visual attributes of people—such as detecting facial expressions or pose.  These performance problems and biases are directly correlated with the problem of class-imbalance in the datasets used to train these machine learning algorithms. There have been a number of efforts that have tried to resolve this issue with training on imbalanced datasets [1], including using different forms of re-sampling [2], adjusting the decision threshold [3], or a mixture-of-experts approach which combines the results of many classifiers [4]. The difficulty with imbalance is recognizing why and when imbalanced data sets are problematic. It can be difficult to distinguish between data associated with a low incidence class and noisy data when training a classifier. This is especially problematic when building models using cloud-based services that utilize training data drawn from readily available sources, such as photos crawled from the web. Tens-of-thousands of cases might be downloaded for such training, and the resulting datasets may be dominated by high prevalence age and skin color categories. Such broad scans of data lead to three challenges for application developers, including machine learning practitioners and roboticists. The first challenge is                                                              * Research performed while Visiting Researcher at Microsoft Research that, by using a cloud-based service, application developers typically cannot directly influence its behavior since they do not have access to the services’ internal processes. The second challenge is that the categories which are derived from a representative quantity of majority data (and thus have likelihoods representative of real-world data streams) may lead to incorrect outcomes when a person in a minority age and skin color category uses the system. In this instance, the learning converges based on theoretically acceptable outcomes but its real-world outcomes may not be socially acceptable. The third challenge deals with the amplification of the problem when the data source is or is almost completely influenced by the dominant class, which may not be representative of the world culture, thus leading to the perpetuation of biased beliefs.  Although the derivations of these problems are different, these situations can bias the classification results, especially when designing learning algorithms for emotion recognition in intelligent systems. Emotion recognition is a growing area of interest, from interpreting moods for effective caregiver robots to recognizing a child’s anxiety level for therapeutic robots. Emotion recognition is the process of identifying human emotion, typically via facial expressions, linguistics, voice, or even body gestures. Most machine learning algorithms for emotion recognition use images to track facial expressions in \\norder to identify basic emotions such as Happy, Angry, Fear, or Surprise. In fact, in a review of online emotion recognition APIs [5], over 50% of the intelligent software packages available used facial expressions to recognize emotions.   Generalization of these algorithms for optimizing performance in the open world is typically achieved by training on large sets of unconstrained data collected ‘in the wild’. The term ‘in the wild’ means images have been captured under natural conditions with varying parameters such as environments and scenes, diverse illumination conditions, head poses and with occlusions. Unfortunately, most of these image sets, by the nature of their collection process, will have small collections of low incidence classes, such as those associated with a younger or older age demographic, or with an ethnic minority. For example, in the yearly Emotion Recognition in the Wild (EmotiW) challenge [6], researchers are provided a dataset to benchmark the performance of their methods on in-the-wild data. The dataset represents the typical expression classes of Angry, Disgust, Fear, Happiness, Neutral, Sadness and Surprise, but focuses primarily on scenes with adults. In this paper, we examine the bias issue found in learning algorithms for intelligent systems by focusing on the emotion recognition problem. We first present baseline outcomes for a cloud-based emotion recognition algorithm applied to images associated with a minority class, in this instance, children’s facial expressions. We then present a hierarchical approach that combines outputs from the cloud-based emotion recognition algorithm with a specialized learner, and show that this methodology can increase overall recognition results by 17.3%. We also verify that this hierarchical algorithm, when applied to an additional corpus of test data, shows similar improvements in the recognition rate. This additional corpus is used to assess the performance of the hierarchical approach in generalizing to new unseen images. We conclude by discussing future work needed to address the problem of bias stemming from poor representation of people in a large training set. II.!RELATED WORK Although there are a number of research efforts focused on children that incorporate the recognition of emotions to enable their functionality, most have not done a systematic analysis of accuracy with respect to their emotion recognition algorithms. For example, with socially-interactive robots, a number of research robots such as Darwin [7], the iCub [8], Avatar [9], and the GRACE robot [10], use emotions to engage children in therapy or learning. Their analysis though is based on overall performance on child engagement and not on emotion recognition. Of those research efforts that have focused on emotion recognition performance [11], very few have focused on children.  In [12], researchers used their own video data collection of children to develop methods to analyze changes in facial expressions of 50 children age 3 to 9, with a focus on problem solving scenarios. Their accuracy measures were not directly based on emotions but rather centered on Facial Action Units, which, when blended together, can be used to represent emotions. In their application, they developed separate linear support vector machines to detect the presence or absence of one of 19 facial actions. The training set consisted of 10000 image frames and the testing set consisted of 200 randomly sampled frames from this set, resulting in a recognition rate for the facial action units that ranged from 61% to 100% depending on the unit. In [13], an approach for learning children’s affective state was presented using three different types of neural network structures, namely a multi-stage radial basis function neural network, a probabilistic neural network, and a multi-class classification support vector machine (SVM).  Using the Dartmouth Database of Children Faces [14], they subdivided images of children age 5 to 9 into a training set of 1040 images and a testing set of 242 images. The training set was clustered into three affective classes: positive (Happy, Pleased, Surprised), negative (Disgust, Sad, Angry) and Neutral. They achieved a maximum overall recognition rate of 85% on the untrained facial test images using the multi-class classification SVM. In [15], a method was discussed for automatic recognition of facial expressions for children. They validated their methodology on the full Dartmouth Database of Children Faces of 1280 images and their 8 emotions classes. The full image set was used for both training and testing using a support vector machine, a C4.5 decision tree, random forest and the multi-layer perceptron method. The SVM achieved the maximum overall recognition rate of 79%. They then tested on the NIMH child emotional faces picture set (NIMH-ChEFS) database [16] with 482 images to assess the generalization accuracy of the classifiers on new unseen images and achieved a maximum overall recognition rate of 68.4% when using the SVM that was trained on the Dartmouth dataset. These represent several efforts that have focused on the explicit evaluation of emotion recognition algorithms as applied to the domain of children’s emotional cues. In the next section, we discuss baseline results derived from using a cloud-based classifier on publically available datasets of children’s facial expressions.  III.!BASELINE RESULTS  Recently, several cloud-based services have offered programming libraries that enable developers to include emotion recognition capabilities in their imaging applications [17]. These include, among others, Google’s Vision API (https://cloud.google.com/vision) and the Microsoft Emotion API (https://www.microsoft.com/cognitive-services), a component of Microsoft’s Cognitive Services. These cloud-based emotion recognition algorithms optimize their performance in the open world by training on large sets of unconstrained data sets collected ‘in the wild.’ To establish a baseline on the capabilities of learning and inference for a minority class, we evaluate the emotion recognition results associated with children’s facial expressions using the Microsoft Emotion API, a deep learning neural network [18]. The emotions detected using the Microsoft Emotion API are Angry, Contempt, Disgust, Fear, Happy, Neutral, Sad, and Surprise. We selected four datasets of children’s faces that are publically available for research purposes and with published  \\n   Neutral Contempt Surprise    Sad Disgust Fear    Fear Angry Happy Fig. 1.!Example stimuli of children associated with the facial expression databases: Top: The Radboud Faces Database, Middle: The Dartmouth Database of Children’s Faces, Bottom: NIMH-ChEFS Database human inter-rater reliability measures associated with the emotion labels. The four datasets were the NIMH Child Emotional Faces Picture Set (NIMH-ChEFS) [16], the Dartmouth Database of Children’s Faces [14], the Radboud Faces Database [19], and the Child Emotions Picture Set (CEPS) [20] (Figure 1). The NIMH Child Emotional Faces Picture Set (NIMH-ChEFS) contains 534 images of children ranging in age from 10 to 17 years old with a mean age of 13.6 years old.  The picture set includes 39 girls and 20 boys covering 5 emotions (Fear, Angry, Happy, Sad and Neutral). We selected an inter-rater reliability value for inclusion in our evaluation at 75% of the raters correctly identifying the intended emotion, which excluded 52 pictures from the original set leaving a final set of 482 pictures. Using inter-rater reliability for inclusion provides a way of quantifying the degree of agreement between two or more coders. We selected 75% as this is the benchmark established for having good agreement among raters when rating among 5-7 categories.  The Dartmouth Database of Children’s Faces contains 1280 images of children ranging in age from 5 to 16 years old with a mean age of 9.72 years old. The picture set includes 40 girls and 40 boys covering 7 emotions (Neutral, Happy, Sad, Angry, Fear, Surprise, and Disgust). For evaluation, we selected the inter-rater reliability cut-off value for inclusion at 75%, which excluded 370 pictures from the original set leaving a final set of 910 pictures. The Radboud Faces Database (RaFD) includes 240 images covering 8 emotions (Neutral, Angry, Sad, Fear, Disgust, Surprise, Happy, and Contempt).  There are 4 boys and 6 girls in the dataset. Based on a 75% inclusion criteria, we excluded 57 pictures from the original set leaving a final set of 183 pictures. Lastly is the Child Emotions Picture Set (CEPS), which contains 273 images of children, ranging in age from 6 to 11 years old with a mean age of 8.9 years old. The dataset includes 9 girls and 8 boys covering 7 emotions (Happy, Sad, Angry, Disgust, Fear, Surprise, and Neutral). Since we did not have access to the individual inter-rater reliability values associated with this dataset, we used the researcher’s inclusion critera, which resulted in a final set of 225 images. We seek to characterize the performance of existing machine learning algorithms on cases from these distinct data sets to develop an understanding of how well the deep learning neural network (i.e. the Microsoft Emotion API) performs on recognizing children’s emotional states. To evaluate the performance of the deep learning algorithm, we parsed each of the final image sets into the Emotion API and tabulated the recognition results. Table I shows the performance for each of the datasets. TABLE I. !DEEP LEARNING RECOGNITION RATES ACROSS THE DIFFERENT STIMULI SETS (IN %): (FE)AR, (AN)GRY, (HA)PPY, (SA)D, (NE)UTRAL, (SU)RPRISED, (DI)SGUST, (CO)NTEMPT  Fe An Di Ha Ne Sa Su Co NIMH-ChEFS  13 43  100 100 48   Dartmouth 25 35 55 100 99 64 91  Radboud 33 54 100 100 100 95 100 50 CEPS 5 50 10 95 92 52 81    With respect to the overall recognition rates, which incorporate the variable data dimensions of the image sets (Table II), the overall emotion recognition rate was 62% for the NIMH-ChEFS dataset, 81% for the Dartmouth dataset, 83% for the Radboud dataset, and 61% for the CEPS dataset. If we compare these results with specialized learning algorithms that have been trained specifically on children’s facial images and as discussed in the related work section, we see that the deep learning algorithm, based on training in the wild, has results that are comparable to the specialized results that used comparatively smaller training sets but with an emphasis on a minority class (Table III). Yet, if we look at the overall emotion recognition rates for adults, the rates should be closer to 88% [30]. Our goal therefore is to capitalize on the power of the cloud-based emotion recognition algorithm while improving the overall recognition rate.  TABLE II. !NUMBER OF IMAGES ASSOCIATED WITH EACH STIMULI SET: (FE)AR, (AN)GRY, (HA)PPY, (SA)D, (NE)UTRAL, (SU)RPRISED, (DI)SGUST, (CO)NTEMPT   Fe An Di Ha Ne Sa Su Co NIMH-ChEFS  102 94   108 98 80     Dartmouth 20 83 101 302 145 135 124   Radboud 21 26 25 30 24 20 29 8 CEPS 20 30 31 56 24 33 31     \\n\\nTABLE III. !DEEP LEARNING ‘IN THE WILD’ APPROACH VERSUS SPECIALIZED LEARNING METHODOLOGIES  Dartmouth Database  with emotions grouped into positive, negative, and neutral affective states Albu [13] 85% Emotion API 84%  Dartmouth Database NIMH-ChEFS Khan [15] 79% 68% Emotion API 81% 62% III.!METHODOLOGY AND RESULTS When we examine the results from the deep learning neural network, we note that Fear has a significantly lower recognition rate than the other emotions across all of the different datasets. If we look at the confusion matrix associated with Fear (Table IV), we also note that Fear is most often confused with Surprise. Facial expressions of Fear and Disgust have repeatedly been found in the emotion recognition literature to be less well recognized than those of other basic emotions [21]. In fact, it has been shown that Surprise is the most frequent error made when trying to recognize expressions of Fear in children [22]. If we look at the basic facial action units that make up these two expressions, it becomes obvious why this confusion occurs. Facial Action Units (AUs) represent the 44 anatomically distinct muscular activities that activate when changes occur in an individual’s facial appearance (Table V). Based on comprehensive comparative human studies, Ekman and Friesen [23, 24] have labeled these muscle movements and identified those believed to be associated with emotional expressions (Table VI) [25]. When examining the facial action units associated with Fear and Surprise (Table V), we confirm that Surprise is actually a subset of Fear (i.e. Surprise ⊆  Fear); and over 60% of the AUs in Fear are also found in the set of Surprise AUs.  As such, Surprise becomes easier to recognize than Fear as a default since there are less distinctive cues to identify. TABLE IV. !CONFUSION MATRIX ASSOCIATED WITH DEEP LEARNING RESULTS   Surprise Neutral Happy Sad Fear Fear      NIMH-ChEFS 74 (83.7%) 11 (10.2%) 0 3 13 (6.1%) Dartmouth 11    (55%) 0 3    (15%) 1      (5%) 5  (25%) Radboud 13 (61.9%) 0 0 1   (4.8%) 7 (33.3%) CEPS 9      (45%) 4     (20%) 3    (15%) 3    (15%) 1    (5%) Surprise      Dartmouth 113 (91.1%) 3    (2.4%) 8   (6.5%) 0 0 Radboud 29  (100%) 0 0 0 0 CEPS 25 (80.6%) 3    (9.7%) 3   (9.7%) 0 0 TABLE V. !FACIAL ACTION UNITS INVOLVED IN EMOTION STIMULI Emotion AUs associated with Emotion Angry 4, 5 and/or 7,  22, 23, 24 Fear 1, 2, 4, 5, 7, 20, 25 or 26 Surprise 1, 2, 5, 25 or 26  TABLE VI. !FACIAL ACTION UNITS AND FACIAL FEATURE IMAGES FROM THE CHILDREN FACIAL EXPRESSION DATASETS Action Unit Description Facial Feature Image 1 Inner Brow Raiser  2 Outer Brow Raiser  4 Brow Lowerer  5 Upper Lid Raiser  7 Lid Tightener  20 Lip Stretcher  22 Lip Funneler  23 Lip Tightener  24 Lip Pressor  25 Lips Apart  26 Jaw Drop   Thus, as a first step in illustrating a process for improving the recognition rates of a generalized machine learning algorithm, we focus on improving the overall recognition rates by improving recognition of the Fear emotion. Figure 2 depicts the overall algorithmic flow of the approach. Given a facial image, facial landmarks are extracted and used to compute a number of anthropometric features. The anthropometric features are then fed into two Support Vector Machines (SVMs) for binary classification, one to distinguish between Fear and Surprise with an explicit bias toward Fear and one to distinguish between Surprise and Not-Surprise with a balanced bias. The design of this construct is to increase the bias toward the minority class (in the first SVM) while ensuring that the recognition of the majority class is not drastically reduced (in the second SVM). We train the SVMs on 50% of the data from three of the datasets of children’s faces and evaluate the results on all four datasets, including the remaining untrained dataset.  \\n\\n Fig. 2.!Feature-based learning approach for emotion recognition of children’s facial expressions A. Extraction of Anthropometric Features Most methods that focus on developing image-based age classification methods typically use features associated with face anthropometry [26].  A face anthropometric model is based on measurements of size and proportions of the human face, i.e. human face ratios. Although age classification is not our direct target application, it does provide some measure for distinguishing between age groups (i.e. children versus adults) based on facial features. We thus utilize various human face ratios as the input into our specialized learning algorithm. In [27], it was shown that four feature distances are sufficient to represent the mathematical relationships among all of the various landmarks for age classification. Since we are interested in emotion classification, we compute all principal ratios, namely: Width of Left Eye, Height of Left Eye, Length of the Nose Bridge, Distance between the Nose Tip and Chin, Width of Mouth, Height of Open Mouth, and Offset Distance between the Inner and Outer Corner of the Eyebrow. These anthropometric features are computed based on extracted face landmarks, as shown in Figure 3, and Equations (1)-(7). \"#$%ℎ\\'()*(%+,*=|+,*)*(%/00*12−+,*)*(%45%*12|   (1) 6\\'7*81#$9*)*09%ℎ=|6\\'7*:\\'\\'%:#9ℎ%2−6\\'7*:\\'\\'%)*(%2|   (2) 6\\'7*;#<%\\'=ℎ#0=|=ℎ#0>\\'7#%#\\'0?@−?6\\'7*;#<@|   (3) A*#9ℎ%\\'()*(%+,*=|+,*)*(%8\\'%%\\'B@−+,*)*(%;\\'<@|  (4)                                                              1 !https://www.projectoxford.ai/face \"#$%ℎ\\'(C\\'5%ℎ=|C\\'5%ℎ:#9ℎ%2−C\\'5%ℎ)*(%2|  (5) A*#9ℎ%\\'(4<*0C\\'5%ℎ=|D<<*1)#<8\\'%%\\'B@−D0$*1)#<;\\'<@|  (6) +,*81\\'EA*#9ℎ%4((7*%=|+,*F1\\'E)*(%/00*1@−   ??+,*F1\\'E)*(%45%*1@|?? (7) Where x and y represent the pixel location in (x,y) screen coordinates and ChinPosition is estimated as the pixel coordinates associated with the bottom center of the Face Rectangle provided by the Face API, which indicates where in the image a face is located. Once computed, all ratios are normalized based on the calculated width and height of the Face Rectangle.                 Fig. 3.!Face Landmarks extracted using the Face API developed by Microsoft Oxford Project1 Once computed, these features are used to train two SVMs for emotion classification. B. SVMs for Classification of Fear versus Surprised Given that there is a bias for Surprise versus Fear associated with children’s facial expressions, our first task was to develop a specialized classifier that biases the results toward the minority class, in this case, the Fear class. Support Vector Machines (SVMs) are supervised learning models that can be used for classification of classes associated with a set of training examples [28]. For our application, we want to better differentiate between the Fear minority class and the Surprise majority class. Thus, any facial expression that was classified (correctly or incorrectly) as Surprised by the deep learning algorithm, we want to re-evaluate with a specialized learner. To enable this process, all emotions labeled as Surprise are fed to the first-level SVM and reclassified into one of two classes: Fear or Surprise. We thus design the first-level SVM to learn the mapping: G↦I, where J∊:L,,∊±1,0=7   (8) In this case, x represents the vector containing anthropometric features, y=1 represents the Surprise class, and y=-1 represents the Fear class.  \\n\\nFor our application, we trained the first-level SVM on 50% of the feature vectors classified as Fear or Surprise by the deep learning algorithm and extracted from the Radboud Faces Database, the Dartmouth Database of Children’s Faces and the NIMH-ChEFS Database.  We did not train on the Child Emotions Picture Set as we wished to assess the capabilities of the new algorithm when faced with unseen facial characteristics. We then biased the class decision threshold of the first-level SVM by selecting the minimum threshold value that maximized the true positive rate associated with Fear.  This, by default, increases the false positive rate of Fear while potentially reducing the true positive rate associated with Surprise. Thus, after parsing the images through the first-level SVM, the minority class has a significantly higher recognition rate but the majority class recognition rate, on average, is reduced as shown in Table VII. The higher recognition rates for the minority class are valid even for the CEPS database which contains data that was not in the training sets of of the SVM classifiers. All recognition rates incorporate the variable data dimensions of the image sets (Table II). TABLE VII. !EMOTION RECOGNITION RATES AFTER TRAINING: ML – DEEP LEARNING ALGORITHM, SVM – FIRST-LEVEL SUPPORT VECTOR MACHINE  Fear Surprise Change in Overall Rec. Rate    ML ML+ SVM ML ML+ SVM NIMH-ChEFS  13% 47%   34% Dartmouth 25% 91% 91% 79% -1.2% Radboud 33% 77% 100% 100% 31.8% CEPS 5% 70% 81% 68% 17.6%  The goal of the second-level SVM is to increase the recognition rate of the majority class to pre-bias levels while still keeping the recognition rate associated with the minority class higher than its original recognition rate. From Table V, we note that Angry and Fear have more Action Units in common than Angry and Surprise. Thus, to reduce the effect of the Action Unit overlap between Surprise and Fear, we trained the second-level SVM on the recognition of two primary classes – (Fear ∨ Angry) and Surprise. We then associated the derived anthropometric feature vector from each image to one of two classes: Surprised and Not-Surprised, where Not-Surprised represented the union of Fear and Angry. In this case, for the mapping: G↦I, where J∊:L,,∊±1,0=7, y=1 is associated with the Surprise class, and y=-1 is associated with the Fear or Angry class. In practice, only those feature vectors that were classified as Fear by the first-level SVM are processed by the second-level SVM. This approach results in an average increase in the recognition of Fear by 41.5% and an increase in the overall recognition rate by 17.3%.   As Table VIII shows, the overall recognition rate increases after parsing through the second-level SVM, even though the recognition rate for the minority class falls to a lower value than in the first-level SVM.  Of special interest, we note that, although recognition of Fear has increased greatly for the Dartmouth database, the recognition rate for Surprise is slightly lower than the original Surprise recognition rate, even after parsing through the second-level SVM. Of all the datasets, the Dartmouth dataset had a large imbalance between Surprise and Fear, in fact this dataset had 6x more images belonging to Surprise than Fear (Table II). As such, this result is not surprising as the balance that we are trying to achieve in our approach is to ensure that the minority class has an increase in benefit with respect to recognition rates, while ensuring that the reduction in benefits to the majority class is not major. If the motivation is, instead, to ensure that the recognition rate for the minority class is maximized, the only requirement is to ignore the second-level SVM and utilize only the output resulting from parsing through the first-level SVM. In the next section, we make some interesting observations about these results and provide discussion on ways to generalize this approach to the broad class of generalized learning algorithms. TABLE VIII. !EMOTION RECOGNITION RATES AFTER TRAINING: ML – DEEP LEARNING ALGORITHM, SVM – SECOND-LEVEL SUPPORT VECTOR MACHINE  Fear Surprise Change in Overall Rec. Rate    ML ML+ SVM ML ML+ SVM NIMH-ChEFS  13% 47%   34% Dartmouth 25% 70% 91% 83% -0.6% Radboud 33% 71% 100% 100% 32.8% CEPS 5% 55% 81% 81% 19.6%  IV.!DISCUSSION  We conclude this paper with a discussion on the presented results and highlight some areas for future efforts that could address the limitations associated with building classifiers when there is imbalanced representation in their training sets.  Recently, there has been an upsurge of attention given to generalized machine learning algorithms and the practices of inequality and discrimination that are potentially being built into them [29]. We know that imbalances exist and thus, our goal in this paper is to present an approach that enables us to capitalize on the power of generalized learning algorithms, while incorporating a process that allows us to tune those results for different target demographics.  Bias in machine learning algorithms will occur anytime there is a large majority class coupled with other minority classes having lower incidence rates, such as those associated with a younger or older age demographic, or an ethnic minority. The challenge is to develop a process for ensuring the overall positive results of the generalized learning approach is maintained, while also increasing the outcomes associated with any minority classes. In this paper, we address this issue by developing a hierarchical approach that couples the results from the generalized learning algorithm with results from a specialized learner. Although we focus on the issue of emotion recognition for intelligent systems, and address emotion \\nrecognition associated with children’s facial expressions, this concept can be applied to similar classification applications. The steps involved are (1) identifying the set(s) of minority classes, (2) developing specialized learners that address the minority class via special focus on the class, and (3) developing a specialized learner that combines signals from both the minority and majority class models. As shown in the results, if at any point, we determine that it is more important to have a maximum outcome rate associated with the minority class, regardless of the outcome rate associated with the majority class, only steps (1) and (2) are necessary. That question on the inclusiveness of a classifier touches on ethics of equity in the performance of  algorithms. Although the presented approach shows validity in addressing the issue of bias, there are still a number of threads that need to be investigated. Future work in this domain includes validating the approach with a focus on a different minority class, validating the approach with a focus on a different classification problem, and validating the approach with different generalized machine learning algorithms. We will also target improving the classification rate of both Fear and Disgust, since both of these expressions are hard to detect, and would provide further evidence of the impact of this methodology. We hope that this work will contribute to raising the sensitivity to the potential challenges in the performance and bias of classifiers when making inferences about people of different ages and skin colors. There are opportunities  for additional research to identify and address these challenges. V.!REFERENCES [1]!Kotsiantis, S., Kanellopoulos, D. and Pintelas, P. “Handling imbalanced datasets: A review,” GESTS International Transactions on Computer Science and Engineering, Vol. 30, pp. 25-36, 2006. [2]!Chawla, N.V., Hall, L. O., Bowyer, K. W. and Kegelmeyer, W. P., “SMOTE: Synthetic Minority Oversampling Technique,” Journal of Artificial Intelligence Research, Vol. 16, pp. 321–357, 2002. [3]!Joshi, M. V., Kumar, V. and Agarwal, R.C. “Evaluating boosting algorithms to classify rare cases: comparison and improvements,” In First IEEE International Conference on Data Mining, pp. 257-264, 2001. [4]!Provost, F. and Fawcett, T. “Robust classification for imprecise environments,” Machine Learning, Vol. 42, pp. 203-231, 2001.  [5]!Doerrfeld, B. “20+ Emotion Recognition APIs That Will Leave You Impressed, and Concerned,” http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned, 2015.  [6]!Dhall, A., Ramana Murthy O.V., Goecke, R., Joshi J. and Gedeon, T. “Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015,” ACM International Conference on Multimodal Interaction (ICMI), 2015. [7]!Brown, L. and Howard A. “Gestural Behavioral Implementation on a Humanoid Robotic Platform for Effective Social Interaction,” IEEE Int. Symp. on Robot and Human Interactive Communication (RO-MAN), pp. 471 – 476, 2014. [8]!Metta, G., Sandini, G., Vernon, D. Natale, L. and Nori, F. “The iCub humanoid robot: an open platform for research in embodied cognition,” 8th workshop on performance metrics for intelligent systems, pp. 50-56, 2008. [9]!Cloutier, P., Park, H.W., MacCalla, J. and Howard, A. “It’s All in the Eyes: Designing Facial Expressions for an Interactive Robot Therapy Coach for Children,” 8th Cambridge Workshop on Universal Access and Assistive Technology, Cambridge, UK, 2016. [10]!Simmons R., et al. “GRACE: An Autonomous Robot for the AAAI Robot Challenge,” AI Magazine, Vol. 24(2), pp. 51-72, 2003. [11]!Sankur, B., Ulukaya, S. and Çeliktutan, O. “A Comparative Study of Face Landmarking Techniques,” EURASIP J. Image and Video Processing, Vol. 13, 2013. [12]!Littleworth, G., Bartlett, M.S., Salamanca, L.P. and Reilly, J. “Automated Measurement of Children’s Facial Expressions during Problem Solving Tasks,” IEEE Int. Conference on Automatic Face and Gesture Recognition, pp. 30–35, 2011. [13]!Albu, F., Hagiescu, D., Vladutu, L. and Puica, M. “Neural network approaches for children\\'s emotion recognition in intelligent learning applications,” in Proc. of EDULEARN 2015, Barcelona, Spain, pp. 3229-3239, 2015. [14]!Dalrymple, KA, Gomez, J, and Duchaine, B. “The Dartmouth Database of Children’s Faces: Acquisition and Validation of a New Face Stimulus Set,” Urgesi C, ed.PLoS ONE. 2013. Vol. 8(11), 2013. [15]!Khan, R.A., Meyer, A. and Bouakaz, S. “Automatic Affect Analysis: From Children to Adults,” International Symposium on Visual Computing, ISVC 2015, pp. 304-313, 2015. [16]!Egger, H.L., Pine, D.S., Nelson, E., et al. “The NIMH Child Emotional Faces Picture Set (NIMH-ChEFS): A new set of children’s facial emotion stimuli,” International Journal of Methods in Psychiatric Research, Vol. 20(3), pp. 145-156, 2011. [17]!Schmidt, A. “Cloud-Based AI for Pervasive Applications,” IEEE Pervasive Computing, Vol. 15(1), pp. 14-18, 2016. [18]!Barsoum, E., Zhang, C., Canton Ferrer, C. and Zhang, Z. “Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution,” ACM International Conference on Multimodal Interaction (ICMI), Tokyo, Japan, 2016. [19]!Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., et. al. “Presentation and validation of the Radboud Faces Database,” Cognition & Emotion, Vol. 4(8), pp. 1377—1388, 2010. [20]!Romani-Sponchiado, A., Sanvicente-Vieira, B., Mottin, C., Hertzog-Fonini, D., Arteche, A. “Child Emotions Picture Set (CEPS): Development of a database of children’s emotional expressions,” Psychology & Neuroscience, Vol. 8(4), pp. 467-478, 2015. [21]!Gagnon, M., Gosselin P., Hudon-ven der Buhs, I., Larocque, K., Milliard, K. “Children’s recognition and discrimination of Fear and Disgust facial expressions,” Journal of Nonverbal Behavior, Vol. 34(1), pp. 27–42, 2010. [22]!Gosselin, P., Roberge, P. and Lavalle´e, M. C. “The development of the recognition of facial emotional expressions comprised in the human repertoire,” Enfance, Vol. 4, pp. 379–396, 1995.  [23]!Ekman, P. and Friesen, W. Facial action coding system: A technique for the measurement of facial movement. Palo Alto, Ca.: Consulting Psychologists Press, 1978. [24]!Friesen, W. and Ekman, P. EMFACS-7: Emotional Facial Action Coding System. Unpublished manual, University of California, California, 1983. [25]!Matsumoto, D. and Ekman, P. “Facial expression analysis,” Scholarpedia, Vol. 3(5), pp. 4237, 2008. [26]!Grd, P. “Two-dimensional face image classification for distinguishing children from adults based on anthropometry,” Thesis submitted to University of Zagreb, 2015. [27]!Alom M.Z., Piao, M-L., Islam M.S., Kim, N. and Park, J-H. “Optimized Facial Features-based Age Classification,” World Academy of Science. Engineering and Technology Conference, Vol. 6, pp. 319–324, 2012. [28]!Joachims, T. “Making Large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning,” B. Schölkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999.  [29]!Crawford, K. “Artificial Intelligence’s White Guy Problem,” New York Times – Opinion, http://www.nytimes.com/2016/06/26/ opinion/sunday/artificial-intelligences-white-guy-problem.html, 2016. [30]!Brodny, G., Kołakowska, A., Landowska, A., Szwoch, M., Szwoch, W. and Wróbel, M. “Comparison of selected off-the-shelf solutions for emotion recognition based on facial expressions,” 9th Int. Conf. on Human System Interactions (HSI), pp. 397-404, 2016.  \\n',\n",
       " 'Aff-Wild: Valence and Arousal ‘in-the-wild’ Challenge\\nStefanos Zafeiriou⋆,3Dimitrios Kollias⋆∗Mihalis A. Nicolaou†\\nAthanasios Papaioannou⋆Guoying Zhao3\\nIrene Kotsia1,2\\n⋆Department of Computing, Imperial College London, UK\\n†Department of Computing, Goldsmiths, University of London, UK\\n3Center for Machine Vision and Signal Analysis, University of Oulu, Finland\\n1School of Science and Technology, International Hellenic University, Greece\\n2Department of Computer Science, Middlesex University, UK\\n⋆{s.zafeiriou, dimitrios.kollias15 }@imperial.ac.uk,†m.nicolaou@gold.ac.uk\\nAbstract\\nThe Affect-in-the-Wild (Aff-Wild) Challenge proposes a\\nnew comprehensive benchmark for assessing the perfor-\\nmance of facial affect/behaviour analysis/understanding\\n‘in-the-wild’. The Aff-wild benchmark contains about 300\\nvideos (over 2,000 minutes of data) annotated with re-\\ngards to valence and arousal, all captured ‘in-the-wild’ (the\\nmain source being Youtube videos). The paper presents the\\ndatabase description, the experimental set up, the baseline\\nmethod used for the Challenge and ﬁnally the summary of\\nthe performance of the different methods submitted to the\\nAffect-in-the-Wild Challenge for V alence and Arousal esti-\\nmation. The challenge demonstrates that meticulously de-\\nsigned deep neural networks can achieve very good perfor-\\nmance when trained with in-the-wild data.\\n1. Introduction\\nBehavioral modeling and analysis constitute a crucial as-\\npect of Human Computer Interaction. Emotion recognition\\nis a key issue, dealing with multimodal patterns, such as\\nfacial expressions, head pose, hand and body gestures, lin-\\nguistic and paralinguistic acoustic cues, as well as phys-\\niological data. However, generating machines which are\\nable to recognize human emotions is a difﬁcult problem, be-\\ncause the emotion patterns are complex, time-varying, user\\nand context dependent, especially when considering uncon-\\ntrolled environments, i.e., ‘in-the-wild’.\\nCurrent research in automatic analysis of facial affect\\naims at developing systems, such as robots and virtual hu-\\nmans, that will interact with humans in a naturalistic way\\n∗The ﬁrst two authors contributed equally in the paper.under real-world settings. To this end, such systems should\\nautomatically sense and interpret facial signals relevant to\\nemotions, appraisals and intentions. Furthermore, since\\nreal-world settings entail uncontrolled conditions, where\\nsubjects operate in a diversity of contexts and environments,\\nsystems that perform automatic human behaviour analysis\\nshould be robust to video recording conditions, the diver-\\nsity of contexts and the timing of display.\\nThe past twenty years research in automatic analysis of\\nfacial behaviour was mainly limited to posed behavior cap-\\ntured in highly controlled recording conditions [ 29,35,33,\\n24]. Some representative datasets, which are still used in\\nmany recent works [ 18], include the Cohn-Kanade database\\n[33,24], the MMI database [ 29,35], the Multi-PIE database\\n[17] and the BU-3D and BU-4D databases [ 41,40]. Nev-\\nertheless, it is now accepted by the community that the\\nfacial expressions of naturalistic behaviour could be radi-\\ncally different from the posed ones [ 9,32,44]. Hence, ef-\\nforts have been made in order to collect subjects displaying\\nnaturalistic behaviour. Examples include the recently col-\\nlected EmoPain [ 4] and UNBC-McMaster [ 27] for analy-\\nsis of pain, the RU-FACS database consisting of subjects\\nparticipating in a false opinion scenario [ 5] and the SE-\\nMAINE [ 27] corpus which contains recordings of subjects\\ninteracting with a Sensitive Artiﬁcial Listener (SAL) under\\ncontrolled conditions. All the above databases have been\\ncaptured in well-controlled recording conditions and mainly\\nunder a strictly deﬁned scenario (e.g., pain estimation).\\nRepresenting human emotions has been a basic topic of\\nresearch in psychology. The most frequently used emotion\\nrepresentation is the categorical one, including the seven ba-\\nsic categories, i.e., Anger, Disgust, Fear, Happiness, Sad-\\nness, Surprise and Neutral [ 13][10]. It is, however, the di-\\nmensional emotion representation [ 39,31] which is more\\n2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n2160-7516/17 $31.00 © 2017 IEEE\\nDOI 10.1109/CVPRW.2017.2481980\\n\\nFigure 1: The 2-D Emotion Wheel\\nappropriate to represent subtle, i.e., not only extreme, emo-\\ntions appearing in everyday human computer interactions.\\nThe 2-D V alence and Arousal Space is the most usual di-\\nmensional emotion representation. Figure 1shows the 2-D\\nEmotion Wheel [ 30], with valence ranging from very pos-\\nitive to very negative and arousal ranging from very active\\nto very passive.\\nThere are various signs of a humans emotions, such as\\nfacial expressions, gestures, paralinguistic speech features\\nand physiological measurements. In the Challenge we fo-\\ncus on facial affect as measured by valence and arousal an-\\nnotations. In particular, we make a considerable effort to\\ngo beyond the current practices in facial behaviour analy-\\nsis and collect and annotate the ﬁrst large scale in-the-wild\\ndatabase of facial affect1. To achieve this, we capitalise\\non the abundance of data available in video-sharing web-\\nsites, such as Y ouTube [ 42]2, and select videos that display\\nthe affective behavior of people, for example videos that\\ndisplay the behaviour of people when watching a trailer,\\na movie, a disturbing clip or reactions to pranks etc. To\\nthis end we have collected 298 videos displaying reactions\\nof 200 subjects. To the best of our knowledge this is the\\nlargest database containing videos of facial behaviour ”in-\\nthe-wild”. For a recent survey on facial behaviour analysis\\nin-the-wild with an emphasis on deep learning methodolo-\\ngies the interested reader may refer to [ 43]. This database\\nhas been annotated by 6-8 lay experts with regards to two\\ncontinuous emotion dimensions, i.e. valence, which records\\n1Currently, there are many challenges in behaviour analysis, including\\nthe series of A VEC [ 37,36,34] challenges. Nevertheless, A VEC uses only\\ndata captured in controlled conditions and under very speciﬁc scenarios.\\nThe only challenge that uses ‘in-the-wild’ data is the series of [ 15,14,16].\\nNevertheless, the samples come from movies and the annotation is limited\\nto the universal expressions.\\n2The collection has been conducted under the scrutiny and approval of\\nImperial College Ethical Committee (ICREC). The majority of the chosen\\nvideos were under Creative Commons License (CCL). For those videos\\nthat were not under CCL, we have contacted the person who created them\\nand asked for their approval to be used in this research.how positive or negative an emotion is, and arousal which\\nmeasures the power of the activation of the emotion.\\nIn the rest of the paper, we ﬁrst describe the generated\\nAff-Wild database (Section 2), afterwards we describe the\\nannotation procedure (Section 3) and then we present the\\nresults of the challenge (Section 3). Subsequently, in Sec-\\ntion 4.1 we make a reference to all methods and respec-\\ntive papers submitted to the Challenge, summarize the ob-\\ntained results in valence and arousal estimation and declare\\nthe winning method. Finally Conclusions are presented in\\nSection 5.\\n2. The Aff-Wild Database\\nWe created a database consisting of 298 videos, with a\\ntotal length of more than 30 hours. The aim was to col-\\nlect spontaneous facial behaviors under arbitrary recording\\nconditions. To this end the videos were collected using the\\nY outube video sharing web-site. The keyword that was used\\nto retrieve the videos was ”reaction”; they display subjects\\nreacting to a variety of stimuli (e.g., tasting something hot or\\ndisgusting). Examples include subjects reacting on an un-\\nexpected plot twist of a movie or series, a trailer of a highly\\nanticipated movie, etc. The subjects display both positive\\nand negative emotions (or combinations of them). In other\\ncases, subjects display emotions while performing an activ-\\nity (e.g., riding a rolling coaster). In some videos, subjects\\nreact on a practical joke, or on positive surprises (e.g., a\\ngift). Most of the videos were in YUV 4:2:0 format, with\\nsome of them being in A VI format; all have been annotated\\nin terms of valence and arousal. Six to eight subjects have\\nannotated the videos following a methodology similar to the\\none proposed in [ 11]. That is, an on line annotation proce-\\ndure was used, according to which annotators were watch-\\ning each video and provided their annotations through a joy-\\nstick. V alence and arousal ranged continuously in [ −1,+1].\\nWe have annotated all subjects that are present in a video.\\nIn total we have 200 subjects, with 130 of them being male\\nand 70 of them female. Figures 2and 3demonstrate some\\nframes of the Aff-Wild database.\\nIn Figures 4,5we present two characteristic examples of\\nfacial images, cropped from two different videos, with their\\nrespective video frame number and the valence and arousal\\nannotation for each of them. We also present a visual repre-\\nsentation of these values on the 2-D emotion space, showing\\nthe change of the reactions/behavior of the person among\\nthese time instances of the video. Time evolution is indi-\\ncated, by using a larger size for the more recent frames and\\na smaller size for the older ones.\\nIt can be veriﬁed that annotations correspond well to the\\nfacial expression displaying in the video frames. It should,\\nhowever, be added that it is often difﬁcult to say which is the\\ntrue emotional state of the acting person from a static frame.\\nThat is why, working with many annotators and selecting\\n1981\\nFigure 2: Some representative frames from the Aff-Wild database.\\nFigure 3: Some challenging frames from the Aff-Wild database.\\nthe ones that are more consistent between them is necessary\\nto get more accurate annotation of the underlying emotion.\\nTable 1: Number of Subjects in the Aff-Wild Database\\nDatabase no of males no of females\\nTrain 106 48\\nTest 24 22\\nTable 2: Attributes of the Aff-Wild Database\\nAttribute Description\\nLength of videos 0.10-14.47 min\\nNo of annotators 6-8\\nTotal no of videos 252(train)+46(test) = 298\\nVideo format A VI , MP4\\n3. Annotation and data processing\\n3.1. Annotation tool\\nFor data annotation, we developed our own application\\nwhich was similar to others like Feeltrace [ 11] and Gtrace\\n[12]. In our application we used a setting with one time-\\ncontinuous annotation for each affective dimension, like in\\nGtrace. We did not want to judge valence and arousal at the\\nsame time, like in Feeltrace, because it would be too cogni-\\ntively demanding to reach a high quality on both. The user\\nat ﬁrst selects if (s)he wants to annotate valence or arousal.\\nThen, the interface of our application asks the user to log\\nin using an identiﬁer, his/her name, and to select an appro-\\npriate joystick. After that, the screen is split into two parts:a scrolling list of all videos is given on the left side and on\\nthe right side there is a scrolling list of all annotated videos.\\nAfter one selects a video to annotate, a screen appears that\\nshows the video and a slider of values ranging in [−1,1].\\nThen the video can be annotated by moving the joystick ei-\\nther up or down. At the same time our application samples\\nthe annotations at a variable time rate. Figure 6shows the\\ngraphical interface of our tool when annotating valence (the\\ntool for arousal is similar).\\n3.2. Annotation guidelines\\nEach annotator was instructed orally and received in-\\nstructions through a multi page document, explaining in de-\\ntail the procedure to follow for the annotation task. This\\ndocument included a short list of some well identiﬁed emo-\\ntional cues for both arousal and valence, in order to provide\\na common introduction on emotions to the annotators, even\\nthough they were rather instructed to use their own feeling\\nfor the annotation task3. Before starting the annotation of\\nthe data, each annotator watched the whole video so as to\\nknow what to expect regarding all emotions being depicted\\nin the video.\\n3.3. Data pre-processing\\nVirtualDub [ 22] was used in order to trim the raw\\nY ouTube videos, mainly at the start and the end of them,\\nso as to remove useless content (e.g., an advertisement).\\nThen another pre-processing step was applied in order to\\nlocate the faces in all frames of the videos. In more detail,\\n3All annotators were computer scientists who were working on face\\nanalysis problems and all had a working understanding of facial expres-\\nsions.\\n1982\\nFigure 4: Annotated Facial Expressions (Person A)\\nFigure 5: Annotated Facial Expressions (Person B)\\nwe extracted a total of 1,180,000 frames using the Menpo\\nsoftware [ 2]. From each frame, we detected the faces using\\nthe method described in [ 26]. We also developed a match-\\ning process between the annotation time stamps and the\\ncropped faces time instances. In particular, for each frame\\ntime instance, we searched for the nearest neighbor, in the\\nannotation time stamp sequence and then linked the latter\\nvalence and arousal annotation values to the corresponding\\nframe time stamp. In cases where we had two annotation\\ntimestamps with same time distance from a frame, we com-\\nputed the average of those two timestamps and attributed\\nthis value to that frame. Finally, we extracted facial land-\\nmarks for all frames using the best performing method in\\n[8].\\nFigure 6: The GUI of the annotation tool when annotating\\nvalence (the GUI for arousal is exactly the same).\\n3.4. Annotation Post-processing\\nWe further extended our annotation tool so that it plays a\\nspeciﬁc video and at the same time plots the correspond-\\ning valence and arousal annotated values. Every expert-\\nannotator (i.e., the annotators that are working directly on\\nthe problem of valence and arousal estimation) watched\\nagain all videos and checked if all annotations were in ac-\\ncordance with the videos, depicting well the emotion ex-\\npressed at all times at the videos. In this way, a further\\nvalidation of annotations was achieved. Using this proce-\\ndure some frames were dropped, especially at the end of\\nthe videos. After the annotations have been validated, we\\ncomputed, for every video, cross-correlations between all\\nannotators. Furthermore, we computed the correlation be-\\ntween the annotation and the tracked facial landmarks. As a\\nconsequence, we ranked the annotators’ correlation for each\\nvideo. Two more experts then watched all videos and, for\\nevery video, selected the most correlated best annotations\\n(between 2 to 4 annotations). We then computed the mean\\nof these annotations; in other words we selected the mean\\nof annotations that were mostly correlated and were given\\ngood evaluation by the experts. Figure 7shows a small part\\nof a video (2000 frames) and the 4 most highly correlated\\nannotations for valence.\\nFigure 8provides a histogram for the annotated values\\nfor valence and arousal in the generated database. As it can\\nbe observed the annotation is currently biased towards pos-\\nitive valence and arousal (something we aim at addressing\\nin future runs of the challenge).\\n4. Experiments with a Baseline\\nDue to the fact that it was the ﬁrst time that estimation\\nof valence and arousal was attempted in in-the-wild videos\\nthe standards approaches, e.g. using Support V ector Re-\\ngression (SVR) etc. on image features [ 28], resulted in\\nvery poor performance. Hence, we implemented a deep\\nneural network approach as the baseline. Currently deep\\n1983\\nFigure 7: The 4 most highly correlated valence annotations\\nover a part of a video\\nFigure 8: Histogram of Annotations\\nneural networks provide the state-of-the-art in many tasks\\nin computer vision, speech analysis and natural language\\nprocessing that require learning from massive data. They\\nhave also achieved good performances in emotion recogni-\\ntion challenges and contests [ 15]. Deep Convolutional Neu-\\nral Networks (CNNs) [ 20][21] include convolutional layers\\nwith feature maps composed of neurons with local receptive\\nﬁelds, shared weights and pooling layers, which are able to\\nautomatically extract non-linear features. The baseline ar-\\nchitecture was based on the structure of the CNN-M [ 7] net-\\nwork. We used the pre-trained on the FaceV alue dataset [ 3]\\nCNN-M network as starting structure and performed trans-\\nfer learning of its convolutional and pooling parts on our de-\\nsigned network. In particular, we used two fully connected\\nlayers, the second being the output layer providing the va-lence and arousal predictions. We either froze the CNN part\\nof the network and performed ﬁne-tuning of the weights of\\nthe fully connected layers, or performed ﬁne-tuning of the\\nweights of the whole network. The exact structure of the\\nnetwork is shown in Table 3. Note that the activation func-\\ntion in the convolutional and batch normalisation layers is\\nthe ReLu one; this is also the case in the ﬁrst fully con-\\nnected one. The activation function of the second fully con-\\nnected layer is linear. It should be also mentioned that the\\nutilized deep learning architecture has been implemented\\non the TensorFlow platform [ 1]. For the pre-trained net-\\nwork we took the one in MatConvNet [ 38] and transformed\\nit into a format recognisable by TensorFlow. Also note that\\nwe follow the TensorFlow’s platform notation for the sizes\\nof all parameters of the convolutional and pooling layers.\\nTable 3: Baseline Architecture based on CNN-M showing\\nthe sizes for the parameters of the convolutional and pooling\\nlayers and the no of hidden units in the fully connected ones\\nLayer ﬁlter ksize stride padding no of units\\nconv 1 [7, 7, 3, 96] [1, 2, 2, 1] ’V ALID’\\nbatch norm\\nmax pooling [1, 3, 3, 1] [1, 2, 2, 1] ’V ALID’\\nconv 2 [5, 5, 96, 256] [1, 2, 2, 1] ’SAME’\\nbatch norm\\nmax pooling [1, 3, 3, 1] [1, 2, 2, 1] ’SAME’\\nconv 3 [3, 3, 256, 512] [1, 1, 1, 1] ’SAME’\\nbatch norm\\nconv 4 [3, 3, 512, 512] [1, 1, 1, 1] ’SAME’\\nbatch norm\\nconv 5 [3, 3, 512, 512] [1, 1, 1, 1] ’SAME’\\nbatch norm\\nmax pooling [1, 2, 2, 1] [1, 2, 2, 1] ’SAME’\\nfc 1 4096\\nfc 2 2\\nFor training the network (in mini batches) we used\\nthe Adam optimizer algorithm. The Mean Squared Error\\n(MSE) was used as the error/cost function. The hyper-\\nparameters being used were: the batch size which was 80,\\nthe constant learning rate being 0.001 and the number of\\nhidden units in the ﬁrst fully connected layer which was\\n4096. We also used biases in the fully connected layers.\\nThe weights of the fully connected layers were initialised\\nfrom a Truncated Normal distribution with a zero mean and\\nvariance equal to 0.1 and the biases were initialised to 1.\\nTraining was performed on a single GeForce GTX TITAN\\nX GPU and the training time was about 4-5 days. No data\\naugmentation techniques were used, because the database\\nwas already large enough. For training the baseline the\\ntraining data have been split to two sets. One was used for\\ntraining the network and the other for validation.\\nTable 4summarizes the obtained MSE and Concordance\\nCorrelation Coefﬁcient (CCC) V alues over the whole gen-\\nerated database by our baseline network. It should be men-\\n1984\\ntioned that the CCC is deﬁned as\\nρc=2sxy\\ns2x+s2y+( ¯x−¯y)2(1)\\nwheresxandsyare the variances of the predicted and\\nground truth values respectively, ¯xand¯yare the correspond-\\ning mean values and sxyis the respective covariance value.\\nFrom the results we deduced that the task is very challeng-\\ning and requires meticulously designed deep learning archi-\\ntectures in order to be tackled.\\nTable 4: Concordance (CCC) and Mean Squared Error\\n(MSE) evaluation of valence & arousal predictions provided\\nby the CNN M baseline architecture\\nCCC MSE\\nV alence Arousal V alence Arousal\\nCNN M 0.15 0.10 0.13 0.14\\n4.1. The AFF-Wild Challenge\\nThe training data (i.e., videos and annotations) of AFF-\\nwild challenge were made publicly available on the 30th of\\nJanuary 2017. The test videos (without annotations) were\\nmade available around 22nd of March. The participants\\ncould submit an entry to the challenge until 2nd of April.\\nTen different research groups were initialy interested\\nin the Aff-Wild challenge. These groups downloaded the\\ndatasets and enquired about different issues of the chal-\\nlenge. Six of them made experimentation and submitted\\ntheir results to the Workshop portal. Based on the perfor-\\nmance they obtained on the test data, three of them ﬁnally\\nsubmitted a paper to the workshop. These are brieﬂy re-\\nported below, while Table 5compares the derived results\\n(in terms of CCC and MSE) by all three methods.\\nTable 5: Concordance (CCC) and Mean Squared Error\\n(MSE) of valence & arousal predictions provided by the 3\\nmethods\\nMethods CCC MSE\\nV alence Arousal V alence Arousal\\nMM-Net 0.196 0.214 0.134 0.088\\nFA TAUV A-Net 0.396 0.282 0.123 0.095\\nDRC-Net 0.042 0.291 0.161 0.094\\nIn [23] (Method MM-Net), a variation of the deep con-\\nvolutional residual neural network is ﬁrst presented for af-\\nfective level estimation of facial expressions . Then multi-\\nple memory networks are used to model temporal relations\\nbetween the video frames. Finally, ensemble models are\\nused to combine the predictions of the multiple memory\\nnetworks, showing that the latter steps improve the initially\\nobtained performance, as far as MSE is concerned, by more\\nthan 10%.In [6] (Method FA TAUV A-Net), a deep learning frame-\\nwork is presented, in which a core layer, an attribute layer,\\nan AU layer and a V-A layer are trained sequentially. The\\nfacial part-based response is ﬁrstly learnt through attribute\\nrecognition Convolutional Neural Networks, and then these\\nlayers are applied to supervise the learning of AUs. Finally,\\nAUs are employed as mid-level representations to estimate\\nthe intensity of valence and arousal.\\nIn [25] (Method DRC-Net), three neural network-based\\nmethods are presented and compared, which are based on\\nInception-ResNet modules redesigned speciﬁcally for the\\ntask of facial affect estimation. These methods are: Shallow\\nInception-ResNet, Deep Inception-ResNet, and Inception-\\nResNet with LSTMs. Facial features are extracted in differ-\\nent scales and simultaneously both the valence and arousal\\nare estimated in each frame. Best results in the experiments\\nare obtained by the Deep Inception-ResNet method.\\nAll participants applied deep learning methods to the\\nproblem. The winning method of our Challenge is\\nFA TAUV A-Net Method, since it achieved the best results\\nin the majority of the metrics used.\\n4.2. Additional Experiments\\nAs organizers we could not participate in the Aff-W chal-\\nlenge. Nevertheless, we believe that it would be interesting\\nto report results with a method that we were developing dur-\\ning the course of the challenge. This method was based on\\nan end-to-end architecture composed of CNNs and Recur-\\nrent Neural Networks (CNN-RNN) which was trained and\\ntested using the Aff-Wild benchmark. In particular, we used\\ndifferent pre-trained CNN and performed transfer learning\\nand ﬁne-tuning for designing the CNN part of the network.\\nBy including an RNN part and retraining the resulting end-\\nto-end architecture we obtained our best results for valence\\nand arousal estimation. More details about this method are\\nreported in [ 19] (Method V A-CRNN). As can be seen, we\\nhave been able to achieve better results than those achieved\\nby the participants of the challenge. These results are shown\\nin Table 6. The above results demonstrate that even though\\nthe task is very challenging an elaborate deep learning ap-\\nproach could achieve very good results (around 0.57 CCC\\nfor valence and 0.43 for arousal)\\nTable 6: Concordance (CCC) and Mean Squared Error\\n(MSE) of valence & arousal predictions provided by our\\nmethod\\nMethod CCC MSE\\nV alence Arousal V alence Arousal\\nV A-CRNN 0.57 0.43 0.08 0.06\\n1985\\n5. Conclusion\\nThe Affect-in-the-Wild (Aff-W) Challenge targets at\\nbench-marking the efforts made in the research ﬁeld of fa-\\ncial affect analysis in-the-wild. To develop the Aff-wild\\nbenchmark we collected and annotated the ﬁrst large scale\\n‘in-the- wild’ database of facial affect, consisting of more\\nthan 30 hours of videos showing reactions of about 200 per-\\nsons, both males and females. In this paper, we described\\nhow the data have been collected and annotated. We give\\nstatistics of the benchmark. Furthermore, we describe the\\nbaseline system that we developed for the challenge. The\\nperformance of the baseline system demonstrates that the\\ndata are very challenging. Hence, they require meticulously\\ndesigned deep learning approaches to be designed and im-\\nplemented for the task. The different approaches which\\nhave been submitted to this challenge show that it is possi-\\nble to develop new methods and obtain performance much\\nhigher than the baseline. Our current efforts are concen-\\ntrated to obtain and annotate more videos, as well as an-\\nnotate the previous videos with more attributes (e.g., facial\\naction units).\\n6. Acknowledgments\\nThe work of Stefanos Zafeiriou has been partially\\nfunded by the FiDiPro program of Tekes (project num-\\nber: 1849/31/2015). The work of Dimitris Kollias was\\nfunded by a Teaching Fellowship of Imperial College Lon-\\ndon. We would like also to acknowledge the contribution of\\nthe Y outube users that gave us the permission to use their\\nvideos (especially Zalzar and Eddie from The1stTake).\\nReferences\\n[1] M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia,\\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ´e,\\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P . Tucker,\\nV . V anhoucke, V . V asudevan, F. Vi ´egas, O. Vinyals, P . War-\\nden, M. Wattenberg, M. Wicke, Y . Y u, and X. Zheng. Tensor-\\nFlow: Large-scale machine learning on heterogeneous sys-\\ntems, 2015. Software available from tensorﬂow.org.\\n[2] J. Alabort-i-Medina, E. Antonakos, J. Booth, P . Snape, and\\nS. Zafeiriou. Menpo: A comprehensive platform for para-\\nmetric image alignment and visual deformable models. In\\nProceedings of the ACM International Conference on Multi-\\nmedia , MM ’14, pages 679–682, New Y ork, NY , USA, 2014.\\nACM.\\n[3] S. Albanie and A. V edaldi. Learning grimaces by watching\\ntv. In Proceedings of the British Machine Vision Conference\\n(BMVC) , 2016.\\n[4] M. S. Aung, S. Kaltwang, B. Romera-paredes, B. Martinez,\\nA. Singh, M. Cella, M. F. V alstar, H. Meng, A. Kemp, A. C.Elkins, N. Tyler, P . J. Watson, A. C. Williams, M. Pantic,\\nand N. Berthouze. The automatic detection of chronic pain-\\nrelated expression: requirements, challenges and a multi-\\nmodal dataset. IEEE Transactions on Affective Computing ,\\n2016.\\n[5] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek,\\nI. Fasel, and J. Movellan. Fully automatic facial action recog-\\nnition in spontaneous behavior. In Automatic Face and Ges-\\nture Recognition, 2006. FGR 2006. 7th International Con-\\nference on , pages 223–230. IEEE, 2006.\\n[6] W.-Y . Chang, S.-H. Hsu, and J.-H. Chien. Fatauva-net : An\\nintegrated deep learning framework for facial attribute recog-\\nnition, action unit (au) detection, and valence-arousal estima-\\ntion. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition Workshop , 2017.\\n[7] K. Chatﬁeld, K. Simonyan, A. V edaldi, and A. Zisserman.\\nReturn of the devil in the details: Delving deep into convo-\\nlutional nets. arXiv preprint arXiv:1405.3531 , 2014.\\n[8] G. G. Chrysos, E. Antonakos, P . Snape, A. Asthana, and\\nS. Zafeiriou. A comprehensive performance evaluation\\nof deformable face tracking” in-the-wild”. arXiv preprint\\narXiv:1603.06015 , 2016.\\n[9] C. Corneanu, M. Oliu, J. Cohn, and S. Escalera. Survey on\\nrgb, 3d, thermal, and multimodal approaches for facial ex-\\npression recognition: History, trends, and affect-related ap-\\nplications. IEEE transactions on pattern analysis and ma-\\nchine intelligence , 2016.\\n[10] R. Cowie and R. R. Cornelius. Describing the emotional\\nstates that are expressed in speech. Speech communication ,\\n40(1):5–32, 2003.\\n[11] R. Cowie, E. Douglas-Cowie, S. Savvidou*, E. McMahon,\\nM. Sawey, and M. Schr ¨oder. ’feeltrace’: An instrument\\nfor recording perceived emotion in real time. In ISCA tuto-\\nrial and research workshop (ITRW) on speech and emotion ,\\n2000.\\n[12] R. Cowie, G. McKeown, and E. Douglas-Cowie. Tracing\\nemotion: an overview. International Journal of Synthetic\\nEmotions (IJSE) , 3(1):1–17, 2012.\\n[13] T. Dalgleish and M. Power. Handbook of cognition and emo-\\ntion . John Wiley & Sons, 2000.\\n[14] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon. Emo-\\ntion recognition in the wild challenge 2014: Baseline, data\\nand protocol. In Proceedings of the 16th International Con-\\nference on Multimodal Interaction , pages 461–466. ACM,\\n2014.\\n[15] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon.\\nEmotion recognition in the wild challenge 2013. In Pro-\\nceedings of the 15th ACM on International conference on\\nmultimodal interaction , pages 509–516. ACM, 2013.\\n[16] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and\\nT. Gedeon. Video and image based emotion recognition chal-\\nlenges in the wild: Emotiw 2015. In Proceedings of the 2015\\nACM on International Conference on Multimodal Interac-\\ntion , pages 423–426. ACM, 2015.\\n[17] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.\\nMulti-pie. Image and Vision Computing , 28(5):807–813,\\n2010.\\n1986\\n[18] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint ﬁne-tuning\\nin deep neural networks for facial expression recognition. In\\nProceedings of the IEEE International Conference on Com-\\nputer Vision , pages 2983–2991, 2015.\\n[19] D. Kollias, M. Nicolaou, I. Kotsia, G. Zhao, and S. Zafeiriou.\\nRecognition of affect in the wild using deep neural networks.\\nInProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition Workshop , 2017.\\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in neural information processing systems , pages\\n1097–1105, 2012.\\n[21] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional\\nnetworks and applications in vision. In Circuits and Sys-\\ntems (ISCAS), Proceedings of 2010 IEEE International Sym-\\nposium on , pages 253–256. IEEE, 2010.\\n[22] A. Lee. Welcome to virtualdub. org!-virtualdub. org, 2002.\\n[23] J. Li, Y . Chen, S. Xiao, J. Zhao, S. Roy, J. Feng, S. Yan, and\\nT. Sim. Estimation of affective level in the wild with multi-\\nple memory networks. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition Workshop ,\\n2017.\\n[24] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews. The extended cohn-kanade dataset (ck+): A\\ncomplete dataset for action unit and emotion-speciﬁed ex-\\npression. In Computer Vision and Pattern Recognition Work-\\nshops (CVPRW), 2010 IEEE Computer Society Conference\\non, pages 94–101. IEEE, 2010.\\n[25] M. Mahoor and B. Hasani. Facial affect estimation in the\\nwild using deep residual and convolutional networks. In Pro-\\nceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition Workshop , 2017.\\n[26] M. Mathias, R. Benenson, M. Pedersoli, and L. V an Gool.\\nFace detection without bells and whistles. In European Con-\\nference on Computer Vision , pages 720–735. Springer, 2014.\\n[27] G. McKeown, M. V alstar, R. Cowie, M. Pantic, and\\nM. Schr ¨oder. The semaine database: Annotated multimodal\\nrecords of emotionally colored conversations between a per-\\nson and a limited agent. Affective Computing, IEEE Trans-\\nactions on , 3(1):5–17, 2012.\\n[28] M. A. Nicolaou, S. Zafeiriou, and M. Pantic. Correlated-\\nspaces regression for learning continuous emotion dimen-\\nsions. In Proceedings of the 21st ACM international con-\\nference on Multimedia , pages 773–776. ACM, 2013.\\n[29] M. Pantic, M. V alstar, R. Rademaker, and L. Maat. Web-\\nbased database for facial expression analysis. In Multimedia\\nand Expo, 2005. ICME 2005. IEEE International Conference\\non, pages 5–pp. IEEE, 2005.\\n[30] R. Plutchik. Emotion: A psychoevolutionary synthesis .\\nHarpercollins College Division, 1980.\\n[31] J. A. Russell. Evidence of convergent validity on the dimen-\\nsions of affect. Journal of personality and social psychology ,\\n36(10):1152, 1978.\\n[32] E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic anal-\\nysis of facial affect: A survey of registration, representation,\\nand recognition. Pattern Analysis and Machine Intelligence,\\nIEEE Transactions on , 37(6):1113–1133, 2015.[33] Y .-l. Tian, T. Kanade, and J. F. Cohn. Recognizing action\\nunits for facial expression analysis. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on , 23(2):97–115,\\n2001.\\n[34] M. V alstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne,\\nM. Torres Torres, S. Scherer, G. Stratou, R. Cowie, and\\nM. Pantic. Avec 2016: Depression, mood, and emotion\\nrecognition workshop and challenge. In Proceedings of the\\n6th International Workshop on Audio/Visual Emotion Chal-\\nlenge , pages 3–10. ACM, 2016.\\n[35] M. V alstar and M. Pantic. Induced disgust, happiness\\nand surprise: an addition to the mmi facial expression\\ndatabase. In Proc. 3rd Intern. Workshop on EMOTION\\n(satellite of LREC): Corpora for Research on Emotion and\\nAffect , page 65, 2010.\\n[36] M. V alstar, B. Schuller, K. Smith, T. Almaev, F. Eyben,\\nJ. Krajewski, R. Cowie, and M. Pantic. Avec 2014: 3d\\ndimensional affect and depression recognition challenge.\\nInProceedings of the 4th International Workshop on Au-\\ndio/Visual Emotion Challenge , pages 3–10. ACM, 2014.\\n[37] M. V alstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bi-\\nlakhia, S. Schnieder, R. Cowie, and M. Pantic. Avec 2013:\\nthe continuous audio/visual emotion and depression recogni-\\ntion challenge. In Proceedings of the 3rd ACM international\\nworkshop on Audio/visual emotion challenge , pages 3–10.\\nACM, 2013.\\n[38] A. V edaldi and K. Lenc. Matconvnet: Convolutional neural\\nnetworks for matlab. In Proceedings of the 23rd ACM inter-\\nnational conference on Multimedia , pages 689–692. ACM,\\n2015.\\n[39] C. Whissel. The dictionary of affect in language, emotion:\\nTheory, research and experience: vol. 4, the measurement\\nof emotions, r. Plutchik and H. Kellerman, Eds., New York:\\nAcademic , 1989.\\n[40] L. Yin, X. Chen, Y . Sun, T. Worm, and M. Reale. A high-\\nresolution 3d dynamic facial expression database. In Auto-\\nmatic Face & Gesture Recognition, 2008. FG’08. 8th IEEE\\nInternational Conference On , pages 1–6. IEEE, 2008.\\n[41] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato. A 3d fa-\\ncial expression database for facial behavior research. In Au-\\ntomatic face and gesture recognition, 2006. FGR 2006. 7th\\ninternational conference on , pages 211–216. IEEE, 2006.\\n[42] L. Y ouTube. Y outube. Retrieved , 27:2011, 2011.\\n[43] S. Zafeiriou, A. Papaioannou, I. Kotsia, M. Nicolaou, and\\nG. Zhao. Facial affect“in-the-wild. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion Workshops , pages 36–47, 2016.\\n[44] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A sur-\\nvey of affect recognition methods: Audio, visual, and spon-\\ntaneous expressions. Pattern Analysis and Machine Intelli-\\ngence, IEEE Transactions on , 31(1):39–58, 2009.\\n1987\\n',\n",
       " 'Affectiva-MIT Facial Expression Dataset (AM-FED): Naturalistic and\\nSpontaneous Facial Expressions Collected In-the-Wild\\nDaniel McDuff†‡, Rana El Kaliouby†‡, Thibaud Senechal‡, May Amr‡, Jeffrey F. Cohn§, Rosalind Picard†‡\\n‡Affectiva Inc., Waltham, MA, USA\\n†MIT Media Lab, Cambridge, MA, USA\\n§Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA\\ndjmcduff@media.mit.edu, {kaliouby,thibaud.senechal,may.amr }@affectiva.com,\\njeffcohn@cs.cmu.edu, picard@media.mit.edu\\nAbstract\\nComputer classiﬁcation of facial expressions requires\\nlarge amounts of data and this data needs to reﬂect the\\ndiversity of conditions seen in real applications. Public\\ndatasets help accelerate the progress of research by provid-\\ning researchers with a benchmark resource. We present a\\ncomprehensively labeled dataset of ecologically valid spon-\\ntaneous facial responses recorded in natural settings over\\nthe Internet. To collect the data, online viewers watched\\none of three intentionally amusing Super Bowl commercials\\nand were simultaneously ﬁlmed using their webcam. They\\nanswered three self-report questions about their experience.\\nA subset of viewers additionally gave consent for their data\\nto be shared publicly with other researchers. This subset\\nconsists of 242 facial videos (168,359 frames) recorded in\\nreal world conditions. The dataset is comprehensively la-\\nbeled for the following: 1) frame-by-frame labels for the\\npresence of 10 symmetrical F ACS action units, 4 asymmet-\\nric (unilateral) F ACS action units, 2 head movements, smile,\\ngeneral expressiveness, feature tracker fails and gender; 2)\\nthe location of 22 automatically detected landmark points;\\n3) self-report responses of familiarity with, liking of, and\\ndesire to watch again for the stimuli videos and 4) base-\\nline performance of detection algorithms on this dataset.\\nThis data is available for distribution to researchers online,\\nthe EULA can be found at: http://www.affectiva.com/facial-\\nexpression-dataset-am-fed/.\\n1. Introduction\\nThe automatic detection of naturalistic and spontaneous\\nfacial expressions has many applications, ranging from\\nmedical applications such as pain detection [ 1], or monitor-\\ning of depression [ 4] and helping individuals on the autism\\nspectrum [ 10] to commercial uses cases such as advertis-ing research and media testing [ 14] to understanding non-\\nverbal communication [ 19]. With the ubiquity of cameras\\non computers and mobile devices, there is growing interest\\nin bringing these applications to the real-world. To do so,\\nspontaneous data collected from real-world environments is\\nneeded. Public datasets truly help accelerate research in an\\narea, not just because they provide a benchmark, or a com-\\nmon language, through which researchers can communicate\\nand compare their different algorithms in an objective man-\\nner, but also because compiling such a corpus and getting\\nit reliably labeled, is tedious work - requiring a lot of effort\\nwhich many researchers may not have the resources to do.\\nThere are a number of publicly available labeled\\ndatabases for automated facial analysis, which have helped\\naccelerate research in automated facial analysis tremen-\\ndously. Databases commonly used for facial action unit\\nand expression recognition include; Cohn-Kanade (in its\\nextended edition know as CK+) [ 11], MMI [ 23], RU-\\nFACS [ 2], Genki-4K [ 24] and UNBC-McMaster Pain\\narchive [ 12]. These datasets are reviewed in Section 2.\\nHowever, all (except the Genki-4K and UNBC-McMaster\\nPain archives) were captured in controlled environments\\nwhich do not reﬂect the the type of conditions seen in real-\\nlife applications. Computer-based machine learning and\\npattern analysis depends hugely on the number of training\\nexamples [ 22]. To date much of the work automating the\\nanalysis of facial expressions and gestures has had to make\\ndo with limited datasets for training and testing. As a result\\nthis often leads to over-ﬁtting.\\nInspired by other researchers who made an effort to share\\ntheir data publicly with researchers in the ﬁeld, we present\\na database of spontaneous facial expressions that was col-\\nlected in naturalistic settings as viewers watched video con-\\ntent online. Many viewers watched from the comfort of\\ntheir homes, which meant that the facial videos contained\\na range of challenging situations, from nonuniform lighting\\n2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n978-0-7695-4990-3/13 $26.00 © 2013 IEEE\\nDOI 10.1109/CVPRW.2013.130867\\n2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n978-0-7695-4990-3/13 $26.00 © 2013 IEEE\\nDOI 10.1109/CVPRW.2013.130875\\n2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n978-0-7695-4990-3/13 $26.00 © 2013 IEEE\\nDOI 10.1109/CVPRW.2013.130875\\n2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n978-0-7695-4990-3/13 $26.00 © 2013 IEEE\\nDOI 10.1109/CVPRW.2013.130881\\n\\nand head movements, to subtle and nuanced expressions. To\\ncollect this large dataset, we leverage Internet crowdsourc-\\ning, which allows for distributed collection of data very ef-\\nﬁciently. The data presented are natural spontaneous re-\\nsponses to ecologically valid online media (video advertis-\\ning) and labels of self-reported liking, desire to watch again\\nand familiarity. The inclusion of self-reported labels is es-\\npecially important as it enables systematic research around\\nthe convergence or divergence of self-report and facial ex-\\npressions, and allows us to build models that predict behav-\\nior (e.g, watching again).\\nWhile data collection is a major undertaking in and of\\nitself, labeling that data is by far a much grander chal-\\nlenge. The Facial Action Coding System (FACS) [ 7]i st h e\\nmost comprehensive catalogue of unique facial action units\\n(AUs) that correspond to each independent motion of the\\nface. FACS enables the measurement and scoring of facial\\nactivity in an objective, reliable and quantitative way, and is\\noften used to discriminate between subtle differences in fa-\\ncial motion. One strength of FACS is the high level of detail\\ncontained within the coding scheme, this has been useful in\\nidentifying new behaviors [ 8] that might have been missed\\nif a coarser coding scheme were used.\\nTypically, two or more FACS-certiﬁed labelers code for\\nthe presence of AUs, and inter-observer agreement is com-\\nputed. There are a number of methods of evaluating the re-\\nliability of inter-observer agreement in a labeling task. As\\nthe AUs differ in how easy they are identiﬁed, it is important\\nto report agreement for each individual label [ 3]. To give a\\nmore complete perspective on the reliability of each AU la-\\nbel, we report two measures of inter-observer agreement for\\nthe dataset described in this paper.\\nThe main contribution of this paper is to present a ﬁrst\\nin the world data set of labeled data recorded over the inter-\\nnet of people naturally viewing online media, the AM-FED\\ndataset contains:\\n1.Facial Videos: 242 webcam videos recorded in real-\\nworld conditions.\\n2.Labeled Frames: 168,359 frames labeled for the pres-\\nence of 10 symmetrical FACS action units, 4 asymmet-\\nric (unilateral) FACS action units, 2 head movements,\\nsmile, expressiveness, feature tracker fails and gender.\\n3.Tracked Points: Automatically detected landmark\\npoints for 168,359 frames.\\n4.Self-report responses: Familiarity with, liking of and\\ndesire to watch again for the stimuli videos\\n5.Baseline Classiﬁcation: Baseline performance of\\nsmile, AU2 and AU4 detection algorithms on this\\ndataset and baseline classiﬁer outputs.To the authors knowledge this dataset is the largest set\\nlabeled for asymmetric facial action units AU12 and AU14.\\nIn the remainder of this paper we describe the data col-\\nlection, labeling and label reliability calculation, and the\\ntraining, testing and performance of smile, AU2 and AU4\\ndetection on this dataset.\\n2. Existing Databases\\nThe Cohn-Kanade (in its extended edition known as\\nCK+) [ 11] has been one of the mostly widely used re-\\nsource in the development of facial action unit and ex-\\npression recognition systems. The CK+ database, contains\\n593 recordings (10,708 frames) of posed and non-posed se-\\nquences, which are FACS coded as well as coded for the six\\nbasic emotions. The sequences are recorded in a lab setting\\nunder controlled conditions of light and head motion.\\nThe MMI database contains a large collection of FACS\\ncoded facial videos [ 23]. The database consists of 1395\\nmanually AU coded video sequences, 300 also have onset-\\nappex-offset annotions. A majority of these are posed and\\nall are recorded in laboratory conditions.\\nThe RU-FACS database [ 2] contains data from 100 par-\\nticipants each engaging in a 2.5 minute task. In the task,\\nthe participants had to act to hide their true position, and\\ntherefore one could argue that the RU-FACS dataset is not\\nfully spontaneous. The RU-FACS dataset is not publicly\\navailable at this time.\\nThe Genki-4K [ 24] dataset contains 4000 images la-\\nbeled as either “smiling” or “non-smiling”. These images\\nwere collected from images available on the Internet and do\\nmostly reﬂect naturalistic smiles. However, these are just\\nstatic images and not video sequences making it impossi-\\nble to use the data to train systems that use temporal infor-\\nmation. In addition, the labels are limited to presence or\\nabsence of smiles and therefore limiting their usefulness.\\nThe UNBC-McMaster Pain archive [ 12] is one of the\\nlargest databases of AU coded videos of naturalistic and\\nspontaneous facial expressions. This is labeled for 10 ac-\\ntion units and the action units are coded with levels of in-\\ntensity making it very rich. However, although of natural-\\nistic and spontaneous expressions the videos were recorded\\nwith control over the lighting, camera position, frame rate\\nand resolution.\\nMulti-PIE [ 9] is a dataset of static facial expression im-\\nages using 15 cameras in different locations and 18 ﬂashes\\nto create various lighting conditions. The dataset includes 6\\nexpressions plus neutral. The JAFFE [ 13] and Semaine [ 18]\\ndatasets contain videos with labeled facial expressions.\\nHowever, Multi-PIE, JAFFE and Semaine were collected\\nin controlled laboratory settings and are not FACS labeled,\\nbut rather have “message judgement” labels, and so are not\\nreadily available for training AU detectors.\\n868\\n876\\n876\\n882\\nO’Toole et al. [ 20] present a database including videos\\nof facial expressions shot under controlled conditions.\\n3. Data Collection\\nFigure 1shows the web-based framework that was used\\nto crowdsource the facial videos and the user experience.\\nVisitors to the website opt-in to watch short videos while\\ntheir facial expressions are being recorded and analyzed.\\nImmediately following each video, visitors get to see where\\nthey smiled and with what intensity. They can compare their\\n“smile track” to the aggregate smile track. On the client-\\nside, all that is needed is a browser with Flash support and\\na webcam. The video from the webcam is streamed in real-\\ntime at 14 frames a second at a resolution of 320x240 to\\na server where automated facial expression analysis is per-\\nformed, and the results are rendered back to the browser for\\ndisplay. There is no need to download or install anything on\\nthe client side, making it very simple for people to partici-\\npate. Furthermore, it is straightforward to easily set up and\\ncustomize “experiments” to enable new research questions\\nto be posed. For this experiment, we chose three successful\\nSuper Bowl commercials: 1. Doritos (“House sitting”, 30\\ns), 2. Google (“Parisian Love”, 53 s) and 3. V olkswagen\\n(“The Force”, 62 s). Viewers chose to view one or more of\\nthe videos.\\nOn selecting a commercial to watch, visitors are asked\\nto 1) grant access to their webcam for video recording and\\n2) to allow MIT and Affectiva to use the facial video for\\ninternal research. Further consent for the data to be shared\\nwith the research community at large is also sought, and\\nonly videos with consent to be shared publicly are shown\\nin this paper. This data collection protocol was approved by\\nthe MIT Committee On the Use of Humans as Experimental\\nSubjects (COUHES) prior to launching the site. A screen-\\nshot of the consent form is shown in Figure 2. If consent\\nis granted, the commercial is played in the browser whilst\\nsimultaneously streaming the facial video to a server. In\\naccordance with MIT COUHES, viewers could opt-out if\\nthey chose to at any point while watching the videos, in\\nwhich case their facial video is immediately deleted from\\nthe server. If a viewer watches a video to the end, then\\nhis/her facial video data is stored along with the time at\\nwhich the session was started, their IP address, the ID of\\nthe video they watched and self-reported responses (if any)\\nto the self report questions. No other data is stored. A sim-\\nilar web-based framework is described in [ 16]. Participants\\nwere aware that their webcam was being used for record-\\ning, however, at no point within the interaction were they\\nshown images from their webcam. This may have had an\\nimpact on their behavior but the majority of videos contain\\nnaturalistic and spontaneous responses.\\nWe collected a total of 6,729 facial videos from 5,268\\npeople who completed the experiment. We disregard videosMEDIAVideo of webcam\\nfootage storedVideo processed to \\ncalculate smile \\nintensity\\nREPORT3.4. 5.\\n6.\\nFlash capture of webcam \\nfootage.  Frames sent \\n(320x240, 15 fps) to \\nserver. Media  clip played \\nsimultaneously.SERVER\\nCLIENT\\nUser can answer self-report \\nquestions and receives a \\ngraphical display of the \\ncalculated smile intensityCONSENT2.\\nParticipant asked if \\nthey will allow access \\nto their webcam \\nstream.Data passed \\nback to client\\n1.\\nParticipant visits \\nsite and chooses \\nto watch a \\ncommerical.\\nFigure 1. Overview of what the user experience was like and the\\nweb-based framework that was used to crowdsource the facial\\nvideos. The video from the webcam is streamed in real-time to\\na server where automated facial expression analysis is performed,\\nand the results are rendered back to the browser for display. All\\nthe video processing was done on the server side.\\nFigure 2. The consent forms that the viewers were presented with\\nbefore watching the video and before the webcam stream began.\\nfor which the face tracker was unable to identify a face in\\nat least 90% of frames; this left 3,268 videos (20.0%). As\\nmentioned earlier, the participants were given the option to\\nmake their face video available for research purposes. For\\n489 (7.3%) of the videos this was checked. Due to the con-\\nsiderable effort required in coding 242 of these videos have\\nbeen hand labeled and are available for public release. We\\nrefer to the public portion of the data collected as the AM-\\nFED dataset. All videos were recorded with a resolution\\nof 320x240 and a frame rate of 14 fps. The data contain\\nmany challenges from an automated facial action and ges-\\nture recognition perspective. Firstly, the lighting is very\\nvaried both in terms of illumination and contrast making\\nappearance vary markedly. Secondly, there is considerable\\nrange in the pose and scale of the viewers’ faces as there\\nwere no restrictions applied to the position of the camera\\nand the viewer’s were not shown any images from their we-\\nbcam. Figure 1 (top) shows a selection of frames from the\\ndataset as examples of the diversity. The properties of the\\nlarger dataset from which the public data is taken can be\\nfound in [ 15]. This demonstrates that the data contains sig-\\nniﬁcantly more varied data, in terms of lighting and pose\\nand position of the participants, than in the CK+ and MMI\\ndatabases. The gender of subjects and whether they are\\nwearing glasses in the video are labeled in the dataset. The\\ndetails are provided in Table 1.\\n869\\n877\\n877\\n883\\nTable 1. Demographic, glasses wearing and facial hair information\\nabout videos in the dataset.\\nGender Glasses Facial hair\\nMale Female Present Present\\n140 102 86 37\\nTable 2. Deﬁnitions of the labels for the dataset and the number\\nof frames and videos in which each label was present (agreed by\\nmajority of labelers). Positive examples of each of the labels are\\nshown in Figure 5\\nLabel Deﬁnition Frames\\nPresentVideos\\nPresent\\nGender Gender of the viewer - 242\\nAU2 Outer eyebrow raise 2,587 50\\nAU4 Brow lowerer 2,274 22\\nAU5 Upper lid raiser 991 11\\nAU9 Nose wrinkler 3 1\\nAU10 Upper lip raiser 26 1\\nAU14 Symmetrical dimpler 1,161 27\\nAU15 Lip corner depressor 1 1\\nAU17 Chin raiser 1,500 30\\nAU18 Lip pucker 89 7\\nAU26 Jaw drop 476 6\\nAU57 Head is forward 253 22\\nAU58 Head is backward 336 37\\nExpressiveness Non-neutral face (may contain\\nAUs that are not labeled)68,028 208\\nSmile Smile (distinct from AU12) 37,623 180\\nTrackerfail Frames in which the track failed\\nto accurately ﬁnd the correct\\npoints on the face18,060 76\\nUnilateral left\\nAU12Left asymmetric AU12 467 6\\nUnilateral\\nright AU12Right asymmetric AU12 2,330 14\\nUnilateral left\\nAU14Left asymmetric dimpler 226 8\\nUnilateral\\nright AU14Right asymmetric dimpler 105 4\\nNegative\\nAU12AU12 and AU4 together - dis-\\ntinct from AU12 in smile62 2\\n110100100010000100000 No. of Frames /Videos  each AU is Present\\nAU02AU04AU05AU10AU14AU17AU18AU26ForwardBackwardExpressiveSmile\\nTrackerfail Uni-L-AU14Uni-L-AU12 Uni-R-AU14Uni-R-AU12negAU12Frames\\nVideos\\nFigure 3. Number of frames in which each label is present (with\\nagreement for >= 50% of labelers).\\nFigure 4. Screenshot of the video labeling tool ViDL used to label\\nthe videos in the dataset.\\n4. FACS Coding\\nEach of the videos were independently labeled, frame-\\nby-frame, by at least three FACS trained coders chosen\\nfrom a pool 16 coders (labeling stage). All 16 coders had\\nundergone FACS training and three were FACS certiﬁed.\\nThe labels were subsequently labeled by another indepen-\\ndent FACS trained individual (QA stage) and discrepancies\\nwithin the coding reviewed (relabeling stage). For label-\\ning we used a web-based, distributed video labeling system\\n(ViDL) which is speciﬁcally designed for labeling affective\\ndata [ 6]. A version of ViDL developed by Affectiva was\\nused for the labeling task. Figure 4shows a screenshot of\\nthe ViDL interface. The labelers were working indepen-\\ndently for the labeling. The coders labeled for presence (bi-\\nnary labels) of AU2, AU4, AU5, AU9, AU12 (unilateral and\\nbilateral), AU14 (unilateral and bilateral), AU15, AU17,\\nAU18 and AU26. Smiles are labeled and are distinct from\\nthe labels for AU12 as AU12 may occur in an expression\\nthat would not necessary be given the label of smile (e.g. a\\ngrimace). The expressiveness label describes the presence\\nof any non-neutral facial expression. The trackerfail label\\nindicates a frame in which the automated Nevenvision fa-\\ncial feature tracker (licensed from Google, Inc.), for which\\nthe detected points are provided with the dataset, were not\\naccurately tracking the correct locations on the face. This\\ngives a total of 168,359 FACS coded frames. Deﬁnitions\\nof the labels and the number of frames in which they were\\nlabeled present by a majority of the labelers are shown in\\nTable 2. Although AU9 and AU15 were coded for, there\\nwere only 1 or 2 examples identiﬁed by a majority of the\\ncoders. Therefore we do not evaluate the reliability of AU9\\nand AU15. In the smile and action unit classiﬁcation sec-\\ntion of this paper, we assume a label is present if over 50%\\nof the labelers agree it is present and assume that a label is\\nnot present if 100% of the labelers agree it is not present.\\nWe do not use the frames that do not satisfy these criteria\\nfor the classiﬁcation task.\\n870\\n878\\n878\\n884\\nAU2 AU4 AU5 AU9\\nAU12 (left)\\nAU12 (right) AU12 (neg.) Smile AU14 (left) AU14 (right) AU14 (sym.)AU15\\nAU17 AU18 AU26\\n AU58 AU57\\nAU10\\nFigure 5. Cropped examples of frames with positive labels for each\\nof the action units coded. Smile and negative AU12 are labeled\\nseparately instead of labeling symmetrical AU12.\\n4.1. Reliability of Labels\\nA minimum of three coders labeled each frame of the\\ndata and agreement between the coders was not necessarily\\n100%. The labels provided in the archive give the break-\\ndown of all the labelers judgements. We present the reli-\\nability of the FACS coding. The reliability for each set of\\nAU labels in a particular video, p, is the mean correlation\\nbetween all pair-wise combinations of the coders labels for\\nthat video sequence. Then the “effective reliability” is eval-\\nuated using the Spearman-Brown measure of effective reli-\\nability [ 21]. The Spearman-Brown measure of reliability is\\ncalculated as:\\nRS−B=Np\\n1+(N−1)p(1)\\nWhere Nis the number of “tests”, in this case the number\\nof coders. The effective reliability accounts for the fact that\\ntheoretically employing more that one coder will mean that\\nrandom errors within the coding begin to cancel out and\\ntherefore the effective reliability is greater than the mean\\nreliability for a particular video.\\nThe weighted-mean Spearman-Brown reliability, across\\nall 242 video sequences, for each of the labels is shown\\nin Figure 6. The weighted-mean reliability was calculated\\nby giving the reliability for each video-AU combination a\\nweighting relative to the number of agreed positive exam-\\nples in that video. As such, a video with very few positive\\nlabels that has poor reliability score is down-weighted rela-\\ntive to one that has many positive examples.\\nAs the reliability measure calculated above does not re-\\nward agreement by labelers in videos that do not contain any\\nexamples of an action unit (i.e. they all label absence of an\\naction for the whole video) we also calculated the percent-\\nage agreement across all pairs of labelers and all frames for\\neach of the labels. The mean percentage agreement across\\nall AUs was 0.98, the minimum was for AU26 = 0.87.AU02AU04 AU05 AU10AU14\\nAU17\\nAU18AU26ForwardBackward\\nExpressiveSmile\\nTrackerfail Uni-L-AU14Uni-L-AU12 Uni-R-AU14 Uni-R-AU12negAU12Mean Spearman−Brown Reliability\\n00.10.20.30.40.50.60.70.8\\nFigure 6. Bar graph showing the mean in the Spearman-Brown\\nreliability for each of the labels\\n12 34\\n5678 91011\\n121314\\n1516171819\\n202122\\nFigure 7. Locations of the 22 landmark points automatically la-\\nbeled using the Nevenvision tracker that are provided with the\\ndataset.\\n5. Fiducial Points\\nThe data is also provided with the frame-by-frame lo-\\ncations of 22 automatically detected landmark points on\\nthe face. The points were detected using the Nevenvision\\ntracker. The locations of the points are shown in Figure 7.\\nIn some cases the tracker could not identify a face. For these\\nframes the automatic labels (landmark points, smile and ac-\\ntion unit classiﬁer outputs) are assigned -1 to indicate that\\nno face was identiﬁed.\\n6. Self-report Responses\\nFollowing viewing a commercial viewers could option-\\nally answer three multiple choice questions: “Did you like\\nthe video?”, “Have you seen it before?” and “Would you\\nwatch this video again?”. A screenshot of the questions is\\nshown in Figure 8. Viewers were not required to answer\\nthe questions and the page would time-out after a time.\\nThe responses to the questions (if any) are provided with\\nthe dataset. For the publicly available data 234 people an-\\nswered the likability question, 219 people answered the fa-\\nmiliarity question and 194 people answered the desire ques-\\ntion. Some preliminary analysis of the relationship between\\nthe facial responses and self-report labels collected can be\\nfound in [ 16,17].\\n7. Experiments\\nWhilst this is not a paper focused on AU detection we\\nprovide baseline performance for automated AU detection.\\n871\\n879\\n879\\n885\\nFigure 8. The self-report questions the viewers were presented\\nwith after watching the commercial.\\nThe action units for which we present results are AU2 (outer\\neyebrow raise), AU4 (brow lowerer), and smile (labelers la-\\nbeled for presence of a “smile” rather than AU12). The out-\\nput of each of the classiﬁers is a probability estimate of the\\npresence of each action unit. The baseline results are set\\nusing the following method. The tracker was used to auto-\\nmatically detect the face and track 22 facial landmark points\\nwithin each frame of the videos. The locations of the fa-\\ncial landmarks are shown in Figure 7. The landmarks were\\nused to locate the face ROI and the segmented face images\\nwere rescaled to 120x120 pixels. An afﬁne warp was per-\\nformed on the bounded face region to account for in-planar\\nhead movement. For the smile detection the landmarks were\\nused to locate a region around the mouth and histogram of\\norientated gradients (HOG) [ 5] features are computed for\\nthe region. The classiﬁers each use a Support V ector Ma-\\nchine (SVM) with RBF kernel, these showed signiﬁcantly\\nbetter performance than random forest classiﬁers. SVMs\\nhave been shown to perform well on smile detection in the\\npast [ 24]. For the AU2 and AU4 classiﬁers the landmarks\\nwere used to locate a region around the eyes and HOG fea-\\ntures were computed for that region.\\nThe AU classiﬁers were trained and validated on exam-\\nples from other datasets collected over the web and simi-\\nlar in nature to the data available in the AM-FED dataset.\\nThe training and validation sets were independent from\\none another and were also person-independent. For testing\\nthe complete set of public frames in this dataset (168,359\\nframes) were taken and those for which there was greater\\nthan 50% agreement of the present of each action unit or\\n100% agreement of the absence of each action unit used.\\nFor training, validation and testing in the design of the\\nclassiﬁers 16,000 frames were used for the AU2 classi-\\nﬁer, 58,000 frames were used for the AU4 classiﬁer and\\n114,000 frames for the smile classiﬁer. In the validation\\nstage the classiﬁer parameters were selected by maximizing\\nthe area under the receiver operating characteristic (ROC)\\ncurve. During validation the HOG parameters and the size\\nof facial ROI were optimized. For the SVM classiﬁer the\\nspread of the RBF kernel ( γ) and the penalty parameter ( C)\\nwere optimized.\\nROC curves were calculated for each of the AU algo-\\nrithms, these are shown in Figure 10respectively. The\\ndecision-boundary was varied to calculate the ROC curves\\nshown. The area under the ROC curve for the smile, AU2\\nand AU4 classiﬁers was 0.90, 0.72 and 0.70.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91\\nFalse Positive RateTrue Positive Rate\\nSmileAU4AU2\\nFigure 10. Receiver Operating Characteristic (ROC) curves for the\\nperformance of the smile, AU2 and AU4 classiﬁers on videos from\\nthe dataset. Smile AUC = 0.90, AU2 AUC=0.72, AU4 AUC=0.70.\\nThe baseline performance shows that accurate AU detec-\\ntion is possible on this challenging, naturalistic and sponta-\\nneous data. However, this paper does not focus on the task\\nof AU detection and there remains room for improvement.\\nIn particular the detection of action units is difﬁcult in low\\nillumination conditions. Greater details of the variations in\\nconditions within the larger dataset from which the labeled\\npublic data is taken can be found in [ 15].\\n8. Distribution Details\\nParticipants provided informed consent for use of their\\nvideo images for scientiﬁc research purposes. Distribution\\nof the dataset is governed by the terms of their informed\\nconsent. The data may be used for research purposes and\\nimages from the dataset used in academic publications. All\\nof the images in the dataset may be used for research pub-\\nlications. Approval to use the data does not allow recip-\\nients to redistribute it and they must adhere to the terms\\nof conﬁdentiality restrictions. The license agreement de-\\ntails the permissible use of the data and the appropriate ci-\\ntation, it can be found at: http://www.affectiva.com/facial-\\nexpression-dataset-am-fed/. Use of the dataset for commer-\\ncial purposes is strictly prohibited.\\n9. Conclusions and Future Work\\nThe main contribution of this paper is to present a ﬁrst in\\nthe world publicly available dataset of labeled data recorded\\nover the Internet of people naturally viewing online media.\\nThe AM-FED contains, 1) 242 webcam videos recorded\\nin real-world conditions, 2) 168,359 frames labeled for the\\npresence of 10 symmetrical FACS action units, 4 asymmet-\\nric (unilateral) FACS action units, 2 head movements, smile,\\n872\\n880\\n880\\n886\\n0 10 20 30 40 50 6000.20.40.60.81Smile Probability \\nMean Hand Labels Thresholded Hand Labels Classifier Output\\n0 5 10 15 20 25 30\\nTime (s) 00.20.40.60.81AU2 Probability\\n0 1 02 03 04 05 06 000.20.40.60.81AU4 Probability\\n0 10 20 30 40 50  \\n  \\n01 02 03 0 4 0 50 \\n \\n01 0 2 0 3 04 0 50 \\nTime (s)\\nAU4AU2Smile\\nFigure 9. Example comparisons between classiﬁer predictions (green) and manually coded labels (blue and black dashed) for six videos\\nwithin the dataset. Threshold of hand labels based on >= 0.5 agreement between coders. Frames from the sequences are shown above.\\nTop) Smile classiﬁcation example, middle) AU2 classiﬁcation example, bottom) AU4 classiﬁcation example. The viewer’s distance from\\nthe camera, their pose and the lighting varies considerably between videos.\\ngeneral expressiveness, feature tracker fails and gender, 3)\\nlocations of 22 automatically detect landmark points, 4)\\nbaseline performance of detection algorithms on this dataset\\nand baseline classiﬁer outputs for smile. 5) self-report re-\\nsponses of familiarity with, liking of and desire to watch\\nagain for the stimuli videos. This represents a rich and ex-\\ntensively coded resource for researchers working in the do-\\nmains of facial expression recognition, affective computing,\\npsychology and marketing.\\nThe videos in this dataset were recorded in real-world\\nconditions. In particular, they exhibit non-uniform frame-\\nrate and non-uniform lighting. The camera position relative\\nthe viewer varies from video to video and in some cases the\\nscreen of the laptop is the only source of illumination. The\\nvideos contain viewers from a range of ages and ethnicities\\nsome with glasses and facial hair.\\nThe dataset contains a large number of frames with\\nagreed presence of facial action units and other labels. The\\nmost common are smiles, AU2, AU4 and AU17 with over\\n1,000 examples of these. The videos were coded for the\\npresence of 10 symmetrical FACS action units, 4 asymmet-\\nric (unilateral) FACS action units, 2 head movements, smile,\\ngeneral expressiveness, feature tracker fails and gender. The\\nrater reliability (calculated using the Spearman-Brown reli-ability metric) was good for a majority of the actions. How-\\never, in cases where there were only a few examples of a\\nparticular action the rate reliability metrics suffered. The\\nlabels with the greatest reliability were smile = 0.78, AU4 =\\n0.72 and expressiveness = 0.71. The labels with the lowest\\nreliability was unilateral AU14 (unilateral) and AU10. This\\nis understandable as the unilateral labels are challenging\\nespecially in frames where the lighting is non-uniform in\\nwhich case the appearance of an asymmetric expression can\\nbe ampliﬁed. AU10 is also relatively rare, only 26 frames\\nwith majority agreed presence, and these come from only\\none video sequences. Therefore small differences in coders\\nagreement might cause the reliability to be low.\\nWe calculate baseline performance for smile, AU2 and\\nAU4 detection on the dataset, the area under the ROC curves\\nwere 0.90, 0.72 and 0.70 respectively. This demonstrates\\nthat accurate facial action detection is possible but that there\\nis room for improvement as there are a number of challeng-\\ning examples. In addition, the labels provide the possibility\\nof testing many other AU classiﬁers on real-world data.\\nWe hope that the release of this dataset will encourage\\nresearchers to test new action unit detection, expression de-\\ntection and affective computing algorithms on challenging\\ndata collected “in-the-wild”. We hope that it will also serve\\n873\\n881\\n881\\n887\\nas a benchmark, enabling researchers to compare the per-\\nformance of their systems against a common dataset and\\nthat this will lead to greater performance for state-of-the-art\\nsystems in challenging conditions.\\nAcknowledgments\\nRichard Sadowsky, Oliver Wilder-Smith, Zhihong Zeng,\\nJay Turcot and Khoulood Ayman provided support with the\\ncrowdsourcing system and labeling.\\nReferences\\n[1] A. Ashraf, S. Lucey, J. F. Cohn, T. Chen, Z. Ambadar,\\nK. Prkachin, P . Solomon, and B. Theobald. The painful face:\\npain expression recognition using active appearance models.\\nInProceedings of the 9th international conference on Multi-\\nmodal interfaces , pages 9–14. ACM, 2007. 1\\n[2] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel,\\nand J. Movellan. Automatic recognition of facial actions in\\nspontaneous expressions. Journal of Multimedia , 1(6):22–\\n35, 2006. 1,2\\n[3] J. F. Cohn, Z. Ambadar, and P . Ekman. Observer-based mea-\\nsurement of facial expression with the Facial Action Coding\\nSystem . Oxford: NY , 2005. 2\\n[4] J. F. Cohn, T. Kruez, I. Matthews, Y . Yang, M. Nguyen,\\nM. Padilla, F. Zhou, and F. De la Torre. Detecting depression\\nfrom facial actions and vocal prosody. In Affective Comput-\\ning and Intelligent Interaction and Workshops, 2009. ACII\\n2009. 3rd International Conference on , pages 1–7. IEEE,\\n2009. 1\\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In Computer Vision and Pattern Recogni-\\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\\non, volume 1, pages 886–893. Ieee, 2005. 6\\n[6] M. Eckhardt and R. Picard. A more effective way to label\\naffective expressions. In Affective Computing and Intelligent\\nInteraction and Workshops, 2009. ACII 2009. 3rd Interna-\\ntional Conference on , pages 1–2. IEEE, 2009. 4\\n[7] P . Ekman and W. Friesen. Facial action coding system. 1977.\\n2\\n[8] P . Ekman and E. Rosenberg. What the face reveals: Basic\\nand applied studies of spontaneous expression using the Fa-\\ncial Action Coding System (F ACS) . Oxford University Press,\\nUSA, 1997. 2\\n[9] R. Gross, I. Matthews, J. F. Cohn, T. Kanade, and S. Baker.\\nMulti-pie. Image and Vision Computing , 28(5):807–813,\\n2010. 2\\n[10] R. Kaliouby and P . Robinson. Real-time inference of com-\\nplex mental states from facial expressions and head ges-\\ntures. Real-time vision for human-computer interaction ,\\npages 181–200, 2005. 1\\n[11] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews. The Extended Cohn-Kanade Dataset (CK+):\\nA complete dataset for action unit and emotion-speciﬁed ex-\\npression. In Computer Vision and Pattern Recognition Work-\\nshops (CVPRW), 2010 IEEE Computer Society Conference\\non, pages 94–101. IEEE, 2010. 1,2[12] P . Lucey, J. F. Cohn, K. Prkachin, P . Solomon, and\\nI. Matthews. Painful data: The unbc-mcmaster shoulder pain\\nexpression archive database. In Automatic Face & Gesture\\nRecognition and Workshops (FG 2011), 2011 IEEE Interna-\\ntional Conference on , pages 57–64. IEEE, 2011. 1,2\\n[13] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. Coding\\nfacial expressions with gabor wavelets. In Automatic Face\\nand Gesture Recognition, 1998. Proceedings. Third IEEE In-\\nternational Conference on , pages 200–205. IEEE, 1998. 2\\n[14] D. McDuff, R. El Kaliouby, K. Kassam, and R. Picard. Af-\\nfect valence inference from facial action unit spectrograms.\\nInComputer Vision and Pattern Recognition Workshops,\\n2010 IEEE Computer Society Conference on . IEEE. 1\\n[15] D. McDuff, R. El Kaliouby, and R. Picard. Crowdsourced\\ndata collection of facial responses. In Proceedings of the 13th\\ninternational conference on Multimodal Interaction . ACM,\\n2011. 3,6\\n[16] D. McDuff, R. El Kaliouby, and R. Picard. Crowdsourcing\\nfacial responses to online videos. IEEE Transactions on Af-\\nfective Computing , 3(4):456–468, 2012. 3,5\\n[17] D. McDuff, R. El Kaliouby, and R. W. Picard. Predicting on-\\nline media effectiveness based on smile responses gathered\\nover the internet. In Automatic Face & Gesture Recognition,\\n2013 IEEE International Conference on . IEEE, 2013. 5\\n[18] G. McKeown, M. V alstar, R. Cowie, M. Pantic, and\\nM. Schroder. The semaine database: annotated multimodal\\nrecords of emotionally colored conversations between a per-\\nson and a limited agent. Affective Computing, IEEE Trans-\\nactions on , 3(1):5–17, 2012. 2\\n[19] D. Messinger, M. Mahoor, S. Chow, and J. F. Cohn. Au-\\ntomated measurement of facial expression in infant–mother\\ninteraction: A pilot study. Infancy , 14(3):285–305, 2009. 1\\n[20] A. J. O’Toole, J. Harms, S. L. Snow, D. R. Hurst, M. R. Pap-\\npas, J. H. Ayyad, and H. Abdi. A video database of moving\\nfaces and people. Pattern Analysis and Machine Intelligence,\\nIEEE Transactions on , 27(5):812–816, 2005. 3\\n[21] R. Rosenthal. Conducting judgment studies: Some method-\\nological issues. The handbook of methods in nonverbal be-\\nhavior research , pages 199–234, 2005. 5\\n[22] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,\\nR. Moore, A. Kipman, and A. Blake. Real-time human pose\\nrecognition in parts from single depth images. In CVPR ,\\nvolume 2, page 3, 2011. 1\\n[23] M. V alstar and M. Pantic. Induced disgust, happiness and\\nsurprise: an addition to the mmi facial expression database.\\nInProceedings of the 3rd International Workshop on EMO-\\nTION: Corpora for Research in Emotion and Affect , page 65,\\n2010. 1,2\\n[24] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. Movel-\\nlan. Toward practical smile detection. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on , 31(11):2106–\\n2111, 2009. 1,2,6\\n874\\n882\\n882\\n888\\n',\n",
       " 'AU-aware Deep Networks for Facial Expression Recognition \\nMengyi Liu, Shaoxin Li, Shiguang Shan, and Xilin Chen, \\nAhstract- In this paper, we propose to construct a deep \\narchitecture, AU-aware Deep Networks (AUDN), for facial \\nexpression recognition by elaborately utilizing the prior knowl\\xad\\nedge that the appearance variations caused by expression can be \\ndecomposed into a batch of local facial Action Units (AUs). The \\nproposed AUDN is composed of three sequential modules: the \\nfirst module consists of two layers, i.e., a convolution layer and \\na max-pooling layer, which aim to generate an over-complete \\nrepresentation encoding all expression-specific appearance vari\\xad\\nations over all possible locations; In the second module, an \\nAU-aware receptive field layer is designed to search subsets of \\nthe over-complete representation, each of which aims at best \\nsimulating the combination of AUs; In the last module, multi\\xad\\nlayer Restricted Boltzmann Machines (RBM) are exploited to \\nlearn hierarchical features, which are then concatenated for \\nfinal expression recognition. Experiments on three expression \\ndatabases CK+, MMI and SFEW demonstrate the effectiveness \\nof AUDN in both lab-controlled and wild environments. All our \\nresults are better than or at least competitive to the best known \\nresults. \\nI. INTRODUCTION \\nIn the recent years, Facial expression recognition has \\nattracted much attention due to its potential applications, \\nsuch as human-computer interaction, multimedia, surveil\\xad\\nlance, and so on. However, the exploration of the specific \\nfacial expression features is still an open problem. Studies in \\nphysiology and psychology indicate that the most descriptive \\nregions for expression are located around certain facial \\nparts (e.g. mouth, nose, eyes, and brows) . Among them, \\nFacial Action Coding System (FACS) [1] is a typical work, \\nwhich was designed to decompose each expression into \\nseveral facial Action Units (AUs) which correspond to above\\xad\\nmentioned facial parts. Based on the definition of FACS, \\nfacial expressions can be more precisely described compared \\nto rigid categorization [2]. \\nGenerally, the previous works on expression recognition \\ncan be categorized into two classes: AU-based methods [3], \\n[4] and appearance-based methods [5], [6]. In [3], they first \\ndetected a number of pre-defined AUs, and then encoded \\ntheir combinations as specific expression according to FACS. \\nSince the definitions of AUs are ambiguous in semantics, \\nit is difficult to achieve accurate AU detection in practice. \\nOn the other hand, many appearance-based methods identify \\nan image or a sequence as one of the basic expression \\ncategories according to appearance features (e.g. local binary \\npatterns (LBP) [6], Gabor [7], SIFT [8], HOG [9]). Although \\nthese methods have obtained satisfactory performance in \\nMengyi Liu, Shaoxin Li, Shiguang Shan and Xilin Chen are with Key \\nLab of Intelligent Information Processing of Chinese Academy of Sciences \\n(CAS), Institute of Computing Technology, CAS, Beijing 100190, China, \\n{mengyi.liu, shaoxin.li, shiguang.shan, xilin.chen}@vipl.ict.ac.cn. some cases, the hand-crafted descriptors applying on a whole \\nface in their schemes are lack of semantic interpretation in \\nexpression. As different facial expressions have explicit local \\nvariations, which are corresponded to AUs, it\\'s intuitive to \\nmake use of these distinctive spatial regions. To this end \\nas well as tackling the difficult AUs detection, [10] and \\n[11] proposed feature selection schemes to automatically \\nsearch for descriptive feature dimensions of local patches \\nwhere certain AUs locates. This strategy is shown to be \\neffective. However, there exist two problems: Firstly, hand\\xad\\ncrafted features are still used in their methods, i.e. LBP, \\nand Haar, which lack in explicit semantic meaning; Sec\\xad\\nondly, selected features are directly concatenated for final \\nexpression recognition, with no further mechanism to learn \\nmore higher-level representation, which may be essential \\nfor bridging the semantic gap between AUs and certain \\nexpressions. Based on above analysis, in this paper we utilize \\nthe prior knowledge about facial AUs but propose to solve the \\nabove problems as follows: First, producing more efficient \\nrepresentation by convolving input image with a batch of \\nlearned descriptors instead of hand-crafted descriptor, which \\ncan explicitly encode expression-specific appearance, such \\nas frown, grin and glare; Second, adding a multi-layer \\nlearning process after AU-aware feature selection to extract \\nincrementally higher-level features layer-by-Iayer. As both \\nsolutions can be characterized as several network layers, we \\nformulate our framework as a multi-layer Deep Networks \\n[12] due to its hierarchical representation ability on feature \\nlearning tasks. \\nIn the deep networks framework, for the purpose of AU\\xad\\naware feature selection, we first introduce the concept of \\n\"receptive field\". As many deep learning methods [14], [15], \\n[16] have achieved high performance of object recognition \\nusing large architectures with numerous features (hidden \\nnodes), one critical concern in such large architectures \\nis to specify the connections of nodes (features) between \\nadjacent layers. Traditionally, the connections are limited \\nby restricting higher level units to receive only lower-level \\ninputs from certain subsets (e.g. based on spatial locality \\n[17], [18]), which is called \"receptive fields\". However, for \\nvarious recognition scenarios, the size and shape of receptive \\nfields may be data-dependent which make it difficult to pre\\xad\\ndefine the fields without prior knowledge. To handle this, \\n[19] proposes to select receptive fields automatically by \\ngrouping sets of most similar features during pre-training \\nof deep networks. Such unsupervised scheme focus most on \\nthe relationship among the features, rather than exploring the \\nrelevance between features and categories, which makes the \\nsingle receptive field inefficient on description and discrim-\\nFace Image Feature Maps Over Complete \\nRepresentation \\nReceptive \\nFields Logistic \\nRBM RBM Regression \\n�,A,,A, \\nHierarchical Feature Linear \\nSVM \\nClassifier \\nLinear \\nSVM \\nClassifier \\nLinear \\nSVM \\nClassifier \\n�-------� y�------- -----\\'}\\\\\\'-------,yr-____ -,I \\'---y-----l \\nOver-complete Representation AU-aware Receptive Fields Hierarchical Feature Learning \\nFig. 1. The pipeline of the proposed method. \\nination. A. Over-complete Representation \\nIn order to construct the receptive fields automatically \\nas well as utilize the co-occurrence information contained \\nin disconnected facial regions, we introduce a supervised \\nfeature selection scheme to group subsets of low-level repre\\xad\\nsentations, which could simulate the different combinations \\nof AUs. In this paper, we call such subsets \"AU-aware \\nReceptive fields (AURFs)\" and this selection procedure is \\nadded into our whole framework for effective mid-level \\nfeature learning. An overview of the proposed method is \\npresented in Fig.I. There are three sequential modules each \\nof which consists of one or more network layers. As the \\nproposed deep architecture can learn features according to \\nthe interpretation of facial AUs, we call it AU-aware Deep \\nNetworks (AUDN). Our AUDN is tested on two well known \\nlab controlled expression database CK+ [21], MMI [22], and \\na wild expression database SFEW [23]. Compared to several \\nhand-crafted appearance features, with the same linear SVM \\nclassifier [24], the learned features not only achieve state-of\\xad\\nthe-art performance but also bears intuitive physiology and \\npsychology appeal. \\nII. THE PROPOSED AUDN \\nThis section details the three modules in the proposed AU\\xad\\naware Deep Networks (AUDN). As is demonstrated in Fig.l, \\nfirstly, convolution layer and max-pooling layer are used to \\ngenerate an over-complete representation. This representation \\ncan explicitly depict specific appearance presented in specific \\nregion. Then a feature selection scheme is used to find \\nAURFs, which describe the combinations of local appearance \\nvariations. Finally, multi-layer RBM [20] is applied to each \\nAURF respectively to learn hierarchical features for facial \\nexpression recognition. As accurate AUs detection is hard to achieve in static \\nfacial images, we try to design an over-complete feature \\nrepresentation over all possible spatial regions by convolving \\nthe dense-sampling facial patches with special filters. Previ\\xad\\nously, many patch-based learning methods tend to generate \\nlocalized filters that simulate the function of simple cells \\nin the primary visual cortex [25], [26]. These works justify \\nthe use of such filters in the standard model for object \\nrecognition. In this paper, we also attempt to learn a bank \\nof specific filters, from all possible local patches from large \\nnumber of expression images. \\nAmong the plenty of algorithms for unsupervised learning, \\nK-means has enjoyed wide adoption for generating code\\xad\\nbooks of \"visual words\" [27], [28], which can be used to \\ndefine higher-level features. This method is also applied to \\nbuild layers in our framework. Suppose the patch size is u\\xad\\nby-u pixels, to obtain an over-complete representation, we \\nset K > u2 in K-means clustering and learn K centroids c( k) (k = 1, 2, ... , K) from all patches after normalizing and \\nwhitening. Then each centroid is considered as a filter to \\nconvolve with the patches in the whole facial images. For \\nan input image with t-by-t patches, each 2D grid of t-by-t \\nresponses for a single filter is generally called a \"map\". In \\nthe end we will get an t-by-t-by-K dimension representation \\nafter the convolutional layer (see Fig.2). \\nDepending on the first layer representation, it is hard \\nto learn features invariant to image transformations (e.g. \\ntranslation). This problem can be generally handled by \\nincorporating \"pooling\" layers. In [16], it has been found \\nthat max-pooling can lead to faster convergence, superior \\ninvariant features selection, and improve generalization. Here \\nthe features go through our max-pooling layers are given by \\nthe maximum activation over adjacent, disjoint spatial blocks \\non each filter map. \\nFig. 2. Examples of filters and corresponding maps in convolutional layer. \\nB. AU-aware Receptive Fields (AURFs) \\nIn this module, we focus on selecting groups of AU\\xad\\naware receptive fields from the outputs of max-pooling layer. \\nAs each feature in the outputs represents the presence of \\na single pattern which simulates specific AU, the selected \\nreceptive fields can depict complex combinations of appear\\xad\\nance variations, which is expected to be consistent with \\nthe interpretation of FACS. To explore the expression-driven \\nAU-aware features, we attempt to apply a greedy maximal \\nrelevance feature selection: iteratively selecting features with \\nthe highest relevance to the target expression category. How\\xad\\never, in such greedy searching scheme, the combinations \\nof individually best features do not necessarily lead to best \\nclassification performance [29]. For example, the features of \\nadjacent regions that contained similar appearance is likely \\nto have similar label-relevance score, but the combination of \\nthem can offer no more descriptive information to classifi\\xad\\ncation. On this consideration, some researchers proposed to \\nselect features with the minimal redundancy as an auxiliary \\ncondition [30], [31], [32], and one of these, based on criteria \\nof minimal-redundancy-maximal-relevance (mRMR) [33], is \\na made-to-order scheme and proved to be effective in our \\nexperiments. \\nIn our approach, relevance is characterized in terms of \\nmutual information, which is widely used to measure depen\\xad\\ndency of variables. Given two random variables x and y, \\ntheir mutual information is defined in accordance with their \\nprobabilistic density functions p(x), p(y), and p(x, y): \\n( ) II p(x,y) I x; y = p(x, y) log p(x)p(y) dxdy. (1) \\nMax-relevance is to find a feature set S with m features {Xi}, \\nwhich has the largest mean relevance to expression labels c. \\nThe scheme can be formularized as: \\n1 max D(S, c), D = 1ST L I(Xi; c). \\nxiES (2) \\nAs the features only satisfying (2) may have unexpected \\nredundancy, the following min-redundancy condition can be Fig. 3. Selected patches in AURF when m = 1,2,8,16. \\nadded to select relatively more diverse features: \\nmin R(S), R = 1;12 L I(Xi;Xj). (3) \\nXi,XjES \\nCombining criterion presented in expression (2) and (3), \\nan greedy search methods is used to find the local-optimal \\nfeatures defined by max(D -R). Suppose we already have \\nSk-l, the set contained k-l features. Our goal is to find the \\nkth feature from the rest P = X -Sk-l. The incremental \\nalgorithm optimizes the following condition: \\n1 max[I(xj; c) --k-\" I(Xj; Xi)]\\' (4) \\nxjEP - 1 L...J \\nxiESk-1 \\nOur overall AU-aware receptive fields selection algorithm \\nis shown in Algorithm. 1. And examples of local patches \\ncorresponding to selected features are shown in Fig.3. \\nAlgorithm 1 : AU-aware Receptive Fields Selection \\nInput \\nOver-complete representation X = {Xi Ii = 1,2, ... , M}; \\nExpression labels of over-complete representation C; \\nSelected feature dimension in each receptive field m; \\nNumber of AU-aware receptive fields N; \\nOutput \\nReceptive fields RF(n) = {Xni Ii = 1,2, ... , m}; \\nAlgorithm \\n1: Initialize selected feature set S = ¢, the set of features \\nto be selected P = X -S; \\n2: for n = 1,2, ... , N do \\n3: Set RF(n) = ¢; \\n4: Select feature Xs from P which has maximal \\nmutual information with c: \\nargmax I(xs; c) \\nxsEP \\n5: Update RF(n) = RF(n) U{xs}, S = SU{xs}, \\nP=X -S; \\n6: for k = 2, ... , m do \\n7: Select feature Xs from P based on equation (4): \\n8: Update RF(n) = RF(n) U{xs}, S = SU{xs}, \\nP=X-S; \\n9: end for \\n10: end for \\nC. Hierarchical Feature Learning \\nIn this module, we attempt to learn even higher-level \\nexpression features from each AU-aware receptive field. \\nRecent neuroscience findings have provided insight into \\nthe principles governing information representation in the \\nmammalian brain, which motivated the emergence of varies \\ndeep machine learning methods [12], [15], [18] focusing on \\ncomputational models for information representation. One \\nof the approaches in [20] applies a restricted Boltzmann \\nmachine (RBM) to model each new layer of higher-level \\nfeatures, which guarantees an increase on the lower-bound \\nof the log likelihood of the data. The Single RBM is a two\\xad\\nlayer (i.e. visible layer and hidden layer), undirected graphic \\nmodel without lateral connections. The nodes in visible layer \\nand hidden layer are represented as Vi, hi respectively. If the \\nvisible units are real values, the configuration of Vi, hi is \\ncharacterized by an energy function as follows: \\nE(v h) = � \"v2_ \"v·W .. h·-\"b·h·-\" c·v· (5) , 2 �\" �\" \"J J � J J �\"\"\\' i i,j j i \\nwhere Wij characterizes the association between visible \\nand hidden nodes and Ci, bj are the biases of visible layer and \\nhidden layer respectively. Probabilistically, this is interpreted \\nas \\nP(v,h) = exp(-�(v,h)), Z = Lexp(-E(v, h)). (6) \\nv,h \\nThe hidden nodes are conditionally independent given the \\nvisible layer nodes, and vice versa. The parameters of RBM \\ncan be optimized by performing stochastic gradient ascent on \\nmaximizing the log-likelihood of training data. As computing \\nthe exact gradient of log-likelihood is intractable, Contrastive \\nDivergence (CD) approximation is used [34] which works \\nfairly well in practice. The three-layers module in this section \\nis formed by stacking RBMs as this way: First an RBM is \\ntrained on the receptive field layer. Then, after the training \\nfirst layer RBM, the weights are frozen and hidden layer \\nact as the input of next layer, and so forth. in the end, a \\nsupervised fine tuning step is performed to adjust the weights \\nfor improvement on particular task. \\nIII. DEEP NETWORK DETAILS \\nThis section details the network structure and parameters \\nused in our experiments. As preprocessing, all the faces \\nin images are detected automatically by Viola-Jones face \\ndetector [35], and then normalized to 32x32 based on the \\nlocation of eyes. \\nFor the first layer, we sample 6-by-6 pixel patches with \\na stride of 1 pixel on the 32-by-32 pixel raw images. Thus \\neach image contains 27-by-27 small patches and K (here \\nwe set K = 100) filters can be learned from all these \\npatches in training set. Then each 32-by-32 image obtains a \\n27-by-27-by-K representation after convolution. To achieve \\nsome extent translation invariance, we apply max-pooling \\nover adjacent, disjoint 3-by-3 patches on each map (an image \\nresponses for a single filter). In the end this yields 9-by-9-\\nby-IOO features as an over-complete representation for each \\nexpression image. After extracting the representation in the first module, we \\napply mRMR for AU-aware receptive fields selection. For \\nthe parameters referred in Algorithm. 1 , we set the feature \\ndimension in each AURF as m = 500, and the number of \\nAURFs N in each dataset depends on the features relevance \\nto class labels. Our experiments show that CK+ contains \\nless noise than the other two database (e.g. non-uniform \\nexpression, wearing accessories, lighting, and so on), so the \\nrelevance of features are much higher than that in MMI or \\nSFEW. In practice, based on the mean relevance of each \\nreceptive field, we set N = 9 in CK + and N = 3 in both \\nMMI and SFEW. \\nIn the last module, we apply two additional RBM layers \\nto learn higher-level features in each AURF. The input \\nlayer\\'s node number equals to the dimension of each AURF \\n(m = 500). The node number of hidden layers are 400 and \\n300 respectively. After unsupervised pre-training, the two \\nlayers RBM can be further supervisedly fine-tuned using \\nlogistic regression. At last, for each AURF, the features in \\nthe first visible layer and each hidden layer are concatenated \\nto construct final hierarchical feature for facial expression \\nrecognition using linear SVM classifier. \\nIV. EXPERIMENTS \\nIn this section, we evaluate the learned hierarchical fea\\xad\\ntures for facial expression recognition. All comparisons are \\nperformed on three datasets: CK+ [21], MMI [22], and a wild \\nexpression database SFEW [23]. The experimental results \\nof our method achieve or outperform the state-of-the-art \\nperformance. \\nA. Experiments on CK + \\nThe CK+ database consists of 593 sequences from 123 \\nsubjects, which is an extended version of Cohn-Kanade \\n(CK) [36] database. The validating emotion labels were only \\nassigned to 327 sequences which were found to meet criteria \\nfor one of 7 discrete emotion (Anger (An): 45, Contempt \\n(Co): 18, Disgust (Di): 59, Fear (Fe): 25, Happiness (Ha): \\n69, Sadness (Sa): 28, and Surprise (Su): 83) based on FACS. \\nIn our experiments, we make use of all these sequences from \\n118 subjects. For each sequence, the first image (neutral face) \\nand three peak frames were used for prototypic expression \\nrecognition which is similar to the settings in [6], [11]. Based \\non the subject ID given in the dataset, we construct 10 \\nperson independent subsets by sampling in ID ascending \\norder with step size equals 10 and adopt lO-fold cross\\xad\\nvalidation. What\\'s more, to avoid parameter sensitivity, only \\nlinear SVM classifier is used in all of our experiments. For \\ncomparison, Table.! lists the recognition accuracies of several \\nmethods, including hand-crafted-feature-based ones and the \\nstate-of-the-art performance. \\nIn the table, \"OR\" represents the method based on fea\\xad\\ntures of Over-complete Representation; \"AURF\" represents \\nthe method based on selected AU -aware Receptive Fields \\n(AURFs); and \"AUDN\" represents the method based on the \\nfinal features achieved by our deep networks. The CSPL in \\nTABLE I TABLE II TABLE III \\nEXPRESSION RECOG NITION ACCURACY ON EXPRESSION RECOG NITION ACCURACY ON EXPRESSION RECOG NITION ACCURACY ON \\nCK+ DATABASE. MMI DATABASE. SPEW DATABASE. \\nMethods Accuracy Methods Accuracy Methods Accuracy \\nLBP \\nSIFf \\nHOG \\nGabor 83.87%(linear) 81.89%(RBF) \\n86.39%(linear) 87.31 %(RBF) \\n89.53%(linear) 88.61 %(RBF) \\n88.61 %(linear) 85.09%(RBF) LBP \\nSIFf \\nHOG \\nGabor 52.93%(linear) 50.37%(RBF) \\n57.80%(linear) 61.46%(RBF) \\n63. 17%(linear) 65.24%(RBF) \\n56.10%(linear) 57.56%(RBF) LBP \\nSIFT \\nHOG \\nGabor 21.29%(linear) 23.71 %(RBF) \\n20.45%(linear) 21.14%(RBF) \\n19.52%(linear) 22.71 %(RBF) \\n19.29%(linear) 19. 14%(RBF) \\nCSPL [11] \\nOR \\nAURF \\nAVON 89.89%(unknown) \\n91.44%(linear) \\n92.22%(linear) \\n92.05%(linear) CSPL [11] \\nOR \\nAURF \\nAVON 73.53%( unknown) \\n68.41 %(linear) \\n69.88%(linear) \\n74.76%(linear) Baseline [23] \\nOR \\nAVRF \\nAVON 19.00%(RBF) \\n24.98%(linear) \\n23.00%(linear) \\n26.14%(linear) \\nTABLE IV TABLE V TABLE VI \\nTHE CONFUSION MATRIX OF AUDN ON \\nCK+ DATABASE. THE CONFUSION MATRIX OF AUDN ON \\nMMI DATABASE. THE CONFUSION MATRIX OF AUDN ON \\nSPEW DATABASE. \\nNe \\nAn \\nCo \\nDi 4.52 \\nFe 9.33 \\nHa 0 \\nSa 20.24 2.38 0 Di Fe Ha Sa Su \\n0.61 0.61 0 0.61 0.61 \\no 0.74 0 \\n1.85 5.56 0 \\no \\n4 \\nSu 1.2 0 1.2 0 An \\nNe \\nAn \\nDi \\nFe 10.71 \\nHa 4.76 \\nSa 20.83 15.63 3.13 \\nSu 15 0 \\n[10] gets the accuracy of 89.89%, which represents state-of\\xad\\nthe-art performance in CK + database. It should be pointed \\nout that CSPL only handled six expression categories from \\n96 subjects, while our experiment considers 8 categories \\n(including Contempt and Neutral (denoted as \"Ne\")), which \\nis much more challenging and practical compared to many \\nexisting methods. \\nWe also compared our \"learned feature\" with the hand\\xad\\ncrafted ones. The experiment settings (the same on Table.l, \\nTable.II and Table.III) are: Image size is 32x32 pixels as \\nsame as before. LBP (944 dimensions): 16 patches with size \\nof 8x8 pixels and 59 dimensions uniformed feature on each \\npatch; SIFT (1152 dimensions): 9 lattice points with 128 \\ndimensions feature on each; HOG (1568 dimensions): 49 \\noverlapped blocks with size of 8x8 pixels, 4 cells and 8 \\nhistogram bins for each block. Gabor (24576 dimensions): \\nconvolutional images with 3 spatial scales and 8 orientations. \\nWe performed both linear SVM and RBF SVM on all the \\nfeatures. The grid search for parameter estimation for RBF \\nis performed over c = 2k, k = 0, 1, ... , 9; g = 2l, l = \\n-5, -4, ... , O. Some results may have gaps with the existing \\nones [6], [7], the two main reasons are: (1) The protocols \\nare different. we performed strict person independent test by \\nlO-nonoverlapping-fold cross-validation. (2) The image size \\nis 32x32 pixels, which is much smaller than other methods. \\nFor classification details, we also show the confusion \\nmatrix of AUDN in Table.lV. We can see that the recognition \\nrates of Contempt and Sadness are not as promising as \\nthat of , for example, Happiness and Surprise. This may \\nbe caused by the unevenly distribution of data, i.e. some \\nof the categories have much fewer samples. The probability \\nof a sample assigned to such minor categories could be \\noverwhelmed by those of major categories. Fe Ha Sa Su An Di Fe Ha Ne Sa Su \\n1.95 0.98 5.85 0.98 An 24.11 8.93 15.18 10.71 16.07 16.96 8.04 \\n0 15.05 0 Di 16.47 14.12 9.41 15.29 18.82 16.47 9.41 \\n6.25 8.33 0 Fe .23 5.05 20.20 19.19 9.09 10.10 13.13 \\nHa 15.79 3.51 \\nNe 22.00 8.00 \\nSa 10.10 8.08 \\n7.5 Su 9.89 9.89 \\nB. Experiments on MMI \\nThe MMI database [22] includes 30 subjects of both sexes \\nand ages from 19 to 62. In the datasets, 213 sequences \\nhave been labeled with six basic expressions, in which 205 \\nsequences are captured frontal view. We use the data from \\nall these 205 sequences as in [11]. Similar to the settings on \\nCK+, the neutral face and three peak frames in each sequence \\nhave been used and lO-fold cross-validation is conducted \\nin the same way. We still perform our experiments on all \\nseven expression categories including neutral, which is more \\ndifficult compared to six categories problem in [11]. What\\'s \\nmore, in both CK+ and MMI experiments, the images we \\nused are 32x32 pixels which are much smaller than 96x96 \\nin [11], nevertheless, better results are achieved using the \\nproposed method. Table.II demonstrate the comparisons of \\nseveral methods and the confusion matrix of AUDN are \\nshown in Table. V. \\nCompared to the results on CK+, the recognition perfor\\xad\\nmance degrade significantly on MMI database due to its \\nchallenging conditions: The subjects posed expressions non\\xad\\nuniformly, and many of them wear accessories (e.g. glasses, \\nmoustache). Thus the traditional hand-crafted features cannot \\noffer discriminative information as such noises exist. How\\xad\\never, our scheme apply the data-dependent dictionary rather \\nthan fixed descriptors, which is more robust dealing with the \\ncomplex conditions. It shows that gradually better results \\nhave been achieved during our learning process. \\nC. Experiments on SFEW \\nFor further validation, we apply our method to a much \\nmore challenging scenario: the expression in the wild. The \\nStatic Facial Expression in the Wild (SFEW) database \\nwhich has been extracted from movies is different from \\nthe available facial expression datasets generated in highly \\ncontrolled lab environments. It is the first attempt to build \\ndatabase depicting real-world or simulated real-world con\\xad\\nditions for expression analysis [23]. According to Strictly \\nPerson Independent (SPI) Protocol for SPEW, the database \\nis divided into two sets. Each set contains seven subfolders \\ncorresponding to the seven expression categories. The sets \\nwere created in strict person independent manner that there \\nis no overlap between training and testing set. In total, there \\nare 700 images (346 in Set!, 354 in Set2) and 95 subjects. \\nThe experiment is set to be two-fold (Fold1: train on Set! \\nand test on Set2; Fold2: train on Set2 and test on Set!) and \\nwe take the average accuracy of two folds for measurement. \\nAs shown in Table.III and Table.VI, our method outper\\xad\\nforms the baseline 19% [23] significantly. However, due \\nto the tough imaging conditions, the faces normalized by \\nautomatically detected eyes location suffer from severe mis\\xad\\nalignment, so that none of the algorithms can work well as \\non CK + or MMI. \\nV. CONCLUSIONS AND FUTURE WORKS \\nIn this paper, we propose to construct a deep architec\\xad\\nture especially for facial expression recognition, which is \\ncalled \"AUDN\". Inspired by the interpretation of FACS, we \\napplying an mRMR feature selection on an over complete \\nrepresentation, the obtained AURFs are shown to be able \\nto simulate specific AUs. Additional multi-layer RBMs can \\nextract higher-level features in each AURF and further fine \\ntuned by logistic regression. Linear SVM classifier is ap\\xad\\nplied to hierarchical feature and final result is obtained by \\naveraging recognition results of each AURF. The proposed \\nAUDN achieve or outperform state-of-the-art performance on \\nthree facial expression database covering both lab control and \\nwild scenario. In future, we will try to search more complex \\ncombination of feature rather than simple non-overlapping \\nAURF to further boost the performance of our method. \\nVI. ACKNOWLEDGMENTS \\nThis paper is partially supported by Natural Science \\nFoundation of China under contracts No. 61025010 and No. \\n61222211. \\nREFERENCES \\n[1] P. Ekman and W. V. Friesen. Facial action coding system. Consulting \\nPsychologists Press,I,1978. \\n[2] C. E. Izard. The face of emotion. New York: Appleton-Century\\xad\\nCrofts, 1971. \\n[3] Y. Tian, T. Kanade, and J. F. Cohn. Recognizing upper face action \\nunits for facial expression analysis. CVPR, 2000. \\n[4] Y. Tong, W. Liao, and Q. n. Facial action unit recognition by \\nexploiting their dynamic and semantic relationships. TPAMI, 29(10): \\n1683-1699, 2007. \\n[5] M. Lyons, J. Budynek, and S. Akamatsu. Automatic classification of \\nsingle facial images. TPAMI, 21(12): 1357-1362, 1999. \\n[6] C. Shan, S.Gong, and P. W. McOwan. Facial expression recognition \\nbased on local binary patterns: A comprehensive study. Image and \\nVision Computing, 27: 803-816, 2009. \\n[7] G. Littlewort, M. S. Bartlett, 1. Fasel, J. Susskind, and J. Movellan. \\nDynamics of facial expression extracted automatically from video. \\nImage and Vision Computing, 2006. [8] U. Tariq, K. Lin, Z. Li, X. Zhou, Z. Wang, V. Le, T. S. Huang, X. \\nLv, and T. X. Han. Emotion recognition from an ensemble of features. \\nFG,20 11. \\n[9] Z. Zeng, L. Yin, X. Wei, X. Zhou, and T. S. Huang. Multi-view facial \\nexpression recognition. FG, 2008. \\n[10] P. Yang, Q. Liu, and D. N. Metaxas. Exploring facial expression with \\ncompositional features. CVPR, 2010. \\n[11] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas. \\nLearning active facial patches for expression analysis. CVPR, 2012. \\n[12] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality \\nof data with neural networks. Science, 313(5786): 504-507, 2006. \\n[13] A. Rao, and N. Thiagarajan. Recognizing facial expressions from \\nvideos using Deep Belief Networks. Technical Report, 2009. \\n[14] D. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidbu\\xad\\nber. High-performance neutral networks for visual object classification. \\nPre-print, 2011. http://arxiv.orglabs/ll02.0183. \\n[15] A. Coates and A. Y. Ng. The importance of encoding versus training \\nwith sparse coding and vector quantization. ICML, 2011. \\n[16] D. Scherer, A. Miler, and S. Behnke. Evaluation of pooling operations \\nin convolutional architectures for object recognition. ICANN, 2010. \\n[17] Y. LeCun, F. Huang, and L. Bottou. Learning methods for generic \\nobject recognition with invarience to pose and lighting. CVPR, 2004. \\n[18] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep \\nbelief networks for scalable unsupervised learning of hierarchical \\nrepresentations. ICML, 2009. \\n[19] A. Coates and A. Y. Ng. Selecting receptive fields in deep networks. \\nNIPS, 2011. \\n[20] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm \\nfor Deep Belief Nets. Neural Computation, 18(7): 1527-1554, 2006. \\n[21] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar. The \\nExtended Cohn-Kanade dataset (CK+): A complete dataset for action \\nunit and emotion-specified expression. CVPRW, 2010. \\n[22] M. F. Valstar and M. Pantic. Induced disgust, happiness and surprise: \\nan addition to the MMI facial expression database. LREC, 2010. \\n[23] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static facial expression \\nanalysis in tough conditions: Data, evaluation protocol and benchmark. \\nICCVW, 2011. \\n[24] R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, and \\nC. J. Lin. LIBLINEAR: A Library for Large Linear Classifi\\xad\\ncation, JMLR, 9(2008): 1871-1874, 2008. Software available at \\nhttp://www.csie.ntu.edu.tw/cj linlliblinear. \\n[25] T. Serre, L. Wolf, and T. Poggio. Object recognition with features \\ninspired by visual cortex. CVPR, 2007. \\n[26] J. Mutch and D. G. Lowe. Object class recognition and localization \\nusing sparse features with limited receptive fields. IJCV, 56(6): 503-\\n511, 2008. \\n[27] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial \\npyramid matching for recognizing natural scene categories. CVPR, \\n2006. \\n[28] J. Winn, A. Criminisi, and T. Minka. Object categorization by learned \\nuniversal visual dictionary. ICCV, 2005. \\n[29] A. K. Jain, R. P. W. Duin, and J. Mao. Statistcial pattern recognition: \\nA review. TPAMI, 22(1): 4-37, 2000. \\n[30] P. Pudil, J. Novovicova, and J. Kittler. Floating search methods in \\nfeature selection. PRL, 15(11): 1119-1125, 1994. \\n[31] J. Jaeger, R. Sengupta, and W. L. Ruzzo. Improved gene selection for \\nclassification of microarrays. PSB, 2003. \\n[32] C. Ding and H. Pengo Minimum redundancy feature selection from \\nmicro array gene expression data. CSB, 2003. \\n[33] H. Peng, F. Long, and C. Ding. Feature selection based on mutual \\ninformation: criteria of max-dependency, max-relevance, and min\\xad\\nredundancy. TPAMI, 27(8): 1226-1237,2005. \\n[34] G. E. Hinton. Training products of experts by minimizing contrastive \\ndivergence. Neural Computation, 14: 1771-1800,2002. \\n[35] P. Viola and M. Jones. Robust real-time object detection. IJCV, 2001. \\n[36] T. Kanade, J. Cohn, and Y. Tian. Comprehensive database for facial \\nexpression analysis. FG, 2000. \\n',\n",
       " 'Automatic Facial Expression Recognition Based on a \\nDeep Convolutional-Neural-Network Structure  \\nKe Shan, Junqi Guo*, Wenwan You, Di Lu, Rongfang Bie \\nCollege of Information Science and Technology \\nBeijing Normal University \\nBeijing, P.R.China \\nguojunqi@bnu.edu.cn \\nAbstract —Facial expression recognition, which many \\nresearchers have put much effort in, is an important portion of affective computing and artificial intelligence. However, human \\nfacial expressions change so subtly that recognition accuracy of \\nmost traditional approaches largely depend on feature extraction. Meanwhile, deep learning is a hot research topic in the field of machine learning recently, which intends to simulate the \\norganizational structure of human brain’s nerve and combine low-\\nlevel features to form a more abstract level. In this paper, we employ a deep convolutional neural network (CNN) to devise a facial expression recognition system, which is capable to discover deeper feature representation of facial expression to achieve \\nautomatic recognition. The proposed system is composed of the \\nInput Module, the Pre-processing Module, the Recognition Module and the Output Module. We introduce both the Japanese Female Facial Expression Database(JAFFE) and the Extended \\nCohn-Kanade Dataset(CK+) to simulate and evaluate the \\nrecognition performance under the influence of different factors (network structure, learning rate and pre-processing). We also introduce a K-nearest neighbor (KNN) algorithm compared with \\nCNN to make the results more convincing. The accuracy \\nperformance of the proposed system reaches 76.7442% and 80.303% in the JAFFE and CK+, respectively, which demonstrates feasibility and effectiveness of our system. \\nKeywords—Facial Expression Recognition; Deep Learning; \\nConvolutional Neural Network \\nI. I NTRODUCTION\\n Early in the 1990s, Picard predicted that Affective \\nComputing would be an important direction for future artificial \\nintelligence research [1]. In 1971, the American psychologist \\nEkman and Friesen defined seven categories of basic facial expression, which are Happy, Sad, Angry, Fear, Surprise, Disgust and Neutral [2]. In 1991, A.Pentland and K.Mase held the first attempt to use optical flow method to determine the \\ndirection of movement of facial muscles. Then, they extracted \\nthe feature vectors to achieve four kinds of automatic expression recognition including Happy, Angry, Disgust, Surprise and got nearly 80% accuracy [3]. \\n In 2006, Hinton and Salakhutdinov published an article in \\n\"Science\" [5], opening the door to a deep learning era. Hinton suggested that the neural network with multiple hidden layers had good ability for learning characteristics. It can improve the accuracy of prediction and classification by obtaining different degrees of abstract representation of the original data. So far, the \\ndeep learning algorithm has achieved good performance in speech recognition, collaborative filtering, handwriting recognition, computer vision and many other fields [4]. \\n The concept of Convolutional Neural Network (CNN) was \\npresented by Yann LeCun et al. in [7] in the 1980s, where a neural network architecture was composed of two kinds of basic layers, respectively called convolutional layers (C layers) and subsampling layers (S layers). However, many years after that, \\nthere was still not a major breakthrough of CNN. One of the \\nmain reasons was that CNN could not get ideal results on large size images. But it was changed when Hinton and his students used a deeper Convolutional Neural Network to reach the optimal results in the world on ImageNet in 2012. Since then, \\nmore attention has been paid on CNN based image recognition. \\nIn this paper, we present a method to achieve facial \\nexpression recognition based on a deep CNN. Firstly we implement face detection by using Haar-like features and histogram equalization. Then we construct a four-layer CNN architecture, including two convolutional layers and two subsampling layers (C-S-C-S). Finally, a Softmax classifier is \\nused for multi-classification. \\n The structure of the paper is as follows: Section 2 introduces \\nthe whole system based on CNN, including the input module, the image pre-processing module, the recognition algorithm \\nmodule and the output module. In Section 3, we simulate and evaluate the recognition performance of the proposed system \\nunder the influence of different factors such as network structure, learning rate and pre-processing. Finally, a conclusion is drawn..  \\nII. F\\nACIAL EXPRESSION RECOGNITION SYSTEM BASED ON CNN\\nA. System Overview\\nThis section starts with the overall introduction of CNN-\\nbased facial expression recognition system. System flow is showed in Fig. 1. \\n* The corresponding author \\n978-1-5090-5756-6/17/$31.00 ©2017 IEEE\\nSERA 2017, June 7-9, 2017, London, UK123\\nWe employ the Extended Cohn-Kanade Dataset (CK+) [8] \\nand  the Japanese Female Facial Expression Database (JAFFE) [9] for the simulation, which are both standard facial expression database categorized for 7 kinds of expressions. First of all, the \\nInput Module obtains the input image 2D data. The Pre-\\nprocessing Module includes 2 steps: face detection and histogram equalization. Thus we can get the main part of the human face and minimize the difference of lighting conditions in backgrounds. Recognition Module is based on convolutional neural network (CNN) algorithms and multiple classifiers Softmax. The Output Module shows MSE convergence figure and calculate the recognition accuracy. If the recognition \\naccuracy does not meet the requirement, re-adjust the network \\nparameters and begin a new round of training until the accuracy is satisfying. Details of each module are described as follow. \\nB. Image Pre-processing \\nWe employ two standard facial expression databases for the \\nsimulation, which are both widely acknowledged by academia. JAFFE contains 213 images of 10 Japanese women, while CK+ covers the expression images of all races of people and has 328 pictures totally. Before the recognition, some pre-processing \\nwork need to be done firstly. In our image pre-processing \\nprocedure, we run a two-step process to reduce the interference in the original images, which are Face Detection and Histogram Equalization. \\n1) Face detection based on Haar-like feature The first step of image pre-processing is face detection. In \\nthe face detection part, detection results are based on the Haar-like feature in OpenCV, which is one of the most classic features for face detection. It was originally proposed by Papageorgiou et al. [10] [11] and also known as the rectangular feature. Haar-like feature templates are divided into three categories, namely edge features, linear features and center surround features. On this basis, Haar-like feature templates Viola and Jones [12] used are shown in Fig. 2. \\nA feature template is composed of black area and white area. \\nAfter placing it on a certain part of the image, we can get the feature value by the subtraction between all the pixels added within the white rectangle coverage and that within the black rectangle coverage. Accordingly, the goal of these black and white rectangles is to quantify facial features to get the distributing information of the face and finally to distinguish the non-face portion and the face portion. \\n A demonstration of our detection results is showed in Fig. 3. \\nWe can see that the Haar-like feature is effective in catching the useful portion of facial expression and removing most of the meaningless background information. Therefore, it can reduce the amount of data we need to deal with, as well as effectively \\navoid the interference of different backgrounds and other objects \\nin the picture on the recognition results. \\n2) Histogram equalization \\nAfter acquiring the very face portion of the image, other \\ntroublesome issues should also be considered. Due to the different lighting conditions when taking pictures, the portions of human face will also show in different brightness, which will Face \\nDetection\\nHistogram \\nEqualizationCK+ Input Module\\nPre-processing\\nModule\\nRecognition\\nModuleconvolutional \\nlayer (C1)\\nsubsampling \\nlayer(S1)\\nconvolutional \\nlayer(C2)\\nsubsampling \\nlayer(S2)\\nSoftmax \\nmultiple \\nclassifierRasterizeAdjust \\nParameters\\nOutput ModuleRecognition \\nAccuracyMSE \\nconvergence \\nFIG.Accuracy is \\nsatisfying?End YesNoJAFFE\\n \\nFig. 1.  System Flow Diagram.  \\nFig. 2.  Demonstration of Haar-like feature templates. \\n \\nOriginal Images \\n \\nAfter Detection and Extraction \\nFig. 3.  Face detection based on Haar-like feature. \\n124\\ninevitably cause large interference on recognition results. Thus, \\nwe decide to conduct histogram equalization(HE) before recognition. Histogram equalization is a simple but effective algorithm in image processing, which can make the gray values distribution in different images more uniform and reduce interference caused by different lighting conditions. \\nAs is showed in Fig. 4, distributions of gray value in different \\npicture of the same expression are very inconsistent before equalization, which causes large interference to recognition algorithm. After histogram equalization, gray value of each image uniformly covers the entire range of gradation, image contrast is improved and gray distribution of different pictures is more unified.  \\nFig. 5 shows more clearly that brighter face portion is \\noptimized by histogram equalization. Thus the important features are better presented and all the images are unified as \\npossible. We can conclude that histogram equalization is \\neffective in reducing interference caused by different lighting \\nconditions. The following experiments also prove it.  \\nC. Structure of CNN-based Recognition Algorithm \\nConvolutional Neural Networks (CNN) is composed of two \\nbasic layers, respectively called convolutional layer (C layer) \\nand subsampling layer (S layer). Different from general deep \\nlearning models, CNN can directly accept 2D images as the input data, so that it has unique advantage in the field of image recognition. \\nA classic CNN model is showed as Fig. 6. 2D images are \\ndirectly inputted into the network, and then convoluted with several adjustable convolutional kernels to generate corresponding feature maps to form layer C1. Feature maps in \\nlayer C1 will be subsampled to reduce their size and form layer S1. Normally, the pooling size is 2×2. This procedure also repeats in layer C2 and layer S2. After extracting enough features, the two-dimensional pixels are rasterized into 1D data and inputted to the traditional neural network classifier. In practical applications, we generally use Softmax as the final multiple classifier. \\nEntering a convolutional layer, the feature map of upper \\nlayer is divided into lots of local areas and convoluted respectively with trainable kernels. After the convolutions are processed by activation function, we will get new output feature \\nmaps. Let the \\nl-th layer is a convolutional layer, the j-th \\noutput in this layer can be expressed as: \\n 1(* )\\njll l l\\nj ii j j\\niMX fX K b−\\n∈=+¦   (1) \\nWherein, jM presents the local area connected by the j-th \\nkernel, l\\nijK is a parameter of convolutional kernel, l\\njb is bias, \\n()f• is the Sigmoid function. \\nIn subsampling layer, the most commonly used method is \\nmean pooling in a 2×2 area. That is, average 4 points in the area \\nas a new pixel value. \\nParameter estimation in CNN still uses the gradient \\nalgorithm of back propagation. However, according to the characteristics of CNN, we should make some modifications in several particular steps. \\nSuggest that the residual error vector spread to the raster \\nlayer is \\nrd. Specifically, it can be written as: \\n 111 112[, , ]T\\nrj m ndd d d= …,   (2) \\nBecause the rasterization is a 2D-to-1D transfer, residual \\nerror vector reverse pass to the subsampling layer is simply needed to re-organized from 1D to 2D matrix. \\nWhen reversing pass from S layer to C layer, different \\npooling method is corresponding to different process of residual error back propagation. In mean pooling, we just average the residual error in current point to 4 points of upper layer. Suggest  \\nFig. 4.  Histograms contrast before and after histogram equalization. \\n \\nFig. 5.  Contrast between the face portion before and after HE. \\n \\nFig. 6.  Structure of CNN. \\n125\\nthat a residual error in a point of S layer isqΔ. After up-\\nsampling, error transferred to C layer can be presented as: \\n ()p q upsample Δ= Δ   (3) \\nThere are trainable parameters in C layer. Therefore, C layer \\nhas two tasks in back propagation: reverse residual error and update its parameters. According to the BP algorithm and \\nconsider the convolution operation, we can get the formula that \\nupdate parameter \\npθ in convolutional layer p is: \\n \\'180(( )* 180( ))q\\npErot X rot pθ∂=Δ∂¦   (4) \\n where rot180 refers to a 180-degree rotation to a matrix. \\nIf a feature map q\\' in the former S layer is connected to a set \\nC in convolutional layer p, the residual error spread to q\\' is: \\n \\'\\'( * 180( ))qp p q\\npCrot X θ\\n∈Δ= Δ¦   (5) \\nAfter all the parameters are updated, the network completes \\na round of training. This process should be carried out for all training samples until the whole network meet the training requirements. \\n In this paper, we employ a C-S-C-S constructed CNN. The \\nsize of convolutional kernel in C layer is 5 × 5, and the initial values are random numbers between -1 and 1. S layer do the mean pooling within the 2 × 2 area of each feature map. The logical relationship of the recognition algorithm between the various parts and functions is shown in Fig.7. The algorithm consists of several basic parts, respectively \\nachieve the function of data input, parameter initialization, network training and testing. \\nFirst of all, the network infrastructure is completely \\nconfigured and parameters of layers are initialized by the initialization module. When initialization is finished, input the training data and training labels into the network. After the training, testing data together with testing labels are inputted into Testing Module. By comparing output judging results of testing data with the testing labels, we can finally get the recognition accuracy. \\nIII. P\\nERFORMANCE EVALUATION  \\nA. Performance Evaluation vs. CNN Structures \\nDifferences of network structures could cause great impact \\non the recognition performance. Generally, we need to rely on experience and continuous testing to get the best network structure for a particular classification task. For feasibility, we fix the four-layer structure as C-S-C-S and make the number of \\nfeature maps of every convolutional layer changeable. In order \\nto better control the variables, the following results is in the case that learning rate Ș (0 <Ș\\x941) equals 0.5. \\nTABLE I  Recognition accuracy of different CNN structures in JAFFE (%)  \\n 10 12 14 \\n4 13.9535 65.1163 69.7674 \\n6 13.9535 76.7442 69.7674 \\n8 58.1395 58.1395 67.4419 \\nAs it can be seen from Table ĉ, although more feature maps \\ncan extract more types of expression features theoretically, too \\nmany features will cause unnecessary interference and decrease the recognition ability of the network. Thus, proper structure of convolution layer is important for obtaining good recognition results. By the experiments, JAFFE get the highest accuracy when C1 has 6 and C2 has 12 feature maps. \\nTABLE II  Recognition accuracy of different CNN structures in CK+ (%) \\n 10 12 14 \\n4 21.2121 77.2727 77.2727 \\n6 77.2727 78.7879 77.7879 \\n8 71.2121 72.7273 21.2121 \\nSimilarly, in CK +, we can get the best results when C1 has \\n6 and C2 has 12 or 14 feature maps showed in the Table Ċ. \\nBased on the two databases, we finally selected 6-12 structure \\nfor network training. All of the recognition results showed below are based on this structure. \\nB. Performance Evaluation vs. Learning Rates \\nLearning rate Ș is the measure to variation of parameter \\nupdating, thus the value is controlled between 0 and 1(0 < Ș\\x941). \\nIf Ș is too large, the variation of network parameters on every \\nupdating will be too sharp, which will affect the stability of the updated parameters. Even worse, it can eventually lead to non-C1 C2 C1 C2 \\nConvolutional\\nkernel and bias in \\nC layerPooling \\nmethod in \\nS layerParameters \\nin classic NN\\nForward \\npropagationCalculate\\nerror & back\\npropagationGradient \\nMethod to \\nmodify the \\nweightsParameter \\nInitialization\\nTraining\\nReach the number \\nof iterations?No\\nYes\\nTesting\\nRecognition \\nAccuracy\\nTrain_data\\nTrain_label\\nTest_data\\nTest_labelInput\\nTest_data\\nOutput \\njudging \\nresultsTest_labelInput\\n \\nFig. 7.  Schematic of recognition algorithm.   \\n126\\nconvergence error with the increase of training times. \\nMeanwhile, if Ș is too small, the process of convergence will \\ntake a long time and consume too much calculating resources. Therefore, the value of Ș needs to be selected appropriately \\naccording to the actual training environment.  \\nFor better recognition performance, we run the tests on \\ndifferent values of Ș. We selected five discrete values between \\n0-1 and get the recognition results in both JAFFE and CK+, \\nwhich are respectively displayed in the Table ċ and the Table \\nČ as below. \\nTABLE III  Recognition accuracy on different learning rates in JAFFE (%)  \\nȘ Accuracy \\n0.1 69.7674 \\n0.3 72.093 \\n0.5 76.7442 \\n0.7 74.4186 \\n0.9 72.093 \\nTABLE IV  Recognition accuracy on different learning rates in CK+ (%) \\nȘ Accuracy \\n0.1 75.7576 \\n0.3 75.7576 \\n0.5 78.7879 \\n0.7 80.303  \\n0.9 77.2727 \\nThe best recognition result is obtained when Ș is 0.5 in \\nJAFFE, and 0.7 in CK+. Higher or lower learning rate will decrease the recognition performance of CNN. \\nIn order to further analyze the impact of different learning \\nrates on the recognition performance, we draw the partial magnification of MSE (Mean Square Error) convergence curve \\nof CK+ as an example. \\nAs we can see more intuitively from Fig. 8, when Ș is too \\nlarge (Ș=0.9,0.7), the sharp change of weights results in the \\noscillation of MSE at the beginning of training. As Ș decreases, \\nthe error convergence tends to be stable. The smaller Ș leads to \\nthe slower MSE convergence. \\nC. Performance Evaluation vs. Image Pre-processing  \\nTo reflect the effect of pre-processing, we discuss the \\nrecognition performance before and after the histogram \\nequalization (HE). The result in JAFFE is showed as Table č. \\nTABLE V  Recognition accuracy before and after HE in JAFFE (%) \\n 0.1 0.3 0.5 0.7 0.9 \\nAfter 69.7674 72.093 76.7442 74.4186 72.093 \\nBefore 69.7674 65.1163 67.4419 51.1628 58.1395 \\nThe same evaluation in CK+ is displayed in Table Ď as well: \\nTABLE VI  Recognition accuracy before and after HE in CK+ (%) \\n 0.1 0.3 0.5 0.7 0.9 \\nAfter 75.7576 75.7576 78.7879  80.303 77.2727 \\nBefore 71.2121 72.7272 75.7576 77.2727 75.7576 \\nThe results show that the recognition accuracy without \\nhistogram equalization is reduced due to brightness interference, compared to the accuracy after equalization on every value of learning rate. It proves that histogram equalization does indeed improve recognition performance of the network. \\nFor deeper analysis, we draw the MSE convergence curve in \\nthe case of training process with and without histogram equalization. As a representative, Fig.9 shows the MSE convergence curve before and after histogram equalization in \\nCK+, where Ș = 0.7. HE Ș Ș\\nHE\\n \\nFig. 9.  MSE convergence curve before and after HE in CK+.  \\n \\nFig. 8.  Partial magnification of MSE convergence curve in CK+. \\n127\\nObviously, MSE converges more slowly when training the \\nnetwork with original images, which means that there is a lot of interference before the histogram is equalized. Much computational resources and training times are consumed to correct the interference caused by brightness differences. By the appropriate pre-process like histogram equalization, we can render the MSE converge faster and finally get the better training results. \\nD. Performance Comparison \\nFor a more comprehensive display of CNN’s performance \\non facial expression recognition task, we introduce a traditional classification method KNN (K-Nearest Neighbor) to make a comparison.  \\nFor equality, the pre-processing of images for KNN is the \\nsame as CNN’s. We get the recognition results in different K values as below. \\nTABLE VII   Recognition accuracy of KNN on different K values in JAFFE (%)  \\nk Accuracy \\n3 58.1395 \\n5 62.7907 \\n7 65.1163  \\n9 62.7907 \\n11 48.8372  \\n13 53.4884 \\nTABLE VIII   Recognition accuracy of KNN on different K values in CK+ (%) \\nk Accuracy \\n10 74.2424 \\n15 77.2727  \\n20 74.2424 \\n25 72.7273 \\n30 72.7273  \\n35 71.2121 \\nThus, we can compare the two best recognition results of \\nCNN and KNN, which is shown in Table đ. \\nTABLE IX  The comparison of best recognition accuracy of CNN and KNN (%) \\n CNN KNN \\nJAFFE 76.7442 65.1163 \\nCK+ 80.303 77.2727 As is shown, CNN’s performance is significantly better than \\nKNN’s in the task of facial expression recognition. KNN is simply based on the spatial location of known data to judge the test data, while CNN can learn deeper features of data and get more reliable recognition results. In a conclusion, the CNN is obviously more suitable for facial expression recognition. \\nIV. C\\nONCLUSIONS  \\nIn this paper, we have proposed a system based on a CNN \\nalgorithm to achieve human facial expression recognition. The whole system is composed of Input Module, Pre-processing Module, Recognition Module and Output Module. First of all, we build a theoretical model of the system, and describe the details of every module, especially the CNN algorithm module. Then we introduce two classic facial expression databases JAFFE and CK+ to simulate the recognition process on MATLAB, and analyze recognition performance in different situations. A KNN algorithm is also employed to make comparison with CNN, which demonstrates that the CNN algorithm is more suitable for facial expression recognition.  \\nR\\nEFERENCES  \\n[1] Picard R. W.. Affective computing. MIT Press. \\n[2] Ekman P, Friesen WV. Constants across cultures in the face and \\nemotion[J]. Journal of personality and social psychology, 1971,17(2): \\n124. \\n[3] Mase K. Recognition of facial expression from optical flow. IEICE Trans \\nE[J]. Ieice Transactions on Information & Systems, 1991, 74(10). \\n[4] X. W. Chen and X. Lin, \"Big Data Deep Learning: Challenges and \\nPerspectives,\" in IEEE Access, vol. 2, no. , pp. 514-525, 2014.doi: \\n10.1109/ACCESS.2014.2325029. \\n[5] Hinton G E; Salakhutdinov R R. Reducing the dimensionality of data with \\nneural networks [J]. Science, 2006, 313:504-507. DOI: \\n10.1126/science.1127647. \\n[6] Hubel DH, Wiesel TN. Receptive fields, binocular interaction and \\nfunctional architecture in the cat’s visual cortex. The Journal of \\nPhysiology. 1962;160(1):106-154.2. \\n[7] Lecun, Y. \"Generalization and Network Design Strategies.\" \\nConnectionism in Perspective 1989. \\n[8] Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., & Matthews, \\nI. (2010). The Extended Cohn-Kanade Dataset (CK+): A complete expression dataset for action unit and emotion-specified expression. Proceedings of the Third International Workshop on CVPR for Human \\nCommunicative Behavior Analysis (CVPR4HB 2010), San Francisco, \\nUSA, 94-101. \\n[9] Michael J. Lyons, Shigeru Akemastu, Miyuki Kamachi, Jiro Gyoba. \\nCoding Facial Expressions with Gabor Wavelets, 3rd IEEE International Conference on Automatic Face and Gesture Recognition, pp. 200-205 \\n(1998). \\n[10] Pageorgiou C., Oren M., Poggio T.. A general framework for object \\ndetection. International Conference on Computer Vision. 1998. 555-562. \\n[11] Oren M., Pageorgiou C., Ppggio T.. Example ୍based object detection in \\nimages by components. IEEE Transaction on Pattern Analysis and \\nMachine Intelligence.2001.23(4): 349-361. \\n[12] Viola P., Jones M.. Rapid object detection using a boosted cascade of \\nsimple features. IEEE Conference on Computer Vision and Pattern \\nRecognition. 2001:511-518. \\n \\n128\\n',\n",
       " 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nIEEE TRANSACTIONS ON CYBERNETICS 1\\nAutomatic Facial Expression Recognition System\\nUsing Deep Network-Based Data Fusion\\nAnima Majumder, Laxmidhar Behera, Senior Member, IEEE , and Venkatesh K. Subramanian\\nAbstract —This paper presents a novel automatic facial\\nexpressions recognition system (AFERS) using the deep net-\\nwork framework. The proposed AFERS consists of four steps:\\n1) geometric features extraction; 2) regional local binary pat-\\ntern (LBP) features extraction; 3) fusion of both the featuresusing autoencoders; and 4) classiﬁcation using Kohonen self-organizing map (SOM)-based classiﬁer. This paper makes threedistinct contributions. The proposed deep network consisting ofautoencoders and the SOM-based classiﬁer is computationallymore efﬁcient and performance wise more accurate. The fusionof geometric features with LBP features using autoencoders pro-vides better representation of facial expression. The SOM-basedclassiﬁer proposed in this paper has been improved by makinguse of a soft-threshold logic and a better learning algorithm. Theperformance of the proposed approach is validated on two widelyused databases (DBs): 1) MMI and 2) extended Cohn–Kanade(CK+). An average recognition accuracy of 97.55% in MMI DBand 98.95% in CK+ DB are obtained using the proposed algo-rithm. The recognition results obtained from fused features arefound to be distinctly superior to both recognition using individ-ual features as well as recognition with a direct concatenation ofthe individual feature vectors. Simulation results validate that theproposed AFERS is more efﬁcient as compared to the existingapproaches.\\nIndex Terms —Autoencoder, deep network, facial expression\\nrecognition system, self-organizing map (SOM), support vectormachine (SVM).\\nI. I NTRODUCTION\\nFACIAL expressions recognition system has received sig-\\nniﬁcant attention among researchers in recent decades\\nmainly because of it is diversiﬁed applications, such as human\\ncomputer interactions, multimedia, surveillance, treatment\\nof mentally retarded patients, and lie detection. The study of\\nMehrabian [1] stated that to understand emotion or intention of\\na person, 55% of the information are conveyed through facialexpressions alone, 38% through vocal cues, and the remaining\\n7% via verbal cues. This encourages the researchers to explore\\ndeeply in the area of facial expressions recognition and anal-ysis (FERA). Ekman et al. [2] asserted after extensive study\\nManuscript received March 28, 2016; revised July 9, 2016 and\\nAugust 8, 2016; accepted October 28, 2016. This paper was recommended\\nby Associate Editor H. Yin.\\nThe authors are with the Department of Electrical Engineering,\\nIndian Institute of Technology Kanpur, Kanpur 208016, India (e-mail:\\nanimam@iitk.ac.in ;lbehera@iitk.ac.in ;venkats@iitk.ac.in ).\\nThis paper has supplementary downloadable multimedia material available\\nat http://ieeexplore.ieee.org provided by the authors.\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore .ieee.org .\\nDigital Object Identiﬁer 10.1109/TCYB.2016.2625419over facial expressions, that facial expressions are universal\\nand innate. They also concluded that six basic expressions,namely, happiness, sadness, disgust, anger, surprise, and fear\\nare universal in nature. During the last few decades, much\\nattention is given in automating features extraction method-ologies [3], [4]. Features extracted from image sequences can\\nbe classiﬁed into either different action units (AUs) or canbe classiﬁed directly to determine the six basic expressions.The key challenges in FERA are features extraction and clas-\\nsiﬁcation, as it is very difﬁcult to select a feature extraction\\ntechnique that works efﬁciently in varying conditions, such asdifferent skin colors, age, gender, and lighting conditions.\\nBased on the existing literature, the features can be broadly\\nclassiﬁed into two main categories: 1) geometric features and\\n2) appearance features. Geometric features require accurate\\nand reliable facial feature detection and tracking methods,as they are often sensitive to noise. On the other hand, the\\nappearance features are difﬁcult to generalize across differ-\\nent persons [ 5]. Yet, both the features play signiﬁcant role\\nin effective recognition of facial expressions. As we look at\\nan expressive face, our brain not only collects information\\nfrom appearance features or wrinkles, it also performs reason-ing of the information from facial geometric features, such as\\ndegree of closeness of eyes, lips, and widening of eyebrows.\\nSchyns et al. [6] explained in their paper about the decod-\\ning and decorrelation process of facial information occurs\\nin human brain. According to this paper, the brain encodes\\nboth the geometric and appearance features at different scalesduring its various processing stages.\\nAs the geometric features and appearance features are com-\\ning from two different features extraction approaches, they are\\nnot likely to be statistically related in any predictable man-\\nner. Particularly, the nonlinearities in the function mappings,from local binary pattern (LBP) feature space to emotion space\\nand from the geometric features space to emotion space are\\nnot of same order. The conventional feature fusion methodssimply concatenate several kinds of features together. Such\\napproach is simple, but the systems which use this kind of\\nfeature fusion may not perform better, or even it may performworse [7]. The reason for such behavior is uneven measure\\nor representation of different kinds of features. Moreover, theelement values of different features can be signiﬁcantly unbal-anced. Thus, a mere concatenation of the two feature vectors\\nresults in poor recognition performance. In this paper, we pro-\\npose a feature fusion framework using autoencoders which canlearn the correlation and generate a better representation of\\nboth the features. The autoencoder has a simple architecture\\n2168-2267 c/circlecopyrt2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSeehttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n2 IEEE TRANSACTIONS ON CYBERNETICS\\nwith an extra-ordinary capability of enhancing a given feature\\nat every successive hidden layer.\\nThe conventional deep learning network [ 8]u s i n gM L P\\nlearns by extracting features progressively from image pix-els which is usually of very high dimension. The learningtakes place in two stages: 1) stacked restricted Boltzmann\\nmachine (RBM) (unsupervised) and 2) error back-propagation\\n(supervised ﬁne tuning). For instance, each face detectedimage of the MMI and the extended Cohn–Kanade (CK +)\\ndatabases (DBs) contains 240 ×240 and 300 ×300 pixels,\\nrespectively. The conventional deep learning network will pro-cess input data of sizes 240 ×240 and 300 ×300, respectively,\\nwhile these data will be further processed through many hid-den layers of the network until one reaches the output layer.\\nThe supervised ﬁne-tuning of the entire network using back-\\npropagation incurs huge computation as massively parallelnonlinear steps are involved [ 9].\\nIn contrast, input data size for the ﬁrst layer of the proposed\\ndeep network is 258 ×1 (the geometric feature vector has\\nthe dimension 22 ×1 and the appearance feature vector has\\nthe dimension 236 ×1). This input data size is negligi-\\nble as compared to the data size that will be used in theconventional deep network [ 8]. The proposed network has\\nonly three layers that include two autoencoder layers and aself-organizing map (SOM)-based classiﬁer. The SOM-basedclassiﬁer processes input data of size 230 ×1. Thus the\\nproposed deep-network as given in this paper has a sim-ple architecture and is computationally more efﬁcient. Theautoencoders used in this paper has following architecture. In\\nthe ﬁrst layer, two separate autoencoders individually process\\ngeometric and appearance features. The concatenated learned\\nrepresentations obtained in the ﬁrst layer are given as input to\\nthe autoencoder at the second layer to learn the higher ordercorrelations between two different types of features. Such a\\ndata fusion technique provides better facial attributes with the\\nlower dimensional feature set. When compared with PCA,the autoencoder performs much better data representation\\nin a lower dimension, especially for nonlinear input data\\ntype [ 10]. The fused feature is further applied to SOM-based\\nclassiﬁer which is an improvement over our previous\\napproach [11 ].\\nThe contributions of the proposed work are summarized as\\nfollows.\\n1) The proposed deep network consisting of autoencoders\\nand the SOM-based classiﬁer has a simple architecture.\\nThe network is easy to train as compared to a deep learn-\\ning architecture with stacked RMB followed by MLP\\nwhich is trained using back-propagation.\\n2) The fusion of geometric features and appearance fea-\\ntures using autoencoders is a novel idea to get betterrepresentation of the facial attributes.\\na) Instead of considering whole face image, 236 LBP\\nfeatures are extracted from automatically detectedfour key facial regions. This approach reduces\\ncomputational burden while preserving location\\ninformation.\\nb) Geometric features of dimension 22 (with\\ndirectional displacement information of faciallandmarks) and appearance feature of dimension236 (using regional LBP) are fused using two lay-ers of autoencoders to get a better representation\\nof facial attributes.\\n3) A soft thresholding technique has been introduced at\\nthe output nodes of the SOM-based classiﬁer whichreduces false prediction rate. An improved learning algo-\\nrithm has been proposed to train the parameters of theclassiﬁer. The combined effect of the proposed soft\\nthresholding technique and the improved learning algo-\\nrithm has enhanced the recognition performance of theSOM-based classiﬁer signiﬁcantly.\\n4) The proposed technique is implemented and validated\\non two popular DBs: 1) MMI and 2) CK +.T h e\\nfusion algorithm is also implemented with support vectormachine (SVM) classiﬁer on both the DBs and the com-parative performance evaluation has been done through\\nextensive simulations.\\nThe remainder of this paper is organized as follows. A\\nbrief review of related previous work has been presentedin Section II. Two different types of features (geometric-\\nbased and regional LBP) extraction approaches are explained\\nin Sections III-A and III-B , respectively. The detailed\\nexplanation of the data fusion technique is given inSection III-C .\\nSOM-based classiﬁcation approach is explained\\nin Section III-D . In Section IV, evaluation performances of two\\ndifferent DBs; CK +and MMI are presented with discussions.\\nFinally, Section Vconcludes this paper.\\nII. R ELATED WORK\\nA. Previous Work on Feature Extraction and\\nClassiﬁcation Techniques\\nMuch progress has been made in geometric and appear-\\nance features extraction approaches [12 ]–[17] for automatic\\nfacial expressions recognition system (AFERS). Two wellestablished geometric or ﬁducial points detection methods\\nare active shape model (ASM) [12 ] and active appearance\\nmodel (AAM) [13]. However, there is one important limita-\\ntion in both the approaches. Each training face image in ASM\\nand AAM needs to be manually labeled with 68 landmarks.It becomes a tedious job to label 68 landmarks manually in\\neach of the training image contained in large DB. Some of the\\npopular appearance features extraction methods are LBPs [ 18],\\nGabor ﬁlters [19 ], linear discriminant analysis (LDA) [ 20] and\\nhistograms of oriented gradients [21]. Although Gabor ﬁlter-\\nbased AFERS gives very good recognition accuracies [ 19], it\\nis associated with high computational cost. As Gabor ﬁlters\\nuse ﬁve different scales and eight different orientations [ 22],\\na face image of size 240 ×240 has the Gabor feature dimen-\\nsion as 240 ×240×40. This is a huge dimensional feature\\nthat adds heavily to the computational cost. In contrast, the\\nLBP [ 14] extracts comparatively a low-dimensional feature\\nvector using nonlinear operations and hence has advantagesover approaches, such as Gabor ﬁlters and LDA. Given afeature vector, the next step in AFERS is to select a proper\\ndimensionality reduction and classiﬁcation technique. Some of\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nMAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 3\\nthe well established dimensionality reduction and classiﬁca-\\ntion techniques are RBMs [10], SVMs [23 ], and deep neural\\nnetworks [24 ], [25]. Zhi et al. [26] used sparse and graph-\\npreserving properties to reduce feature vector dimensions forfacial expressions recognition. SOM has been extensively usedin [27] and [ 28] as an unsupervised clustering technique, as it\\nhas the inherent capability of providing a topological orderingof the classes. Lawrence et al. [27] used local image sampling,\\nan SOM and a convolutional neural network to recognize face.\\nSome extension of SOM-based clustering and dimensionality\\nreduction approaches are visualization induced SOM (ViSOM)and growing ViSOM [ 28]. High level emotion recognition has\\nbeen recently reported in the literature, such as recognition ofsign languages [15] and detection of deception [ 17].\\nB. Previous Work on Data Fusion Techniques\\nA multimodal approach to feature extraction can lead to\\nthe development of a robust AFERS as facial expres-sions are subjected to diverse personalities, gender,race, color, and lighting conditions. Fusion of differ-\\nent kind of features has been adopted in the following\\nworks: [ 15], [19], [22], [29 ], and [30]. Yu et al. [29]\\nintroduced a new dimensionality reduction technique basedon spectral embedding for fusion of multiple features.Zavaschi et al. [30] proposed an ensemble of classiﬁers using\\nLBP and Gabor features. They further use multiobjectivegenetic algorithm (MOGA) based on accuracy and sizeof ensemble. Both LBP and Gabor ﬁlter-based features\\ngive appearance or texture information, while geometric\\nor shape information is missing in their work. Moreover,use of MOGA makes the processing very slow due to the\\ninvolvement of Gabor features which is very high dimensional\\ndata. Senechal et al. [19] introduced a method of combining\\ngeometric features (coefﬁcients of AAM) and local Gabor\\nbinary pattern histograms using multikernel learning (MKL)\\nalgorithm. They use one MKL SVM algorithm for each AU.Again, use of Gabor ﬁlters leads to higher computation cost\\nduring training, as for every face image, they are generating\\n3×6 images with three spatial frequencies and six orien-\\ntations. Liu et al. [15] integrated low level (geometric and\\nappearance) and high level features (eyebrow gesture and headnods, head shakes events) through multiscale, spatio-temporal\\nanalysis for recognition of nonmanual grammatical markers\\nin American Sign Language.\\nC. Proposed Work in the Context\\nIn the introduction, it has been emphasized that human cog-\\nnition makes use of both geometric and appearance features\\nto categorize facial expressions. It is found that the fusion\\nof geometric and appearance features that can provide a low-\\ndimensional meaningful feature space has not been explored\\nin depth in the literature. This paper solves this importantproblem using two layers of autoencoders in a deep-network\\nframework for the ﬁrst time. In addition, the proposed deep\\nnetwork includes an SOM-based classiﬁer that ﬁnds emotioncategories from the meaningful feature space obtained from\\nthe autoencoders. This SOM-based classiﬁer is the improvedversion of our previous work [ 11]—this previous work was\\nbased\\non 26-D geometric feature vector. Introduction of the\\nsoft threshold logic instead of hard thresholding and theimprovement in the learning algorithm are twofold modiﬁ-\\ncation in the present classiﬁer as compared to the previouswork [ 11]. This paper makes another fundamental contribu-\\ntion in automatic LBP-based feature extraction, where LBP isapplied to the four key regions of the face. This approach thushandles redundant information in a face in a computationally\\neffective manner while preserving local location information.\\nThe proposed approach has thus distinct advantages over [ 31]\\nin terms of generating a low-dimensional feature vector.\\nIII. P\\nROPOSED FACIAL EXPRESSIONS\\nRECOGNITION SYSTEM\\nThe proposed AFERS algorithm involves the following\\nsteps: geometric features extraction, regional LBP features\\nextraction, data fusion using autoencoders, and SOM-based\\nclassiﬁer.\\nA. Geometric Feature Extraction\\nA geometric feature vector xg∈ R22consisting of eye\\nprojection ratios and directional displacement information of\\nfacial points is used in this paper. Fig. 2shows an automatic\\nextraction of four key facial regions using facial geometricinformation. The steps for extracting geometric features froma given image are as follows.\\n1) Detect face, Get face height H\\nface.\\n2) Detect both the eyes.\\n3) Locate eyes’ centers (x1,y1)and(x2,y2). Estimate eyes’\\nregion which also include eyebrow regions. Get the eye\\nregion height.\\n4) Using the eyes’ center and the face height, estimate the\\nnose and lips regions.\\nIn successive frames, the facial regions are detected and the\\nfeatures are extracted from each detected region. In a video\\nsequence, where either face detection algorithm or eye detec-tion algorithm fails, the previous frame’s information is used to\\nestimate the location of face and eyes’ regions. This approach\\nincreases the detection accuracy of key facial regions and morevideo frames have been included in this paper which were dis-\\ncarded previously in our earlier work [11]. The Algorithm 1\\nbelow explains how the facial regions are detected in the\\nsuccessive frames. Once the key regions are detected, the\\neyes, eyebrows, nostrils, and lips are segmented using various\\nvision-based algorithms proposed in [11 ]. After segmenting\\nthe regions, the features are extracted from them. The eye fea-\\ntures are extracted by taking the ratio of horizontal and vertical\\nprojection of the segmented eye region that gives degree of an\\neye opening. In this paper, we consider that every sequence\\nstarts with a neutral expression and the ﬁrst frame is con-sidered as the reference frame. The facial landmarks whose\\ndirectional displacement features are to be calculated.\\n1) Left corner point, right corner point, and upper mid point\\nof left and right eyebrows, totally six points with 12displacement information along xandydirection.\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n4 IEEE TRANSACTIONS ON CYBERNETICS\\nAlgorithm 1 Facial Regions Detection in Successive Frames\\n1:forframe i←1t o Ndo\\n2: Detect face using Viola Jones’ [ 32] object detection algorithm.\\n3: Located both left and right eyes using using Viola Jones’ object\\ndetection algorithm.\\n4: ifThe face and eyes are detected in the ﬁrst frame then\\n5: Assign the detected face and eye regions as Rface\\ndectandReyedect\\n6: Detect the face in the next frame.\\n7: ifFace is not detected then\\n8: Assign the estimated face region Rface\\nestm as the previous frame’s\\nface ROI (region of interest) Rface\\ndect.\\n9: end if\\n10: Estimate the regions of eyes using the eyes’ ROI of the previous\\nframe. Reye\\nestm=Reye\\ndect11: Apply the Viola Jones’ object detection algorithm to detect both\\nthe eyes.\\n12: ifEither or both the eyes are not detected then\\n13: Assign the estimated regions detected eyes’ region.\\n14: end if\\n15: Once the eyes’ regions are set, apply the eye segmentation\\nalgorithm proposed in our previous work [ 11].\\n16: The eyebrows and lips regions are estimated based on the location\\nof eyes’ centers and face height as explained in our previous work [ 11].\\n17: else Discard the video sequence and start with new video sequence.\\n18: end if\\n19:end for\\nFig. 1. Selective AAM landmarks to extract geometric feature vector from\\nCK+DB.\\n2) Left corner point, right corner point, upper mid point,\\nand lower mid point of lips. These four points again give\\neight displacement information.\\nAlong with the displacement information of each key facial\\npoints, these features also maintain the direction and location\\ninformation of the features. With reference to our previous\\nwork [ 11], the feature vector dimension has been reduced by\\nfour. It has been observed, that the upper and lower mid points\\nof the eyebrows contain almost same information. Thus, to\\nreduce the unnecessary dimension, we drop information oflower mid points of both the eyebrows. The landmark points\\ndetection approach for MMI DB involves color-based tech-\\nniques which are described in detail in [ 11]. However, the\\nCK+ DB contains a large number of gray scale images which\\nare associated with 68 landmarks. We extract the similar geo-metric features using some of the facial landmarks labeled in\\nthe DB. Fig. 1is a pictorial description of the facial landmarks\\nchosen for extracting 22-D geometric features. In CK +DB,\\nevery sequence starts with a neutral expressions and ends withpeak expressions. Using this concept, we choose the landmarksFig. 2. Estimation of key facial regions using face height and eyes’ location.\\nGiven the face height Hface, eyes’ center (x1,y1), (x 2,y2)and eyes’ height,\\nthe expected regions of the nose, eyebrows, and lips are calculated.\\nof the ﬁrst frame as the reference and the 22-D features are\\nextracted in each of the successive images as follows.\\n1) The differences between (x,y)coordinates of landmark\\npoints on the reference frame and successive frames arecalculated for eyebrows and lips features.\\n2) For eyes, the distance between the ycomponent of upper\\nand lower points are considered as horizontal projection(H\\nproj)and distance between xcomponent of two corner\\npoints are considered as vertical projection (Vproj).\\n3) The projection ratio is given as (Hproj/Vproj), which is\\nconsidered as the feature for the eye.\\nB. Automatic Extraction of Regional LBP Features\\nLBP has been considered as a powerful texture features\\nextraction technique since its introduction. Ojala et al. [18]\\nproposed the LBP for texture features extraction. The basicassumption was that texture has locally two complementary\\naspects: 1) pattern and 2) strength [33 ]. Given a gray scale\\nimage I(x,y), with intensity value g\\nc=I(x,y)at the location\\n(x,y), the LBP is calculated as follows. For an evenly spaced\\ncircular neighborhood with Psampling points and radius R\\naround the center pixel (x,y), the gray value of pixel at the\\npth sampling point is gp, given by\\ngp=I/parenleftbig\\nxp,yp/parenrightbig\\n,p=0,..., P−1( 1 )\\nwhere xpand ypare the coordinate values of the sampling\\npoint. The thresholding binary operator S(gp−gc)can be\\ngiven as\\nS/parenleftbig\\ngp−gc/parenrightbig\\n=/braceleftBigg\\n1i f gp≥gc\\n0 otherwise.\\nThen the LBP P,Rfeatures are computed by taking deci-\\nmal equivalent of the Pbit binary operator S(gp−gc).\\nOjala et al. [18] proved that only a subset of the 2Ppatterns\\nextracted using basic LBP operator is sufﬁcient to describe\\nmost of the texture information within the image. In thispaper, we use uniform patterns in-stead of basic LBP patterns.\\nFurther explanation of the LBP is available in [ 34]. When the\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nMAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 5\\nFig. 3. Examples of resultant LBP images of four basic facial regions. The\\nupper row shows the facial regions and lower row shows the corresponding\\nLBP images.\\nLBP histogram of dimension 59 is extracted from the whole\\nface image, the features encode only the presence of micro\\npatterns. No information about their location is available.\\nMoreover, when the whole face is considered, it introducesunwanted nonlinearities to the data which may create difﬁcul-\\nties in learning a good classiﬁer. To overcome the problem,\\nwe introduce a concept of four key regions of the face whichare detected automatically by our algorithm. The regions also\\ncover all the possible subregions. The four key regions are:\\n1) left eye region including left eyebrow and a part of fore-head; 2) right eye region including right eyebrow and a part of\\nforehead; 3) nose region containing surrounding cheek regions;\\nand 4) the lips region containing cheek and chin regions.The regions are extracted using the basic facial geometry.\\nLBP are extracted from all the four basic facial regions. Each\\nregion (R\\n1,...R4)contributes a uniform LBP feature vector\\nof dimension 59. When all the feature vectors are concatenated\\nin order, we obtain a new feature vector of dimension 236, that\\npreserves approximate location and shape information of the\\nfacial region. The concatenated histogram can be deﬁned as\\nHk,rj=/summationdisplay\\nx,yS{LBP(x,y)=k} (2)\\nwhere rjis the region j∈{1,2,3,4}and kis the bin,\\nk∈{0,..., 58}.F i g . 3shows an example of four key facial\\nregions and the corresponding LBP images.\\nC. Data Fusion Using Autoencoders\\nAn autoencoder is an unsupervised learning algorithm\\nwhich applies back-propagation method to approximate the\\ninput feature vector. The main purpose of the autoencoder is\\nto learn a better representation of a feature set in a compressed\\nform. Fig. 4shows an example of an autoencoder. For a given\\ninput vector x=x1,x2,..., x6, the autoencoder tries to learn\\na function such that the network output h(x) approximates the\\ninput vector x. The hidden representation of xis given by ˆh(x)\\nˆhj(x)=f/parenleftbig\\naj(x)/parenrightbig\\nwhere aj(x)=bj+/summationdisplay\\nkWjkxk (3)\\nˆhj(x)is the output of the hidden unit jafter applying activa-\\ntion function f(.). Generally, the activation function f(.)is a\\nsigmoid function. The reconstructed output ˆxis obtained by\\nusing a decoding function. It can be given as\\nˆx=g/parenleftbig\\nˆak/parenrightbig\\nwhere ˆak=ck+/summationdisplay\\njW∗\\njkˆhj(x). (4)Fig. 4. Architecture of an autoencoder with single hidden layer.\\nFig. 5. Proposed data fusion technique based on autoencoders.\\nAgain, the function g(.) is mostly chosen as a sigmoid func-\\ntion. It is empirically observed [ 24] that after training RBM,\\nW∗becomes equivalent to WT. In the decoding part of the\\nautoencoder W∗is set to WTwhich is similar to RBM [35 ].\\nThis approach does not allow the network to learn a triv-ial identity function. Choosing W\\n∗=WTalso gives better\\nresults even if the number of hidden nodes is larger than theinput nodes. The parameters b\\njand ckare set to 1. In the\\nproposed approach, we train the model with geometric and\\nregional LBP features individually using two separate autoen-\\ncoders. The represented features of ﬁrst layer are concatenatedand given as input to the autoencoder of the second layer. The\\nﬁnal represented features of the second layer (output of the\\nhidden nodes at second layer) are considered as the fused fea-\\nture. Fig. 5is a pictorial illustration of proposed data fusion\\ntechnique. In the ﬁrst step geometric features x\\ng∈R22and\\nappearance features xl∈R236are separately passed through\\ntwo different autoencoders. In the next step the hidden lay-\\ners’ outputs of two autoencoders are concatenated and passedthrough another autoencoder. Output of the hidden unit jat\\nﬁrst layer for the geometric feature vector x\\ngand regional\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n6 IEEE TRANSACTIONS ON CYBERNETICS\\nLBP feature vector xlcan be calculated as\\nˆhz\\nj(x)=1\\n1+e−az\\nj(x)(5)\\nwhere\\naz\\nj(x)=bzj+/summationdisplay\\nkWz\\njkxk (6)\\nwhere azis the hidden unit’s output at node jbefore applying\\nthe activation function. zis the data type (either geomet-\\nric type or regional LBP type), bzjis the bias term for the\\nfeature vector x. In case of the geometric features the input\\nfeature vector is xgand for the regional LBP features it is xl.\\nConsidering ˆhg∈RMas the output of the hidden layer for the\\ngeometric feature vector xgandˆhl∈RNas the output for the\\nregional LBP feature xlafter applying the activation function,\\nthe concatenated feature vector x2(which is an input vector\\nfor the second layer) is hence given by\\nx2=/bracketleftBig\\nˆhg\\n1,ˆhg2,...,ˆhg\\nM,ˆhl\\n1,ˆhl2,...,ˆhl\\nN/bracketrightBig\\n(7)\\nwhere Mand Nare the total number of hidden nodes of\\nthe autoencoders for geometric and regional LBP features,\\nrespectively. The ﬁnal fused feature vector\\nx=ˆhf(x2) (8)\\nis generated using (5). In an autoencoder the weights and bias\\nare updated using stochastic gradient descent algorithm. The\\nupdate rules for the weight W(l)and the bias b(l)at layer l\\nof the network with a training sample {x(i),y(i)}where i∈\\n(1,2,3,...,M )is given as\\nW(l)(i+1)=W(l)(i)−α∂\\n∂W(l)(i)J/parenleftBig\\nW(l),b(l);x(i),y(i)/parenrightBig\\n(9)\\nb(l)(i+1)=b(l)(i)−α∂\\n∂b(l)(i)J/parenleftBig\\nW(l),b(l);x(i),y(i)/parenrightBig\\n(10)\\nwhere the J(W,b;x(i),y(i))is the cost function with respect\\nto a single training example (x(i),y(i)). The cost function is\\ndeﬁned as\\nJ/parenleftBig\\nW(l),b(l);x(i),y(i)/parenrightBig\\n=1\\n2/bardblW(l)Th(x)−x/bardbl2−λ\\n2/summationdisplay\\nj/summationdisplay\\nk/parenleftBig\\nw(l)\\njk/parenrightBig2\\n(11)\\nwhere the ﬁrst term is the sum square error term [error betweentheˆx=W\\nTh(x) and x, the input at layer l−1]. The sec-\\nond term is regularization term or the weight decay term. The\\nparameter λis the weight decay parameter. The readers can\\nrefer [36] for more details about the parameters update rule.Once the fused feature vector ˆh\\nf(x)is obtained after train-\\ning the autoencoders, the fused feature vector is classiﬁedinto six basic expressions using both SOM-based classiﬁerand SVM. The SOM-based classiﬁer which is an improvement\\nover our previous work [ 11] is described in Section III-D .W e\\nhave used the library [ 37] for implementing the autoencoder\\nwhere an exercise for implementing sparse autoencoder (usingMATLAB) is given.Fig. 6. SOM-based classiﬁcation technique. In the SOM, each node\\nrepresents the local characteristics of the input feature space, where weight\\nvector wγ, matrix Aγ, and bias yγare the related parameters for the node γ.\\nD. SOM-Based Classiﬁer\\nClassiﬁcation techniques based on SOM have been cate-\\ngorized into two different kinds: 1) hard classiﬁcation [ 38]\\nand 2) soft classiﬁcation [39], [40 ]. Linear vector quanti-\\nzation (LVQ) and K-nearest neighbor methods are mainly\\nused as a precursor to SOM for hard classiﬁcation tech-\\nniques. However, one major disadvantage of the LVQ-based\\napproach is that, it needs to generate codebook vectors fora given problem which is often very difﬁcult. In case of\\nK-nearest neighbor-based approach, the input vector is classi-\\nﬁed to a category which has the highest likelihood for each\\nreference vector. It is difﬁcult to directly estimate the likeli-\\nhood for a category by computing the probability from the\\nn-dimensional region to which the input vector belongs [ 38].\\nHard classiﬁcation techniques are thus gradually taken over bysoft classiﬁcation techniques. Li and Eastmann [ 40] proposed\\nan SOM-based soft classiﬁcation technique using two non-\\nparametric algorithms, called SOM commitment and SOM\\ntypicality. However, the approach again has some limitations:it gives the same soft answer to all the pixels associated with\\none node and it does not take into account the distance infor-\\nmation between the labeled pixels and the prototype of eachnode. Other variant of SOM-based classiﬁcation techniques\\ncan be found in [41 ] and [ 42]. The proposed SOM-based\\nclassiﬁer as presented in this section is a soft classiﬁcation\\ntechnique which is an improvement of our previous work [ 11].\\nThe SOM [ 43] is a topology preserving network, where the\\nneural lattice discretizes the entire feature space to a ﬁnitenumber of small discrete zones, each zone is represented by a\\nsingle neuron as shown in Fig. 6. The emotional feature space\\nis discretized in terms of Kohonen 2-D lattice of size M×N.\\nGiven a fused feature vector xcorresponding to a facial\\nimage, the output of the classiﬁer yrepresents the emotion\\nclass, such as happiness, sadness, disgust, anger, surprise,and fear. yis thus a 6-D vector. Happiness is represented as\\ny=[1−1−1−1−1−1]\\nTwhere ﬁrst term is ON while all\\nother terms are OFF. Other facial categories are representedin a similar manner. For example, in case of sadness, the sec-\\nond term will be ON while all other terms will be OFF. Thefused feature vector as derived from the autoencoders is a\\n230-D vector, i.e., x∈R\\n230. The input–output mapping from\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nMAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 7\\nthe facial fused feature space to emotion categories can be\\nmathematically expressed as\\ny=f(x),y∈R6;x∈R230. (12)\\nThis highly nonlinear mapping can be linearized using ﬁrstorder Taylor series expansion around each neuron. Given thateachγth neuron is associated with a vector w\\nγ∈R230in the\\nlattice, the linear model associated with this neuron can beexpressed as\\ny\\nout\\nγ=yγ+A/parenleftbig\\nx−wγ/parenrightbig\\nwhere, Aγ=∂f\\n∂x|x=w γ∈R6×230\\nyγ=f/parenleftbig\\nwγ/parenrightbig\\n(ideally ). (13)\\nIn the SOM-based classiﬁer, neighbors participate in deci-\\nsion making although winning neuron has the bigger say. The\\ncollective response of the network given an input xis given as\\nyout=/summationtextM×N\\nγ=1hi,γyout\\nγ/summationtextM×N\\nγ=1hi,γ(14)\\nwhere hi,γis the neighborhood function for the best matching\\nunit (BMU) iand the neuron γ.T h eB M U iis obtained by\\ni=arg min\\nγ/bardblx−wγ/bardbl. (15)\\nIn this approach, the collective response model (14 )i s\\nexpressed using locally valid linear models associated with\\neach neuron γ. This model ( 14) thus approximates the global\\nnonlinear mapping y=f(x)by learning local linear map-\\nping. This approach has twofold beneﬁts—fast learning andbetter accuracy. The learning problem is to update param-\\neters w\\nγ,yγ,Aγgiven training pairs {x,yd}. Neighborhood\\nfunction hi,γ(n)and the spread σ(n)fornth iteration are cal-\\nculated as in (14) and(18) of our previous work [11]. A\\nsigmoid activation function is used at each node of the net-\\nwork to classify the input feature vector into six fundamental\\nexpressions. Instead of setting a hard threshold value [ 11]f o r\\ndiscrete outputs from the network, we indeed ﬁnd the greatest\\nvalue Grover all the sigmoid function (tan hyperbolic) outputs.\\nThe node which achieves the Gr, is set to 1 and the rest are\\nset to −1. Essentially, the node that has given the maximum\\nactivation is chosen as the recognized class. Let the sigmoid\\nfunction output of kth class is denoted by ysig\\nk\\nysigk=eyout\\nk−e−youtk\\neyout\\nk+e−youtk(16)\\nifysig\\nk=Gr,then yk=1\\nelse yk=−1 (17)\\nThis may be described as a winner dominates policy. In [ 11],\\nthe hard-threshold was set to 0.5. It was observed that the\\nresponse of multiple output nodes crossed this hard-thresholdin many occasions. It was also observed that sometimes no\\nresponse at the output nodes crossed this hard-threshold. By\\nincorporating the proposed approach as shown in (17), the\\nmodel guarantees that only one response at the output nodes\\nwill be ON while the rest will be OFF. This feature of theproposed SOM-based classiﬁer is necessary as every facial\\nimage in the considered two-DBs is labeled to only onecategory of emotion. It has been experimentally observed\\nthat parameter updates based on the minimization of least\\nsquare error provides better accuracy, we have considered thefollowing least square cost function:\\nE=\\n6/summationdisplay\\nk=1Ek=1\\n26/summationdisplay\\nk=1/parenleftBig\\nyd\\nk−ysig\\nk/parenrightBig2\\n. (18)\\nThe update of the parameter wγis retained same as in SOM,\\nwhich is given as [ 11, eq. (15)]. To update the parameters Aγ\\nandyγthe steps are given below. The derivatives of the cost\\nfunction with respect to the parameters at each output node k\\nare given as\\n∂E\\n∂yγ=−/parenleftBig\\nyd\\nk−ysig\\nk/parenrightBig\\nhi,γ\\n/parenleftBig/summationtextM×N\\nγ=1hi,γ/parenrightBig/parenleftbigg\\n1−/parenleftBig\\nysigk/parenrightBig2/parenrightbigg\\n(19)\\n∂E\\n∂aγ=−/parenleftBig\\nyd\\nk−ysig\\nk/parenrightBig\\nhi,γ/parenleftbig\\nx−wγ/parenrightbig\\n/parenleftBig/summationtextM×N\\nγ=1hi,γ/parenrightBig/parenleftbigg\\n1−/parenleftBig\\nysigk/parenrightBig2/parenrightbigg\\n(20)\\nwhere aγis the kth row of the matrix Aat neuron γin\\nthe SOM network. The update of parameters yγand Aγ\\nare as given in (25)–(27) of our previous work [11]. The\\nproposed Kohonen SOM-based classiﬁer uses the combined\\nbeneﬁt of both supervised and unsupervised learning algo-\\nrithms. The parameters update laws in (19 ) and ( 20) show that\\ntarget error (yd\\nk−ysig\\nk)(supervised) as well as neighborhood\\nindex hi,γ(unsupervised) inﬂuence the parameters updates.\\nThe update of the parameter wγfor each neuron γwithin the\\nneighborhood hi,γis done in unsupervised approach, whereas,\\nthe parameter updates for yγandAγuse both supervised and\\nunsupervised terms. Thus, the network is a combination of\\nboth supervised and unsupervised techniques. This is our novel\\ncontribution in development of the SOM-based classiﬁer.\\nIV . E XPERIMENTAL RESULTS\\nExperiments are performed on CK +[44] and MMI [ 45]D B s .\\nSix basic expressions deﬁned earlier are used for both the DBs.CK+ DB comes with labeled emotions and corresponding 68\\nlandmarks tracked using AAM [ 13]. There are 327 sequences\\nin total in the DB. Each sequence has one of the seven expres-sions (anger, contempt, disgust, fear, happiness, sadness, andsurprise). The sequence starts with a neutral expression and\\nends with peak expression. It has been observed that in some\\nof the image sequences, the landmark ﬁle is missing. Also insome of the images, emotion labeling is missing. Since we\\nhave considered six basic emotions, such as happiness, sad-\\nness, disgust, anger, surprise, and fear in MMI DB, we take\\nonly those images from CK+ DB which have one of these\\nsix expressions. Some of the initial images in a sequence are\\nalmost but not totally neutral; to eliminate those expressions,\\nwe set a threshold on the norm of the feature vector for the\\ngeometric feature vector. Only the corresponding images areused for extracting LBP feature vector. Total 3718 images con-\\ntaining 123 subjects are used from CK+ DB. In the MMI DB,\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n8 IEEE TRANSACTIONS ON CYBERNETICS\\nnot all the video clips start with a neutral expressions. Since\\nwe need at least the ﬁrst frame to be of neutral expressionsto compute geometric features, we take only those video clips\\nwhich have one of the six emotions labeled and neutral frame\\nin the beginning. Thus, from MMI DB, we use 20 differentsubjects with 96 image sequences. The face image from MMI\\nand CK+ DBs are normalized to a standard scale (240×240)\\nand(300×300), respectively, after the face detection is per-\\nformed. The normalized face is then transformed to removeslight orientation changes. The angle for rotation is obtained\\nusing two eyes’ centers and the nose tip detected automati-cally [11]. To remove any kind of biases and to bring the feature\\nvectors into a common scale, both the appearance and the geo-\\nmetric features are normalized using standard normalization\\ntechniques. All the experiments are performed with tenfold\\ncross validation. To be more precise, the dataset is partitionedinto ten equally sized groups. Only one group is considered\\nas testing set while remaining nine groups are considered as\\ntraining set. This process is repeated for all the ten sets. Theconfusion matrices of all the ten sets are averaged to get the\\nﬁnal confusion matrix. Details of experimental results for both\\nCK+ and MMI DBs are presented below.\\nA. Experimental Results on MMI and CK+ Databases\\nThe proposed data fusion technique presented in\\nSection III-C is used to obtain fused feature vector.\\nThe number of hidden units for geometric feature vector in\\nboth MMI and CK+ DBs is chosen as 20. In case of LBP\\nfeature vector, the hidden units for CK +and MMI DBs\\nare set to 170 and 150, respectively. In the second layer of\\nthe autoencoder the number of hidden units for both CK +\\nand MMI DBs is chosen as 230. The choice of number of\\nhidden units at each layer is based on parameters tuning. The\\nbest performance is obtained using these above mentionedparameter values. An SOM-based network of size 10 ×8,\\nwith the initial learning rate η\\nI=0.9 and ﬁnal learning rate\\nηF=0.005 is used for the experimental purpose. In case\\nof SVM we use LibSVM for the experiment. The approach\\nis one-against-one and the kernel is taken as RBF, number\\nof generation used is 10 and Gamma =0.1. We also tested\\nwith varying parameters and with different kernels types\\n(polynomial and linear). Among them, the best performance\\nis obtained using RBF kernel for the above stated parameters.Recognition accuracies of both CK +and MMI DBs for all\\nthe types of data, are shown as confusion matrices in tabularform.\\n1) Results of MMI Database: Table Ishows the confusion\\nmatrix of MMI DB for the 22-D geometric facial features\\nwhen classiﬁed using the SOM-based classiﬁer. An aver-\\nage recognition accuracy of 91 .17% is obtained with highest\\nrecognition accuracy 96.67% in case of anger and lowestaccuracy 86.13% in case of sadness. Table IIshows the con-\\nfusion matrix of MMI DB for regional LBP features. Results\\nshows an average recognition accuracy of 90.3% with high-\\nest recognition accuracy 96 .3% for happiness and 82.34% for\\ndisgust.\\nThe confusion matrix of MMI DB for fused data is shown\\nin Table IV. An average recognition accuracy of 97.55% withTABLE I\\nSOM R ESULTS FOR 22-D G EOMETRIC FEATURES (MMI DB)\\nTABLE II\\nSOM R ESULTS FOR 236-D LBP F EATURES (MMI DB)\\nTABLE III\\nSVM R ESULTS FOR 230-D F USED FEATURES (MMI DB)\\nhighest recognition accuracy 98 .71% in case of surprise and\\nlowest 96 .28% in case of anger is achieved. It is observed\\nthat classiﬁcation from the fused data outperforms that from\\nthe individual features in terms of average recognition accu-\\nracy. It exceeds geometric-based features by 6.38 and regionalLBP features by 7.28%. The confusion matrix of the fused\\nfeature vector of MMI DB using SVM classiﬁer is given\\nin Table III. We observe an average recognition accuracy\\nof 93.2%. When tested on individual feature vectors of, the\\nSVM shows 89.75% and 90.05% average recognition accu-\\nracies for geometric and regional LBP features, respectively.The plots shown in Fig. 7summarizes the SOM -based recog-\\nnition results all the three types of features. The plots clearlyshow the superiority of fused features recognition results overthe two types of individual features.\\n2) Results of CK+ Database: The confusion matrices for\\nthe 22-D geometric features and 236-D regional LBP fea-tures for the CK +DB are shown in Tables VandVIbelow.\\nIt is observed that using only geometric-based features, we\\nobtain an average recognition accuracy of 96.06% which out-\\nperforms the average recognition accuracy of regional LBP\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nMAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 9\\nTABLE IV\\nSOM R ESULTS FOR 230-D F USED FEATURES (MMI DB)\\nFig. 7. Comparison between SOM-based recognition results of MMI DB for\\ndifferent types of features.\\nTABLE V\\nSOM R ESULTS FOR 22-D G EOMETRIC FEATURES (CK+DB)\\nfeatures of dimension 236 by 2.47%. The geometric-based fea-\\ntures extracted for CK+ DB are based on AAM. The regional\\nLBP features gives an average recognition accuracy of 93.59%\\nwith highest recognition accuracy 96. 64% in case of surprise\\nand lowest recognition accuracy 84.09% in case of fear. The\\ngeometric-based approach gives highest recognition accuracy\\nof 98.17% in case of surprise and 94 .89% in case of happi-\\nness. The confusion matrix of fused features using CK+ DB\\nis tabulated in Table VIII. It is observed that the recognition\\naccuracy of fused features is higher than the individual fea-\\ntures. An average recognition accuracy of 98 .95% is achieved\\nwith highest recognition accuracy 99.44% in case of happi-\\nness and a lowest recognition accuracy of 98 .54% is obtained\\nin case of fear.TABLE VI\\nSOM R ESULTS FOR 236-D LBP F EATURES (CK+DB)\\nTABLE VII\\nSVM R ESULTS FOR 230-D F USED FEATURES (CK+DB)\\nTABLE VIII\\nSOM R ESULTS FOR 230-D F USED FEATURES (CK+DB)\\nTABLE IX\\nAVERAGE RECOGNITION ACCURACY IN %U SING SOM\\nTable VIIshows the recognition accuracies of the fused fea-\\ntures for CK+ DB when classiﬁed using SVM. We observe\\nan average recognition accuracy of 95 .01% for CK+ DB.\\nWhen tested on individual feature vectors of CK +DB, the\\nSVM shows average recognition accuracies of 93 .5% and\\n89.3% for geometric and regional LBP features, respectively.\\nFig. 8summarizes the recognition performance of the SOM-\\nbased classiﬁcation for all the three types of features. The\\nplots clearly shows that the fused feature leads the individual\\nfeatures.\\n3) Extensive Performance Validation: To validate the\\nperformance of the proposed approach, we did several\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n10 IEEE TRANSACTIONS ON CYBERNETICS\\nFig. 8. Comparison between SOM-based recognition results of CK +DB\\nfor different types of features.\\nTABLE X\\nAVERAGE RECOGNITION ACCURACY IN %U SING SVM\\nexperimental studies. First, tested on concatenated feature vec-\\ntor that simply concatenates the 22-D geometric feature vector\\nand the 236-D LBP feature vector. Both the features were nor-\\nmalized before concatenation. The randomized concatenated\\nfeature vector is classiﬁed using the SOM-based classiﬁer. The\\naverage recognition results of both the DBs given in Table IX\\nshow that the performance of the proposed fused feature vec-\\ntor is signiﬁcantly better than just simple concatenated feature\\nvector. Several experiments are carried out to demonstrate thevarying performances for different number of hidden nodes\\nat the last layer. The testing results for both MMI and CK +\\nDBs are given as supplementary material due to page limita-\\ntion. The performances with more hidden layers are also being\\nanalyzed, but did not ﬁnd any signiﬁcant improvement over\\nthe stated approach; in-fact the performance deteriorates onincreasing the number of hidden layers. For CK+ and MMI\\nDBs we tested with one more hidden layer having 230 nodes.The other parameters are kept same (for both MMI and CK +\\nDBs) as those stated earlier in this paper for the best fusion\\nresults. We observed an average recognition accuracy of 89 .3%\\nfor MMI DB and 92.54% for CK+ DB. The plot shown in\\nFigs. 7and8summarizes the recognition performances of all\\nthe four types of features (geometric features alone, regional\\nLBP features alone, fused features, and concatenated features).\\nIn a nutshell we can see the average recognition accuracies\\nof SOM and SVM classiﬁers in Tables IXand X, respec-\\ntively. It is observed that the recognition accuracy of fused\\nfeatures is better than both geometric-based and regional LBP\\nfeatures. We also observe that the recognition accuracy of theconcatenated features is much lower than the individual fea-\\ntures when used alone. A comparative study of the proposedTABLE XI\\nPERFORMANCE COMPARISON OF FUSED FEATURE VECTOR TAKEN\\nFROM MMI AND CK+DBSWITHEXISTING WORK\\nTABLE XII\\nSOM R ESULTS FOR 22-D G EOMETRIC FEATURES (MMI DB)\\nUSING THRESHOLD AS 0.5 ATNETWORK OUTPUT\\nfusion technique with some recent research work have been\\npresented in Table XI. In this paper, the dataset taken from\\nMMI DB, uses 20 subjects containing 96 video sequences,\\nwhereas only 12 subjects containing 81 video sequences were\\nused in our previous study [ 11]. To have a fair compari-\\nson with our previous work [ 11] we perform the following\\nexperiment. This comparison uses the present dataset, takenfrom MMI DB with 20 subjects. The geometric feature vec-\\ntorx\\ng∈R22is used to train the SOM-based model proposed\\nin our previous work [11]. Table XIIgives the recognition\\nresults in the form of confusion matrix. Table Ishown pre-\\nviously, presents a confusion matrix for the same geometric\\nfeature vector xg∈R22using the proposed soft thresholding\\ntechnique. Tenfold cross validation technique is used for the\\nexperiments. It is observed that, application of the proposed\\nsoft thresholding method at each of the output nodes improvesthe recognition performance. An average recognition accuracy\\nof 87.55% is achieved when hard threshold is applied. On\\napplication of the soft thresholding, the recognition accuracyis 91.17%, the improvement is quite signiﬁcant.\\nV. C\\nONCLUSION\\nThis paper has presented a deep-network-based AFERS,\\nwhere ﬁrst two layers of autoencoders effectively fuse two\\nentirely different kinds of features (geometric and appear-\\nance) for better representation of the facial data. Effectivefusion of two different kind features using auto-encoders as\\npresented in this paper is ﬁrst of its kind. The SOM-based\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\nMAJUMDER et al. : AFERS USING DEEP NETWORK-BASED DATA FUSION 11\\nclassiﬁer in the third layer combines beneﬁts of both super-\\nvised and unsupervised learning algorithms. This classiﬁer isan improved version of the previous work [ 11]. It is observed\\nthat the use of hard thresholding technique [ 11] sometimes\\nmisleads the class prediction. The proposed soft threshold-ing criteria presented in this paper deals with this problem\\neffectively. The mathematical principle of modeling a non-\\nlinear mapping in terms of locally valid linear mappings asgiven in ( 14) has been provided in this version. Parameter\\nupdate laws have been modiﬁed while minimizing a least-square function. The learning steps as given in (19 ) and ( 20)\\nare different as compared to the previous work [ 11]. Another\\nimportant feature of the present deep-network architecture isthat it has a simple architecture consisting of three layers and\\nthe network has to process the data of maximum dimension\\nof 258 ×1.\\nThe proposed approach is rigorously validated on two\\nwidely used DBs, namely, MMI and CK +. The recogni-\\ntion results of fused data are compared with the recognitionperformance of individual features and also with that of\\nconcatenated feature vector. It is evident that the recogni-\\ntion results using fused features outperforms the recognitionresults of individual features and the concatenated features.\\nRecognition performance of the proposed data fusion tech-\\nnique is also studied by putting SVM classiﬁer in place ofSOM-based classiﬁer. The performance of the fused features\\nusing the proposed SOM-based classiﬁer gives signiﬁcant\\nimprovement over the SVM, with an 3 .94% increase in recog-\\nnition accuracy for CK+ DB and 4.36% for MMI DB. It is\\nalso observed that the proposed data fusion technique enhancesrecognition performance irrespective of the classiﬁcation tech-\\nnique used. The proposed facial expression recognition system\\nis found to be successful in recognizing each of the sixexpressions in all the experiments with a very high accu-\\nracy, however, its performance in real-life environment is\\nyet to be investigated. Real-life DBs, like LFW [ 51] can be\\nused to evaluate the performance of the proposed recogni-\\ntion system, once it is trained with existing labeled facial\\nexpressions DBs. The presented geometric features extrac-tion approach considers the ﬁrst frame in every sequence\\nto be neutral expression, which may not be always true\\nin real-life DBs. In such scenerios, an average normalizedface can be used as reference frame to extract geometric\\nfeatures. The proposed appearnace features are independent\\nof any reference frame and hence can be extracted directlyfrom automatically detected facial regions. Once the individ-\\nual features are extracted, the proposed data fusion framework\\ncan be used directly to generate better represented fusedfeature vector. The real-life images must be preprocessed\\nand normalized before extracting the features to make them\\nrobust to various facial transformations. Facial planar rota-\\ntion can be corrected by calculating rotation angle between\\ntwo detected eye centers. Interested readers may refer [52 ]t o\\ndeal with partial facial occlusions if any. An online train-\\ning mechanism of the proposed data fusion framework can\\nalso be explored for real-life applications, such as teach-ing a robot to learn facial expressions by imitating human\\nexpressions [53].A\\nCKNOWLEDGMENT\\nThe authors would like to thank the Council of Scientiﬁc\\nand Industrial Research, for providing scholarship support\\ntoward the completion of this paper. They also would like\\nto thank S. Dutta for his valuable discussions and help in\\nimplementing this paper.\\nREFERENCES\\n[1] A. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA:\\nAldine, 2007.\\n[2] P. Ekman, W. V . Friesen, and J. C. Hager, Facial Action Coding System .\\nSalt Lake City, UT, USA: Human Face, 2002.\\n[3] L. Ma and K. Khorasani, “Facial expression recognition using construc-\\ntive feedforward neural networks,” IEEE Trans. Syst., Man, Cybern. B,\\nCybern., vol. 34, no. 3, pp. 1588–1595, Jun. 2004.\\n[4] C.-T. Tu and J.-J. J. Lien, “Automatic location of facial feature points\\nand synthesis of facial sketches using direct combined model,” IEEE\\nTrans. Syst., Man, Cybern. B, Cybern. , vol. 40, no. 4, pp. 1158–1169,\\nAug. 2010.\\n[5] S. Moore and R. Bowden, “Local binary patterns for multi-view facial\\nexpression recognition,” Comput. Vis. Image Understand., vol. 115,\\nno. 4, pp. 541–558, 2011.\\n[6] P. G. Schyns, L. S. Petro, and M. L. Smith, “Transmission of facial\\nexpressions of emotion co-evolved with their efﬁcient decoding in the\\nbrain: Behavioral and brain evidence,” Plos One, vol. 4, no. 5, p. e5625,\\n2009.\\n[7] Y . Fu, L. Cao, G. Guo, and T. S. Huang, “Multiple feature fusion by sub-\\nspace learning,” in Proc. Int. Conf. Content Based Image Video Retrieval ,\\nNiagara Falls, ON, Canada, 2008, pp. 127–134.\\n[8] G. E. Hinton, S. Osindero, and Y .-W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural Comput. , vol. 18, no. 7, pp. 1527–1554,\\n2006.\\n[9] C. Szegedy et al., “Intriguing properties of neural networks,” arXiv\\npreprint arXiv:1312.6199 , Dec. 2013.\\n[10] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[11] A. Majumder, L. Behera, and V . K. Subramanian, “Emotion recogni-\\ntion from geometric facial features using self-organizing map,” Pattern\\nRecognit., vol. 47, no. 3, pp. 1282–1293, 2014.\\n[12] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape\\nmodels-their training and application,” Comput. Vis. Image Understand.,\\nvol. 61, no. 1, pp. 38–59, 1995.\\n[13] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance\\nmodels,” in Computer Vision—ECCV98 . Heidelberg, Germany: Springer,\\n1998, pp. 484–498.\\n[14] T. Ojala, M. Pietikäinen, and T. Maenpaa, “Multiresolution gray-scale\\nand rotation invariant texture classiﬁcation with local binary patterns,”IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp. 971–987,\\nJul. 2002.\\n[15] J. Liu et al., “Non-manual grammatical marker recognition based on\\nmulti-scale, spatio-temporal analysis of head pose and facial expres-\\nsions,” Image Vis. Comput. , vol. 32, no. 10, pp. 671–681, 2014.\\n[16] L. Zhong, Q. Liu, P. Yang, J. Huang, and D. N. Metaxas, “Learning\\nmultiscale active facial patches for expression analysis,” IEEE Trans.\\nCybern., vol. 45, no. 8, pp. 1499–1510, Aug. 2015.\\n[17] X. Yu et al., “Is interactional dissynchrony a clue to deception? insights\\nfrom automated analysis of nonverbal visual cues,” IEEE Trans. Cybern. ,\\nvol. 45, no. 3, pp. 492–506, Mar. 2015.\\n[18] T. Ojala, M. Pietikäinen, and D. Harwood, “A comparative study of tex-\\nture measures with classiﬁcation based on featured distributions,” Pattern\\nRecognit., vol. 29, no. 1, pp. 51–59, 1996.\\n[19] T. Senechal et al., “Facial action recognition combining heterogeneous\\nfeatures via multikernel learning,” IEEE Trans. Syst., Man, Cybern. B,\\nCybern., vol. 42, no. 4, pp. 993–1005, Aug. 2012.\\n[20] M. H. Siddiqi, R. Ali, A. M. Khan, Y .-T. Park, and S. Lee, “Human facial\\ne \\nxpression recognition using stepwise linear discriminant analysis and\\nhidden conditional random ﬁelds,” IEEE Trans. Image Process., vol. 24,\\nno. 4, pp. 1386–1398, Apr. 2015.\\n[21] Y . Pang, H. Yan, Y . Yuan, and K. Wang, “Robust CoHOG feature extrac-\\ntion in human-centered image/video management system,” IEEE Trans.\\nSyst., Man, Cybern. B, Cybern. , vol. 42, no. 2, pp. 458–468, Apr. 2012.\\nThis article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\\n12 IEEE TRANSACTIONS ON CYBERNETICS\\n[22] W. Liu and Z. Wang, “Facial expression recognition based on\\nfusion of multiple Gabor features,” in Proc. 18th Int. Conf. Pattern\\nRecognit. (ICPR), vol. 3. Hong Kong, 2006, pp. 536–539.\\n[23] M. F. Valstar, I. Patras, and M. Pantic, “Facial action unit detection\\nusing probabilistic actively learned support vector machines on tracked\\nfacial point data,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis.\\nPattern Recognit. Workshops (CVPR) Workshops , San Diego, CA, USA,\\nJun. 2005, p. 76.\\n[24] H. Larochelle, Y . Bengio, J. Louradour, and P. Lamblin, “Exploring\\nstrategies for training deep neural networks,” J. Mach. Learn. Res. ,\\nvol. 10, pp. 1–40, Jan. 2009.\\n[25] P. Liu, S. Han, Z. Meng, and Y . Tong, “Facial expression recognition\\nvia a boosted deep belief network,” in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit. , Columbus, OH, USA, 2014, pp. 1805–1812.\\n[26] R. Zhi, M. Flierl, Q. Ruan, and B. W. Kleijn, “Graph-preserving sparse\\nnonnegative matrix factorization with application to facial expressionrecognition,” IEEE Trans. Syst., Man, Cybern. B, Cybern. , vol. 41, no. 1,\\npp. 38–52, Feb. 2011.\\n[27] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, “Face recognition:\\nA convolutional neural-network approach,” IEEE Trans. Neural Netw.,\\nvol. 8, no. 1, pp. 98–113, Jan. 1997.\\n[28] W. Huang and H. Yin, “Visom for dimensionality reduction in\\nface recognition,” in Advances in Self-Organizing Maps. Heidelberg,\\nGermany: Springer, 2009, pp. 107–115.\\n[29] K. Yu, Z. Wang, M. Hagenbuchner, and D. D. Feng, “Spectral\\nembedding based facial expression recognition with multiple features,”Neurocomputing, vol. 129, pp. 136–145, Apr. 2014.\\n[30] T. H. H. Zavaschi, A. S. Britto, Jr., L. E. S. Oliveira, and A. L. Koerich,\\n“Fusion of feature sets and classiﬁers for facial expression recognition,”Expert Syst. Appl., vol. 40, no. 2, pp. 646–655, 2013.\\n[31] G. Zhao and M. Pietikainen, “Dynamic texture recognition using local\\nbinary patterns with an application to facial expressions,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915–928, Jun. 2007.\\n[32] P. Viola and M. J. Jones, “Robust real-time face detection,” Int. J.\\nComput. Vis. , vol. 57, no. 2, pp. 137–154, 2004.\\n[33] M. Pietikäinen, A. Hadid, G. Zhao, and T. Ahonen, “Local binary pat-\\nterns for still images,” in Computer Vision Using Local Binary Patterns ,\\nvol. 40. London, U.K.: Springer, 2011, pp. 13–47.\\n[34] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recognition\\nbased on local binary patterns: A comprehensive study,” Image Vis.\\nComput. , vol. 27, no. 6, pp. 803–816, 2009.\\n[35] R. Salakhutdinov, A. Mnih, and G. Hinton, “Restricted Boltzmann\\nmachines for collaborative ﬁltering,” in Proc. 24th Int. Conf. Mach.\\nLearn. , Corvallis, OR, USA, 2007, pp. 791–798.\\n[36] A. Ng, “Sparse autoencoder,” CS294A Lecture Notes , Stanford Univ.,\\nStanford, CA, USA, 2011, p. 72.\\n[37] A. Ng. (2011). Sparse Autoencoder Exercise . Accessed on Jun. 30, 2014.\\n[Online]. Available: http://uﬂdl.stanford.edu/wiki/index.php/\\nExercise:Sparse_Autoencoder\\n[38] Y . Ito and S. Omatu, “Category classiﬁcation method using a self-\\norganizing neural network,” Int. J. Remote Sens., vol. 18, no. 4,\\npp. 829–845, 1997.\\n[39] F. Giacco, C. Thiel, L. Pugliese, S. Scarpetta, and M. Marinaro,\\n“Uncertainty analysis for the classiﬁcation of multispectral satelliteimages using SVMs and SOMs,” IEEE Trans. Geosci. Remote Sens.,\\nvol. 48, no. 10, pp. 3769–3779, Oct. 2010.\\n[40] Z. Li and J. R. Eastman, “Commitment and typicality measures\\nfor the self-organizing map,” Int. J. Remote Sens., vol. 31, no. 16,\\npp. 4265–4280, 2010.\\n[41] H. Han, X.-L. Wu, and J.-F. Qiao, “Nonlinear systems modeling\\nbased on self-organizing fuzzy-neural-network with adaptive compu-\\ntation algorithm,” IEEE Trans. Cybern. , vol. 44, no. 4, pp. 554–564,\\nApr. 2014.\\n[42] C.-H. Wang, C.-Y . Chen, and K.-N. Hung, “Toward a new task assign-\\nment and path evolution (TAPE) for missile defense system (MDS) usingintelligent\\nadaptive SOM with recurrent neural networks (RNNs),” IEEE\\nTrans. Cybern. , vol. 45, no. 6, pp. 1134–1145, Jun. 2015.\\n[43] T. Kohonen, “The self-organizing map,” Proc. IEEE , vol. 78, no. 9,\\npp. 1464–1480, Sep. 1990.\\n[44] P. Lucey et al., “The extended Cohn–Kanade dataset (CK+): A\\ncomplete dataset for action unit and emotion-speciﬁed expression,”\\ninProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.\\nWorkshops (CVPRW) , San Francisco, CA, USA, 2010, pp. 94–101.\\n[45] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, “Web-based\\ndatabase for facial expression analysis,” in Proc. IEEE Int. Conf.\\nMultimedia Expo (ICME) , Amsterdam, The Netherlands, Jul. 2005,\\npp. 317–321.[46] Z. Wang, S. Wang, and Q. Ji, “Capturing complex spatio-temporal rela-\\ntions among facial muscles for facial expression recognition,” in Proc.\\nIEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Portland, OR, USA,2013, pp. 3422–3429.\\n[47] M. Liu, S. Shan, R. Wang, and X. Chen, “Learning expressionlets on\\nspatio-temporal manifold for dynamic facial expression recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Columbus,OH, USA, 2014, pp. 1749–1756.\\n[48] J. Li and E. Y . Lam, “Facial expression recognition using deep neural\\nnetworks,” in Proc. IEEE Int. Conf. Imaging Syst. Techn. (IST) , 2015,\\npp. 1–6.\\n[49] S. Elaiwat, M. Bennamoun, and F. Boussaid, “A spatio-temporal\\nRBM-based model for facial expression recognition,” Pattern Recognit. ,\\nvol. 49, pp. 152–161, Jan. 2016.\\n[50] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper in facial\\nexpression recognition using deep neural networks,” in Proc. IEEE\\nWinter Conf. Appl. Comput. Vis. (WACV) , Lake Placid, NY , USA, 2016,\\npp. 1–10.\\n[51] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, “Labeled\\nfaces in the wild: A database for studying face recognition in uncon-strained environments,” Dept. Comput. Sci., Univ. Massachusetts,Amherst, MA, USA, Tech. Rep. 07-49, 2007.\\n[52] K. Seshadri and M. Savvides, “Towards a uniﬁed framework for pose,\\nexpression, and occlusion tolerant automatic facial alignment,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 38, no. 10, pp. 2110–2122,\\nOct. 2016.\\n[53] S. Boucenna, P. Gaussier, P. Andry, and L. Hafemeister, “A robot\\nlearns the facial expressions recognition and face/non-face discrimi-nation through an imitation game,” Int. J. Soc. Robot. , vol. 6, no. 4,\\npp. 633–652, 2014.\\nAnima Majumder received the B.Tech. degree from\\nthe North Eastern Regional Institute of Science\\nand Technology, Itanagar, India, in 2005, and the\\nM.Tech. degree from the Shri Guru Govind SinghjiInstitute of Engineering and Technology, Nanded,India, in 2007.\\nShe was a Senior Software Engineer with Robert\\nBosch Engineering and Business Solutions Ltd.,Bengaluru, India. She is currently a Ph.D. Scholarwith the Department of Electrical Engineering,\\nIndian Institute of Technology Kanpur, Kanpur,\\nIndia. Her current research interests include computer vision, machine learn-\\ning, and image processing.\\nLaxmidhar Behera (S’92–M’03–SM’03) received\\nthe B.Sc. and M.Sc. degrees in engineering fromthe National Institute of Technology Rourkela,Rourkela, India, in 1988 and 1990, respectively,\\nand the Ph.D. degree from the Indian Institute of\\nTechnology Delhi, New Delhi, India, in 1996.\\nHe is currently a Professor with the Department\\nof Electrical Engineering, Indian Institute of\\nTechnology Kanpur, Kanpur, India. He was a\\nReader with the Intelligent Systems Research Center,University of Ulster, U.K., from 2007 to 2009. He\\nwas also a Visiting Researcher/Professor with FHG, Germany, and ETH\\nZurich, Zürich, Switzerland. He has over 200 papers to his credit published\\nin refereed journals and presented in conference proceedings. His currentresearch interests include intelligent control, robotics, semantic signal/music\\nprocessing, neural networks, control of cyber-physical systems, and cognitive\\nmodeling.\\nVenkatesh K. Subramanian received the B.E.\\ndegree from Bangalore University, Bengaluru, India,\\nin 1987, and the M.Tech. and Ph.D. degrees from\\nthe Indian Institute of Technology Kanpur (IITK),Kanpur, India, in 1989 and 1995, respectively.\\nHe is currently a Professor with the Department\\nof Electrical Engineering, IITK. His current researchinterests include signal processing, image and videoprocessing, signal and system theory, and computer\\nvision with applications in robotics.\\n',\n",
       " '1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nIEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  1 \\n Automatic  Facial Expression Recognition  \\nUsing Features of Salient Facial  Patches  \\nS L Happy , Student Member , IEEE , and Aurobinda Routray,  Member, IEEE  \\nAbstract — Extraction of discriminative feature s from salient facial patches plays a vital role in effectiv e facial expression \\nrecognition. The accurate detection of  facial landmark s improves the localization of the salient patch es on face images . This paper  \\npropose s a novel framework for expression recognition by using appearance features of selected facial  patches . A few prominent  \\nfacial  patches , depending on the position of facial landmarks,  are extracted  which are active during emotion  elicitation . The se \\nactive  patches are further processed  to obtain  the salient patches which contain discriminative features  for classification of  each \\npair of expressions , thereby selecting different facial patches as salient for different pair of expression classes . One-against -one \\nclassification method is adopted using these features . In addition, an automated learning -free f acial landmark detection technique  \\nhas been proposed , which  achieve s similar performances as that of other state -of-art landmark detection methods , yet requires \\nsignificantly less execution time . The proposed method is found to perform  well consistently in different resolutions , hence, \\nproviding a solution for expression recognition in low resolution images . Experiments o n CK+ and JAFFE  facial expression \\ndatabase s show the effectiveness of the proposed system . \\nIndex Terms —Facial expression analysis , facial landmark detection, feature selection,  salient facial patches , low resolution \\nimage .  \\n——————————    \\uf075   ——————————  \\n1 INTRODUCTION\\nacial expression, being a fundamental mode of com-\\nmunicating human emotions, finds its applicati ons in \\nhuman -computer interaction (HCI), health -care, sur-\\nveillance, driver safety, deceit detection etc. T remendous \\nsuccess being achieved in the field s of face detection and \\nface recognition, affective computing has received sub-\\nstantial  attention among th e researchers in the domain of \\ncomputer  vision.  Signals, which can be used for affect \\nrecognition , include facial expression, paralinguistic fea-\\ntures of speech, body language, physiological signals (e.g. \\nElectromyogram (EMG), Electrocardiogram (ECG), Elec-\\ntrooculogram (EOG), Electroencephalography (EEG), \\nFunctional Magnetic Resonance Imaging (fMRI) etc.). A re-\\nview of signals and methods  for affective computing  is re-\\nported in [1], according to which, most of the research on \\nfacial  expression analysis are based on detection of basic \\nemotions  [2]: anger, fear, disgust, happiness, sadness, and \\nsurprise. A number of  novel methodologies for facial ex-\\npression recognition  have  been proposed  over the last dec-\\nade.  \\nEffective  expression analysis hugely depends upon the \\naccurate repre sentation  of facial features . Facial Action \\nCoding System (FACS)  [3] represents face by measuring  all \\nvisually observable facial movements in terms of Action \\nUnits (AUs)  and associates them  with the facial expres-\\nsions.  Accurat e detection of AUs depends upon proper \\nidentification and tracking of different facial muscles irre-spective of pose, face shape, illumination, and image reso-\\nlution . According to Whitehill et al. [4], the detection of all \\nfacial  fiducial points is even more challenging than expres-\\nsion recognition itself.  Therefore, most of the existing algo-\\nrithms are ba sed on geometric  and appearance based fea-\\ntures. The models  based on geometric features  track the \\nshape and size of the face and f acial components such as \\neyes, lip corners , eyebrows  etc., and categorize the expres-\\nsions  based on relative position of these  facial components . \\nSome researchers ( e.g., [5], [6], [7], [8]) used shape models \\nbased on a set of characteristic points on the face to classify \\nthe expressions. However, these  methods usually require \\nvery accurate and reliable detection as well as  tracking of \\nthe facial landmarks  which are difficult to achieve  in many \\npractical situations. Moreover, the distance between  facial \\nlandmarks vary from person to person, thereby  making the \\nperson independent expression recognition system less re-\\nliable.  Facial expressions involve  change in local texture. In \\nappearan ce-based methods  [9], a bank of filters  such as Ga-\\nbor wavelets, Local Binar y Pattern (LBP) etc.  are applied to \\neither the whole -face or specific face regions to encode the  \\ntexture . The super ior performance of appearance based \\nmethods to the geometry  based features  is reported in [4]. \\nThe appearance -based methods generate s high dimen-\\nsional vector whi ch are further represented in lower di-\\nmensional subspace by applyi ng dimensionality reduction \\ntechnique s, such as  principal component analysis (PCA),  \\nlinear discriminant  analysis (LDA)  etc. Finally, the classifi-\\ncation is performed in learned subspace.  Although the \\ntime and space costs are higher  in appearance based meth-\\nods, the preserv ation of discriminative inform ation makes \\nthem  very popular.  \\nExtraction of facial features by dividing the face region \\ninto sev eral blocks achieves better accuracy  as reported  by \\nxxxx-xxxx/0x/$xx.00 © 2014  IEEE  F \\n————————————————  \\n\\uf0b7 S L Happy and A. Routray are with the Department of Electrical Engineer-\\ning, Indian Institute of Technology, Kharagpur, India.  \\nE-mail: {happy, aroutray}@iitkgp.ac.in . \\n \\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n2 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n many researchers  ( [9], [10], [11], [12], [13], [14], [15]). How-\\never, this approach fails with improper face alignm ent and \\nocclusions.  Some e arlier works  [16], [17] on extraction of \\nfeatures from specific face regions  mainly determine  the \\nfacial regions which contributes more toward discrimina-\\ntion of expressions  based on the training data. However, in \\nthese approaches, the positions and sizes of the facial \\npatches vary  according to the training data . Therefore, it is \\ndifficult to conceive a generic system  using these ap-\\nproaches. In this paper, we propose  a novel facial land-\\nmark detection technique as well as a salient patch based \\nfacial expression recognition framework with significant \\nperformance  at different image resolutions.  The proposed  \\nmethod localizes face as well as the facial landmark points  \\nin an image , thereby extracting some salient patches that \\nare estimated during training stage. The  appearance fea-\\ntures from these patches are fe d to a multi -class classifier \\nto classify the images into six basic expression classes . It is \\nfound that t he proposed facial landmark detection system \\nperforms similar to the state -of-the-art methods  in near \\nfrontal images  with lower  computational complexity. The \\nappearance features with lower number of  histogram bins \\nare used to reduce the computation. Empirically the salient \\nfacial patches are selected  with predefined positions and \\nsizes, which  contribute significantly toward s classification  \\nof one expression from others.  Once the salient patches are \\nselected, the e xpression recognition becomes eas y irrespec-\\ntive of  the data . Affective computing aims at effective emo-\\ntion recognition in low resolution images. The experi-\\nmental results shows that the proposed system performed \\nbetter in low resolution images.  \\nThe paper is organized as follows. Section 2 presents a \\nreview of earlier w orks. The proposed framework is  pre-\\nsented in Section 3 . Section 4 and 5 discusses the facial \\nlandmark detection and feature extraction technique re-\\nspectively. Experimental results and discussion are pro-\\nvided in Section 6. Section  7 concludes the paper.  \\n2 RELATED WORK \\nFor better performance in facial expression recognition, \\nthe importance  of detection of facial landmark s is undeni-\\nable.  Face alignment is a n essential step  and is usually car-\\nried out by detection and horizontal positioning of eyes.  \\nFacial landmar k detection is followed by feature extrac-\\ntion. Selection of feature s also affects the classification ac-\\ncuracy.  In [18], an active Infra -Red illumination along with \\nKalman filtering is used for accurate  tracking facial com-\\nponent s. Performance is improved by the use of both geo-\\nmetric and appearance features. Here the initial positions \\nof facial landmarks are figured out using face geometry, \\ngiven the position of eyes, which is not convenient. Tian et \\nal. [19] also used relative distance ( lip corner, eye, brow \\netc.) and transient features (wrinkles, furrows etc.) for rec-\\nognizing AUs present in lower face. However, the use of \\nCanny edge detector for extracting appearance features is \\nnot flexible in differ ent illumination and determining the \\npresence of furrows using threshold is uncertain.  Uddin et \\nal. [20] report ed good performance by using image differ-\\nence method for observing changes in expressions . The major issue is  the la ndmark selection  which  is carried out  \\nmanually by matching the eye and mouth regions. In [21], \\na relative geometrical distance based approach is de-\\nscribed which uses computationally expensive Gabor fil-\\nters for landmark detectio n and tracking. They used com-\\nbined SVM and HMM models as classifiers . \\nDeformable models, to fit into new data instances, have \\nbecome popular for facial landmark detection. Active \\nshape models (ASM) determine shape, scale and pose by \\nfitting an appropriate point distribution model (PDM) to \\nthe object of interest. Active appearance models (AAM)  \\n[22] combines both shape and texture models to represent \\nthe object, hence providing superior result to ASM. AAM \\nis widely used (  [23], [24], [25], [26], [27]) for detection and \\ntracking of non -rigid facial landmarks. However, its per-\\nformance is poor in person independent scenarios. Manual \\nplacement of the landmark points in training data for con-\\nstruction of the shape model is a  tedious task and  time con-\\nsuming process  in these models. Constrained Local Model \\n(CLM) framework proposed by Cristina cce et al. [28] has \\nbeen proved as a better tool for person independent facial \\nlandmark detection. All the above  said deformable models  \\nuse PCA to learn the variability of shapes  and textures  of-\\nfline. CLM algorithm is further m odified by Saragih et al. \\n[29] who proposed Regularized Landmark Mean Shift  \\n(RLMS) algorithm with improved landmark localization \\naccuracy. Asthana et al. [30] proposed Discriminative Re-\\nsponse Map Fi tting (DRMF) method for the CLM frame-\\nwork for the generic face fitting scenario in both controlled \\nand natural imaging conditions.  Though satisfactory re-\\nsults has been  achieved using these deformable models, \\nhigh computational cost  is an obstacle in using them  in \\nreal-time applications . Chew et al. [31] established the fact \\nthat, appearance based models work robustly even with \\nsmal l alignment errors, and perform  the same as that of a \\nclose to perfect alignment.  Therefore, slight  error in land-\\nmark detection will not hamper the purpose.  In our exper-\\niments, we used a computationally inexpensive learning -\\nfree meth od for landmar k detection that serves the pur-\\npose as efficiently as  recent DRMF based CLM method  \\n[30]. \\nAn effective feature ideally discriminates between the \\nexpressions while minimizing the intra -class variance, and \\nshould be easily extracted from raw images of different \\nresolutions. Among the appearance features, Gabor -wave-\\nlet representations ha ve been widely adopted in face image \\nanalysis  [32], [33] due to their superior performance. How-\\never, the computation of Gabor -features  is both time and \\nmemory intensive; besides, they are sensitive to scaling . \\nRecently the Local Binary Patterns (LBP) proved them-\\nselves  as an effective appearance features for facial image \\nanalysis  [10], [34], [35]. Jabid et al. [11] developed local fa-\\ncial descriptor based on Local Description Patterns  (LDP) \\ncodes and obtained  better performance  than LBP features. \\nRecently Dhall et al. [36] reported higher performance of \\nLocal P hase Quantization  (LPQ) in facial expression recog-\\nnition.  In [12], Local Directional Pattern Variance (LDPv) \\nis proposed which encodes contrast information using lo-\\ncal va riance of directional responses.  However, S han et al. \\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  3 \\n [37] found LBP features  to be robust for analysis of low res-\\nolution images. Therefore, w e used  the LBP histogram s as \\nappearance features .  \\nPCA  ( [38], [39]) and LDA (  [40], [41], [42]) are used as a \\ntool for dimensionality reduction  as well as classification \\nin expression  recognition. In [43], authors reported the \\nhigher performance of PCA -LDA fusion method.  An en-\\ncrypted domain based facial expression recognition sys-\\ntem is proposed in [44] which uses local fisher discriminant  \\nanalysis to achieve  accuracy as good as in normal images. \\nExpression subspace is introduced in [45] which explains \\nthat the same expressions lie on the same subspace and \\nnew expressions can be generated from one image by pro-\\njecting it into different emotion subspaces.  \\nMost of the prop osed methods use  full face image, \\nwhile a few use  features extracted from specific facial \\npatches. In [13], face image is divided into several sub re-\\ngions  (7x6) and local features (7x6x59  dimensional fea-\\ntures ) are extracted. Th en, the discriminative LBP histo-\\ngram bins are selected by using Adaboost technique for \\noptimum classification. Similar approaches are reported in \\n[14], [15], and [46]. In such cases, s mall misalignment \\nwould cause displacement of the sub region  locations, \\nthereby increas ing error in classification. Moreover,  for dif-\\nferent person s the size and shape of  facial organs are not \\nthe same, so , it cannot be assured that the same facial posi-\\ntion always present in one particular block in all images. \\nHence, local patch selection based approach is adopted  in \\nour experiments . In [47], authors divided the face into 64 \\nsub regions  and explored the common faci al patches which \\nare active for most expressions and special facial patches \\nwhich are active for specific expressions. Using multi task \\nsparse learning method, they used feature s of a few num-\\nber of facial patches to classify facial expressions . Song et \\nal. [16] used eight facial patches based on specific landmark  \\nposition s to observe the skin deformations caused  by ex-\\npressions . The authors have  used binary classifiers to gen-\\nerate a Boolean variable for presence or absence of ski n \\nwrinkles. However, the se patches do not include the tex-\\nture of lip corners, which is important for expression  \\nrecognition . Moreover, the occlusion of forehead by hair may result in false recognition.  In [17], authors extracte d \\nGabor features of different scales from the face image and \\ntrained using Adaboost to select the salient patches for \\neach expression. However, the salient patch size and posi-\\ntion is different when trained with different databases. \\nTherefore, a unique crit eria cannot be established for \\nrecognition of expressions in unknown images.  \\nSome issues related to r eal-time detection of facial land-\\nmarks and expression recognition  remain unaddressed so \\nfar. Most of the research es in this field  are carried out on \\ndifferent datasets with  suitable performance criteria befit-\\nting to the database . For example , selection of prominent \\nfacial areas improves the performance . However , in most \\nof the literature , the si ze and position of these facial  \\npatches are reported to be different for different databases. \\nTherefore, our experiments attempt to identify  the salient \\nfacial areas  having generalized discriminative features for \\nexpression classification . Selection of sal ient patches re-\\ntaining  discriminating  features between each pair of facial \\nexpressions improve d the accuracy . The size and location \\nof patches are  kept same for different databases for the \\npurpose of generalization. In addition, the proposed \\nframework has the potential to recognize expressions in \\nlow-resolution images.  \\n3 PROPOSED METHODOLOGY  \\nChange s in facial expression s involve contraction and \\nexpansion of facial muscles which alters the position of fa-\\ncial landmarks. Along with the facial muscles, the textur e \\nof the area also changes. This paper attempts to  under-\\nstand the contribution of different facial areas toward au-\\ntomatic expression recognition.  In other words, the paper \\nexplores  the facial patches which generates discriminative \\nfeatures to separate two expressions  effectively .  \\nThe overview of the proposed method is shown in Fig. \\n1. Observations from  [47], [16] suggest that  accurate facial \\nlandmark detection and extractio n of appearance features \\nfrom active face regions improve  the performance  of ex-\\npression recognition . Therefore , the first ste p is to localize \\n \\nFig. 1. Overview of the proposed system  \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n4 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n the face followed by detection of  the landmarks.  A learn-\\ning-free approach  is proposed  in which the eyes and nose \\nare detected  in the face image  and a coarse region of inter-\\nest (ROI) is marked around  each . The l ip and eyebrow cor-\\nners are detected from respective  ROIs. Locations of active \\npatches are defined with respect to the  location of land-\\nmarks.  Fig. 2 shows the steps involved in automated facial \\nlandmark detection and active patch extraction. In training \\nstage, all the active facial patches are evaluated and the \\nones  having  features  of maximum variation  between pair s \\nof expressions are sel ected . These selected  features are fur-\\nther projected into lower dimensional subspace and classi-\\nfied into different expressions using  a multi -class classifier.  \\nThe training phase includes  pre-processing , selection of fa-\\ncial patches,  extraction of  appearance  feature s and learning \\nof the  multi -class  classifiers . In an unseen image, the pro-\\ncess first  detects the facial landmarks, then extracts the fea-\\ntures from the selected salient patches , and finally classi-\\nfies the expressions.  \\n4 FACIAL LANDMARK DETECTION  \\nThe facial patches which are active during different fa-\\ncial expression s are studied  in [47]. It is reported that  some  \\nfacial patches are common  during elicitation of all basic ex-\\npressions and some are confined to a single expression . \\nThe result s indicate  that these active patches are positioned  \\nbelow  the eye s, in between the eyebrows, around  the nose \\nand mouth corners. To extract these patches from face im-\\nage, we need to locate the facial components  first followed \\nby the extraction o f the patches around these organs. Un-zueta et al . [48] proposed a robust, learning -free, light-\\nweight generic face mod el fitting method for localization \\nof the facial organs. Using local gradient analysis, this \\nmethod finds the facial features and adjusts the deforma-\\nble 3D face model so that its projection on image will match \\nthe facial feature points. In this paper,  such a learning -free \\napproach wa s adopted  for localization of facial landmarks.  \\nWe have extracted  the active  facia l patches with respect to \\nthe position of eyes, eyebrows, nose , and lip corners  using \\nthe geometrical statistics  of the face.  \\n4.1 Pre-processing  \\nA low pass filtering was performed using a 3x3 Gauss-\\nian mask to remove noise from the facial images followed \\nby face detection for face localization. We used Viola -Jones \\ntechnique  [49] of Haar -like features w ith Adaboost learn-\\ning for face detection . It has lower computational complex-\\nity and was sufficiently accurate for detection of near -\\nfrontal and near -upright face images. Using integral image \\ncalculation, it can detect face regardless of scale and loca-\\ntion in real time . The localized  face was extracted and \\nscaled to bring it to a common resolution. This made  the \\nalgo rithm  shift invariant, i.e. insensitive to the location of \\nthe face on image. Histogram equalization was carried out \\nfor lighting corrections . \\n4.2 Eye and Nose Localization  \\nTo reduce the computational complexity  as well as the \\nfalse detection rate , the coarse region of interest s (ROI) for \\neyes and nose were selected  using geometrical positions  of \\nface. Both the eyes were detected separately using Haar  \\nclassifier s trained for each eye . The H aar classifier returns  \\nthe vertices  of the rectangular area of detected eyes . The \\neye centers are computed as the mean of these coordinates . \\nSimilarly, nose position was also detected using Haar  cas-\\ncades. In our expe riment, for more than 9 8% cases these \\nparts  were detected properly . In case the eyes or nose was \\nnot detected using Haar classifier s, the system relies on the \\nlandmark coordinates detected by anthropometric  statis-\\ntics of face . The position of eyes were used for up-right face \\nalignment  as the position s of eyes do not change with facial \\nexpressions.  \\n4.3 Lip Corner Detection  \\nInspired by the work of Nguyen et al. [50], we used fa-\\ncial topographies  for detection of lip and eyebrow corners. \\nThe ROIs for lips and eyebrows were selected as a function \\nof face width positioned with respect to the facial organs. \\nThe ROI for mouth was extracted using the position of  \\nFig. 2. Framework for automated facial landmark detection and active patch extraction, (a) face detection , (b) coarse ROI selection for eyes \\nand nose, (c) eyes and nose detection followed by coarse ROI selection for eyebrows and lips, (d) detection of corners of lip  and eyebrows, \\n(e) finding the facial landmark locations, (f) extraction of active facial patch es. \\n \\nFig. 3. Lip corner localization, (a) lips ROI, (b) applying horizontal \\nSobel edge detector, (c) applying Otsu threshold, (d) removing spu-\\nrious, (e) applying morphological operations to render final con-\\nnected component for lip corner localization.  \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  5 \\n nose as reference  (Fig. 3a). The upper  lip always produce s \\na distinct edge  which can be detected using a horizontal \\nedge detector. Sobel edge detector  [51] was used for this \\npurpose. In images with different expressions, a lot of \\nedge s were obtained which was further  threshold  by using \\nOtsu  method  [52]. In this process, a binary image was ob-\\ntained  containing many connected regions. Using con-\\nnected component analysis, the spurious components hav-\\ning an area less than a threshold were removed . Further, \\nmorphological dilation operation was carried out on the \\nresulting  binary image. Finally, the connected component \\nwith largest area which was just below the nose region was \\nselected as upper lip region. Fig. 3 shows diff erent stages \\nof the process . The  algorithm steps are  given  below.  \\nAlgorithm 1. Lip corner detection  \\nGiven:  aligned face ROI and nose position  \\n 1: select coarse lips ROI using face width  and nose position  \\n 2: apply Gaussian blur to the lips ROI \\n 3: apply  horizontal sobel operator for edge detection  \\n 4: apply Otsu -thresholding  \\n 5: apply morphological dilation operation  \\n 6: find the connected components  \\n 7: remove the spurious connected components using \\nthreshold technique  to the number of pixels  \\n 8: scan the image from the top  and select  the first con-\\nnected component as upper lip position  \\n 9: locate the left  and right most positions of connected \\ncomponent as lip corners  \\nSometimes, due to shadow below the nose, the upper \\nlip could not be segmented properly . A case is shown in \\nFig. 4. In such cases, the upper lip wa s not segmented as a \\nwhole  and the connected component obtained at th e end \\nresembled  half of the upper lip . Hence, the extreme ends \\nof this connected component  did not satisfy the bilateral \\nsymmetry property , i.e. the lip corners should have been  at \\nmore or less equal distance s from vertical central line of \\nface. These situ ations we re detected by putting a threshold \\nto the  ratio of  distance between the  lip corners to the max-\\nimum of distance s of the lip corner s from the vertical cen-\\ntral line . In such cases, the second connected component \\nbelow the nose  was considered as the o ther part of upper \\nlip. Thus the lip corners we re detected with the help of  two \\nconnected components.  By using the above said methods, \\nfalse detection of lip corner points we re minimized.   \\n4.4 Eyebrow  Corner  Detection  \\nWith the knowledge of position s of eyes, t he coarse \\nROIs of eyebrows were selected. The eyebrow s were de-tected following the same steps a s that of upper lip detec-\\ntion. However, we observed that performing an adaptive \\nthreshold operation before applying horizontal sobel oper-\\nator improve d the accura cy of eyebrow corner localization.  \\nThe use of horizontal edge detector reduce d the false de-\\ntection of eyebrow position s due to partial occlusion by \\nhair. The inner eyebrow  corner  was detected accurately in \\nmost of the images . Fig. 5 shows intermediate steps in eye-\\nbrow corner detection.  \\n4.5 Extraction of Active Facial Patches  \\nDuring an expression, t he local patches we re extracted \\nfrom the face image depending upon the position o f active \\nfacial muscles. We have considered the appearance of fa-\\ncial regions  exhibiting considerable variations  during one \\nexpression. For example, wrinkle in upper nose region is \\nprominent in disgust expression and absent in other ex-\\npressions. Similarly, regions around lip corners undergo \\nsignificant change s and its appearance features are dissim-\\nilar for different expressions. From our observations, sup-\\nported by the research of Zhong et al. [47], we used the ac-\\ntive facial patch es as shown in Fig. 6 for our experiment.  \\nFig. 4. Lip corner localization in a case where upper lip is not entirely \\nconnected , (a-c) same as Fig. 3, (d-e) selection of two connected \\ncomponents by scanning from top, (f) localized lip corner.  \\n \\nFig. 5. Eyebrow corner localization, (a) rectangles showing search ROI \\nand plus marks showing the detection result, (b & f) eye ROIs, (c & g) \\napplying adaptive threshold on ROIs, (d & h) applying horizontal sobel \\nedge detector followed by Otsu threshold and morphological operations, \\n(e & i) final connected components for corner  localization.  \\n \\nFig. 6. Position of facial patches  \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n6 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n The patches does not have  very  fixed position on the face \\nimage. Rather, their location depends upon the positions \\nof facial landmarks. The size of all facial patches were kept \\nequal and w as approximately one -ninth  of the width of the \\nface. Here onwards, we will refer the patches by the num-\\nbers assigned to it. As shown in Fig. 6, 𝑃1, 𝑃4, 𝑃18, and  𝑃19 \\nwere directly extracted from the positions of lip corners \\nand inner eyebrows respectively.  𝑃16 was at the center of \\nboth the eyes; and 𝑃17 was the patch above  𝑃16. 𝑃3 and  𝑃6 \\nwere located in the midway of eye and nose.  𝑃14 and  𝑃15 \\nwere located just below eyes. 𝑃2, 𝑃7, and 𝑃8 were clubbed \\ntogether and located at one side of nose position. 𝑃9 was \\nlocated just below 𝑃1. In a similar  fashion  𝑃5, 𝑃11, 𝑃12, \\nand  𝑃13 were located. 𝑃10 was located at the center of posi-\\ntion of 𝑃9 and 𝑃11. \\n5 FEATURE EXTRA CTION  AND CLASSIFICATION  \\nLBP wa s widely used as a robust i llumination invariant \\nfeature descriptor. This operator generates a binary num-\\nber by comparing the neighbouring  pixel values with the \\ncenter pixel value  [53]. The pattern with 8 neighborhoods \\nis given by  \\n𝐿𝐵𝑃 (𝑥,𝑦)=∑𝑠7\\n𝑛=0(𝑖𝑛−𝑖𝑐)2𝑛 \\nwhere  𝑖𝑐 is the pixel value at coordinate (𝑥,𝑦) and 𝑖𝑛 are the \\npixel value s at coordinate s in the neighborhood of  (𝑥,𝑦), \\nand  \\n𝑠(𝑥)={1,𝑥≥0\\n0,𝑥<0 \\nThe histogram s of LBP image  can be utilize d as feature  \\ndescriptors , given by  \\n𝐻𝑖=∑𝐼\\n𝑥,𝑦{ 𝐿𝐵𝑃 (𝑥,𝑦)=𝑖}, 𝑖=0,1,…,𝑛−1 \\nwhere 𝑛 is the number of labels produced by LBP operator. \\nUsing different bin width s, the histograms  can be grouped \\nto discover  different features . For instance, LBP with 8 \\nneighboring points produces 256 labels. If we collect its \\nhistogram s in 32 bins, then  we are basically grouping the \\npatterns [0,7], [8,15],  [16,23],  …, and [248,255]  together.  \\nThis is same as ignoring the least significant bits of the un-\\nsigned integ er, i.e. using patterns of one side of local neigh-\\nborhood . Fig. 7 shows the pattern generated due to 32 his-\\ntogram bins where the upper row neighbors do not con-\\ntribute towards the pattern label . We used 16, 32 and 256 \\nbin histogram s in the experiments.   \\nIn addition, uniform LBP and rotation invariant uni-\\nform LBP values  [53] are also used in our experiment  and \\ntheir performance s are compare d. Uniform ity measure  (𝑈) \\ncorresponds to the number of bitwise transition s from 0 to \\n1 or vice-versa  in a pattern  when the bit pattern is traversed circularly . For instance, the pattern (0000 0001 )2 and \\n(00100110) 2 have 𝑈 values  2 and 4 respectively. The pattern \\nis called uniform  (LBPu2) when  𝑈≤2. This reduces the \\nleng th of the 8-neighborhood patterns  to 59 -bin histo-\\ngrams . The effect of rotation can be removed by assigning \\na unique identifier to each rotation invariant pattern , given \\nby  \\n𝐿𝐵𝑃𝑟𝑖𝑢 2={∑𝑠7\\n𝑛=0(𝑖𝑛−𝑖𝑐),𝑖𝑓 𝑝𝑎𝑡𝑡𝑒𝑟𝑛  𝑖𝑠 \"𝑢𝑛𝑖𝑓𝑜 𝑟𝑚\"\\n9,          𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 \\n \\nThus, the rotational invariant  uniform LBP with 8 neigh-\\nborhood produces  10 histogram bins.   \\n5.1 Learning Salient Facial Patches Across \\nExpressions  \\nIn most of the literature s, all the facial features are con-\\ncatenated to rec ognize the expression. However, this gen-\\nerates a feature vector  of high dimension . We observed \\nthat the features from a fewer  facial patches can replace the \\nhigh  dimensional features  without significant dimin ution \\nof the recognition accuracy . From human pe rception, not \\nall facial patches are responsible for recognition of one ex-\\npression. The facial patches responsible for recognition of \\neach expression can be  used separately to recognize that \\nparticular expression. Based on this hypothesis, we evalu-\\nated the  performance of each facial patch for recognition of \\ndifferent expressions.  \\nFurther , some expressions share similar movements  of \\nfacial muscles;  features of such patches are redundant  \\nwhile classifying the expressions . Therefore, after extract-\\ning the activ e facial patches, we selected the salient facial \\npatches responsible for discrimination between  each pair \\nof basic expressions. A facial patch is considered to be dis-\\ncriminative between two expressions, if the features ex-\\ntracted from this patch can classif y the two expressions ac-\\ncurately. Note that not all active  patches are salient for \\nrecognition of all expressions. For all possible pair of ex-\\npressions ( \\u200a6C2 ), all the 19 active patches were evaluated by \\nconducting  a ten-fold cross validation test . The patches \\nthat result maximum discrimination  were selected  for rep-\\nresenting the expressions . \\nThe LBP histogram features in lower resolution images \\nare sparse  in nature because of the smaller patch area. LDA \\nwas applied for projecting the se features to th e discrimi-\\nnating dimensions and to choose the salient patches ac-\\ncording to their discriminative performance . LDA finds \\nthe hyper -plane that minimizes the intra -class scatter ( 𝑆𝑤), \\nwhile maximizing the inter -class scatter ( 𝑆𝑏). It is also used \\nas a tool for interpretation of importance of the features. \\nHence  it can be considered as a transformation into a lower \\ndimensi onal space  for optimal discrimination between \\nclasse s. The intra -class scatter ( 𝑆𝑤) and inter -class scatter \\n(𝑆𝑏) are given by  \\n𝑆𝑏=∑𝑁𝑖𝑛\\n𝑖=1(𝑥𝑖̅−𝑥̅)(𝑥𝑖̅−𝑥̅)𝑇 \\n𝑆𝑤=1\\n𝑁𝑖−1∑∑(𝑥𝑖,𝑗−𝑥𝑖̅)(𝑥𝑖,𝑗−𝑥𝑖̅)𝑇𝑁𝑖\\n𝑗=1𝑛\\n𝑖=1  \\nFig. 7. Patterns generated by one side of local neighborhood which \\nproduces 32 histogram bins  \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  7 \\n where 𝑥𝑖,𝑗 is the 𝑗th feature vector in 𝑖th class. Here 𝑛 num-\\nber of classes, having 𝑁𝑖 number of images in 𝑖th class, \\nhave 𝑥̅, the mea n vector of all the training data, and 𝑥𝑖̅, the \\nmean of 𝑖th class . LDA aims at maximization of 𝑆𝑏 while \\nminimizing  𝑆𝑤, i.e. maximization of ratio of determinant of \\n𝑆𝑏to the determinant of  𝑆𝑤. \\nΦ𝑙𝑑𝑎=arg𝑚𝑎𝑥\\nΦ|Φ𝑇𝑆𝑏Φ|\\n|Φ𝑇𝑆𝑤Φ| \\nThis ratio is called as Fishers criterion. This can be com-\\nputed by s olving  the generalized eigenvalue  problem \\ngiven  as: \\n𝑆𝑏Φ𝑙𝑑𝑎−𝑆𝑤Φ𝑙𝑑𝑎Λ=0 \\n⇒𝑆𝑤−1𝑆𝑏Φ𝑙𝑑𝑎=Φ𝑙𝑑𝑎Λ \\nwhere Λ is the diagonal eige nvalue matrix and Φ𝑙𝑑𝑎 is the \\nset of discriminant vectors of 𝑆𝑏 and 𝑆𝑤 corresponding to \\nthe 𝑛−1 largest generalized eigenvalues. Thus, Fisher cri-\\nterion is maximized when the projection matrix Φ𝑙𝑑𝑎 is \\ncomposed of eigenvectors of  𝑆𝑤−1𝑆𝑏, subject to  𝑆𝑤 being  \\nnon-singular. As suggested by Belhumeur et al. [54], PCA \\nwas applied to the signal prior to  LDA. By doing so, the \\nsignal wa s projected to lower dimensional space assuring \\nthe non -singula rity of within class scatter matrix . Moreo-\\nver, this PCA -LDA fusion [43] improves the performance. \\nTherefore, we applied PCA on the training set for dimen-\\nsionality  reduction  followed by LDA .  \\nWe calculated the saliency of all facial patch es for all \\npair of expressions  and it wa s expressed in terms of sali-\\nency scores . The saliency of a patch represents  the ability \\nof the features from the patch  to accurately  classify  a pair \\nof expressions . The saliency score of a patch between a p air \\nof expressions is the classification accuracy of the features \\nfrom that patch  in classifying the two expressions . Here \\nPCA -LDA wa s used for classification purpose to deter-\\nmine the saliency  score. In a similar fashion, saliency score \\nof all patches for each pair of expressions we re calculated.  \\nWe used one -against -one strategy  for expression classifica-\\ntion purpose . While classifying between a pair of expres-\\nsions, the features were extracted  from those facial  patches  \\nwhich have high  saliency score.  The fea ture vector s from \\nthe salient patches we re concatenated to construct a higher \\ndimensional feature vector. Thus, the dimension of the fea-\\nture vector depends  upon the number of patches selected \\nfor classification purpose.   \\nWe applied PCA to reduce the dimens ionality of the \\nfeature  vector . Thus, by projecting the feature vector s from  \\nsalient patches  to the optimal sub -space  obtained by above \\nmethod, we can find the  lower dimensional vector  with \\nmaximum discrimination for different classes . The weight \\nvectors , corresponding to the salient patches of each pair \\nof expression  classes , generated during the training stage \\nwere used  during testing.  \\n5.2 Multi -class Classification  \\nSVM wa s used for classification of extracted features \\ninto differe nt expression categories. SVM [55] is a popular \\nmachine learning algorithm which maps the feature vector \\nto a different plane, usually to a higher dimensional plane , \\nby a non -linear mapping , and finds a linear decision  hyper \\nplane  for classification of tw o classes . Since SVM is a binary classifier, we implemented  one-against -one (OAO) tech-\\nnique  for multi -class classification  [56]. In OAO approach , \\na classifier is trained between each pair of classes; hence \\n\\u200a𝐾C2 number of  classifiers we re constructed in total , where \\n𝐾 is the number of classes . Using voting strategy, a vector \\ncan be classified to the class  having the highest number of \\nvotes.  After  sever al experiments with linear, polynomial , \\nand radial basis function (RBF)  kernel s, we selected  RBF \\nkernels  for its superior  classification performance .  \\n6 EXPERIMENTS AND DISCUSSION  \\nThe proposed method was evaluated by using two \\nwidely used facial expression databases , i.e., Japanese Fe-\\nmale Facial Expressions (JAFFE)  [57] and Cohn -Kanade  \\n(CK+) [58]. We have used ten -fold cross validation to eval-\\nuate the performance of the proposed method. As dis-\\ncussed earlier, face detection was carried out on all images \\nfollowed by scalin g to bring the face to a common resolu-\\ntion. Facial landmarks were detected and  salient facial \\npatches were extracted  from  each face image . During train-\\ning stage, a  SVM classifier wa s trained bet ween each pair \\nof expressions . Here the training  data we re the  concate-\\nnated LBP histogram features extracted from the salient \\npatches containing discriminative characteristics between \\nthe given pair of expression classes. Similarly, \\u200a6C2 numbers  \\nof SVM classifiers we re constructed and used for evaluat-\\ning the performance on the test -set. \\n6.1 Experiments on the Cohn -Kanade Database  \\nThe Cohn -Kanade database contains both male and fe-\\nmale  facial  expressi on image sequences  for the six basi c \\nemotions . In our experiments, the last image from each  se-\\nquence was selected where the expression is at its peak  in-\\ntensity . The number of instances for each expression  varies \\naccording to its availability. In our experiments on  CK+ da-\\ntabase, we used  329 images in total : anger (41), disgust (45), \\nfear ( 53), happiness ( 69), sadness ( 56), and surprise ( 65).  \\n \\nFig. 8. The recognition rate in images with different face resolutions \\nin CK+ database  0.80.820.840.860.880.90.920.94RECOGNITION ACCURACY\\nRESOLUTION16bin 32bin 256bin\\nLBPu2 LBPriu2\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n8 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n 6.1.1  Analysis  of histogram binwidth  and face \\nresolution  \\nWe determined the optimal resolution and bin width  of \\nthe histogram empirically. The expression recognition per-\\nformance of different feature vectors  were studied for face \\nresolution s starting from 48x48 to 192x192. The low reso-\\nlution images were o btained by down sampling the im-\\nages. The classifiers were trained and evaluated at differ-\\nent resolutions of face images. In our experiment s, we ob-\\nserved  minimum  accuracy of 82 % at 48x48 face resolution  \\nas shown in  Fig. 8. In [59], it is reported that with  a face \\nimage of 48x64  resolution , some  facial landmarks, such as, \\nlip and eye corners  are difficult to detect . Therefore, it is \\nuncertain if expressions can be recognized at this resolu-\\ntion. However, f rom our experiment, we obtained pretty \\ngood accuracy at all resolutions.  This establishes the ro-\\nbustness of  the appearance features extracted from the sa-\\nlient patches at different resolutions.  Since we have imple-\\nmented a voting method, the result is based on votes of \\u200a6C2 \\nclassifiers, threby, reducing  the classification error due to  \\na single classifier.  \\nWe also observed that the use of LBPu2 features  pro-\\nduced  better  accuracy compared to other features at all res-\\nolutions. The performance of the other f eature vectors are \\nalike . The uniform patterns removes the noisy estimates in \\nthe image by accumulating them into one histogram bin, \\nthereby increasing the recognition accuracy.  \\n6.1.2  Performance improvement : use of block - \\nhistograms  \\nBy implementing block -based feature extraction tech-\\nnique , more local f eature s were add ed to the feature vec-\\ntor. This process makes the  feature vector to be  a combina-\\ntion of local as well as global features.  Empirically we  ob-\\nserved that the performance wa s imp roved when each se-\\nlected patch wa s further divided into four equal blocks. In \\nour experiments, the feature vector was obtained by  con-\\ncatenatin g the  features obtained from each block  of the sa-\\nlient patch es and the results are  shown in  Fig. 9. It was ob-\\nserved that the 16 -bin histograms, 32 -bin hist ograms, 256 -\\nbin histograms, and uniform LBP features performed alike  \\nat all resolutions . We performed the experiments on a \\nstandard face resolution of 96x96 . At this r esolution, all the features  except LBPriu2  had similar performance s. Feature  \\nvector with low dimension reduces the computational \\ncomplexity . Therefore, we selected the histogram feature s \\nwith 16 -bins as the optimal trade -off between speed and \\naccuracy.   Table 1 shows t he confusion matrix of six emo-\\ntions based on the proposed method.  The quality of the \\noverall classification is evaluated  by calculating the  macro -\\naverage  [60] of precision, recall and F-score . The proposed \\nsystem attained a  balanced  F-score  of 94.39% with 94.1% \\nrecall  and 94.69%  precision.   \\nAs observed from Table 1, surprise expression achieved \\nbest recognition rate which is usually characterized by \\nopen mouth  and upward eyebrow movement . The system \\nperformed worst for anger expression and classification er-\\nror was maximum  between a nger and sadness  since they \\ninvolve similar and subtle changes.  Here onwards, all the \\nexperiments  are based on face resolution of 96x96.  \\n6.1.3  Optimum number of salient patches  \\nNumber of patches used for classification also affects \\nthe performance in terms of speed and accuracy. Fig. 10 \\nshows average of accuracies of all expressions  with respect \\nto the number of salient patches used for classification with \\na face resolution of  96x96.  It is apparent  from Fig. 10 that \\nthe use of features from all the 19 patches can classify all \\nexpressions with an accuracy of 93.87%.  It is clear that  even \\nthe use of  appearance features of a single salient patch can \\ndiscriminate between each pair  expressions efficiently  \\nwith  recognition rate of 91.19% . This implies that the use \\nof rest of the features from other patches contribu te mini-\\nmum toward s the discriminative features. More the num-\\nber of patches used, more is the size of  the feature vector. TABLE 1 \\nTHE CONFUSION MATRIX USING PROPOSED METHOD ON  \\nCK+ DATABASE  \\n \\n \\nFig. 9. Improvement in the recognition rate at different resolutions by \\nusing block histograms   \\nFig. 10. The recognition rate using different number of salient patches  \\n0.850.870.890.910.930.95RECOGNITION ACCURACY\\nRESOLUTION16bin 32bin 256bin\\nLBPu2 LBPriu2\\n0.890.90.910.920.930.940.950.96\\n12345678910111213141516171819Recognition accuracy\\nNumber of patches\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  9 \\n This  increases the computational burde n. Therefore, in-\\nstead of using all the facial patches , we can rely on some \\nsalient facial patches  for expression recognition. This will \\nimprove the computational complexity as well as  robust-\\nness of the features especially  when a face is partially oc-\\ncluded.  In our experiments , we used top four salient \\npatches in our experiment s which results an accurac y close \\nto 95%. Note that the combination of patches varies across \\ndifferent expression pairs . \\n6.1.4  Performance comparison  \\nThe proposed method wa s compared with the results \\nobtained by other approaches reported in the literature. \\nLack of the knowledge of the data and evaluation protocol \\nused by different literature s makes the comparison task \\ndifficult.  However , we compared the performance of the \\nsystem with literatures that adopted  similar protocols  in \\nCK+ dataset . Table 2 compares the performance of the pro-\\nposed method wit h the state -of-the-art methods. From Ta-\\nble 2, Uddin et al. [20] reported high est recognition perfor-\\nmance for disgust, fear, and happiness  probably due to  the \\nuse of temporal features through Hidden M arkov Model . \\nHowever, the perform ance of the proposed system  is com-\\nparable with the other systems  as it achieved an average \\nrecognition rate of 94.09%.  Nevertheless , the high recogni-\\ntion rate is obtained using features from specific facial \\npatches  and without using features of temporal do main .  \\n6.2 Experiments on JAFFE Database  \\nWhile testing  on JAFFE database, we used the same pa-rameters obtained for Cohn -Kanad e database . In our ex-\\nperiments on JAFFE database, we used 183 images in total: \\nanger (30), disgust (32), fear (29), happiness (31), sadness \\n(31), and surprise (30). The confusion matrix , as in Table 3, \\nshows  the co nsistent performance of the proposed method . \\nAn overall accuracy  of 91.8 % was obtained.  From the ex-\\nperiments on JAFFE database, it was observed that t he pro-\\nposed system recognises all expressions with 91.8% recall \\nand 92.63% precision  achieving  an average F-score  of \\n92.22% . The system performed worst for sadness expres-\\nsion as it misclassified sadness as anger .  \\n6.3 Experiments on Fused Database  \\nFor generalization, we have fused the samples of two \\ndatabases together to train the classifier [62]. Sample Level \\nfusion was performed by putting the images  of both data-\\nbases  together . The training set was constructed by ran-\\ndomly elect ing 90% of the data from each expression of \\neach database . The rest data were  used as testing set. The \\nmodel s were trained, and their performances were evalu-\\nated on samples of individual databases in testing set. This \\nexperiment was repeated for ten times. By learning the fea-\\ntures from different databases, the classifier performs bet-\\nter in various situations . All samples were treated with \\n \\nFig. 11. Comparison between performance of the proposed landmark \\ndetection method and the DRMF based CLM model in  BIOID data-\\nbase.  TABLE 2 \\nPERFORMANCE COMPARISON OF DIFFERENT STATE-OF-THE-\\nART APPROACHES ON CK+  DATABASE  \\n [20] [61] [47] [16] [17] Pro-\\nposed \\nsystem  \\nAn 82.5 87.03  71.39  90.56  87.1 87.8 \\nDi 97.5 91.58  95.33  86.04  90.2 93.33  \\nFe 95 90.98  81.11  84.61  92 94.33  \\nHa 100 96.92  95.42  93.61  98.07  94.2 \\nSa 92.5 84.58  88.01  90.24  91.47 96.42  \\nSu 92.5 91.23  98.27  92.3 100 98.46  \\nAvg  93.33  90.38  88.255  89.56  93.14  94.09  \\nAn = anger, Di = disgust, Fe = fear, Ha = happiness, Sa = \\nsadness, Su = surprise, and Avg = Average recognition \\nrate.  \\nTABLE 3 \\nTHE CONFUSION MATRIX USING PROPOSED METHOD ON  \\nJAFFE DATABASE  \\n TABLE 4 \\nCOMPARISON BETWEEN PE RFORMANCE AND TIME C OMPLEXITY \\nOF THE PROPOSED LAND MARK DETECTION METHO D AND THE \\nDRMF  BASED CLM  MODEL O N CK+  DATABAS E \\n \\n(Recognition accuracy is obtained by u sing the proposed salient \\npatch extraction based method. The execution time analysis is based \\non unoptimized MATLAB code in a Dual -Core Intel Pentium i5 CPU \\nwith 3.2 GHz. ) \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n10 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n equal probability of selec tion for training or testing . There-\\nfore,  it is e xpected that the database with more samples \\nshould dominate in  performance . However, t he proposed  \\nmethod performed well o n both databases with a s ignifi-\\ncant accuracy of 89 .64% and 8 5.06% on CK+ and JAFFE  da-\\ntabases respectively . The top four salient patches for clas-\\nsification of each pair of expressions is provided in Table \\n5. \\n6.4 Performance of the landmark detector  \\nWe have used the images of BIOID  [63] dataset without \\nspectacles for evaluating the performance of the proposed \\nlandmark detection algorithm. It contains manual ly la-\\nbelled facial landmarks  which serves the purpose of \\nground truth during training and testing . The average Eu-\\nclidian distance error from point to point for each land-\\nmark  location is used as the distance measure, which is \\ngiven as : \\n𝑒=1\\n𝑛𝑠∑𝑑𝑖𝑛\\n𝑖=1 \\nHere 𝑑𝑖s are the Euclidean distance  errors for the land-\\nmarks, 𝑛 is the number of landmarks,  and 𝑠 is the distance \\nbetween the eye s pupils  used as  the scaling factor . Fig. 11 \\nshows the cumulative distribution of the detection accu-\\nracy by using proposed method.  Its performance wa s com-\\npared with the performance of the recent DRMF method \\nbased CLM model [30]. The performa nce of expression \\nrecognition by the two landmark detection methods wa s \\nalso compared . After detection of facial landmarks,  the ac-\\ntive patches  were extracted and the procedure s as dis-\\ncussed in section 5 were followed for expression classifica-\\ntion. As shown in  Table 4, both the methods produce al-\\nmost similar ac curacy in CK+ database while the proposed \\nlandmark detection method takes very small time in com-\\nparison to the other one. Although the DRMF based CLM \\nmethod finds the facial organ locations accurately, it some-\\ntimes fails to fit to their shapes in different  expressions, es-\\npecially on lip s. When the mouth is open or tightly closed, \\nor when the lip corners are pulled down, it at times gener-\\nates average lip -shape instead of completely fitting to lips. \\nThus, lip corner patches we re extracted at incorrect loca-\\ntions resulting poor classification. However, the proposed \\nlandmark detection method accurately finds the facial landmarks in most of the images thereby extracting fea-\\ntures from appropriate patch locations. The slight misa-\\nlignment error is automatically taken  care by the appear-\\nance feature.  \\n7 CONCLUSION  \\nThis paper has presented a computationally efficient  fa-\\ncial expression recognition system for accurate classifica-\\ntion of  the six universal expressions.  It investigates the rel-\\nevance of different facial patches in  the recognition of dif-\\nferent facial expressions.  All major  active  regions on face \\nare extracted which are responsible for the face defor-\\nmation  during an expression.  The position and size of \\nthese active regions are predefined. The system analyses \\nthe acti ve patches and determines the salient  areas on face \\nwhere the features are discriminative for different expres-\\nsions.  Using the appearance features from the salient \\npatches, the system performs the one -against -one classifi-\\ncation task and determines the expr ession based on major-\\nity vote.  \\nIn addition, a facial landmark detection method is de-\\nscribed which detects some facial points accurately with \\nless computat ional cost .  Expression  recognition is carried \\nout using the proposed landmark detection method  as well \\nas the recently proposed CLM model based on DRMF  \\nmethod . In both cases, recognition accuracy is almost sim-\\nilar, whereas computational cost of the propos ed learning -\\nfree method is significantly less. Promising results has \\nbeen obtained by using block based LBP histogram fea-\\ntures  of the salient patches. Extensive experiments has \\nbeen carried out on two facial expression databases and the \\ncombined dataset . Experiments are conducted using vari-\\nous binwidths of LBP histograms, uniform LBP and rota-\\ntion invariant  LBP features. Low dimensional features are \\npreferred , with sufficient recognition accuracy,  to decrease \\ncomputational complexity . Therefore, t he 16-bin L BP his-\\ntogram features  are used  from four salient patches at face  \\nresolution of 96x96 for obtaining  best performance with a \\nsuitable trade -off between speed and accuracy.  Our system \\nfound the classification between anger and sadness trou-\\nblesome  in all datab ases. The system appears to perform \\nwell in CK+ dataset with a n F-score  of 94.39% . Using the TABLE 5 \\nTHE SALIENT PATCHES DERIVED FROM FUSION OF CK+  AND JAFFE  DATABASES  \\n \\n\\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  11 \\n salient  patches obtained by training on CK+ dataset , the \\nsystem achieves a n F-score  of 92.22% in JAFFE dataset. \\nThis proves the generic performance of the system.  The \\nperformance of the proposed system is comparable with \\nthe earlier  works with similar approach, nevertheless  our \\nsystem is fully automated .  \\nInterestingly, the local features at the salient patches \\nprovide consistent performance at different resolutions . \\nThus, the proposed method can be employed  in real -world \\napplications with low resolution imaging in real -time. Se-\\ncurity cameras, for instance , provide low resolution im-\\nages which can be analysed effectively using the proposed \\nframework.   \\nInstead of the w hole face, the proposed method classi-\\nfies the emotion by assessing a few  facial patches. Thus, \\nthere is a chance of improvement in performance with par-\\ntially occluded images which has not been addressed  in \\nthis study. This analysis is confined to databases  without \\nfacial hair s. There is a possibility  of improvement by using \\ndifferent appearance features. Dynamics of expression in \\ntemporal domain is also not considered in this study.  It \\nwould  be interesting to explore the system  incorporated \\nwith  motion feat ures from different facial patches. The ex-\\necution time reported for the proposed algorithm  is based \\non an un -optimi zed MATLAB code. However, the optimal \\nimplementation of the proposed framework will signifi-\\ncantly improve the computational cost and real -time ex-\\npression recognition can be achieved with substantial ac-\\ncuracy.  Further analysis and efforts are required to im-\\nprove the performance by addressing some of the above \\nmentioned issues . \\n8 ACKNOWLEDGMENT  \\nThe authors  would like to thank Prof. Jeffery Cohn for  \\nthe use of the Cohn –Kanade database, and Dr. Michael J. \\nLyons for the use of the JAFFE database.  The authors grate-\\nfully acknowledge the contribution of the reviewers’ com-\\nments.  \\n9 REFERENCES  \\n \\n[1]  R. A. Calvo and S. D\\'Mello, \"Affect detecti on: An \\ninterdisciplinary review of models, methods, and their \\napplications,\" IEEE Transactions on Affective Computing, vol. 1, \\nno. 1, pp. 18-37, 2010 .  \\n[2]  C. Izard, \"Innate and Universal Facial Expressions: Evidence \\nfrom Developmental and Cross -Cultural  Research,\" \\nPsychological Bull., vol. 115, pp. 288-299, 1994 .  \\n[3]  P. Ekman, W. V. Friesen and J. C. Hager, \"FACS Manual,\" Salt \\nLake City, UT: A Human Face, May 2002 .  \\n[4]  J. Whitehill, M. S. Bartlett and J. Movellan, \"Automatic facial \\nexpression recog nition,\" in Social Emotions in Nature and Artifact , \\nOxford University Press, 2013 .  \\n[5]  Y. Chang, C. Hu and M. Turk, \"Manifold of facial expression,\" \\nIEEE International Workshop on Analysis and Modeling of Faces and \\nGestures, p. 28–35, 2003 .  \\n[6]  M. Pa ntic and I. Patras, \"Dynamics of facial expression: Recognition of facial ac - tions and their temporal segments from \\nface profile image sequences,\" IEEE Transactions on Systems, \\nMan, and Cybernetics, vol. 36, no. 2, p. 433–449, 2006 .  \\n[7]  M. Pantic and L . Rothkrantz, \"Facial action recognition for facial \\nexpression analysis from static face images,\" IEEE Transactions \\non Systems, Man, and Cybernetics, vol. 34, no. 3, p. 1449 –1461 , \\n2004 .  \\n[8]  I. Cohen, N. Sebe, A. Garg, L. Chen and T. Huang, \"Facial \\nexpre ssion recognition from video sequences: Temporal and \\nstatic modeling,\" Comput. Vis. Image Understand., vol. 91, p. 160–\\n187, 2003 .  \\n[9]  S. M. Lajevardi and Z. M. Hussain, \"Automatic facial expression \\nrecognition: feature extraction and selection,\" Signal,  Image and \\nVideo Processing, vol. 6, no. 1, pp. 159-169, 2012 .  \\n[10]  G. Zhao and M. Pietikainen, \"Dynamic texture recognition \\nusing local binary patterns with an application to facial \\nexpressions,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. \\n6, p. 915–928, 2007 .  \\n[11]  T. Jabid, M. Kabir and O. Chae, \"Robust facial expression \\nrecognition based on local directional pattern,\" ETRI Journal, \\nvol. 32, pp. 784-794, 2010 .  \\n[12]  M. H. Kabir, T. Jabid and O. Chae, \"A Local Directional Pattern \\nVariance  (LDPv) based Face Descriptor for Human Facial \\nExpression Recognition,\" 7th IEEE Int. Conf. on Advanced Video \\nand Signal Based Surveillance, pp. 526-532, 2010  .  \\n[13]  C. Shan and R. Braspenning, \"Recognizing facial expressions \\nautomatically from video,\" in Handbook of ambient intelligence and \\nsmart environments , 2010 , pp. 479-509. \\n[14]  C. Shan, S. Gong and P. W. McOwan, \"Robust facial expression \\nrecognition using local binary patterns,\" IEEE International \\nConference on Image Processing, 2005 .  \\n[15]  C. Shan and T. Gritti, \"Learning Discriminative LBP -Histogram \\nBins for Facial Expression Recognition,\" British Machine Vision \\nConference, 2008 .  \\n[16]  M. Song, D. Tao, Z. Liu, X. Li and M. Zhou, \"Image ratio features \\nfor facial expression recognition applic ation,\" IEEE Transactions \\non Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 40, no. \\n3, pp. 779-788, 2010 .  \\n[17]  L. Zhang and D. Tjondronegoro, \"Facial expression recognition \\nusing facial movement features,\" IEEE Transactions on Affective \\nComput ing, vol. 2, no. 4, pp. 219-229, 2011 .  \\n[18]  Y. Zhang and Q. Ji, \"Active and dynamic information fusion for \\nfacial expression understanding from image sequences,\" IEEE \\nTransactions on Pattern Analysis and Machine Intelligence, vol. 27, \\nno. 5, pp. 699 -714, 2005 .  \\n[19]  Y. Tian, T. Kanade and J. F. Cohn, \"Recognizing lower face \\naction units for facial expression analysis,\" IEEE International \\nConference on Automatic Face and Gesture Recognition, pp. 484-490, \\n2000 .  \\n[20]  M. Z. Uddin, J. J. Lee and T. -S. Ki m, \"An enhanced independent \\ncomponent -based human facial expression recognition from \\nvideo,\" IEEE Transactions on Consumer Electronics, vol. 55, no. 4, \\npp. 2216 -2224 , 2009 .  \\n[21]  M. F. Valstar and M. Pantic, \"Combined support vector \\nmachines and hidden m arkov models for modeling facial action \\ntemporal dynamics,\" Human–Computer Interaction, pp. 118-127, \\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\n12 IEEE TRANSACTIONS ON  AFFECTIVE COMPUTING,   MANUSCRIPT ID  \\n 2007 .  \\n[22]  T. Cootes, G. Edwards and C. Taylor, \"Active appearance \\nmodels,\" IEEE Trans. on Pattern Analysis and Machine Intelligence, \\nvol. 23, no. 6, pp . 681--685, 2001 .  \\n[23]  S. Lucey, I. Matthews, C. Hu, Z. Ambadar, F. D. l. Torre and J. \\nCohn, \"AAM derived face representations for robust facial \\naction recognition,\" 7th International Conference on Automatic Face \\nand Gesture Recognition, 2006 .  \\n[24]  A. B. Ashraf, S. Lucey, T. C. Jeffrey F. Cohn, Z. Ambadar, K. M. \\nPrkachin and P. E. Solomon, \"The painful face –pain expression \\nrecognition using active appearance models,\" Image and Vision \\nComputing, vol. 27, no. 12, pp. 1788 -1796 , 2009 .  \\n[25]  B. Abboud, F. Davoine and M. Dang, \"Facial expression \\nrecognition and synthesis based on an appearance model,\" \\nSignal Processing: Image Communication, vol. 19, no. 8, pp. 723-\\n740, 2004 .  \\n[26]  A. Asthana, J. Saragih, M. Wagner and R. Goecke, \"Evaluating \\naam fitting methods for facial expression recognition,\" 3rd \\nInternational Conference on Affective Computing and Intelligent \\nInteraction and Workshops, pp. 1-8, 2009 .  \\n[27]  C. Martin, U. Werner and H. -M. Gross, \"A real -time facial \\nexpression recognition system based on active appearance \\nmodels using gray images and edge images,\" 8th IEEE \\nInternational Conference on Automatic Face & Gesture Recognition, \\n2008 .  \\n[28]  D. Cristinacce and T. F. Cootes, \"Feature Detection and Tracking \\nwith Constrained Local Models,\" in British Machine Vision \\nConference , 2006 .  \\n[29]  J. M. Saragih, S. Lucey and J. F. Cohn., \"Deformable model \\nfitting by regularized landmark mean -shift,\" International \\nJournal of Computer Vision, vol. 91, no. 2, pp. 200-215, 2011 .  \\n[30]  A. Asthana, S. Zafeir iou, S. Cheng and M. Pantic, \"Robust \\ndiscriminative response map fitting with constrained local \\nmodels,\" in IEEE Conference on Computer Vision and Pattern \\nRecognition , 2013 .  \\n[31]  S. W. Chew, P. Lucey, S. Lucey, J. Saragih, J. F. Cohn, I. \\nMatthews and S.  Sridharan, \"In the pursuit of effective affective \\ncomputing: The relationship between features and registration,\" \\nIEEE Transactions on Systems, Man, and Cybernetics, Part B: \\nCybernetics, vol. 42, no. 4, pp. 1006 -1016 , 2012 .  \\n[32]  M. Bartlett, G. Littlew ort, M. Frank, C. Lainscsek, I. Fasel and J. \\nMovellan, \"Recognizing facial expression: machine learning and \\napplication to spontaneous behavior,\" in IEEE Computer Society \\nConf. on Computer Vision and Pattern Recognition , 2005 .  \\n[33]  M. Lyons, J. Budynek and S. Akamatsu, \"Automatic \\nclassification of single facial images,\" IEEE Trans. on Pattern \\nAnalysis and Machine Intelligence, vol. 21, no. 12, pp. 1357  - 1362  , \\nDec 1999 .  \\n[34]  A. Hadid, M. Pietikainen and T. Ahonen, \"A discriminative \\nfeature space for detecting and recognizing faces,\" in IEEE \\nComputer Society Conf. on Computer Vision and Pattern \\nRecognition , 2004 .  \\n[35]  S. L. Happy, A. George and A. Routray, \"A real time facial \\nexpression classification system using Local Binary Patterns,\" \\n4th Int. Co nf. on Intelligent Human Computer Interaction, 2012 .  \\n[36]  A. Dhall, A. Asthana, R. Goecke and T. Gedeon, \"Emotion recognition using PHOG and LPQ features,\" in IEEE \\nInternational Conference on Automatic Face and Gesture Recognition \\nand Workshops , 2011.  \\n[37]  C. Shan, S. Gong and P. W. McOwan, \"Facial expression \\nrecognition based on local binary patterns: A comprehensive \\nstudy,\" Image and Vision Computing, vol. 27, no. 6, pp. 803-816, \\n2009 .  \\n[38]  K. I. Kim, K. Jung and H. J. Kim, \"Face recognition usi ng kernel \\nprincipal component analysis,\" IEEE Signal Processing Letters, \\nvol. 9, no. 2, pp. 40-42, 2002 .  \\n[39]  A. J. Calder, A. M. Burton, P. Miller, A. W. Young and S. \\nAkamatsu, \"A principal component analysis of facial \\nexpressions,\" Vision research, vol. 41, no. 9, pp. 1179 -1208 , 2001 .  \\n[40]  H.-B. Deng, L. -W. Jin, L. -X. Zhen and J. -C. Huang, \"A new facial \\nexpression recognition method based on local gabor filter bank \\nand pca plus lda,\" International Journal of Information Technology, \\nvol. 11, no. 11, pp. 86-96, 2005 .  \\n[41]  Z. Zhang, Y. Yan and H. Wang, \" Discriminative filter based \\nregression learning for facial expression recognition,\" 20th IEEE \\nInternational Conference on Image Processing (ICIP), pp. 1192  - 1196  \\n, 2013  .  \\n[42]  C. Shan, S. Gong and P. W. McOwan, \"A comprehensive \\nempirical study on linear subspace methods for facial \\nexpression analysis,\" IEEE Conf. on Computer Vision and Pattern \\nRecognition Workshop, pp. 153-153, 2006 .  \\n[43]  S.-K. Oh, S. -H. Yoo and W. Pedrycz, \"De sign of face recognition \\nalgorithm using PCA -LDA combined for hybrid data pre -\\nprocessing and polynomial -based RBF neural networks: Design \\nand its application,\" Expert Systems with Applications, vol. 40, no. \\n5, pp. 1451 -1466 , 2013 .  \\n[44]  Y. Rahulamathavan , R.-W. P. J. A. Chambers and D. J. Parish, \\n\"Facial Expression Recognition in the Encrypted Domain Based \\non Local Fisher Discriminant Analysis,\" IEEE Transactions on \\nAffective Computing, vol. 4, no. 1, pp. 83-92, 2013 .  \\n[45]  H. Mohammadzade and D. Hatzin akos, \"Projection into \\nexpression subspaces for face recognition from single sample \\nper person,\" IEEE Transactions on Affective Computing, vol. 4, no. \\n1, pp. 69-82, 2013 .  \\n[46]  S. Moore and R. Bowden, \"Local binary patterns for multi -view \\nfacial expressi on recognition,\" Computer Vision and Image \\nUnderstanding, vol. 115, no. 4, pp. 541-558, 2011 .  \\n[47]  L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang and D. N. Metaxas, \\n\"Learning active facial patches for expression analysis,\" in IEEE \\nConference on Computer Vi sion and Pattern Recognition (CVPR) , \\n2012 .  \\n[48]  L. Unzueta, W. Pimenta, J. Goenetxea, L. P. Santos and F. \\nDornaika, \"Efficient generic face model fitting to images and \\nvideos,\" Image and Vision Computing, vol. 32, no. 5, pp. 321-334, \\n2014 .  \\n[49]  P. Vi ola and M. Jones, \"Rapid object detection using a boosted \\ncascade of simple features,\" in IEEE Conference on Computer \\nVision and Pattern Recognition , 2001 .  \\n[50]  D. Nguyen, D. Halupka, P. Aarabi and A. Sheikholeslami, \"Real -\\ntime face detection and lip fe ature extraction using field -\\nprogrammable gate arrays,\" IEEE Transactions on Systems, Man, \\nand Cybernetics, Part B: Cybernetics, vol. 36, no. 4, pp. 902-912, \\n1949-3045 (c) 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation\\ninformation: DOI 10.1109/TAFFC.2014.2386334, IEEE Transactions on Affective Computing\\nHAPPY AND ROUTRAY:  AUTOMATIC FACIAL EXP RESSION RECOGNITION USING FEATURES OF SA LIENT FAC IAL PATCHES  13 \\n 2006 .  \\n[51]  R. C. Gonzalez and R. E. Woods, Digital Image Processing (3rd \\nEdition), Pearson Educ ation, 2008 .  \\n[52]  N. Otsu, \"A threshold selection method from gray -level \\nhistograms,\" IEEE Transactions on Systems, Man and Cybernetics, \\nvol. 9, no. 1, pp. 62-66, 1979 .  \\n[53]  T. Ojala, M. Pietikainen and T. Maenpaa, \"Multiresolution gray -\\nscale and rot ation invariant texture classification with local \\nbinary patterns,\" IEEE Transactions on Pattern Analysis and \\nMachine Intelligence, vol. 24, no. 7, pp. 971-987, 2002 .  \\n[54]  P. N. Belhumeur, J. P. Hespanha and D. Kriegman, \"Eigenfaces \\nvs. fisherfaces: Rec ognition using class specific linear \\nprojection,\" IEEE Transactions on Pattern Analysis and Machine \\nIntelligence, vol. 19, no. 7, pp. 711-720, 1997 .  \\n[55]  C. Cortes and V. Vapnik, \"Support -vector networks,\" Machine \\nlearning, vol. 20, no. 3, pp. 273 -297, 1995 .  \\n[56]  C.-W. Hsu and C. -J. Lin, \"A comparison of methods for \\nmulticlass support vector machines,\" IEEE Transactions on \\nNeural Networks, vol. 13, no. 2, pp. 415-425, 2002 .  \\n[57]  M. J. Lyons, M. Kamachi and J. Gyoba, \"Japanese Female Facial \\nExpressi ons (JAFFE),\" Database of digital images, 1997 . \\n[58]  P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar and I. \\nMatthews, \"The Extended Cohn -Kande Dataset (CK+): A \\ncomplete facial expression dataset for action unit and emotion -\\nspecified expression,\" in 3rd IEEE Workshop on CVPR for Human \\nCommunicative Behavior Analysis , 2010 .  \\n[59]  Y. Tian, T. Kanade and J. F. Cohn, \"Facial expression \\nrecognition,\" in Handbook of face recognition , London, Springer , \\n2011 , pp. 487-519. \\n[60]  M. Sokolova and G. Lapal me, \"A systematic analysis of \\nperformance measures for classification tasks,\" Information Processing & Management, vol. 45, no. 4, pp. 427-437, 2009 .  \\n[61]  P. A, N. H.A., S. M.G. and Y. N., \"Gauss –Laguerre wavelet \\ntextural feature fusion with geometrical  information for facial \\nexpression identification,\" EURASIP Journal on Image and Video \\nProcessing, 2012 .  \\n[62]  Z. Zhang, C. Fang and X. Ding, \"Facial expression analysis \\nacross databases,\" IEEE International Conference on Multimedia \\nTechnology, pp. 317 -320, 2011 .  \\n[63]  O. Jesorsky, K. Kirchberg and R. Frischholz, \"Robust face \\ndetection using the hausdorff distance,\" in 3rd International \\nConference on Audio - and Video -Based Biometric Person \\nAuthentication , Halmstad, Sweden, 2001 .  \\n \\n \\n S L Happy has r eceived  the B.Tech. (Hons.) \\ndegree from Institute of Technical Education \\nand Research (ITER), India in 2011. Now he is \\npursuing the M. S. degree from Indian Institute \\nof Technology Kharagpur, India. His research \\ninterests include pattern recognition, com-\\nputer vision and facial expression analysis.  \\nAurobinda Routray  has received his Masters \\ndegrees in 1991 from IIT Kanpur, India  and his \\nPhD in 1999 from  Sambalpur University, India. \\nHe has also worked as a postdoctoral re-\\nsearcher at  Purdue University, USA,  during  \\n2003 -2004 . He is currently working as a profes-\\nsor in the Department of Electrical Engineering, Indian Insti-\\ntute of Technology, Kharagpur. His research interest s include \\nnon-linear and statistical signal processing, signal based fault \\ndetection and di agnosis, real time and embedded signal pro-\\ncessing, numerical linear algebra, and data driven diagnostics.  \\n \\n\\n',\n",
       " '1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 1\\nBAUM-1: A Spontaneous Audio-Visual Face 1\\nDatabase of Affective and Mental States 2\\nSara Zhalehpour, Onur Onder, Zahid Akhtar, Cigdem Eroglu Erdem, Member, IEEE 3\\nAbstract\\nIn affective computing applications, access to labeled spontaneous affective data is essential for testing the\\ndesigned algorithms under naturalistic and challenging conditions. Most databases available today are acted\\nor do not contain audio data. We present a spontaneous audio-visual affective face database of affective and\\nmental states. The video clips in the database are obtained by recording the subjects from the frontal view using\\na stereo camera and from the half-proﬁle view using a mono camera. The subjects are ﬁrst shown a sequence\\nof images and short video clips, which are not only meticulously fashioned but also timed to evoke a set of\\nemotions and mental states. Then, they express their ideas and feelings about the images and video clips they\\nwatch in an unscripted and unguided way in Turkish. The target emotions, include the six basic ones (happiness,\\nanger, sadness, disgust, fear, surprise) as well as boredom and contempt. We also target several mental states,\\nwhich are unsure (including confused, undecided), thinking, concentrating, interested (including curious), and\\nbothered. Baseline experimental results on the BAUM-1 database show that recognition of affective and mental\\nstates under naturalistic conditions is quite challenging. The database is expected to enable further research on\\naudio-visual affect and mental state recognition under close-to-real scenarios.\\nIndex Terms\\nfacial expression recognition, audio-visual affective database, emotional corpora, mental state recognition,\\naffective computing, spontaneous expressions, emotion recognition from speech, dynamic facial expression\\ndatabase.4\\nF5\\n1\\nINTRODUCTION\\n6\\nW\\ne would like to acknowledge the support received from the Turkish Scientiﬁc and Technical Research Council (T ¨UB˙ITAK) under project\\n110E056.\\n\\x0fThis work was done when all authors were with the Department of Electrical and Electronics Engineering, Bahcesehir University, Ciragan\\nCad., Besiktas, Istanbul, Turkey.\\nE-mail: cigdem.eroglu@eng.bahcesehir.edu.tr, onurndr@yahoo.com\\n\\x0fS. Zhalehpour is currently with INRS-EMT, Montreal, Canada.\\nE-mail: sara.zhalehpour@emt.inrs.ca\\n\\x0fZ. Akhtar is currently with the Department of Mathematics and Computer Science, University of Udine, Via delle Scienze 206, 33100 Udine,\\nItaly.\\nE-mail: zahid.akhtar@uniud.it\\nManuscript received April xx, xxxx; revised September xx, xxxx.\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 2\\nNon-verbal messages constitute an important part of human-to-human communication, since they enhance 7\\nor modify our verbal messages and convey information about our emotional and mental states. Major compo- 8\\nnents of non-verbal messages are facial expressions, body/head gestures and the paralinguistic properties of 9\\nspeech. The importance of emotions in our daily interactions motivated researchers to design and implement 10\\nautomatic algorithms to recognize emotional expressions with the ultimate goal of achieving intelligent human- 11\\nmachine interaction [1], [2]. As a result, the ﬁeld of affective computing has attracted a lot of attention from 12\\nresearchers in the last decade due to its wide range of application areas including multi-modal human computer 13\\ninteraction, security [3] (lie-detection etc.), education, health-care [4], [5], marketing and advertising. 14\\nThere are six basic facial expressions that have been shown to be universal, which are happiness, surprise, 15\\nanger, sadness, fear and disgust [6]. It has been reported that humans can recognize emotions such as happiness 16\\nand surprise easily even from low resolution images [7]. However, their recognition accuracy is very low for 17\\nanger and sadness, and the worst for fear and disgust. Trained observers are reported to achieve an average 18\\nfacial expression recognition accuracy rate of 87% [7]. 19\\nAccess to annotated affective databases is a prerequisite for researchers to train and test the performance of 20\\ntheir affect recognition algorithms. Collecting and annotating affective databases is a challenging task, especially 21\\nif natural (spontaneous) multi-modal expressions are desired. Below we ﬁrst brieﬂy review the state of the art 22\\non affective databases available in the literature and then state the contribution and scope of this work. 23\\n1.1 Prior Work 24\\nMost of the affective databases that are accessible by researchers are acted and uni-modal [2], [8], [9], [10], 25\\n[11]. Predominantly, acted datasets are recorded under very constrained conditions and resulting expressions 26\\nare exaggerated. One of the most popular acted databases is the extended Cohn-Kanade database (CK+) [8], 27\\nwhich encompasses 123 subjects with 327 labeled sequences pursuant to six basic emotions (anger, happiness, 28\\nsadness, disgust, surprise and fear) [6] and contempt. The ﬁrst frame of each sequence contains a neutral 29\\nexpression and the last frame of the sequence reﬂects the expression at its apex. Another image-based acted 30\\nfacial expression database is the Jaffe database [12], that contains 219 images of 10 Japanese females exhibiting 31\\nthe six basic emotions. The MMI database [13] consists of mostly posed videos of facial expressions showing 32\\ncomplete temporal patterns (i.e. starting from onset, which is followed by apex and offset phases). Some of the 33\\nvideos contain spontaneous laughter. Bosphorus database [10] is a 3D facial expression database incorporating 34\\nvarious head poses and occlusions. 35\\nThere are several audio-visual acted databases in the literature. The GEMEP dataset [9], [14] is comprised 36\\nof videos of 10 actors coached by a professional director. During the recording session, the actors were asked 37\\nspeciﬁcally to utter combinations of meaningless phoneme sequences. It is worth mentioning that, out of 7000 38\\naudio-visual affective video clips reﬂecting 18 emotions, only 289 samples representing ﬁve emotions (anger, 39\\nfear, joy, relief, sadness) were selected for the GEMEP-FERA challenge [14]. Another audio-visual acted database 40\\nis eNTERFACE [15], which contains video clips of 44 subjects uttering selected scripts in English while reﬂecting 41\\nthe six basic emotions. 42\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 3\\nTABLE 1\\nOverview of Audio-Visual Affective Databases. The acronyms in the table are: SBE: Six Basic Emotions (Happiness, Sadness,\\nAnger, Fear, Disgust, Surprise) and Neutral, NBE: Non-Basic Emotions, MS: Mental States\\nDatabase Posed\\nvs. Language Annotation Number\\nof Number\\nof\\nNon-Posed subjects video\\nclips\\nBAUM-1 Non-Posed T\\nurkish SBE,\\n2 NBE, 3 MS 31 1222\\nBAUM-2\\n[24] Non-Posed English,\\nTurkish SBE,\\n1 NBE 286 1047\\nAFEW\\n[22] Non-Posed English SBE 330 1426\\nBelfast\\n[21] Non-Posed English SBE 100 239\\nSemaine\\n[17] Non-Posed English 3\\nBE, 10 NBE 150 959\\nIEMOCAP\\n[20] Posed English 3\\nBE, 1 NBE 10 1039\\nGEMEP\\n[14] Posed None SBE,\\n12 NBE 10 7000\\nFanelli\\n[27] Posed,\\n3D English SBE\\n+ MS 14 1109\\neNTERF\\nACE [15] Posed English SBE 44 1166\\nAssembling\\ndatasets that encompass spontaneous or naturalistic expressions is a strenuous and tedious job 43\\n[16], [17], [18], [19]. The SEMAINE [17] database was collected under constrained lab settings and contains 44\\nnaturalistic expressions from 150 subjects, who are in a conversation with a “sensitive artiﬁcial listener”. 45\\nAnother induced emotional expressions database is the FEED database [18], which contains 18 subjects. The 46\\nscripted affective dyadic conversations were induced by 10 professional actors in the IEMOCAP database 47\\n[20], which resulted in more than 10 hours of audio-visual recordings. In particular, facial motions of the 48\\nactors were captured using a system utilizing markers, while the utterances were labeled along the three 49\\ndimensions indicating valence, activation, and dominance. Recently, DISFA [19] has been introduced, which 50\\nis a spontaneous facial action intensity database. Different from most of the data sets, the Belfast naturalistic 51\\ndatabase [21] is comprised of clips accumulated manually from TV programs as well as clips recorded through 52\\ndyadic discussions. 53\\nThere are recent efforts towards collecting naturalistic databases from movies such as the AFEW [22] 54\\nand BAUM-2 databases [23], [24]. Although using movie clips to gather affective databases do not replace 55\\npure spontaneity, they are more naturalistic and challenging as compared to acted databases. Several other 56\\napproaches use crowdsourcing to collect facial expressions from the internet [25] in response to short video 57\\nclips or to score the captured facial expressions [26]. However, they mainly concentrate on positive emotions 58\\nand do not consider mental states. Also, very few efforts for naturalistic 3D audio-visual [27] or 3D facial 59\\nexpression databases [28] have been rendered using costly 3D scanners. 60\\nIn Table 1, we give a summary of the audio-visual databases available in the literature. It can be observed 61\\nthat BAUM-1 is the only non-posed database in the literature that contains the six basic emotions as well as 62\\nseveral non-basic emotions and mental states in a language different from English. 63\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 4\\n1.2 Contribution and Scope 64\\nAs summarized in the previous section, there are a few audio-visual naturalistic affective databases in the 65\\nliterature, which are mostly in English and do not contain any expressions related to the mental states. In this 66\\nwork, we present a re-acted spontaneous audio-visual face database of affective and mental states in Turkish. 67\\nThe subjects watch a sequence of still images and short video clips, which are meticulously devised and timed 68\\nto evoke a set of emotions and mental states. Emotion elicitation using video clips is a well-validated method 69\\n[29], [30]. The subjects express their feelings and ideas about the stimuli they have watched on the screen 70\\nin their own words, without using predetermined scripts. The database contains recordings reﬂecting the six 71\\nbasic emotions (happiness, anger, sadness, disgust, fear, surprise ) as well as boredom and contempt . The 72\\ndatabase also contains several mental states, namely unsure (confused, undecided), thinking, concentrating, 73\\nandbothered. The subjects are not guided in any way about how to perform the facial expressions. The database 74\\nconsists of simultaneous recordings of subjects using two cameras, both of which can record in high deﬁnition 75\\nformat. The ﬁrst one is a stereo camera, which is placed in front of the subject at the top of the screen. The 76\\nsecond one is a mono camera, which is placed to capture a half proﬁle view of the subject. The categorical 77\\nannotations of the recordings also include a score indicating the intensity of the emotion on a scale of 0 to 5. 78\\nThe organization of the paper is as follows. In Section 2, we describe the data acquisition process in detail 79\\nincluding emotion elicitation and recording. In Section 3, we give the details of the post-processing steps 80\\nincluding segmentation, annotation and organization of the database. In Section 4, we outline the used method 81\\nfor multi-modal recognition of affective and mental states for the purpose of establishing baseline results on 82\\nthe database. In Section 5, experimental results on BAUM-1 database are provided and compared with the 83\\nresults on the eNTERFACE [15] database. Finally in Section 6, we provide conclusions and future directions for 84\\nresearch. 85\\n2 M ETHOD FOR DATAACQUISITION 86\\nThe BAUM-1 database has been recorded in a studio, which was designed speciﬁcally as described below. In 87\\nthe studio, subjects ﬁrst watch a stimuli video on a monitor in front of them and then express their feelings in 88\\ntheir own words while they are being recorded with multiple cameras. 89\\n2.1 Recording Setup 90\\nIn the designed studio a green curtain has been used as background (see Fig. 1(a), (b)). A Sony HDR-TD20 91\\nStereo HD camera is used for recording the frontal view of the face and a Sony HDR-XR200 Mono camera is 92\\nused for recording from a half proﬁle view with an angle of approximately 45 degrees. Illumination is provided 93\\nby three 1000 Watt tungsten (Red Head) lights directed towards the ceiling to provide a smooth lighting. Two 94\\nspherical lights are located carefully on both sides of the face to minimize shadows on the face of the subject. 95\\nFor recording the audio, a Rode NTG 2 shotgun (directional) microphone has been used. A monitor is located 96\\nin front of the subject at eye level to watch the stimuli video (see Fig. 1(c)). A clap-board has been used to assist 97\\nin the synchronization of audio and video streams during post-processing. 98\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 5\\n(a)\\n (b)\\n (c)\\nFig.\\n1. (a),(b) The setup of the studio showing the location of stereo and mono cameras, microphone and lights. (c) The subjects\\nwatch the stimuli shown on the monitor and express their ideas and feelings about the stimuli in their own words.\\n2.2 Recording of Acted Clips 99\\nBefore starting the spontaneous session, we asked the subjects to utter several sentences in Turkish with certain 100\\nemotions and imagining speciﬁc scenarios, which is a similar procedure as in the eNTERFACE dataset [15]. We 101\\ncall the acted part of the database as BAUM-1a. In the acted recordings, we targeted 8 emotions and mental 102\\nstates, which are listed below together with the imagined scenarios: 103\\n\\x0fHappiness: You just learned that you have won the lottery and you are telling it to a friend of yours. 104\\n\\x0fSadness: You have to explain your friend that his father has passed away. 105\\n\\x0fFear: You are kidnapped and the kidnappers are holding a gun towards you. You have to beg for your 106\\nlife. 107\\n\\x0fAnger: You have just caught the thief who has stolen your wallet. 108\\n\\x0fDisgust: You have discovered an insect in your soup. 109\\n\\x0fConfusion: You didn’t understand the lecture and asking the lecturer to explain again. 110\\n\\x0fBoredom: You have been waiting for a bus for at least an hour. 111\\n\\x0fInterest (Curiosity): You want to learn about your friend’s secret. 112\\n2.3 Elicitation of Emotional and Mental States for Spontaneous Recordings 113\\nIn BAUM-1 database, our main focus was to obtain spontaneous audio-visual expressions of emotional and 114\\nmental states. In order to bring a subject into the mood of the emotion, we ﬁrst asked each subject to watch 115\\na carefully designed “stimuli video”, which contains a sequence of images and video clips selected to evoke 116\\nthe target emotions and mental states. The target emotions and mental states are: happiness, anger, sadness, 117\\ndisgust, fear, surprise, boredom, contempt, unsure (confused etc.), thinking, concentrating, and bothered. 118\\nEmotion elicitation using ﬁlms is a well-validated procedure in the literature [29], [31]. While watching a 119\\ncertain portion of the stimuli video (or shortly after that), the subjects are asked to explain their feelings and 120\\nideas about the video or the image just shown. Subjects are also recorded while they are watching the video 121\\nto capture spontaneous facial expressions. After the recording process, the whole recording is segmented and 122\\nannotated, which will be explained in more detail in Section 3. 123\\nThe total duration of the stimuli video is approximately 35 minutes and there are 30 second intervals in 124\\nbetween video clips or images for the subjects to express their own feelings and ideas in their own words. 125\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 6\\nThe total recording session lasts about 50 minutes for each subject. The audio-visual stimuli video consist of 126\\n29 images and videos that are expected to elicit the desired emotions and mental states. These 29 images and 127\\nvideo clips (scenes) have been carefully selected from a larger set of candidates retrieved from the internet and 128\\nthe International Affective Picture System [32] as well as several works of Escher [33] (to elicit confusion). The 129\\nselection from this larger set has been done by making a demonstration of the whole collection to a jury, which 130\\nconsisted of 19 students from the department of psychology, who were asked to give a score to each video on 131\\na scale of 0 to 5. The stimuli, which received an average score below 2 were eliminated and highest scoring 132\\nstimuli were retained. The ordering for the elicitation of the emotions were designed in such a way that the 133\\nmost negative and intense video clips (such as an autopsy scene) were shown towards the end. There were also 134\\nneutral clips in between emotion transitions so that the subjects had enough time to recover from one emotion 135\\nand were ready to enter the mood of the next emotion. Images were also displayed long enough to enable 136\\nelicitation of the target emotion or mental state. In Table 2, we summarize the contents of each video clip or 137\\nimage in the stimuli video together with the target emotion or mental state. 138\\n2.4 Participants and the Spontaneous Recording Process 139\\nThe data was collected from 31 subjects, 17 of which are female, which are shown in Fig. 2. All subjects are 140\\nnative speakers of Turkish, and have an age range of 19-65. Prior to each recording session, we explained the 141\\nwhole procedure to the subjects in detail and warned them about possible disturbing scenes (e.g. autopsy scene), 142\\nindicating that they can quit the session at any point. Each subject also signed a consent form, which states that 143\\nthe subject has understood and accepted the procedure and indicates whether all recordings of the subject can 144\\nbe used and shared for research purposes. All subjects except one of them (subject 5) gave permission for their 145\\nimages to be used in publications. The subjects completed the 50 minute long recording process by watching the 146\\nstimuli video on their own and expressed their thoughts and feelings with their own words when prompted. 147\\nSeveral limitations of the used emotion elicitation procedure has been observed. First, the subjects may not 148\\nexpress completely spontaneous emotions since they are aware of the cameras and they are being recorded. 149\\nHence, some subjects may tend to suppress their emotions, whereas some others may tend to exaggerate. We 150\\nalso had a difﬁculty in elicitation of fear since it is difﬁcult to frighten the subjects in a secure ofﬁce environment, 151\\nwhich is inline with the observations stated in [29]. Another issue is the “discreteness” of the emotions [29], 152\\nwhich means inducing a single emotion/mental state, with no traces of others. It was observed that this was 153\\nespecially difﬁcult since several emotions or mental states co-exist, which can be observed from the labels given 154\\nby the annotators. This is explained in more detail in Sections 3.2 and 3.3 below. 155\\n3 P OSTPROCESSING AND ANNOTATION 156\\nAfter the video of a subject is recorded, it is divided into smaller segments, which are then annotated. Below 157\\nwe give the details of the segmentation and annotation processes. 158\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 7\\nTABLE 2\\nContents and ordering of the stimuli video. Target emotions or mental states are also listed.\\nV\\nideo/picture number Content Target emotional or mental state\\n1 Horses\\nillusion (image) Unsure (confusion, undecided)\\n2 Stair paradox (image) Unsure, Concentrating, Thinking\\n3 Wheel illusion (image) Unsure, Concentrating, Thinking\\n4 Lines illusion (image) Unsure, Concentrating, Thinking\\n5 Puppies (video) Happiness/Amusement\\n6 Funny advertisement (video) Happiness/Amusement\\n7 Funny stand-up show (video) Happiness/Amusement\\n8 Space (image) Neutral\\n9 Parking a car (video) Contempt\\n10 Children (image) Happiness\\n11 Crazy sportsmen (video) Surprise\\n12 Bored man (image) Boredom\\n13 A video of family ﬁght Anger/Sadness\\n14 Child and vulture (image) Sadness\\n15 Sick newborn (image) Sadness\\n16 Dead child and father (image) Sadness/Anger\\n17 Murdering of a cat (video) Anger/Sadness\\n18 Fisherman (image) Neutral/Boredom\\n19 Thinking person (image) Neutral/Boredom\\n20 Man pointing a gun (image) Anger\\n21 Shark (image) Fear\\n22 A vomiting man (image) Disgust\\n23 Waterfalls (image) Neutral\\n24 Car accident (video) Sadness\\n25 Drunk driver (video) Sadness/Anger\\n26 Badly injured hand (image) Disgust\\n27 Clips from horror movies Fear/Bothered\\n28 Autopsy video Fear/Bothered\\n29 Illusionist cutting his wife Surprise/Fear\\n3.1\\nSegmentation and Organization of the Database 159\\nDuring post-processing, ﬁrst the recorded stereo and mono video and audio streams are synchronized using 160\\nthe clap-board information in the audio and video streams. The precision of the synchronization is about 1 161\\nframe, which corresponds to 1/30 seconds. Then the recording is segmented into short video clips using Sony 162\\nVegas software (see Fig. 3) so that there is a single emotion or mental state expression in a clip. There might be 163\\na few clips which contains simultaneous expression of two emotions (e.g., happily surprised). The segmented 164\\nclips are then rendered by fusing the audio and video channels and then saved by giving a name indicating 165\\nthe subject and clip number. Original stereo (frontal) and mono (half-proﬁle) videos were recorded in high 166\\ndeﬁnition resolution ( 1080\\x021920) and video clips were rendered in standard deﬁnition ( 576\\x02720) resolution. 167\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 8\\nFig.\\n2. The 31 subjects who volunteered for the recordings of BAUM-1 database. All subjects (except subject 5) gave their consent for\\ntheir images to be used in publications.\\n(a) (b)\\nFig.\\n3. (a) Segmentation of the recording into clips using Sony Vegas software. (b) GTrace annotation tool is used for annotating the\\nclips [35].\\nA mono frontal view version of the database is also prepared, which consists of the right view of the stereo pair 168\\ndownsampled to a resolution of 480\\x02854[34]. An example recording can be seen in Fig. 4. We also provide 169\\nsubtitles in English using .srt ﬁles for each video clip. 170\\n3.2 Annotation 171\\nBAUM-1a database contains clips containing expressions of ﬁve basic emotions (happiness, sadness, anger, 172\\ndisgust, fear) along with expressions of boredom, confusion (unsure) and interest (curiosity). BAUM-1s database 173\\ncontains clips reﬂecting six basic emotions and also expressions of boredom, contempt, confusion, thinking, 174\\nconcentrating, bothered, and neutral. We used some of the mental state labels indicated by the taxonomy in [36] 175\\nto annotate the mental states. 176\\nEach clip in the database is annotated by ﬁve annotators using the GTrace tool [35] (see Fig.3(b)), which has 177\\na simple and friendly user interface. Prior to the annotations, the annotators were made familiar with the facial 178\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 9\\n(a)\\n (b)\\n(c)\\nFig.\\n4. An example recording from the BAUM-1 database. (a) A frontal stereo recording. (b) A mono recording from half proﬁle. (c)\\nThe speech channel.\\nexpressions by using the Mind Reading Software [37]. The annotator watches the clip as many times as he/she 179\\nwants, selects the emotion or mental state that is dominant in the clip and ﬁnally gives a score between 0 and 180\\n5, which represents the intensity of the emotional or mental state expression in the clip. Finally, each clip is 181\\ngiven a label by using majority voting over the ﬁve annotators. The scores of the selected label are averaged to 182\\ndetermine the ﬁnal score for the clip. 183\\n3.3 Inter-Annotator Agreement 184\\nWe used the Kappa statistic [38], [39] to estimate the amount of inter-annotator agreement in the BAUM-1 185\\ndatabase, which assesses differences between the agreement among annotators (i.e. “observed” agreement) and 186\\nagreement that would occur by chance alone (i.e. “expected” agreement). The Kappa statistic ranges between -1 187\\nto 1, where 1 indicates perfect agreement, while 0 indicates agreement by chance. The negative values specify 188\\nsystematic disagreement between annotators. The expression for calculation of Kappa ( \\x14) is as follows: 189\\n\\x14=Po\\x00Pc\\n1\\x00Pc; (1)\\nwher\\nePodenotes the relative observed agreement among raters and Pcdenotes the hypothetical probability 190\\nof agreement by chance. The Kappa value was calculated between each pair of annotators and then averaged. 191\\nThe average Kappa statistic yielded a value of 0.67 for the BAUM-1a database, which can be considered as 192\\nsubstantial agreement [39] between annotators. The highest agreement was between annotator 1 and annotator 193\\n3 (see Fig.5), with a value of 0.76. The average Kappa value for BAUM-1s was 0.51, with a maximum pairwise 194\\nagreement of 0.57, which can be interpreted as moderate agreement. We noticed that annotator 4 had the least 195\\nagreement with the other annotators. If we exclude annotator 4 from the averaging process, the Kappa values 196\\nbecome 0.72 and 0.54 for BAUM-1a and BAUM-1s, respectively. Moderate agreement is expected for BAUM-1s 197\\ndatabase since the emotional and mental state expressions are spontaneous and sometimes subtle, hence they 198\\nare challenging to recognize even for humans. There are also multiple emotions in some sequences (e.g. angrily 199\\nsurprised, happily surprised) and the annotators might have a difﬁculty in choosing the dominant emotion. 200\\n3.4 Facial Feature Point Tracking 201\\nIn many facial expression recognition algorithms, the ﬁrst step is to track the location of salient points (i.e.landmarks) 202\\non the face. We used three different facial landmark tracking methods and compared them experimentally [40], 203\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 10\\nFig.\\n5. The Kappa values for each pair of annotators for BAUM-1a database.\\n(a)\\n (b)\\n (c)\\nFig.\\n6. (a) Facial landmarks (cyan stars) and the head pose angle (written in forehead) have been successfully tracked using the\\nmethod in [40] for most clips in the database. (b) Facial feature tracking sometimes fails under sudden head movements [40]. (c)\\nCHEHRA [41] (left) and IntraFace [42] (right) face trackers have been observed to give better results in difﬁcult cases.\\n[41], [42], [43]. In [40], a model based on mixtures of trees is used, which shows a good performance over a wide 204\\nrange of head poses from frontal to proﬁle. CHEHRA tracker [41] is based on a cascade of linear regressors 205\\nfor incremental training of robust discriminative deformable models, which can automatically adapt to person- 206\\nspeciﬁc properties and imaging conditions. The IntraFace tracker [42] uses a supervised descent method for 207\\nnon-rigid image alignment to track proﬁle-to-proﬁle faces. 208\\nSeveral landmark tracking examples can be seen in Fig. 6 (a), which shows that the method in [40] works 209\\nacceptably well for tracking a total of 68 facial landmarks on the face, which are shown with cyan star signs. 210\\nThe head pose angle in the yaw direction is also estimated in the range [-90,90] degrees with an increment of 211\\n15 degrees, which are shown as written in the forehead in Fig. 6 (a). The face tracker [40] sometimes fails to 212\\nestimate the location of the facial landmarks and the head pose as can be seen in Fig. 6(b). The facial landmarks 213\\nare visually inspected, and acceptable tracking results are used in the facial expression recognition experiments 214\\n(e.g. 1184 of 1222 sequences have acceptable results in BAUM-1s dataset). In such difﬁcult cases, CHEHRA and 215\\nIntraFace trackers have been observed to give better tracking results as shown in Fig. 6 (c), which track 49 facial 216\\npoints. 217\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 11\\nTABLE 3\\nBasic Properties of BAUM-1 Database.\\nProperty BAUM-1a BAUM-1s\\nActed Spontaneous\\nNumber\\nof clips 273 1222\\nNumber of subjects 31 (13 female, 18 male)\\nAge Range of Subjects 19-65 19-65\\nLanguage of clips Turkish Turkish\\nClip Length (min/max/average) in sec. 0.60/16.98/4.07 0.43/9.34/1.82\\nVideo Format AVI MP4/AVI\\nNumber of annotators 5 5\\nKappa value 0.72 0.54\\nNumber\\nof Clips Per Expression\\n(min/max/average scores over 5)\\nHappiness 27 (2.00/4.36/3.28) 179 (1.00/4.86/3.30)\\nAnger 43 (2.43/4.83/3.43) 56 (1.00/5.00/2.86)\\nSadness 38 (1.97/3.10/3.08) 139 (1.00/4.52/2.83)\\nDisgust 35 (2.50/4.45/3.51) 86 (1.00/4.90/3.53)\\nFear 36 (1.61/4.49/3.30) 38 (1.00/4.70/2.98)\\nSurprise - 43 (1.00/4.50/3.38)\\nBoredom 27 (2.12/4.65/3.16) 22 (1.00/4.40/3.24)\\nContempt - 16 (1.50/4.30/3.20)\\nUnsure 38 (1.78/4.20/2.97) 148 (1.00/5.00/3.02)\\nInterest 29 (2.07/3.83/3.00) -\\nNeutral - 187\\nThinking - 112 (1.10/4.33/2.99)\\nConcentrating - 64 (1.00/4.30/2.77)\\nBothered - 91 (1.00/4.40/3.05)\\n3.5\\nProperties and Novelties of the BAUM1 Database 218\\nIn Fig. 7 and Fig. 8 some example frames from the clips in BAUM-1a and BAUM-1s databases are shown, 219\\nrespectively. The properties of the databases are summarized in Table 3. The BAUM-1 database is a novel 220\\ncontribution to the affective computing area since it contains spontaneous recordings of the six basic emotions 221\\nand several mental states in Turkish. 222\\n3.6 Availability 223\\nThe database is publicly available for research purposes only and can be obtained ofﬁcially upon request via 224\\nthe web site [34]. 225\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 12\\n(a)\\n(b) (c) (d) (e) (f) (g) (h)\\nFig.\\n7. Images selected from the video recordings in the BAUM-1a acted database are shown for subject 22 (top) and subject 11\\n(bottom). The ﬁgures in parenthesis indicate the sequence and frame numbers of (top / bottom) images. (a) Anger (s6-f75 / s7-f83)\\n(b) Boredom (s9-f118 / s10-f12) (c) Disgust (s7-f10 / s8-f11) (d) Fear (s4-f108 / s6-f53) (e) Interest (s10-f68 / s11-f2) (f) Happiness\\n(s1-f49 / s3-f74) (g) Sadness (s3-f46 / s4-f74) (h) Unsure (s8-f10 / s11-f11)\\n(a)\\n(b) (c) (d) (e) (f) (g) (h)\\n(i)\\n(j) (k) (l) (m)\\nFig.\\n8. Example frames from the clips in the BAUM-1s spontaneous database for subject 22 (top) and subject 21 (bottom). The ﬁgures\\nin parenthesis indicate the sequence and frame numbers of (top / bottom) images. (a) Anger (s32-f3 / s50-f19) (b) Boredom (s67-f44\\n/ s59-f6) (c) Bothered (s35-f91 / s22-f6) (d) Concentrating (s30-f82 / s21-f15) (e) Contempt (s56-f22 / s36-f130) (f) Disgust (s66-f92 /\\ns78-f14) (g) Fear (s59-f18 / s63-f15) (h) Happiness (s22-f15 / s10-f19) (i) Neutral (s40-f46 / s48-f50) (j) Sadness (s55-f43 / s52-f51)\\n(k) Surprise (s47-f7 / s62-f21) (l) Thinking (s31-f42 / s71-f3) (m) Unsure (s43-f102 / s11-f6)\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 13\\n4 M ULTI-MODAL RECOGNITION OF AFFECTIVE AND MENTAL STATES 226\\nIn order to demonstrate the usefulness and the challenging nature of the collected BAUM-1 database, we 227\\nconducted multi-modal affective and mental state recognition experiments on the acted BAUM-1a and sponta- 228\\nneous BAUM-1s databases. In these experiments, we employ a multi-modal affect recognition algorithm based 229\\non apex frame selection that was recently developed by the authors [44], [45]. Below, we brieﬂy explain the 230\\nvisual and audio features used in the experiments, and the fusion process. We also brieﬂy summarize the apex 231\\nframe selection method presented in [44], [45] for the sake of completeness. The experimental results are given 232\\nin Section 5. 233\\n4.1 Extraction of Visual Features from Video 234\\nThere are many approaches in the literature for facial expression recognition (FER) from images [2], [46]. An 235\\naffective video contains many frames, where emotions are expressed with varying intensities in each frame. 236\\nThus, one of the biggest challenges in emotion recognition from video is determining which frames to use and 237\\nhow to use them in order to attain the maximum possible recognition accuracy. A propitious technique is to 238\\nuse a single frame or a set of selected frames representing the emotional expression with high intensities (i.e. 239\\nat the apex or peak phase). This approach assumes that there is only a single emotion expressed in the whole 240\\nvideo clip. In this work, we use an approach based on apex frame selection [45], which is summarized below. 241\\nThe peak frame selection is preceded by a face detection and alignment process. Then, the procedure followed 242\\nfor computation of visual features is explained. 243\\n4.1.1 Face Detection and Alignment 244\\nBefore selecting the peak frames and extracting the facial features from the video clip, the face is detected 245\\nor tracked at each frame of the video and aligned so that global transformations of the head are minimized 246\\nbetween frames. There are many approaches for face detection and tracking in the literature [40], [47], [48], [49]. 247\\nIn this work, the locations of eyes in all frames are detected utilizing three facial feature trackers, namely the 248\\nalgorithms by Zhu et. al. [40], CHEHRA [41] and IntraFace [42]. The face region is rescaled and cropped to a 249\\nsize of 168\\x02126so that the distance between the eye centers is 64 pixels. The face region is tessellated into 250\\nsub-blocks of size 8\\x026 = 48 and the 18 sub-blocks that are irrelevant to the expression are discarded (e.g 251\\naround the hair and background) [24], [45]. In the remaining (relevant) blocks, which are numbered from 1 to 252\\n30, we extract the Local Phase Quantization features as described below. 253\\n4.1.2 Facial Features 254\\nWe experimented with two different facial features, namely LPQ [50] and POEM [51] features, which are brieﬂy 255\\ndescribed below. 256\\nLocal Phase Quantization (LPQ) features were proposed for blur-insensitive image texture classiﬁcation [50] 257\\nand have also has been successfully employed for facial expression recognition [52]. Local Phase Quantization is 258\\nsimilar to Local Binary Patterns (LBP) [53] in the sense that they both use local histograms to construct feature 259\\nvectors. LBP features have has also been popular for facial expression recognition [54], [55]. Since LPQ has 260\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 14\\ndemonstrated better performance than LBP for facial expression recognition [22], [24], [50], [52], we adopted 261\\nthe former one to extract texture-based (i.e. appearance) features from facial images. 262\\nLPQ features use the phase information of the 2D short-term DFT in local neighborhoods. The resulting 263\\nDFT is sampled at four frequencies. The samples are decorrelated, quantized and represented using integers 264\\nbetween 0-255. Finally, 256-bin histograms in sub-blocks of the face region are concatenated and used as feature 265\\nvectors to represent the face for classiﬁcation. More details about extraction of LPQ features can be found in 266\\n[50]. 267\\nWe exploited the LPQ code available from [56] in our experiments. The DFT is computed in a uniform 268\\nwindow and the window size is 3\\x023. The resulting LPQ feature vector is of length 7680, since there are 30 269\\nblocks, each of which is represented by a 256-bin LPQ histogram. 270\\nPatterns of Oriented Edge Magnitudes (POEM) features [51] of a pixel are calculated by replacing the 271\\nintensity values in the calculation of the traditional LBP features by gradient magnitudes using the accumulated 272\\nlocal histogram of gradient directions over a region around that pixel. Since POEM features are calculated at 273\\ndifferent scales using cells and blocks, it can capture both local and global information. It is also more robust to 274\\nlighting variations as compared to LBP since gradient magnitudes are used instead of pixel intensities. 275\\n4.1.3 Peak Frame Selection 276\\nWe used the maximum dissimilarity based peak frame selection (MAXDIST) method [45] in the experiments. It 277\\nis assumed that the potential peak frames are “maximally dissimilar” to the rest of the frames in the video clip. 278\\nHence, ﬁrst the dissimilarity between frames of a video clip are computed using appearance-based features of 279\\nthe face (e.g. the LPQ features). Then, these scores are arranged in a dissimilarity matrix so that the elements in 280\\ntheithrow of the matrix represent the distance scores between the LPQ histogram vectors of frame iand the 281\\nrest of the frames in the video clip. The scores of each row of the dissimilarity matrix are then averaged and 282\\nordered. Finally, the peak frames are determined by selecting rows corresponding to the highest Kscores, since 283\\nthose frames are estimated to be the most “dissimilar” frames in the video clip. The reader is referred to [45] 284\\nfor further details of the MAXDIST method. 285\\nDuring the experiments, choosing six peak frames for each video clip have been found to give good results. 286\\nThe ﬁnal visual feature vector, x1, of a video clip is calculated by averaging the LPQ feature vectors of the 287\\nselected peak frames. 288\\n4.2 Extraction of Speech Features 289\\nThe Mel-Frequency Cepstral Coefﬁcients (MFCC) [57] and relative spectral features (RASTA) based on per- 290\\nceptual linear prediction (PLP) [58] were utilized to calculate the audio features for emotion recognition. As 291\\na pre-processing step, silent intervals of audio including the leading and trailing edges were eliminated by 292\\nsoft-thresholding the energy over short windows. Then, the MFCC and RASTA-PLP features were extracted 293\\nusing 12 and 20 order ﬁlters, respectively, utilizing a window of length 25msec and an overlap ratio of 50%. 294\\nFinally, the 12 MFCC and 13 RASTA-PLP coefﬁcients were merged with the ﬁrst and second time derivatives 295\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 15\\nand nine statistical functions (such as min, max etc.) were extracted [45]. The above feature extraction process 296\\nproduces an audio feature vector, x2, of length 75\\x029 = 675 , which is used for classiﬁcation. 297\\n4.3 Classiﬁcation and Fusion of Facial and Speech Features 298\\nThe well-known Support Vector Machine (SVM) [59] classiﬁer was employed for classiﬁcation of audio and 299\\nvideo features. The SVM classiﬁer used for the video features utilizes a linear kernel to surpass the curse of 300\\ndimensionality problem since the dimension of video features is high. An SVM classiﬁer with a radial basis 301\\nkernel was used for audio features using one-against-all approach. It is worth noting that audio features were 302\\nnormalized to the interval [0, 1] before classiﬁcation. 303\\nA decision level fusion technique was applied to integrate the decision probabilities of each modality and 304\\nemotion. After a thorough investigation of several probability fusion approaches [60], [61], it was inferred that 305\\ntheweighted product rule was the most successful [62] in our experiments. In particular, in the weighted 306\\nproduct rule, the probabilities attained from each modality for a clip under test are multiplied and then the 307\\nlabel of the maximum product is chosen [45]. 308\\nWe represent the feature vectors of the audio and visual modalities with x2andx1, respectively. Let \\x152and 309\\n\\x151denote the trained classiﬁers for the audio and visual modalities, where the probability estimated for the kth310\\nemotion/mental state in each modality is denoted by P(~!kjxi; \\x15i); i= 1; 2. These probabilities are merged as 311\\ngiven below: 312\\nP(!kjx1; x2) =2∏\\ni=1[P(~!kjxi; \\x15i)]Wi; k= 1; 2; : : : ; 6 (2)\\n!\\x03= max\\nkP(!kjx1; x2); k= 1; 2; : : : ; 6; (3)\\nwhere ~!kand!krepresent the label for the kthemotion without and with fusion, respectively. The parameter 313\\n!\\x03represents the estimated emotion label of the video clip, and Widenotes the weight used for each modality. 314\\n5 B ASELINE AUDIO-VISUAL AFFECTIVE AND MENTAL STATE RECOGNITION EXPERIMENTS 315\\nIn this section, we present the single modality and multi-modal affective and mental state recognition results on 316\\nthe collected BAUM-1a and BAUM-1s databases. We also carried out experiments on the eNTERFACE database, 317\\nwhich is a well-known acted audio-visual database in the literature, which contains six basic emotions. In 318\\nall the experiments given below, we employed 5-fold subject independent cross-validation to ensure subject 319\\nindependent results. 320\\n5.1 Results on BAUM-1a database 321\\nThere are 273 acted video clips in the BAUM-1a database representing 8 emotional and mental states. The 322\\nnumber of clips in each class are given in Table 3. We conducted two sets of experiments on the BAUM-1a 323\\ndatabase using LPQ features and IntraFace tracker [42]. In the ﬁrst set, we used the ﬁve basic emotions, namely, 324\\nanger, disgust, fear, happiness, and sadness. In the second set of experiments we also included boredom, interest 325\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 16\\nTABLE 4\\nSingle and multi-modal affective and mental state recognition accuracies on BAUM-1a database using LPQ features and IntraFace\\ntracker [42].\\n5\\nBasic Emotions 8 Emotions/\\nMental States\\nV\\nideo-Based 47.44 % 31.24 %\\nAudio-Based 71.71 % 63.53 %\\nAudio-Visual (sum rule) 72.33 % 61.09 %\\nAudio-Visual (product rule) 71.56 % 61.24 %\\nAudio-Visual (weighted product rule) 75.32 % 65.84 %\\nT\\nABLE 5\\nConfusion matrix: BAUM-1a database, 5 emotions, audio modality. Figures represent percentages and the average recognition rate\\nis 71.71%.\\nAnger\\nDisgust Fear Happiness Sadness\\nAnger 87.88 6.26 1.82\\n0.0 4.04\\nDisgust 10.0 73.33 5.0 3.33 8.33\\nFear 23.69 8.19 46.62 7.00 14.50\\nHappiness 10.00 6.19 12.38 60.57 10.86\\nSadness 2.00 2.50 5.33 0.00 90.17\\nand\\nunsure in addition to the ﬁve basic emotions. As we can see in Table 4, the audio-based recognition accuracy 326\\n(71.71%) is higher than the video-based accuracy (47.44%) for the 5 emotion case. The audio-visual accuracy is 327\\n75.32% using the weighted product rule with W1= 1andW2= 3. The confusion matrices are given in Table 5, 328\\nTable 6 and Table 7 for the 5 emotion experiments. We can observe from the confusion matrices that anger 329\\n(87.88%) and sadness (90.17%) have the highest two recognition rates from the audio channel (see Table 5), 330\\nand disgust (59.33%) and sadness (63.56%) have the highest two recognition rates from the video channel (see 331\\nTable 6). All the emotions except anger beneﬁt from the fusion process and the average recognition rate after 332\\nfusion rises to 75.32% (see Table 7). 333\\nThe gap between audio (63.53%) and video accuracies (31.24%) becomes larger when we include boredom, 334\\ninterest and unsure to the experiments. The confusion matrices for the 8 class experiments are given in Table 8, 335\\nTable 9 and Table 10. Similar to the 5 class case, anger (82.75%) and sadness (76.78%) have the highest two 336\\nrecognition rates from the audio channel (see Table 8). Disgust (74.13%) and anger (52.97%) have the highest 337\\ntwo recognition rates from the video channel (see Table 9). Boredom can not be recognized from the video 338\\nchannel at all. The average recognition rate rises to 65.84% after decision level fusion. 339\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 17\\nTABLE 6\\nConfusion matrix: BAUM-1a database, 5 emotions, visual modality using LPQ features and IntraFace tracker. Figures represent\\npercentages and the average recognition rate is 47.44%.\\nAnger\\nDisgust Fear Happiness Sadness\\nAnger 51.77 9.87 12.93\\n17.37 8.06\\nDisgust 12.50 59.33 11.50 0.00 16.67\\nFear 21.21 10.19 32.90 16.52 19.17\\nHappiness 13.52 9.52 48.10 22.67 6.19\\nSadness 13.11 11.33 6.00 6.00 63.56\\nT\\nABLE 7\\nConfusion matrix: BAUM-1a database, 5 emotions, audio-visual using LPQ features and IntraFace tracker. Fusion is done with\\nweighted product rule (W 2= 3). Figures represent percentages and the average recognition rate is 75.32%.\\nAnger\\nDisgust Fear Happiness Sadness\\nAnger 78.71 12.93\\n4.04 2.5 1.82\\nDisgust 10.00 81.67 5.00 0.00 3.33\\nFear 21.21 2.00 63.29 9.00 4.50\\nHappiness 6.67 9.52 12.86 64.10 6.86\\nSadness 2.00 2.50 4.00 0.00 91.50\\nT\\nABLE 8\\nConfusion matrix: BAUM-1a database, 8 emotions/mental states, audio modality. Figures represent percentages and the average\\nrecognition rate is 63.53%.\\nAnger\\nBoredom Disgust Fear Happiness Interest Sadness Unsure\\nAnger 82.75 4.72 8.48 0.00\\n0.00 0.00 4.04 0.00\\nBoredom 18.00 47.50 12.50 0.00 8.00 0.00 10.00 4.00\\nDisgust 10.00 2.50 76.67 2.50 5.83 0.00 2.50 0.00\\nFear 17.69 4.00 11.19 56.62 2.00 0.00 4.50 4.00\\nHappiness 10.00 0.00 6.19 6.67 66.95 0.00 6.86 3.33\\nInterest 10.00 2.86 6.19 5.00 0.00 59.29 10.00 6.67\\nSadness 2.00 0.00 11.17 2.00 0.00 3.33 76.78 4.72\\nUnsure 3.33 2.00 7.67 13.67 4.00 5.00 22.67 41.67\\nMar\\nch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 18\\nTABLE 9\\nConfusion matrix: BAUM-1a database, 8 emotions/mental states, visual modality. Figures represent percentages and the average\\nrecognition rate is 31.24%.\\nAnger\\nBoredom Disgust Fear Happiness Interest Sadness Unsure\\nAnger 52.97 0.00 12.30\\n3.83 8.94 0.00 14.64 7.32\\nBoredom 21.48 0.00 2.63 16.34 26.35 4.22 13.70 15.28\\nDisgust 2.50 0.00 74.13 6.15 6.85 2.63 3.51 4.22\\nFear 21.15 0.00 3.51 24.24 2.11 13.25 18.47 17.27\\nHappiness 23.11 0.00 10.04 0.00 42.06 7.03 7.73 10.04\\nInterest 11.66 0.00 24.59 34.63 6.52 6.02 0.00 16.56\\nSadness 26.57 0.00 9.13 21.08 6.15 2.34 25.06 9.66\\nUnsure 28.33 0.00 7.73 18.27 7.73 4.22 13.70 20.03\\nT\\nABLE 10\\nConfusion matrix: BAUM-1a database, 8 emotions/mental states, audio-visual . Fusion is done with weighted product rule (W 2= 3).\\nFigures represent percentages and the average recognition rate is 65.84%.\\nAnger\\nBoredom Disgust Fear Happiness Interest Sadness Unsure\\nAnger 84.29 4.72 4.44 0.00\\n2.50 0.00 4.04 0.00\\nBoredom 14.00 45.00 10.00 2.50 8.00 0.00 10.00 10.50\\nDisgust 7.50 2.50 76.67 2.50 8.33 0.00 2.50 0.00\\nFear 14.86 0.00 8.19 61.95 4.50 0.00 4.50 6.00\\nHappiness 6.67 0.00 3.33 6.67 70.29 3.33 9.71 0.00\\nInterest 10.00 0.00 5.71 2.86 0.00 64.76 13.33 3.33\\nSadness 0.00 0.00 6.94 4.22 0.00 3.33 78.56 6.94\\nUnsure 3.33 0.00 5.67 13.67 4.00 7.00 16.67 49.67\\n5.2\\nResults on BAUM-1s database 340\\nWe carried experiments on the BAUM-1s database, which has a total of 1222 clips from 31 subjects. There are 341\\n13 emotional and mental states, which are Anger (An), Disgust (Di), Fear (Fe), Happiness (Ha), Sadness(Sa), 342\\nSurprise (Su), Boredom (Bo), Contempt (Co), Unsure (Un), Neutral (Ne), Thinking (Th), Concentrating (Con), 343\\nBothered (Bot). 344\\nBelow, we present the results of two different sets of experiments using LPQ features and IntraFace tracker 345\\n[42]. In the ﬁrst set, we used the six basic emotions (544 clips), and in the second set, we used all the 13 classes 346\\nlisted in the previous paragraph. The single and multi-modal affective and mental state recognition accuracies 347\\nare given in Table 11. We can see that combining the audio and video based results at the decision level increases 348\\nthe recognition rate by 6%for the six emotion case and by 0:5%for the 13 emotion case. 349\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 19\\nTABLE 11\\nSingle and multi-modal affective and mental state recognition accuracies on BAUM-1s database using LPQ features and IntraFace\\ntracker [42].\\n6\\nBasic Emotions 13 Emotions/\\nMental States\\nV\\nideo-Based 45.04% 25.17%\\nAudio-Based 29.41% 15.29%\\nAudio-Visual (sum rule) 45.78% 24.91%\\nAudio-Visual (product rule) 48.57% 25.19%\\nAudio-Visual (weighted product rule) 51.29% 25.76%\\nT\\nABLE 12\\nSingle and multi-modal emotion recognition accuracies on eNTERFACE database using LOSO cross-validation (based on LPQ\\nfeatures and IntraFace tracker [42].)\\nLOSO\\nCV\\nV\\nideo-based Results 42.16%\\nAudio-based Results 72.95%\\nAudio-Visual Results (Sum Rule) 74.95%\\nAudio-Visual Results (Product Rule) 76.48%\\nAudio-Visual Results (Weighted Product Rule, W2= 3) 77.02%\\n5.3\\nResults on eNTERFACE database 350\\nWe also carried out audio-visual emotion recognition experiments on the eNTERFACE’05 dataset [15], which 351\\ncontains clips of 44 subjects from 14 different nationalities. The subjects are asked to act the six basic emotions 352\\nwhile uttering selected sentences in English with target emotions. 353\\nIn the following, we report the emotion recognition results not only for each modality but also after decision 354\\nlevel fusion. We use the LPQ features together with the IntraFace tracker [42] to extract the facial features. Since 355\\nthe samples are distributed almost uniformly over the emotions for each subject, we used a leave-one-subject- 356\\nout cross validation method (LOSO). In Table 12, we present the experimental results of subject independent 357\\naudio-visual emotion recognition rates. It can be observed that for visual and audio modalities, emotion 358\\nrecognition accuracies are 42.16% and 72.95%, respectively. We obtained an accuracy of 77.02% after decision 359\\nlevel fusion. The reported results clearly indicate that audio based classiﬁcation is better as compared to the 360\\nvisual-features based classiﬁcation. The confusion matrices for the audio, visual and audio-visual experiments 361\\nare given in Table 13, Table 14 and Table 15, respectively. It is also worth noting that multi-modal fusion is 362\\nbeneﬁcial for enhancing the recognition accuracy of all emotions except anger. Anger and happiness have 363\\nhighest emotion recognition accuracies, which are 83.62% and 86.13%, respectively, after fusion. Happiness and 364\\ndisgust are the emotions that experience the highest increases in their recognition rates after audio-visual fusion, 365\\nsince they show an increase of 11% and 4% in their accuracy, respectively. 366\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 20\\nTABLE 13\\nConfusion matrix: eNTERFACE database, audio modality (LOSO). Figures represent percentages and the average recognition rate\\nis 72.95%\\nEstimated\\nEmotion\\nAnger\\nDisgust Fear Happiness Sadness Surprise\\nAnger 88.4 1.4 3.3\\n2.3 2.3 2.3\\nDisgust 5.6 71.2 7.0 4.2 6.0 6.0\\nFear 7.4 9.8 64.2 4.7 7.4 6.5\\nHappiness 7.0 4.7 1.9 75.3 7.0 4.2\\nSadness 3.3 6.5 5.1 6.0 72.6 6.5\\nSurprise 4.2 5.1 7.4 6.5 10.7 66.0\\nT\\nABLE 14\\nConfusion matrix: eNTERFACE database, video modality (LOSO). Figures represent percentages and the average recognition rate\\nis 42.16%.\\nEstimated\\nEmotion\\nAnger\\nDisgust Fear Happiness Sadness Surprise\\nAnger 23.43 20.13\\n8.00 14.41 11.56 22.48\\nDisgust 4.89 59.49 5.78 21.46 5.27 3.11\\nFear 7.94 12.32 23.05 14.03 25.40 17.27\\nHappiness 4.95 14.13 5.52 59.94 4.38 11.08\\nSadness 6.79 10.92 13.71 7.17 44.89 16.51\\nSurprise 8.25 4.13 10.16 21.33 13.71 42.41\\nT\\nABLE 15\\nAudio-visual confusion matrix for the eNTERFACE database (LOSO). Figures represent percentages and the average recognition\\nrate is 77.02%.\\nEstimated\\nEmotion\\nAnger\\nDisgust Fear Happiness Sadness Surprise\\nAnger 83.62 3.56 4.89\\n3.37 1.02 3.56\\nDisgust 3.24 74.92 9.90 6.22 3.68 2.03\\nFear 7.81 6.60 65.65 1.46 7.49 10.98\\nHappiness 1.33 5.94 0.44 86.13 3.33 2.83\\nSadness 2.22 4.51 4.89 1.46 79.56 7.37\\nSurprise 3.11 0.89 9.27 7.68 6.67 72.38\\nMar\\nch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 21\\nTABLE 16\\nSingle and multi-modal emotion recognition accuracies on eNTERFACE, BAUM-1a and BAUM-1s databases using three different\\nface trackers (Zhu [40], CHEHRA [41] and IntraFace [42]) and two different facial features (LPQ [50], POEM [51]. Figures are given in\\npercentages.\\neNTERF\\nACE BAUM-1a\\nBAUM-1a BAUM-1s\\nBAUM-1s\\n(6 emotions) (5\\nemotions) (8 classes) (6\\nemotions) (13 classes)\\nAudio-only 72.95 71.71 63.53 29.41 15.29\\nV\\nideo-only (Zhu+LPQ) 38.22 46.60 26.30 43.75 24.07\\nV\\nideo-only (Zhu+POEM) 32.95 48.69 29.64 46.32 23.65\\nV\\nideo-only (CHEHRA+LPQ) 40.08 43.65 30.16 43.38 25.08\\nV\\nideo-only (CHEHRA+POEM) 33.26 45.13 31.53 44.30 25.00\\nV\\nideo-only (IntraFace+LPQ) 42.16 47.44 31.24 45.04 25.17\\nV\\nideo-only (IntraFace+POEM) 36.12 46.24 32.39 47.06 23.56\\nAudio-V\\nisual (Zhu+LPQ) 76.79 74.42 65.06 50.00 26.18\\nAudio-V\\nisual (Zhu+POEM) 75.45 76.38 63.45 50.37 25.51\\nAudio-V\\nisual (CHEHRA+LPQ) 77.40 73.94 63.53 50.37 25.42\\nAudio-V\\nisual (CHEHRA+POEM) 74.55 75.24 63.83 49.26 25.25\\nAudio-V\\nisual (IntraFace+LPQ) 77.02 75.32 65.84 51.29 25.76\\nAudio-V\\nisual (IntraFace+POEM) 75.78 76.54 64.13 50.18 25.68\\n5.4\\nSummary of the Experimental Results 367\\nIn order to compare the emotion recognition accuracies on the acted eNTERFACE and BAUM-1 databases, 368\\nwe summarize the uni-modal and multi-modal accuracies in Table 16 using three different face trackers (Zhu, 369\\nCHEHRA and IntraFace) and two different facial features (LPQ and POEM). We can see that eNTERFACE 370\\nand BAUM-1a databases, which are both acted in different languages show similar characteristics, in the sense 371\\nthat the emotion recognition accuracy from audio is much higher as compared to the emotion recognition 372\\naccuracy from video. However, the audio-based accuracies are much lower than the video-based accuracies for 373\\nthe BAUM-1s database both for the 6 class and 13 class cases. BAUM-1s accuracies for the 13 class case are much 374\\nlower as compared to the 6 class case, which implies that recognition of mental states is quite challenging. If we 375\\ncompare the video-based results, we can observe that IntraFace tracker gives slightly higher accuracies in most 376\\nof the cases except for BAUM-1a (5 emotions). If we compare the audio-visual accuracies in Table 16, we can 377\\nconclude that the results using three different face trackers and facial features are comparable on all databases 378\\nwith 1-2% differences in accuracies. 379\\n6 C ONCLUSION 380\\nWe presented a new spontaneous audio-visual Turkish database, BAUM-1 containing expressions of affective 381\\nas well as mental states. The challenging nature of the database was demonstrated via baseline audio-visual 382\\nemotion recognition experiments based on a peak frame selection approach. Although current algorithms can 383\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 22\\nachieve reasonable recognition rates on acted expressions, which are somewhat exaggerated, the accuracies 384\\ndrop dramatically for naturalistic and subtle expressions. 385\\nThe BAUM-1 database can be useful a resource to the affective computing community as it contains multi- 386\\nmodal, close-to-natural, affective and mental state expressions as opposed to the mostly single-modality and 387\\nposed databases available in the literature. The database can be combined with other databases in other 388\\nlanguages to investigate multi-language cross-corpora aspects of emotion recognition from speech. Investigating 389\\nthe correlation between speech-related and face-related features and emotion-independent face recognition 390\\ncould be other directions for future research. 391\\nACKNOWLEDGMENTS 392\\nThe authors would like to thank Prof. Metehan Irak from the Department of Physchology, Bahcesehir University, 393\\nfor the useful discussions during the design of the stimuli video. 394\\nREFERENCES 395\\n[1] N. Sebe, I. Cohen, and T. S. Huang, Internet Imaging VI,\\nser. ISBN / ISSN: 0-8194-5643-8, 2005, vol. 5670, no. 5670, ch. 396\\nMultimodal Approaches for Emotion Recognition: A Survey, pp. 56 – 67. 397\\n[2] Z. H. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, “A survey of affect recognition methods: Audio, visual, and spontaneous 398\\nexpressions,” IEEE T\\nransactions onPattern Analysis andMachine Intelligence,\\nvol. 31, no. 1, pp. 39–58, 2009. 399\\n[3] A. Ryan, J. Cohn, S. Lucey, J. Saragih, P. Lucey, F. D. la Torre, and A. Rossi, “Automated facial expression recognition system,” 400\\ninPr\\noceedings oftheInternational Carnahan Confer\\nence onSecurity T\\nechnology, 2009, pp. 172–177. 401\\n[4] G. C. Littlewort, M. S. Bartlett, and K. Lee, “Automatic coding of facial expresssions displayed during posed and genuine pain,” 402\\nImage andV\\nision Computing,\\nvol. 27, no. 12, pp. 1797–1803, 2009. 403\\n[5] A. B. Ashraf, S. Lucey, J. F. Cohn, T. Chen, Z. Ambadar, K. M. Prkachin, and P. E. Solomon, “The painful face - pain expression 404\\nrecognition using active appearance models,” Image andV\\nision Computing,\\nvol. 27, no. 12, pp. 1788–1796, 2009. 405\\n[6] P. Ekman and W. V. Friesen, Pictur\\nesofFacial Ef\\nfect. Palo Alto, CA: Consulting Psychologists Press, 1976. 406\\n[7] J. Bassili, “Emotion recognition: the role of facial movement and the relative importance of upper and lower areas of the face,” 407\\nJ.Pers. Soc.Psychol,\\nvol. 37, pp. 2049–2058, 1979. 408\\n[8] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews, “The extended cohn-kanade dataset (CK+): A 409\\ncomplete dataset for action unit and emotion-speciﬁed expression,” in Pr\\noceedings ofIEEE workshop onCVPR forHuman 410\\nCommunicative Behavior Analysis,\\nSan Francisco, USA, 2010. 411\\n[9] T. Banziger and K. R. Scherer, Blueprint forAf\\nfective Computing: ASour\\ncebook. Oxford University Press, 2010, ch. Introducing 412\\nthe Geneva multimodal emotionportrayal (GEMEP) corpus, pp. 271–294. 413\\n[10] A. Savran, H. D. N. Alyuz, O. Celiktutan, B. Gkberk, B. Sankur, and L. Akarun, “Bosphorus database for 3D face analysis,” in 414\\nFirst COST 2101 W\\norkshop onBiometrics andIdentity Management (BIOID 2008),\\n2008. 415\\n[11] E. Sariyanidi, H. Gunes, and A. Cavallaro, “Automatic analysis of facial affect: A survey of registration, representation and 416\\nrecognition,” IEEE T\\nransactions onPattern Analysis andMachine Intelligence,\\n2014. 417\\n[12] M. J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial expressions with gabor wavelets,” in Pr\\noc.Thir\\ndIEEE 418\\nInternational Confer\\nence onAutomatic Face andGestur\\neRecognition,\\nJapan, 1998, pp. 200–205. 419\\n[13] M. Pantic, M. F. Valstar, R. Rademaker, and L. Maat, “Web-based database for facial expression analysis,” in Pr\\noc.IEEE Int. 420\\nConf. onMultimedia andExpo (ICME’05),\\nhttp://www.mmifacedb.com/, Amsterdam, The Netherlands, 2005. 421\\n[14] M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. R. Scherer, “The ﬁrst facial expression recognition and anaylsis challenge,” 422\\ninIEEE Int.Conf. Face andGestur\\neRecognition (FG’2011),\\n2011. 423\\n[15] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The eNTERFACE05 audio-visual emotion database,” in Pr\\noceedings oftheFirst 424\\nIEEE W\\norkshop onMultimedia Database Management,\\n2006. 425\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 23\\n[16] M. Grimm, K. Kroschel, and S. Narayanan, “The vera am mittag german audio-visual emotional speech database,” in Proc.Int. 426\\nConf. Multimedia andExpo (ICME),\\n2008. 427\\n[17] G. Mckeown, M. F. Valstar, R. Cowie, M. Pantic, and M. Schroeder, “The SEMAINE database: Annotated multimodal records 428\\nof emotionally coloured conversations between a person and a limited agent,” IEEE T\\nrans. Af\\nfective Computing,\\nvol. 3, no. 1, 429\\npp. 5–17, 2012. 430\\n[18] F. Wallhoff, “Facial expressions and emotion database [online],” Available: http://www.mmk.ei.tum.de/ waf/fgnet/feedtum.html, 431\\n2006. 432\\n[19] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F. Cohn, “Disfa: A spontaneous facial action intensity database,” 433\\nIEEE T\\nransactions onAf\\nfective Computing,\\nvol. 4, no. 2, pp. 151–160, 2013. 434\\n[20] C. Busso, M. Bulut, C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. .Narayanan, “IEMOCAP: 435\\nInteractive emotional dyadic motion capture database,” Journal ofLanguage Resour\\ncesand Evaluation,\\nvol. 42, no. 4, pp. 436\\n335–359, 2008. 437\\n[21] E. Douglas-Cowie, R. Cowie, and M. Schoder, “A new emotion database: Considerations, sources and scope,” in Pr\\noc.ISCA 438\\nITR\\nWonSpeech andEmotion,\\n2000, pp. 39–44. 439\\n[22] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, “Collecting large, richly annotated facial-expression databases from movies,” 440\\nIEEE Multimedia,\\nvol. 19, no. 3, pp. 34–41, 2012. 441\\n[23] C. Turan, C. Kansin, S. Zhalehpour, Z. Aydin, and C. E. Erdem, “A method for extraction of audio-visual facial clips from 442\\nmovies,” in IEEE Signal Pr\\nocessing andApplications Confer\\nence (SIU),\\nGirne, Northern Cyprus, April 2013. 443\\n[24] C. E. Erdem, C. Turan, and Z. Aydin, “BAUM-2: A multilingual audio-visual affective face database,” Multimedia T\\nools and 444\\nApplications,\\n2014. 445\\n[25] D. McDuff, R. E. Kaliouby, and R. W. Picard, “Crowdsourcing facial responses to online videos,” IEEE T\\nransactions onAf\\nfective 446\\nComputing,\\nvol. 3, no. 4, pp. 456–468, 2012. 447\\n[26] J.-Y. Zhu, A. Agarwala, A. A. Efros, E. Shechtman, and J. Wang, “Mirror mirror: Crowdsourcing better portraits,” ACM 448\\nT\\nransactions onGraphics (SIGGRAPH Asia 2014),\\nvol. 33, no. 6, 2014. 449\\n[27] G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and L. V. Gool, “A 3-D audio-visual corpus of affective communication,” IEEE T\\nrans. 450\\nonMultimedia,\\nvol. 12, no. 6, pp. 591–598, 2010. 451\\n[28] X. Zhang, L. Yin, J. Cohn, S. Canavan, M. Reale, A. Horowitz, and P. Liu, “A highresolution spontaneous 3d dynamic facial 452\\nexpression database,” in International confer\\nence onautomatic face andgestur\\ner\\necognition (FG’13),\\nShanghai, China, April 453\\n2013. 454\\n[29] J. J. Gross and R. W. Levenson, “Emotion elicitation using ﬁlms,” Cognition andEmotion,\\nvol. 9, no. 1, pp. 87–108, 1995. 455\\n[30] R. Westermann, K. Spies, G. Stahl, and F. W. Hesse, “Relative effectiveness and validity of mood induction procedures: a 456\\nmeta-analysis,” Eur\\nopean Journal ofSocial Psychology,\\nvol. 26, no. 4, pp. 557 – 580, 1996. 457\\n[31] X. Zhang, L. Yin, J. F. Cohn, S. J. Canavan, M. Reale, A. Horowitz, and P. Liu, “A high-resolution spontaneous 3d dynamic facial 458\\nexpression database,” in IEEE Int.Conf. onAutomatic Face andGestur\\neRecognition,\\n2013. 459\\n[32] P. J. Lang, M. M. Bradley, and B. N. Cuthbert, “International affectivepicture system (iaps): Technical manual and affective 460\\nratings,” NIMH Center for the Study of Emotion and Attention, Tech. Rep., 1997. 461\\n[33] M. C. Escher, “http://www.mcescher.com/.” 462\\n[34] O. Onder, S. Zhalehpour, and C. E. Erdem, “Bahcesehir university multimodal face database of spontaneous affective ans 463\\nmental states (BAUM-1),” http://baum1.bahcesehir.edu.tr/, 2014. 464\\n[35] R. Cowie, C. Cox, J.-C. Martin, A. Batliner, D. Heylen, and K. Karpouzis, Emotion-Oriented Systems: TheHumaine Handbook. 465\\nBerlin,\\nHeidelberg: Springer-Verlag, 2011, ch. Issues in Data Labelling, pp. 215–244. 466\\n[36] R. A. Kaliouby, “Mind-reading machines: automated inference of complex mental states,” Cambridge, ISSN 1476-2986 UCAM- 467\\nCL-TR-636, 2005. 468\\n[37] “Mind reading software,” essica Kingsley Publishers, 2003. 469\\n[38] M. W. Watkins and M. Pacheco, “Interobserver agreement in behavioral research,” Journal ofBehavioral Education,\\nvol. 10, 470\\nno. 4, pp. 205–212, 2000. 471\\n[39] A. J. Viera and J. M. Garrett, “Understanding interobserver agreement: The kappa statistic,” Family Medicine,\\nvol. 37, no. 5, 472\\n2005. 473\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 24\\n[40] X. Zhu and D. Ramanan, “Face detection, pose estimation and landmark localization in the wild,” in Computer Vision and 474\\nPattern Recognition (CVPR),\\nRhode Island, 2012. 475\\n[41] A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic, “Incremental face alignment in the wild,” in InPr\\noc.of2013 IEEE Confer\\nence 476\\nonComputer V\\nision andPattern Recognition (CVPR),\\n2014. 477\\n[42] X. Xuehan and F. D. la Torre, “Supervised descent method and its applications to face alignment,” in InPr\\noc.IEEE Confer\\nence 478\\nonComputer V\\nision andPattern Recognition (CVPR 2013),\\nPortland, Oregon, USA, June 2013, pp. 532 – 539. 479\\n[43] ——, “Global supervised descent method,” in InPr\\noc.IEEE Confer\\nence onComputer V\\nision andPattern Recognition (CVPR 480\\n2015),\\nBoston, MA, USA, June 2015, pp. 2664 – 2673. 481\\n[44] S. Zhalehpour, Z. Akhtar, and C. E. Erdem, “Multimodal emotion recognition with automatic peak frame selection,” in IEEE 482\\nInternational Symposioum onInnovations inIntelligent Systems andApplications (INIST\\nA), 2014, pp. 116–121. 483\\n[45] ——, “Multimodal emotion recognition based on peak frame selection from video,” Signal, Image andV\\nideo Pr\\nocessing, 2015, 484\\nDOI: 10.1007/s11760-015-0822-0. 485\\n[46] S. Ulukaya and C. E. Erdem, “Gaussian mixture model based estimation of the neutral face shape for emotion recognition,” 486\\nvol. 32, pp. 11–23, 2014. 487\\n[47] P. Viola and M. J. Jones, “Robust real-time face detection,” International Journal ofComputer V\\nision, vol. 57, no. 2, pp. 137–154, 488\\n2004. 489\\n[48] C. E. Erdem, S. Ulukaya, A. Karaali, and A. T. Erdem, “Combining haar feature and skin color based classiﬁers for face 490\\ndetection,” in IEEE 36th International Confer\\nence onAcoustics, Speech andSignal Pr\\nocessing (ICASSP 2011),\\nPrague, 2011. 491\\n[49] J. M. Saragih, S. Lucey, and J. F. Cohn, “Deformable model ﬁtting by regularized landmark mean-shift,” International Journal 492\\nofComputer V\\nision (IJCV),\\nvol. 91, pp. 200–215, 2011. 493\\n[50] V. Ojansivu and J. Heikkil, “Blur insensitive texture classiﬁcation using local phase quantization,” Lectur\\neNotes inComputer 494\\nScience,\\nvol. 5099, pp. 236–243, 2008. 495\\n[51] N. Vu and A. Caplier, “Face recognition with patterns of oriented edge magnitudes,” in Eur\\nopean Confer\\nence onComputer 496\\nV\\nision (ECCV),\\nser. LNCS, vol. 6311, 2010, pp. 313–326. 497\\n[52] A. Dhall, R. Goecke, and T. Gedeon, “Emotion recognition using PHOG and LPQ features,” in InPr\\noc.oftheW\\norkshop on 498\\nFacial Expr\\nession Recognition andAnalysis Challenge FERA2011, IEEE Automatic Face andGestur\\neRecognition Confer\\nence 499\\nFG2011,\\nSanta Barbara (CA), USA, 2011. 500\\n[53] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and rotation invariant texture classiﬁcation with local 501\\nbinary patterns,” IEEE T\\nrans. Pattern Anlaysis andMachine Intelligence,\\nvol. 24, no. 7, pp. 971–987, 2002. 502\\n[54] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recognition based on local binary patterns: A comprehensive study,” 503\\nImage andV\\nision Computing,\\nvol. 27, pp. 803–816, 2009. 504\\n[55] G. Zhao and M. Pietikainen, “Dynamic texture recognition using local binary patterns with an application to facial expressions,” 505\\nIEEE T\\nrans. Pattern Anlaysis andMachine Intelligence,\\nvol. 29, no. 6, pp. 915–928, 2007. 506\\n[56] “Machine vision group, matlab codes for local phase quantization,,” http://www.cse.oulu.ﬁ/CMV/Downloads/LPQMatlab, 507\\nlast Accessed: 01/07/2013. 508\\n[57] X. Huang, A. Acero, and H. Hon, Spoken Language Pr\\nocessing: Aguide totheory\\n,algorithm, and system development. 509\\nPr\\nentice Hall, 2001. 510\\n[58] H. Hermansky and N. Morgan, “Rasta processing of speech,” IEEE T\\nransactions onSpeech andAudio Pr\\nocessing, vol. 2, no. 4, 511\\npp. 578–589, 1994. 512\\n[59] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” ACM T\\nransactions onIntelligent Systems and 513\\nT\\nechnology, vol. 2, pp. 27:1–27:27, 2011. 514\\n[60] P. K. Atrey, M. A. Hossain, A. E. Saddik, and M. S. Kankanhalli, “Multimodal fusion for multimedia analysis: a survey,” 515\\nMultimedia Systems,\\nvol. 16, pp. 345–379, 2010. 516\\n[61] J. Kittler, M. H. R. P. Duin, and J. Matas, “On combining classiﬁers,” IEEE T\\nrans. Pattern Analysis andMachine Intelligence, 517\\nvol.\\n20, no. 3, p. 226239, 1998. 518\\n[62] Y. Wang, L. Guan, and A. N. Venetsanopoulos, “Kernel cross-modal factor analysis for information fusion with application to 519\\nbimodal emotion recognition,” IEEE T\\nransactions onMultimedia,\\nvol. 14, no. 3, pp. 597–607, 2012. 520\\nMarch 16, 2016 DRAFT\\n1949-3045 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2553038, IEEE\\nTransactions on Affective Computing\\nJOURNAL\\nOF LATEX CLASS FILES, VOL. XX, NO. X, SEPTEMBER XXXX 25\\nSaraZhalehpour received both the B.Sc. and M.Sc. degrees in Telecommunications Engineering from the 521\\nUniversity of Tabriz, Iran in 2009 and 2012, respectively. She received her second M.Sc. degree in Electrical 522\\nand Electronics Engineering from Bahcesehir University, Turkey in 2014. She joined INRS-EMT, Montreal, 523\\nCanada as a PhD student in September 2014. Her main research areas of interest are audio-visual emotion 524\\nrecognition and human-computer interaction. 525\\n526\\nOn\\nur Onder received the B.Sc. degree in Electrical and Electronics Engineering from Ege University, Izmir 527\\nin 2010. He received the M.Sc. degree in Electrical and Electronics Engineering from Bahcesehir Univer- 528\\nsity,Istanbul in 2014. He is currently a Ph.D. student at Dokuz Eylul University, Izmir. His research interests 529\\ninclude digital image and video processing, human-computer interaction, machine learning, and remote sens- 530\\ning. 531\\n532\\nZahid\\nAkhtar received the Ph.D. degree in Electronic and Computer Engineering from the University of 533\\nCagliari, Italy in 2012. Currently, he is working as a research associate in the Department of Mathematics 534\\nand Computer Science at the University of Udine, Italy. His research interests include computer vision, pattern 535\\nrecognition and image processing with applications to biometrics, affective computing and security systems. 536\\n537\\nCigdem\\nEroglu Erdem received the B.S. and M.S. degrees in Electrical and Electronics Engineering from 538\\nBilkent University, Ankara, Turkey in 1995 and 1997, respectively. She received the Ph.D. degree in Electri- 539\\ncal and Electronics Engineering from Bogazici University, Istanbul, in 2002. From September 2000 to June 540\\n2001, she was a visiting researcher in the Department of Electrical and Computer Engineering, University of 541\\nRochester, NY , USA. Between 2003-2004, she was a postdoctoral fellow at the Faculty of Electrical Engineer- 542\\ning at Delft University of Technology, the Netherlands, where she was also afﬁliated with the video processing 543\\ngroup at Philips Research Laboratories, Eindhoven. She is currently a professor in the Department of Electrical 544\\nand Electronics Engineering at Bahcesehir University, Istanbul, Turkey. Her research interests are in the areas of digital image and 545\\nvideo processing, including affective computing, vision and scene understanding and human computer 546\\nMarch 16, 2016 DRAFT\\n',\n",
       " '498 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nBilinear Models for 3-D Face and\\nFacial Expression Recognition\\nIordanis Mpiperis, Sotiris Malassiotis, and Michael G. Strintzis , Fellow, IEEE\\nAbstract— In this paper, we explore bilinear models for jointly\\naddressing 3-D face and facial expression recognition. An elasti-cally deformable model algorithm that establishes correspondence\\namong a set of faces is proposed ﬁrst and then bilinear models that\\ndecouple the identity and facial expression factors are constructed.Fitting these models to unknown faces enables us to perform facerecognition invariant to facial expressions and facial expressionrecognition with unknown identity. A quantitative evaluation ofthe proposed technique is conducted on the publicly availableBU-3DFE face database in comparison with our previous workon face recognition and other state-of-the-art algorithms forfacial expression recognition. Experimental results demonstratean overall 90.5% facial expression recognition rate and an 86%rank-1 face recognition rate.\\nIndex Terms— Bilinear model, elastically deformable model, 3-D\\nface recognition, 3-D facial expression recognition.\\nI. I NTRODUCTION\\nTHE 3-D geometry of the face conveys valuable informa-\\ntion on the identity as was recently demonstrated by 3-D\\nface-recognition research [3], [4]. But in addition, deformationof the facial surface caused by facial expressions may also pro-vide cues that are useful for automatic facial expression classi-\\nﬁcation.\\nTraditionally, researchers have examined these problems\\nseparately (e.g., using neutral images for face recognition andknown identity for facial expressions). In practice, however,\\nsuch decoupling does not exist. Thus, an ideal 3-D face analysis\\nsystem should be able to recognize simultaneously: 1) facesacross any possible expression and 2) facial expressions andtheir intensity independent of identity, gender, and ethnicity.\\nAlso, in order to be applicable under real-life conditions, this\\nface analyzer should meet additional requirements, such as full\\nManuscript received November 20, 2007; revised March 19, 2008. First pub-\\nlished June 6, 2008; last published August 13, 2008 (projected). This work\\nwas supported in part by the European Commission under the FP6 IST Project:\\n“PASION-Psychologically Augmented Social Interaction over Networks” (con-\\ntract No FP6-027654) and FP6 IST Network of Excellence: “3DTV-IntegratedThree-Dimensional Television—Capture, Transmission, and Display” (contract\\nFP6-511568) and in part by the Greek Ministry of Development-General Sec-\\nretariat of Research and Technology under Project: PENED 2003 Ontomedia\\n(03E\\n/1475). The associate editor coordinating the review of this manuscript and\\napproving it for publication was Prof. Vijaya Kumar Bhagavatula.\\nI. Mpiperis and M. G. Strintzis are with the Centre for Research and Tech-\\nnology Hellas/Informatics and Telematics Institute, Thessaloniki 57001, Greece\\nand also with the Information Processing Laboratory of Electrical and Computer\\nEngineering Department, Aristotle University of Thessaloniki, Thermi-Thessa-loniki 541 24, Greece (e-mail: iordanis@iti.gr; strintzi@iti.gr).\\nS. Malassiotis is with the Centre for Research and Technology Hellas/Infor-\\nmatics and Telematics Institute, Thermi-Thessaloniki 57001, Greece (e-mail:\\nmalasiot@iti.gr).\\nDigital Object Identiﬁer 10.1109/TIFS.2008.924598automation in all stages of processing from 3-D data acquisi-\\ntion to face and facial expression classiﬁcation, near-real time\\nresponse and robustness to head pose, aging effects, and partialor self occlusion.\\nIn this paper, we present a technique for joint 3-D identity-in-\\nvariant facial expression recognition and expression-invariant\\nface recognition (Fig. 1). First, we present a novel model-basedframework for establishing correspondence among 3-D pointclouds of different faces. This allows us to subsequently create\\nbilinear models that decouple expression and identity factors.\\nA bootstrap set of faces is used to tune an asymmetric bilinearmodel which is incorporated in a probabilistic framework forthe recognition of the six prototypic expressions proposed by\\nEkman and Friesen [5]. Face recognition, on the other hand, is\\nperformed by “modulating” the probe surface to resemble theexpression depicted in the associated gallery image. This allowsexpression-invariant face recognition. Modulation is performed\\nusing the expression and identity bilinear model. Finally, we\\npresent results evaluating our algorithms using the BU–3DFE\\n1\\nface database [6], and demonstrates comparisons with our pre-vious work [2] on 3-D face recognition and Wang et al. ’s work\\n[1] on 3-D facial expression recognition.\\nA. Related Work\\nOver the past three decades, face and facial expression recog-\\nnition have received growing interest within the computer vi-\\nsion community. Most works in the literature assume neutral\\nexpression for 3-D face recognition (e.g., [7]–[9]), and use 2-Dimagery for facial expression recognition. Extensive surveys on3-D face recognition and facial expression recognition from 2-D\\nimages and video may be found in [3] and [10], respectively.\\nHere, we focus on research performed recently toward expres-sion-invariant 3-D face recognition and facial expression recog-nition from 3-D surfaces.\\nTo cope with facial expressions, the majority of techniques in\\nthe literature detect regions of the face that are affected by facialexpressions and then try to minimize their contribution to sim-ilarity computation. Other techniques use expression-invariant\\nfeatures or representations.\\nIn [11] and [12], the authors use an annotated face model that\\nis deformed elastically to ﬁt each face thus allowing the anno-tation of its different anatomical areas such as the nose, eyes,\\nmouth, etc. To account for expressions, the authors then clas-\\nsify faces using wavelet coefﬁcients representing face areas thatremain unaffected by facial expressions, such as eyes and nose.However, the best recognition rate is achieved when the whole\\n1The database is available at the http://www.cs.binghamton.edu/~lijun/Re-\\nsearch/3DFE/3DFE_Analysis.html\\n1556-6013/$25.00 © 2008 IEEE\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 499\\nface is used, which implies that rejection of deformable parts\\nof the face leads to the loss of valuable discriminative infor-mation. Similarly, Chang et al. [13] follow a multiregion tech-\\nnique in which multiple overlapping regions around the nose\\nare matched using the iterative closest point algorithm (ICP).This approach suffers too from the disadvantage that deformableparts of the face that still encompass discriminative information\\nare rejected during matching.\\nOn the other hand, Li and Zhang [14] classify faces using ex-\\npression-invariant descriptors that are based on surface curva-ture, geodesic distance, and attributes of a 3-D mesh ﬁtted to the\\nface. The descriptors are weighted appropriately to form a ﬁnal\\nface signature that is used to identify faces. Bronstein et al. [15]\\nuse multidimensional scaling on pair-wise geodesic distances to\\nembed the surface to a 4-D sphere where classiﬁcation is per-\\nformed on the basis of normalized moments. In the same spirit,\\nwe used a geodesic polar parameterization of the facial surfaceto construct expression-invariant attribute images [2] which canbe classiﬁed following a variety of 2-D classiﬁcation techniques.\\nThe approaches [2], [14], [15] depend on the assumption that\\nfacial skin deforms isometrically, which is not valid in case ofextreme expressions. In addition, the computation of expres-sion-invariant features using curvature and geodesic distance is\\nproblematic because of their sensitivity to noise, which is also\\nmagniﬁed by the approximate character of the isometric mod-elling of facial deformations.\\nAlthough great effort has been directed towards 3-D face\\nrecognition, not many works have examined 3-D facial ex-\\npression recognition. Instead, most studies on automatic facialexpression recognition are based on still images and imagesequences [16]–[18]. The only work using purely 3-D in-\\nformation we are aware of is that of Wang et al. [1]. In that\\nwork, expression classiﬁcation is based on the distribution ofgeometric descriptors deﬁned on different regions of the facialsurface which are delimited according to the neuroanatomic\\nknowledge of the conﬁguration of facial muscles and their\\ndynamics. In each region, surface points are labeled accordingto their principal curvatures and then histograms of the labelsare used to classify facial expressions. However, this technique\\nis also limited by the need of computation of curvature features\\nwhich may be problematic as we have already described.\\nAll of the aforementioned techniques treat 3-D face recogni-\\ntion and expression recognition as two separate problems. To the\\nbest of our knowledge, there are no reported studies about joint\\nexpression-invariant facial identity recognition and identity-in-variant facial expression recognition based on 3-D information.A few researchers have addressed the problem using 2-D images\\nby trying to encode identity and expression variability of facial\\nappearance in independent control parameters that are then usedfor recognition. Vasilescu and Terzopoulos [19] use the N-modeSVD tensor decomposition on face images to separate the in-\\nﬂuence of identity, pose, illumination, and expression, while\\nWang and Ahuja [20] use higher-order singular value decompo-sition (SVD) to recognize and synthesize facial expressions. Bi-linear models proposed by Tenenbaum and Freeman [21], [22]\\noffer an efﬁcient way for modeling bifactor interactions, since\\nthey combine simplicity in training and implementation with thecapability of capturing subtle interactions between factors. Assuch, bilinear models are used in this work to model the 3-D fa-\\ncial surface as the interaction of expression and identity compo-nent. After separating the parameters which control expression\\nand those which control identity, joint expression-invariant face\\nrecognition and identity-invariant expression recognition are ef-ﬁciently achieved.\\nApart from presenting a novel uniﬁed framework for 3-D face\\nand facial expression recognition, this work introduces several\\ncontributions.\\n• We propose using bilinear models for handling joint iden-\\ntity and expression contribution to facial appearance and\\nwe provide a generic solution for the minimization in-\\nvolved in bilinear model training. The proposed techniqueis applicable even with incomplete data sets in contrastto the conventional SVD approach which is intended for\\nevenly distributed training data.\\n• We present a novel technique for establishing point corre-\\nspondence among faces which is based on an elasticallydeformable 3-D model and is achieved by solving a simple\\nlinear system of equations. Unlike other relevant tech-\\nniques, we use surface-to-model and model-to-surfacedistances during model deformation which leads to moreplausible point correspondence.\\n• Correspondence is established automatically by building\\na low-dimensional face eigenspace, but at the expenseof possible poor correspondence in the mouth region incase of expressions with a widely open mouth. To handle\\nthis problem caused by the rough approximation of the\\nface manifold, we detect landmarks that deﬁne the mouthboundary instead of using complex nonlinear models asusual.\\nThe rest of this paper is organized as follows: In Section II,\\nwe present how elastically deformable models can be employedto resolve the problem of point correspondence and how theuse of landmarks can improve model ﬁtting. Bilinear models’\\ntraining and ﬁtting are described in Section III while their ap-\\nplications on facial expression recognition and face recognitionare demonstrated, respectively, in Sections IV and V. The pro-posed algorithm is evaluated in Section VI where experimental\\nresults are reported and conclusions are drawn.\\nII. E\\nSTABLISHING POINT CORRESPONDENCE\\nA. Elastically Deformable Model\\nOur goal is to establish a point-to-point correspondence\\namong 3-D facial surfaces guaranteeing alignment amonganatomical facial features. Since faces are represented asclouds of 3-D points acquired by a 3-D sensor, it is more conve-\\nnient that this correspondence be established instead between\\nsurface models ﬁtted to the point clouds. We achieve this by de-forming a prototypic facial surface model (neutral expression,average identity) so that it resembles the expression/identity\\ndepicted by the point clouds. Once this deformation has been\\nestimated, ﬁnding correspondences is straightforward.\\nIn this work, the face is modelled as a subdivision surface,\\nsimilar to [23]. A triangular 3-D mesh\\nwith\\n vertices\\nis used, where\\n are the coordi-\\nnates of each vertex. At each subdivision step, each edge of the\\n500 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nFig. 1. Flowchart of the proposed algorithm for joint 3-D face and facial expression recognition.\\nmesh is split with the introduction of new vertices using linear\\nsubdivision rules.2Thus, each triangle is subdivided into four\\nsubtriangles and the mesh becomes more dense. Each vertex of\\nthe resulting 3-D mesh may be written as a linear combinationof the vertices of the previous level mesh and eventually of theinitial mesh\\n. That is\\n(1)\\nwhere\\n is the set of vertex coordinates at level\\n of subdivi-\\nsion and\\n is a matrix which may be easily computed given\\nthe subdivision rule. After an inﬁnite number of subdivisions,\\nthe mesh converges to a continuous smooth surface which is a\\nfunction of the initial mesh and the subdivision rule. In practice,however, we do not have to make inﬁnite subdivisions, sinceafter a few levels (e.g., three in our experiments), the mesh be-\\ncomes dense enough to approximate the subdivision surface. For\\nnotation simplicity, let the mesh\\nat the last level of subdivi-\\nsion that best approximates the subdivision surface be called the\\nsubdivision mesh and let\\n denote its vertices.\\nIf\\n also denotes the corresponding matrix with entries\\n , then\\naltogether we have\\n(2a)\\n(2b)\\nAs is evident from the above, the geometry of the subdivision\\nsurface may be uniquely determined by deﬁning the base mesh\\nvertices and topology (e.g., vertex connectivity).\\nNow let us return to the problem of ﬁtting the subdivision\\nsurface to a 3-D cloud of points. To make the ﬁt anatomicallyvalid, a set of landmarks corresponding to anatomically salient\\npoints of the face has to be deﬁned on the surface and the cloud\\n2We have used the Loop subdivision scheme [23].of points. Let\\n and\\n denote the points of the cloud\\nand\\n ,\\n the associated landmarks (e.g., point\\ncorresponds to the left eye leftmost point,\\n corresponds to\\nthe left eye rightmost point, and so on). Similarly, we select asubset\\nof vertices on the base mesh\\n that anatomically\\ncorrespond to the\\n landmarks. According to the subdivision\\nrule, the coordinates of vertices in\\n remain untouched on\\nthe subdivision mesh but are indexed differently. Thus, we caneasily deﬁne a table\\nthat maps each landmark index\\n to the\\ncorresponding vertex index of\\n .\\nFitting the subdivision surface to the cloud of 3-D points is\\nformulated as an energy minimization problem. We deﬁne adeformation energy which consists of terms giving rise to op-posing forces between the subdivision surface and the cloud of\\npoints. The interaction of these forces deforms the subdivision\\nsurface or equivalently displaces the vertices of the base meshwhich control the surface, until equilibrium is established. Theterms comprising the deformation energy are deﬁned so that cer-\\ntain intuitively reasonable criteria are met.\\nThe deformation should obey the a priori known correspon-\\ndences between landmarks and associated mesh vertices. There-fore, the ﬁrst term of the deformation energy minimizes the dis-\\ntance of each landmark from its corresponding vertex in\\n.\\nThat is\\n(3)\\nThe contribution of this term is important in the ﬁrst stages\\nof optimization because it drives the minimization close tothe global minimum, preventing it from being trapped in localminima of the objective function.\\nThe ﬁnal form of the subdivision surface should also be as\\nclose as possible to the original surface represented by the cloudof points. Therefore, the distance between the surface and thecloud should be minimized. We formulate two terms for this\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 501\\nminimization—one for the distance directed from the subdivi-\\nsion surface to the point cloud, that is between each vertex of thesubdivision-mesh and the nearest point of the cloud, and one for\\nthe distance with reverse direction. The use of two terms leads to\\ntwo forceﬁelds between the surface and the point cloud, whichare approximately similar in ﬂat regions of the face but quitedifferent in regions of high curvature (see Fig. 3). The resulting\\nforceﬁeld is smoother than both of them separately and estab-\\nlishes anatomically more plausible correspondence.\\nIf we deﬁne function\\n, which returns the index\\n of the\\npoint\\n closest to the\\n vertex\\n and function\\n that returns\\ninversely the index of the\\n vertex nearest to point\\n , we can\\nwrite the aforementioned described energy terms as\\n(4)\\n(5)\\nIf the subdivision-mesh deformation is based solely on the\\ncorrespondence between landmarks and the proximity to the\\ncloud of points, the result will be a rough mesh with folded\\ntriangles. This is because the forces that act on the mesh mayattract its vertices to disparate positions and, thus, fold the tri-angles. This problem can be overcome by posing a smoothness\\nconstraint to make the underconstrained optimization tractable.\\nThe smoothness is deﬁned as a measure of the elastic energy ofthe base mesh which penalizes nonparallel displacements of theedges and is given by\\n(6)\\nwhere\\n is the set of\\n ’s neighbors,\\n is its cardinality, and\\nand\\n are the initial positions of the vertices.\\nThe deformation energy, whose minimization is sought, is\\ndeﬁned as the weighted sum of the aforementioned energy terms\\n(7)\\nThe coordinates of mesh\\n vertices\\n that minimize\\ncan be found by differentiating (7) with respect to\\n and set-\\nting the partial derivatives equal to zero. Differentiation leads toa linear system which can be solved easily. However, we may\\ndispense with differentiation by writing\\nin matrix notation\\n(see the Appendix) and show that\\n is minimized by the solu-\\ntion of the overdetermined linear system (8) solved using SVD.\\n(8)\\nMatrices\\n and\\n show whether a base-mesh vertex cor-\\nresponds to a landmark and which is the neighborhood of each\\nvertex, respectively.\\n is a vector formed by landmarks coordi-\\nnates while\\n is the vector of base-mesh vertex initial positions.\\n,\\n ,\\n , and\\n are used to deﬁne which vertices (\\n ,\\n )\\nFig. 2. Fitting base mesh to a surface. (a) Base mesh. (b) Original surface. (c)\\nBase mesh ﬁtted to the surface.\\ncorrespond to which points of the cloud (\\n ,\\n). (More details\\nare given in the Appendix.)\\nAs we have already described, vertex displacement is gov-\\nerned by two opposing forceﬁelds: 1) forces stemming from\\n,\\n , and\\n , which attracts the subdivision mesh toward\\nthe cloud of points and 2) forces stemming from\\n , which try\\nto keep vertices to their initial positions. However, the forces\\ndue to\\n and\\n attract vertices toward the nearest points\\nof the cloud instead of the anatomically corresponding points.This is not a problem if the mesh is relatively close to the cloud,since nearest points and anatomically corresponding points al-\\nmost coincide. But if the mesh is relatively far from the cloud,\\nvertices may be displaced so that anatomically erroneous corre-spondence is established. To overcome this problem, we iteratethe minimization process several times, letting vertices move\\nprogressively until they converge to a ﬁnal position. Thereby, at\\nthe\\nth iteration, vertices are updated according to\\n(9)\\nwhere\\n is the step size usually chosen in [0.2, 0.8], and\\nis the solution of the system (8) at the\\n th iteration. We note\\nthat\\n ,\\n , and\\n in (8) vary at each iteration.\\nFinally, since the most time-consuming part of the minimization\\nis searching for nearest points according to (4) and (5), a space\\npartitioning technique, kD-trees [24], is applied to acceleratethe optimization. An illustration of mesh ﬁtting can be seen inFig. 2.\\nThe formulation of the point-to-point correspondence\\nproblem as an energy minimization problem described by (7)and (8) has several advantages over other approaches reportedin the literature. In [25], the authors solve the problem of 3-D\\ncorrespondence using the optical ﬂow between the associated\\ncolor images. Apart from the need of texture, the color imagesshould also be parameterized in the same cylindrical coordinateframe which is another problem of its own. Therefore, this\\ntechnique cannot be applied when the 3-D data are textureless\\nand in the form of a point cloud as in our case. A possiblealternative for our method is the use of elastically adaptivedeformable models, proposed by Metaxas [26] and used by\\nKakadiaris et al. [12] and Passalis et al. [11] for face recog-\\nnition. However, this method involves the integration of asystem of second-order differential equations, which is moredemanding than the solution of a linear system as we propose.\\n502 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nFig. 3. Correspondence is based on proximity. The deformable model is de-\\npicted with the continuous curve while the cloud of points has the dashed one.\\nArrows show the point which is closest to their start. Dotted arrows depict the\\ncloud-to-model correspondences, while continuous arrows depict the model-to-\\ncloud correspondence. The usefulness of using both sets of directed distances\\nis apparent in the high curvature region, where there is a discontinuity in themodel-to-cloud set of correspondences (continuous arrows).\\nAllen et al. [27] present the method that is most similar to\\nours, even though it is intended for the establishment of point\\ncorrespondence between whole human body scans. They alsouse landmarks and minimize the distance between the modeland the surface in hand using a smoothness term for regulariza-\\ntion. The main difference, however, is that they use only dis-\\ntances directed from the model to the cloud of points, while wealso use the distances with the inverse direction. This is impor-tant in the early stages of the optimization when the model is still\\nfar from the cloud (see Fig. 3). In this case, the model-to-cloud\\ndistances (continuous arrows in Fig. 3) may have discontinuitiesin regions of high curvature and, thus, give rise to anatomicallyinaccurate correspondences. Also using the other set of directed\\ndistances (dotted arrows in Fig. 3), the resulting vector ﬁeld be-\\ncomes smoother and helps the optimization from getting trappedin local minima.\\nB. Subspace-Guided Elastically Deformable Model\\nIn most cases of practical interest, the number of facial land-\\nmarks which can be detected automatically and relatively ac-\\ncurately is limited. Furthermore, the facial surface captured by\\ndepth acquisition devices usually includes extraneous parts ofthe body, such as the neck and the upper head. In such a case,mesh ﬁtting cannot be based on forces stemming from land-\\nmarks’ correspondence and surface-to-mesh distance. Recov-\\nering the surface deformation in this case is an ill-posed problemwhich, however, becomes more tractable by assuming that thedeformations lie on a subspace of low dimensionality. This sub-\\nspace may be estimated from training data by means of prin-\\ncipal component analysis (PCA). Once the faces of the trainingset have been set in correspondence following the procedure de-scribed in the previous section, base-mesh vertices of any novel\\nface may be written as a linear combination of eigenmeshes\\n(10)\\nFig. 4. Mouth boundary detection. (a) The original face surface. (b) A vertical\\nproﬁle used to deﬁne a mouth bounding box. The tip of the nose, the upper and\\nlower points of the mouth are denoted by /84, /85, and /76, respectively. (c) The\\nsign of mean curvature /72of the surface. The sign of /72along with a measure of\\ncornerness inside the mouth bounding box is used to detect the mouth boundary\\nand deﬁne the associated landmarks. (d) The base mesh ﬁtted to the original sur-\\nface using mouth-associated landmarks. (e) The base mesh ﬁtted to the originalsurface without mouth-associated landmarks.\\nwhere\\n is the truncated matrix which describes the principal\\nmodes of deformation,\\n is the control vector, and\\n is the mean\\nvector of aligned vertex coordinates (i.e., mean face).\\nNow, by replacing (10) in (8) (the terms\\n and\\n are ex-\\ncluded), we may rewrite the deformation energy as a function\\nof\\n. Similar to the analysis in the previous section, the con-\\ntrol vector\\n which minimizes\\n is the solution of the linear\\nsystem\\n(11)\\nOur experiments showed that this approach converges to the\\nglobal minimum of the deformation energy most of the time. Itwas also observed that failure usually occurs if the person dis-\\nplays an expression with a widely open mouth such as when\\ndisplaying surprise. Mesh ﬁtting in this case may result in a de-formed closed mouth (Fig. 4), which implies that the low-di-mensional approximation of the face manifold is so rough in\\nthe region of mouth that optimization is trapped in a local min-\\nimum. To overcome this problem, one can use a more complexmodel to capture the structure of the face manifold. For instance,in [28], the authors use active appearance models [29] combined\\nwith a multilayer perceptron to establish correspondence among\\n2-D face images. This, however, makes the problem nonlinear.Thus, in this paper, we have experimented with a simpler tech-\\nnique that relies on the detection of the mouth boundary.\\nMouth boundary is detected using depth and curvature in-\\nformation following an approach similar to that of [30]. A ver-tical proﬁle curve, as illustrated in Fig. 4, is used to detect the\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 503\\nupper and lower points of the mouth contour exploiting the fact\\nthat these points are local extrema of the proﬁle curve. Then, abounding box of the mouth is deﬁned where the mean curvature\\nof the surface\\n is calculated. Mean curvature is\\nused because it obtains opposite signs on the lip ridges and themouth hollow assuming that the\\naxis is parallel to the gaze\\ndirection. A measure of cornerness\\n is also computed to\\nidentify the horizontal outermost points of the mouth contour,\\nthat is, the corners of the lips. Cornerness is deﬁned by the equa-tion\\n(12)\\nwhich is derived by the Harris corner detector [30]. Mean cur-\\nvature and cornerness are then fused with a MINI -MAX rule to a\\nsingle index that indicates the extent to which points of the sur-\\nface belong to the boundary. The boundary is ﬁnally formed byﬁtting a spline curve through points with indices above a certainthreshold set empirically. Once the boundary has been detected,\\nthe associated landmarks are included in the formulation of the\\ndeformation energy, and the linear system in (11) is modiﬁed to\\n(13)\\nBoundary detection relies on the quality of 3-D data since it\\ninvolves estimation of second-order surface derivatives whichare sensitive to noise. As expected, higher depth quality re-sults in better detection. However, our experiments showed that\\neven an approximate detection of the mouth boundary is enough\\nfor the minimization of the deformation energy to avoid localminima. Therefore, the proposed system is insensitive to smallboundary mislocalizations.\\nIn summary, the overall procedure for establishing correspon-\\ndence among faces is as follows: During the training phase, cor-respondence among a set of annotated training 3-D images isestablished. These faces are rigidly registered with each other\\nusing the ICP algorithm and then each facial point cloud is mod-\\neled by a subdivision surface. Modeling consists in ﬁnding thebase-mesh vertices that minimize the deformation energy de-ﬁned by (7) with minimization achieved by iterating (8) upon\\nconvergence. Once correspondence is established, PCA is ap-\\nplied to learn the principal modes of base-mesh deformation.\\nIn the test phase, for a novel unseen face, its mouth boundary\\nhas to be detected ﬁrst. Then (13) is solved to obtain the con-\\ntrol parameters\\nfrom which the base-mesh vertices\\n may be\\nrecovered (10). Having ﬁtted a subdivision surface to the newface, correspondence with all faces in the gallery is established,since all ﬁtted surfaces originate from the deformation of an av-\\nerage face surface.\\nIII. M\\nODELING FACIAL EXPRESSION AND IDENTITY VARIATION\\nOnce point correspondence is established, each facial surface\\ncan be represented by the vector\\n of the base-mesh verticessince it deﬁnes unambiguously the subdivision surface that\\napproximates the cloud of facial points. Then, one may classifyfaces or facial expressions using typical pattern classiﬁcation\\ntechniques that rely on distance metrics of vector spaces.\\nHowever, this approach will result in degraded recognitionperformance, because such a representation cannot distinguishwhether a certain shape of the face is attributed to the identity\\nof the person or the expression he or she displays. For example,\\nwe cannot distinguish whether a puffy cheek comes from anoverweight person or if it is due to a smiling expression. Itbecomes clear, therefore, that we have to encode identity and\\nexpression in independent control parameters in order to be able\\nto perform joint expression-invariant facial identity recognitionand identity-invariant facial expression recognition.\\nIn this work, we use bilinear models to capture the iden-\\ntity-expression structure of face appearance. Bilinear models are\\nlinear in either factor when the other is held constant and, assuch, they share almost all of the advantages of linear models:they are simple in structure, computation, and implementation,\\nthey can be trained with well-known algorithms, and their com-\\nplexity can be easily adjusted by their dimensionality as a com-promise between exact reproduction of training data and gener-alization during testing [22]. Simultaneously and despite their\\nsimplicity, they can model subtle interactions by allowing fac-\\ntors to modulate each other’s contribution multiplicatively.\\nWe use two types of bilinear models: 1) the symmetric and 2)\\nthe asymmetric bilinear model, suitable for identity and expres-\\nsion recognition, respectively. In case of face recognition, we\\nneutralize the effect of expressions by deforming the matchedfaces so that they display a common expression (e.g., a neutralexpression or the expression displayed by the gallery face). This\\nis accomplished by modifying their expression control parame-\\nters which are extracted by ﬁtting faces to a symmetric bilinearmodel. The asymmetric model on the contrary is used to per-form identity-invariant expression recognition. After training\\nthe model with several subjects depicting different facial expres-\\nsions, it is incorporated in a maximum-likelihood classiﬁcationframework which allows facial expression discrimination acrossidentity.\\nIn the following sections, we describe, in detail, the sym-\\nmetric and asymmetric bilinear model and we present a novelgeneral solution of the minimization involved in their training.\\nA. Symmetric Model\\nLet\\nbe the stacked column vector of the\\n base-mesh\\nvertices of the facial surface of person\\n with expression\\n . The\\ndimension of\\n , which is\\n , is denoted by\\n for simplicity.\\nThen, each component\\n is given by the general bilinear form\\n[21], [22]\\n(14)\\nHere,\\n and\\n are the control parameters which control expres-\\nsion and identity, respectively, while\\n are the coefﬁcients\\n504 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nwhich model the interaction of the factors and we try to esti-\\nmate from the training set. The aforementioned equation can bewritten equivalently as\\n(15)\\nwhere\\n are stacked vectors of\\ndimension\\n . Now, it is clear that\\n is the bilinear\\ncombination of basis vectors\\n which are mixed by control\\ncoefﬁcients\\n and\\n . The aforementioned equation also shows\\nthat control parameters symmetrically weigh the basis vectors\\nand, therefore, this model is referred to as the symmetric bilinearmodel.\\nLet us assume that\\nfaces exist in our database belonging\\nto\\n individuals and each one depicting one of\\n possible ex-\\npressions. Our goal is to ﬁnd the interaction coefﬁcients\\nand the control parameters\\n and\\n for each individual-ex-\\npression couple. Using the matrix notation, (14) is simpliﬁed to\\n(16)\\nwhere\\n ,\\n , and\\n. Since, in practice, a facial expression from the\\n pos-\\nsible ones may not be available for some people, we also deﬁne\\nthe zero-one function\\n which is one if the\\n th face\\nbelongs to the individual\\n with expression\\n . Unknown coefﬁ-\\ncients arise from the minimization of the total squared error [21]\\n(17)\\nBy differentiating and setting the partial derivatives equal to\\nzero, we end up with the system of equations\\n(18)\\n(19)\\n(20)\\nwhere\\n is the number of training faces which\\nbelong to subject\\n displaying expression\\n and\\n is their sum\\n. Equation(20) is a general Sylvester equation [31] which can be rewritten\\nas\\n(21)\\nwhere\\n is the Kronecker product operator and\\n is the\\nmatrix vectorization operator which stacks the columns of thematrix.\\nInteraction matrices\\nand control vectors\\n and\\n may\\nnow be found by iterating (21), (18), and (19), respectively. Toensure the stability of the solution, updating is performed pro-gressively according to the rule\\n(22)\\nwhere\\n is the step size which is usually chosen in [0.2, 0.8],\\nstands for the ﬁnal value of\\n ,\\nor\\n in the\\n th itera-\\ntion, and\\n stands for the value resulting from (18), (19), or\\n(21), respectively.\\nIt should be noted that convergence depends on the dimen-\\nsionalities\\n of\\n , and\\n of\\n , which control the exactness of\\nthe training data reproduction. Convergence is guaranteed if\\nand\\n are less than or equal to\\n and\\n , the number of expres-\\nsions and the number of individuals, respectively. If\\n is equal\\nto\\n , and\\n is equal to\\n , training data are reproduced exactly,\\nwhile more coarse but also more compact representations result\\nif these dimensionalities are decreased.\\nThe minimization of the total squared error through (18),\\n(19), and (21) presented here differs from the optimization\\nscheme proposed by Tenenbaum and Freeman in [22] and their\\nprevious work. There, minimization is achieved by means ofSVD applied to the\\nmean observation block matrix\\n......\\n (23)\\nwhere\\n is the mean vertex vector of the facial surfaces of\\nsubject\\n with expression\\n . This technique relies on evenly dis-\\ntributed data across expression and identity. In practice, how-\\never, data may not be evenly distributed across expression andidentity or even worse, there may not be available data for aparticular expression-identity combination. Then,\\nwill have\\nsome indeterminate entries and SVD will not be applicable.\\nOne remedy is ﬁlling the missing entries with the mean valuesof the appropriate expression and identity, but this substitutiondoes not guarantee the global minimization of the total squared\\nerror and, thus, the best ﬁt of the bilinear model to the training\\ndata.\\n3In contrast, the proposed method may be applied directly\\n3Generalizations of bilinear models using tensors, such as [19] and [32], face\\nthe same problem since the estimation of model parameters is accompished by\\napplying SVD to matrices resulting from tensor mode- /110ﬂattening.\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 505\\nFig. 5. Deformation of facial surface across expression and identity control parameters. Inside boxes are surfaces which are stored in the BU-3DFE da tabase.\\nSpeciﬁcally, subjects F0004, M0044 and F0002 display the sad and happy expressions. The rest surfaces have been generated by linear interpolation of expression\\nand identity control parameters of the former surfaces.\\nwithout any assumptions for the data distribution, but at the ex-\\npense of computational complexity.\\nAn illustration of facial surface modeling using the sym-\\nmetric bilinear model is shown in Fig. 5, where the surfaceshave been generated by linearly interpolating the coefﬁcientscorresponding to two expressions and three identities.\\nB. Asymmetric Model\\nAs already mentioned in the previous section, the symmetric\\nmodel symmetrically weighs the coefﬁcients which controlidentity and expression. This symmetry implies that the model\\ncan generalize on both directions, identity and expression,\\nwhich is a desirable property for both face and expressionrecognition. However, there is a difference between theserecognition tasks. In case of face recognition, it is usually\\nassumed that unseen faces are free to display an unrestricted\\nnumber of possible expressions. In case of expression recog-nition instead, the number of possible expressions is usuallyconsidered ﬁnite (most studies classify expressions to the six\\nprototypic universal expressions proposed by Ekman [5]). In\\nthe latter case, the symmetric model can be modiﬁed so thata better ﬁt to expressions can be achieved thus improvingrecognition performance. This is accomplished by letting the\\ninteraction coefﬁcients\\nvary with the expression control\\nparameters\\n [22], that is\\n(24)Using the aforementioned deﬁnition and (14), the vector repre-\\nsentation of the face is now given by\\n(25a)\\n(25b)\\nwhere now matrix\\n controls expression. The identity of the\\nface is still controlled by vector\\n .\\nFitting the asymmetric model to the training data consists in\\nﬁnding the expression matrices\\n and identity vectors\\n that\\nminimize the total squared error [21]\\n(26)\\nand\\n are obtained by differentiating the error function and\\nsetting the partial derivatives to be equal to zero. Following thisprocedure,\\nand\\n should satisfy the system of [21]\\n(27)\\n(28)\\nwhich is ﬁnally solved by iterating the equations according to\\nrule (22) until the values of\\n and\\n converge.\\n506 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nThe exactness of training data reproduction is determined by\\nthe number of columns of matrix\\n or, equivalently, the dimen-\\nsionality of vector\\n . More exact reconstruction is achieved if\\nmore columns are used. Nevertheless, the number of columns\\nshould be restricted in order to avoid overﬁtting [22] and it mustbe less than the number of subjects in the training set so that thesolution of (27) and (28) is feasible.\\nIV . F\\nACIAL EXPRESSION RECOGNITION\\nIn this section, we show how a facial expression classiﬁer may\\nbe built on the basis of an asymmetric bilinear model ﬁtted to a\\ntraining set of faces.\\nA. Training\\nThe input is a set of training faces annotated with salient fa-\\ncial points. This set should contain images of several people de-picting different facial expressions. First, anatomical correspon-\\ndence among raw data is established by means of the elastically\\ndeformable base mesh\\nas explained in Section II-A. Then,\\nthe resulting model parameters are used to build the 3-D faceeigenspace so that newly seen faces may be processed without\\nthe need of facial landmarks.\\nAn asymmetric bilinear model is then ﬁtted to the deformable\\nmodel parameters\\nof the training faces. That is, we estimate\\nthe expression control matrices\\n and identity control vectors\\nthat minimize the total squared reconstruction error given\\nby (26). This is done by iterating equations (27) and (28) untilthe relative change in the Frobenius norm of both\\nand\\nbecomes less than a predeﬁned constant threshold.\\nEstimated\\n and\\n are then used to build a maximum-\\nlikelihood classiﬁer. The goal is to estimate the likelihood ofthe surface model parameters\\nfor each expression, that is, the\\nconditional probability density function (pdf)\\n , where\\nis one of the prototypic expressions. To this end, we assumethat the vertex vector\\ndeﬁning the facial surface of person\\nwith expression\\n is a random vector with spherical gaussian\\npdf centered at the prediction of the asymmetric bilinear model.\\nThat is\\n(29)\\nwhere\\n is the error variance. Using the total probability the-\\norem, the conditional pdf of\\n assuming expression\\n may now\\nbe written as\\n(30)\\nwhere\\n stands for the a priori probability of person\\n which\\nmay be considered constant for all subjects and equal to\\n .\\nB. Classiﬁcation\\nThe facial expression of a novel test face is classiﬁed simply\\nby comparing the conditional pdfs of\\n (30) for all expressions.\\nFirst, the test surface is set into anatomical correspondence withall meshes in the training set. If landmarks are available for\\nthe test face, then the landmark-guided deformation of the basemesh explained in Section II-A is followed; otherwise, the sub-\\nspace-guided deformation described in Section II-B is applied.Once the surface model parameters\\nare found, the expression\\nof the face is classiﬁed to the prototypic expression with the\\ngreatest likelihood, that is, the expression\\n for which\\n(31)\\nV. F ACE RECOGNITION\\nSimilar to facial expression recognition, face recognition also\\ninvolves a bootstrapping phase before classiﬁcation. However,\\nclassiﬁcation follows a different approach which consists in al-tering the expression of the probe face. This is accomplished by\\nmodulating the parameters of a symmetric bilinear model ﬁtted\\nto the probe face in order to force it to display the expressionof every gallery face before matching. In the following text, wepresent this procedure in detail.\\nA. Bootstrapping\\nSince subjects to be classiﬁed are free to display various ex-\\npressions, a bootstrap set of faces is used to train a symmetricbilinear model so that we may be able to generalize on any novelunseen identity or expression. The bootstrap set is also used for\\nlearning the mesh deformation eigenspace so that newly seen\\nfaces may be processed without the need for facial landmarks.\\nBootstrap faces are ﬁrst set into anatomical correspondence\\nas described in Section II-A and the mesh eigenspace is then\\nlearned by means of PCA. The training of the symmetric model\\nstarts by training an asymmetric bilinear model such as the oneused for facial expression recognition. This is done to obtain ini-tial values for the identity control vectors\\n. After training the\\nasymmetric model, vectors\\n are used to train an initial sym-\\nmetric model following the SVD approach presented in [22].Analytically, this is done by applying SVD to the mean observa-tion block matrix deﬁned in (23). By deﬁning the vector trans-\\npose operator\\n, which transposes a matrix block-wise as\\nshown in Fig. 6, the mean observation matrix may be written as\\n(32)\\n(33)\\nwhere matrices\\n ,\\n, and\\n are comprised with\\n ,\\n, and\\n(15), respectively\\n......\\n (34)\\nMatrix\\n is initialized by the identity control vectors es-\\ntimated by the asymmetric model. Then, the SVD of\\nis computed and matrix\\n is up-\\ndated by the ﬁrst\\n rows of\\n . Similarly, the SVD of\\nis computed and matrix\\n is updated\\nby the ﬁrst\\n rows of\\n . This constitutes one iteration of the\\nerror minimization algorithm whose convergence is guaranteed.\\nUpon convergence,\\n results in\\n .\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 507\\nFig. 6. Vector transpose operator /40/91 /1 /93\\n /41acting on a matrix which is consid-\\nered to consist of 2 /21 blocks. The operator transposes the matrix blockwise.\\n(For more details, the reader is referred to [22].) By rearranging\\n, we may compute interaction matrices\\n .\\nThen, ﬁnal optimal parameters are found by iterating equations\\n(18), (19), and (21) until convergence is achieved.\\nB. Gallery Image Processing\\nUsing the optimal mixing matrices\\n found from the boot-\\nstrap set, we extract the expression and identity control vectors\\nfor each gallery face. This is accomplished by minimizing thereconstruction squared error\\n(35)\\nMinimization is achieved similarly to the minimization of the\\ntotal squared error in (17) during the training of the symmetricbilinear model. By differentiating (35) and setting partial deriva-tives equal to zero, control vectors are found by iterating the\\nsystem of equations\\n(36)\\n(37)\\nEquations are iterated until the change in the norm of\\n and\\nbecomes less than a threshold. The estimated expression control\\nvectors of the gallery faces are subsequently used during classi-ﬁcation to modify the expression displayed by the probe face.\\nC. Classiﬁcation\\nThe classiﬁcation of a novel probe face starts by setting it into\\ncorrespondence with the gallery faces according to Section II.\\nIf landmarks are available, the landmark-guided approach\\n(Section II-A) is followed; otherwise, the subspace-guidedtechnique (Section II-B) is applied using the mesh eigenspacelearned from the bootstrap set. We note that we have made no\\nassumptions about the expression of the gallery faces meaning\\nthat they are allowed to display various expressions that arepossibly different from that of the probe faces. Then, to handlethe inﬂuence of expression on surface matching, we force\\nthe probe face to display the expression of the gallery face\\nbefore matching (Fig. 7). Its expression control vector has to\\nFig. 7. Expression manipulation. The ﬁrst column shows the original 3-D\\nface scans of the same subject displaying a smiling (ﬁrst row) and a surprising\\n(bottom row) expression. The second column shows the elastically deformable\\nmodel ﬁtted to the original surfaces, while the third column shows recon-\\nstructions of the surfaces using bilinear model coefﬁcients. Neutralization\\nof expressions shown in the fourth column is achieved by modulating the\\nexpression control parameters.\\nbe extracted by ﬁtting the symmetric bilinear model as in the\\ncase of the gallery faces. Then, by substituting the expression\\ncontrol vector by the corresponding vector of the gallery face,we may reconstruct a new probe facial surface which depicts anexpression similar to that of the gallery face. If the new probe\\nface is represented by the vertex vector\\nwhile the gallery\\nface is represented by the vertex vector\\n , then expression-free\\ncomparison between them may be established on the inverse oftheir vertex vectors’ squared Euclidean distance\\n(38)\\nVI. P ERFORMANCE EV ALUATION\\nIn this section, we evaluate the performance of bilinear\\nmodels on the Binghamton University 3-D Facial Expression\\n(BU-3DFE) database [6] and we provide comparisons withWang et al. ’s work [1] on facial expression recognition and our\\nprevious work [2] on face recognition. BU-3DFE is preferred\\nover other popular databases, such as FRGC [33], because\\nexpressions are well deﬁned and distributed better.\\nBU-3DFE contains 100 subjects—56 female and 44\\nmale—with a variety of ethnic/racial ancestries, including\\nwhite, black, East Asian, Middle East Asian, hispanic latino,\\nand others. The subjects display the six universal expressionsof anger, fear, disgust, happiness, sadness, and surprise in fourlevels of intensity: low, middle, high, and highest. For each\\nsubject, there is also a 3-D face scan with neutral expression,\\nthus resulting in a total number of 2500 face scans in the data-base. 3-D facial data are in the form of VRML models whichcontain about 13 000–21 000 polygons and are associated with\\na set of feature points located on the eyes, eyebrows, nose,\\nmouth, and the boundary of the face. These ﬁducial points havebeen detected manually and are used as landmarks during theestablishment of point correspondence in training stages. It is\\nemphasized that in the following experiments, landmarks are\\nused only in training stages and not in testing stages.\\n508 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nA. Facial Expression Recognition\\nIn this series of experiments, we demonstrate the performance\\nof the asymmetric model in the recognition of the six prototypicexpressions following the ten-fold cross-validation approach.\\nIn each experiment, BU-3DFE subjects are divided randomly\\nin two sets: 1) a training set consisting of 90 subjects and 2) atest set consisting of the remaining ten subjects, thus ensuringthe independence on a subject identity. First, the base mesh\\nis built by selecting\\n vertices lying on an average facial\\nsurface and then training faces are set into point correspondenceas described in Section II-A. The training set is also used to learnthe mesh deformation subspace (250 principal modes are kept)\\nand then to train the asymmetric bilinear model. That is, we es-\\ntimate the six matrices\\ncorresponding to the six possible ex-\\npressions and the 90 identity control vectors\\n corresponding\\nto the subjects of the training set. The column number of\\nand the dimension of\\n is set to 80 while the number of rows\\nis equal to the triple of vertices number\\n . The entries\\nof\\n and\\n are initialized randomly and then they are com-\\nputed by iterating (27) and (28) until the relative change in their\\nFrobenius norm goes below a threshold (0.01) or a maximum\\nnumber of iterations (150–180) is reached. Estimated matrices\\nand vectors\\n are then used to build the maximum-likeli-\\nhood classiﬁer, letting the error variance be\\n .\\nDuring testing, each test face is set into correspondence fol-\\nlowing the subspace-guided approach described in Section II-B(we assume that landmarks are not available for test faces).Once the vertex vector\\nis computed, the expression of the\\nface is classiﬁed to the class with the highest likelihood (see\\nSection IV).\\nThis procedure (training-testing) is repeated ten times en-\\nsuring that every subject is included in one test set. Results\\nare averaged and presented as a confusion matrix in Table I.Expressions have been labelled by the subjects who have per-formed them and this labelling is used as ground truth in our\\nexperiments. In order to provide a measure of comparison, we\\nreport that the average recognition rate achieved by human ex-perts [6] varies from 94.1% for low intensities up to 98.1% forthe highest intensities. We also provide Table II which shows\\nthe corresponding confusion matrix obtained according to [1].\\nFrom Table I, it can be seen that the highest misclassiﬁcationoccurs between the expressions of anger and sadness. The de-crease in these recognition rates is attributed to their similarity\\nespecially in low intensities. We note that in our experiments, we\\nused for testing facial models of all intensities for every expres-sion, instead of the highest two as in [1]. The main differencebetween the angry and sad expression lies mostly on the conﬁg-\\nuration of the eyebrows, which cannot be effectively captured\\nusing depth (at least with our point correspondence technique)especially in low intensities, where the difference is so subtleeven for a human eye. Nevertheless, the total average recogni-\\ntion rate of 90.5% that was achieved proves the overall superior\\nperformance of the proposed algorithm.\\nB. Face Recognition\\nTo evaluate 3-D face recognition performance, we adopt the\\nfollowing procedure which simulates a realistic application sce-\\nnario. We split the database into two parts based on subjectTABLE I\\nEXPRESSION RECOGNITION BASED ON THE ASYMMETRIC BILINEAR MODEL\\nValues in parentheses are standard deviations.\\nTABLE II\\nEXPRESSION RECOGNITION BASED ON PRIMITIVE SURFACE FEATURE\\nDISTRIBUTION (WANG et al. [1])\\nidentity. The ﬁrst part serves as a bootstrap set and is used for\\ntraining the elastically deformable model (subspace learning de-\\nscribed in Section II-B) and computing the bilinear model coef-\\nﬁcients. The rest of the data, the test set, is split into the galleryset that contains a single 3-D image per subject (neutral or non-neutral), and the probe set that contains the images to be classi-\\nﬁed (various facial expressions).\\nThe bootstrap set is comprised of 50 subjects chosen ran-\\ndomly from the database while the gallery and probe set con-\\nsist of the remaining 50 subjects. Faces in the bootstrap set are\\nﬁrst set into correspondence following the landmark-guided ap-proach of Section II-A. Then, they are used to learn the meshdeformation subspace (250 principal modes are kept again) and\\nto train a symmetric bilinear model. The training of the model\\nstarts by training an asymmetric bilinear model. such as the oneused for facial expression recognition before. This model is usedto initialize the training of another initial symmetric model fol-\\nlowing the SVD approach described in Section V. The dimen-\\nsion of vectors\\nis set to 45 while the dimension of vectors\\nis set to 5. The ﬁnal symmetric model results from the op-\\ntimization of the estimated parameters\\n ,\\n, and\\n , which\\nis achieved by iterating equations (18), (19), and (21) until the\\nrelative change in their Frobenius norm goes below a threshold(0.01) or a maximum number of iterations (150–180) is reached.\\nThen we proceed with gallery image processing. First, the\\ndeformable model has to be ﬁtted to each gallery image. We as-\\nsume that we do not have any landmarks and, therefore, the sub-space-guided approach described in Section II-B is used. Then,the bilinear model is ﬁtted to each gallery image to acquire its\\nexpression and identity control parameters.\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 509\\nFig. 8. Cumulative match characteristic (CMC) and receiver operating characteristic (ROC) for face recognition based on: (a) the symmetric bilinea r model and\\n(b) the geodesic polar representation presented in [2]. Error bars indicate a standard deviation in CMC and a 95% conﬁdence interval in ROC.\\nThis procedure (deformable model and bilinear model ﬁtting)\\nis also repeated for each probe image during testing. Having ob-tained its expression and identity control parameters, the probe\\nimage is then compared with every image in the gallery to ob-\\ntain the similarity score (see Section V).\\nWe have repeated the aforementioned experiments on several\\nrandomly chosen subdivisions of bootstrap and test sets, under\\nthe constraint that all subjects are included at least once in the\\ntest set. The recognition results are averaged and presented inFig. 8, which shows the cumulative match characteristic and thereceiver operating characteristic (ROC) of the proposed system\\ncompared with the results obtained by our previous work in\\n[2]. There, we used an expression-invariant face representation,based on geodesic polar coordinates and an isometric model ofthe facial surface deformation, which proved to be better than\\nBronstein et al. ’s [15] canonical images and a PCA-based al-\\ngorithm. We also note that some images depicting extreme ex-pressions that violated the isometry assumption and had to beexcluded from experiments in [2] are now included in these ex-\\nperiments. The increase in the rank-1 recognition rate shows that\\nthe proposed algorithm may deal well even with extreme ex-pressions which are one of the main limitations of current 3-Dface-recognition algorithms.\\nThe deformable model and bilinear model training requires a\\nfew hours in a typical Pentium V , 3 GHz, 1-GB RAM worksta-tion running nonspeed-optimized code. On the other hand, depthacquisition is performed in less than 2 ms [6], while processing\\nof a novel image takes less than 3 s (2 s for point correspon-\\ndence and 1 s for bilinear model ﬁtting), which means that theproposed algorithm may be used in near-real-time systems.\\nC. Limitations\\nIn the previous sections, we showed that bilinear models may\\neffectively capture the bifactor nature of the facial surface geom-\\netry and, thus, lead to high facial identity and expression recog-\\nnition rates. Nevertheless, there are still some issues that limitrecognition performance, especially face recognition, and pro-vide room for further investigation.The main limitation is the need for a large bootstrap set which\\nshould also be annotated with respect to facial expressions. Themore different expressions that are present in the bootstrap set,\\nthe better the estimation of the interaction matrices\\nis, and\\nthe better the novel face ﬁt is. Training with a few expressionsleads to unbalanced generalization ability in favor of identitywhich, in turn, leads to better surface approximation but with\\npoorer expression control. However, building and annotating in\\npractice a bootstrap set may be difﬁcult considering that a greatnumber of possibly ambiguous expressions have to be classiﬁedinto a ﬁnite number of expression classes.\\nAnother factor that affects performance is the accuracy of\\npoint correspondence between faces. In our experiments, weobserved that poor correspondence substantially affects bilinearmodel training and eventually recognition performance. The\\nproblem is twofold: During training, the bilinear model cannot\\nlearn the true identity-expression manifold, implying errorsin bilinear parameters’ estimation. During testing, expressionmanipulation is actually applied on a slightly (or quite) dif-\\nferent face. This error is further ampliﬁed by inaccurate bilinear\\nparameters, leading to a distorted facial surface.\\nVII. S\\nUMMARY\\nIn this paper, we proposed a technique for joint 3-D face and\\nfacial expression recognition. We ﬁrst presented a novel model-based approach for establishing point correspondence among\\nfaces which involves the solution of a simple linear system. We\\nalso proposed using directed distances both from the mesh to thecloud of facial points and inversely which leads to a smootherforceﬁeld and, thus, more plausible anatomical correspondence.\\nAnother advantage of this approach is that correspondence may\\nbe performed fully automatically after training the system witha number of facial surfaces annotated with anatomical salientpoints. We also provided a solution for the problem of open\\nmouth in this case whose conﬁguration might cause problems\\nduring the establishment of correspondence. Then, we proposedbilinear models for joint face and expression recognition and weprovided the general solution of the error minimization during\\n510 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , VOL. 3, NO. 3, SEPTEMBER 2008\\nthe training of the symmetric bilinear model. Our algorithm was\\nﬁnally evaluated on the BU-3DFE database and proved its su-periority over expression-invariant face representations for face\\nrecognition [2] and primitive surface features for expression\\nrecognition [1].\\nA\\nPPENDIX\\nLet\\n be the\\n -dimensional vector of the base-mesh ver-\\ntices. Also, let\\n be a\\n -dimensional vector\\ncontaining landmark coordinates\\n and\\n ,\\n , 3-D\\nvectors so that\\nif vertex\\n of\\n corresponds\\nto a landmark\\notherwise(39)\\nif vertex\\n is a landmark vertex\\notherwise.(40)\\n4Then, (3) yields\\n(41)\\nwhere\\n using incom-\\nplete Cholesky decomposition.\\nBy deﬁning the\\n and\\n vectors\\n and\\n , respectively,\\nand the\\n and\\n block matrices\\n and\\n made\\nby 3\\n 3 blocks\\n(42)\\n(43)\\n(44)\\n(45)\\nEquations (4) and (5) yield\\n(46)\\n(47)\\nThe elastic energy\\n in (6) can be written in matrix notation\\nusing the\\n block matric\\nas follows:\\n(48)\\n4 /48\\nis the vector /91/48 /48 /48 /93\\n , /49\\nis the vector /91/49 /49 /49/93\\n and /73\\nis the 3 /23 identity\\nmatrix.where\\n is a truncated triangular matrix resulting from the in-\\ncomplete Cholesky decomposition of\\n .\\nBy using the above equations and (7), it can be easily shown\\nthat\\n is minimized by the solution of the overdetermined\\nlinear system (49), which is solved by using SVD\\n(49)\\nREFERENCES\\n[1] J. Wang, L. Yin, X. Wei, and Y. Sun, “3D facial expression recognition\\nbased on primitive surface feature distribution,” in Proc. Conf. Com-\\nputer Vision and Pattern Recognition , 2006, vol. 2, pp. 1399–1406.\\n[2] I. Mpiperis, S. Malassiotis, and M. G. Strintzis, “3D face recognition\\nwith the geodesic polar representation,” IEEE Trans. Inf. Forensics Se-\\ncurity , vol. 2, no. 3, pp. 537–547, Sep. 2007.\\n[3] K. Bowyer, K. Chang, and P. Flynn, “A survey of approaches and chal-\\nlenges in 3D and multi-modal 3D+2D face recognition,” Comp. Vision\\nImage Understanding , vol. 101, pp. 1–15, 2006.\\n[4] K. Chang, K. Bowyer, and P. Flynn, “An evaluation of multi-modal 2D\\n+ 3D face biometrics,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27,\\nno. 4, pp. 619–624, Apr. 2005.\\n[5] P. Ekman and W. Friesen , Facial Action Coding System (FACS):\\nManual . Palo Alto, CA: Consulting Psychologists Press, 1978.\\n[6] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, “A 3D facial ex-\\npression database for facial behavior research,” in Proc. 7th Int. Conf.\\nAutomatic Face and Gesture Recognition , Apr. 2007, pp. 211–216.\\n[7] X. Lu and A. Jain, “Deformation analysis for 3D face matching,” in\\nProc. 7th IEEE Workshop Applications of Computer Vision , Jan. 2005,\\nvol. 1, pp. 99–104.\\n[8] C. Samir, A. Srivastava, and M. Daoudi, “Three-dimensional face\\nrecognition using shapes of facial curves,” IEEE Trans. Pattern Anal.\\nMach. Intell. , vol. 28, no. 11, pp. 1858–1863, Nov. 2006.\\n[9] C.-S. Chua, F. Han, and Y.-K. Ho, “3D human face recognition using\\npoint signature,” in Proc. 4th IEEE Int. Conf. Automatic Face Gesture\\nRecognition , 2000, pp. 233–238.\\n[10] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial ex-\\npressions: The state of the art,” IEEE Trans. Pattern Anal. Mach. In-\\ntell., vol. 22, no. 12, pp. 1424–1445, Dec. 2000.\\n[11] G. Passalis, I. Kakadiaris, T. Theoharis, G. Toderici, and N. Murtuza,\\n“Evaluation of 3D face recognition in the presence of facial expres-\\nsions: An annotated deformable model approach,” presented at the\\nIEEE Workshop Face Recognition Grand Challenge Experiments, Jun.\\n2005.\\n[12] I. Kakadiaris, G. Passalis, G. Toderici, M. Murtuza, Y. Lu, N. Karam-\\npatziakis, and T. Theoharis, “Three-dimensional face recognition in the\\npresence of facial expressions: An annotated deformable model ap-\\nproach,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 29, no. 4, pp.\\n640–649, Apr. 2007.\\n[13] K. Chang, K. Bowyer, and P. Flynn, “Multiple nose region matching\\nfor 3D face recognition under varying facial expression,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 28, no. 10, pp. 1695–1700, Oct. 2006.\\n[14] X. Li and H. Zhang, “Adapting geometric attributes for expression-in-\\nvariant 3D face recognition,” in Proc. IEEE Int. Conf. Shape Modeling\\nApplications , 2007, pp. 21–32.\\n[15] A. M. Bronstein, M. M. Bronstein, and R. Kimmel, “Expression-in-\\nvariant representations of faces,” IEEE Trans. Image Process. , vol. 16,\\nno. 1, pp. 188–195, Jan. 2007.\\n[16] P. Aleksic and A. Katsaggelos, “Automatic facial expression recogni-\\ntion using facial animation parameters and multistream HMMs,” IEEE\\nTrans. Inf. Forensics Security , vol. 1, no. 1, pp. 3–11, Mar. 2006.\\n[17] S. Ioannou, G. Caridakis, K. Karpouzis, and S. Kollias, “Robust feature\\ndetection for facial expression recognition,” EURASIP J. Image Video\\nProcess. , 2007.\\n[18] C. S. Lee and A. Elgammal, “Nonlinear shape and appearance models\\nfor facial expression analysis and synthesis,” in Proc. 18th Int. Conf.\\nPattern Recognition , 2006, pp. 497–502.\\n[19] M. A. O. Vasilescu and D. Terzopoulos, “Multilinear subspace analysis\\nof image ensembles,” in Proc. IEEE Conf. Computer Vision and Pattern\\nRecognition , Jun. 2003, pp. 93–99.\\n[20] H. Wang and N. Ahuja, “Facial expression decomposition,” in Proc.\\nNinth IEEE Int. Conf. Computer Vision , Oct. 2003, vol. 2, pp. 958–965.\\nMPIPERIS et al. : BILINEAR MODELS FOR 3-D FACE AND FACIAL EXPRESSION RECOGNITION 511\\n[21] J. Tenenbaum and W. Freeman, “Separating style and content,” Ad-\\nvances in Neural Information Processing Systems , vol. 9, 1997.\\n[22] J. Tenenbaum and W. Freeman, “Separating style and content with bi-\\nlinear models,” Neural Comput. , vol. 12, pp. 1247–1283, 2000.\\n[23] C. Mandal, H. Qin, and B. C. Vemuri, “Novel FEM-based dynamic\\nframework for subdivision surfaces,” Comput.-Aided Design , vol. 32,\\nno. 8, pp. 479–497, 2000.\\n[24] J. L. Bentley, “K-d trees for semidynamic point sets,” in Proc. 6th Annu.\\nSymp. Computational Geometry , 1990, pp. 187–197.\\n[25] V. Blanz and T. Vetter, “Face recognition based on ﬁtting a 3D mor-\\nphable model,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25, no. 9,\\npp. 1063–1074, Sep. 2003.\\n[26] D. Metaxas and I. Kakadiaris, “Elastically adaptive deformable\\nmodels,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 2, no. 10, pp.\\n1310–1321, Oct. 2002.\\n[27] B. Allen, B. Curless, and Z. Popovic ´, “The space of human body\\nshapes: Reconstruction and parameterization from range scans,” in\\nProc. SIGGRAPH: ACM SIGGRAPH Papers , New York, 2003, pp.\\n587–594, ACM.\\n[28] G. Edwards, A. Lanitis, C. Taylor, and T. Cootes, “Statistical models\\nof face images: Improving speciﬁcity,” in Proc. British Machine Vision\\nConf. , 1996, vol. 2, pp. 765–774.\\n[29] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance\\nmodels,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23, no. 6, pp.\\n681–685, Jun. 2001.\\n[30] X. Lu and A. K. Jain, “Multimodal facial feature extraction for auto-\\nmatic 3D face recognition” Dept. Comput. Sci., Michigan State Univ.,\\nEast Lansing, Tech. Rep. MSU-CSE-05-22, Aug. 2005.\\n[31] C. V. Loan, “The ubiquitous kronecker product,” J. Comput. Appl.\\nMath. , vol. 123, p. 85100, 2000.\\n[32] D. Lin, Y. Xu, X. Tang, and S. Yan, “Tensor-based factor decomposi-\\ntion for relighting,” in Proc. IEEE Int. Conf. Image Processing , Sep.\\n2005, vol. 2, pp. 386–389.\\n[33] P. Phillips, P. Flynn, T. Scruggs, K. Bowyer, J. Chang, K. Hoffman,\\nJ. Marques, J. Min, and W. Worek, “Overview of the face recognition\\ngrand challenge,” in Proc. IEEE Computer Vision Pattern Recognition ,\\nJun. 2005, pp. 947–954.\\nIordanis Mpiperis received the Diploma in elec-\\ntrical and computer engineering from Aristotle\\nUniversity of Thessaloniki, Thessaloniki, Greece,in 2004, where he is currently pursuing the Ph.D.\\ndegree in electrical and computer engineering.\\nHe holds research and teaching assistantship\\npositions at the Aristotle University of Thessaloniki.\\nHe is a Postgraduate Research Fellow with the Infor-\\nmatics and Telematics Institute, Centre for Research\\nand Technology Hellas, Thessaloniki. His research\\ninterests include biometrics, multimodal interfaces\\nfor human-machine communication, computer vision, and pattern recognition.\\nSotiris Malassiotis was born in Thessaloniki,\\nGreece, in 1971. He received the B.S. and Ph.D.\\ndegrees in electrical engineering from the Aristotle\\nUniversity of Thessaloniki, Thessaloniki, Greece, in\\n1993 and 1998, respectively.\\nFrom 1994 to 1997, he was conducting research\\nin the Information Processing Laboratory at the\\nAristotle University of Thessaloniki. Currently, he\\nis a Senior Researcher (Grade C) in the Informatics\\nand Telematics Institute, Thessaloniki. He has\\nparticipated in many European and national research\\nprojects. He is the author of more than 20 articles in refereed journals and\\nmore than 50 papers in international conferences. His research interests include\\nbiometrics, computer vision, stereoscopic and range image analysis, pattern\\nrecognition, and computer graphics.\\nMichael G. Strintzis (S’68–M’70–SM’80–F’03)\\nreceived the Diploma in electrical engineering fromthe National Technical University of Athens, Athens,\\nGreece, in 1967 and the M.A. and Ph.D. degrees\\nin electrical engineering from Princeton University,\\nPrinceton, NJ, in 1969 and 1970, respectively.\\nHe joined the Electrical Engineering Department,\\nUniversity of Pittsburgh, Pittsburgh, PA, where he\\nserved as an Assistant Professor from 1970 to 1976\\nand an Associate Professor from 1976 to 1980.\\nDuring that time, he worked in the area of stability\\nof multidimensional systems. Since 1980, he has been a Professor of electrical\\nand computer engineering at the Aristotle University of Thessaloniki, Thessa-\\nloniki, Greece. He has worked in the areas of multidimensional imaging and\\nvideo coding. Over the past ten years, he has authored more than 100 journal\\npublications and over 200 conference presentations. In 1998, he founded the\\nInformatics and Telematics Institute, currently part of the Centre for Research\\nand Technology Hellas, Thessaloniki.\\nProf. Strintzis was awarded the Centennial Medal of the IEEE in 1984 and\\nthe Empirikeion Award for Research Excellence in Engineering in 1999.\\n',\n",
       " 'Error',\n",
       " 'Collecting\\nLarge, Richly\\nAnnotated\\nFacial-Expression\\nDatabases from\\nMovies\\nAbhinav Dhall\\nAustralian National University\\nRoland Goecke\\nUniversity of Canberra, Australia\\nSimon Lucey\\nCommonwealth Scientific and Industrial Research\\nOrganization (CSIRO)\\nTom Gedeon\\nAustralian National University\\nCollecting richly annotated, large\\ndatasets representing real-world\\nconditions is a challenging task.With the progress in computer\\nvision research, researchers have developed ro-bust human facial-expression analysis solu-tions, but largely only for tightly controlledenvironments. Facial expressions are the visible\\nfacial changes in response to a person’s inter-\\nnal affective state, intention, or social commu-nication. Automatic facial-expression analysishas been an active research field for morethan a decade, with applications in affectivecomputing, intelligent environments, lie detec-tion, psychiatry, emotion and paralinguisticcommunication, and multimodal human-\\ncomputer interface (HCI).\\nIn the automatic human facial-analysis do-\\nmain, realistic data plays an important role.However, as anyone in the facial-analysis com-munity will attest, such datasets are extremelydifficult to obtain. Much progress has beenmade in the facial- and human-activity-recog-nition fields in the past few years due to the\\navailability of realistic databases as well as ro-\\nbust representation and classification tech-niques. However, although several popularfacial-expression databases exist, the majorityhave been recorded in tightly controlled labora-tory environments, where the subjects wereasked to generate certain expressions. Theselab scenarios are in no way a true representa-\\ntion of the real world. Ideally, we want a dataset\\nof spontaneous facial expressions in challeng-ing real-world environments.\\nTo address this problem, we have collected\\ntwo new facial-expression databases derivedfrom movies via a semiautomatic recommen-der-based method. We extracted a database of\\ntemporal and static facial expressions from\\nscenes in movies, environments that moreclosely resemble the real world than those ofprevious datasets. The database contains videosshowing natural head poses and movements,close-to-real-world illumination, multiple sub-jects in the same frame, occlusions, and search-able metadata. The datasets also cover a large\\nage range, including toddler, child, and teen-\\nager subjects, which are missing in other cur-rently available temporal facial-expressiondatabases.\\nInspired by the Labeled Faces in the Wild\\n(LFW) database,\\n1we call our temporal database\\nActed Facial Expressions in the Wild (AFEW)\\nand its static subset Static Facial Expressions in\\nthe Wild (SFEW).2In this context, ‘‘in the\\nwild’’ refers to the challenging conditions inwhich the facial expressions occur rather thanspontaneous facial expressions. We believethat these datasets will help advance facial-expression research and act as a benchmarkfor experimental validation of facial-expressionanalysis algorithms in real-world environments.\\nConstructing Facial-Expression Datasets\\nUntil now, researchers have manually col-lected all facial-expression databases, which is\\ntime consuming and error prone. To address\\nthis limitation, we propose a video clip[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 34\\nLarge-Scale Multimedia Data Collections\\nTwo large facial-\\nexpression databasesdepicting\\nchallenging real-\\nworld conditionswere constructedusing a semi-automatic approachvia a recommendersystem based onsubtitles.\\n1070-986X/12/$31.00 /C14c2012 IEEE Published by the IEEE Computer Society 34\\nrecommender system based on subtitle pars-\\ning. Rather than manually scan a full movie,\\nour labelers reviewed only the video clips sug-\\ngested by the recommender system, whichsearched for clips with a high probability of asubject showing a meaningful expression.This method lets us collect and annotatelarge amounts of data quickly. Based on theavailability of detailed information regardingthe movies and their content on the Web,the labelers then annotated the video clipswith dense information about the subjects.\\nWe used an XML-based representation for\\nthe database metadata, which makes it search-able and easily accessible using any conven-tional programming language.\\nOver the past decade, researchers have devel-\\noped robust facial-expression analysis methods,which along with their different databases have\\nfollowed various experimental protocols. This\\nseverely limits the ability to objectively evalu-ate the different methods. In response, wehave defined clear experimental protocols,which represent different subject dependencyscenarios.\\nGiven the huge amount of video data on the\\nWeb, it is worthwhile to investigate the prob-\\nlem of facial-expression analysis in tough con-\\nditions. For the AFEW dataset, we labeled thevideo clips with one of six basic expressions:anger, disgust, fear, happiness, sadness, sur-prise, or neutral. The database captures facialexpressions, natural head pose movements,occlusions, subjects’ races, gender, diverseages, and multiple subjects in a scene. Our base-line results show that current facial-expression\\nrecognition approaches that have reportedly\\nachieved high recognition rates on existingdatasets cannot cope with such realistic envi-ronments, underpinning the need for a newdatabase and further research.\\nAlthough movies are often shot in some-\\nwhat controlled environments, they are signifi-cantly closer to real-world environments than\\ncurrent lab-recorded datasets. We do not\\nc l a i mt h a tA F E Wi sas p o n t a n e o u sf a c i a l - e x p r e s -sion database. However, clearly, method actorsattempt to mimic real-world human behaviorto give audiences the illusion that they arebehaving spontaneously, not posing, in mov-ies. The AFEW dataset, in particular, addressesthe issue of temporal facial expressions in diffi-cult conditions that are approximating real-world conditions, which provides for a muchmore difficult test set than currently availabledatasets.\\nRelated Databases\\nOne of the earliest databases published is thewidely used Cohn-Kanade database,\\n3which\\ncontains 97 subjects who posed in a lab situa-tion for the six universal and neutral expres-sions. Its extension CK+ contains 123subjects, but the new videos were shot in asimilar environment.\\n3The Multi-PIE database\\nis another popular database that containsboth temporal and static samples recorded inthe lab over five sessions.\\n4It contains 337 sub-\\njects covering different pose and illuminationscenes. Each of these databases were con-structed manually, with the subjects posingin sequential scenes. The MMI database is asearchable temporal database with 75 sub-jects.\\n6All of these are posed, lab-controlled en-\\nvironment databases. The subjects display\\nvarious acted (not spontaneous) expressions.\\nThe recording environment is nowhere nearreal-world conditions.\\nThe RU-FACS (Rutgers and University of Cal-\\nifornia, San Diego, Facial Action Coding System[FACS]) database is a FACS -coded temporal\\ndatabase containing spontaneous facial expres-sions,\\n6but it is proprietary and unavailable to\\nother researchers. The Belfast database consistsof a combination of studio recordings and TVprogram grabs labeled with particular expres-sions.\\n7The number of TV clips in this database\\nis sparse. Compared to the manual method\\nused to construct and annotate these data-\\nbases, our recommender system method isfaster and more easily accessible. The metadataschema is in XML and, hence, easily search-able and accessible from a variety of languagesand platforms. In contrast, CK, CK+, Multi-PIE, RU-FACS, and Bel fast must be searched\\nmanually.\\nThe Japanese Female Facial Expression\\n(JAFFE) database is one of the earliest static fa-cial-expression datasets.\\n8It contains 219 images\\nof 10 Japanese females. However, it has a lim-ited number of samples and subjects and was\\nalso created in a lab-controlled environment.\\nIn one of the first experiments on close-to-\\nreal data, Marco Paleari, Ryad Chellali, and\\nBenoit Huet proposed a bimodal, audio-videofeatures-based system.\\n9The database was con-\\nstructed from TV programs, but it is fairly\\nsmall, with only 107 clips.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 35July/C151September 2012\\n35\\nTable 1 compares thes e and other facial-\\nexpression databases.\\nOur AFEW database is similar in spirit to the\\nLFW database1and the Hollywood Human\\nActions (HOHA) dataset.15These contain varied\\npose, illumination, age, gender, and occlusion.\\nHowever, LFW is a static facial-recognitiondatabase created from single face imagesfound on the Web specifically for face recogni-tion, and HOHA is an action-recognition data-base created from movies.\\nDatabase Contributions\\nThe AFEW and SFEW databases offer severalnovel contributions to the state of the art.AFEW is a dynamic, temporal facial-expressiondata corpus consisting of short video clips offacial expressions in close-to-real-world envi-ronments. To the best of our knowledge,SFEW is also the only static, tough conditionsdatabase covering the seven facial-expressionclasses.\\nOur subjects ranged from 1 to 70 years old,\\nwhich makes the resulting datasets generic interms of age, unlike other facial-expressiondatabases. The databases have many clipsdepicting children and teenagers, which canbe used to study facial expressions in youngersubjects. The datasets can also be used forboth static and temporal facial age research.\\nTo the best of our knowledge, AFEW is cur-\\nrently the only facial-expression database withmultiple labeled subjects in the same frame.This will enable interesting studies on variousthemes (expressions) involving scenes withmultiple subjects, who might or might nothave the same expression at a given time.\\nThe databases also exhibit close-to-real illu-\\nmination conditions. The clips include sceneswith indoor, nighttime, and outdoor naturalillumination. Although movie studios use\\ncontrolled illumination conditions, even in\\noutdoor settings, these are closer to naturalconditions than lab-controlled environments\\nand, therefore, are valuable for facial-expression\\nresearch. The diverse nature of the illuminationconditions in the dataset makes it useful for not\\njust facial-expression analysis but potentially\\nalso for facial recognition, facial alignment,age analysis, and action recognition.\\nT h em o v i e sw ec h o s ec o v e ral a r g es e to f\\nactors. Many actors appear in multiple moviesin the dataset, which will enable researchersto study how their expressions have evolved\\nover time, whether they differ for different gen-\\nres, and so forth.\\nThe design of the database schema is based\\non XML. This enables further informationabout the data and its subjects to be added eas-ily at any stage without changing the videoclips. This means that detailed annotations\\nwith attributes about the subjects and the\\nscene are possible.\\nThe database download website will also\\ncontain information regarding the experimentprotocols and training and test splits for bothtemporal and static facial-expression recogni-\\ntion (FER) experiments.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 36\\nTable 1. Comparison of temporal facial expression databases.*\\nDatabaseConstruction\\nprocess EnvironmentAge\\nrange Illumination Occlusion Subjects SearchableSubject\\ndetailsMultiple\\nsubjects\\nAFEW Assisted CTR 1 /C15170 CTN Yes 330 Yes Yes Yes\\nBelfast7Manual TV & Lab ? C Yes 100 No No No\\nCK3Manual Lab 18 /C15150 C No 97 No No No\\nCK+3Manual Lab 18 /C15150 C No 123 No No No\\nF.TUM9Manual Lab ? C No 18 No No No\\nGEMEP10Manual Lab ? C Yes 10 No No No\\nM-PIE4Manual Lab 27.9 C Yes 337 No No No\\nMMI5Manual Lab 19 /C15162 C Yes 29 Yes No No\\nPaleari11Manual CTR /C151 CTN Yes /C151 No No No\\nRU-FACS6Manual Lab 18 /C15130 C Yes 100 No No No\\nSemaine12Manual Lab ? C Yes 75 Yes No No\\nUT-Dallas13Manual Lab 18 /C15125 C Yes 284 No No No\\nVAM14Manual CTR ? C Yes 20 No No No\\n* C stands for controlled, CTN for close to natural, and CTR for close to real.IEEE MultiMedia\\n36\\nDatabase Creation\\nTo construct the database, we followed a semi-\\nautomatic approach and divided the processinto two parts (see Figure 1). First, the subtitlesare extracted and parsed in the recommendersystem. Second, a human labeler annotates\\nthe recommended clips based on information\\navailable on the Internet.\\nSubtitle Extraction\\nWe purchased and analyzed 54 movie DVDs.We extracted subtitles for the deaf and hearingimpaired (SDH) and closed caption (CC) subti-\\ntles from the DVDs because they contain infor-\\nmation about the audio and nonaudio contextsuch as emotions and information about theactors and scene (for example, [CHEERING],[SHOUTS], and [SURPRISED]). We extractedthe subtitles from the movies using the Vob-Sub Rip (VSRip) tool (www.videohelp.com/\\ntools/VSRip). For the movies that VSRip\\ncould not extract subtitles, we downloadedthe SDH from the Web. The extracted subtitleimages were parsed using optical character rec-ognition (OCR) and converted into the .srtsubtitle format using the Subtitle Edit tool(www.nikse.dk/se). The .srt format contains\\nthe start time, end time, and textual content\\nwith millisecond accuracy.\\nVideo Recommender System\\nOnce the subtitles have been extracted, weparse the subtitles and search for expression-related keywords—for example, happy, sad,surprised, shouts, cries, groans, cheers, laughs,\\ns o b s ,s i l e n c e ,a n g r y ,w e e p i n g ,s o r r o w ,d i s a p -\\npoint, and amazed. If fo und, the system rec-\\nommends video clips to the labeler. Theclip’s start and end time is extracted from thesubtitle information. The system plays the\\nvideo clips sequentially, and the labeler enters\\ninformation about the clip and its charactersand actors from the Web. If clips contain mul-tiple actors, the labeling sequence is based ontwo criteria. For actors appearing in the same\\nframe, the order of annotation is left to right.\\nIf the actors appear at different timestamps,then it is in the order of appearance. The dom-inating expression in the video is labeled asthe theme expression . The labeling is then\\nstored in an XML metadata schema. Finally,the labeler enters the character’s age or his orher estimated age if this information isunavailable.\\nIn total, the subtitles from the 54 DVDs con-\\ntained 77,666 individual subtitles. Out of these,the recommender system suggested 10,327clips corresponding to subtitles containing ex-pressive keywords. The labelers chose 1,426\\nclips from these on the basis of criteria such\\nas the visible presence of subjects, at leastsome part of the face being visible, and the dis-play of meaningful expressions.\\nBecause subtitles are manually created by\\nhumans, they can contain errors. This mightlead to a situation where the recommender sys-tem suggests an erroneous clip. However, the\\nlabelers can reject a recommendation. When[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 37\\nFigure 1. Database creation process. A subtitle is extracted from a DVD and then parsed by the\\nrecommender system. In this example, from the 2009 movie The Hangover, when the subtitle contains the\\nkeyword ‘‘laugh,’’ the tool plays the corresponding clip. The human labeler then annotates the subjects inthe scene, using a GUI tool, based on the information about the subjects in the clip available on the Web.The resulting annotation, which in this case contains the information about a scene containing multiplesubjects, is stored in the XML schema shown at the bottom of the diagram.\\nRecommender system\\nRecommended clip\\nLabeler\\nExtract\\nclosed-caption\\n subtitlesJuly/C151September 2012\\n37\\nannotating the clips, the labelers use the clips’\\nvideo, audio, and subtitle information to\\nmake informed decisions. We can use the pro-\\nposed recommender system to easily addmore clips to the database and scale it up inthe future.\\nDatabase Annotations\\nOur database contains metadata about thev i d e oc l i p si na nX M L - b a s e ds c h e m a ,w h i c h\\nenables efficient data handling and updating.\\nThe human labelers densely annotated thevideo clips with the expression and subjectinformation.\\nThe subject information contains various\\nattributes describing the actor and/or characterin the scene:\\n/C138Pose. This denotes the head pose based onthe labeler’s observation. In the current ver-sion, we manually classify the head pose asfrontal or nonfrontal.\\n/C138Character age . Frequently, only the age of the\\nlead actors’ characters are available on theWeb. The labeler estimated any other ages.\\n/C138Actor name . Here we provide the actor’s real\\nname.\\n/C138Actor age . The labelers extracted the actor’s\\nreal ages from www.imdb.com. In a few\\ncases, the age information was missing, so\\nthe labeler estimated it.\\n/C138Expression of person .T h i sd e n o t e st h ee x p r e s -\\nsion class of the character as labeled by the\\nhuman observer. This could differ from thehigher-level expression tag because theremight be multiple people in the frame show-\\ning different expressions with respect toeach other and the scene/theme.\\n/C138Gender . Here we provide the actor’s gender.\\nExpression tag specifies the theme expression\\nconveyed by the scene. The expressions weredivided into the six expression classes, plus neu-tral. The default value is based on the search\\nkeyword found in the subtitle text—for exam-\\nple, we use happiness for ‘‘smile’’ and ‘‘cheer.’’Human observer can change it based on theirobservation of the audio and scene in the clip.\\nThis XML-based metadata schema has two\\nmajor advantages. First, it is easy to use andsearch using any standard programming lan-guage on any platform that supports XML. Sec-ond, the structure makes it simple to add new\\nattributes about the video clips in the future,\\nsuch as the pose of the person in degrees andscene information, while keeping the existingdata and ensuring that pre-existing tools canexploit this information with minimal changes.\\nCurrently, the database metadata indexes\\n1,426 video clips. Table 2 gives the databasedetails. Additional information on how to ob-tain the database and its experimental proto-\\ncols are available at http://cs.anu.edu.au/few.\\nSFEW\\nStatic facial-expression analysis databases suchas Multi-PIE and JAFFE are lab-recorded data-bases in tightly controlled environments. Weextracted frames from AFEW to create a static\\nimage database that more closely represents the\\nreal world. Later, we describe the three versionsof SFEW, which are based on the level of subjectdependency for evaluating facial-expressionrecognition performance of systems in different\\nscenarios. The strictly person-independent ver-\\nsion of SFEW is described in an earlier work\\n4\\nand is posted as a challenge on the Benchmark-ing Facial Image Analysis Technologies (BEFIT)website (http://fipa.cs.kit.edu/511.php).\\nComparison with Other Databases\\nTo evaluate our datasets, we compared the per-\\nformance of state-of-the-art descriptors on\\nAFEW and SFEW with that on existing, widelyused datasets. Specifically, we compared AFEWto the CK+ database, which is an extension of\\nthe Cohn-Kanade database.\\n3Ab a s i cf a c i a l[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 38\\nTable 2. AFEW database attributes.\\nAttribute Description\\nLength of sequences 300 /C1515,400 ms\\nNumber of sequences 1,426\\nTotal number of expressions\\n(including multiple subjects)1,747\\nVideo format AVI\\nMaximum number of clips of a subject 134Minimum number of clips of a subject 1Number of labelers 2\\nNumber of subjects 330\\nNumber of clips per expression Anger (194), disgust (123), fear (156),\\nsadness (165) happiness (387),\\nneutral (257), surprise (144)IEEE MultiMedia\\n38\\nexpression consists of various temporal dy-\\nnamic stages: onset, apex, and offset stage. In\\nCK+, all videos follow the temporal dynamic\\nsequence: neutral !onset!apex,w h i c hi s\\nnot a true reflection of how expressions aredisplayed in real-world situations because thedata about the offset phase is missing.\\nWe also argue that all data containing the\\ncomplete temporal sequence might not alwaysbe available. For example, a person entering ascene might already be happy and close to the\\nhighest intensity of happiness (onset). Earlier\\nsystems trained on existing databases like CK+have learned on such stages. However, theavailability of full temporal dynamic stages isnot guaranteed in real-world settings. In ourd a t a b a s e ,t h i si sn o tf i x e dd u et oi t sc l o s e - t o -natural settings. To extract a face, we computedthe Viola-Jones detector\\n16over the CK+ sequen-\\nces. In our comparison experiments, we usedsix common classes from both the AFEW and\\nCK+ databases (anger, fear, disgust, happiness,\\nsadness, and surprise).\\nWe compared SFEW with the JAFFE and\\nMulti-PIE databases in two experiments:\\n/C138a comparison of SFEW, JAFFE, and Multi-PIE\\non the basis of four common expression\\nclasses (disgust, neutral, happiness, and sur-prise) and\\n/C138a comparison of SFEW and JAFFE on allseven expression classes.\\nWe computed feature descriptors on the\\ncropped faces from all the databases. The\\ncropped faces were divided into 4 /C24 blocks\\nfor local binary pattern (LBP),\\n17local phase\\nquantization (LPQ),17and pyramid histogram\\nof gradients (PHOG).18For LBP and LPQ, we\\nset the neighborhood size to eight. For PHOG,\\nbin length was eight, pyramid levels Lwere 2,\\nand angle range equaled [0, 360]. We appliedprincipal component analysis (PCA) on theextracted features and kept 98 percent of thevariance. For classification, we used a support\\nvector machine (SVM) learned model. The ker-\\nnel was C-support vector classification (C-SVC),with a radial basis function (RBF) kernel. Weused five-fold cross validation to select theparameters. For AFEW, the static descriptorswere concatenated.\\nLBP-TOP performed the best out of all the\\nmethods. The overall expression classification\\naccuracy is much higher for CK+ (see Figure 2).For the SFEW four-expression class experi-\\nment, the classification accuracy on the Multi-\\nPIE subset was 86.25 and 88.25 percent forLPQ and PHOG, respectively. For JAFFE, it was83.33 percent for LPQ and 90.83 percent for\\nPHOG. For SFEW, it was 53.07 percent for\\nLPQ and 57.18 percent PHOG. For the seven-expression class experiment, the classificationaccuracy for JAFFE was 69.01 percent for LPQ\\nand 86.38 percent for PHOG. For SFEW, it was\\n43.71 percent for LPQ and 46.28 percent forPHOG. Thus, LPQ and PHOG achieve a high ac-curacy on JAFFE and Multi-PIE, but a signifi-\\ncantly lower accuracy for SFEW.\\nIn our opinion, the primary reason for the\\npoor performance of state-of-the-art descriptorson AFEW and SFEW is that the databases on\\nwhich these state-of-the-art methods havebeen experimented on were recorded in lab-based environments. Expression analysis in\\nclose-to-real-world sit u a t i o n si san o n t r i v i a l\\ntask and requires more sophisticated methods\\na ta l ls t a g e so ft h ea p p r o a c h ,s u c ha sr o b u s tface localization and tracking, illumination,\\nand pose invariance.\\nExperimentation Protocols\\nOver the years, researchers have proposedmany facial-expression recognition methods\\nbased on experiments on various databases fol-lowing different protocols, making it difficultto compare the results fairly. Therefore, we cre-\\nated strict experimentation protocols for both\\ndatabases. The different protocols are based onthe level of person dependency present in thesets (see Table 3).[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 39\\nFigure 2. Performance of LBP, PHOG, LPQ, and LBP-TOP on the CK+ and\\nAFEW databases. The descriptors performed poorly on the AFEW dataset.80\\n010203040506070\\nLBP PHOG LPQ LBP-TOPCK + AFEWJuly/C151September 2012\\n39\\nThe BEFIT workshop challenge2falls under\\nStrictly Person Indepe ndent (SPI) for SFEW.\\nData, labels, and other protocols will be\\nmade available on the database website.\\nAFEW Partial Person Independent (PPI) con-\\ntains 745 videos and AFEW SPI contains 741videos in two sets. AFEW Strictly Person Inde-pendent (SPI) contains 40 videos of the actorDaniel Radcliffe for four expression categories\\n(fear, happiness, neutral, and surprise). For\\nSFEW, SFEW SPS contains 76 images of DanielRadcliffe for five expression classes (anger,fear, happiness, neutral, and surprise). SFEWPPI contains 700 images and SFEW SPI con-tains 700 images in two sets.\\nBaseline\\nFor all the protocols for SFEW, we computedthe baselines based on the method defined in\\nour earlier work.\\n2(These results are an average\\nof training and testing on the sets.) PHOG and\\nLPQ features were computed on the croppedface. The features were concatenated togetherto form a feature vector. For dimensionality re-duction, we computed PCA and kept 98 per-\\ncent of the variance. Furthermore, we used a\\nnonlinear SVM to learn and classify expres-sions. (Again, see our earlier work for the pa-rameter selection details.\\n2) To encode the\\ntemporal data, we computed LBP-TOP fea-\\ntures, as in the previous section.\\nTable 4 shows the classification accuracy for\\nboth databases and their protocols. The lowclassification accuracy results demonstrate\\nthat the current methods are inappropriate for\\nreal-world scenarios.\\nConclusions\\nFacial-expression analysis is a well-researched\\nfield. However, progress in the field has been\\nhampered due to the unavailability of data-bases depicting real-world conditions. State-\\nof-art FER methods that have performed well\\non existing datasets do not work well on thedatasets we propose here. This is due to alack of robust ‘‘in the wild’’ face-alignment\\nmethods and efficient temporal descriptors.\\nAs part of future work, we will adapt current\\nmethods and extend algorithms for FER in\\ntough conditions. AFEW contains group-level-expression video clips, which in the future\\ncan be used to develop systems that analyze\\ntheme expressions in scenes containing groupsof people. We believe that these datasets will\\nenable novel contributions to facial-expression\\nresearch and act as a benchmark for experimen-tal validation of facial-expression analysis algo-rithms in real-world environments. MM\\nReferences\\n1. G.B. Huang et al., Labeled Faces in the Wild: A\\nDatabase for Studying Face Recognition in Uncon-\\nstrained Environments, tech. report 07-49, Univ.\\nof Massachusetts, Amherst, 2007.\\n2. A. Dhall et al., ‘‘Static Facial Expression Analysis\\nin Tough Conditions: Data, Evaluation Protocoland Benchmark,’’ Proc. IEEE Int’l Conf. Computer\\nVision Workshop (BeFIT), IEEE Press, 2011,\\npp. 2106 /C1512112.\\n3. P. Lucey et al., ‘‘The Extended Cohn-Kanade\\nDataset (CK+): A Complete Dataset for Action\\nUnit and Emotion-Specified Expression,’’ Proc.\\nIEEE Conf. Computer Vision and Pattern Recognition\\nWorkshops (CVPR4HB), IEEE CS Press, 2010,\\npp. 94 /C151101.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 40\\nTable 4. Average classification accuracies of different protocols.\\nProtocol Anger (%) Disgust (%) Fear (%) Happiness (%) Neutral (%) Sadness (%) Surprise (%) Average (%)\\nAFEW PPI 32.5 12.3 14.1 44.2 33.8 25.2 21.8 26.3\\nAFEW SPI 40.1 7.9 14.5 37.0 40.1 23.5 8.9 24.5\\nAFEW SPS /C151/C151 0.0 50.0 0.0 /C151 50.0 25.0\\nSFEW PPI 29.5 43.5 48.5 35.5 33.0 12.0 35.0 33.8\\nSFEW SPI 23.0 13.0 13.9 29.0 23.0 17.0 13.5 18.9\\nSFEW SPS 35.0 /C151 45.8 0.0 7.1 /C151 0.0 17.5Table 3. Experimentation protocol scenarios for SFEW and AFEW.\\nProtocol AFEW/SFEW training-test content\\nStrictly Person Specific (SPS) Same single subject\\nPartial Person Independent (PPI) A mix of common and different subjects\\nStrictly Person Independent (SPI) Different subjects2\\n40\\n4. R. Gross et al., ‘‘Multi-PIE,’’ Proc. 8th IEEE Int’l\\nConf. Automatic Face and Gesture Recognition (FG),\\n2008, pp. 1 /C1518, doi:10.1109/AFGR.2008.4813399.\\n5. M. Pantic et al., ‘‘Web-Based Database for Facial\\nExpression Analysis,’’ Proc. IEEE Int’l Conf. Multi-\\nmedia and Expo (ICME), IEEE CS Press, 2005,\\npp. 317 /C151321.\\n6. M.S. Bartlett et al., ‘‘Automatic Recognition of Fa-\\ncial Actions in Spontaneous Expressions,’’ J. Multi-\\nmedia, vol. 1, no. 6, 2006, pp. 22 /C15135.\\n7. E. Douglas-Cowie, R. Cowie, and M. Schro ¨der,\\n‘‘A New Emotion Database: Considerations,\\nSources and Scope,’’ Proc. ISCA ITRW on Speech\\nand Emotion, 2000, pp. 39 /C15144.\\n8. M.J. Lyons et al., ‘‘Coding Facial Expressions with\\nGabor Wavelets,’’ Proc. IEEE Int’l Conf. Automatic\\nFace Gesture Recognition and Workshops (FG), IEEE\\nCS Press, 1998, p. 200.\\n9. F. Wallhoff, ‘‘Facial Expressions and Emotion Data-\\nbase,’’ 2006, www.mmk.ei.tum.de/~waf/fgnet/\\nfeedtum.html.\\n10. T. Ba ¨nziger and K. Scherer, ‘‘Introducing the Geneva\\nMultimodal Emotion Portrayal (GEMEP) Corpus,’’\\nBlueprint for Affective Computing: A Sourcebook,\\nK. Scherer, T. Ba ¨nziger, and E. Roesch, eds.,\\nOxford Univ. Press, 2010.\\n11. M. Paleari, R. Chellali, and B. Huet, ‘‘Bimodal\\nEmotion Recognition,’’ Proc. 2nd Int’l Conf. Social\\nRobotics (ICSR), Springer, 2010, pp. 305 /C151314.\\n12. G. McKeown et al., ‘‘The SEMAINE Corpus of\\nEmotionally Coloured Character Interactions,’’Proc. IEEE Int’l Conf. Multimedia and Expo (ICME),\\nIEEE CS Press, 2010, pp. 1079 /C1511084.\\n13. A.J. O’Toole et al., ‘‘A Video Database of Moving\\nFaces and People,’’ IEEE Trans. Pattern Analysis\\nand Machine Intelligence, vol. 27, no. 5, 2005,\\npp. 812 /C151816.\\n14. M. Grimm, K. Kroschel, and S. Narayanan, ‘‘The\\nVera am Mittag German Audio-Visual EmotionalSpeech Database,’’ Proc. IEEE Int’l Conf. Multimedia\\nand Expo (ICME), IEEE CS Press, 2008, pp. 865 /C151868.\\n15. I. Laptev et al., ‘‘Learning Realistic Human Actions\\nfrom Movies,’’ Proc. IEEE Conf. Computer Vision\\nand Pattern Recognition (CVPR), IEEE CS Press,\\n2008, pp. 1 /C1518.\\n16. P .A. Viola and M.J. Jones, ‘‘Rapid Object Detection\\nUsing a Boosted Cascade of Simple Features,’’\\nProc. IEEE Conf. Computer Vision and Pattern Recog-\\nnition (CVPR), IEEE CS Press, 2001, pp. 511 /C151518.\\n17. D. Huang et al., ‘‘Local Binary Patterns and its Appli-\\ncation to Facial Image Analysis: A Survey,’’ IEEE\\nTrans. Systems, Man, and Cybernetics, Part C: Applica-\\ntions and Reviews, vol. 41, no. 6, 2011, pp. 1 /C15117.18. A. Bosch, A. Zisserman, and X. Munoz, ‘‘Repre-\\nsenting Shape with a Spatial Pyramid Kernel,’’\\nProc. 6th ACM Int’l Conf. Image and Video Retrieval(CIVR), ACM Press, 2007, pp. 401 /C151408.\\nAbhinav Dhall isa d\\n octoral candidate in the Re-\\nsearch School of Computer Science at the AustralianNational University. His research interests include\\naffective computing, computer vision, pattern recog-\\nnition, and human-computer interaction. Dhall has aBS in computer science from the DAV Institute ofEngineering and Technology, India. He is a student\\nmember of IEEE and was awarded the 2010 Australian\\nLeadership Award Scholarship. Contact him atabhinav.dhall@anu.edu.au.\\nRoland Goecke leads the Vision and Sensing Group\\nin the Faculty of Information Sciences and Engineer-\\ning at the University of Canberra, Australia. His re-\\nsearch interests include affective computing,computer vision, human-computer interaction, and\\nmultimodal signal processing. Goecke has a PhD in\\ncomputer science from the Australian National Uni-versity. He is a member of IEEE. Contact him atroland.goecke@ieee.org.\\nSimon Lucey is a Senior Research Scientist in the\\nCSIRO and a \"Futures Fellow Award\" recipient from\\nthe Australian Research Council. He holds adjunctProfessorial positions at the University of Queenslandand, Queensland University of Technology. His re-\\nsearch interests include computer vision and ma-\\nchine learning and their application to humanbehavior. He is a member of IEEE and was awardedthe 2009 ARC Future Fellowship by the Australian Re-\\nsearch Council. Lucey has a PhD in computer science\\nfrom the Queensland University of Technology, Bris-bane. Contact him at simon.lucey@csiro.au.\\nTom Gedeon is the chair professor of computer\\nscience at the Australian National University and pres-\\nident of the Computing Research and Education Asso-ciation of Australasia. His research interests includethe development of automated systems for informa-\\ntion extraction and synthesis into humanly useful in-\\nformation resources, mostly using fuzzy systems andneural networks. Gedeon has a PhD from the Univer-sity of Western Australia. He is a member of IEEE and\\na former president of the Asia-Pacific Neural Network\\nAssembly. Contact him at tom@cs.anu.edu.au.[3B2-9] mmu2012030034 .3d 12/7/012 10:42 Page 41July/C151September 2012\\n41\\n',\n",
       " 'IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009 1217\\nColor Face Recognition for Degraded Face Images\\nJae Young Choi, Yong Man Ro, Senior Member, IEEE , and Konstantinos N. (Kostas) Plataniotis, Senior Member, IEEE\\nAbstract —In many current face-recognition (FR) applications,\\nsuch as video surveillance security and content annotation in aweb environment, low-resolution faces are commonly encounteredand negatively impact on reliable recognition performance. Inparticular, the recognition accuracy of current intensity-based FRsystems can signiﬁcantly drop off if the resolution of facial imagesis smaller than a certain level (e.g., less than 20 ×20 pixels).\\nTo cope with low-resolution faces, we demonstrate that facial colorcue can signiﬁcantly improve recognition performance comparedwith intensity-based features. The contribution of this paper istwofold. First, a new metric called “variation ratio gain” (VRG) isproposed to prove theoretically the signiﬁcance of color effect on\\nlow-resolution faces within well-known subspace FR frameworks;\\nVRG quantitatively characterizes how color features affect therecognition performance with respect to changes in face resolu-tion. Second, we conduct extensive performance evaluation studiesto show the effectiveness of color on low-resolution faces. In partic-ular, more than 3000 color facial images of 341 subjects, which arecollected from three standard face databases, are used to performthe comparative studies of color effect on face resolutions to bepossibly confronted in real-world FR systems. The effectiveness ofcolor on low-resolution faces has successfully been tested on threerepresentative subspace FR methods, including the eigenfaces, theﬁsherfaces, and the Bayesian. Experimental results show that colorfeatures decrease the recognition error rate by at least an orderof magnitude over intensity-driven features when low-resolutionfaces (25 ×25 pixels or less) are applied to three FR methods.\\nIndex Terms —Color face recognition (FR), face resolution, iden-\\ntiﬁcation, variation ratio gain (VRG), veriﬁcation (VER), videosurveillance, web-based FR.\\nI. INTRODUCTION\\nFACE recognition (FR) is becoming popular in research and\\nis being revisited to satisfy increasing demands for video\\nsurveillance security [1]–[3], annotation of faces on multimedia\\ncontents [4]–[7] (e.g., personal photos and video clips) in webenvironments, and biometric-based authentication [58]. Despite\\nthe recent growth, precise FR is still a challenging task due to\\nill-conditioned face capturing conditions, such as illumination,\\nManuscript received September 14, 2008; revised December 28, 2008. First\\npublished March 24, 2009; current version published September 16, 2009. The\\nwork of J. Y . Choi and Y . M. Ro was supported by the Korean Government\\nunder Korea Research Foundation Grant KRF-2008-313-D01004. The work ofK. N. Plataniotis was supported in part by the Natural Science and Engineering\\nResearch Council of Canada under the Strategic Grant BUSNet. This paper was\\nrecommended by Associate Editor J. Su.\\nJ. Y . Choi and Y . M. Ro are with the Image and Video System Laboratory,\\nKorea Advanced Institute of Science and Technology (KAIST), Daejeon305-732, Korea (e-mail: jygchoi@kaist.ac.kr; ymro@ee.kaist.ac.kr).\\nK. N. Plataniotis is with the Edward S. Rogers, Sr. Department of Electrical\\nand Computer Engineering, University of Toronto, Toronto, ON M5S 3G4,\\nCanada, and also with the School of Computer Science, Ryerson University,Toronto, ON M5B 2K3, Canada (e-mail: kostas@comm.toronto.edu).\\nColor versions of one or more of the ﬁgures in this paper are available online\\nat http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TSMCB.2009.2014245pose, aging, and resolution variations between facial images\\nbeing of the same subject [8]–[10]. In particular, many current\\nFR-based applications (e.g., video-based FR) are commonly\\nconfronted with much-lower-resolution faces (20×20 pixels\\nor less) and suffer largely from them [2], [11], [12]. Fig. 1\\nshows practical cases in which the faces to be identiﬁed or\\nannotated have very small resolutions due to limited acquisitionconditions, e.g., faces captured from long distance closed-\\ncircuit television (CCTV) cameras or camera phones. As can\\nbe seen in Fig. 1(a) and (b), the faces enclosed in red boxeshave much lower resolution and additional blurring, which\\noften lead to unacceptable performance in the current grayscale\\n(or intensity)-based FR frameworks [13]–[18].\\nIn the practical FR applications, which frequently encounter\\nlow-resolution faces , it is of utmost importance to select face\\nfeatures that are robust against severe variations in face resolu-\\ntion and to make efﬁcient use of these features. In contrast to the\\nintensity-driven features, color-based features are known to beless susceptible to resolution changes for objection recognition\\n[20]. In particular, the psychophysical results of the FR test in\\nhuman visual systems showed that the contribution of facialcolor becomes evident when the shapes of faces are getting\\ndegraded [21]. Recently, considerable research effort has been\\ndevoted to the efﬁcient utilization of facial color information toimprove the recognition performance [22]–[29]. For the color\\nFR reported so far, questions could be categorized as follows:\\n1) Was color information helpful in improving the recogni-tion accuracy compared with using grayscale only [22]–[29];\\n2) how were three different spectral channels of face images\\nincorporated to take advantages of face color characteristics\\n[22], [24], [25], [28], [29]; and 3) which color space was the\\nbest for providing discriminate power needed to perform thereliable classiﬁcation tasks [22], [25], [26]? To our knowledge,\\nhowever, the color effect on face resolution has not yet been\\nrigorously investigated in the current color-based FR works,and no systematic work suggests the effective color FR frame-\\nwork robust against much-lower-resolution faces in terms of\\nrecognition performance.\\nIn this paper, we carry out extensive and systematic studies\\nto explore the facial color effect on the recognition performance\\nas the face resolution is signiﬁcantly changed. In particular, wedemonstrate the signiﬁcant impact of color on low-resolution\\nfaces by comparing the performance between grayscale and\\ncolor features. The novelty of this paper comes from thefollowing.\\n1) The derivation of a new metric, which is the so-called\\nvariation ratio gain (VRG), for providing the theoreti-\\ncal foundation to prove the signiﬁcance of color effecton low-resolution faces. Theoretical analysis was made\\nwithin subspace-based FR methods, which is currently\\n1083-4419/$25.00 © 2009 IEEE\\n1218 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nFig. 1. Practical illustrations of extremely small-sized faces in FR-based applications. (a) Surveillance video frame from “Washington Dulles Int ernational\\nAirport.” The two face regions occupy approximately 18 ×18 pixels of the video frame shown, having an original resolution of 410 ×258 pixels. (b) Personal\\nphoto from a “Flickr” [19] web site. The face region occupies about 14 ×14 pixels in the picture shown, having an original resolution of 500 ×333 pixels.\\none of the most popular FR techniques [30], [31] due to\\nreliability in performance and simplicity in implementa-tion. VRG quantitatively characterizes how color features\\naffect recognition performance with respect to changes in\\nface resolution.\\n2) Extensive and comparative recognition performance eval-\\nuation experiments to show the effectiveness of color on\\nlow-resolution faces. In particular, 3192 frontal facial im-ages corresponding to 341 subjects collected from three\\npublic data sets of the Carnegie Mellon University Pose,\\nIllumination, and Expression (CMU PIE) [32], Facial\\nRecognition Technology (FERET) [33], and the Extended\\nMultimodal Veriﬁcation for Teleservices and SecurityApplications Database (XM2VTSDB) [34] were used to\\ndemonstrate the contribution of color to improved recog-\\nnition accuracy over various face resolutions commonlyencountered from still-image- to video-based real-world\\nFR systems. In addition, the effectiveness of color has\\nsuccessfully been tested on three representative subspaceFR methods—principal component analysis [35] (PCA or\\n“eigenfaces”), linear discriminant analysis [8], [36] (LDA\\nor “ﬁsherfaces”), and Bayesian [37] (or “probabilisticeigenspace”). According to experimental results, the ef-\\nfective use of color features drastically reduces the lower\\nbound of face resolution to be reliably recognizable inthe computer FR beyond what is possible with intensity-\\nbased features.\\nThe rest of this paper is organized as follows. The next sec-\\ntion provides background about the low-resolution-face prob-\\nlem in the current FR works. Section III introduces the proposed\\ncolor FR framework. In Section IV, we ﬁrst deﬁne variationratio and then make a theoretical analysis to explain the effect\\nof color on variation ratio. In Section V, based on an analysis\\nmade in Section IV, VRG is proposed to provide a theoreticalinsight on the relationship between color effect and face resolu-\\ntions. Section VI presents the results of extensive experiments\\nperformed to demonstrate the effectiveness of color on low-resolution faces. The conclusion is drawn in Section VI.\\nII. R\\nELATED WORKS\\nIn the state-of-the-art FR research, a few works dealt with\\nface-resolution issues. The main concern in these works would\\nbe summarized as follows: 1) what is the minimum face reso-lution to be potentially encountered with the practical applica-\\ntions and to be detectable and recognizable in the computer FRsystems [2], [13], [14], [38]–[40] and 2) how do low-resolution\\nfaces affect the detection or recognition performances\\n[15]–[17], [40]. In the cutting-edge FR survey literature [2],15×15 pixels is considered to be the minimum face resolution\\nfor supporting reliable detection and recognition. The CHIL\\nproject [14] reported that normal face resolution in video-basedFR is from 10 to 20 pixels in the eye distance. Furthermore, they\\nindicated that the face region is usually 1/16th of commonly\\nused TV recording video frames of resolutions of 320 ×\\n240 pixels. Furthermore, the FR vendor test (FRVT) 2000\\n[12] studied the effect of face resolution on the recognitionperformance until the eye distance on the face is as low as 5 ×\\n5 pixels. In the research ﬁelds of face detection, 6 ×6p i x e l so f\\nfaces has been reported so far to be the lowest resolution that isfeasible for automatic detection [40]. Furthermore, the authors\\nof [39] proposed the face detection algorithm that supports\\nacceptable detection accuracy, even until 11 ×11 pixels.\\nSeveral previous works also examined how low-resolution\\nfaces impact on recognition performance [15]–[17]. Their\\nworks were carried out through intensity-based FR frameworks.They reported that much-lower-resolution faces signiﬁcantly\\ndegrade recognition performance in comparison with higher-\\nresolution ones. In [15], face registration and recognitionperformances were investigated with various face resolutions\\nranging from 128 ×128 to 8 ×8 pixels. They revealed that\\nface resolutions below 32 ×32 pixels show a considerable\\ndecreased recognition performance in PCA and LDA. In [16],\\nface resolutions of 20 ×20 and 10 ×10 pixels dramatically\\ndeteriorated recognition performance compared with 40 ×\\n40 pixels in video-based FR systems. Furthermore, the author of\\n[17] reported that the accuracy for face expression recognitionis dropped off below 36 ×48 pixels in the neural-network-\\nbased recognizer.\\nObviously, low-resolution faces impose a signiﬁcant restric-\\ntion on current intensity-based FR applications to accomplish\\nreliability and feasibility . To handle low-resolution-face prob-\\nlems, resolution-enhancement techniques such as “superresolu-tion” [18], [41], [42] are traditional solutions. These techniques\\nusually estimate high-resolution facial images from several\\nlow-resolution ones. One critical disadvantage, however, is thatthe applicability of these techniques is limited to restricted FR\\ndomain. This is because they require a sufﬁcient number of\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1219\\nmultiple low-resolution images captured from the same identity\\nfor the reliable estimation of high-resolution faces. In practice,\\nit is difﬁcult to always support such requirement in practical\\napplications (e.g., the annotation of low-resolution faces on\\npersonal photos or snapshot Web images). Another drawbackto these approaches is the requirement of a complex framework\\nfor the estimation of an image degradation model. It is also\\ncomputationally demanding for the reconstruction of a high-resolution face image. In this paper, we propose an effective\\nand simple method of using face color features to overcome the\\nlow-resolution-face problem. The proposed color FR methodimproves degraded recognition accuracy, which is caused by\\nlow-resolution faces, by a signiﬁcant margin compared to con-\\nventional intensity-based FR frameworks. In addition, contraryto previous resolution-enhancement algorithms, our approach is\\nnot only simple in implementation but also guarantees extended\\napplicability to FR applications where only a single colorimage with a low resolution is available during actual testing\\noperations.\\nIII. C\\nOLOR FR F RAMEWORK\\nIn this section, we formulate the baseline color FR frame-\\nwork [20] that can make efﬁcient use of facial color features toovercome low-resolution faces. Red–green–blue (RGB) color\\nface images are ﬁrst converted into another different color\\nspace (e.g., YC\\nbCrcolor space). Let Ibe a color face image\\ngenerated in the color space conversion process. Then, let sm\\nbe anmth spectral component vector of I(in the form of a col-\\numn vector by lexicographic ordering of the pixel elements of2-D spectral images), where s\\nm∈RNmandRNmdenotes an\\nNm-dimensional real space. Then, the face vector is deﬁned\\nas the augmentation (or combination) of each spectral compo-nents\\nmsuch that x=[sT\\n1sT2···sT\\nK]T, where x∈RN,\\nN=/summationtextK\\nm=1Nm, andTrepresents the transpose operator of\\nthe matrix. Note that each smshould be normalized to zero\\nmean and unit variance prior to their augmentation. Face vector\\nxcan be generalized in that, for K=1, the face vector could be\\ndeﬁned by grayscale only, while for K=3, it could be deﬁned\\nby a spectral component conﬁguration like YCbCrorYQ C r\\nby column order from YCbCrandYIQ color spaces.\\nMost subspace FR methods are separately divided into the\\ntraining and testing stages. Given a set {Ii}M\\ni=1ofMcolor\\nface images, Iishould be ﬁrst rescaled into the prototype\\ntemplate size to be used for the creation of a corresponding facevector x\\ni. With a formed training set {xi}M\\ni=1ofMface vector\\nsamples, the feature subspace is trained and constructed. The\\nrationale behind the feature subspace construction is to ﬁnd aprojection matrix Φ=[ e\\n1e2···eF]by optimizing crite-\\nria to get a lower dimensional feature representation f=ΦTx,\\nwhere each column vector eiis a basis vector spanning the\\nfeature subspace Φ∈RN×F, andf∈RF. It should be noted\\nthatF/lessmuchN. For the testing phase, let {gi}G\\ni=1b eag a l l e r y\\n(or target) set consisting of Gprototype enrolled face vectors of\\nknown individuals, where gi∈RN. In addition, let pbe an un-\\nknown face vector to be identiﬁed or veriﬁed, which is denoted\\nas a probe (or query), where p∈RN. To perform FR tasks\\non the probe, gi(i=1,...,G )andpare projected onto thefeature subspace to get corresponding feature representations\\nsuch that\\nfgi=ΦTgi,f p=ΦTp (1)\\nwhere fgi∈RFandfp∈RF. A nearest-neighbor classiﬁer\\nis then applied to determine the identity of pby ﬁnding the\\nsmallest distance between fgi(i=1,...,G )andfpin the\\nfeature subspace as follows:\\n/lscript(p)=/lscript(gi∗),i∗=a r gG\\nmin\\ni=1/bardblfgi−fp/bardbl (2)\\nwhere /lscript(·)returns a class label of face vectors, and /bardbl·/bardbl denotes\\nthe distance metric. To exploit why the role of color is getting\\nsigniﬁcant as face resolution is decreased within our baseline\\ncolor FR framework, a theoretical analysis will be given in thefollowing sections.\\nIV . A\\nNALYSIS OF COLOR EFFECT AND FACE RESOLUTION\\nWang and Tang [43] proposed a face difference model that\\nestablishes a uniﬁed framework of PCA, LDA, and Bayesian\\nFR methods. Based on this model, intra- and extrapersonal\\nvariations of feature subspace are critical factors in determiningthe recognition performance in the three methods. These two\\nparameters are quantitatively well represented by the variation\\nratio proposed in [44]. Before exploiting the color effect onthe recognition performance with respect to changes in face\\nresolution, we begin by introducing the variation ratio and\\nexplore how chromaticity components affect the variation ratio\\nwithin our color FR framework.\\n1\\nA. Variation Ratio\\nIn PCA, covariance matrix Ccan be computed by using the\\ndifferences between all possible pairs of two face vectors [43]\\nincluded in {xi}M\\ni=1such that\\nC=M/summationdisplay\\ni=1M/summationdisplay\\nj=1(xi−xj)(xi−xj)T. (3)\\nThen, Cis decomposed into intra- (or within) and extrapersonal\\n(or between class) covariance matrices [43], which are denoted\\nas IC and EC, respectively. IC and EC are deﬁned as\\nIC=/summationdisplay\\nl(xi)=l(xj)(xi−xj)(xi−xj)T\\nEC=/summationdisplay\\nl(xi)/negationslash=l(xj)(xi−xj)(xi−xj)T(4)\\nwhere l(·)is a function that returns a class label of xias input.\\nAs pointed out in [43], the total variation that resides in\\nthe feature subspace is divided into intra- and extrapersonal\\n1In this paper, a theoretical analysis in only the PCA-based color FR\\nframework is given. Our analysis, however, is readily applied to LDA andBayesian due to the same intrinsic connection of intra- and extrapersonal\\nvariations described in [43].\\n1220 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nvariations related to IC and EC, respectively. From a classiﬁca-\\ntion point of view, it is evident that the recognition performance\\nis enhanced as the constructed feature subspace learns and\\ncontains a larger variation of EC than that of IC. From this\\nprinciple, the ratio of extra- to intrapersonal variations can beadopted as an important parameter that reﬂects the discrimina-\\ntive power of feature space [45]. To deﬁne variation ratio, ﬁrst,\\nletΦbe an eigenvector matrix of C, and then, let Var\\nΦ(IC) and\\nVarΦ(EC) be intra- and extrapersonal variations of the feature\\nsubspace spanned by Φ, which are computed as [44]\\nVarΦ(IC)=tr(ΦTICΦ),VarΦ(EC)=tr(ΦTECΦ) (5)\\nwhere tr(·)is a trace operator of the matrix. Using (5), the\\nvariation ratio (J)is deﬁned as\\nJ=VarΦ(EC)\\nVarΦ(IC). (6)\\nAsJincreases, a trained feature subspace relatively includes a\\nlarger variation of EC in comparison to that of IC. Therefore,\\nJrepresents a well-discriminative capability of the feature\\nsubspace for classiﬁcation tasks. In (6), the formulation of\\nVarΦ(IC) and VarΦ(EC) is similar to that of the J-statistic\\n[55] used in the ﬁeld of economics. However, it should be\\npointed out that the metric is used in a novel and quite different\\nway. In particular, the J-statistic has been used as a criterion\\nfunction to determine the optimal unknown parameter vectors\\n[55], while VarΦ(IC) and VarΦ(EC) are used to represent\\nquantitatively the discriminative “effectiveness” of the featuresubspace spanned by Φ.\\nB. Intra- and Extrapersonal Variations in Color FR\\nIn the following section, without loss of generality, we\\nassume that the ith face vector x\\niis a conﬁguration of one\\nluminance (si1)and two different chromaticity components\\n(si2andsi3) so that xi=[sT\\ni1sT\\ni2sT\\ni3]T. By substituting\\n[sT\\ni1sT\\ni2sT\\ni3]Tintoxiin (3), Cis written as\\nC=⎡\\n⎣C11C12C13\\nC21C22C23\\nC31C32C33⎤\\n⎦ (7)\\nwhere Cmn=/summationtextM\\ni=1/summationtextM\\nj=1(sim−sjm)(sin−sjn)T, andm,\\nn=1,2,3. As shown in (7), Cis a block covariance\\nmatrix whose entries are partitioned into covariance or cross-\\ncovariance submatrices Cmn.F o rm=n,Cmnis a covariance\\nsubmatrix computed from a set {sim}M\\ni=1; otherwise, for m/negationslash=n,\\nCmn is a cross-covariance submatrix computed between\\n{sim}M\\ni=1and{sin}M\\ni=1, where Cmn=CT\\nnm.F r o m( 4 ) ,t h eI C\\nand EC decompositions of Cshown in (7) are represented as\\nIC=⎡\\n⎣IC11IC12IC13\\nIC21IC22IC23\\nIC31IC32IC33⎤\\n⎦\\nEC=⎡\\n⎣EC11EC12EC13\\nEC21EC22EC23\\nEC31EC32EC33⎤\\n⎦ (8)where IC mnand EC mnare\\nICmn=/summationdisplay\\nl(xi)=l(xj)(sim−sjm)(sin−sjn)T\\nECmn=/summationdisplay\\nl(xi)/negationslash=l(xj)(sim−sjm)(sin−sjn)T. (9)\\nLikeC, IC and EC are also block covariance matrices.\\nTo explore the color effect on variation ratio, we analyze\\nhow IC mnand EC mn, which are computed from two different\\nchromaticity components of smandsn(m,n=2,3), impact\\non the construction of variations of IC and EC in (8). By the\\nproof given in the Appendix, trace values of IC and EC can bewritten as\\ntr(IC)= 3/summationdisplay\\nm=1tr(IΛmm),tr(EC)=3/summationdisplay\\nm=1tr(EΛmm)(10)\\nwhere IΛmm andEΛmm are diagonal eigenvalue matrices\\nof IC mm and EC mm, respectively. Using (5) and the cyclic\\nproperty of the trace operator, the variations of IC and EC arecomputed as\\nVar\\nΦ(IC) = tr(ΦΦTIC)=t r ( IC)\\nVarΦ(EC) = tr(ΦΦTEC)=t r ( EC) (11)\\nwhere Φis an eigenvector matrix of Cdeﬁned in (7).\\nFurthermore, using (5) and the diagonalization of a matrix, the\\nvariations of IC mmand EC mmare computed as\\nVarΦmm(ICmm)=t r ( IΛmm)\\nVarΦmm(ECmm)=t r ( EΛmm) (12)\\nwhere Φmmis an eigenvector matrix of Cmm, andm=1,2,3.\\nIt should be noted that, in case of m=1,VarΦ11(IC11)and\\nVarΦ11(EC11)denote intra- and extrapersonal variations cal-\\nculated from a luminance component of the face vector, while\\nothers (m=2,3)are corresponding variations computed from\\ntwo different chromaticity components.\\nSubstituting (11) and (12) into (10), intra- and extraper-\\nsonal variations of the feature subspace spanned by Φcan be\\nrepresented as\\nVarΦ(IC)=3/summationdisplay\\nm=1VarΦmm(ICmm)\\nVarΦ(EC)=3/summationdisplay\\nm=1VarΦmm(ECmm). (13)\\nFrom (13), we can see that the variation of IC and EC is\\nequal to the summation of the variations of the respective\\ndiagonal submatrices of IC mmand EC mm, respectively. This\\nmeans that VarΦ(IC) and VarΦ(EC) are partially decom-\\nposed into three independent portions of VarΦmm(ICmm)and\\nVarΦmm(ECmm), where m=1,2,3. This confers an impor-\\ntant implication about the effect of color on the variation ratio in\\nthe color-based FR. Two different chromaticity components can\\nmake an independent contribution to construct the intra- and\\nextrapersonal variations in a separate manner with luminance.\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1221\\nAside from the independent contribution, since each spectral\\ncomponent of skin-tone color has its own inherent character-\\nistics [38], [46], [47], VarΦmm(ICmm)andVarΦmm(ECmm)\\nmay differently be changed by practical facial imaging con-\\nditions, e.g., illumination and spatial-resolution variations. Asa result, intra- and extrapersonal variations in the color-based\\nFR are formed by the composition of variations computed\\nfrom each spectral component along with different imagingconditions. On the contrary, in the traditional grayscale-based\\nsubspace FR, the distribution of the intra- and extrapersonal\\nvariations (denoted as Var\\nΦ11(IC11)andVarΦ11(EC11))i nt h e\\nfeature subspace spanned by Φ11is entirely governed by the\\nstatistical characteristic of only the luminance component.\\nC. Color Boosting Effect on Variation Ratio Along With\\nFace Resolution\\nNow, we make an analysis of the color effect on variation\\nratio with respect to changes in face resolutions. Our analysisis based on the following two observations: 1) As proven in\\nSection IV-B, each spectral component can contribute in an\\nindependent way to construct the intra- and extrapersonal vari-ations of the feature subspace in color-based FR; as described\\nin [54] and [58], such independent impact on evidence fusion\\nusually facilitates a complementary effect between different\\ncomponents for recognition purposes, and 2) the robustness of\\nthe color features against variation in terms of face resolution;previous research [20], [48], [49] revealed that chromatic con-\\ntrast sensitivity is mostly concentrated on low-spatial frequency\\nregions compared to luminance; this means that intrinsic fea-tures of face color are even less susceptible to a decrease\\nor variation of the spatial resolution. Considering these two\\nobservations, it is reasonable to infer that two chromaticitycomponents can play a supplement role in boosting the de-\\ncreased variation ratio caused by the loss in the discriminative\\npower of the luminance component arising from low-resolutionface images.\\nTo quantize the color boosting effect on variation ratio over\\nchanges in the face resolution, we will now derive a simplemetric, which is called variation ratio grain (VRG). Using\\n(6) and (12), the variation ratio, which is parameterized by\\nface resolution (γ), for an intensity-based feature subspace is\\ndeﬁned as\\nJ\\nlum(γ)=VarΦ11(γ)(EC11(γ))\\nVarΦ11(γ)(IC11(γ)). (14)\\nIt should be noted that all terms in (14) are obtained from\\na training set of intensity facial images having resolution γ.\\nOn the other hand, using (13), the variation ratio for a color-augmentation-based feature subspace is deﬁned as\\nJ\\nlum+chrom(γ)=VarΦ(γ)(EC(γ))\\nVarΦ(γ)(IC(γ))\\n=3/summationtext\\nm=1VarΦmm(γ)(ECmm(γ))\\n3/summationtext\\nm=1VarΦmm(γ)(ICmm(γ)).(15)\\nFig. 2. Average variation ratios and the corresponding standard deviations\\nwith respect to six different face-resolution parameters γ. Note that the margin\\nbetween curves of Jlum(γ)andJlum+chrom(γ)represents the numerator of\\nVRG deﬁned as in (16).\\nFinally, a VRG having input argument γis deﬁned as\\nVR G(γ)=Jlum+chrom(γ)−Jlum(γ)\\nJlum(γ)×100. (16)\\nVR G(γ)measures the relative amount of variation ratio in-\\ncreased by chromaticity components compared to that from\\nonly luminance at face resolution γ. Therefore, it reﬂects well\\nthe degree of the effect of color information on the improved\\nrecognition performance with respect to changes in γ.\\nTo validate the effectiveness of VRG as a relevant metric\\nfor the purpose of quantization of the color effect along with\\nvariations in face resolutions, we conducted an experiment\\nusing three standard color face DBs of CMU PIE, FERET,and XM2VTSDB. A total of 5000 facial images were collected\\nfrom three data sets and were manually cropped using the eye\\nposition provided by ground truth. Each cropped facial image\\nwas ﬁrst rescaled to a relatively high resolution of 112 ×\\n112 pixels. To simulate the effect of lowering the face resolutionfrom different distances to the camera, the 5000 facial images\\nwith 112 ×112 pixels were ﬁrst blurred and then subsequently\\ndownsampled by ﬁve different factors to produce ﬁve differentlower-resolution facial images [18]. For blurring, we used a\\npoint spread function, which was set to a 5 ×5 normalized\\nGaussian kernel with zero mean and a standard deviation ofone pixel. After the blurring and downsampling processing,\\nwe obtained six sets, each of which consisted of 5000 facial\\nimages with six different face resolutions: 112 ×112, 86 ×86,\\n44×44, 25 ×25, 20 ×20, and 15 ×15 pixels (see Fig. 2).\\nWe calculated J\\nlum(γ)andJlum+chrom(γ)in (16) over six dif-\\nferent face resolution γparameters. For this, 500 facial images\\nwere randomly selected from each set and then used to compute\\nvariation ratios by using (14) and (15). The selection process\\nwas repeated 20 times so that the variation ratios computed herewere the averages of 20 random selections. For luminance and\\nchromaticity components, the YC\\nbCrcolor space was adopted\\nsince it has been widely used in image (JPEG) and video\\n(MPEG) compression standards.\\n1222 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nExperimental results are shown in Fig. 2. In Fig. 2, Jlum(γ)\\ndenotes the average variation ratio calculated from luminance\\nface images with resolution γ, i.e., the Yplane from the YCbCr\\ncolor space. Furthermore, Jlum+chrom(γ)denotes the average\\nvariation ratio computed from YCbCrcomponent conﬁgura-\\ntion samples. To guarantee the stability of measured variation\\nratios, the standard deviations for all cases of Jlum(γ)and\\nJlum+chrom(γ)are shown in Fig. 2 as well. As can be seen in\\nFig. 2, at a high resolution (above 44 ×44 pixels), the margin\\nbetween Jlum(γ)andJlum+chrom(γ)is relatively small. This is\\nbecause the luminance component is even more dominant thantwo chromaticity components in determining J\\nlum+chrom(γ).\\nHowever, we can observe that Jlum(γ)noticeably falls off at\\nlow resolution (25 ×25 pixels or less) compared to those com-\\nputed from high-resolution faces (above 44 ×44 pixels). On the\\nother hand, Jlum+chrom(γ)has a slower decay compared with\\nJlum(γ)even as the face resolution becomes much lower. In\\nparticular, when the face resolution γis below 25 ×25 pixels,\\nthe difference between Jlum(γ)andJlum+chrom(γ)is much\\nlarger compared to cases of face resolutions above 44 ×\\n44 pixels. This result is mostly due to the fact that lumi-\\nnance contrast sensitivity drops off at low spatial frequenciesmuch faster than chromatic contrast sensitivity. Hence, two\\nchromaticity components in (15) can compensate a decreased\\nextrapersonal variation caused by luminance faces with lowresolution.\\nV. E\\nXPERIMENTS\\nIn the practical FR systems, there are two possible FR\\napproaches to perform FR tasks over lower-resolution probeimages [13]. The ﬁrst method is to prepare multiple training\\nsets of multiresolution facial images and then construct multiple\\nfeature subspaces, each of which is charged with a particularface resolution of a probe. An alternative method is that a\\nlower-resolution probe is reconstructed to be matched with the\\nprototype resolution of training and gallery facial images byadopting resolution-enhancement or interpolation techniques.\\nThe second method would be appropriate in typical surveillance\\nFR applications in which high-quality training and galleryimages are usually employed, but probe images transmitted\\nfrom surveillance cameras (e.g., CCTV) are often at a low\\nresolution. To demonstrate the effect of color on low-resolutionfaces in both FR scenarios, two sets of experiments have been\\ncarried out in our experimentation. The ﬁrst experiment is to\\nassess the impact of color on recognition performance with\\nvarying face resolutions of probe-given multiresolution trained\\nfeature subspaces. On the other hand, the second experimentis to conduct the same assessment when a single-resolution\\nfeature subspace trained with high-resolution facial images is\\nonly available to the actual testing operation.\\nA. Face DB for the Experiment and FR Evaluation Protocol\\nThree de facto standard data sets of CMU PIE, Color FERET,\\nand XM2VTSDB have been used to perform the experiments.\\nThe CMU PIE [32] includes 41 368 color images of 68 sub-\\njects (21 samples/subject). Among them, 3805 images have\\nthe coordinate information of facial feature points. From these\\nFig. 3. (a) Examples of facial images from CMU PIE. These images have\\nillumination variations with “room lighting on” conditions. (b) Examples of\\nfacial images from FERET. The ﬁrst and second rows show image examples offaandfbsets. (c) Examples of facial images from XM2VTSDB. Note that the\\nfacial images in each column belong to the same subject, and all facial images\\nare manually cropped using eye coordinate information. Each cropped facial\\nimage is rescaled to the size of 112 ×112 pixels.\\n3805 images, 1428 frontal-view facial images with neutral ex-\\npression and illumination variations were selected in our exper-\\nimentation. For one subject, 21 facial images have 21 differentillumination variations with “room lighting on” conditions. The\\nColor FERET [33] consists of 11 388 facial images correspond-\\ning to 994 subjects. Since the facial images are captured overthe course of 15 sessions, there are pose, expression, illumina-\\ntion, and resolution variations for one subject. To support the\\nevaluation of recognition performance in various FR scenarios,the Color FERET is to be divided into ﬁve sets: “ fa,” “fb,” “fc,”\\n“dup1 ,” and “ dup2” partitions [33]. The XM2VTSDB [34] is\\ndesigned to test realistic and challenging FR with four sessionsrecorded with no control on severe illumination variations. It\\nis composed of facial images taken on digital video recordings\\nfrom 295 subjects over a period of one month. Fig. 3 showsexamples of facial images selected from three DBs. All facial\\nimages shown in Fig. 3 were manually cropped from original\\nimages using the eye position provided by a ground truth set.\\nTo construct the training and probe (or test) sets in both sets\\nof experiments, a total of 3192 facial images from 341 subjects\\nwere collected from three public data sets. During the collection\\nphase, 1428 frontal-view images of 68 subjects were selected\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1223\\nFig. 4. Examples of facial images from color FERET according to six different face resolutions. A low-resolution observation below the original 112 ×112\\npixels is interpolated using nearest-neighbor interpolation.\\nfrom CMU PIE; for one subject, facial images had 21 different\\nlighting variations. From the Color FERET, the 700 frontal-view images of 140 subjects (5 samples/subject) were cho-\\nsen from the fa,fb,fc, and dup1 sets. From XM2VTSDB,\\n1064 frontal-view images of 133 subjects were obtained fromtwo different sessions; each subject included eight facial images\\nthat contained illumination and resolution variations. Further-\\nmore, we constructed a gallery set composed of 341 differentsamples corresponding to 341 different subjects to be identiﬁed\\nor veriﬁed. Note that, here, gallery images had neutral illumi-\\nnation and expression according to the standard regulation forgallery registration described in [59].\\nTo acquire facial images with varying face resolutions, we\\ncarried out resizing over the original collected DB sets. Fig. 4shows examples of facial images containing face-resolution\\nvariations used in our experiments. We took original high-\\nresolution images of faces (shown in the leftmost image of\\nFig. 4), synthetically blurred them with a Gaussian kernel\\n[41], and then downsampled them so as to simulate a lowerresolution effect as closely as possible to practical camera\\nlens. As a result, six different face resolutions of 112 ×112,\\n86×86, 44 ×44, 25 ×25, 20 ×20, and 15 ×15 (pixels)\\nwere generated to cover face resolutions that are commonly\\nencountered from practical still-image- to video-based FR\\napplications previously reported in [14]–[16], and [18].\\nTable I shows the grayscale features, different kinds of color\\nspaces and chromatic features, and spectral component conﬁg-\\nurations used for our experiments. As shown in Table I, for thegrayscale face features, the “R” channel from the RGB color\\nspace and the grayscale conversion method proposed in [56]\\nwere adopted in our experiments. The R channel of skin-tonecolor is known to be the best monochrome channel for FR [28],\\n[29]. Moreover, in [56], the 0.85·R+0.10·G+0.05·Bis\\nreported to be an optimal grayscale conversion method for facedetection. For the spectral component conﬁguration features,\\ntheYC\\nbCr,YIQ , andL∗a∗b∗color spaces were used in our\\nexperimentation. The YIQ color space deﬁned in the National\\nTelevision System Committee video standard was adopted. The\\nYCbCrcolor space is scaled and is the offset version of the\\nYUV color space [57]. Moreover, the L∗a∗b∗color space\\ndeﬁned in the CIE perceptually uniform color space was used.\\nThe detailed description of the used color spaces is given in[57]. As described in [57], the YC\\nbCrand YIQ color spaces\\nseparate RGB into “luminance” (e.g., Yfrom the YC bCrcolor\\nspace) and “chrominance” (or chromaticity) information (e.g.,C\\nborCrfrom the YCbCrcolor space). In addition, since\\ntheL∗a∗b∗color space is based on the CIE XYZ color space\\n[57], it is separated into “luminance” (L∗)and “chromaticity”\\n(a∗andb∗) components. To generate the spectral componentTABLE I\\nGRAYSCALE FEATURES AND DIFFERENT KINDS OF COLOR SPACES\\nAND SPECTRAL COMPONENT CONFIGURATIONS USED IN OUR\\nEXPERIMENTATION .NOTE THAT THE GRAYSCALE FEATURE IS\\nCOMBINED WITH THE CHROMATIC FEATURES TO GENERATE\\nTHE SPECTRAL COMPONENT CONFIGURATIONS\\nconﬁgurations depicted in Table I, two different chromaticity\\ncomponents from the used color spaces are combined with a\\nselected grayscale component.\\nFor FR experiments, all facial images were preprocessed\\naccording to the recommendation of the FERET protocol [33]\\nas follows: 1) Color facial images were rotated and scaledso that the centers of eye were placed on the speciﬁc pixels;\\n2) color facial images were rescaled into one of ﬁxed template\\nsize among six different spatial resolutions; 3) a standard\\nmask was applied to remove nonface portions; 4) each spectral\\ncomponent of color facial images was separately normalized tohave zero mean and unit standard deviations; 5) each spectral\\nimage was transformed to a corresponding column vector; and\\n6) each column vector was used to form a face vector deﬁnedin Section III, which covers both grayscale only and spectral\\ncomponent conﬁgurations shown in Table I.\\nTo show the stability of the signiﬁcance of color effect on\\nlow-resolution faces regardless of FR algorithms, three repre-\\nsentative FR methods, which are PCA, Fisher’s LDA (FLDA),\\nand Bayesian, were employed. In subspace FR methods, therecognition performance heavily relies on the number of linear\\nsubspace dimensions (feature dimension) [50]. Thus, the sub-\\nspace dimension was carefully chosen and then ﬁxed over sixdifferent face resolutions to make a fair comparison of perfor-\\nmance. For PCA, the PCA process in FLDA, and Bayesian,\\na well-known 95% energy capturing rule [50] was adoptedto determine subspace dimension. In these experiments, the\\nnumber of training samples was 1023 facial images so that\\nthe subspace dimension was experimentally determined as 200\\nto satisfy the 95% energy capturing rule. Mahalanobis [51],\\n1224 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nEuclidean distances, and “maximum a posteriori probability”\\nwere used for similarity metrics in PCA, FLDA, and Bayesian,\\nrespectively.\\nIn FR tasks, the recognition performance results can be\\nreported for identiﬁcation and veriﬁcation (VER). Identiﬁcationperformance is usually plotted on a cumulative match char-\\nacteristic (CMC) curve [33]. The horizontal axis of the CMC\\ncurves is the rank, while the vertical axis is the identiﬁcationrate. The best found correct recognition rate (BstCRR) [50]\\nwas adopted as the identiﬁcation rate for fair comparison.\\nFor the VER performance, the receiver operating characteristic(ROC) [52] curve is popular. The ROC curve plots the face\\nVER rate (FVR) versus the false accept rate (FAR). For an\\nexperimental protocol, the collected set of 3192 facial imageswas randomly partitioned into two sets: training and probe\\n(or test) sets. The training set consisted of (3 samples ×\\n341 subjects) facial images, with the remaining 2169 facialimages for the probe set. There was no overlapping between\\nthe two sets for an evaluation of the used FR algorithms’\\ngeneralization performance with regard to the color effect on\\nface resolution. To guarantee the reliability of the evaluation,\\n20 runs of random partitions were executed, and all of theexperimental results reported here were averaged over 20 runs.\\nB. Experiment 1: To Assess the Impact of Color in\\nMultiresolution Trained Feature Subspace FR Scenario\\nIn experiment 1, it should be noted that the face resolution of\\neach pair of training, gallery, and probe sets were all the same.\\nSince six different face resolutions were used, each feature\\nsubspace was trained with a respective set of facial imageswhose spatial resolution was one of six different kinds. We\\nperformed the comparative experiment to compare the recogni-\\ntion performances between the two different grayscale featuresdepicted in Table I. Our experimentation indicates that the R\\ngrayscale [28], [29] shows a better performance for most of\\nthe face resolutions, as shown in Fig. 4, in the PCA, FLDA,\\nand Bayesian methods. However, the performance difference\\nbetween the two grayscale conﬁgurations is marginal. Thus, Rwas selected as the grayscale feature of choice for the exper-\\niments aiming to the effect of color on low-resolution faces.\\nIn addition, “RQC\\nr” shows the best BstCRR performance of\\nall kinds of spectral component conﬁgurations represented in\\nTable I in all face resolutions and the three FR algorithms. This\\nresult is consistent with a previous one [26] that reported that“QC\\nr” is the best chromaticity component in the FR grand\\nchallenge DB and evaluation framework [33]. Hence, RQCr\\nwas chosen as a color feature in the following experiments.\\nFig. 5 shows the CMC curves for the identiﬁcation rate (or\\nBstCRR) comparisons between the grayscale and color features\\nwith respect to six different face resolutions in the PCA, FLDA,and Bayesian FR methods. As can be seen in CMC curves\\nobtained from the grayscale R feature (in the left side of Fig. 5),\\nthe differences in BstCRR between face resolutions of 112 ×\\n112, 86 ×86, and 44 ×44 pixels are relatively marginal in\\nall three FR methods. However, the BstCRRs obtained from\\na low resolution of 25 ×25 pixels and below tend to be\\nsigniﬁcantly deteriorated in all three FR methods. For example,for PCA, FLDA, and Bayesian methods, the rank-one BstCRRs\\n(identiﬁcation rate of top response being correct) decline from\\n77.20%, 83.69%, and 82.46% to 56.03%, 37.29%, and 62.32%,\\nrespectively, as face resolution is reduced from 112 ×112 to\\n15×15 pixels.\\nIn case of CMC curves from the RQC\\nrcolor feature (on the\\nright side of Fig. 5), we can ﬁrst observe that color information\\nimproves the BstCRR compared with grayscale features overall face resolutions in all three FR algorithms. In particular, it is\\nevident that color features make a substantial enhancement of\\nthe identiﬁcation rate as face resolutions are 25 ×25 pixels\\nand below. In PCA, 56.03%, 59.81%, and 60.97% of rank-\\none BstCRRs for 15 ×15, 20 ×20, and 25 ×25 grayscale\\nfaces increase to 69.70%, 62.16%, and 75.14%, respectively,by incorporating color feature QC\\nr. In FLDA, the color feature\\nraises rank-one BstCRRs from 37.29%, 49.72%, and 56.48% to\\n62.16%, 74.64%, and 77.45% for 15 ×15, 20 ×20, and 25 ×\\n25 face resolutions, respectively. Furthermore, in Bayesian,\\nrank-one BstCRRs increase from 62.23%, 69.17%, and 71.05%\\nto 75.14%, 82.46%, and 84.07% for 15 ×15, 20 ×20, and\\n25×25 face resolutions, respectively.\\nTo demonstrate the color effect on the VER performance\\naccording to face-resolution variations, the ROC curves are\\nshown in Fig. 6. We followed the protocol of FRVT [52] to\\ncompute the FVR to the corresponding FAR ranging from 0.1%to 100%, and the z-score normalization [54] technique was\\nused. Similar to the identiﬁcation performance in Fig. 6, face\\ncolor information signiﬁcantly improves the VER performanceat low-resolution faces (25 ×25 pixels and below) compared\\nwith high-resolution ones. For example, when facial images\\nwith a high resolution of 112 ×112 pixels are applied to\\nPCA, FLDA, and Bayesian, 5.84%, 4.04%, and 2.18% VER\\nenhancements at a FAR of 0.1% are attained from the color\\nfeature in PCA, FLDA, and Bayesian methods, respectively. Onthe other hand, in case of a low resolution of 15 ×15 pixels,\\nthe color feature achieves 19.46%, 38.58%, 15.90% VER\\nimprovement at the same FAR for the respective method.\\nTable II shows the comparison results of VRGs deﬁned in\\n(16) with respect to six different face resolutions in PCA.TheVR G(γ)for each face resolution γhas been averaged\\nover 20 random selections of 1023 training samples generated\\nfrom 3192 collected facial images. The corresponding standarddeviation for each VR G(γ)is also given to guarantee the\\nstability of the VR G(γ)metric. From Table II, we can see\\nthatVR G(γ)computed from high-resolution facial images\\n(higher than 44 ×44 pixels) are relatively small compared with\\nthose from low-resolution images (25 ×25 pixels or lower).\\nThis result is largely attributed to the dominance of grayscaleinformation at high-resolution facial images to build intra-\\nand extrapersonal variations in the feature subspace, so that\\nthe contribution of color is comparatively small. Meanwhile,in low-resolution color faces, VR G(γ)becomes much larger,\\nsince color information can boost the decreased extrapersonal\\nvariation, thanks to its resolution-invariant contrast characteris-tic and independent impact on constructing variations of feature\\nsubspace [20]. The results in Table II verify that face color\\nfeatures play a supplement role in maintaining an extrapersonal\\nvariation of feature subspace against face-resolution reduction.\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1225\\nFig. 5. Identiﬁcation rate (or BstCRR) comparison between grayscale and color features with respect to six different face resolutions of each pair of training,\\ngallery, and probe facial images in the three FR methods. The graphs on the left side resulted from grayscale feature R, while those on the right side wer e generated\\nfrom color feature RQCrfor each face resolution. (a) PCA. (b) FLDA. (c) Bayesian.\\nC. Experiment 2: To Assess the Impact of Color in a\\nSingle-Resolution Trained Feature Subspace FR Scenario\\nIn the practical subspace-based FR applications with face-\\nresolution constraints (e.g., video surveillance), a single fea-ture subspace is usually provided to perform identiﬁcation or\\nVER tasks on probes. It is reasonable to assume that the fea-\\nture subspace is pretrained with relatively high-resolution faceimages [13]. On the other hand, the probes to be tested may\\nhave lower and various face resolutions due to heterogeneous\\nacquisition conditions. Therefore, the objective of Experiment 2is to evaluate the color effect on recognition performance in the\\nFR scenario where high-resolution training images are used to\\nconstruct a single feature subspace, while probe images havevarious face resolutions. In Experiment 2, the face resolution\\nof training images was ﬁxed as 112 ×112 pixels, while the\\nresolution of probe was varied as six different resolutions, as\\nshown in Fig. 4. Since the high-quality gallery images areusually preregistered in FR systems before testing probes [33],\\nwe assume that the resolution of gallery is the same as thetraining facial images, i.e., 112 ×112 pixels. In Experiment 2,\\nR from the RGB color space was used as a grayscale feature.\\nDue to the best performance from Experiment 1, RQC\\nrwas\\nadopted as a color feature.\\nFig. 7 shows the CMC curves with respect to six different\\nprobe resolutions in both cases of grayscale (in the left side) and\\ncolor features (in the right side) in PCA, FLDA, and Bayesian.\\nTo obtain a low-dimensional feature representation for a lowerface-resolution probe, the probe has been upsampled to have the\\nsame resolution of training faces by using a cubic interpolation\\ntechnique in Fig. 7. From Fig. 7, in case of a grayscale feature,we can see a considerable identiﬁcation rate degradation in all\\nthree FR methods, considering low-resolution (25 ×25 pixels\\nand below) probes compared with relatively high-resolutioncounterparts (above 44 ×44 pixels). In particular, similar to the\\n1226 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nFig. 6. FVR comparison at FAR ranging from 0.1% to 100% between grayscale and color features with respect to six different face resolutions in the three\\nFR algorithms. The graphs on the left side came from grayscale feature R, while those on the right side were obtained from color feature RQCrfor each face\\nresolution. Note that the z-score normalization technique was used to compute FVR and FAR. (a) PCA. (b) FLDA. (c) Bayesian.\\nTABLE II\\nCOMPARATIVE EVA L UAT I O N O F VRGs D EFINED IN (16) W ITHRESPECT TO SIXDIFFERENT FACE RESOLUTIONS OF TRAINING\\nIMAGES IN PCA. G RAYSCALE AND COLOR FEATURES USED FOR COMPUTATION OF VRGs A RERAND RQCr\\n(SEETABLE I), R ESPECTIVELY .NOTE THAT THE UNIT OF VRGs I SPERCENT\\nresults from Experiment 1, the identiﬁcation rate resulting from\\nFLDA is signiﬁcantly deteriorated at low-resolution probes.The margins of a rank-one identiﬁcation rate between 112 ×\\n112 and each 25 ×25, 20 ×20, and 15 ×15 pixel grayscale\\nprobe in FLDA are 25.66%, 43.77%, and 62.41%, respectively.In case of a color feature, the BstCRR improvement is made\\nat all probe face resolutions in all three FR algorithms. As\\nexpected, face color information greatly improves the identiﬁ-cation performance obtained from low-resolution probes (25 ×\\n25 pixels and below) compared with grayscale feature. In PCA,by incorporating a color feature, the BstCRR margins between\\nagrayscale probe of the 112 ×112 resolution and a color\\nprobe of the 25 ×25, 20 ×20, and 15 ×15 resolutions are\\nreduced to 3.33%, 4.77%, and 8.02%, respectively. In FLDA,\\nthese differences are decreased to 6.65%, 7.28%, and 11.60%\\nat 25 ×25, 20 ×20, and 15 ×15 resolutions, respectively.\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1227\\nFig. 7. Identiﬁcation rate comparison between grayscale and color features with respect to six different face resolutions of probe images. The graph s on the left\\nside resulted from R as a grayscale feature from the RGB color space, while those on the right side were generated from RQCras a color feature for each face\\nresolution. Note that a single feature subspace trained with face images having a resolution of 112 ×112 pixels was given to test probe images with varying face\\nresolutions. (a) PCA. (b) FLDA. (c) Bayesian.\\nIn addition, in Bayesian, 1.47%, 2.61%, and 5.64% perfor-\\nmance margin decreases are achieved with the aforementioned\\nthree different probe resolutions, thanks to the color feature.\\nTable III presents the FVRs at a FAR of 0.1% obtained from\\nthe R grayscale and RQCrcolor features with respect to six dif-\\nferent face resolutions of probes in three FR methods. Similar\\nto the identiﬁcation rates shown in Fig. 7, the color feature hasa great impact on the FVR improvement at low-resolution faces\\n(25×25 pixels and below) in all three FR algorithms. In case\\nof 15 ×15 probe resolutions in PCA, FLDA, and Bayesian, the\\ncolor feature makes FVR improvements of 15.67%, 54.05%,\\nand 15.62% at a FAR of 0.1%, respectively, in comparison with\\ncorresponding FVRs from grayscale probes.\\nVI. D\\nISCUSSION AND CONCLUSION\\nAccording to the results from Experiments 1 and 2, there\\nwas a commonly harsh drop-off of identiﬁcation and VER ratescaused by a low-resolution grayscale image (25 ×25 pixels\\nor less) in PCA, FLDA, and Bayesian methods. Considering\\nthe performance sensitivity depending on variations in face\\nresolution, FLDA is found to be the weakest to low-resolutiongrayscale faces (25 ×25 pixels and below) of all three methods.\\nAs shown in the CMC curves on the left side of Figs. 5(b)\\nand 7(b), the margins of identiﬁcation rates between 112 ×\\n112 and 15 ×15 pixels were even 46.40% and 62.41%,\\nrespectively. The underlying reason behind such weakness is\\nthat optimal criteria used to form the feature subspace in FLDAtakes strategy with emphasis on the extrapersonal variation by\\nattempting to maximize it. Therefore, the recognition perfor-\\nmance in FLDA is even more sensitive to the portion of ex-\\ntrapersonal variation in the feature subspace compared with the\\nother two methods. Since grayscale features from much-lower-resolution images have a difﬁculty in providing a sufﬁcient\\namount of extrapersonal variation to the construction of the fea-\\nture subspace, the recognition performance could signiﬁcantly\\n1228 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\nTABLE III\\nFVR C OMPARISONS AT A FAR OF0.1% B ETWEEN GRAYSCALE AND COLOR FEATURES WITHRESPECT TO SIXDIFFERENT FACE RESOLUTIONS OF\\nPROBE IMAGES IN THE THREE FR A LGORITHMS .RF ROM THE RGB C OLOR SPACE WASUSED AS A GRAYSCALE FEATURE ,WHILE THE RQCr\\nCONFIGURATION WASEMPLOYED AS A COLOR FEATURE .NOTE THAT THE z-SCORE NORMALIZATION WASUSED TO COMPUTE FVR V ERSUS FAR\\nbe decreased. On the contrary, thanks to the color’s boosting\\ncharacteristic of the extrapersonal variation, color features in\\nFLDA outperformed by 24.86% and 50.81% margins in case\\nof 15 ×15 pixels, compared with corresponding grayscale\\nimages, as shown in Figs. 5(b) and 7(b), respectively. Asanother interesting ﬁnding, Bayesian is more robust to face-\\nresolution variations than PCA and FLDA. For example, from\\nthe CMC curves in the left side of Fig. 7, the performancedifference between 112 ×112 and 25 ×25 pixels was not so\\neven with 8.54% compared with 15.40% and 25.66% obtained\\nfrom PCA and FLDA, respectively. A plausible reason undersuch robustness lies in the fact that Bayesian depends more\\non the statistical distribution of the intrapersonal variation\\nrather than the extrapersonal variation [30], [37] so that therecognition performance is less likely affected by the reduc-\\ntion of the extrapersonal variation caused by low-resolution\\nimages.\\nTraditionally, low-resolution FR modules have extensively\\nbeen used in video-surveillance-like applications. Recently, FR\\napplications in the web environment are getting increasing\\nattention due to the popularity of online social networks (e.g.,\\nMyspace and Facebook) and their high commercialization po-tentials [4]–[7]. Under a web-based FR paradigm, many devices\\nsuch as cellular phone cameras and web cameras often produce\\nlow-resolution or low-quality face images which, however,can be used for recognition purposes [4], [5]. As shown in\\nour experimentation, color-based FR outperforms grayscale-\\nbased FR over all face resolutions. In particular, thanks tocolor information, both identiﬁcation and VER rates obtained\\nby using low-resolution 25 ×25 or 20 ×20 templates are\\ncomparable to rates obtained by using much larger grayscaleimages such as 86 ×86 pixels. Moreover, as shown in Fig. 3,\\nthe face DB, which is used in our experimentation, contains\\nimages obtained under varying illumination conditions. Hence,the robustness of color in low-resolution FR appears to be stable\\nwith respect to the variation in illumination, at least, in our\\nexperimentation. These results demonstrate that facial color can\\nreliably and effectively be utilized in real-world FR systems\\nof practical interest, such as video surveillance and promisingweb applications, which frequently have to deal with low-\\nresolution face images taken under uncontrolled illumination\\nconditions.A\\nPPENDIX\\nLetIΦmmandIΛmmbe eigenvector and corresponding di-\\nagonal eigenvalue matrices of IC mmin (9), where m=1,2,3.\\nThat is\\nIΦT\\nmmICmmIΦmm=IΛmm. (A.1)\\nUsing IΦmm(m=1,2,3), we deﬁne a block diagonal matrix\\nQgiven by\\nQ= diag( IΦ11,IΦ22,IΦ33). (A.2)\\nNote that Qis an orthogonal matrix. Using (8) and (A.2), we\\nnow deﬁne matrix IS as\\nIS=QTIC Q\\n=⎡\\n⎣IΛ11 IΦT\\n11IC12IΦ22IΦT\\n11IC13IΦ33\\nIΦT\\n22IC21IΦ11 IΛ22 IΦT\\n22IC23IΦ33\\nIΦT\\n33IC31IΦ11IΦT\\n33IC32IΦ22 IΛ33⎤\\n⎦.\\n(A.3)\\nIS in (A.3) is similar to IC since there exists an invertible matrix\\nQsatisfying IS =Q−1ICQ=QTICQ, where Q−1=QT.D u e\\nto their similarity , IS and IC have the same eigenvalues and\\ntrace value, so that tr(IS)=t r ( IC). Note that tr(IΛmm)is the\\nsum of all the eigenvalues of IC mm.U s i n g tr(IS)=t r ( IC),\\ntr(IC)can be expressed as\\ntr(IC)=3/summationdisplay\\nm=1tr(IΛmm). (A.4)\\nA similar derivation to (A.1)–(A.3) is also readily applied to EC\\nshown in (8). That is, tr(EC)can represented as\\ntr(EC)=3/summationdisplay\\nm=1tr(EΛmm) (A.5)\\nwhere EΛmm(m=1,2,3)is a diagonal eigenvalue matrix of\\nECmm.\\nCHOI et al. : COLOR FACE RECOGNITION FOR DEGRADED FACE IMAGES 1229\\nACKNOWLEDGMENT\\nThe authors would like to thank the anonymous reviewers\\nfor their constructive comments and suggestions. The authors\\nwould also like to thank the FERET Technical Agent, the U.S.\\nNational Institute of Standards and Technology (NIST) forproviding the FERET database.\\nR\\nEFERENCES\\n[1] R. Chellappa, C. L. Wilson, and S. Sirohey, “Human and machine\\nrecognition of faces: A survey,” in Proc. IEEE , May 1995, vol. 83,\\npp. 705–740.\\n[2] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, “Face recognition:\\nA literature survey,” ACM Comput. Surv. , vol. 35, no. 4, pp. 399–458,\\nDec. 2003.\\n[3] K. W. Bowyer, “Face recognition technology: Security versus privacy,”\\nIEEE Technol. Soc. Mag. , vol. 23, no. 1, pp. 9–19, Jun. 2004.\\n[4] Z. Zhu, S. C. H. Hoi, and M. R. Lyu, “Face annotation using transduc-\\ntive kernel ﬁsher discriminant,” IEEE Trans. Multimedia , vol. 10, no. 1,\\npp. 86–96, Jan. 2008.\\n[5] L. Chen, B. Hu, L. Zhang, M. Li, and H. J. Zhang, “Face annotation for\\nfamily photo album management,” Int. J. Image Graph. , vol. 3, no. 1,\\npp. 1–14, 2003.\\n[6] S. Satoh, Y . Nakamura, and T. Kanade, “Name-it: Naming and detecting\\nfaces in news videos,” IEEE Trans. Multimedia , vol. 6, no. 1, pp. 22–35,\\nJan.–Mar. 1999.\\n[7] J. Y . Choi, S. Yang, Y . M. Ro, and K. N. Plataniotis, “Face annotation for\\npersonal photos using context-assisted face recognition,” in Proc. ACM\\nInt. Conf. MIR , 2008, pp. 44–51.\\n[8] Z. Wangmeng, D. Zhang, Y . Jian, and W. Kuanquan, “BDPCA plus\\nLDA: A novel fast feature extraction technique for face recognition,”\\nIEEE Trans. Syst., Man, Cybern. B, Cybern. , vol. 36, no. 4, pp. 946–953,\\nAug. 2006.\\n[9] Q. Li, J. Ye, and C. Kambhamettu, “Linear projection methods in face\\nrecognition under unconstrained illumination: A comparative study,” in\\nProc. IEEE Int. Conf. CVPR , 2004, pp. II-474–II-481.\\n[10] R. Singh, M. Vatsa, A. Ross, and A. Noore, “A mosaicing scheme for\\npose-invariant face recognition,” IEEE Trans. Syst., Man, Cybern. B,\\nCybern. , vol. 37, no. 5, pp. 1212–1225, Oct. 2007.\\n[11] J. H. Lim and J. S. Jin, “Semantic indexing and retrieval of home photos,”\\ninProc. IEEE Int. Conf. ICARCV , 2007, pp. 186–191.\\n[12] D. M. Blackburn, J. M. Bone, and P. J. Phillips, “Face recognition ven-\\ndor test 2000: Evaluation report,” Defense Adv. Res. Projects Agency,\\nArlington, V A, 2001.\\n[13] J. Y . Choi, Y . M. Ro, and K. N. Plataniotis, “Feature subspace determi-\\nnation in video-based mismatched face recognition,” in Proc. IEEE Int.\\nConf. AFGR , 2008, pp. 14–20.\\n[14] H. K. Ekenel and A. Pnevmatikakis, “Video-based face recognition evalu-\\nation in the CHIL project—Run1,” in Proc. IEEE Int. Conf. AFGR , 2006,\\npp. 85–90.\\n[15] B. J. Boom, G. M. Beumer, L. J. Spreeuwers, and R. N. J. Veldhuis,\\n“The effect of image resolution on the performance of a face recognitionsystem,” in Proc. IEEE. Int. Conf. CARV , 2006, pp. 1–6.\\n[16] A. Hadid and M. Pietikainen, “From still image to video-based face\\nrecognition: An experimental analysis,” in Proc. IEEE Int. Conf. AFGR ,\\n2004, pp. 813–818.\\n[17] L. Tian, “Evaluation of face resolution for expression analysis,” in Proc.\\nIEEE Int. Conf. CVPR , 2004, p. 82.\\n[18] B. K. Gunturk, A. U. Batur, Y . Altunbasak, M. H. Hayes, III, and\\nR. M. Mersereau, “Eigenface-domain super-resolution for face recogni-tion,” IEEE Trans. Image Process. , vol. 12, no. 5, pp. 597–606, May 2003.\\n[19] [Online]. Available: http://www.ﬂickr.com\\n[20] L. H. Wurm, G. E. Legge, L. M. Isenberg, and A. Lubeker, “Color\\nimproves object recognition in normal and low vision,” J. Exp. Psychol.\\nHum. Percept. Perform. , vol. 19, no. 4, pp. 899–911, Aug. 1993.\\n[21] A. Yip and P. Sinha, “Role of color in face recognition,” J. Vis. ,v o l .2 ,\\nno. 7, p. 596, 2002.\\n[22] L. Torres, J. Y . Reutter, and L. Lorente, “The importance of the color\\ninformation in face recognition,” in Proc. IEEE Int. Conf. ICIP , 1999,\\npp. 627–631.\\n[23] M. Rajapakse, J. Tan, and J. Rajapakse, “Color channel encoding with\\nNMF for face recognition,” in Proc. IEEE Int. Conf. ICIP , 2004, vol. 3,\\npp. 2007–2010.[24] C. F. Jones, III and A. L. Abbott, “Optimization of color conversion for\\nface recognition,” EURASIP J. Appl. Signal Process. , vol. 2004, no. 4,\\npp. 522–529, 2004.\\n[25] P. Shih and C. Liu, “Comparative assessment of content-based face image\\nretrieval in different color spaces,” Int. J. Pattern Recogn. Artif. Intell. ,\\nvol. 19, no. 7, pp. 873–893, 2005.\\n[26] P. Shih and C. Liu, “Improving the face recognition grand challenge\\nbaseline performance using color conﬁgurations across color spaces,” in\\nProc. IEEE Int. Conf. Image Process. , 2006, pp. 1001–1004.\\n[27] B. Karimi, “Comparative analysis of face recognition algorithms and\\ninvestigation on the signiﬁcance of color,” M.S. thesis, Concordia Univ.,Montreal, QC, Canada, 2006.\\n[28] M. T. Sadeghi, S. Khoushrou, and J. Kittler, “Conﬁdence based gating\\nof colour features for face authentication,” in Proc. Int. Workshop MCS ,\\n2007, vol. 4472, pp. 121–130.\\n[29] J. Wang and C. Liu, “A general discriminant model for color face recog-\\nnition,” in Proc. IEEE Int. Conf. ICCV , 2007, pp. 1–6.\\n[30] B. Moghaddam, “Principal manifolds and probabilistic subspaces for vi-\\nsual recognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 6,\\npp. 780–788, Jun. 2002.\\n[31] J. Lu, K. N. Plataniotis, A. N. Venetsanopoulos, and S. Z. Li, “Ensemble-\\nbased discriminant learning with boosting for face recognition,” IEEE\\nTrans. Neural Netw. , vol. 17, no. 1, pp. 166–178, Jan. 2006.\\n[32] T. Sim, S. Baker, and M. Bsat, “The CMU pose, illumination, and expres-\\nsion database,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25, no. 12,\\npp. 1615–1618, Dec. 2003.\\n[33] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET evalu-\\nation methodology for face-recognition algorithms,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 22, no. 10, pp. 1090–1104, Oct. 2000.\\n[34] K. Messer, J. Mastas, J. Kittler, J. Luettin, and G. Maitre, “XM2VTSDB:\\nThe extended M2VTS database,” in Proc. IEEE Int. Conf. AVBPA , 1999,\\npp. 72–77.\\n[35] M. A. Turk and A. P. Pentland, “Eigenfaces for recognition,” J. Cogn.\\nNeurosci. , vol. 3, no. 1, pp. 71–86, 1991.\\n[36] P. N. Belhumeur, J. P. Hesphanha, and D. J. Kriegman, “Eigenfaces vs.\\nFisherfaces: Recognition using class speciﬁc linear projection,” IEEE\\nTrans. Pattern. Anal. Mach. Intell. , vol. 9, no. 7, pp. 711–720, Jul. 1997.\\n[37] B. Moghaddam, T. Jebara, and A. Pentland, “Bayesian face recognition,”\\nPattern Recognit. , vol. 33, no. 11, pp. 1771–1782, 2000.\\n[38] R. Hsu, M. A. Monttaleb, and A. Jain, “Face detection in color images,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 5, pp. 696–706,\\nMay 2002.\\n[39] A. J. Colmenarez and T. S. Huang, “Face detection and tracking of faces\\nand facial features,” in Proc. IEEE Int. Conf. CVPR , 1997, pp. 657–661.\\n[40] S. Hayashi and O. Hasegawa, “A detection technique for degraded face\\nimages,” in Proc. IEEE Int. Conf. CVPR , 2006, pp. 1506–1512.\\n[41] S. Baker and T. Kanade, “Limits on super-resolution and how to break\\nthem,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 9, pp. 1167–\\n1183, Sep. 2002.\\n[42] F. W. Wheeler, X. Liu, and P. H. Tu, “Multi-frame super-resolution for\\nface recognition,” in Proc. IEEE Int. Conf. BTAS , 2007, pp. 1–6.\\n[43] X. Wang and X. Tang, “A uniﬁed framework for subspace face recogni-\\ntion,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 26, no. 9, pp. 1222–\\n1228, Sep. 2004.\\n[44] J. Wang, K. N. Plataniotis, and A. N. Venetasanopoulos, “Selecting\\ndiscriminant eigenfaces for face recognition,” Pattern Recognit. Lett. ,\\nvol. 26, no. 10, pp. 1470–1482, Jul. 2005.\\n[45] J. Xiao-Yuan and D. Zhang, “A face and palmprint recognition approach\\nbased on discriminant DCT feature extraction,” IEEE Trans. Syst., Man,\\nCybern. B, Cybern. , vol. 34, no. 6, pp. 2405–2415, Dec. 2004.\\n[46] H. Stokman and T. Gevers, “Selection and fusion of color models for\\nimage feature detection,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 29,\\nno. 3, pp. 371–381, Mar. 2007.\\n[47] Y . Ohta, T. Kanade, and T. Sakai, “Color information for region segmen-\\ntation,” Comput. Graph. Image Process. , vol. 13, no. 3, pp. 222–241,\\nJul. 1980.\\n[48] D. H. Kelly, “Spatiotemporal variation of chromatic and achromatic\\ncontrast thresholds,” J. Opt. Soc. Amer. , vol. 73, no. 6, pp. 742–749,\\nJun. 1983.\\n[49] J. B. Derrico and G. Buchsbaum, “A computational model of spatiochro-\\nmatic image coding in early vision,” J. Vis. Commun. Image Represent. ,\\nvol. 2, no. 1, pp. 31–38, Mar. 1991.\\n[50] J. Wang, K. N. Plataniotis, J. Lu, and A. N. Venetsanopoulos, “On solv-\\ning the face recognition problem with one training sample per subject,”\\nPattern Recognit. , vol. 39, no. 6, pp. 1746–1762, Sep. 2006.\\n[51] V . Perlibakas, “Distance measures for PCA-based face recognition,”\\nPattern Recognit. Lett. , vol. 25, no. 12, pp. 1421–1430, Apr. 2004.\\n1230 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 39, NO. 5, OCTOBER 2009\\n[52] P. J. Grother, R. J. Micheals, and P. J. Phillips, “Face recognition vendor\\ntest 2002 performance metrics,” in Proc. Int. Conf. Audio- Video-Based\\nBiometric Person Authentication , 2003, vol. 2688, pp. 937–945.\\n[53] P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, C. Jin, K. Hoffman,\\nJ. Marques, M. Jaesik, and W. Worek, “Overview of the face recognition\\ngrand challenge,” in Proc. IEEE Int. Conf. CVPR , 2005, pp. 947–954.\\n[54] A. Jain, K. Nandakumar, and A. Ross, “Score normalization in multi-\\nmodal biometric systems,” Pattern Recognit. , vol. 38, no. 12, pp. 2270–\\n2285, Dec. 2005.\\n[55] L. P. Hansen, “Large sample properties of generalized method of moments\\nestimators,” Econometrica , vol. 50, no. 4, pp. 1029–1054, 1982.\\n[56] J. Lu, M. Thiyagarajah, and H. Zhou, “Converting a digital image from\\ncolor to gray-scale,” U.S. Patent 20 080 144 892, Jun. 19, 2008.\\n[57] R. Lukac and K. N. Plataniotis, Color Image Processing: Methods and\\nApplication . New York: CRC, 2007.\\n[58] A. K. Jain, A. Ross, and S. Prabhaker, “An introduction to biometric\\nrecognition,” IEEE Trans. Circuits Syst. Video Technol. , vol. 14, no. 1,\\npp. 4–20, Jan. 2004.\\n[59] Proposed Draft Amendment to ISO/IEC 19794-5 Face Image Data on\\nConditions for Taking Pictures , Mar. 1, 2006.\\nJae Young Choi received the B.S. degree from\\nKwangwoon University, Seoul, Korea, in 2004 andthe M.S. degree from the Korea Advanced Instituteof Science and Technology (KAIST), Daejeon, Ko-\\nrea, in 2008, where he is currently working toward\\nthe Ph.D. degree with the Image and Video SystemLaboratory.\\nHe was an Intern Researcher for the Electronic\\nTelecommunications Research Institute, Daejon, in2007. In 2008, he was a Visiting Student Researcherat the University of Toronto, Toronto, ON, Canada.\\nHis research interests include face recognition/detection, image/video indexing,\\npattern recognition, machine learning, MPEG-7, and personalized broadcastingtechnologies.\\nYong Man Ro (M’92–SM’98) received the B.S.\\ndegree from Yonsei University, Seoul, Korea, and the\\nM.S. and Ph.D. degrees from the Korea AdvancedInstitute in Science and Technology (KAIST),Daejon, Korea.\\nIn 1987, he was a Researcher with Columbia\\nUniversity, New York, NY , and from 1992 to 1995,he was a Visiting Researcher with the University ofCalifornia, Irvine, and with KAIST. In 1996, he was\\na Research Fellow with the University of California,\\nBerkeley. He is currently a Professor and the Director\\nof the Image and Video System Laboratory, Korea Advanced Institute of Sci-ence and Technology (KAIST), Daejeon. He participated in international stan-\\ndardizations including MPEG-7 and MPEG-21, where he contributed several\\nMPEG-7 and MPEG-21 standardization works, including the MPEG-7 texturedescriptor and MPEG-21 DIA visual impairment descriptors and modality\\nconversion. His research interests include image/video processing, multimedia\\nadaptation, visual data mining, image/video indexing, and multimedia security.\\nDr. Ro was the recipient of the Young Investigator Finalist Award of the\\nInternational Society for Magnetic Resonance in Medicine in 1992 and the\\nScientist Award (Korea), in 2003. He has served as a Technical Program Com-\\nmittee member for many international conferences, including the InternationalWorkshop on Digital Watermaking (IWDW), Workshop on Image Analysisfor Multimedia Interactive Services (WIAMI), Asia Information Retrieval\\nSymposium (AIRS), Consumer Communications and Networking Conference,\\netc., and as the Co-Program Chair of the 2004 IWDW.\\nKonstantinos N. (Kostas) Plataniotis (S’90–M’92–\\nSM’03) received the B.Eng. degree in computer\\nengineering from the University of Patras, Patras,Greece, in 1988 and the M.S. and Ph.D. degrees\\nin electrical engineering from the Florida Insti-\\ntute of Technology, Melbourne, in 1992 and 1994,respectively.\\nHe is currently a Professor with the Edward S.\\nRogers, Sr. Department of Electrical and Computer\\nEngineering, University of Toronto, Toronto, ON,Canada, where he is a member of the Knowledge\\nMedia Design Institute and the Director of Research for the Identity, Privacy,\\nand Security Initiative and is an Adjunct Professor with the School of Com-\\nputer Science, Ryerson University, Toronto. His research interests include bio-metrics, communications systems, multimedia systems, and signal and imageprocessing.\\nDr. Plataniotis is the Editor-in-Chief for the IEEE S\\nIGNAL PROCESSING\\nLETTERS for 2009–2011. He is a Registered Professional Engineer in the\\nprovince of Ontario and a member of the Technical Chamber of Greece. He\\nwas the 2005 recipient of IEEE Canada’s Outstanding Engineering Educator\\nAward “for contributions to engineering education and inspirational guidanceof graduate students” and is the corecipient of the 2006 IEEE T\\nRANSACTIONS\\nONNEURAL NETWORKS Outstanding Paper Award for the paper entitled “Face\\nRecognition Using Kernel Direct Discriminant Analysis Algorithms,” which\\nwas published in 2003.\\n',\n",
       " '1366 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nColor Local Texture Features for\\nColor Face Recognition\\nJae Young Choi, Yong Man Ro , Senior Member, IEEE , and Konstantinos N. Plataniotis , Senior Member, IEEE\\nAbstract— This paper proposes new color local texture features,\\ni.e., color local Gabor wavelets (CLGWs) and color local binarypattern (CLBP), for the purpose of face recognition (FR). Theproposed color local texture features are able to exploit the dis-criminative information derived from spatiochromatic texturepatterns of different spectral channels within a certain local faceregion. Furthermore, in order to maximize a complementary effecttaken by using both color and texture information, the opponent\\ncolor texture features that capture the texture patterns of spatial\\ninteractions between spectral channels are also incorporated intothe generation of CLGW and CLBP. In addition, to performthe ﬁnal classiﬁcation, multiple color local texture features (eachcorresponding to the associated color band) are combined withina feature-level fusion framework. Extensive and comparative ex-periments have been conducted to evaluate our color local texturefeatures for FR on ﬁve public face databases, i.e., CMU-PIE, ColorFERET, XM2VTSDB, SCface, and FRGC 2.0. Experimentalresults show that FR approaches using color local texture featuresimpressively yield better recognition rates than FR approachesusing only color or texture information. Particularly, comparedwith grayscale texture features, the proposed color local texturefeatures are able to provide excellent recognition rates for faceimages taken under severe variation in illumination, as well as forsmall- (low-) resolution face images. In addition, the feasibility\\nof our color local texture features has been successfully demon-\\nstrated by making comparisons with other state-of-the-art colorFR methods.\\nIndex Terms— Color face recognition (FR), color local texture\\nfeatures, color spaces, combination, Gabor wavelets, local binarypattern (LBP).\\nI. I NTRODUCTION\\nFACE recognition (FR) has received a signiﬁcant interest\\nin pattern recognition and computer vision due to the wide\\nrange of applications including video surveillance [1], biometricidentiﬁcation [2], and face indexing in multimedia contents [3].\\nAs in any classiﬁcation task, feature extraction is of great impor-\\ntance in the FR process. Recently, local texture features [4]–[6]\\nManuscript received October 14, 2010; revised January 24, 2011 and April\\n28, 2011; accepted May 16, 2011. Date of publication September 15, 2011; dateof current version February 17, 2012. This work was supported by Brain Korea\\n21 Project, BK Electronics and Communications Technology Division, KAIST\\nin 2011. The associate editor coordinating the review of this manuscript and\\napproving it for publication was Dr. Arun A. Ross.\\nJ. Y. Choi and Y. M. Ro are with the Image and Video Systems Lab, De-\\npartment of Electrical Engineering, Korea Advanced Institute of Science and\\nTechnology (KAIST), Daejeon 305-732, Korea (e-mail: jygchoi@kaist.ac.kr;\\nymro@ee.kaist.ac.kr).\\nK. N. Plataniotis is with the Multimedia Lab, Department of Electrical and\\nComputer Engineering, University of Toronto, ON M5S3G4, Canada (e-mail:\\nkostas@comm.utoronto.ca).\\nColor versions of one or more of the ﬁgures in this paper are available online\\nat http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TIP.2011.2168413have gained reputation as powerful face descriptors because\\nthey are believed to be more robust to variations of facial pose,\\nexpression, occlusion, etc. In particular, Gabor wavelets [7] and\\nlocal binary pattern (LBP) [8] texture features have proven to behighly discriminative for FR due to different levels of locality.\\nThere has been a limited but increasing amount of work on\\nthe color aspects of textured image analysis [9]–[11]. Results in\\nthese works indicate that color information can play a comple-mentary role in texture analysis and classiﬁcation/recognition,and consequently, it can be used to enhance classiﬁcation/recog-\\nnition performance. In [9], the authors performed an empirical\\nevaluation study that compares color indexing, grayscale tex-ture, and color texture methods for classiﬁcation tasks on textureimages data set taken under either constant (static) or varying\\nillumination conditions. Experimental result shows that, for the\\ncase of static illumination condition, color texture descriptorsgenerally perform better than their grayscale counterparts. In[10], three grayscale texture techniques including local linear\\ntransform, Gabor ﬁltering, and co-occurrence methods are ex-\\ntended to color images. This paper reports that the use of colorinformation can improve classiﬁcation performance obtainedusing only grayscale texture analysis techniques. In [11], incor-\\nporating color into a texture analysis can be beneﬁcial for classi-\\nﬁcation/recognition schemes. In particular, the authors showedthat perceptually uniform color spaces\\nand hue, satura-\\ntion, and value\\n perform better than red, green, and blue\\nfor color texture analysis.\\nFollowing the aforementioned studies, it is natural to expect\\nbetter FR performance by combining color and texture informa-tion than by using only color or texture information. However,\\nat the moment, how to effectively make use of both color and\\ntexture information for the purpose of FR still remains an openproblem. The aim of this paper is to suggest a new color FRframework, which effectively combines color and texture infor-\\nmation, aiming to improve FR performance. The main contri-\\nbution of our paper is threefold.\\n1) This paper proposes the ﬁrst so-called color local texture\\nfeatures. Speciﬁcally, we develop two effective color localtexture features, i.e., color local Gabor wavelets (CLGWs)\\nand color LBP (CLBP), both of which are able to encode\\nthe discriminative features derived from spatiochromatic\\ntexture patterns of different spectral channels (or bands)\\nwithin a certain local region. In addition, to make full use\\nof both color and texture information, the opponent color\\ntexture features that capture the texture patterns of spatialinteractions between spectral bands are incorporated into\\nthe generation of CLGW and CLBP. This allows for ac-\\nquiring more discriminative color local texture features, as\\n1057-7149/$26.00 © 2011 IEEE\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1367\\ncompared with conventional grayscale texture features, for\\nimproving FR performance.\\n2) The effective way of combining color local texture features\\nhas not been explored in the current FR works. This papersuggests the feature-level fusion approach in order to inte-grate multiple color local texture features [each extracted\\nfrom an associated color component (or spectral) image]\\nfor the ﬁnal classiﬁcation. As demonstrated by our exper-imental results, the feature-level fusion approach worksbetter than the decision-level (or score-level) fusion ap-\\nproach in terms of maximizing complementary effect that\\nlies on both color and texture information.\\n3) Comparative and extensive experiments have been\\nconducted to investigate the effectiveness of proposed\\ncolor local texture features. For this, ﬁve public face\\ndatabases (DB), i.e., CMU-PIE [12], Color FERET [13],XM2VTSDB [14], SCface [48], and FRGC 2.0 [15], areused. Our experimental results show that the FR approach\\nusing our color local texture features achieves even better\\nFR performance than the FR approach relying only oncolor or texture information. In particular, the importantresults exploited by our experimental study are that color\\nlocal texture features are highly effective for low-reso-\\nlution images and are robust against severe variation inillumination (caused by the interruption of backgroundcolored light and cast shadow), as compared with conven-\\ntional grayscale texture features. Furthermore, we clearly\\nvalidate the feasibility of our color local texture featuresby comparing the best results of our method with theresults of other state-of-the-art color FR methods.\\nThe rest of this paper is organized as follows: Section II re-\\nviews the existing FR methods using texture features and FRmethods using color information. In Section III, the proposedcolor FR framework using color local texture features is out-\\nlined. Section IV details the proposed color local texture feature\\nextraction approach. In Section V, we present our fusion ap-proach to combining multiple color local texture features to per-form FR. In Section VI, we present comparative experimental\\nresults that demonstrate the effectiveness of color local texture\\nfeatures. Conclusions constitute Section VII.\\nII. R\\nELATED WORK\\nSo far, numerous approaches using texture features have been\\ndeveloped and successfully applied to FR. These works roughlyfall into four categories: 1) methods using Gabor wavelets [7]or LBP features [8]; 2) methods using the fusion of global and\\nlocal face features [5]; 3) methods using the fusion of magnitude\\nand phase information of Gabor wavelets [6]; 4) methods usingfusion of Gabor and LBP [16]. However, most of these workshave been limited to grayscale texture analysis.\\nFR using color information is a relatively new research topic\\nin the area of automatic FR. Initial works on color FR focused ondetermining ﬁxed color-component conﬁgurations (from var-ious color spaces) suitable for FR through empirical compar-isons. In [17], the\\n,\\n , and\\n color spaces were\\nexamined in the context of FR. The results show that facialcolor contains complementary information and that the accu-\\nracy of FR is affected by the color space chosen. Shih and Liu[18] also studied the use of different color spaces and colorcomponent conﬁgurations for improving the FR performance.They showed that the\\n(luminance, chrominance blue,\\nand chrominance red),\\n (luminance, in-phase, and quadra-\\nture), and\\n color spaces are better suited for the\\npurpose of FR than several other color spaces. In [19] and [20],combining spectral components across different color spaces isfound to be useful for enhancing the FR accuracy. In particular,in [20], a new hybrid color space “\\n” is proposed, where\\nthe\\n channel is taken from the\\n color space and the\\n and\\nchannels are taken from the\\n and\\n color spaces,\\nrespectively. The authors show that the\\n color represen-\\ntation can achieve a better FR performance.\\nRecent works proposed the use of alternative grayscale or\\ncolor space conversions in order to further obtain an enhancedFR performance. In [21], the authors proposed an optimalconversion of color images in the\\ncolor space into a\\nmonochromatic form. They developed a color image discrimi-\\nnant model to ﬁnd a set of optimal combination coefﬁcients and\\ndemonstrated the usefulness of the proposed monochromaticrepresentation. Liu [22] proposed three new color repre-sentations, i.e., the so-called uncorrelated color space, theindependent color space, and the discriminating color space.The author shows that the later three-color representations areeffective for enhancing the FR performance, as compared withthe use of color images represented in the\\ncolor space. In\\n[23], the authors found out a common characteristic of a pow-erful color space for FR by analyzing the transformation matrixof the different color spaces from the\\ncolor space. In\\naddition, based on the characteristic of powerful color spaces,they proposed color space normalization techniques, which areable to convert weak color spaces into powerful ones, so thatbetter FR performance can be obtained by making use of these\\nnormalized color spaces.\\nMore recently, a little research effort has been dedicated to\\napply a multiple-feature encoding scheme to multiple and dif-ferent color-component images. In [45], the authors proposeda hybrid color-and-frequency-feature (CFF) method for colorFR. A new hybrid color space\\n, which combines the\\ncomponent image of the\\n color space and the chromatic\\ncomponents\\n and\\n of the\\n color space, is proposed. The\\nCFF method is devised to extract the complementary featuresfrom the real part, the imaginary part, and the magnitude of the\\n,\\n, and\\n color-component images, respectively, in the dis-\\ncrete-cosine-transform (DCT) frequency domain. Multiple fre-quency features each extracted from the corresponding compo-nent image are combined using a weighted sum rule for the\\npurpose of FR. In [24], the authors propose the FR method\\nthat fuses multiple global and local features derived from a hy-brid color space\\n. Speciﬁcally, three different image en-\\ncoding schemes are suggested, i.e., a patch-based Gabor imagerepresentation for the\\ncomponent image, a multiresolution\\nLBP feature fusion scheme for the\\n component image, and a\\ncomponent-based DCT multiple encoding for the\\n component\\nimage. This color FR method achieves considerably better FRperformance, as compared with the reported results on FRGCversion 2 Experiment 4 [15], by means of fusing three similarity\\n1368 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nFig. 1. Proposed color FR framework based on color local texture features. /89/67\\n /67\\nis used for an illustration of color space conversion.\\nmatrices (using a weighted sum rule) that are generated from\\nthree component images in the\\n color space. In [52], mul-\\ntispectral LBP histograms are extracted from the\\n color\\nface image and concatenated into a regional feature for FR.For three channels in the\\ncolor space, multispectral LBP\\nhistograms are computed using three monochrome and six op-ponent LBP operators. In [53], the authors suggest six multi-\\nscale color LBP operators for the visual-object-class recogni-\\ntion tasks. In particular, in this work, the opponent-LBP andnOpponent-LBP operators are proposed. The opponent-LBP isobtained by computing the LBP over all three channels (in the\\ncolor space) of the opponent color space, whereas the\\nnOpponent-LBP is generated by performing the LBP over twochannels of the normalized opponent color space.\\nAlthough previous works in color-based FR have success-\\nfully demonstrated the importance of color information in orderto obtain improved FR performance, a large part of color FR re-search [17]–[23], [45] has been limited to developing color FRmethods using only global-based feature extraction techniquessuch as DCT and principal component analysis (PCA). As such,the complementary effect taken by combining color and localtexture information on the FR problem (e.g., illumination or\\npose constraints) has not been systematically explored in the\\ncurrent color FR work. The aim of this paper is to ﬁll this blankby presenting the effective FR framework based on integratingcolor and local texture information in an effective way.\\nIII. F\\nRAMEWORK OF FR U SING COLOR LOCAL\\nTEXTURE FEATURES\\nAs shown in Fig. 1, the proposed color FR framework using\\ncolor local texture features consists of three major steps: colorspace conversion and partition, feature extraction, and combi-nation and classiﬁcation.\\nA face image represented in the\\ncolor space is ﬁrst\\ntranslated, rotated, and rescaled to a ﬁxed template [13],yielding the corresponding aligned face image. Subsequently,the aligned\\ncolor image is converted into an image repre-\\nsented in another color space. Note that not only conventionallinear or nonlinear color spaces (e.g.,\\n or\\n )b u t\\nalso new color spaces devised for the purpose of FR (e.g.,normalized color space proposed in [23]) can be used for colorspace conversion. Each of the color-component images ofcurrent color model is then partitioned into local regions assuggested by [4].\\nIn the next step, texture feature extraction is independently\\nand separately performed on each of these local regions. Since\\ntexture features are extracted from the local face regions ob-\\ntained from different color channels , they are referred to as\\n“color local texture features.” Note that the key to FR usingcolor information is to extract the so-called opponent texture\\nfeatures [25] between each pair of two spectral images, as wellas unichrome (or channelwise) texture features. This allows forobtaining much more complementary texture features for im-proving the FR performance, as compared with grayscale tex-ture feature extraction, where only the luminance of an image istaken into account.\\nSince\\ncolor local texture features (each obtained from the\\nassociated local region and spectral channel) are available, wehave to combine them to reach the ﬁnal classiﬁcation. To thisend, multimodal fusion techniques [26] are employed for inte-\\ngrating multiple color local texture features for improving the\\nFR performance. In the following subsections, the detailed ex-planation of color local texture feature extraction and combi-nation and classiﬁcation steps are provided (For further detailsregarding color space conversion and partition methods, refer to[27] and [4], respectively).\\nIV . E\\nXTRACTION OF COLOR LOCAL TEXTURE FEATURES\\nHere, we present the methods of extracting the proposed\\ncolor local texture features from a color image. Two commonlyused texture feature representations are considered, i.e., Gaborwavelet and LBP. Here, these grayscale texture features areextended to the multispectral texture features using color infor-mation. Speciﬁcally, given a color image, the texture operatoris applied on each separate color channel. In addition, we can\\nfurther extend the texture operator to make use of opponent\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1369\\nFig. 2. Illustration of the Gabor wavelet representations with ﬁve scales and eight orientations of sample local regions. Three local region images f or the same\\nfacial component (e.g., eye or nose) are separately obtained from three color channels in the /89/67\\n /67\\ncolor space. The text enclosed in brackets represents the values\\nof orientation /117and scale /118(which are used to obtain the below Gabor wavelet representations placed in the same column) in the following format: ( orientation ,\\nscale ).\\ncolors. The term “opponent color” here follows the deﬁnition\\nsuggested by [25], i.e., all pairs of different color channels arenamed “opponent colors.”\\nBefore describing the method of extracting the color local tex-\\nture features, we introduce notation commonly used throughoutthe following subsections. Let us assume that\\ndifferent kinds\\nof spectral images are generated from an\\n color face image\\nvia color space conversions prespeciﬁed. Then, let\\n be the\\nth spectral image (e.g., chrominance component “\\n ” from the\\ncolor space), where\\n . Moreover, assuming\\nthat\\n is divided into\\n local face regions (as described in\\nFig. 1), we denote its\\n th local region by\\n .\\nA. Extraction of Color Local Gabor Wavelets\\nGabor wavelets can be obtained based on Gabor ﬁlters [6]\\nthat detect amplitude-invariant spatial frequencies of pixel grayvalues. Gabor wavelet features have been widely adopted in FRdue to the robustness against illumination changes. The 2-DGabor ﬁlter can be deﬁned as follows [7]:\\n(1)\\nwhere\\n and\\n deﬁne the orientation and the scale of the Gabor\\nﬁlters,\\n ,\\n denotes the norm operator,\\n,\\n ,\\n ,\\n is the maximum\\nfrequency, and\\n is the spacing factor between ﬁlters in the fre-\\nquency domain [7]. Note that the Gabor ﬁlters in (1) can take avariety of different forms, along with different\\nscales and\\norientations.\\nTo reﬂect and encode the local properties of the\\n th spectral\\nimage\\n when computing its associated Gabor wavelet, the\\nfollowing operation can be performed [7]:\\n(2)where\\n denotes the convolution operator and\\n is a\\nGabor wavelet representation (with orientation\\n and scale\\n )\\nof the\\n th local region\\n of the\\n th spectral image. Hence,\\nset\\nforms a set of Gabor wavelet representations corresponding to\\nlocal regions of\\n . Fig. 2 shows the Gabor wavelet repre-\\nsentations (the magnitude) of sample local region images. Notethat, in Fig. 2, three local region images (one for each colorband) corresponding to the same facial component (e.g., eye ornose) differ in the pattern of Gabor wavelet representations. Thisindicates that they can provide different complementary infor-mation for the purpose of FR.\\nIn addition, in order to exploit the additional information\\ncontained between spectral bands, we now deﬁne an opponent\\nlocal Gabor representation. Let\\nand\\n be\\nthe Gabor wavelet representations (with a particular orienta-\\ntion\\n ) of the\\n th local regions\\n and\\n of two\\ndifferent spectral bands\\n and\\n , respectively. Using\\nand\\n , the opponent Gabor representation between\\nand\\n are then computed as follows:\\nfor all\\n (3)\\nwhere\\n (\\n can be calcu-\\nlated in a similar way) and\\n and\\n denote the different scales\\nof the Gabor ﬁlters used.\\nIn order to make an effective use of different complementary\\ninformation, we have to reserve locality information and to\\nencompass color texture information. To this end, all Gabor\\n1370 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nwavelet representations\\n and\\n with a\\nparticular\\n are concatenated, resulting in an augmented feature\\nvector named the CLGW feature. For concatenation, we form\\na column vector, which we write as\\n , from\\n by\\nstacking its rows (or columns).\\n can be calculated\\n(from\\n ) in a similar way.\\nThe CLGW feature for the\\n th spectral image\\n is then de-\\nﬁned as follows:\\nfor\\n (4)\\nwhere\\n is the transpose operator,\\n denotes\\nthe unichrome Gabor feature such that\\n, and\\n denotes\\nthe opponent Gabor feature deﬁned as\\n. Recall that\\n and\\n denote the number of orientations and\\nscales, respectively, and\\n is the number of local regions.\\nNote that\\n consists of a total of\\n unichrome local\\nGabor features\\n , whereas\\n is represented by a\\ntotal of\\n opponent local Gabor features\\nconditioned on\\n .\\nB. Extraction of Color LBP\\nGiven\\n different color-component images\\n, the unichrome (or channelwise) LBP feature is\\nseparately and independently computed from each\\n . Note\\nthat, in the computation of the unichrome LBP feature, theuniform LBP operator [8] is adopted because a typical faceimage contains only a small number of LBP values (called the\\nuniform pattern), as reported in [8]. Let us denote that\\nis the\\ncenter pixel position of\\n and\\n are\\nequally spaced pixels (or sampling points) on a circle of radius\\nthat form a circular neighborhood of the center pixel\\n. The unichrome LBP operation for the center pixel position\\nof\\n is then deﬁned as follows [8], [28]:\\nLBP\\nif\\notherwise(5)\\nwhere\\n(6)\\nand\\n ,\\n denotes the\\npixel values of\\n at\\n , and\\n denotes the pixel value at\\n of\\na circular neighborhood. Now, let\\n denote an LBP image cor-\\nresponding to\\n . Note that each of the pixel values of\\nis ﬁlled with LBP\\n at its given pixel location\\n .\\nSpeciﬁcally, if\\n (i.e., for the case of uniform pattern), each\\npixel of\\n is labeled as a value of\\n .\\nOtherwise, it will be labeled as constant\\n . The\\nFig. 3. Illustration of the LBP operation process performed on each color-com-\\nponent image corresponding to respective channel in the /89/73 /81 color space. Note\\nthat, for the sake of clear visualization, pixel values of LBP images are scaled\\nto the range of [0, 255].\\naforementioned LBP operation performed on each color com-\\nponent image from the\\n color space is illustrated in Fig. 3.\\nTo reﬂect and encode the local properties of a color compo-\\nnent image\\n , we compute a regional LBP pattern histogram\\nfor each local region\\n . Note that every re-\\ngional histogram consists of\\n bins, i.e.,\\nbins for the patterns with two transitions, two bins for the pat-terns with zero transitions, and one bin for all nonuniform pat-\\nterns. The LBP histogram for the\\nth local region\\n is com-\\nputed as follows:\\nfor\\n (7)\\nwhere\\nif\\n is true\\nif\\n is false(8)\\nand\\n denotes the\\n th LBP value in the range of\\nand, therefore,\\n values denote the number of pixels with\\nthe LBP value\\n within the local region\\n .\\nUsing (7), the regional LBP descriptor for\\n can be de-\\nﬁned as follows:\\n(9)\\nNote that, in (9),\\n provides regional LBP histogram in-\\nformation for\\n . In order to keep the information about the\\nspatial relation of facial local regions, all of the\\nvalues are concatenated into a single column vector\\ntermed as the unichrome LBP feature for\\n , i.e.,\\n(10)\\nTo obtain the opponent LBP features, each pair of different\\ncolor channels is used. For this, the center coordinate for aneighborhood and the neighbor itself are taken from different\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1371\\nFig. 4. (a) Three LBP pattern histograms each computed by performing unichrome LBP operation [deﬁned in (5)] on (enclosed by white boxes) the correspo nding\\nthree local regions that are obtained from each color channel in the /89/73 /81 color space. (b) Three LBP pattern histograms each computed by performing the opponent\\nLBP operation [deﬁned in (11)] on a pair of two corresponding local regions that are obtained from two different color channels in the /89/73 /81 color space.\\ncolor channels. Based on (5), given a pair of\\n and\\n , the\\nopponent LBP operation is deﬁned as follows:\\nLBP\\nif\\notherwise\\nfor\\n (11)\\nwhere\\n(12)\\nand\\n denotes a pixel value of\\n at its coordinate\\n(i.e., the center pixel of a circular neighborhood) and\\ndenotes the pixel values of\\nthat form a circular neighborhood of the center pixel\\n of\\n .\\nUsing (7), (9), and (10), the opponent LBP feature denoted by\\nfor the pair of\\n and\\n can be readily computed from\\nLBP\\n shown in (11).Finally, the proposed color LBP (CLBP) feature for\\n can\\nbe deﬁned as follows:\\nfor\\n (13)\\nNote that, in (13),\\n consists of one unichrome LBP\\nfeature (extracted from\\n ) and\\n opponent LBP features\\ncomputed between\\n (with a particular\\n ) and\\n different\\n(\\n and\\n ).\\nFig. 4 shows six different LBP pattern histograms deﬁned\\nby (7). The ﬁrst three histograms are derived from performingunichrome LBP operations on three corresponding local re-gions, one for each channel in the\\ncolor space, whereas\\nthe other three histograms are derived from performing op-ponent LBP operations on a pair of two corresponding localregions of two different color channels. It is easy to see thatthese histograms have different patterns of distribution ofLBP values so that they are complementary to one another forclassiﬁcation purposes. Indeed, the proposed\\nis likely\\nto contain much more discriminating information than what a\\nsingle grayscale LBP operation can provide.\\n1372 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nV. C OMBINING COLOR LOCAL TEXTURE FEATURES FOR FR\\nThis section suggests the way of combining color local tex-\\nture features for achieving the best FR performance. For sim-\\nplicity of notation, we denote a color local texture feature (ob-\\ntained from a color-component image\\n )b y\\n . Hence,\\n can\\nrepresent either\\n or\\n deﬁned in Section IV. In ad-\\ndition, let\\n be an unknown\\n color face image (to be\\nidentiﬁed or veriﬁed), which is denoted as a probe. In addition,let\\nbe a gallery set consisting of prototype enrolled\\ncolor face images (to be recognized), each of which denoted by\\n, of known individuals (i.e.,\\n ). Furthermore, without\\nany loss of generality, we denote the individual color local tex-ture features of\\nand\\n by\\n and\\n , respectively, where\\n.\\nGiven the\\n color local texture features for recognizing\\n ,\\nour goal is to attain the best FR performance by combining thembased on the information fusion theory [29] for pattern classiﬁ-cation. Techniques for fusing multiple evidences (i.e., multiple\\nclassiﬁcation results or multiple features) can be generally clas-\\nsiﬁed into two classes, i.e., fusion at the “feature level” and fu-sion at the “decision level.” In the proposed method, we makeuse of the feature-level fusion strategy for two reasons: 1) Asreported in [26], “feature-level fusion” can generally achieve abetter classiﬁcation result compared with performance obtainedusing the “decision-level” fusion. 2) Our experimentation usingthe most popular FR DBs, such as the FRGC 2.0 data set, in-dicates that “feature-level” fusion methods achieve better FRperformance than the competing “decision-level” fusion frame-work (for more details, please refer to the experimental assess-ment in Section VI-B).\\nFollowing the aforementioned observation, we adopt infor-\\nmation fusion techniques performed at the level of features.Using feature-level information fusion techniques, color local\\ntexture features of\\n(or\\n ) could be simply concatenated into\\na longer global feature vector. However, it should be noted thatdirectly applying a nearest-neighbor (NN) classiﬁer to such aconcatenated feature vector could suffer from the degradation inthe FR performance caused by the high dimensionality and theredundant information. To overcome the aforementioned lim-itation, low-dimensional feature extraction techniques are em-ployed. Let us denote the\\nth face feature extractor by\\n (e.g.,\\nPCA) in order to extract a low-dimensional feature of the colorlocal texture feature\\n. Note that\\n can be formed with a\\ntraining set of color local texture features\\n , all of which are\\ncomputed from the\\n th color-component training images\\n .\\nThen, the low-dimensional features of\\n and\\n are obtained\\nas follows (using the corresponding\\n ):\\n(14)\\nwhere\\n ,\\n ,\\n denotes\\n -dimensional real space,\\nand\\n . Then,\\n complementary low-dimensional\\nfeatures, given by (14), are combined at the level of the fea-tures (by concatenating low-dimensional features in the columnorder), i.e.,\\n(15)\\nwhere\\n ,\\n , and\\n .It is important to note that, in (15), each low-dimensional fea-\\nture\\n (or\\n ) should be independently normalized in order to\\nhave zero mean and unit variance prior to their concatenation.By doing so, we may avoid the negative effect of the magni-tude dominance of one low-dimensional feature over the others.\\nConsequently, the fusion method described in (15) allows for\\neffectively facilitating a complementary effect between its dif-ferent components, leading to positively affecting the classiﬁca-tion performance.\\nTo perform FR tasks (identiﬁcation or veriﬁcation) on\\n,a n\\nNN classiﬁer is then applied to determine the identity of\\n by\\nmatching the corresponding\\n with the closest\\n [20].\\nVI. E XPERIMENTS\\nAn extensive experimental study was carried out to investi-\\ngate the effectiveness of the proposed color local texture fea-\\ntures for FR. The following subsection describes in detail theface DBs used in our experiments, as well as our evaluationmethodology.\\nA. Experimental Setup and Condition\\nFour publicly available face DBs, i.e., CMU-PIE [12], Color\\nFERET [13], XM2VTSDB [14], SCface [48], and FRGC 2.0[15], were used to evaluate the proposed color local texture fea-tures. All facial images used in our experiments were manually\\ncropped from original images based on the locations of the two\\neyes. The eye coordinates are those supplied with the originaldata set. Each cropped facial image was rescaled to the size of120\\n120 pixels (see Fig. 5). After alignment, each of the facial\\nimages with a size of 120\\n 120 is divided into the 64 different\\nface local regions to compute the proposed color local texturefeatures. Thus, the size of each local region is 15\\n15 pixels.\\nTo construct a face feature extractor\\n (described in\\nSection V), ﬁve popular low-dimensional feature extraction\\ntechniques were used, i.e., PCA [30], Fisher’s linear discrimi-nant analysis (FLDA) [31], Enhanced Fisher linear discriminantModel (EFM) [7], eigenfeature regularization and extraction\\n(ERE) [32], and kernel direct discriminant analysis (KDDA)\\n[33]. Note that a radial basis function was adopted as the kernelfunction for implementing the KDDA [33]. The PCA and theFLDA are commonly used as benchmarks for the evaluation\\nof the performance of FR algorithms [13]. The EFM is one of\\nthe popular face feature extraction techniques based on lineardiscriminant analysis (LDA). In particular, it has been reportedin [7] that the EFM guarantees a better generalization perfor-\\nmance than FLDA by avoiding the overﬁtting problem. The\\nERE outperforms all other LDA-based FR methods discussedin [32]. In [33], the KDDA shows considerably a better FRperformance compared with other popular kernel-based FR\\nmethods (such as kernel PCA and kernel LDA). As for the\\nNN classiﬁers, the Euclidean distance was used for the FLDAand the KDDA, whereas the cosine distance measure was usedfor the EFM and the ERE (as recommended in [7] and [32],\\nrespectively), and the Mahalanobis distance [34] was employed\\nfor the PCA.\\nIn our experiments, ﬁve different scales and eight different\\norientations, which are most widely used for FR [6], [7], are em-\\nployed to construct a set of Gabor ﬁlter banks deﬁned in (1). In\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1373\\naddition, we downsampled each Gabor wavelet representation\\nby a factor of 36 for the purpose of reducing the dimensionalityand the computational complexity [7]. The downsampled ver-\\nsion of each Gabor representation is then used to produce the\\ncorresponding column vector. Note that, as described in [7], theFR performance difference is quite marginal, as compared withthe case where downsampling is not performed. In the case of\\nLBP operations shown in (5) and (11), following the recommen-\\ndation of [8], we adopted the LBP\\noperator (i.e.,\\nand\\n ).\\nIn order to generate color-component images, two effective\\ncolor representations (devised for the purpose of FR) were used\\nin our experiments, i.e., the normalized hybrid “\\n ” [23] and\\n(\\ntaken from\\n ,\\ntaken from\\n , and\\n taken\\nfrom\\n ) color representations [20]. Note that, to derive the\\nnormalized hybrid “\\n ” color representation, we employed\\nthe across-color-component normalization technique suggestedby [23] as this method achieves the best FR performance of allnormalized color spaces evaluated in [23]. In addition,\\ncolor representation shows the best FR performance of all pos-sible color-component image conﬁgurations discussed in [20].\\nIn our experiments, the cumulative match curve [13] and the\\nface veriﬁcation rate (FVR) at the false accept rate (FAR) [35]\\nwere used for measuring the identiﬁcation and veriﬁcation FR\\nperformance, respectively. Note that, in general, the FR perfor-mance relies on the number of low-dimensional features used[32], [36]. Therefore, in order to realize a fair comparison, the\\nbest found correct recognition rate (BstCRR) proposed in [36]\\nwas selected as the identiﬁcation rate. In addition, in all exper-\\niments, the frontal-view images with neutral illumination andexpression were used to build the gallery set.\\nB. Effectiveness of Color Local Texture Features for FR\\nHere, we compare the FR using the proposed color local tex-\\nture features with the FR using grayscale texture features and the\\nFR using only color information. For FR using grayscale tex-ture features, only luminance information is applied to extractgrayscale Gabor wavelet or LBP features. In our experiments,\\nthe “\\n” channel [20] from\\n color space was adopted for\\nextracting grayscale texture features. For FR using only colorinformation, given\\ndifferent color component images, indi-\\nvidual color-component vectors were ﬁrst generated in the form\\nof a column vector by lexicographic ordering of the pixel ele-\\nments of corresponding color-component images [20]. In orderto guarantee fair and stable comparisons with the method usingour color local texture features, the low-dimensional features of\\nthese\\ncolor-component vectors were then combined at the\\nlevel of features in the same way as described in (15) (but notusing Gabor or LBP). The resulting concatenated features areapplied to FR.\\nIn order to validate the advantage of making use of the fea-\\nture-level fusion approach to combining color local texture fea-tures, we report the experimental results obtained using the pro-posed feature-level fusion, as well as using the decision-level\\nfusion. For the case of using the decision-level fusion, a sig-\\nmoid function [37] followed by a sum normalization method[37] was used to normalize the matching scores (e.g., distance\\nFig. 5. (a) Examples of the facial images with ﬂash illumination from the\\nCMU-PIE DB. (b) Examples of facial images with uncontrolled illumination\\ncondition from the XM2VTSDB. (c) Examples of facial images with pose\\nvariations from the Color FERET DB. (d) Examples of facial images with\\nvariation in face resolutions from the Color FERET DB.\\nscores). Furthermore, the sum rule [29] was adopted for com-\\nbining these multiple matching scores at the decision level. It\\nhas been shown in [29] that the sum rule achieves the best clas-siﬁcation performance in comparison with other decision-levelfusion strategies, such as product and median rules.\\nIn the following three subsections, we present comparative\\nexperimental results to validate the effectiveness of color localtexture features for FR under variations in illumination, pose,and resolution, respectively.\\nResults on Illumination Variation: In order to validate the ro-\\nbustness of the proposed color local texture features (namely,CLGW and CLBP) against extensive variations in illumination,the CMU-PIE and XM2VTSDB face DBs were used. For this,\\n1428 frontal images of 68 subjects (21 images per subject) were\\ncollected from the CMU-PIE; the facial images for each sub-ject have 21 different illumination variations (using the “roomlighting off” condition). From the XM2VTSDB, 900 frontal\\nimages of 100 subjects were obtained; each subject included\\nnine facial images captured with no control on severe illumi-nation variations. As a result, through using random partition,the training set consisted of 6 images\\n168 subjects, whereas\\nthe remaining 1320 images were used to create a probe set. In\\norder to guarantee stable experimental results, 40 independent\\nruns of aforementioned random partitions were executed. Thus,the all the following results reported in Section VI-B were av-\\neraged over 40 runs.\\nFig. 5(a) and (b) shows examples of facial images used in this\\nexperiment. We can see that the face images shown in Fig. 5 aresubject to severe illumination changes caused by the interrup-\\ntion of background colored light and cast shadow.\\nTable I shows the average rank-1 identiﬁcation rates of the FR\\nusing the two proposed CLGW and CLBP features and the FR\\n1374 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nTABLE I\\nAVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )AND CORRESPONDING STANDARD DEVIATIONS FOR EV ALUATING\\nTHE EFFECTIVENESS OF COLOR LOCAL TEXTURE FEATURES ON ILLUMINATION VARIATION .NOTE THAT /90/82 /71 AND /82/81 /67\\nCOLOR SPACES WEREEMPLOYED . (a) CLBP. (b) CLGW\\nNote that FL corresponds to the feature-level fusion method, whereas DL corresponds to the decision-level fusion\\nmethod. In addition, the bold value denotes the best result of FR approaches in each low-dimensional feature extractiontechnique, and the similar notations are also used in the following tables.\\nmethods using grayscale Gabor and LBP features, and the FR\\nmethods using only color information. From Table I, we have\\nthe following observations:\\n1) Compared with both the FR using grayscale texture fea-\\ntures and the FR using only color information, the FR\\nperformances of two proposed CLGW and CLBP are\\nclearly better for all low-dimensional feature extractiontechniques. This result can validate the complementaryeffect, supplemented with additional information by the\\nvirtue of fusing chromaticity and texture features.\\n2) The color local texture features perform clearly better than\\ntheir grayscale counterparts. In particular, the grayscaleLBP severely suffers from the nonmonotonic illumination\\nchanges, as compared with the grayscale Gabor. How-\\never, the proposed CLBP is much more robust againstillumination changes than its grayscale counterpart. Theperformance increases, as compared with the grayscale\\nLBP, can be increased up to around 15%, 22%, 18%, and\\n17% for the PCA, the FLDA, the KDDA, and the ERE,respectively.\\n3) For the approach of combining color local texture features,\\nthe feature-level fusion method yields a considerably better\\nFR performance than the decision-level fusion method.\\nResults on Pose Variation: We further assess the usefulness\\nof the color local texture features under moderate pose varia-\\ntions. A total of 1378 face images of 107 subjects were col-\\nlected from the Color FERET face DB. It should be noted thatonly the rotated face images that both eyes can be reliably iden-tiﬁed for normalization were collected. The facial images used\\ninclude ﬁve different pose angles ranging from\\nto\\n[see Fig. 5(c)]. Moreover, it should be noted that all the images\\nhave neutral expression and illumination. By using random par-tition, the training set consisted of 535 images (5 images\\n107subjects), whereas the probe set contained the remaining 843\\nimages of the same 107 subjects.\\nThe comparison results are described in Table II. It is shown\\nthat the proposed CLFG and CLBP outperform both their oppo-nent grayscale texture features and the associated FR methods\\nusing only color information for all feature extraction methods\\nused. In particular, grayscale Gabor is found to be much weakerthan grayscale LBP in terms of reliably recognizing face im-ages with pose changes. However, with CLGW (for the case of\\nusing\\ncolor space and FL), we can attain improvement\\nin the identiﬁcation rate of about 9%, 12%, 10%, and 8%, inthe order of the PCA, the FLDA, the KDDA, and the ERE, re-spectively. Furthermore, for the case of using CLBP, as com-\\npared with grayscale LBP, identiﬁcation rates can be improved\\nby 12%, 8%, 7%, and 6% for the PCA, the FLDA, the KDDA,and the ERE, respectively.\\nThe underlying reason for this is that our color local texture\\nfeatures are designed to exploit the discriminative information\\nderived from spatiochromatic texture patterns residing within\\nlocal face regions . Hence, when recognizing face images taken\\nunder moderate pose variations, discriminating color texture\\npatterns arising from local face regions are more likely to be\\npreserved for a FR purpose, as compared with the texture pat-terns globally extracted from the entire face region. In addition,it has been reported in [49] and [50] that color information is\\nhelpful for improving recognition/classiﬁcation rates for a va-\\nriety of objects under modest changes in the orientation. Con-sequently, based on the results in Table II, combining the colorand texture feature information can lead to the improved recog-\\nnition of face images captured with moderate pose changes.\\nResults on Face Resolution Variation: Some real-life FR ap-\\nplications (particularly FR on video surveillance or mobile de-vices) are likely to be confronted with low-resolution and low-\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1375\\nTABLE II\\nAVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )AND CORRESPONDING STANDARD DEVIATIONS FOR EV ALUATING\\nTHE EFFECTIVENESS OF COLOR LOCAL TEXTURE FEATURES ON POSE CHANGE .NOTE THAT /90/82 /71 AND /82/81 /67\\nCOLOR SPACES WEREEMPLOYED . (a) CLBP. (b) CLGW\\nquality face images [38], [39]. The goal of this experiment is\\nto measure the degree to which the proposed color local texturefeatures are robust against small-resolution face images, rela-\\ntive to grayscale texture features. To this end, 1428 frontal-view\\nimages of 68 subjects were selected from CMU-PIE; for onesubject, facial images have 21 different lighting variations. Fur-thermore, from the Color FERET, the 1120 frontal-view images\\nof 140 subjects (eight samples per subject) were chosen from\\nthefa,fb,fc, and dup1 sets. The training set consisted of 1040\\nimages (5 samples\\n208 subjects), whereas the remaining 1508\\nfacial images were used for the probe set. To obtain facial im-\\nages with varying face resolutions, we performed resizing over\\nthe aforementioned original collected data set. Fig. 5(d) showsexamples of facial images containing face resolution variations.We took original high-resolution face images of size of 120\\n120 pixels, synthetically blurred them with a Gaussian kernel[40], and then downsampled them in order to simulate a lowerresolution effect as closely as possible to practical camera lens.As a result, ﬁve different face resolutions of 120\\n120, 86\\n86, 44\\n 44, 32\\n 32, 20\\n 20 pixels were generated.\\nIn real-life surveillance-like FR applications, it is reason-\\nable to assume that high-resolution face images are chosen astraining and gallery images. On the other hand, the probe to be\\ntested may have lower and various face resolutions [20], [38].\\nHence, in this experiment, the face resolution of training and\\ngallery images was ﬁxed as 120\\n120 pixels, whereas the reso-\\nlution of probe was varied to ﬁve different resolutions, as shown\\nin Fig. 5(d). In addition, in order to match a low-resolution probe\\nto a high-resolution gallery face, the probe has been upsampledby using a cubic interpolation technique before recognition.\\nFig. 6 shows the results obtained for color local texture and\\ngrayscale texture features that are extracted from face images\\nof different face resolutions. It can be seen that, with the de-crease in the face resolution, the FR performances of grayscaleGabor and grayscale LBP features drop much more quickly thanthose of their opponent color local texture features for both the\\nKDDA and the ERE. Note that local texture features generallyencode detailed variations within some local areas in the face.\\nThus, some facial details contained in the grayscale face image\\nmay be lost at small resolution, yielding the severe degradationin the FR performance. On the contrary, the chromatic contrastsensitivity information can be generally concentrated on low\\nspatial frequencies [41], [42]. Hence, the texture patterns ex-\\ntracted from chromaticity components are able to be reservedat small resolution. For this reason, the texture features of chro-maticity components can compensate the decreased FR perfor-\\nmance caused by a low-resolution face image. The observation\\nthat color local texture features are much more robust to varia-tion in face resolution than grayscale texture features is a veryencouraging result, considering that extracting reliable features\\nfrom low-quality face images is a critical problem in emerging\\nFR applications such as FR from video and FR on mobile de-vices [43], [44].\\nC. Comparisons With Other Color FR Methods\\nHere, to compare the best results of our method with the re-\\nsults obtained by other state-of the-art color FR methods, thefollowing two testbeds have been constructed using four dif-\\nferent face DBs:\\n• Data set 1: A total of 3612 facial images from 341 subjects\\nwere collected from three public face DBs to constructthe training and probe sets. During collection phase,\\n1428 frontal-view images of 68 subjects were selected\\nfrom CMU-PIE; for one subject, facial images had 21different lighting variations. From Color FERET, the 1120frontal-view images of 140 subjects (eight samples per\\nsubject) were chosen from “fa,” “fb,” “fc,” and “dup1”\\nsets. From XM2VTSDB, 1064 frontal-view images of133 subjects were obtained from two different sessions;each subject included eight facial images that contained\\n1376 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nFig. 6. Rank-1 identiﬁcation rates for demonstrating the robustness of color local texture features on face resolution variations. Note that the /90/82 /71 color space\\nand the feature-level fusion approach are used for CLBP and CLGW. (a) KDDA. (b) ERE.\\nFig. 7. Comparisons with other state-of-the-art color FR methods using “Data set 1.” Note that the /90/82 /71 color space and the feature-level fusion approach are\\nused for the proposed methods using CLBP (or CLGW). (a) EFM. (b) ERE.\\nillumination and resolution variations. The training set is\\nrandomly constructed to include 1705 images (5 samples\\n341 subjects), whereas the remaining 1907 images are\\nfor the probe set.\\n• Data set 2: The objective of constructing this data set is\\nto justify the effectiveness of the proposed method whentraining and testing images have been captured usingdifferent cameras. For this purpose, 1950 facial images of\\n130 different subjects (15 images\\n130 subjects) were\\ncollected from the SCface DB [48] to construct the trainingand probe sets. Note that all of the collected facial imagesfor each subject are taken with ﬁve different surveillance\\ncameras, each labeled “cam1” through “cam5” [48].\\nSpeciﬁcally, there are three images per subject for eachcamera, captured at three different distances (4.20, 2.60,and 1.00 m). Using random partition, the training set\\nconsisted of 910 images (7 images\\n130 subjects), and\\nthe remaining 1040 facial images were used for the probeset. In addition, we constructed a gallery set consistingof 130 frontal facial mug shots (taken with a consumer\\ndigital camera) of 130 different subjects.\\nIn our experiments, we compare the proposed methods with\\nsix state-of-the-art color FR methods [21]–[24], [45], [46]. Notethat, when implementing other color FR methods, we have\\nstrictly followed the implementation guidelines (and parameter\\nsettings) listed in the respective published paper for the purposeof stable comparisons. Speciﬁcally, for the method in [45],the hybrid “\\n” color space (\\n from\\n and\\n and\\nfrom\\n color spaces, respectively) was used as proposed. In\\naddition, as recommended by [45], the same size of masks usedto select frequency sets in the frequency domain was used. Forthe method in [23], the normalized hybrid “\\n” color space\\nusing the across-color-component normalization technique\\n[23] was used as this method achieves the best FR performanceof all normalized color spaces evaluated. For the methodin [21], we implemented its extended version based on the\\ncolor space. Furthermore, following the same parameter\\nvalues as used in [21], the initial value of the CID algorithmand the convergence threshold were set to “[1/3,1/3,1/3]”and “0.1,” respectively. For the method in [22], the Comon\\nindependent-component-analysis algorithm [46] was used in\\nour experiment to compute mutual information between colorcomponents and high-order statistics, as suggested in [22]. Forthe method in [24], the hybrid “\\n” color space was used\\nas proposed. Moreover, strictly following [24], we used the\\nsame method for partitioning the Gabor image representationinto an ensemble of patches, as well as for determining DCTmask sizes used to select various DCT feature sets.\\nFig. 7 shows the comparison results. Note that the experi-\\nmental results shown in Fig. 7 are obtained using “Data set1.” As shown in Fig. 7, the proposed methods using CLBP (orCLGW) achieve better or comparable FR performance com-\\npared with the other six color FR methods, for both the EFM\\nand the ERE. In particular, the proposed method (for the caseof using the CLBP and the feature-level fusion) attains the bestrank-1 identiﬁcation rates of up to 94.15% and 97.82% for the\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1377\\nTABLE III\\nCOMPARISONS OF AVERAGE RANK-1 I DENTIFICATION RATES (INPERCENT )TOSHOW THE EFFECTIVENESS OF THE PROPOSED METHODS\\nWHEN COLOR FACE IMAGES CAPTURED WITHDIFFERENT CAMERAS AREUSED DURING THE TRAINING AND\\nTESTING STAGES .NOTETHAT EXPERIMENTAL RESULTS AREOBTAINED USING “DATA SET 2”\\nEFM and the ERE, respectively, of all color FR methods under\\nconsideration. These comparison results validate the feasibility\\nof color FR methods using the proposed CLBP and CLGWfeatures.\\nTable III presents comparative results on the “Data set 2.”\\nAs shown in Table III, both CLBP and CLGW considerably\\noutperform their grayscale counterparts (i.e., grayscale LBPand grayscale Gabor), respectively, for the EFM and the ERE.For instance, in the case of the ERE, 15.44% and 12.98%\\nimprovements in the rank-1 identiﬁcation rate can be attained\\nfrom the proposed CLBP and CLGW, respectively. In addition,from the comparison, we can see that the results of the proposedmethods are better than those obtained from the other six color\\nFR methods. Consequently, the results shown in Table III con-\\nﬁrm the feasibility of our methods in terms of improving the FRperformance when color face images captured with differentcameras are used during the training and testing stages.\\nD. Evaluation of Proposed Methods on the FRGC 2.0 Data Set\\nIn addition, we conducted comparative experiments on the\\nFRGC 2.0 data set to further evaluate our color local texture\\nfeatures. Here, “FRGC Experiment 4” is chosen to assess theproposed method because FRGC Experiment 4 has been re-ported to be the most challenging FRGC experiment [22]. Note\\nthat direct comparisons are made with other state-of-the-art re-\\nsults recently reported by other researchers on these FRGC 2.0data set. As such, all the results for the comparison are directlycited from papers recently published. It should be also noted\\nthat we report the veriﬁcation rates at FAR\\n(which is\\ncorresponding to ROC III curve [15]) for the purpose of directcomparison with other color FR methods, since recent publishedstudies have reported the veriﬁcation performance of competing\\nstate-of-the-art solutions using FAR\\n. This guarantees\\nfair and reliable comparisons of our methods with other colorFR methods. In addition, note that the EFM was adopted for aface feature extractor in our method because all other color FR\\nmethods make use of the EFM for extracting low-dimensional\\nfeatures.\\nTable IV tabulates the comparative results on the FRGC 2.0\\ndata set. The results show that color FR methods using ourTABLE IV\\nCOMPARISONS WITHOTHER STATE -OF-THE-ARTCOLOR FR M ETHODS ON\\n“FRGC 2.0 E XPERIMENT 4.” N OTETHAT /122-SCORE NORMALIZATION [26] I S\\nUSED TO COMPUTE FVR AND FAR\\nCLBP and CLGW work comparatively well with the published\\nresults on the FRGC 2.0 data set. In particular, the veriﬁcation\\nperformance of our method using CLBP, i.e., 91.01%, is quitecomparable with the most recent best performance, i.e., 92.43%,reported in [24]. Note that, differing from the results reported in\\nFig. 7 and Table III, the color FR solution proposed in [24] per-\\nforms the best, followed in order by the proposed CLBP andCLGW, as shown in Table IV. This is mainly due to the factthat implementation parameters recommended in [24] may be\\noptimally tuned up for the purpose of recognizing face images\\nwithin the FRGC 2.0 data set but not tuned up for other FR DBsused to obtain the results in Fig. 7 and Table III.\\nE. Comparisons of Unichrome and Opponent Texture Parts\\nRecall from Section IV that the proposed color local texture\\nfeatures consist of two parts, i.e., unichrome (or channelwise)\\nand opponent texture features. Here, we have performed addi-tional experiments to investigate which part (i.e., unichromeversus opponent feature representations) contributes the most\\nto the FR performance. In addition, we have investigated the\\nuse of “combined (or fused)” unichrome and opponent tex-ture features and compared against a framework separately\\nutilizing unichrome and opponent texture features. To this\\nend, the face image data set collected from CMU-PIE, Color\\nFERET, and XM2VTSDB were used in our experimentation. Adetailed description of the experimental data set used is givenin Section VI-C.\\n1378 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nFig. 8. Comparisons of rank-1 identiﬁcation rates obtained for “combined” (or fused) unichrome and opponent texture features with those obtained us ing sepa-\\nrately unichrome and opponent texture features. Note that the /90/82 /71 color space and the feature-level fusion approach are used. (a) EFM. (b) ERE.\\nThe results are given in Fig. 8. Note that, for comparison\\npurposes, the identiﬁcation rates obtained for grayscale tex-ture features are provided as the baseline performance. From\\nFig. 8(a) and (b), the following three observations can be made:\\n1) the unichrome and opponent texture parts contribute simi-larly for FR; 2) by combining unichrome and opponent textureparts, the FR performances can be impressively improved, in-\\ndicating that the unichrome and opponent texture parts are able\\nto provide different information and to be mutually compensa-tional in terms of enhancing the FR performance; and 3) bothunichrome and opponent texture parts considerably outperform\\nthe baseline grayscale texture (although separately used).\\nVII. C\\nONCLUSION\\nThis paper has investigated the contribution of color to ex-\\nisting texture features for improving the FR performance. This\\npaper has examined how to effectively exploit the discrimi-\\nnating information by combining color and texture information,as well as its fusion approach. To this end, under the frameworkof local pattern encoding, we have proposed two effective color\\nlocal texture features, i.e., CLBP and CLGW. Furthermore, in\\norder to combine multiple color local texture features, we havesuggested the feature-level fusion approach, which maximizestheir complementary effect in the context of FR. We exper-\\nimentally reveal that color FR methods based on CLBP and\\nCLGW signiﬁcantly outperform the methods relying only ontexture or color information. One particularly important resultis that our color local texture features allows for a signiﬁcant\\nimprovement in the FR accuracy when recognizing face images\\ntaken under a severe change in illumination (at least, in the datasets used in our experimentation), as well as for low-resolutionface images, as compared with their grayscale counterparts.\\nIn addition, comparative experimental results show that color\\nFR methods using color local texture features yield better orcomparable FR performance compared with those obtainedusing other recent advanced color FR methods.\\nThe experimental study in this paper has been limited to eval-\\nuating the effectiveness of color local texture features that areextracted from ﬁxed color-component conﬁguration consistingof three components (such as\\n). It has been observed in\\n[47] that different color spaces possess distinct characteristics\\nand effectiveness in terms of discriminating power for the vi-sual-classiﬁcation task. Hence, for the future work, we will de-velop the method of selecting an optimal subset of color compo-nents (from a number of different color spaces), aiming to ob-\\ntain more discriminating color local texture features for FR. Tothis end, we plan to make use of multiclass boosting color fea-\\nture selection algorithm proposed in [54] to ﬁnd the best color\\nlocal texture features, each of which corresponding to a partic-ular local face region (such as eye or mouth).\\nIn addition, in this paper, to perform color FR, multiple color\\nlocal texture features have been combined at the feature level\\nwith uniform (equal) weights. For the future work, we willdevise a more effective weighted feature-level fusion schemewhere color local texture features are fused with different\\nweights. The weight of each color local texture feature could\\nbe determined based on the degree to which color local texturefeatures contribute to the improvement in the FR performance.\\nR\\nEFERENCES\\n[1] K. W. Bowyer, “Face recognition technology: Security versus privacy,”\\nIEEE Technol. Soc. Mag. , vol. 23, no. 1, pp. 9–19, Spring 2004.\\n[2] A. K. Jain, A. Ross, and S. Prabhaker, “An introduction to biometric\\nrecognition,” IEEE Trans. Circuits Syst. Video Technol. , vol. 14, no. 1,\\npp. 4–20, Jan. 2004.\\n[3] J. Y. Choi, W. De Neve, Y. M. Ro, and K. N. Plataniotis, “Automatic\\nface annotation in photo collections using context-based unsupervisedclustering and face information fusion,” IEEE Trans. Circuits Syst.\\nVideo Technol. , vol. 20, no. 10, pp. 1292–1309, Oct. 2010.\\n[4] J. Zou, Q. Ji, and G. Nagy, “A comparative study of local matching\\napproach for face recognition,” IEEE Trans. Image Process. , vol. 16,\\nno. 10, pp. 2617–2628, Oct. 2007.\\n[5] Y. Su, S. Shan, X. Chen, and W. Gao, “Hierarchical ensemble of global\\nand local classiﬁers for face recognition,” IEEE Trans. Image Process. ,\\nvol. 18, no. 8, pp. 1885–1896, Aug. 2009.\\n[6] S. Xie, S. Shan, X. Chen, and J. Chen, “Fusing local patterns of\\nGabor magnitude and phase for face recognition,” IEEE Trans. Image\\nProcess. , vol. 19, no. 5, pp. 1349–1361, May 2010.\\n[7] C. Liu and H. Wechsler, “Gabor feature based classiﬁcation using the\\nenhanced ﬁsher linear discriminant model for face recognition,” IEEE\\nTrans. Image Process. , vol. 11, no. 4, pp. 467–476, Apr. 2002.\\n[8] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local\\nbinary pattern: Application to face recognition,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 28, no. 12, pp. 2037–2041, Dec. 2006.\\n[9] T. Maenpaa and M. Pietikainen, “Classiﬁcation with color and texture:\\nJointly or separately,” Pattern Recognit. , vol. 37, no. 8, pp. 1629–1640,\\nAug. 2004.\\n[10] A. Drimbarean and P. F. Whelan, “Experiments in colour texture anal-\\nysis,” Pattern Recognit. Lett. , vol. 22, no. 10, pp. 1161–1167, Aug.\\n2001.\\n[11] G. Paschos, “Perceptually uniform color spaces for color texture anal-\\nysis: An empirical evaluation,” IEEE Trans. Image Process. , vol. 10,\\nno. 6, pp. 932–937, Jun. 2001.\\n[12] T. Sim, S. Baker, and M. Bsat, “The CMU pose, illumination, and ex-\\npression database,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,\\nno. 12, pp. 1615–1618, Dec. 2003.\\nCHOI et al. : COLOR LOCAL TEXTURE FEATURES FOR COLOR FACE RECOGNITION 1379\\n[13] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET\\nevaluation methodology for face recognition algorithms,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 22, no. 10, pp. 1090–1104, Oct. 2000.\\n[14] K. Messer, J. Mastas, J. Kittler, J. Luettin, and G. Maitre,\\n“XM2VTSDB: The extended M2VTS database,” in Proc. IEEE\\nInt. Conf. AVBPA , 1999, pp. 72–77.\\n[15] P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, J. Chang, K.\\nHoffman, J. Marques, J. Min, and W. Worek, “Overview of the facerecognition grand challenges,” in Proc. IEEE Int. Conf. Comput. Vis.\\nPattern Recog. , 2005, pp. 947–954.\\n[16] W. Zhang, S. Shan, W. Gao, X. Chen, and H. Zhang, “Local Gabor\\nbinary pattern histogram sequence (LGBPHS): A novel non-statisticalmodel for face representation and recognition,” in Proc. IEEE ICCV ,\\n2005, pp. 786–791.\\n[17] L. Torres, J. Y. Reutter, and L. Lorente, “The importance of the\\ncolor information in face recognition,” in Proc. IEEE ICIP , 1999, pp.\\n627–631.\\n[18] P. Shih and C. Liu, “Comparative assessment of content-based face\\nimage retrieval in different color spaces,” Int. J. Pattern Recog. Artif.\\nIntell. , vol. 19, no. 7, pp. 873–893, Nov. 2005.\\n[19] P. Shih and C. Liu, “Improving the face recognition grand challenge\\nbaseline performance using color conﬁgurations across color spaces,”inProc. IEEE ICIP , 2006, pp. 1001–1004.\\n[20] J. Y. Choi, Y. M. Ro, and K. N. Plataniotis, “Color face recognition for\\ndegraded face images,” IEEE Trans. Syst., Man, Cybern. B, Cybern. ,\\nvol. 39, no. 5, pp. 1217–1230, Oct. 2009.\\n[21] J. Wang and C. Liu, “Color image discriminant models and algorithms\\nfor face recognition,” IEEE Trans. Neural Netw. , vol. 19, no. 12, pp.\\n2088–2098, Dec. 2008.\\n[22] C. Liu, “Learning the uncorrelated, independent, and discriminating\\ncolor spaces for face recognition,” IEEE Trans. Inf. Forensics Security ,\\nvol. 3, no. 2, pp. 213–222, Jun. 2008.\\n[23] J. Yang, C. Liu, and L. Zhang, “Color space normalization: Enhancing\\nthe discriminating power of color spaces for face recognition,” Pattern\\nRecognit. , vol. 35, no. 1, pp. 615–625, 2002.\\n[24] Z. Liu and C. Liu, “Fusion of color, local spatial and global frequency\\ninformation for face recognition,” Pattern Recognit. , vol. 43, no. 8, pp.\\n2882–2890, Aug. 2010.\\n[25] A. Jain and G. Healey, “A multiscale representation including op-\\nponent color features for texture recognition,” IEEE Trans. Image\\nProcess. , vol. 7, no. 1, pp. 124–128, Jan. 1998.\\n[26] A. Jain, K. Nandakumar, and A. Ross, “Score normalization in mul-\\ntimodal biometric systems,” Pattern Recognit. , vol. 38, no. 12, pp.\\n2270–2285, Dec. 2005.\\n[27] R. Lukac and K. N. Plataniotis , Color Image Processing: Methods and\\nApplication . New York: CRC Press, 2007.\\n[28] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale\\nand rotation invariant texture classiﬁcation with local binary patterns,”IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp. 971–987,\\nJul. 2002.\\n[29] J. Kittler, “On combining classiﬁers,” IEEE Trans. Pattern Anal. Mach.\\nIntell. , vol. 20, no. 3, pp. 226–239, Mar. 1998.\\n[30] M. A. Turk and A. P. Pentland, “Eigenfaces for recognition,” J. Cogn.\\nNeurosci. , vol. 3, no. 1, pp. 71–86, 1991.\\n[31] P. N. Belhumeur, J. P. Hesphanha, and D. J. Kriegman, “Eigenfaces vs.\\nFisherfaces: Recognition using class speciﬁc linear projection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711–720, Jul.\\n1997.\\n[32] X. Jiang, B. Mandal, and A. Kot, “Eigenfeature regularization and ex-\\ntraction in face recognition,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 30, no. 3, pp. 383–394, Mar. 2008.\\n[33] J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “Face recogni-\\ntion using kernel direct discriminant analysis algorithms,” IEEE Trans.\\nNeural Netw. , vol. 14, no. 1, pp. 117–126, Jan. 2003.\\n[34] V. Perlibakas, “Distance measures for PCA-based face recognition,”\\nPattern Recognit. Lett. , vol. 25, no. 6, pp. 711–724, Apr. 2004.\\n[35] P. J. Grother, R. J. Micheals, and P. J. Phillips, “Face recognition\\nvendor test 2002 performance metrices,” in Proc. Int. Conf. Audio\\nVideo-Based Biometric Person Authentication , 2003, vol. 2688, LNCS,\\npp. 937–945.\\n[36] J. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “Regularized dis-\\ncriminant analysis for the small sample size problem in face recog-nition,” Pattern Recognit. Lett. , vol. 24, no. 16, pp. 3079–3087, Dec.\\n2003.[37] C. L. Liu, “Classiﬁer combination based on conﬁdence transforma-\\ntion,” Pattern Recognit. , vol. 38, no. 1, pp. 11–28, Jan. 2005.\\n[38] P. H. Hennings-Yeomans, B. V. K. Vijaya Kumar, and S. Baker, “Ro-\\nbust low-resolution face identiﬁcation and veriﬁcation using high-res-\\nolution features,” in Proc. IEEE ICIP , 2009, pp. 33–36.\\n[39] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, “Face recog-\\nnition: A literature survey,” ACM Comput. Surv. , vol. 35, no. 4, pp.\\n399–458, Dec. 2003.\\n[40] S. Baker and T. Kanade, “Limits on super-resolution and how to break\\nthem,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 9, pp.\\n1167–1183, Sep. 2002.\\n[41] D. H. Kelly, “Spatiotemporal variation of chromatic and achromatic\\ncontrast thresholds,” J. Opt. Soc. Amer. , vol. 73, no. 6, pp. 742–749,\\nJun. 1983.\\n[42] J. B. Derrico and G. Buchsbaum, “A computational model of spati-\\nochromatic image coding in early vision,” J. Vis. Commun. Image Rep-\\nresent. , vol. 2, pp. 31–38, 1991.\\n[43] M. Kim, S. Kumar, V. Pavlovic, and H. Rowley, “Face tracking and\\nrecognition with visual constraints in real-world videos,” in Proc. IEEE\\nInt. Conf. CVPR , 2008, pp. 1–8.\\n[44] S. Mukherjee, Z. Chen, A. Gangopadhyay, and S. Russell, “A secure\\nface recognition system for mobile-devices without the need of decryp-tion,” in Proc. Workshop SKM , 2008, pp. 11–16.\\n[45] Z. Liu and C. Liu, “A hybrid color and frequency features method for\\nface recognition,” IEEE Trans. Image Process. , vol. 17, no. 10, pp.\\n1975–1980, Oct. 2008.\\n[46] P. Comon, “Independent component analysis, a new concept?,” Signal\\nProcess. , vol. 36, no. 3, pp. 287–314, Apr. 1994.\\n[47] H. Stokman and T. Gevers, “Selection and fusion of color models for\\nimage feature detection,” IEEE Trans. Pattern Anal. Mach. Intell. , vol.\\n29, no. 3, pp. 371–381, Mar. 2007.\\n[48] M. Grgic, K. Delac, and S. Grgic, “SCface—Surveillance cameras face\\ndatabase,” Multimedia Tools Appl. , vol. 51, no. 3, pp. 863–879, Feb.\\n2011.\\n[49] G. D. Finlayson, S. S. Chatterjee, and B. V. Funt, “Color angular in-\\ndexing,” in Proc. ECCV , 1996, pp. 16–27.\\n[50] S. S. Chatterjee, “Color invariant object and texture recognition,” M.S.\\nthesis, Simon Fraser Univ., Burnaby, BC, Canada, 1995.\\n[51] G. Healey and L. Wang, “Illumination-invariant recognition of texture\\nin color images,” J. Opt. Soc. Amer. A, Opt. Image Sci. Vis. , vol. 12, no.\\n9, pp. 1877–1883, 1995.\\n[52] C. H. Chan, J. Kittler, and K. Messer, “Multispectral local binary pat-\\ntern histogram for component-based color face veriﬁcation,” in Proc.\\nIEEE Int. Conf. BTAS , 2007, pp. 1–7.\\n[53] C. Zhu, C. E. Bichot, and L. Chen, “Multi-scale color local binary pat-\\nterns for visual object classes recognition,” in Proc. IEEE ICPR , 2010,\\npp. 3065–3068.\\n[54] J. Y. Choi, Y. M. Ro, and K. N. Plataniotis, “Boosting color feature\\nselection for color face recognition,” IEEE Trans. Image Process. , vol.\\n20, no. 5, pp. 1–10, May 2011.\\nJae Young Choi received the B.S. degree from\\nKwangwoon University, Seoul, South Korea, in\\n2004 and the M.S. and Ph.D. degrees from the Korea\\nAdvanced Institute of Science and Technology\\n(KAIST), Daejeon, South Korea, in 2008 and 2011,\\nrespectively.\\nIn 2008, he was a visiting researcher at the\\nUniversity of Toronto, Toronto, ON, Canada. He\\nis currently a Postdoctoral Researcher with theUniversity of Toronto and KAIST. His research\\ninterests include face recognition/detection/tracking,\\nmedical image processing, pattern recognition, machine learning, computer\\nvision, and the Social Web. He is the author or co-author of over 30 refereed\\nresearch publications in the aforementioned research areas.\\nDr. Choi was the recipient of the Samsung HumanTech Thesis Prize in 2010.\\n1380 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 21, NO. 3, MARCH 2012\\nYong Man Ro (S’85–M’92–SM’98) received\\nthe B.S. degree from Yonsei University, Seoul,\\nKorea, and the M.S. and Ph.D. degrees from Korea\\nAdvanced Institute of Science and Technology\\n(KAIST), Daejeon, Korea.\\nIn 1987, he was a Visiting Researcher with Co-\\nlumbia University, NY , and from 1992 to 1995, he\\nwas a Visiting Researcher with the University of Cali-\\nfornia, Irvine, and KAIST. He was a Research Fellow\\nwith the University of California, Berkeley, and a Vis-\\niting Professor with the University of Toronto in 1996\\nand 2007, respectively. He is currently a Full Professor with KAIST, where he\\nis directing the Image and Video Systems Laboratory. He participated in the\\nMPEG-7 and MPEG-21 international standardization efforts, contributing to the\\ndeﬁnition of the MPEG-7 texture descriptor, the MPEG-21 Digital Item Adapta-tion visual impairment descriptors, and modality conversion. His research inter-\\nests include image/video processing, multimedia adaptation, visual data mining,\\nimage/video indexing, and multimedia security.\\nDr. Ro was a recipient of the Young Investigator Finalist Award of the In-\\nternational Society for Magnetic Resonance in Medicine in 1992 and the Sci-\\nentist Award in Korea in 2003. He served as a TPC member of international\\nconferences such as the International Workshop on Digital forensics and Wa-termarking (IWDW), the International Workshop on Image Analysis for Mul-\\ntimedia Interactive Services, AIRS, and the IEEE Consumer Communications\\nand Networking Conference, and he was the co-program chair of IWDW 2004.\\nKonstantinos N. (Kostas) Plataniotis(S’90–M’92–SM’03) received the B.Eng. degree\\nin computer engineering from the University of\\nPatras, Patras, Greece, in 1988 and the M.S and\\nPh.D. degrees in electrical engineering from Florida\\nInstitute of Technology, Melbourne, in 1992 and1994, respectively.\\nHe is a Professor with The Edward S. Rogers Sr.\\nDepartment of Electrical and Computer Engineering,\\nUniversity of Toronto in Toronto, ON, Canada, and an\\nAdjunct Professor with the School of Computer Sci-\\nence, Ryerson University, Canada. He is the Director of the Knowledge Media\\nDesign Institute with the University of Toronto, where he is also the Director of\\nResearch for the Identity, Privacy and Security Institute. His research interests\\ninclude biometrics, communications systems, multimedia systems, and signaland image processing.\\nDr. Plataniotis is the Editor-in-Chief (2009–2011) for the IEEE S\\nIGNAL\\nPROCESSING LETTERS , a registered professional engineer in the province of\\nOntario, and a member of the Technical Chamber of Greece. He is the 2005\\nrecipient of the IEEE Canada’s Outstanding Engineering Educator Award “for\\ncontributions to engineering education and inspirational guidance of graduate\\nstudents” and the corecipient of the 2006 IEEE T RANSACTIONS ON NEURAL\\nNETWORKS Outstanding Paper Award for the published paper in 2003 entitled\\n\"Face Recognition Using Kernel Direct Discriminant Analysis Algorithms.\\n',\n",
       " 'Deep Learning based FACS Action Unit Occurrence and Intensity\\nEstimation\\nAmogh Gudi, H. Emrah Tasli, Tim M. den Uyl, Andreas Maroulis\\nVicarious Perception Technologies, Amsterdam, The Netherlands\\nAbstract — Ground truth annotation of the occurrence and\\nintensity of FACS Action Unit (AU) activation requires great\\namount of attention. The efforts towards achieving a common\\nplatform for AU evaluation have been addressed in the FG 2015\\nFacial Expression Recognition and Analysis challenge (FERA\\n2015). Participants are invited to estimate AU occurrence\\nand intensity on a common benchmark dataset. Conventional\\napproaches towards achieving automated methods are to train\\nmulticlass classiﬁers or to use regression models. In this paper,\\nwe propose a novel application of a deep convolutional neural\\nnetwork (CNN) to recognize AUs as part of FERA 2015\\nchallenge. The 7 layer network is composed of 3 convolutional\\nlayers and a max-pooling layer. The ﬁnal fully connected layers\\nprovide the classiﬁcation output. For the selected tasks of the\\nchallenge, we have trained two different networks for the two\\ndifferent datasets, where one focuses on the AU occurrences\\nand the other on both occurrences and intensities of the AUs.\\nThe occurrence and intensity of AU activation are estimated\\nusing speciﬁc neuron activations of the output layer. This way,\\nwe are able to create a single network architecture that could\\nsimultaneously be trained to produce binary and continuous\\nclassiﬁcation output.\\nI. INTRODUCTION\\nA picture is worth a thousand words, but how many\\nwords is the picture of a face worth? As humans, we make\\na number of conscious and subconscious evaluations of a\\nperson just by looking at their face. Identifying a person\\ncan have a deﬁning inﬂuence on our conversation with them\\nbased on past experiences; estimating a person’s age, and\\nmaking a judgement on their ethnicity, gender, etc. makes\\nus sensitive to their culture and habits. We also often form\\nopinions about that person (that are often highly prejudiced\\nand wrong); we analyse his or her facial expressions to\\ngauge their emotional state (e.g. happy, sad), and try to\\nidentify non-verbal communication messages that they intent\\nto convey (e.g. love, threat). We use all of this information\\nwhen interacting with each other. In fact, it has been argued\\nthat neonates, only 36 hours old, are able to interpret some\\nvery basic emotions from faces and form preferences [1]. In\\nolder humans, this ability is highly developed and forms one\\nof the most important skills for social and professional inter-\\nactions. Indeed, it is hard to imagine expression of humour,\\nlove, appreciation, grief, enjoyment or regret without facial\\nexpressions.\\nHuman face constantly conveys information, both con-\\nsciously and subconsciously. However, as basic as it is for\\nhumans to visually interpret this information, it is quite a\\nbig challenge for machines. Such studies have been proven\\nimportant in many different ﬁelds like psychology [2], hu-mancomputer interaction [3], visual expression monitoring\\n[4]- [5] and market research [6]. Conventional semantic facial\\nfeature recognition and analysis techniques mostly suffer\\nfrom lack of robustness in real life scenarios.\\nThis paper proposes a method to interpret semantic infor-\\nmation available in faces in an automated manner without\\nrequiring manual design of feature detectors, using the\\napproach of Deep Learning. FG 2015 Facial Expression\\nRecognition and Analysis challenge invites researchers to\\ndevelop methods to estimate facial AU occurrence and inten-\\nsity. The proposed network architecture in this study has been\\npreviously developed for detecting facial semantic features\\n(like emotions, age, gender, ethnicity etc.) present in faces\\n[7]. The same architecture is trained for the given tasks using\\nthe ground truth datasets.\\nIn the network architecture development, effects of various\\nhyper-parameters of deep neural networks have been inves-\\ntigated towards achieving an optimal conﬁguration. Further-\\nmore, the relation between the effect of high-level concepts\\non low level features is explored through an analysis of\\nthe similarities in low-level descriptors of different semantic\\nfeatures.\\nII. RELATED WORK\\nThere has been previous efforts on detecting the facial AU\\noccurrence as well as the intensity. The study in [8] utilizes\\nthe facial landmark displacement as a measure of intensity\\ndynamics. Regression based models [9] and multiclass clas-\\nsiﬁers are also [10] utilized for such purposes.\\nLatest developments in computational hardware have re-\\ndirected attention to deep neural networks for many computer\\nvision tasks. In the light of such studies deep learning\\nmethods have been proposed as a highly promising approach\\nin solving such facial semantic feature recognition tasks. A\\nrecent work for facial recognition by Taigman et al. [11]\\nhas shown near-human performance using deep networks.\\nThanks to the use of pre-processing steps like face alignment\\nand frontalization, and the use of a very large dataset, a\\nrobust and invariant classiﬁer is produced that sets the state-\\nof-the-art in the Labeled Faces in the Wild dataset [12].\\nIn the task of emotion recognition from faces, Tang [13]\\nsets the state-of-the-art on the Facial Expression Recog-\\nnition Challenge-2013 (FERC-2013). This is achieved by\\nimplementing a two stage network: a convolutional network\\ntrained in a supervised way on the ﬁrst stage, and a Support\\nVector Machine as the second stage, which is trained on\\nthe output of the ﬁrst stage. The initial global contrast\\nnormalization has proven to be beneﬁcial and hence has also\\nbeen adopted as a pre-processing step in this paper.\\nIII. DATASETS\\nFERA 2015 challenge provides two ground truth datasets:\\nthe BP4D, and the SEMAINE Dataset. The BP4D Dataset\\ncontains occurrence annotations for Action Units 1, 2, 4, 6,\\n7, 10, 12, 14, 15, 17, 23, as well as intensity annotations for\\nAction Units 6, 10, 12, 14, 17. The SEMAINE data contains\\noccurrence annotations for Action Units 2, 12, 17, 25, 28,\\n45. Details of the dataset are explained in the FERA 2015\\nbaseline paper [14].\\nIV. METHOD\\nIn this section, we describe the proposed method used\\nto detect the occurrences and intensities of Facial AUs\\nfrom face images. The algorithmic pipeline depicted in 2 is\\nprimarily based on the master’s thesis work [7]. The whole\\npipeline is composed of the initial pre-processing step and\\nfollowed by the classiﬁcation step using the deep neural\\nnetwork.\\nA. Pre-Processing\\nThe AU classiﬁer (the deep network) is designed to accept\\nfrontal images of cropped faces as input. Therefore, we apply\\nan initial pre-processing step to detect the face and handle\\npose variations. In an effort to normalize the input intensity\\ndifferences due to lighting conditions, we also apply a global\\ncontrast normalization in this initial step. The basic pipeline\\nof these pre-processing steps are as follows:\\nFace Location Normalization :\\n\\x0fWe ﬁnd faces in the images using a face detection\\nalgorithm (using the facial points based on the method\\ndescribed in [15], and as provided by the challenge)\\nand extract the crop of the face such that the image is\\ncentered around the face.\\n\\x0fWe perform in-plane rotation in order to remove tilt in\\nthe X-Y plane. This is achieved by enforcing the line\\nconnecting the two eyes to be horizontal.\\n\\x0fWe resize the image such that the approximate scale\\nof the face is constant. This is done by ensuring the\\ndistance between the two eyes to be constant. The ﬁnal\\nresolution of the face-crop is set to 48\\x0248pixels.\\nGlobal Contrast Normalization :\\n\\x0fWe convert the face-crop to 32-bit grayscale images\\nto minimize the computational complexity of using\\nmultiple color channels.\\n\\x0fWe normalize the pixel values by the standard deviation\\nof pixel values (face-crop) in the whole video session,\\nas done for single images in [16].\\nThis pre-processing pipeline is illustrated in Figure 1.\\nIn addition, in order to increase the variance of training\\ndata (with the intention of making the network more robust),\\nwe add a horizontally mirrored copy of all training images.\\nInput Image Face LocalizationFace Alignment\\nFace Location Normalization\\nGlobal Contrast NormalizationMean Pixel \\nRemovalMean \\nPixelStandard \\nDeviation\\nStandard Deviation \\nDivisionGrayscale \\nConversion\\nFig. 1: The Pre-processing pipeline.\\nB. The Deep Neural Network\\nThe estimation of AUs from the pre-processed face-crops\\nis performed by a Deep Convolutional Neural Network.\\nIn our framework, the input image size of the network is set\\nto48\\x0248grayscale pixels arranged in a 2D matrix. This\\npre-processed image is fed to the ﬁrst hidden layer of the\\nnetwork: A convolutional layer with a kernel size of 5\\x025\\nhaving a stride of 1 both dimensions. The number of parallel\\nfeature-maps in this layer is 64. The 44\\x0244output image\\nproduced by this layer is then passed to a max-pooling layer\\n[17] of kernel size 3\\x023with a stride of 2 in each dimension.\\nThis results in a sub-sampling factor of 1=2, and hence the\\nresulting image is of size 22\\x0222. The second hidden layer is\\nalso a 64 feature-map convolutional layer with a kernel size\\nof5\\x025(and stride 1). The output of this layer is a 18\\x0218\\npixel image, and this feeds directly into the third hidden\\nlayer of the network, which is again a convolutional layer\\nof 128 feature maps with a kernel size of 4\\x024(and stride\\n1). Finally, the output of this layer, which is of dimension\\n15\\x0215, is fed into the last hidden layer of the network,\\na fully connected linear layer with 3072 neurons. Dropout\\ntechnique [18], [19] is applied to this fully connected layer,\\nwith a dropout probability of 0.2. The output of this layer is\\nconnected to the output layer, which is composed of either\\n11 or 6 neurons (11 for the BP4D database, and 6 for the\\nSEMAINE dataset), each representing one AU class label.\\nAll layers in the network are made up of ReLu units/neurons\\n[20]. This architecture is illustrated in Figure 2.\\nTraining of the deep neural network is done using stochas-\\ntic gradient descent with momentum in mini-batch mode,\\nwith batches of 100 data samples. Negative log-likelihood is\\nused as the objective function.\\nIt should be noted that for the BP4D dataset, while the\\nnetwork was trained on occurrence of AUs 1, 2, 4, 7, 15, 23,\\nit was also trained on the intensities of AUs 6, 10, 12, 14,\\n17. This way, the same network is able to participate in both\\nthe occurrence sub-challenge (for the BP4D dataset), as well\\nas the two intensity sub-challenges.\\n[48 x 48]\\nInput \\nLayerGlobal Contrast \\nNormalization\\n[3 x 3]\\nMax Pooling\\n(Stride of 2)\\n[5x5] x 64\\nConvolutional \\nLayer\\n[5x5] x 64\\nConvolutional \\nLayer[4x4] x 128\\nConvolutional \\nLayer\\n[3072]\\nFully Connected \\nLayer[11/6]\\nOutput Layer[3072] [15x15] [18x18] [22x22] [44x44] [48x48] [48x48]\\nFace Location \\nNormalization\\nInput \\nImageFig. 2: The architecture of the deep convolutional neural network.\\nThe training of the deep network involved over 140,000\\nimages of the BP4D dataset and 90,000 images of the\\nSEMAINE Dataset (excluding mirrored copies). Initially,\\n50% of these were used in the training sets, while the rest\\n(development sets) was used as the validation sets. However,\\nin order to make full use of the annotated data given, 100% of\\nthe data was included in the training sets in the ﬁnal versions\\nof the networks, which were trained for a ﬁxed number of\\nepochs (determined by validation set performance in previous\\ntraining experiments).\\nC. Post-Processing\\nThe raw output of the deep network (the activations of the\\nﬁnal layer) essentially denote the conﬁdence scores for the\\npresence/absence of each AU, where an extremely low value\\nrepresents a strong absence of an AU, while a high value\\nrepresents its strong presence. In an ideal case, the decision\\nthreshold of the deep network must lie on the mid-point of\\nvalues representing the ground-truth presence and absence\\nof an AU in the training set. However, many conditions\\ncan result in this threshold being skewed to one of the\\ntwo extremes (e.g., uneven distribution of occurrences in the\\ntraining set).\\nAs a work-around to this problem, we optimize the de-\\ncision threshold of the AUs on a validation set, i.e., we set\\nthe decision thresholds to a value that gives be highest F1\\nscore, when tested on a validation set. In our case, we ﬁrst\\nseparated the annotated dataset into two partitions: a training\\nset and a validation set (same as the development set by the\\nchallenge). Next, we train the network only on the training\\nset, and test all possible values of the decision threshold on\\nthe development validation set. Finally, we apply these best-\\nperforming thresholds as the decision thresholds for our ﬁnal\\nnetwork, which is trained on the complete set of provided\\ndata (including the development set).\\nV. RESULTS\\nThe training of these networks took roughly 15 hours each\\non a 1000+ core GPU. The learning curve and the weights\\nof the trained network (on the BP4D Dataset) can be seen\\nin Figure 3.\\n0.010.020.020.030.030.04\\n0 5 10 15 20 25Mean Squared Error\\nEpochsTrain Set\\nValid Set(a) Learning curve of the net-\\nwork w.r.t. mean squared error on\\nthe training and validation set.\\n(b) Visualization of the ﬁrst-\\nlayer convolutional feature-map\\nweights.\\nFig. 3: Deep neural network learning curve and weights\\ntrained on the BP4D Dataset.\\nBP4D Dataset SEMAINE Dataset\\nAction Unit F1 Score Action Unit F1 Score\\nAU01 0.399 AU02 0.372\\nAU02 0.346 AU12 0.707\\nAU04 0.317 AU17 0.067\\nAU06 0.718 AU25 0.602\\nAU07 0.776 AU28 0.040\\nAU10 0.797 AU45 0.257\\nAU12  0.793\\nAU14 0.681\\nAU15 0.235\\nAU17 0.368\\nAU23 0.309\\nMean 0.522 Mean 0.341\\nTABLE I: Occurance sub-challenge results for BP4D and\\nSEMAINE datasets.\\nAction Unit MSE PCC ICC\\nAU06 1.287 0.664 0.663\\nAU10 1.249 0.735 0.733\\nAU12 0.928 0.788 0.788\\nAU14 1.686 0.591 0.549\\nAU17 0.757 0.329 0.329\\nMean 1.181 0.621 0.613TABLE II: Fully automated intensity estimation sub-\\nchallenge results for the BP4D dataset.\\nAction Unit MSE PCC ICC\\nAU06 1.125 0.423 0.423\\nAU10 0.963 0.555 0.543\\nAU12 0.803 0.632 0.613\\nAU14 1.554 0.533 0.495\\nAU17 1.198 0.244 0.219\\nMean 1.129 0.478 0.459\\nTABLE III: Pre-segmented intensity estimation sub-\\nchallenge results for the BP4D dataset.\\nThe performance of the network on the test set on the\\nthree sub-challenges of FERA-2015 are presented in Tables\\nI,II and III. The evaluation methods used in these results\\nare the F1 Score, the Mean Squared Error (MSE), Pearsons\\nCorrelation Coefﬁcient (PCC) and the Intra-class Correla-\\ntion Coefﬁcient (ICC). The sub-challenges, test set and the\\nevaluation measure are explained in [14].\\nAs can be seen, our deep network performs with a mean\\nF1 score of 0.522 on the BP4D test set, and 0.341 on the SE-\\nMAINE dataset in the occurrence sub-challenge. The method\\ngives an average mean squared error of 1.181 and 1.129\\non the fully automated and pre-segmented intensity sub-\\nchallenges respectively. Additionally, on the BP4D dataset,\\nit was observed that video-segment global contrast normal-\\nization pre-processing step contributes an improvement of\\n0.051 to the ﬁnal F1 score, while the post-processing step\\nimproves the ﬁnal F1 score by 0.113.\\nIt can also be seen that the performance of our method\\non the SEMAINE dataset was much lower than on the\\nBP4D dataset. One of the main contributing factors to this\\nobservation is the low number of individual faces included\\nin the SEMAINE training data (31 people), as compared\\nto the BP4D training data (41 individuals). In fact, we have\\nbeen able to attain much higher performance for emotion and\\nfacial characteristics estimation in previous experiments [7],\\nwhere the same network was trained on datasets containing\\nmore than 200 individuals. This suggests that the deep\\nnetwork ends up over-ﬁtting by learning the identities of the\\nindividuals in the training set.\\nSome classiﬁcation examples (from the validation set) by\\nthe network can be viewed in Figure 4.\\nAU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23\\nAb Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab\\nAb Ab Ab Ab Ab Ab Ab Ab Ab Ab Ab\\nAU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23\\nAb Pr Ab C Pr D Ab Ab Ab Ab Ab\\nAb Ab Ab D Pr D D C Ab Ab Ab\\nAU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23\\nAb Ab Pr Ab Pr C Ab Ab Pr C Ab\\nAb Ab Pr B Pr C Ab C Pr A Pr\\nAU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23\\nAb Ab Ab B Pr D B B Pr Ab Ab\\nPr Ab Pr C Pr D C C Ab Ab Pr\\nAU1 AU2 AU4 AU6 AU7 AU10 AU12 AU14 AU15 AU17 AU23\\nAb Ab Ab B Ab C B Ab Ab B Pr\\nAb Ab Pr C Pr C C C Pr A PrFig. 4: Example AU estimation of images from the BP4D\\nDataset. The ﬁrst row corresponds to the ground truth, and\\nthe second row corresponds to the AU estimation by the\\nproposed method.\\nVI. CONCLUSIONS AND FUTURE WORKS\\nA. Conclusions\\nIn this paper, a deep learning approach has been demon-\\nstrated for detecting the occurrence and estimating the inten-\\nsity of facial AUs. This approach is primarily based on the\\nuse of convolutional neural networks on two dimensional\\npre-processed and aligned images of faces. Deep convolu-\\ntional networks have been recently very successful in very\\ndifferent visual tasks. That has been the main motivation of\\nutilizing it in this speciﬁc challenge. However, the results\\nhave not been as promising as expected. We believe that this\\nis due to the fact that the deep learning techniques depend\\non being trained on big datasets with high variation in the\\ndata. In the datasets supplied in the challenge, there is little\\nvariation in terms of the number of individuals and this can\\ncause over ﬁtting of the network on the individual faces in\\nthe dataset.\\nB. Future Works\\nWe have observed the following items as potential future\\nwork:\\n\\x0fThis framework proposed in this paper does not take\\ninto account the temporal dimension of the input data\\n(frames of videos). Assuming time-domain information\\ncould improve the results, 3-D convolutional networks\\nas proposed in [21], could readily ﬁt into our frame-\\nwork.\\n\\x0fMore extensive experimentation with alternative pre-\\nprocessing techniques could be carried out. Methods\\nlike whitening are used in a wide range of machine\\nlearning tasks to reduce redundancy, and could be used\\nto aid deep networks as well.\\n\\x0fA limiting factor in the conducted experiments is the\\navailable computational resources. This calls for more\\nexperimentation on larger networks, as the true optimal\\nperformance of these networks can only be achieved\\nafter extending the upper bound restrictions on the\\nnetwork size.\\nREFERENCES\\n[1] Teresa Farroni, Enrica Menon, Silvia Rigato, and Mark H Johnson,\\n“The perception of facial expressions in newborns,” European Journal\\nof Developmental Psychology , vol. 4, no. 1, pp. 2–13, 2007.\\n[2] Paul Ekman, Wallace V Freisen, and Sonia Ancoli, “Facial signs of\\nemotional experience.,” Journal of personality and social psychology ,\\nvol. 39, no. 6, pp. 1125, 1980.\\n[3] Maja Pantic and Leon JM Rothkrantz, “Toward an affect-sensitive\\nmultimodal human-computer interaction,” Proceedings of the IEEE ,\\nvol. 91, no. 9, pp. 1370–1390, 2003.\\n[4] H. Emrah Tasli and Paul Ivan, “Turkish presidential elections\\ntrt publicity speech facial expression analysis,” in arXiv preprint\\narXiv:1408.3573 , 2014.\\n[5] H Emrah Tasli, Amogh Gudi, and Marten Den Uyl, “Integrating\\nremote ppg in facial expression analysis framework,” in ACM\\nInternational Conference on Multimodal Interaction (ICMI) . ACM,\\n2014, pp. 74–75.\\n[6] Thales Teixeira, Michel Wedel, and Rik Pieters, “Emotion-induced\\nengagement in internet video advertisements,” Journal of Marketing\\nResearch , vol. 49, no. 2, pp. 144–159, 2012.\\n[7] Amogh Gudi, “Recognizing semantic features in faces using deep\\nlearning,” M.S. thesis, Faculty of Science, University of Amsterdam,\\n2014.\\n[8] Michel Valstar and Maja Pantic, “Fully automatic facial action unit\\ndetection and temporal analysis,” in Computer Vision and Pattern\\nRecognition Workshop, 2006. CVPRW’06. Conference on . IEEE, 2006,\\npp. 149–149.\\n[9] Arman Savran, Bulent Sankur, and M Taha Bilge, “Regression-\\nbased intensity estimation of facial action units,” Image and Vision\\nComputing , vol. 30, no. 10, pp. 774–784, 2012.\\n[10] Mohammad H Mahoor, Steven Cadavid, Daniel S Messinger, and\\nJeffrey F Cohn, “A framework for automated measurement of the\\nintensity of non-posed facial action units,” in Computer Vision and\\nPattern Recognition Workshops, 2009. CVPR Workshops 2009. IEEE\\nComputer Society Conference on . IEEE, 2009, pp. 74–80.\\n[11] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf,\\n“Deepface: Closing the gap to human-level performance in face\\nveriﬁcation,” in Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition , 2013, pp. 1701–1708.\\n[12] Gary B Huang and Erik Learned-Miller, “Labeled faces in the wild:\\nUpdates and new reporting procedures,” .\\n[13] Yichuan Tang, “Deep learning using linear support vector machines,”\\ninWorkshop on Challenges in Representation Learning, ICML , 2013.\\n[14] Michel Franc ¸ois Valstar, “Fera 2015 - second facial expression\\nrecognition and analysis challenge,” in Automatic Face & Gesture\\nRecognition and Workshops (FG 2015), 2015 IEEE International\\nConference on . IEEE, 2015.\\n[15] Xuehan Xiong and Fernando De la Torre, “Supervised descent method\\nand its applications to face alignment,” in Computer Vision and Pattern\\nRecognition (CVPR), 2013 IEEE Conference on . IEEE, 2013, pp. 532–\\n539.\\n[16] Adam Coates, Andrew Y Ng, and Honglak Lee, “An analysis of single-\\nlayer networks in unsupervised feature learning,” in International\\nConference on Artiﬁcial Intelligence and Statistics , 2011, pp. 215–\\n223.\\n[17] Yann LeCun, Koray Kavukcuoglu, and Cl ´ement Farabet, “Convolu-\\ntional networks and applications in vision,” in Circuits and Systems\\n(ISCAS), Proceedings of 2010 IEEE International Symposium on .\\nIEEE, 2010, pp. 253–256.\\n[18] Nitish Srivastava, Improving neural networks with dropout , Ph.D.\\nthesis, University of Toronto, 2013.\\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “Imagenet\\nclassiﬁcation with deep convolutional neural networks,” in Advances\\nin neural information processing systems , 2012, pp. 1097–1105.\\n[20] Vinod Nair and Geoffrey E Hinton, “Rectiﬁed linear units improve re-\\nstricted boltzmann machines,” in Proceedings of the 27th International\\nConference on Machine Learning (ICML-10) , 2010, pp. 807–814.\\n[21] Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia,\\nand Atilla Baskurt, “Sequential deep learning for human action\\nrecognition,” in Human Behavior Understanding , pp. 29–39. Springer,\\n2011.\\n',\n",
       " '1 \\nDeep Neural Networks with Relativity Learning \\nfor Facial Expression Recognition \\n! ! 2 • 3 Y L\\'! D h T 3 Yanan Guo , Dapeng Tao, Jun Yu ,Hao XIOng , aotang 1, ac eng ao \\n\\'Yunnan University, China, 2Hangzhou Dianzi University, China, 3University of Tchnology Sydney, \\nAustralia \\nY ananguo. YNU@qq.com, dapeng.tao@gmail.com, yujun@hdu.edu.cn, hao.xiong@student.uts.edu.au, \\nIiyaotang@ynu.edu.cn, dacheng.tao@uts.edu.au. \\nABSTRACT \\nFacial expression recogmtlOn aims to classify facial \\nexpression as one of seven basic emotions including \"neutral \". \\nThis is a difficult problem due to the complexity and subtlety \\nof human facial expressions, but the technique is needed in \\nimportant applications such as social interaction research. \\nDeep learning methods have achieved state-of-the -art \\nperformance in many tasks including face recognition and \\nperson re- identification. Here we present a deep learning \\nmethod termed Deep Neural Networks with Relativity \\nLearning (DNNRL) , which directly learns a mapping from \\noriginal images to a Euclidean space, where relative distances \\ncorrespond to a measure of facial expression similarity. \\nDNNRL updates the model parameters according to sample \\nimportance. This strategy results in an adjustable and robust \\nmodel. Experiments on two representative facial expression \\ndatasets (FER-2013 and SFEW 2.0) are performed to \\ndemonstrate the robustness and effectiveness ofDNNRL. \\nIndex Terms- Facial expression, deep feature learning, \\nconvolutional neural network, social interaction. \\n1 INTRODUCTION \\nHuman emotion recognition is attracting attention due to its \\nmany practical applications in, for example, social interactions, \\nhuman-robot interactions, and call center systems. Facial \\nexpression recognition (FER), a basic part of motion analysis, \\ncan be used to recognize internal human emotions. FER \\nmethods attempt to classify facial expression in a given image \\nor sequence of images as one of six basic emotions (anger, \\ndisgust, fear, happiness, sadness, and surprise) or as \"neutral \" \\n[4]. Although much effort and some progress has been made \\nin the field, accurately recognizing facial expression remains \\ndifficult due to the complexity and subtlety of facial \\nexpressions and how they relate to emotion. \\nMost FER systems involve three main steps: face image \\npre-processing, feature extraction, and classification. Deriving an effective and robust facial representation from raw face \\nimages is extremely important for FER. In general, methods \\nused to extract facial representations can be grouped into two \\ncategories: action unit (AU)-based methods and \\nappearance- based methods. AU-based methods, originally \\nproposed by Ekman et al. [4], detect the presence of individual \\nAUs which are then extracted to form a feature based on their \\ncombinations. However, each AU detector usually requires \\ncareful hand engineering (such as through reliable facial \\ndetection and tracking) to ensure good performance, which is \\nhard to accommodate in many situations. Appearance- based \\nmethods [15] represent an individual\\'s emotion from their \\nfacial shape and texture. Appearance-based methods apply \\ntraditional handcrafted features such as Gabor wavelets [10], \\nLocal Binary Pattern (LBP) features [12], and Motion History \\nImages (MHIs) [17] to either specific face regions or the \\nwhole face to extract the facial appearance changes; these \\nmethods have achieved impressive performance on several \\naccepted FER benchmarks. However, most handcrafted \\nfeatures are limited by not being directly applicable to \\npractical problems and lack generalizability when applied to \\nnovel data. \\nA number of well-established problems in computer vision \\nhave recently benefited from the rise in deep learning as \\nappearance- based feature representations or classifiers. Deep \\nlearning methods have boosted performance in a variety of \\ntasks including face recognition, speech recognition, and \\nobject detection. However, most deep learning methods \\nminimize cross-entropy loss and employ the softmax \\nactivation function for prediction. Thus, when updating the \\nmodel, they treat all samples equally and do not consider the \\nfact that giving difficult samples more weight could learn a \\nmore robust model. \\nHere we present a scheme termed Deep Neural Networks \\nwith Relativity Learning (DNNRL) for FER. DNNRL directly \\nlearns a mapping from original images to a Euclidean space, \\nwhere relative distances correspond to a measure of facial \\nexpression similarity. Furthermore, DNNRL updates the \\nmodel according to sample importance, leading to a more \\nadjustable and robust model. We conduct extensive \\nexperiments on two well-known facial expression datasets \\n(FER-2013 and SFEW 2.0) and obtain results that are \\nsignificantly better than traditional deep learning or other \\nstate-of-the-art methods. \\nThe remainder of the paper is organized as follows: in \\nSection 2, we briefly review related works about feature \\nrepresentation using appearance-based methods for FER \\nproblems. We detail the newly proposed DNNRL in Section 3. \\nThe experimental results on the two representative datasets are \\npresented in Section 4, and we conclude in Section 5. \\n2 RELATED WORK \\nIn general, feature representation using appearance -based \\nmethods can be group into two main categories: handcrafted \\nfeatures and deep features. \\nGabor-wavelet representations have been extensively used \\nin face image analysis and show promising performance. \\nHowever, Gabor-wavelet representations are computationally \\ncostly. Bartlett et al. [2] showed that Gabor-wavelet \\nrepresentations derived from every 48 x 48 face image have \\nthe high dimensionality of 0(105). LBP describes local \\ntexture variation and is often used with a holistic \\nrepresentation. Shan et al. [14] observed that LBP features \\nperform robustly and stably over a range of low-resolution \\nimages, and the method showed promising performance in \\nlow-resolution video sequences captured in practice. \\nDeep learning has also recently obtained state-of-the -art \\nperformance for FER problems as an appearance-based feature \\nrepresentation or classifier. Khorrami et al. [7] showed that \\nconvolutional neural networks (CNNs) are effective, and they \\nintroduced a method to decipher which part of the face image \\ninfluences the CNN\\'s predictions. Mollahosseini et al. [11] \\nproposed a deep learning architecture consisting of two \\nconvolutional layers, each followed by max pooling layers and \\nfour Inception layers, to address the FER problem across \\nmUltiple representative face datasets. By minimizing a \\nmargin-based loss rather than the cross-entropy loss, Tang et \\nal. [16] demonstrated that switching from softmax to SVM is \\nsimple and beneficial for classification. \\n3. DEEP NEURAL NETWORKS WITH RELATIVITY \\nLEARNING \\nDeep learning methods have, therefore, achieved excellent \\nperformance in many applications including face recognition \\nand person re- identification. However, when updating the \\ndeep learning model, they treat all samples equally and do not \\nconsider that giving difficult samples more weight learns a \\nmore robust model. In this section, we present a novel deep \\nfeature learning method, DNNRL, which can update the \\nmodel according to sample importance. \\n3.1 The Network Architecture \\nThe overview of the network architecture is shown in \\nFigure 1. The network consists of three elements. First, the \\nnetwork contains three convolutional layers followed by one 2 \\nmax pooling layer. Following these layers, three inception \\nmodules are added consisting of 1 xl, 3 x 3, and 5 x 5 \\nconvolution layers in parallel. Each inception module is \\nfollowed by a pooling layer. The final layer is a fully \\nconnected layer with normalization [8] and dropout [6]. The \\nInception layer allows for improved recognition of local facial \\nfeatures, since smaller convolutions are applied to local \\nfeatures while larger convolutions approximate global features; \\nlooking at local features including the mouth and eyes allows \\nhumans to easily distinguish most emotions [1]. Normalization \\nensures that the relative distance derived from two images has \\nan upper bound. Dropout is a statistical randomness that \\nreduces the risk of network overfitting. The non-linear \\nactivation functions for all convolutional layers are set as \\nrectified linear units (ReLUs) [8], and using the ReLU \\nactivation function avoids the vanishing gradient problem \\ncaused by some other activation functions. Finally, Batch \\nnormalizations [9] are used for all convolutional layers to be \\nless careful about initialization and use much higher learning \\nrates. The network configuration is shown in Table 1. \\nTo train a more robust model, data are pre- processed as \\nfollows. The images are first resized to 96x96. Next, we \\nrandomly scale the resized images, where the range of scaling \\nis [-lO, lO] . We then pad 0 round scaled images to the size of \\n120x 120. Finally, we rotate the obtained images randomly in \\nthe range of [-15°,15°] and crop these images randomly to \\n108 xl 08. The random rotation generates additional unseen \\nsamples and, therefore, makes the network even more robust \\nto deviated and rotated faces. The number of images is \\nenlarged by a factor of lO. Thus, the images are normalized to \\na standard size of 108 xl 08, and this manipulation causes \\nshape distortion that can train a more accurate model. The top \\nand the third rows in Figure 2 give 14 examples of raw faces, \\nwhile the second and the fourth rows show their corresponding \\npre-processed faces. Each column represents the same \\nexpression: from left to right, angry, disgust, fear, happy, sad, \\nsurprise, and neutral. \\nFigure 2. Examples of pre-processed faces. The first and the \\nthird rows show the original faces, while the second and the \\nfourth rows show their corresponding pre-processed faces. \\nEach column represents the same expression. From left to \\nright: angry, disgust, fear, happy, sad, surprise, and neutral. \\n, \\n, \\n\\\\ \\n, \\n, \\n, \\n, \\n\" \\n, \\n, \\nFigure 1. The network architecture. \\nTable 1. Network configuration \\nLayer type Patch Output Size/Stride \\nconvolution 3x3/2 54x54x32 \\nconvolution 3x311 54x54x32 \\nconvolution 3x311 54x54x64 \\nmax pooling 3x3/2 27x27x64 \\ninception(3a) - 27x27x256 \\nmax pooling 3x3/2 13x13x256 \\ninception( 4a) - 13x13x288 \\nmax pooling 3x3/2 6x6x288 \\ninception(5a) - 6x6x288 \\nave pooling 3x3/3 2x2x288 \\ndropout(20% ) - 2x2x288 , \\n\\\\ \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\nI \\n, \\n, Inception \\n, \\n\\\\ \\nInception \\n, \\n, \\n, \\n\\\\ \\nI \\nI \\nInception \\nI \\nI \\nI \\nI \\nI \\n, \\n, \\nParams \\n288 \\n9K \\n18K \\n-\\n203K \\n-\\n823K \\n-\\n926K \\n-\\n-Ops \\n839K \\n26M \\n53M \\n-\\n148M \\n-\\n139M \\n-\\n33M \\n-\\n-3 \\nlinear lxlx512 589K 589K \\n3.2 Loss Function and Optimization \\nGiven a network and treating it as an end-to-end system, \\ntriplet loss [13] can be directly applied to the practical \\nproblem of expression recognition; that is, triplet loss training \\naims to learn an optimal feature representation that maximizes \\nthe relative distance. However, when updating the model, all \\nsamples are treated equally and giving difficult samples more \\nweight to learn a more robust model is not considered. Here \\nwe utilize the exponential function to address this problem. \\nTake a set of triplets P = {Pi}\\' Pi = {(Pi\\' P;, p;)} , \\nwhere Pj and P; are images with the same expression and \\nPi and P; are images with different expressions. Let \\nf ( x) denote the network output of image x; that is, \\nf ( x) is the embedded feature representation of image x. \\nWe want to learn effective features f (Pi)\\' f (p;), and \\nf (p;) to ensure that f (Pi) is closer to all other features \\nf (p;) of the same expression than to any feature f (p;) \\nof any other expression; we are, therefore, prone to giving \\ndifficult samples more weight when updating the model. Thus, \\nthe loss function is \\nL = I L. \\n1 \\n(pj,p; ,pi )EP \\n(1) \\nwhere the fixed scalar y 2: 0 is a learning margin to prevent \\nlearning very difficult samples (the relative distance between \\nIlf (Pi) -f (p; )112 and Ilf (Pi) -f (p; )112 is smaller than \\nr ), and a is a tradeoff parameter. \\nFigure 3 shows (. The smaller the relative distance, the \\nlarger the gradient of Li; that is, we are biased to updating the \\nmodel from difficult samples, and when the relative distance is \\nlarge (the relative distance between Ilf (Pi) -f (p; )112 and \\nIlf (Pi) -f (p; )112 is large), the gradient of Li is close to \\no and the model is only updated slightly when learning simple \\nsamples. \\nloss \\nrelative distance \\nFigure 3. The exponential triplet-based loss function. The x \\ncoordinate is the relative distance and the y coordinate is the \\ncorresponding loss. The absolute value of the gradient of loss \\ndecreases as the relative distance increases. \\nThe gradients with respect to f (Pi)\\' f (p;), or f (p;) \\nare \\nwhere � -2a� [(f (Pi) -f (p;)) -(f (Pi) -f (p; ))] \\n,llf (Pi) -f (p; )112 -Ilf (Pi) -f (p;)112 > r \\n0, Ilf (Pi) -f (p;)W -Ilf (pJ -f (p;)112 ::; r \\n-2a�(f(p;) -f(P;))\\' \\n,llf(Pi) -f(P;)W -llf(Pi) -f(p;)112 > r \\n0, Ilf (Pi) -f (p;)W -Ilf (Pi) -f (p; )112 ::; r \\n(2) \\n-2a�(f(pJ -f(P;))\\' \\n,llf(Pi) -f(p;)112 -llf(Pi) -f(P;)W > r \\no,llf(Pi)-f(p;)W -llf(Pi)-f(p;)112::; r \\n(3) \\n-2a�(f(p;) -f(pJ), \\n,llf(Pi) -f(P;)W -llf(Pi) -f(p;)112 > r \\n0, Ilf (Pi) -f (p; )112 -Ilf (pJ -f (p; )112 ::; r \\n(4) \\ndenotes e-allf(pt)-f(Pill! \\' -llf(Pt)-f(P;lI!\\' -y. Hence, the \\nloss function in (1) can be easily integrated in back \\npropagation in neural networks. \\n4. EXPERIMENT AL RESULTS \\nWe conducted facial expression recognition experiments on \\ntwo widely used datasets, the FER-2013 dataset [5] and the \\nSFEW 2.0 dataset [3], to demonstrate the effectiveness of the \\nproposed method. The FER-2013 dataset contains 27,809 \\ntraining images, 3,589 validation images, and 3,589 test 4 \\nimages. Faces are labeled with any of the six basic expressions \\nor neutral: the number of images representing the six basic \\nexpressions (anger, disgust, fear, happiness, sadness, and \\nsurprise) and neutral are 4953, 547, 5121, 8989, 6077, 4002, \\nand 6189, respectively. This dataset was created using the \\nGoogle image search API and contains large variations \\nreflecting real-world conditions. The images are 48x48 \\npixels. \\nThe SFEW 2.0 dataset was created from Acted Facial \\nExpressions in the Wild (AFEW) [3] using the key-frame \\nextraction method. It contains 880 training samples, 383 \\nvalidation samples, and 372 test samples. Since SFEW 2.0 is a \\ncompetition dataset in the Wild Challenge (EmotiW) 2015, \\ntest sample labeling is private and, therefore, unavailable. \\nFurthermore, SFEW 2.0 is relatively small, rendering it prone \\nto overfitting when trained on the DNNRL. Hence, we \\npre-trained the model on the FER-2013 training set and fine \\ntuned on the SFEW 2.0 and FER-2013 training sets; the test \\nsamples are FER-2013 test images. The SFEW 2.0 dataset \\nincludes different head poses, occlusions, backgrounds, and \\nclose to real- world illuminations; thus, face alignment is \\nnecessary to extract face-related information and for unifying \\nall face images to the same domain. We used SDM [18] for \\nface alignment, transformed the aligned faces to grayscale, \\nand resized to 48x48, which is the same as FER-2013 data. \\nFigure 4 shows examples of face alignment by SDM, where \\nthe top row shows original images and the bottom row shows \\ntheir corresponding aligned faces. \\nFigure 4. Examples of face alignment by SDM, where the top \\nrow shows original images and the bottom row shows their \\ncorresponding aligned faces. \\nWe first normalized the images in the above two datasets to \\n96 x 96 and then pre-processed these images using the \\nmanipulation described in Section 3.1. After obtaining deep \\nfeatures from the DNNRL model, to make a class choice, we \\nused a k-NN classifier to obtain the recognition accuracy, \\nwhere any test sample was classified by a majority vote of its \\nk training samples, with the test sample being assigned to the \\nclass most common among its k nearest training samples. Use \\nof the k-NN classifier is reasonable, because the DNNRL loss \\nis also based on Euclidean distances. We determined the value \\nof k for sample testing when the validation samples obtained \\nthe highest accuracy on the value of k. \\nThe initial learning rate was set to 0.01, while the minimum \\nlearning rate was set to 0.000l. Each training epoch had \\n[N / 128] batches, with the training samples randomly \\nselected from the training set. The learning margin y was set \\nto 0, and the tradeoff parameter a was set to 0.2. The trained \\nnetwork parameters and loss at each epoch were recorded. The \\nvalidation loss was assessed after each round of training; if the \\nvalidation loss increased by more than 10%, the learning rate \\nwas reduced by one-tenth and the previous network with the \\nbest validation loss was reloaded. The termination criterion for \\nthe training model was judged by the new lowest value for the \\nvalidation loss in 20 iterations. \\n4.1 Performance on FER-2013 \\nThe perfonnance of DNNRL and baselines on FER-2013 are \\nshown in Table 2. \"DNN\" refers to the accuracy of the \\nproposed network described in Section 3.1 with softmax, and \\n\"T-DNN\" refers to the accuracy of the proposed DNNRL with \\nthe k-NN classifier, where the loss function of DNNRL is \\nreplaced by the triplet-based loss. \"DNNRL \" refers to the \\naccuracy of the proposed DNNRL with the k-NN classifier. \\nAlexNet [8] and FER-2013 Champion [16] results are also \\nlisted. From Table 2, it can be seen that DNNRL obtains the \\nhighest recognition rate of the tested methods, demonstrating \\nits effectiveness and robustness for FER. DNN outperfonns \\nAlexNet because the Inception layer improves local feature \\nrecognition. T-DNN outperforms DNN because it can directly \\nlearn a mapping from original images to a Euclidean space, \\nwhere relative distances correspond to a measure of facial \\nexpression similarity. DNNRL outperforms T-DNN because it \\nupdates the model more or less according to the sample \\ndifficulty. \\n0.11 0.02 0.12 0.01 0.10 \\n0.02 0.00 0.00 0.04 \\n0.20 0.07 0.06 \\n0.03 \\n0.01 \\n0.00 0.09 \\n0.00 0.05 0.04 \\n�\\'\" % 1> .r� .r« \\n\"�Il �& .... �\".rt � .... 0:\" �\". v .... �/ &.r.r ;s\\'& \\nFigure 5. DNNRL classification confusion matrices on \\nFER-2013 test set (trained on FER-2013 training set). \\nThe confusion matrix of DNNRL on the FER-2013 dataset \\n(trained on FER-2013 training set) is shown in Figure 5. The \\nconfusion matrix between the ground-truth class label and the \\nmost likely inferred class label information provide a better \\nunderstanding of DNNRL\\'s limitations. As expected, \\nconfusion frequently occurs between \"anger\", \"fear\", and \\n\"sadness \" because they create similar motions. The confusion \\nmatrices also show that \"natural \" is easily misclassified as \\n\"sadness \". 5 \\nTable 2. Classification accuracy of the proposed DNNRL with \\nother methods on the FER-2013 test set (trained on FER-2013 \\ntraining set). \\nAccu DNNRL T-DNN DNN AlexNet FERwin \\n[16] \\nTest 0.7060 0.7013 0.6922 0.6482 0.693 \\n4.2 Performance on SFEW 2.0 and FER-2013 datasets \\nThe performance of DNNRL and baselines on FER-2013 \\n(trained on SFEW 2.0 and FER-2013 training sets) are shown \\nin Table 3. The baselines include AlexNet [8], DNN, and \\nT-DNN. DNNRL obtains the highest recognition rate of the \\ntested methods and outperforms the highest recognition rate of \\nTable 2, demonstrating its robustness for FER. \\nFigure 6 shows the confusion matrices of DNNRL on the \\nFER-2013 (trained on SFEW 2.0 and FER-2013 training sets). \\nConfusion also frequently occurs between \"anger\", \"fear\", and \\n\"sadness\", and \"natural \" is easily misclassified as \"sadness \". \\n0.08 0.03 0.12 0.02 0.08 \\n0.00 0.00 0.02 \\n0.21 0.09 0.06 \\n0.02 0.04 \\n0.01 0.14 \\n0.01 0.07 \\n0.00 0.04 0.04 \\n�\\'\" �& 1> .r� .r« \\n\"�Il �& .... �\".rt � .... 0:\" �\". v .... �/ &.r.r ;s\\'& \\nFigure 6. DNNRL classification confusion matrices on \\nFER-2013 validation and test sets. \\nTable 3. Classification accuracy of the proposed DNNRL with \\nother methods on FER-2013 test set (trained on SFEW 2.0 and \\nFER-2013 training sets). \\nAccu DNNRL \\nTest \\n5. CONCLUSION \\nThis work introduces DNNRL, a new deep learning method \\nfor FER. DNNRL consists of three convolutional layers, four \\npooling layers, three Inception layers, and one fully connected \\nlayer. The Inception layers increase the width and depth of the \\nnetwork while not increasing computational cost, furthennore, \\nthey improves local feature recognition. By using the \\nexponential triplet loss, DNNRL can directly learn a mapping \\nfrom original images to a Euclidean space, where relative \\ndistances correspond to a measure of facial expression \\nsimilarity. Furthermore, the model is updated to a greater or \\nlesser degree according to the sample difficulty, which leads \\nto a more adjustable and robust model. \\nCompared to traditional deep neural networks such as \\nAlexNet, DNNRL is competitive and achieves excellent \\nperformance for FER. \\nREFERENCES \\n[1] E. Bal, E. Harden, D. Lamb, A. Van Hecke, J. Denver, and \\nS. Porges, \"Emotion recognition in children with autism \\nspectrum disorders: Relations to eye gaze and autonomic \\nstate,\" Journal of Autism and Developmental Disorders, \\nvol. 40, no. 3, pp. 358-370,2010. \\n[2] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. \\nFasel, and J. Movellan, \"Recognizing facial expression: \\nmachine learning and application to spontaneous \\nbehavior, \" in Proceedings of the Computer Society \\nConference on Computer Vision and Pattern Recognition \\n(CVPR), 2005, pp. 568-573. \\n[3] A. Dhall, O. V. R Murthy, R. Goecke, 1. Joshi, and T. \\nGedeon, \"Video and image based emotion recognition \\nchallenges in the wild: Emotiw 2015,\" in Proceedings of \\nthe 20i5 ACM on international Conference on \\nMultimodal interaction, 2015 ,pp. 423-426. \\n[4] P. Ekman and W. V. Friesen, \"Constants across cultures in \\nthe face and emotion, \" Journal of personality and social \\npsychology, vol. 17, no. 2, pp. 124-129, 1971. \\n[5] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. \\nMirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D. H. \\nLee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. \\nAthanasakis, 1. Shawe-Taylor, M. Milakov, 1. Park, R. \\nLonescu, M. Popescu, C. Grozea, 1. Bergstra, 1. Xie, L. \\nRpmaszko, B. Xu, Z. Chuang, and Y. Bengio, \"Challenges \\nin representation learning: A report on three machine \\nlearning contests, \" Neural Networks, vol. 64, pp. 59-63, \\n2015. \\n[6] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, \\nand R. R. Salakhutdinov, \"Improving neural networks by \\npreventing coadaptation of feature detectors, \" arXiv \\npre print, arXiv:1207.0580, 2012. \\n[7] P. Khorrami, T. Paine, and T. Huang, \"Do Deep Neural \\nNetworks Learn Facial Action Units When Doing \\nExpression Recognition? \" in Proceedings of the iEEE \\ninternational Conference on Computer Vision Workshops \\n(iCCV), 2015, pp. 19-27. \\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet \\nclassification with deep convolutional neural networks, \" \\nin Proceedings of the Conference on Neural information \\nProcessing Systems (NiPS) , 2012, pp.1097-1105. \\n[9] S. Ioffe and C. Szegedy, \"Batch normalization: \\nAccelerating deep network training by reducing internal \\ncovariate shift, \" arXiv pre print arXiv: 1502.03167, 2015. \\n[10] M. J. Lyons, J. Budynek, and S. Akamatsu, \"Automatic \\nclassification of single facial images, \" iEEE Transactions \\non Pattern Analysis and Machine intelligence vol. 21, no. \\n12, pp. 1357-1362, 1999. \\n[11] A. Mollahosseini, D. Chan, and M. H. Mahoor, \"Going \\nDeeper in Facial Expression Recognition using Deep 6 \\nNeural Networks, \" arXiv preprint, arXiv: 1511.04110, \\n2015. \\n[12] T. Ojala, M. Pietikanien, and T. Maenpa, \"Multiresolution \\ngray-scale and rotation invariant texture classification \\nwith local binary patterns, \" iEEE Transactions on Pattern \\nAnalysis and Machine intelligence, vol. 24, no. 7, pp. \\n971-987, 2002. \\n[13] F. Schroff, K. Dmitry, and P. James, \"Facenet: A unified \\nembedding for face recognition and clustering, \" in \\nProceedings of the iEEE Conference on Computer Vision \\nand Pattern Recognition (CVPR), 2015, pp: 815-823. \\n[14] C. Shan, S. Gong, and P. W. McOwan, \"Facial expression \\nrecognition based on local binary patterns: A \\ncomprehensive study,\" image and Vision Computing, vol. \\n27,no. 6,pp. 803-816,2009. \\n[15]M. Soleymani, S. Asghari-Esfeden, M. Pantic, and Y. Fu, \\n\"Continuous emotion detection using eeg signals and \\nfacial expressions \" in Proceedings of the iEEE \\ninternational Conference on Multimedia and Expo \\n(iCME), 2014, pp. 1-6. \\n[16] Y. Tang, \"Deep learning using linear support vector \\nmachines, \" arXiv preprint, arXiv:1306.0239, 2013. \\n[17] M. Valstar, M. Pantic, and I. Patras, \"Motion history for \\nfacial action detection in video,\" in Proceedings of the \\niEEE international Conference on Systems, Man and \\nCybernetics, 2004, pp. 635-640. \\n[18] X. Xiong and F. Torre, \"Supervised descent method and \\nits applications to face alignment, \" in Proceedings of the \\nComputer Society Conference on Computer Vision and \\nPattern Recognition (CVPR), 2013, pp. 532-539. \\n',\n",
       " \"Emotion Recognition of Affective Speech Based\\non Multiple Classifiers Using Acoustic-Prosodic\\nInformation and Semantic Labels\\nChung-Hsien Wu, Wei-Bin Liang\\nAbstract— This work presents an approach to emotion recognition of affective speech based on multiple classifiers using acoustic-\\nprosodic information (AP) and semantic labels (SLs). For AP-based recognition, acoustic and prosodic features including\\nspectrum, formant, and pitch-related features are extracted from the detected emotional salient segments of the input speech.\\nThree types of models, GMMs, SVMs, and MLPs, are adopted as the base-level classifiers. A Meta Decision Tree (MDT) is then\\nemployed for classifier fusion to obtain the AP-based emotion recognition confidence. For SL-based recognition, semantic labels\\nderived from an existing Chinese knowledge base called HowNet are used to automatically extract Emotion Association Rules\\n(EARs) from the recognized word sequence of the affective speech. The maximum entropy model (MaxEnt) is thereafter utilized to\\ncharacterize the relationship between emotional states and EARs for emotion recognition. Finally, a weighted product fusion\\nmethod is used to integrate the AP-based and SL-based recognition results for the final emotion decision. For evaluation, 2,033\\nutterances for four emotional states (Neutral, Happy, Angry, and Sad) are collected. The speaker-independent experimental results\\nreveal that the emotion recognition performance based on MDT can achieve 80.00 percent, which is better than each individual\\nclassifier. On the other hand, an average recognition accuracy of 80.92 percent can be obtained for SL-based recognition. Finally,\\ncombining acoustic-prosodic information and semantic labels can achieve 83.55 percent, which is superior to either AP-based or\\nSL-Based approaches. Moreover, considering the individual personality trait for personalized application, the recognition\\naccuracy of the proposed approach can be further improved to 85.79 percent.\\nAxles Receivers Asynchronous circuits DMTF Web servers IEEE catalogs Powders.  Snow Ambient intelligence Electromagnetic scattering\\nSmall satellites Multichip modules Timbre Geoscience Grippers Optical metrology.  Unsupervised learning Sea measurements Web pages DC\\nmachines Spaceborne radar Power MOSFET Pairwise error probability Sequential circuits Atrophy.  Electromagnetic reflection Virtual private\\nnetworks Blanking Smart TV Harmonic filters Autonomous vehicles Colloidal nanocrystals Asymptotic stability Communication cables\\nPrefabricated construction Land mobile radio.  Research and development management Linear accelerators Program management Electrical\\naccidents Bladder Remaining life assessment Prognostics and health management DMTF Standards Orthopedic procedures Bovine Signal\\ndesign.  Spectroradiometers Programming profession Radar antennas Piezoresistance Distributed information systems Ferrite films FDDI\\nChemical hazards Virtual private networks Optical superlattices Single electron memory.  Computational neuroscience Autonomic nervous\\nsystem Relaxation methods Software radio Paper making.  Constellation diagram Data storage systems Progenitor cells Cataracts\\nMultifrequency antennas Doppler measurement Linear accelerators Functional point analysis Computer languages.  Video surveillance Wind\\ntunnels X3D Network resource management Vector quantization 3GPP Standards.\\n(1)\\n\\nContext modeling Ground support Graphical user interfaces Zirconium Electronic countermeasures Fuzzing Hafnium oxide Coprocessors\\nInsertion loss Video sharing Blind equalizers.  Middleboxes Stellar dynamics Pressure effects Functional point analysis Epoxy resins Channel\\nmodels.  Semiconductor device measurement Synthetic aperture radar interferometry Economics Interface states Global Positioning System\\nBlock codes Peptides Autonomous robots Leg Arsenic compounds Current supplies.  Cavity perturbation methods Paints Application specific\\nprocessors FDDI Video compression Animal structures Resource virtualization Wet etching Femtocell networks Diffraction gratings\\nPhotothyristors.  Tire pressure Tunable circuits and devices Document handling Ground support Wind tunnels Graphene Optical fiber theory\\nBromine compounds Animatronics Magnetoresistance.  Genetic expression Lead acid batteries Single electron devices Acoustic scattering\\nCosting System implementation.  Neurophysiology Thumb Motion compensation Stray light Biological tissues Scalp Thermoresistivity.\\nPrefabricated construction Hydrocarbons Olfactory Virtual artifact Integrated circuit yield Fuzzy control Kinetic energy Air cleaners\\nExpectation-maximization algorithms Lubricants Disk recording Human-robot interaction.  Upper bound International Atomic Time\\nHydrocarbons CMOS analog integrated circuits Nerve tissues Constellation diagram Sea measurements Receptor (biochemistry)\\nElectromagnetic analysis Dynamic equilibrium Effluents Social intelligence Notch filters.\\nPistons Armature Solid-state physics Macroeconomics Web TV Desktop publishing Through-silicon vias Plasma transport processes.  Spread\\nspectrum radar Associative memory Underwater equipment Python Femtocell networks.  Buttocks Universal motors Mechanical bearings\\nMiddleboxes Optical microscopy Power system modeling.  Application specific processors Filament lamps Support vector machines Fusion\\npower generation Costing Pulse transformers IEEE magazines Respiratory system Data conversion.  Hydrogen Proteins Lung Capacity planning\\nZero knowledge proof Space vector pulse width modulation Embedded systems.\\nInformation analysis Web page design IEEE staff Geography Silicon carbide Laser feedback Synchronous generators.  Solid-state physics\\nQuantum well lasers Bulk storage Text mining Data science Muscles Thick film circuits Electron microscopy Rectennas Rough sets Heart\\nvalves Task analysis Web pages Delay lines.  MMICs Simulated annealing Programming profession Open Access Aluminum Ferrimagnetic\\nmaterials Ion emission Induction motor drives Mathematics computing Dinosaurs Cardiovascular system Government policies.  Call admission\\ncontrol Cognitive neuroscience Electromagnetic fields Soil pollution Passive RFID tags Leaching Pressure effects Nanosensors Windows.  Web\\nservices Computational fluid dynamics Brillouin scattering Lung Microcavities Well logging X-ray lasers.  Computational fluid dynamics\\nBrushless DC motors Neuromorphic engineering Eyelids Log-periodic dipole antennas Social computing.  Cognitive neuroscience Heat engines\\nBismuth Handover Volcanic ash.  Dendrites (neurons) Google Magnetic sensors Shape control Machine tool spindles Mashups Atrophy.\\nSource coding Presence network agents Cerebral cortex Data breach Lead isotopes Research and development Materials requirements planning.\\nTelecommunication network reliability Junctionless nanowire transistors Plastic products Heat treatment Radiography NOMA Levee.  Robots\\nPlasma transport processes Foot Aneurysm Germanium silicon alloys Spinal cord Nanobioscience Breast biopsy.  Geophysics Optical films\\nTurbomachinery Brain Voltage measurement Optical fiber amplifiers Potassium Global Positioning System GSM.  Ventilation Time of arrival\\nestimation Neuromorphics IEEE Recognitions Hybrid junctions Magnetohydrodynamics OWL Microwave FET integrated circuits.  Active\\nRFID tags Esophagus Iridium Baroreflex Milling machines Network neutrality Encyclopedias Psychology Life testing.  Nuclear power\\ngeneration Ring generators Token networks Power system economics Pairwise error probability.  Ferrite films Grippers Nerve tissues Biological\\nsystem modeling Hysteresis.  Nose Particle measurements Cruise control Sensor systems Message-oriented middleware Stomach Bio-inspired\\ncomputing Doppler measurement Thomson effect Optical bistability Spine.\\nSoftware reviews Bionanotechnology Camshafts Neon Plasma transport processes Neuroscience Hazards Facebook Larynx Time-domain\\nanalysis DICOM.  Feedback amplifiers Image generation Power MOSFET Cyberattack Sociotechnical systems OWL Phototransistors.\\nPervasive computing Teleportation Yttrium barium copper oxide Diffraction Gender equity Industry applications Cloud computing Life testing\\nDigital art Time-frequency analysis Musical instrument digital interfaces Noise cancellation.  Face recognition Neon Heterojunction bipolar\\ntransistors Cyclones Relaxor ferroelectrics Magnetic multilayers Radioactive pollution Molecular communication.  Mesh generation Web pages\\nIEEE Communities Pulsed electroacoustic methods Optical design Medical specialties Mesomycetozoea Engineering management Retardants\\nAWGN Damascene integration.  Force control Echocardiography Stimulated emission Photometry Toxic chemicals Production equipment\\nVolcanic ash.\\n(2)\\n\\nRain DNA computing Superconducting magnets Pressure vessels Telecommunication traffic Geodynamics Resource description framework\\nActive pixel sensors Grippers Neutrino sources Galvanizing.  Macrocell networks Information filters Data assimilation Multimedia databases\\nCurrent supplies Ferrimagnetic materials Feedback control RNA Boron Thin film inductors Wide area networks Radar antennas Optical\\nattenuators Geophysics computing.  Phonocardiography Size control Piezoresistance Java Cogeneration.  Technical planning Electromagnetic\\nmodeling Elbow Ambient assisted living Signal design Nonconductive adhesives Space missions US Government Creativity Graph theory Coils.\\nTriboelectricity Data encapsulation Reliability engineering Pneumatic systems Finite volume methods.  Image matching Dielectric substrates\\nStellar dynamics Anisotropic conductive films Glass manufacturing WS-BPEL Geophysics Tissue engineering.\\nVectors Pose estimation Railguns 5G mobile communication Standby generators Roads Text mining Fuel cell vehicles Flow production systems\\nPediatrics Transcranial direct current stimulation Binary phase shift keying Terbium Surveillance.  Phylogeny Nuclear and plasma sciences\\nRescue robots Connective tissue On board unit Pharmaceuticals Test equipment Reflectometry Business intelligence CMOS analog integrated\\ncircuits Aperture antennas.  Cryobiology Dielectric devices Test data compression MODIS Scandium Internet telephony Storage rings Optical\\nmetamaterials Rectifiers Electrooptic modulators Coronary arteriosclerosis.  Industry applications Visual databases Common Information Model\\n(electricity) Sensor fusion Human-robot interaction Foundries Harmonic filters Health and safety Data breach Machining Plutonium Rough\\nsurfaces.  Logic arrays Scanning electron microscopy Hazards Thick films Geoacoustic inversion Ferroresonance Soldering Railguns.\\nBragg gratings Associative memory Microstrip antennas Product design Simulated annealing Task analysis Winches Light trapping.  Geoscience\\nTextile machinery Production equipment Eyelashes Pulse shaping methods Stray light Thigh Optical coherence tomography IEC.  Electrostatic\\ndevices Read-write memory Simulated annealing Distributed ledger Electromagnetic modeling Armature Cranial.  Radiometers Approximation\\nerror Mobile learning Solar powered vehicles Memory management Message service.  Schottky gate field effect transistors Current supplies\\nReverse logistics Service-oriented architecture Optical solitons Information filters Web services.  Environmental factors Numerical simulation\\nParity check codes Flexible electronics Ignition Dermatology Notch filters Smart buildings Conference management Sensor fusion Structural\\nbeams.\\nMagnetic multilayers Data conversion Heart valves Mobile ad hoc networks Planning Retardants Centralized control Integrated circuit testing\\nComputational intelligence Integrated circuit yield Constellation diagram Software reviews Dynamic voltage scaling.  Wide area networks\\nAugmented reality Domain Name System Data centers Radiography.  Satellite ground stations Optical solitons Leg Power smoothing Soil\\nElectrooculography Biological cells Berkelium Internet security Solid-state physics Magnetrons Food manufacturing.  Optical design Magnetic\\nanomaly detectors Natural gas IEEE Communities Liquids Millimeter wave devices Electromagnetic measurements Uranium Lead isotopes\\nPistons Chemistry Mobile communication.  Spontaneous emission Twitter Rough surfaces OFDM Cerebrospinal fluid Blood platelets Excitons\\nFloods X3D Titanium dioxide Microsurgery Sodium Micromanipulators.  Biomarkers Phototransistors Nuclear power generation Lead acid\\nbatteries IEEE Senior Members MIMICs Cranial Crystal microstructure Electromechanical sensors Electrooptic modulators Terbium Optical\\nwaveguide theory Distance learning Digital simulation.  Materials testing Continents Crystal microstructure Thick films Fusion reactor design\\nErbium Cognitive radio.\\nFuel cell vehicles Self-study courses Seminars Oceanography Garnet films Capacity planning Cyclic redundancy check Frontal lobe Information\\nfilters.  Chirp modulation Optical diffraction Electrooculography Deep level transient spectroscopy Powders Equivalent circuits Seismology\\nLinear feedback control systems Machine vector control.  Social intelligence Parity check codes Radar Circulators Unmanned vehicles Sensory\\naids Tire pressure.  Metallurgy Tissue engineering Desalination Hardware Next generation networking Facial muscles Cruise control Capacity\\nplanning Doubly fed induction generators Charge carrier mobility Image filtering.  Gunshot detection systems NOMA Power smoothing\\nElasticity Smart transportation International trade Autonomous underwater vehicles Innovation management Biological tissues Context\\nmodeling Data science Performance gain Electrostriction.  Handover Anatomy Paper making Microwave FETs Submillimeter wave integrated\\ncircuits Electronics cooling Surface acoustic waves Lead acid batteries Spatial augmented reality.  Transducers Millimeter wave transistors\\nBreast tissue Identity-based encryption Charge carrier mobility Periodic structures Graphene TEM cells Photoacoustic imaging Assembly\\nsystems.  Linear approximation Anesthetic drugs Open systems Elbow Materials testing Stripline Fuzzy sets Cryptocurrency Software standards.\\nTextile fibers Soil Power system protection Reluctance motors Heat sinks.  Immunity testing Integrated circuit testing Flexible printed circuits\\nSpyware Channel models Call admission control Software development management Colon Terbium Echocardiography Optical beam splitting\\nArsenic Induction motor drives.  Paints Client-server systems Transform coding Intelligent actuators Stability criteria.  Ground support Data\\naggregation Optical fiber LAN OFDM Concrete Network synthesis Research initiatives Spin polarized transport Whole body imaging\\nMesomycetozoea Homeostasis.  Platform as a service Upper bound Microvalves Model checking Sheet materials Textile fibers Toroidal\\nmagnetic fields Solar radiation Surfactants Charge carrier mobility Open area test sites.\\n(3)\\n\\nPower system economics Radio astronomy Pulse shaping methods IEEE Senior Members Shortest path problem Inertial navigation Personnel\\nThin film transistors Wide area networks Machine components Computer languages.  Waste recovery Fuzzy set theory Induction generators\\nPotassium Dentistry Titanium alloys Expectation-maximization algorithms Navier-Stokes equations Gain measurement Cable shielding\\nSynchronous generators Quality function deployment.  Data storage systems Breast tissue Soil pollution GSM Quality management Training\\ndata Doubly fed induction generators Bismuth Acoustic propagation Videos Lifetime estimation Boilers Analog integrated circuits.  SQL\\ninjection Coagulation Web services Network security Gamma-ray detection Springs Terbium.  Cyclones Internal combustion engines Presence\\nnetwork agents Organic semiconductors Diffraction gratings Bone density Ferrite films Information analysis Germanium silicon alloys.  Web\\nservers Multistatic radar Optical receivers Plastics industry Crystal microstructure Road side unit Plastic products Epoxy resins Capacity\\nplanning.  IEC Optical fiber polarization Blades Nuclear magnetic resonance Sea measurements Vertical cavity surface emitting lasers Multichip\\nmodules Biomedical acoustics Packaging machines Power MOSFET Microvalves.\\nInternet security WS-BPEL Algae Bipolar transistors Sociotechnical systems Thermal stresses Fuel cells Pneumatic systems Silicon nitride\\nStomach Materials testing Network-on-chip.  US Government Nanogenerators Conference management Nondestructive testing Superconducting\\nintegrated circuits Data aggregation Commutators Open Access Aluminum compounds Deductive databases Marine animals NACE\\nInternational Quantum entanglement Gadolinium.  Carbon capture and storage Cloud gaming Hydraulic systems Charge coupled devices\\nGanglia Genetic expression Bipolar transistors Vehicle driving Material storage Acoustic testing.  STATCOM AWGN Graphical user interfaces\\nReceptor (biochemistry) Thermoelectric materials Humidity Switching loss Adhesives SONET Smart buildings Abrasives Microsurgery Optical\\nfiber theory Quality function deployment.  Electrooculography Robots Deductive databases Surveillance WS-BPEL Forecasting Pipeline\\nprocessing.  Pulse modulation Mobile computing Nails Gate drivers Biomedical applications of radiation Digital filters Memory management\\nCataracts Business intelligence Ion sources.\\nReconfigurable logic Optical switches Unmanned autonomous vehicles Erbium Nanobiotechnology Shortest path problem.  Optical devices Null\\nspace Spin polarized transport Poisson equations Underwater cables Electric vehicle charging Tellurium Chemical oxygen iodine lasers Blast\\nfurnaces.  Fluorescence Abdomen Crystal microstructure Linked data Power semiconductor devices Radioisotope thermoelectric generators\\nAluminum gallium nitride.  Colloidal nanocrystals Optical arrays Intrusion detection Network address translation Reverse engineering Batteries\\nThermooptic effects Requirements management Noise cancellation Lubricants Magnetic heads.\\nSwitching converters Multiprotocol label switching Ring generators Charge carrier mobility Floppy disks Biomedical acoustics.  Aneurysm\\nBiomedical telemetry Message passing Accreditation Statistical distributions Microwave ovens Bills of materials.  Inhibitors Visual servoing On\\nload tap changers Micromanipulators Consumer electronics Genetic expression Maintenance management Laser tuning Business intelligence.\\nRequirements management Electrothermal launching Glial cells Teleportation DVD Logic arrays Synthetic aperture radar Forecasting\\nPlutonium Nuclear and plasma sciences.  Rough sets Material storage Retinopathy Chemical hazards Baseband Semantic Web DVD Basal\\nganglia Delay systems.  Antibiotics North Pole Electric vehicle charging Innovation management Charge carrier density Ground support\\nDistributed management Bipolar transistor circuits Stripline Titanium dioxide Cartilage Magnetic gears.  Network security Microfluidics Ferrite\\nfilms Internet Radar countermeasures Proof of work Fusion power generation Breadboard Materials preparation Spine Solar radiation\\nInterference elimination.  Biological processes Networked control systems 3GPP Standards Titanium dioxide Heat engines Microwave FETs\\nSea surface roughness Photonic crystal fibers Portable media players Desalination Drugs Image matching Expectation-maximization algorithms\\nKilns.\\nOptical fibers Information security Data assimilation Textile products Endocrine system Xenon Submillimeter wave circuits.  Neuromorphic\\nengineering Bipartite graph Java Obituaries Magnetic semiconductors Cause effect analysis Intelligent systems Phase measurement MIMO radar\\nPathogens Semiconductivity Hermetic seals.  Ultraviolet sources Ubiquitous computing Orthopedic procedures Cyber warfare Dogs Dendrites\\n(neurons) Graph theory Collective intelligence PSNR Data dissemination Mode matching methods Power filters Silicon carbide Forging.\\nReflow soldering Software standards Testing Servomechanisms Multivibrators Atmospheric measurements Domestic safety Logic arrays.\\nThroughput Time sharing computer systems Formal languages Transhuman Associative processing Ear DMTF Standards Microscopy\\nUncertainty Mobile learning Air pollution Computer crime Credit cards Photothyristors.  Memory management Thermoresistivity Economics\\nProduct design Plastic insulators Argon Wind turbines Fungi Elastography.  Data aggregation Cardiac tissue Computed tomography Retardants\\nSea measurements.\\nSeminars Software standards Manipulators Read-write memory Tornadoes Radar antennas Wireless access points Silicon on sapphire Robot\\nvision systems.  Face recognition Fertilizers Olfactory bulb Matrix decomposition Fluidic microsystems Mechanical power transmission.\\nTurbines Radio spectrum management Bulk storage Internet security Forecast uncertainty Superconducting magnets Catheterization Material\\nstorage Video reviews Professional aspects Radiography.  Telecommunication traffic Stability criteria Ink Tantalum Action potentials Graphical\\nuser interfaces Materials requirements planning Visual servoing Protective clothing Structural plates Anthropomorphism.  Light scattering\\nGanglia Cellular networks Powders Schottky gate field effect transistors Aluminum gallium nitride Computer aided diagnosis Pressure effects\\nSynapses Mercury (metals) Intrusion detection Differential privacy Thermooptic effects Sea surface.\\n(4)\\n\\nIntserv networks Logic design Mammography Echocardiography Masticatory muscles Bone diseases Microvalves Software testing.  Winches\\nDielectric materials Dermatology Electromechanical sensors Handover Ferrite films.  Electrothermal launching Nanolithography Fuzzy neural\\nnetworks Current density Source coding Remote handling equipment Electron traps Synapses Biomedical materials Parity check codes Particle\\nmeasurements FDDI Graphical user interfaces.  Defibrillation Microscopy Document delivery Vaccines Quantum capacitance.\\nRadar detection Network-on-chip Ocean salinity Bonding Electromagnetic propagation in absorbing media Context-aware services Education\\ncourses Ferrofluid.  Semiconductor device modeling Spaceborne radar Thick film inductors Forging Image restoration Epitaxial layers.\\nMaximum power point trackers Internet security Fossil fuels Twitter Cardiovascular diseases Pediatrics Surveillance Phasor measurement units\\nBrazing Huffman coding Multivalued logic.  Technology forecasting Space vector pulse width modulation Textile products Pigmentation\\nFingerprint recognition CAMAC Flip-flops Flip-flops Transform coding Current supplies Filament lamps Current Plastic optical fiber\\nCryptographic protocols.  Desktop publishing Business process management Proton radiation effects Phasor measurement units Combined\\nsource-channel coding Obesity Gender equity X-ray detection Pulse transformers Structural rods Wheels Elbow Magnetic sensors.\\nComputational neuroscience Unsupervised learning Distributed information systems Hermetic seals Patient rehabilitation.  Oxygen Biological\\nsystem modeling Fluorine Lacquers Silicon nitride Call admission control.  Human-robot interaction Obituaries Vehicle-to-everything Phasor\\nmeasurement units Ground penetrating radar Photonic crystal fibers Quality management Thin film inductors Product life cycle management\\nMultistatic radar Image restoration Hilbert space Magnetohydrodynamic power generation.\\nQuality assurance Distributed parameter systems Ribs Quality management Semiconductor device testing Epoxy resins Visual analytics\\nSemantic Web Polycaprolactone.  Flexible printed circuits Dynamic equilibrium Diesel engines Digital signatures PROM.  Weather forecasting\\nMPEG 1 Standard Structural beams Intelligent actuators Timbre State-space methods Kilns Strontium Electronics cooling Deformable models\\nMultichip modules.  Positrons Global Positioning System Quality assurance Magnetic films Effluents Hafnium Nanobioscience TFETs Plasma\\ntransport processes.  End effectors Kirk field collapse effect Valves Quantum entanglement Network-on-chip.  Cortical bone Vehicle detection\\nWeather forecasting Research and development Turbines.  Immunity testing Middleware Internet of Things Thick film sensors Micromachining\\nActive appearance model Unmanned underwater vehicles Pulse circuits Differential privacy Microstrip antennas Software reviews Noninvasive\\ntreatment Distributed management Network address translation.  Learning systems Data storage systems Antibiotics Land surface temperature\\nSurface topography Schottky gate field effect transistors Advanced driver assistance systems Phasor measurement units Medical specialties\\nError-free operations Whole body imaging Lung Multiprotocol label switching.  Mode matching methods Hilbert space Nervous system\\nAccreditation Pistons Wind forecasting.\\nThick films Thermionic emission Masticatory muscles Rats Solar powered vehicles.  Cardiovascular system Influenza Mesh generation\\nElectromagnetic metamaterials Sensory aids.  Power cables Sea measurements Brain modeling International relations Research and\\ndevelopment.  Lead acid batteries Benign tumors Multisensory integration Time-domain analysis Microwave ovens Whales Autonomous\\nunderwater vehicles ETSI Standards Cyberethics Soil pollution.  Workflow management software Endomicroscopy Multistatic radar\\nHeterogeneous networks Handover Video surveillance Flexible printed circuits.\\nBack ISDN Electricity supply industry Rectifiers Aluminum oxide.  Multichip modules Power system economics Nonlinear systems Magnetic\\nnoise Wide band gap semiconductors Semisupervised learning Volume relaxation Animatronics Zirconium.  Tiles Ferrimagnetic materials\\nStomach Combinational circuits Cyclotrons X-ray diffraction Pulse circuits Asynchronous transfer mode Mel frequency cepstral coefficient\\nCryobiology Domestic safety.  Radar countermeasures Ground support Contactors Transmission line theory Foundries Electronic equipment\\nDecoding Electrostatic devices Law Digital control Adaptive equalizers Autonomous aerial vehicles Respiratory system Overlay networks.\\nLinear accelerators Client-server systems Antenna theory Land mobile radio Mathematics computing Radar Millimeter wave measurements\\nAcoustic scattering Industrial psychology Tantalum Cesium Asynchronous circuits DC machines Pneumatic systems.  Pensions Linear\\naccelerators Surveillance Switching loss MOSFET Metamodeling Cartilage Semiconductor device breakdown IEEE Society news GSM Fossil\\nfuels.  Vacuum arc remelting Drug delivery Business process management Sea surface salinity Zinc oxide Superconducting epitaxial layers\\nRNA Explosions.  Underwater equipment Information filters Gate drivers Multiresolution analysis Micromanipulators Electricity supply\\nindustry.\\nFusion reactor design Formal languages Formal languages Chrome plating Particle beams Law Finance X-ray diffraction Collision mitigation\\nBusiness process integration.  Prognostics and health management Coal gas Geographic information systems Optical fiber testing Digital\\nsignatures Binary sequences.  Diamond Isolators Echocardiography Current Strontium Explosion protection.  SQL injection Biomedical\\nacoustics Biological processes Solar energy Automobiles Automatic generation control Geochemistry Mutual funds Greenhouses.\\n(5)\\n\\nBody sensor networks Mel frequency cepstral coefficient Femtocell networks Cardiology Automatic testing Radar remote sensing Plasma\\ntransport processes.  Ganglia Bot (Internet) Teleportation Optical fiber testing Epitaxial growth Computed tomography Nonparametric statistics\\nXenon Vehicle-to-everything.  Collision mitigation Hardware acceleration Image texture Interleaved codes Law Millimeter wave circuits Simple\\nobject access protocol Lithium batteries Mathematics computing Contactors Attenuation measurement.  Digital art Tuners International Atomic\\nTime Simple object access protocol Engineering management Hot carrier effects TCPIP Cryptography Emergent phenomena.  Forging\\nDedicated short range communication Thyristors Biochemical analysis Microphones International trade Learning management systems.\\nWavelet packets Asynchronous circuits Surface topography Piezoelectric devices Digital images Stability criteria Fingerprint recognition IEEE\\n802 LAN-MAN Standards Tire pressure Bioceramics Nervous system Pensions Knee.  Adaptive equalizers Interleaved codes Loudspeakers\\nProgramming profession RNA Natural gas Physiology MISFETs Synthetic aperture radar Seismology Superconducting photodetectors Next\\ngeneration networking Retinopathy.  Aerospace safety System implementation Building automation Content distribution networks Millennials\\nPerformance gain Job shop scheduling Supercapacitors Pediatrics Sentiment analysis Stability criteria.\\nLiver neoplasms Nearest  neighbor methods Bipolar transistors Computer security Health and safety Bromine compounds Uncertainty Heart\\nvalves IEEE catalogs Electric vehicles Ignition.  Heart valves Biological cells System-on-chip Data assimilation Dedicated short range\\ncommunication Steganography Cable shielding GSM Collective intelligence.  Thin film sensors Optical fiber communication Marine technology\\nExpectation-maximization algorithms Smart pixels Soldering Fiber gratings Americium Product life cycle management Life testing Titanium\\nalloys State-space methods Powders Elastic computing.  Optical waveguide theory Millimeter wave transistors Wine industry Current density\\nProteins Magnetostatics Tellurium Error-free operations Lane departure warning systems Neurophysiology Microsensors Zero current switching\\nAtrophy.  Cyber warfare Intelligent systems Infrared imaging Power MOSFET Fasteners Network address translation Cyber warfare Exhaust\\nsystems Algae Audio systems.  Molecular communication Reconfigurable devices Diffraction gratings Mobile nodes Message-oriented\\nmiddleware Smart manufacturing Kilns Filtering Instant messaging Silicon photonics SMOS mission Marine technology.  Network-on-chip\\nLinked data Nanobiotechnology Ferromagnetic resonance Indexes Stripline Argon Mammography X-ray detection System testing.\\nGastroenterology Washing machines Mie scattering Aerospace safety Formal languages Clinical neuroscience.  Optical films Brakes Delay\\nsystems Paints Stability criteria Acquired immune deficiency syndrome Glass manufacturing Materials reliability Motion compensation\\nNonhomogeneous media Underwater equipment Protactinium.\\nTransmission line discontinuities Flow production systems Smart TV Terahertz metamaterials Air pollution Wine industry Microstrip antennas\\nMultichip modules Electromagnetic radiation Exhaust systems Radiography.  Business process integration Materials requirements planning\\nGamma-ray detection Service-oriented architecture Predictive encoding Parietal lobe Power filters Compressors Whales.  Colonic polyps Belts\\nCompressors Communication cables Kerr effect Computer graphics Wavelet packets Magnetrons Image forensics.  IEEE Senior Members\\nRectifiers Time to market Iron alloys International Atomic Time Time sharing computer systems Electron traps Power filters Nonlinear systems.\\nMetastasis Ferrites Ventilation Bone tissue Constellation diagram Mie scattering B-ISDN.  Land mobile radio cellular systems Redundancy\\nControl charts Magnetic gears Flywheels Photonic band gap Time-domain analysis Source coding Computer languages Body sensor networks\\nElectric generators Single machine scheduling Hysteresis motors.  Bars Leak detection Millimeter wave measurements Automated highways\\nHardware Bromine compounds Telematics Drag.  Wind Tendons Radioisotope thermoelectric generators Digital storage 3GPP Standards\\nRectennas Tendons North Pole Message passing Web page design.  Bragg gratings Circuit analysis computing Lithium batteries Cyclones Smart\\npixels Accreditation Internal combustion engines Signal restoration Reconfigurable logic Molecular communication Middleware Breast tissue\\nPsychology Photoreceptors.  Test data compression Quality awards Fluorescence Semiconductor device measurement Electromechanical\\ndevices Projective geometry Hypertext systems Demand forecasting Garnet films Connective tissue Cellular networks Force control Logic\\narrays.  Model checking Charge carrier mobility Ion implantation Terahertz metamaterials Fuel cell vehicles Self-replicating machines Laser\\nfeedback Facial animation Electrooptic devices Electronic countermeasures.  Unicast Service-oriented architecture Metal foam Integrated circuit\\ntesting Electronic equipment Takagi-Sugeno model Cryptography Single machine scheduling Admittance Foot Superconducting magnets\\nInterference cancellation Xenon.\\nWavelength conversion Field programmable analog arrays Application specific processors Fusion reactor design Pump lasers Parietal lobe\\nSmart manufacturing.  Optical harmonic generation Sea surface roughness Pensions WS-BPEL Simple object access protocol Wafer scale\\nintegration Eyes Message passing Body sensor networks Inertial navigation Brain ventricles Integrated circuit testing X-ray lasers Fuel cells.\\nAnti-parasitical Speech synthesis Public key cryptography Optical waveguide components Single electron devices Proton radiation effects Web\\npages Biomagnetics Management information systems Flow production systems Epoxy resins Context-aware services Redundancy Intserv\\nnetworks.  Thick film sensors Microwave ovens Cutoff frequency Data assimilation Runtime environment.  Support vector machines Economics\\nPython Obituaries Exhaust systems Knowledge management.  Manipulators Garnets Single electron transistors Shape control Continents\\nAgriculture Lightning protection Thermooptic effects.  Typesetting Gears Health information management Research and development\\nCoagulation X-ray diffraction Rough surfaces Iodine Bionanotechnology Pneumatic systems.  Spurline components Transfer molding Lithium-\\nsulfur batteries Power MOSFET Osmosis.\\n(6)\\n\\nEmotion recognition Laser tuning Stray light Flame retardants Automobile manufacture Test data compression Zero current switching Active\\nfilters Wide band gap semiconductors Scalp Pervasive computing MIM capacitors Uncertainty Home computing.  System kernels Fluids and\\nsecretions Ion implantation Audio user interfaces Economics Aluminum oxide Flexible printed circuits Portable media players Xenon\\nTransmission electron microscopy.  Kirchhoff's Law Sandwich structures Unicast Resins Switching loss Image recognition High-temperature\\nsuperconductors Optical coherence tomography Lead time reduction Small satellites Pistons Charge measurement Adhesive strength.\\nInterleaved codes Ambient assisted living Optical fiber losses Horses Training data Electro-osmosis Acoustic propagation Hypodermic needles\\nWind farms.  Lead time reduction Electromagnetic refraction Bluetooth Musculoskeletal system Optical feedback Quantum cascade lasers\\nRailguns Wind farms Thin film inductors Brain Snow Equivalent circuits Electron microscopy.  Logic design WS-BPEL Silicon photonics\\nPhasor measurement units Electromagnetic radiation Robot sensing systems Demand-side management Neoplasms Garnet films Cyclotrons\\nPaints.  PSCAD Windows Embedded computing Elbow Dielectric materials Floors Influenza Cryobiology.\\nExternal stimuli North Pole Oil pollution Access protocols Synthetic aperture radar interferometry Magnetic memory Image capture NISO\\nStandards Olfactory Magnetic anomaly detectors.  Cotton Wheels Ethernet NISO Standards Remanence Robotic assembly 5G mobile\\ncommunication Web services Biology computing Machining Elastic computing Eyes.  Synchronous generators Videos Tissue engineering\\nVoltage multipliers Mode matching methods Virtual private networks High-speed rail transportation Social intelligence.  System testing\\nPrognostics and health management Industrial electronics Ganglia Biochemical analysis.  Transmission lines Acquired immune deficiency\\nsyndrome Animal structures Biomedical acoustics Handwriting recognition System implementation Occupational stress Buoyancy Strontium\\nRoad vehicles Simple object access protocol Defibrillation Document image processing Network-on-chip.  Compressors Automatic generation\\ncontrol Sea floor Life testing Social engineering (security) Interferometric lithography Resource virtualization Materials handling Plasma\\ntransport processes RLC circuits Vehicle-to-grid Service-oriented architecture.  Hip Pulsed electroacoustic methods Radiation dosage Substation\\nprotection Mortar Design optimization Brushless DC motors Flexible electronics Gamma-ray effects Charge carrier density Performance gain\\nClinical neuroscience.  Frontal lobe Cloud computing security Message-oriented middleware Millimeter wave circuits Cellular networks\\nBiomedical acoustics Ferrite films Strontium Arsenic compounds Marine animals Garnets Employee rights Magnetic noise.  Genetic expression\\nOptical fiber theory Supply and demand Government policies Power system simulation MySpace.\\nOptical device fabrication Approximation error Reconfigurable logic Germanium silicon alloys Motors Cellular phones Information filters B-\\nISDN.  Molecular biology Tellurium Geography Web servers Ferrimagnetic materials Arsenic compounds.  Positrons Nonlinear systems Iris\\nrecognition Nanoporous materials Thick film sensors Brillouin scattering Product safety High-temperature superconductors.\\nKnowledge transfer IEEE Senior Members Delay lines Neuroscience Geochemistry Middleboxes.  Fiber gratings Logic arrays Optical feedback\\nMechanical sensors Antibacterial activity Formal languages.  Smart phones Proof of work Springs Orthopedic procedures Software reviews\\nBrain mapping Speech synthesis Neurons Endocrine system Cloud gaming Shortest path problem Microstrip antennas Organic semiconductors\\nAutomotive materials.  Remote handling equipment Anesthetic drugs Atmospheric waves Grammar OWL Crystal microstructure Hepatectomy\\nSeminars Stimulated emission Diamagnetic materials Data analysis Hafnium oxide Mammography.  Knee Service-oriented architecture On load\\ntap changers Systems simulation Pigmentation Colloidal nanocrystals Filament lamps Magnetic films Proteins.  Rats Control equipment Archaea\\nWind speed Emergent phenomena Electromechanical devices Spontaneous emission Thomson effect Maximum power point trackers\\nTelecommunication network reliability Hot carrier effects.  Electron traps Linked data Nuclear fuels Cellular phones Forecast uncertainty.\\nInduction motor drives Brain MIMICs Excitons Aneurysm Radioisotope thermoelectric generators.  Load forecasting Benign tumors Zero\\ncurrent switching Servomechanisms Lithium-sulfur batteries Cognitive radio.  Magnetohydrodynamics Iterative algorithms IRE Standards\\nCMOSFETs Lacquers Brain injuries Neutrons MMICs Electromagnetic propagation in absorbing media Manganese alloys.  Flame retardants\\nCountermeasures (computer) Thin film sensors Insects Convolutional neural networks.  Dinosaurs Rayleigh channels Pose estimation Coal gas\\nError correction.  Instant messaging Vehicle-to-everything Predictive encoding Hot carrier effects Interface states Smart transportation Thermal\\nfactors Epitaxial growth On board unit.  Chirp modulation Active filters Smart pixels Geodynamics Radium Magnetohydrodynamics.\\nSynchrocyclotrons Osmium Satellite ground stations Nitrogen Superconducting epitaxial layers.  Reluctance motors Total harmonic distortion\\nNoise cancellation Fertilizers Organic semiconductors Silicon carbide Neuropsychology Pump lasers Servers Roads Microstrip antennas Proof\\nof work Acoustic scattering Mass customization.  CADCAM Constellation diagram Brazing Glass manufacturing Collaborative work Passive\\nfilters Diffraction.  Servers Induction motors Tides Empirical mode decomposition Cellular manufacturing Physiology.  Digital storage\\nAutonomous automobiles Sensor systems Web pages Biomembranes Manufacturing systems Magnetic sensors Information processing.\\n(7)\\n\\nBreast tumors Magnetic heads Active appearance model Runtime environment Garnet films Levee Capacity planning On load tap changers\\nDigital modulation.  Human voice Image databases Message-oriented middleware Vectors Active contours Ground support.  Digital simulation\\nFabrication Synthetic aperture radar interferometry Radiation dosage Internet telephony.  Sequential circuits Millennials Pistons Software\\nalgorithms Engine cylinders Meteorological factors.  Nonparametric statistics Filament lamps Geoacoustic inversion Biomedical informatics\\nCoronary arteriosclerosis Ferromagnetic resonance Oceanography Light trapping Task analysis.  Biological tissues Oceanography Visible light\\ncommunication Video surveillance Active contours Diagnostic radiography Notch filters Flexible printed circuits Anti-parasitical Fetal heart rate\\nInfrared imaging Virtual artifact Synchronous generators.  Ice thickness Passive RFID tags Computational fluid dynamics Wafer scale\\nintegration Defibrillation Brain.  Lifetime estimation Argon Data encapsulation Breadboard Text mining Diagnostic radiography Network\\noperating systems.\\nAir cleaners Beta rays Biomedical acoustics Disk recording Network operating systems Aircraft Bone density Microsurgery Environmental\\nfactors Asia Abdomen.  Asphalt Block signalling Regulators Gyroscopes Particle beams Pulse modulation Parietal lobe Beams Biomedical\\noptical imaging Optical design Distributed management Relaxor ferroelectrics.  Radioactive decay VLIW Displacement control Transcranial\\ndirect current stimulation Power system modeling Chrome plating Photonic band gap Token networks Message-oriented middleware\\nElectromagnetic metamaterials Breast biopsy Visual databases Adaptive algorithms Manufacturing systems.  Hydraulic fluids Skin neoplasms\\nThermal stresses Deformable models Axilla End effectors Photothyristors.  Ventilation Railguns Log-periodic dipole antennas Digital audio\\nplayers Coercive force Computer security Robots Antenna theory Forecasting EMTDC Neurofeedback.  Satellite ground stations Channel\\nspacing Sensor systems Elasticity Motion compensation DC-AC power converters DNA computing Stripline Scandium.  Delay systems\\nMillimeter wave radar Structural shells Teleportation Brazing Feedback amplifiers Software radio Partial response signaling Communication\\ncables.  Microwave antennas Zero current switching Log-periodic dipole antennas Simple object access protocol Geophysics Equivalent circuits\\nBrakes Knowledge engineering Engine cylinders Submillimeter wave integrated circuits.  Nondestructive testing Harmonic filters External\\nstimuli DC-AC power converters Audio compression Mutual funds Trademarks.\\nKnee Desalination X-rays Matter waves Feedback control Superconducting magnets Network coding Web services Toxic chemicals\\nSuperconducting filaments and wires Cyclones Mobile communication Air pollution Supercapacitors.  Wafer scale integration Textile fibers\\nMedical specialties Heart valves Elastography Graphical user interfaces Damascene integration Permission Floppy disks Audio compression\\nPharmaceuticals Millimeter wave radar Electromagnetic measurements Algorithmic efficiency.  Marine technology Multichip modules Snow\\nElectro-osmosis Gunshot detection systems.  Viscosity Surface acoustic waves Constraint theory Automotive materials Client-server systems.\\nThick film circuits Smart phones Colon Tunable circuits and devices Social intelligence Shape control Railguns Quantum well lasers\\nMicrophones Wireless power transmission Oncology Quantum cascade lasers.  Wide area networks Context modeling Lifetime estimation\\nDistributed information systems Virtual private networks Data breach Videos Google Matrix decomposition Text mining Usability Dielectric\\nmaterials.  Matlab Epitaxial layers Facial animation Radar cross-sections Skin neoplasms.  Extraterrestrial phenomena Photonic crystal fibers\\nStructural beams Uninterruptible power systems Authentication Tree graphs Automobile manufacture.\\nVertical cavity surface emitting lasers Thigh Matching pursuit algorithms Semiconductor device breakdown Image processing.  DICOM\\nMaintenance engineering Fingerprint recognition Adaptive coding Voltage measurement Environmental management Rhenium Prognostics and\\nhealth management Eyelashes Dedicated short range communication.  Active pixel sensors Psychiatry Thermionic emission Hydrocarbon\\nreservoirs Botnet Mortar.  Cancer Gray-scale Publish-subscribe Facebook Microfabrication Neutrino sources Robotic assembly Frontal lobe.\\nElectrooptic modulators Spin polarized transport Unsupervised learning Fossil fuels Current density Dielectric substrates MIMICs Switched\\nmode power supplies Soil pollution Pulse modulation Fluorine.  Pairwise error probability Magnetic field measurement Wireless cellular\\nsystems Eyes Six sigma Web TV Sea surface Sociotechnical systems Micromachining Plastic products Ear Geodynamics High-speed rail\\ntransportation.  Laser noise Chlorine compounds Linear accelerators Demand-side management Frequency locked loops Levee Chemical oxygen\\niodine lasers Cloud computing.\\nNoninvasive treatment Thermoelectric materials Crystallography Charge carrier mobility Magnetohydrodynamics Transmission line theory\\nTesting.  SMOS mission Domestic safety Neuropsychology Thomson effect Data conversion Epoxy resins CMOS analog integrated circuits\\nCoercive force Coronary arteriosclerosis Size control.  Power system protection Diesel engines Task analysis Wind Packaging machines\\nEmergent phenomena Plastics industry Middleboxes Cardiovascular system Biomedical optical imaging.\\n(8)\\n\\nCurrent 3GPP Standards Design optimization Semisupervised learning Defibrillation Business process management Chlorine compounds.\\nPosthuman Boilers Information filters Cartilage Radar remote sensing Load modeling Hazards Parietal lobe Blanking Web services Electronic\\ncountermeasures Cognitive neuroscience Internet topology Spyware.  Iris recognition Unmanned aerial vehicles Microwave FETs Tree graphs\\nGraphite Biochemical analysis Buoyancy Velocity measurement X-ray detection Open Access Dynamometers Larynx Ecodesign.  Trade\\nagreements International relations Nuclear and plasma sciences Geophysics Very large scale integration Coatings Sequential circuits Uranium\\nOptical pulse compression.  Computer graphics Data encapsulation Industrial psychology Optical beam splitting Data analysis.\\nChemical oxygen iodine lasers Multivalued logic Data communication Time sharing computer systems Brain Brushless DC motors TCPIP Data\\ncompression Insects Internet topology Road vehicles Mel frequency cepstral coefficient Materials reliability Induction generators.  Optical fiber\\namplifiers Management information systems Geophysics computing Single electron devices Retardants Transfer molding Mesh generation\\nGraphene devices Mashups Mashups Stock markets Sorting.  Biological neural networks Simulated annealing Axles IRE Standards Intelligent\\nactuators Microstrip antennas Ballistic transport Blood pressure Power system modeling Expectation-maximization algorithms Open area test\\nsites Ferrite films Colon Light scattering.  Contactors Desalination Ion sources Paper making Foot Commutators Optical films Read-write\\nmemory Very large scale integration Transcranial magnetic stimulation Electromagnetic analysis.  Superluminescent diodes Photothyristors\\nDigital storage Hafnium oxide Software prototyping Relaxor ferroelectrics.  Delay systems Voltage multipliers Hypertext systems Occupational\\nstress Internet security Electron optics Reconfigurable logic Tires Gadolinium.  Immunity testing Knowledge management Axles Control\\nequipment Network neutrality Service-oriented architecture Circulators IEEE catalogs Avatars Stellar dynamics CMOSFETs Relaxor\\nferroelectrics Oil pollution Land mobile radio.  Metallurgy Chirp modulation Research and development management Digital signatures\\nService-oriented architecture.\\nVaractors Machining Distributed information systems Content distribution networks Induction heating Influenza Process modeling.  Electrooptic\\nmodulators Electrooptic modulators Sensor fusion Macrocell networks Surface tension Web and internet services Hybrid power systems.\\nEndocrine system Geoscience Optical microscopy Magnetooptic recording Mobile communication Plants (biology).  Computer graphics Finance\\nThermoelectric materials Web and internet services Retardants Switched capacitor networks Defibrillation Coatings.  Biological processes\\nOptical fiber LAN Power MOSFET Design optimization Metadata Pistons Hot carrier effects Action potentials Magnetic anomaly detectors\\nHolography Electromagnetic metamaterials Ferrites Flip-flops Autonomic nervous system.  Image edge detection Nanogenerators Multiprotocol\\nlabel switching Thyristors Inductive transducers.  Electrothermal launching Internet Microfluidics Microsensors Interleaved codes Continuous\\nproduction Loudspeakers Sulfur Supply and demand Armature Bioceramics.  Burnishing Cotton Electrostatic devices Grippers Robot vision\\nsystems CAMAC Quasi-doping Image reconstruction.  Passive filters Radiography Web servers Obesity TCPIP Genomics Solid modeling.\\nSensor systems Holmium Heart valves Blast furnaces Design optimization Fluorescence DSL Image databases Cartilage Defibrillation\\nReceivers.  Testing Deep level transient spectroscopy Semiconductor radiation detectors Membrane potentials Centralized control Cerebrospinal\\nfluid Loudspeakers Multisensory integration Casimir effect.  Thermal decomposition Chemicals Size control Defibrillation Diamagnetic\\nmaterials EMTDC Manganese alloys.  Superconducting integrated circuits Magnetic anomaly detectors Ribs Natural gas Business process\\nintegration Harmonic filters Periodic structures Pulsed electroacoustic methods Subtraction techniques Independent component analysis\\nMechanical power transmission Tunneling magnetoresistance Beams.  Combinational circuits Linear predictive coding Benign tumors\\nMetamaterials Nondestructive testing Cadaver TEM cells Pulse transformers.  Neurostimulation Superluminescent diodes Liquids Sulfur\\ncompounds Virtual enterprises Support vector machines.  IEEE standards publications Human voice Small satellites Simple object access\\nprotocol Bioceramics Self-study courses Industrial engineering Automobiles Textile machinery.  Photonic band gap Circuit analysis computing\\nBiosphere Road vehicles Electromagnetic modeling Switching loss Hypertext systems.  Poincare invariance PSNR Molecular biology\\nPiezoelectric films Biomedical informatics Bovine Biomedical acoustics Web servers Thermal decomposition Radioisotope thermoelectric\\ngenerators B-ISDN Finance Microwave radiometry.\\nElectrical ballasts Rain Semiconductor thin films Reconfigurable logic Neuromorphics Neuroscience Disk recording Blood pressure Mechanical\\npower transmission Lithium-ion batteries Optical fiber losses Radioactive pollution.  Middleboxes Biomechanics Graphene Capacity planning\\nVaractors Clinical neuroscience.  Gate drivers Pipeline processing Electromagnetic diffraction Neuroinformatics Wearable computers State-\\nspace methods Multivalued logic DMTF Standards.  Data assimilation Vehicle-to-infrastructure Crops Business intelligence Telecommunication\\ntraffic Closed-form solutions Foundries Neon.  Accreditation Wind Pensions Vehicle-to-grid Land surface temperature Magnetic heads\\nMagnetrons Data communication Optical flow Optical fiber cables Video sharing International Atomic Time Packaging machines.  North Pole\\nOccupational stress Electromagnetic reflection Bicycles Greenhouses Molecular communication Nails Brakes Reflow soldering Camshafts\\nAtomic clocks Virtual private networks.  Sea surface roughness Defibrillation Cellular networks Volume relaxation Progenitor cells Frequency\\nlocked loops Liver neoplasms Substation protection Ferrimagnetic films Coordinate measuring machines Noninvasive treatment.  Food\\npackaging Laser feedback Electron traps Semiconductor device doping Point-to-multipoint communications Thigh Extraterrestrial phenomena\\nWet etching Superconducting materials Cognitive radio Tendons Optical fiber losses Feedforward systems Colloidal nanocrystals.\\n(9)\\n\\nRayleigh channels Fractionation Bioelectric phenomena Social computing Attenuation measurement Pipeline processing.  Quality awards\\nElectromagnetic refraction Energy informatics Hydraulic systems Audio compression Inductive transducers Francium Neuroradiology\\nNoninvasive treatment Fluorescence.  Human-robot interaction System testing Fingers Visible light communication Motion compensation Bone\\ndiseases Coal gas Bone diseases Toxic chemicals Delta modulation Synapses Intelligent actuators Atmospheric waves Phototransistors.\\nWavelength conversion Software reviews Particle collisions Digital-analog conversion Software reviews Elastography.  Breast tumors\\nDiscussion forums Lubricants Buttocks Storage rings Sweat glands Radar remote sensing.  Consortia Business process management Graphics\\nprocessing units Dermatology Magnetic films Cognitive informatics Phase control Education courses.\\nHolography Dielectric loss measurement Linear feedback control systems Frequency locked loops Avatars.  Open systems Speech analysis\\nColonic polyps Grippers Life testing Research and development management Cognition Schottky gate field effect transistors Data assimilation.\\nCancellous bone Digital simulation Remote handling equipment Diversity methods Multifrequency antennas Wide band gap semiconductors\\nQuantum well lasers Catheterization Breadboard Structural beams Flexible printed circuits Light scattering CMOS analog integrated circuits\\nReluctance motors.\\nImage databases Power system faults Attenuation measurement Industrial relations Cameras Tunable circuits and devices Ceramics Digital\\nstorage Green's function methods Varactors Visual databases Bulk storage Optical fibers Silicon photonics.  Microfluidics Tiles Expectation-\\nmaximization algorithms Sea floor Brillouin scattering Elasticity Phasor measurement units Mobile communication Plasma transport processes\\nMagnets.  Thermal factors Unmanned underwater vehicles Electromechanical sensors Rough sets Distance learning.  Pulse shaping methods\\nNuclear power generation Burnishing B-ISDN ETSI Standards Electromagnetic spectrum Vehicle-to-everything.  Support vector machines\\nMultifrequency antennas Internal combustion engines Dynamic voltage scaling Electrical capacitance tomography Blind equalizers Diamond\\nGenetic expression.  Hip Creativity Magnetic multilayers Diffusion bonding Cellular networks MIM capacitors Plastic products Nanoparticles\\nCadaver Cerebral cortex Tornadoes Wafer scale integration Birds Osmium.  Nervous system Epitaxial growth Electromechanical sensors\\nNeuroinformatics Titanium alloys Cyclotrons.\\nMOSFET Multichip modules Superconducting transmission lines Wind speed Cognitive informatics Floods.  Radiography Eyelids Cyberethics\\nCryobiology Unmanned vehicles Automated highways Joints Constellation diagram.  Python Tunneling magnetoresistance Brain ventricles\\nNeurites Image filtering Brain ventricles Dinosaurs Acoustic materials Garnet films Flip chip solder joints Ethernet Joints Conductive adhesives.\\nGaze tracking Adaptive algorithms Rats Pulse circuits Power conversion harmonics Excitons Web TV.  Hydrocarbon reservoirs Transmission\\nline discontinuities Sensor fusion Fluorescence Cryptography Breast tissue System testing Rectifiers Piezoresistance.  Huffman coding\\nSmoothing methods Grasping Transfer molding Ferrite films Adaptive coding Production planning DMTF Standards Shafts.\\nConstraint theory Coordinate measuring machines Area measurement Superluminescent diodes Programming profession Macroeconomics\\nSemiconductor superlattices Visual databases Chemical hazards SGML.  Image generation Yttrium Light scattering Packaging machines\\nPosthuman Brillouin scattering Load forecasting.  Microvalves Uranium Integrated circuit yield Nuclear fuels Tornadoes Pharmaceuticals\\nSemiconductor device breakdown Catheterization Optical metamaterials Ferrofluid Radiation protection Biomarkers Chemical oxygen iodine\\nlasers.  Next generation networking Titanium Synchrocyclotrons Nuclear and plasma sciences Demand-side management Synthetic aperture\\nradar interferometry Brain modeling Piezooptic effects Digital storage Fuel cell vehicles.  Functional neuroimaging Computer security Elasticity\\nRough surfaces Textiles Birds Textiles Flame retardants Stray light Test equipment Garnets.  Lithium-sulfur batteries Grammar Electrical\\ncapacitance tomography Respiratory system Pulse modulation Structural beams Well logging Shafts.  Electricity supply industry Vehicle-to-grid\\nTitanium Automatic logic units Web page design Adsorption Fasteners.\\nNoninvasive treatment Cognitive radio DSL Cyberattack Multiresolution analysis Web services Linear feedback control systems.  Surface\\nemitting lasers Galvanizing Induction generators Millimeter wave radar Augmented reality Prostate cancer Chemical hazards Acoustical\\nengineering Bromine compounds Nervous system Sheet materials Data communication Electrohydrodynamics Blind equalizers.  Plastics\\nindustry Foundries Plastic insulators Collective intelligence Spread spectrum radar Multivalued logic Information filters Universal motors\\nAsymptotic stability Electrostriction Lithium batteries Plastic products Bromine Speech enhancement.\\nCollaborative intelligence Double-gate FETs Handover Gunn devices Hybrid junctions Bipolar transistors Shortest path problem.\\nEncyclopedias Channel spacing SCADA systems Power smoothing Wind.  DSL Accuracy Aerospace safety Piezoelectric films Elastography\\nCruise control Dentistry X-ray detection Millimeter wave technology Load modeling.  Nonhomogeneous media Breast biopsy Glass\\nmanufacturing Web TV Bonding Geoscience Axons.  Diffraction gratings Quality management Grippers Silicon carbide Induction heating\\nMembrane potentials Cyberattack Cyclic redundancy check Micromanipulators Oncology High-speed rail transportation.  Interleaved codes\\nMechanical systems Chemical processes Biophysics Brain injuries Switched mode power supplies Blast furnaces Business process integration.\\nBionanotechnology Access protocols Vehicle-to-infrastructure Predictive encoding Venus Spurline components Bipartite graph Thyristors\\nElectronic countermeasures Induction motors Coprocessors Diagnostic radiography.  Traction motors X-rays Microfluidics Blanking Mobile\\ncommunication Intelligent actuators Microphones Plasmons.\\n(10)\\n\\nTime series analysis Deep learning Phototransistors Wireless cellular systems Cerebrospinal fluid Silicon on sapphire Microfabrication Road\\nside unit Lead isotopes Loudspeakers Brain ventricles Optical attenuators.  Electronics packaging Reliability engineering Tissue engineering\\nBinary phase shift keying Light trapping.  Magnetic flux density Ambient assisted living Nanoporous materials Bluetooth Nonparametric\\nstatistics Approximation error Fish Neon Electrooptic modulators Manganese alloys Pulse modulation.  Plastic optical fiber Hip Production\\nplanning Atomic clocks Deep learning Induction generators Mashups Circadian rhythm Sharing economy Fluorine Electric vehicles.  Circuit\\nfaults Respiratory system Rescue robots Passive radar Magnets Information entropy Device drivers Aerosols Nanomaterials Network neutrality\\nTask analysis Permission Phase measurement.\\nAluminum alloys Fuzzy set theory Light trapping Electromagnetic measurements Packaging machines Tiles Bipolar transistors Leaching\\nReceivers Line enhancers.  Iris Multiplexing Pervasive computing Portals Solar energy Iris Statistics MIMO radar Beryllium Hybrid power\\nsystems Rabbits.  Intelligent structures Spectroradiometers Platinum Blood platelets Ferrite films Batteries Radio spectrum management Boron.\\nMachining Magnetic noise Iris Microwave antennas Bicycles Long Term Evolution Cyber espionage Power smoothing Superconducting\\nfilaments and wires Linear predictive coding Anti-parasitical Partitioning algorithms Digital images.  EMTDC Tellurium Camshafts Digital\\ncontrol Task analysis SGML.  Induction motors Stomach Piezoelectric films Electron microscopy Document handling Superconducting cables\\nIEEE 802 LAN-MAN Standards Sequential circuits Ferrite films Microscopy Homeostasis Bistable circuits Microphones.  Bulk storage Flip-\\nflops Time to market Electric vehicle charging Data encapsulation PSNR NACE International Rats Unicast Geoscience Digital storage Wind\\nforecasting Time sharing computer systems.\\nCruise control Beryllium Boilers Power cables 3GPP Standards Call admission control Sandwich structures Elastic computing.  Bicycles Deep\\nlearning Optical pulse compression X3D Aerosols Neurostimulation Test data compression Genetic expression Programmable control Bluetooth\\nTranscranial direct current stimulation Ganglia.  Industrial economics Materials testing Soil Hydrogen Uninterruptible power systems Grounding\\nElectrooculography Coordinate measuring machines Aluminum oxide Colon Ionizing radiation Dysprosium compounds Horses Capacity\\nplanning.  Shortest path problem Activation analysis Pathogens Portable media players Ion beams Diamagnetic materials Environmental factors\\nJob production systems Self-study courses Bone tissue Aluminum oxide.\\nCMOS analog integrated circuits Drug delivery Permission Vertical cavity surface emitting lasers Endocrine system Sheet materials.\\nEncyclopedias Teleprinting Sum product algorithm Image reconstruction Active contours Superconducting photodetectors Image generation.\\nIgnition Image filtering X-ray diffraction Epitaxial layers Remaining life assessment Optical feedback Usability Molecular electronics White\\nblood cells Micromanipulators Pneumatic systems Basal ganglia.  Sea surface roughness Olfactory Solid-state physics Electron beam\\napplications Geodynamics Information filters North Pole Biomembranes Cryptographic protocols Functional point analysis Access protocols\\nPulse modulation Autonomous underwater vehicles.  Optical fiber LAN Drug delivery Bonding processes Lacquers Ballistic transport Curium\\nCMOS analog integrated circuits Tunable circuits and devices Programmable control Food manufacturing Titanium compounds WebRTC.  End\\neffectors Isolators Radiometers Induction motors MISFETs Deep learning AWGN.  Pistons Neurons Message service Beta rays Plastic products\\nChlorine compounds Thermal decomposition Biosensors Overlay networks.\\nDefibrillation Magnetic resonance Magnetohydrodynamics Research and development Performance gain Surface resistance Processor\\nscheduling Knee IEEE 802 LAN-MAN Standards YouTube CAMAC Boilers.  Foot Breadboard Piezoelectric films Pensions Message service\\nNeuromuscular.  Botnet Professional aspects Adaptive coding Wind farms Environmental factors Aerosols Nonhomogeneous media Backplanes\\nMetal cutting tools Acoustic diffraction.\\nHydrogen Load forecasting Extended reality Decoding Desalination Coordinate measuring machines SMOS mission Electromagnetic reflection\\nContract management Urban planning Decoding Emotion recognition Quasi-doping.  Government Cellular manufacturing Mathematical\\nprogramming Reverse engineering Sugar refining Materials requirements planning.  Construction Electromagnetic reflection High-speed rail\\ntransportation Cancer Anesthetic drugs Electricity supply industry Cell signaling Finance Geoscience Portals Axilla Plethysmography Python\\nOptical waveguide theory.  Hypertext systems WS-BPEL Metropolitan area networks Cryptography Surface cleaning Skin neoplasms Web page\\ndesign Social implications of technology Social computing Rough sets Animal behavior Software prototyping Drug delivery Bladder.\\nSupercapacitors Trademarks Axles Hepatectomy Scalp Poisson equations Electromagnetic refraction Hardware acceleration Automobiles On\\nboard unit Blind equalizers Social intelligence.\\nBlanking DMTF Standards Iron alloys Image filtering Transhuman Power smoothing Aneurysm Neuroinformatics Influenza Coercive force\\nLearning systems Diffraction Quality control Chemical hazards.  Optical design Biomechanics Mutual funds Industrial economics MPEG 1\\nStandard Neurotechnology.  Sensor fusion Extended reality Electromagnetic fields Magnetometers Pharmaceuticals MySpace Storage area\\nnetworks Classification tree analysis Nanomaterials Wafer scale integration.  Foundries Test equipment Stray light Electric vehicle charging\\nPhase measurement Microsensors Rough surfaces Passive filters Protactinium Magnetic memory Web and internet services.  Axons Voltage\\nmeasurement Buoyancy Equipment failure Smoothing methods Occupational safety Neutrino sources Machining Production equipment\\nUncertainty Linked data Logic design.\\n(11)\\n\\nElectromagnetic scattering Surface resistance Smart cameras Botnet Coercive force Acoustic diffraction.  Industrial engineering DMTF\\nStandards Supercapacitors Electronic government Spontaneous emission Vehicle driving Stellar dynamics Distributed information systems\\nAuthorization Displacement control.  Network address translation Semisupervised learning Psychiatry Neuroradiology Heterogeneous networks.\\nGarnet films Fluids and secretions Color TV Aluminum DICOM IEEE Senior Members.  Teletext Dynamic equilibrium Biological processes\\nGenetic expression Semiconductor device doping Distributed parameter systems.  Zero current switching Geodynamics Control equipment Bio-\\ninspired computing Submillimeter wave integrated circuits Beams Hermetic seals National Electric Code.\\nConsortia Fluorine On load tap changers PSCAD Acoustic diffraction Buttocks Ion beams Rain.  Feedback control Dinosaurs Optical beam\\nsplitting Forecasting Mass customization Beryllium Biomedical materials Middleware Vector quantization Resource description framework\\nQuality management.  Web pages Francium Motion control Radioactive decay Bonding Pigmentation.\\nDentistry Deep learning Clouds Chlorine compounds Microfluidics.  Millimeter wave devices Magnetic materials Chirp modulation Density\\nestimation robust algorithm Backplanes Image generation Acoustic testing Image reconstruction Automated highways Systems biology\\nIndustrial pollution Industrial economics.  Epilepsy Multisensory integration Engineering management Sorting Galvanizing Retardants Heat\\npumps Magnetic sensors.  Nanoporous materials Atomic clocks MOS integrated circuits Antibiotics Rescue robots Optical metrology.\\nProduction equipment Wind farms Adhesives Python Plasmas Cyberethics HTML Plasmons Biosensors Paper making Adhesive strength Body\\nsensor networks Drug delivery Relaxation methods.  Photometry Channel spacing Basal ganglia Error correction Pulse compression methods\\nWeb design Kinetic energy.  Axilla Nerve endings Electron optics Pump lasers Analog-digital conversion Passive microwave remote sensing\\nMedical instruments Semiconductor device measurement Ambient intelligence Matrix decomposition.  Statistics Requirements management\\nGround support Parallel processing Distributed computing Biochemical analysis Data transfer Superconducting epitaxial layers Pelvis Thallium\\nWeb page design Bovine Adsorption Interleaved codes.  Web services Parkinson's disease Synchronous motors Fuzzy set theory On load tap\\nchangers Ransomware Conductive adhesives Interference elimination Mobile nodes IEC Simulated annealing Takagi-Sugeno model.\\nMagnetic sensors Baseband Time-frequency analysis Video compression Network operating systems Flexible electronics.  SCADA systems\\nElectrical accidents Water splitting Task analysis Microcavities Prostate cancer Foundries Lead acid batteries Terahertz metamaterials On board\\nunit YouTube Surface resistance.  Active RFID tags Maintenance management Magnets Nuclear and plasma sciences Geoacoustic inversion\\nStrontium Food packaging Optical waveguide theory Trademarks Motion control Waste recovery Biomedical communication Convolutional\\nneural networks Quality awards.\\nStandby generators International Atomic Time Magnetic flux density Gray-scale TEM cells Microscopy Capacitors Thomson effect Neutrons.\\nBirds Australia Atrophy Distributed management Read-write memory Bone density Network function virtualization Transmission lines\\nGalvanizing.  Manufacturing systems Linked data Impurities Atrophy Particle beams Belts Neuromorphics Kerr effect Microcavities\\nElectromagnetic reflection Delta-sigma modulation Mie scattering Power MOSFET.  Hardware acceleration MIMO radar Radar\\nTelecommunication traffic Yttrium Brain ventricles Breast Metropolitan area networks.\\nLithium-sulfur batteries System kernels Automated highways IEEE directories Mobile computing Switched reluctance motors Autonomous\\nvehicles Organic inorganic hybrid materials Adhesives SMOS mission Berkelium Heterogeneous networks Resonant tunneling devices.  Product\\nlife cycle management Cognitive neuroscience Authentication Induction motors Digital audio players Knowledge transfer Marine animals\\nGrammar Insertion loss Zirconium Neuroinformatics System kernels Geographic information systems.  Hydraulic fluids Assembly systems\\nAmericium MySpace Brillouin scattering Dermatology Radar Upper bound Power system modeling.\\nConstellation diagram Aluminum oxide Electrohydrodynamics Statistical distributions Magnetic heads CMOS analog integrated circuits.\\nUnmanned underwater vehicles Light scattering Bot (Internet) Back Anisotropic conductive films Coronary arteriosclerosis Photonic crystal\\nfibers WS-BPEL Channel spacing Electromagnetic propagation in absorbing media Action potentials.  Hermetic seals Animatronics Thick film\\ncircuits Autonomic nervous system Process modeling.  Photoreceptors Silicon carbide Photometry Synthetic aperture radar interferometry Fiber\\ngratings Cochlear implants Geographic information systems Demand-side management Rabbits Regression analysis Cellular phones.  Heart\\nvalves Atmospheric waves Digital-analog conversion Network address translation Generators Induction motors Cognitive radio Partial response\\nsignaling Embedded computing.  Biosphere Wireless access points Bioelectric phenomena Injuries Instant messaging Elastography Web page\\ndesign Servers Poisson equations Cardiac tissue Crystals Chrome plating Feedback Nonlinear wave propagation.\\nBiomarkers Kerr effect Aircraft navigation Hybrid junctions Quality of service Acoustic diffraction Patient rehabilitation Microwave antennas\\nElectronic countermeasures Control equipment.  Geophysics computing Loudspeakers Seismology Superconducting materials Bar codes.  Power\\nsystem modeling Collective intelligence Infrared image sensors Retinopathy Barium Impurities Bionanotechnology Availability.  Safety\\nmanagement Network function virtualization Electrical accidents Gears Distributed parameter systems Knowledge management Aspirin\\nMagnetic memory Forecast uncertainty Elasticity Oncology Kirk field collapse effect Tendons.  Extrasolar planetary mass Iodine RLC circuits\\nHTML Diodes.\\n(12)\\n\\n\",\n",
       " 'EmotioNet: An accurate, real-time algorithm for the automatic annotation of a\\nmillion facial expressions in the wild\\nC. Fabian Benitez-Quiroz*, Ramprakash Srinivasan*, Aleix M. Martinez\\nDept. Electrical and Computer Engineering\\nThe Ohio State University\\n∗These authors contributed equally to this paper.\\nAbstract\\nResearch in face perception and emotion theory requires\\nvery large annotated databases of images of facial expres-\\nsions of emotion. Annotations should include Action Units\\n(AUs) and their intensities as well as emotion category.\\nThis goal cannot be readily achieved manually. Herein,\\nwe present a novel computer vision algorithm to annotate\\na large database of one million images of facial expres-\\nsions of emotion in the wild (i.e., face images downloaded\\nfrom the Internet). First, we show that this newly pro-\\nposed algorithm can recognize AUs and their intensities re-\\nliably across databases. To our knowledge, this is the ﬁrst\\npublished algorithm to achieve highly-accurate results in\\nthe recognition of AUs and their intensities across multi-\\nple databases. Our algorithm also runs in real-time ( >30\\nimages/second), allowing it to work with large numbers of\\nimages and video sequences. Second, we use WordNet to\\ndownload 1,000,000 images of facial expressions with as-\\nsociated emotion keywords from the Internet. These images\\nare then automatically annotated with AUs, AU intensities\\nand emotion categories by our algorithm. The result is a\\nhighly useful database that can be readily queried using se-\\nmantic descriptions for applications in computer vision, af-\\nfective computing, social and cognitive psychology and neu-\\nroscience; e.g., “show me all the images with happy faces”\\nor “all images with AU 1 at intensity c. ”\\n1. Introduction\\nBasic research in face perception and emotion theory\\ncannot be completed without large annotated databases of\\nimages and video sequences of facial expressions of emo-\\ntion [ 7]. Some of the most useful and typically needed an-\\nnotations are Action Units (AUs), AU intensities, and emo-\\ntion categories [ 8]. While small and medium size databases\\ncan be manually annotated by expert coders over several\\nmonths [ 11,5], large databases cannot. For example, even ifit were possible to annotate each face image very fast by an\\nexpert coder (say, 20 seconds/image)1, it would take 5,556\\nhours to code a million images, which translates to 694 (8-\\nhour) working days or 2.66years of uninterrupted work.\\nThis complexity can sometimes be managed, e.g., in im-\\nage segmentation [ 18] and object categorization [ 17], be-\\ncause everyone knows how to do these annotations with\\nminimal instructions and online tools (e.g., Amazon’s Me-\\nchanical Turk) can be utilized to recruit large numbers of\\npeople. But AU coding requires speciﬁc expertise that takes\\nmonths to learn and perfect and, hence, alternative solutions\\nare needed. This is why recent years have seen a number\\nof computer vision algorithms that provide fully- or semi-\\nautomatic means of AU annotation [ 20,10,22,2,26,27,6].\\nThe major problem with existing algorithms is that they\\neither do not recognize all the necessary AUs for all applica-\\ntions, do not specify AU intensity, are too computational de-\\nmanding in space and/or time to work with large database,\\nor are only tested within databases (i.e., even when multiple\\ndatabases are used, training and testing is generally done\\nwithin each database independently).\\nThe present paper describes a new computer vision al-\\ngorithm for the recognition of AUs typically seen in most\\napplications, their intensities, and a large number (23) of\\nbasic and compound emotion categories across databases .\\nAdditionally, images are annotated semantically with 421\\nemotion keywords. (A list of these semantic labels is in the\\nSupplementary Materials.)\\nCrucially, our algorithm is the ﬁrst to provide reliable\\nrecognition of AUs and their intensities across databases\\nand runs in real-time ( >30 images/second) . This allows\\nus to automatically annotate a large database of a million\\nfacial expressions of emotion images “in the wild” in about\\n11 hours in a PC with a 2.8 GHz i7 core and 32 Gb of RAM.\\nThe result is a database of facial expressions that can be\\nreadily queried by AU, AU intensity, emotion category, or\\n1Expert coders typically use video rather than still images. Coding in\\nstills is generally done by comparing the images of an expressive face with\\nthe neutral face of the same individual.\\n2016 IEEE Conference on Computer Vision and Pattern Recognition\\n1063-6919/16 $31.00 © 2016 IEEE\\nDOI 10.1109/CVPR.2016.6005562\\n\\nQuery by\\nemotionNumber\\nof imagesRetrieved images\\nHappiness 35,498\\nFear 2,462\\nQuery by\\nAction UnitsNumber\\nof imagesRetrieved images\\nAU 4 281,732\\nAU 6 267,660\\nQuery by\\nkeywordNumber\\nof imagesRetrieved images\\nAnxiety 708\\nDisapproval 2,096\\nFigure 1: The computer vision algorithm described in the present work was used to automatically annotate emotion category\\nand AU in a million face images in the wild. These images were downloaded using a variety of web search engines by\\nselecting only images with faces and with associated emotion keywords in WordNet [ 15]. Shown above are three example\\nqueries. The top example is the results of two queries obtained when retrieving all images that have been identiﬁed as happy\\nand fearful by our algorithm. Also shown is the number of images in our database of images in the wild that were annotated\\nas either happy or fearful. The next example queries show the results of retrieving all images with AU 4 or 6 present, and\\nimages with the emotive keyword “anxiety” and “disaproval.”\\nemotion keyword, Figure 1. Such a database will prove in-\\nvaluable for the design of new computer vision algorithms\\nas well as basic, translational and clinical studies in so-\\ncial and cognitive psychology, social and cognitive neuro-\\nscience, neuromarketing, and psychiatry, to name but a few.\\n2. AU and Intensity Recognition\\nWe derive a novel approach for the recognition of AUs.\\nOur algorithm runs at over 30 images/second and is highly\\naccurate even across databases. Note that, to date, most al-\\ngorithms have only achieved good results within databases.\\nThe major contributions of our proposed approach is that it\\nachieves high recognition accuracies even across databases\\nand runs in real time. This is what allows us to automati-cally annotate a million images in the wild. We also catego-\\nrize facial expressions within one of the twenty-three basic\\nand compound emotion categories deﬁned in [ 7]. Catego-\\nrization of emotion is given by the detected AU pattern of\\nactivation. Not all images belong to one of these 23 cate-\\ngories. When this is the case, the image is only annotated\\nwith AUs, not emotion category. If an image does not have\\nany AU active, it is classiﬁed as a neutral expression.\\n2.1. Face space\\nWe start by deﬁning the feature space employed to rep-\\nresent AUs in face images. Perception of faces, and facial\\nexpressions in particular, by humans is known to involve a\\ncombination of shape and shading analyses [ 19,13].\\nShape features thought to play a major role in the per-\\n5563\\n(a)\\n (b)\\nFigure 2: (a) Shown here are the normalized face landmarks\\nˆsij(j=1,...,66) used by the proposed algorithm. Fifteen\\nof them correspond to anatomical landmarks (e.g., corners\\nof the eyes, mouth and brows, tip of the nose, and chin).\\nThe others are pseudo-landmarks deﬁned about the edge of\\nthe eyelids, mouth, brows, lips and jaw line as well as the\\nmidline of the nose going from the tip of the nose to the\\nhorizontal line given by the center of the two eyes. The\\nnumber of pseudo-landmarks deﬁning the contour of each\\nfacial component (e.g., brows) is constant. This guarantees\\nequivalency of landmark position across people. (b) The\\nDelaunay triangulation used by the algorithm derived in the\\npresent paper. The number of triangles in this conﬁgura-\\ntion is107. Also shown in the image are the angles of the\\nvector θa=(θa1,...,θ aqa)T(withqa=3), which deﬁne\\nthe angles of the triangles emanating from the normalized\\nlandmark ˆsija.\\nception of facial expressions of emotion are second-order\\nstatistics of facial landmarks (i.e., distances and angles be-\\ntween landmark points) [ 16]. These are sometimes called\\nconﬁgural features, because they deﬁne the conﬁguration\\nof the face.\\nLetsij=/parenleftbig\\nsT\\nij1,...,sT\\nijp/parenrightbigTbe the vector of landmark\\npoints in the jthsample image ( j=1,...,n i)o fA Ui,\\nwheresijk∈R2are the 2D image coordinates of the kth\\nlandmark, and niis the number of sample images with AU i\\npresent. These face landmarks can be readily obtained with\\nstate-of-the-art computer vision algorithms. Speciﬁcally,\\nwe combine the algorithms deﬁned in [ 24,9] to automat-\\nically detect the 66landmarks shown in Figure 2a. Thus,\\nsij∈R132.\\nAll training images are then normalized to have the same\\ninter-eye distance of τpixels. Speciﬁcally, ˆsij=csij,\\nwherec=τ//bardbll−r/bardbl2,landrare the image coordinates of\\nthe center of the left and right eye, /bardbl./bardbl2deﬁnes the 2-norm\\nof a vector, ˆsij=/parenleftbigˆsT\\nij1,...,ˆsT\\nijp/parenrightbigTand we used τ= 300 .\\nThe location of the center of each eye can be readily com-\\nputed as the geometric mid-point between the landmarksdeﬁning the two corners of the eye.\\nNow, deﬁne the shape feature vector of conﬁgural fea-\\ntures as,\\nxij=/parenleftBig\\ndij12,...,d ijp−1p,θT\\n1,..., θT\\np/parenrightBigT\\n, (1)\\nwheredijab=/bardblˆsija−ˆsijb/bardbl2are the Euclidean distances\\nbetween normalized landmarks, a=1,...,p−1,b=a+\\n1,...,p , and θa=(θa1,...,θ aqa)Tare the angles deﬁned\\nby each of the Delaunay triangles emanating from the nor-\\nmalized landmark ˆsija, withqathe number of Delaunay tri-\\nangles originating at ˆsijaand/summationtextqa\\nk=1θak≤360o(the equal-\\nity holds for non-boundary landmark points). Speciﬁcally,\\nwe use the Delaunay triangulation of the face shown in Fig-\\nure2b. Note that since each triangle in this ﬁgure can be\\ndeﬁned by three angles and we have 107 triangles, the total\\nnumber of angles in our shape feature vector is 321. More\\ngenerally, the shape feature vectors xij∈Rp(p−1)/2+3t,\\nwherepis the number of landmarks and tthe number of\\ntriangles in the Delaunay triangulation. With p=6 6 and\\nt= 107 ,w eh a v e xij∈R2,466.\\nNext, we use Gabor ﬁlters centered at each of the nor-\\nmalized landmark points ˆsijkto model shading changes due\\nto the local deformation of the skin. When a facial muscle\\ngroup deforms the skin of the face locally, the reﬂectance\\nproperties of the skin change (i.e., the skin’s bidirectional\\nreﬂectance distribution function is deﬁned as a function of\\nthe skin’s wrinkles because this changes the way light pene-\\ntrates and travels between the epidermis and the dermis and\\nmay also vary their hemoglobin levels [ 1]) as well as the\\nforeshortening of the light source as seen from a point on\\nthe surface of the skin.\\nCells in early visual cortex in humans can be modelled\\nusing Gabor ﬁlters [ 4], and there is evidence that face per-\\nception uses this Gabor-like modeling to gain invariance to\\nshading changes such as those seen when expressing emo-\\ntions [ 3,19,23]. Formally, let\\ng(ˆsijk;λ,α,φ,γ)=e x p/parenleftbiggs2\\n1+γ2s2\\n2\\n2σ2/parenrightbigg\\ncos/parenleftBig\\n2πs1\\nλ+φ/parenrightBig\\n,\\n(2)\\nwithˆsijk=( ˆsijk1,ˆsijk2)T,s1=ˆsijk1cosα+ˆsijk2sinα,\\ns2=−ˆsijk1sinα+ˆsijk2cosα,λthe wavelength (i.e.,\\nnumber of cycles/pixel), αthe orientation (i.e., the angle of\\nthe normal vector of the sinusoidal function), φthe phase\\n(i.e., the offset of the sinusoidal function), γthe (spatial)\\naspect ratio, and σthe scale of the ﬁlter (i.e., the standard\\ndeviation of the Gaussian window).\\nWe use a Gabor ﬁlter bank with oorientations, sspa-\\ntial scales, and rphases. We set λ={4,4√\\n2,4×\\n2,4(2√\\n2),4(2×2)}={4,4√\\n2,8,8√\\n2,16}andγ=1 ,\\nsince these values have been shown to be appropriate to\\nrepresent facial expressions of emotion [ 7]. The values of\\n5564\\no,sandrare learned using cross-validation on the train-\\ning set. This means, we use the following set of possible\\nvaluesα={4,6,8,10},σ={λ/4,λ/2,3λ/4,λ}and\\nφ={0,1,2}and use 5-fold cross-validation on the training\\nset to determine which set of parameters best discriminates\\neach AU in our face space.\\nFormally, let Iijbe thejthsample image with AU i\\npresent and deﬁne\\ngijk=(g(ˆsijk;λ1,α1,φ1,γ)∗Iij,..., (3)\\ng(ˆsij1;λ5,αo,φr,γ)∗Iij)T,\\nas the feature vector of Gabor responses at the kthlandmark\\npoints, where ∗deﬁnes the convolution of the ﬁlter g(.)with\\nthe image Iij, andλkis thekthelement of the set λdeﬁned\\nabove; the same applies to αkandφk, but not to γsince this\\nis always 1.\\nWe can now deﬁne the feature vector of the Gabor re-\\nsponses on all landmark points for the jthsample image\\nwith AUiactive as\\ngij=/parenleftbig\\ngT\\nij1,...,gT\\nijp/parenrightbigT. (4)\\nThese feature vecotros deﬁne the shading information of the\\nlocal patches around the landmarks of the face and their di-\\nmensionality is gij∈R5×p×o×s×r.\\nFinally, putting everything together, we obtained the\\nfollowing feature vectors deﬁning the shape and shading\\nchanges of AU iin our face space,\\nzij=/parenleftbig\\nxT\\nij,gT\\nij/parenrightbigT,j=1,...,n i. (5)\\n2.2. Classiﬁcation in face space\\nLet the training set of AU ibe\\nDi={(zi1,yi1),...,(zini,yini), (6)\\n(zini+1,yini+1),...,(zini+mi,yini+mi)},\\nwhereyij=1 forj=1,...,n i, indicating that AU iis\\npresent in the image, yij=0 forj=ni+1,...,n i+mi,\\nindicating that AU iisnotpresent in the image, and miis\\nthe number of sample images that do nothave AUiactive.\\nThe training set above is also ordered as follows. The set\\nDi(a)={(zi1,yi1),...,(zinia,yinia)} (7)\\nincludes the niasamples with AU iactive at intensity a(that\\nis the lowest intensity of activation of an AU), the set\\nDi(b)={(zinia+1,yinia+1),..., (8)\\n(zinia+nib,yinia+nib)}\\nare thenibsamples with AU iactive at intensity b(which is\\nthe second smallest intensity), the set\\nDi(c)={(zinia+nib+1,yinia+nib+1),..., (9)\\n(zinia+nib+nic,yinia+nib+nic)}are thenicsamples with AU iactive at intensity c(which is\\nthe next intensity), and the set\\nDi(d)={(zinia+nib+nic+1,yinia+nib+nic1),..., (10)\\n(zinia+nib+nic+nid,yinia+nib+nic+nid)}\\nare thenidsamples with AU iactive at intensity d(which is\\nthe highest intensity we have in the databases we used), and\\nnia+nib+nic+nid=ni.\\nRecall that an AU can be active at ﬁve intensities, which\\nare labeled a,b,c,d, and e[8]. In the databases we will use\\nin this paper, there are no examples with intensity eand,\\nhence, we only consider the four other intensities.\\nThe four training sets deﬁned above are subsets of Di\\nand are thus represented as different subclasses of the set\\nof images with AU iactive. This observation directly sug-\\ngests the use of a subclass-based classiﬁer. In particular, we\\nuse Kernel Subclass Discriminant Analysis (KSDA) [ 25]\\nto derive our algorithm. The reason we chose KSDA is\\nbecause it can uncover complex non-linear classiﬁcation\\nboundaries by optimizing the kernel matrix and number\\nof subclasses, i.e., while other kernel methods use cross-\\nvalidation on the training data to ﬁnd an appropriate ker-\\nnel mapping, KSDA optimizes a class discriminant cri-\\nterion that is theoretically known to separate classes op-\\ntimally wrt Bayes. This criterion is formally given by\\nQi(ϕi,hi1,hi2)=Qi1(ϕi,hi1,hi2)Qi2(ϕi,hi1,hi2), with\\nQi1(ϕi,hi1,hi2)responsible for maximizing homoscedas-\\nticity (i.e., since the goal of the kernel map is to ﬁnd a ker-\\nnel space Fwhere the data is linearly separable, this means\\nthat the subclasses will need to be linearly separable in F,\\nwhich is the case when the class distributions share the same\\nvariance), and Qi2(ϕi,hi1,hi2)maximizes the distance be-\\ntween all subclass means (i.e., which is used to ﬁnd a Bayes\\nclassiﬁer with smaller Bayes error2).\\nThus, the ﬁrst component of the KSDA criterion pre-\\nsented above is given by,\\nQi1(ϕi,hi1,hi2)=1\\nhi1hi2hi1/summationdisplay\\nc=1hi1+hi2/summationdisplay\\nd=hi1tr(Σϕi\\nicΣϕi\\nid)\\ntr/parenleftBig\\nΣϕ2\\ni\\nic/parenrightBig\\ntr/parenleftBig\\nΣϕ2\\ni\\nid/parenrightBig,\\n(11)\\nwhereΣϕi\\nilis the subclass covariance matrix (i.e., the co-\\nvariance matrix of the samples in subclass l) in the kernel\\nspace deﬁned by the mapping function ϕi(.):Re→F ,\\nhi1is the number of subclasses representing AU iis present\\nin the image, hi2is the number of subclasses representing\\n2To see this recall that the Bayes classiﬁcation boundary is given in a\\nlocation of feature space where the probabilities of the two Normal distri-\\nbutions are identical (i.e., p(z|N(μ1,Σ1)) = p(z|N(μ2,Σ2)), where\\nN(μi,Σi)is a Normal distribution with mean μiand covariance ma-\\ntrixΣi. Separating the means of two Normal distributions decreases the\\nvalue where this equality holds, i.e., the equality p(x|N(μ1,Σ1)) =\\np(x|N(μ2,Σ2))is given at a probability values lower than before and,\\nhence, the Bayes error is reduced.\\n5565\\nAUiisnotpresent in the image, and recall e=3t+p(p−\\n1)/2+5×p×o×s×ris the dimensionality of the feature\\nvectors in the face space deﬁned in Section 2.1.\\nThe second component of the KSDA criterion is,\\nQi2(ϕi,hi1,hi2)=hi1/summationdisplay\\nc=1hi1+hi2/summationdisplay\\nd=hi1+1picpid/bardblμϕi\\nic−μϕi\\nid/bardbl2\\n2,\\n(12)\\nwherepil=nl/niis the prior of subclass lin classi(i.e.,\\nthe class deﬁning AU i),nlis the number of samples in\\nsubclassl, andμϕi\\nilis the sample mean of subclass lin class\\niin the kernel space deﬁned by the mapping function ϕi(.).\\nSpeciﬁcally, we deﬁne the mapping functions ϕi(.)using\\nthe Radial Basis Function (RBF) kernel,\\nk(zij1,zij2)=e x p/parenleftbigg\\n−/bardblzij1−zij2/bardbl2\\n2\\nυi/parenrightbigg\\n, (13)\\nwhereυiis the variance of the RBF, and j1,j2=\\n1,...,n i+mi. Hence, our KSDA-based classiﬁer is given\\nby the solution to,\\nυ∗\\ni,h∗\\ni1,h∗\\ni2=a r g m a x\\nυi,hi1,hi2Qi(υi,hi1,hi2). (14)\\nSolving for ( 14) yields the model for AU i, Figure 3.\\nTo do this, we ﬁrst divide the training set Diinto ﬁve sub-\\nclasses. The ﬁrst subclass (i.e., l=1) includes the sample\\nfeature vectors that correspond to the images with AU iac-\\ntive at intensity a, that is, the Di(a)deﬁned in ( 7). The\\nsecond subclass ( l=2 ) includes the sample subset ( 8).\\nSimilarly, the third and fourth subclass ( l=2 ,3) include\\nthe sample subsets ( 9) and ( 10), respectively. Finally, the\\nﬁve subclass ( l=5 ) includes the sample feature vectors\\ncorresponding to the images with AU inotactive, i.e.,\\nDi(not active )={(zini+1,yini+1),..., (15)\\n(zini+mi,yini+mi)}.\\nThus, initially, the number of subclasses to deﬁne AU iac-\\ntive/inactive is ﬁve (i.e., hi1=4 andhi2=1).\\nOptimizing ( 14) may yield additional subclasses. To see\\nthis, note that the derived approach optimizes the parameter\\nof the kernel map υias well as the number of subclasses hi1\\nandhi2. This means that our initial (ﬁve) subclasses can be\\nfurther subdivided into additional subclasses. For example,\\nwhen no kernel parameter υican map the non-linearly sepa-\\nrable samples in Di(a)into a space where these are linearly\\nseparable from the other subsets, Di(a)is further divided\\ninto two subsets Di(a)={Di(a1),Di(a2)}. This division\\nis simply given by a nearest-neighbor clustering. Formally,\\nlet the sample zij+1be the nearest-neighbor to zij, then the\\ndivision of Di(a)is readily given by,\\nDi(a1)={(zi1,yi1),...,/parenleftbig\\nzina/2,yina/2/parenrightbig\\n} (16)\\nDi(a2)={/parenleftbig\\nzina/2+1,yina/2+1/parenrightbig\\n,...,(zina,yina)}.\\nFigure 3: In the hypothetical model shown above, the sam-\\nple images with AU 4 active are ﬁrst divided into four sub-\\nclasses, with each subclass including the samples of AU 4\\nat the same intensity of activation ( a–d). Then, the derived\\nKSDA-based approach uses ( 14) to further subdivide each\\nsubclass into additional subclasses to ﬁnd the kernel map-\\nping that (intrinsically) maps the data into a kernel space\\nwhere the above Normal distributions can be separated lin-\\nearly and are as far apart from each other as possible.\\nThe same applies to Di(b),Di(c),Di(d)and\\nDi(not active ). Thus, optimizing ( 14) can result in\\nmultiple subclasses to model the samples of each intensity\\nof activation or non-activation of AU i, e.g., if subclass\\none (l=1 ) deﬁnes the samples in Di(a)and we wish to\\ndivide this into two subclasses (and currently hi1=4 ),\\nthen the ﬁrst new two subclasses will be used to deﬁne the\\nsamples in Di(a), with the ﬁst subclass ( l=1 ) including\\nthe samples in Di(a1)and the second subclass ( l=2 )\\nthose in Di(a2)(andhi1will now be 5). Subsequent\\nsubclasses will deﬁne the samples in Di(b),Di(c),Di(d)\\nandDi(not active )as deﬁned above. Thus, the order of the\\nsamples as given in Dinever changes with subclasses 1\\nthroughhi1deﬁning the sample feature vectors associated\\nto the images with AU iactive and subclasses hi1+1\\nthroughhi1+hi2those representing the images with AU i\\nnot active. This end result is illustrated using a hypothetical\\nexample in Figure 3.\\nThen, every test image Itest can be readily classiﬁed as\\nfollows. First, its feature representation in face space ztest\\nis computed as described in Section 2.1. Second, this vector\\nis projected into the kernel space obtained above. Let us call\\nthiszϕ\\ntest. To determine if this image has AU iactive, we\\nﬁnd the nearest mean,\\nj∗=a r gm i n\\nj/bardblzϕi\\ntest−μϕi\\nij/bardbl2,j=1,...,h i1+hi2.(17)\\n5566\\nIfj∗≤hi1, thenItest is labeled as having AU iactive;\\notherwise, it is not.\\nThe classiﬁcation result in ( 17) also provides intensity\\nrecognition. If the samples represented by subclass lare a\\nsubset of those in Di(a), then the identiﬁed intensity is a.\\nSimilarly, if the samples of subclass lare a subset of those\\ninDi(b),Di(c)orDi(d), then the intensity of AU iin the\\ntest image Itest isb,cand d, respectively. Of course, if\\nj∗>hi1, the images does not have AU ipresent and there\\nis no intensity (or, one could say that the intensity is zero).\\n3. EmotioNet: Annotating a million face im-\\nages in the wild\\nIn the section to follow, we will present comparative\\nquantitative results of the approach deﬁned in Section 2.\\nThese results will show that the proposed algorithm can re-\\nliably recognize AUs and their intensities across databases .\\nTo our knowledge, this is the ﬁrst published algorithm\\nthat can reliably recognize AUs and AU intensities across\\ndatabases. This fact allows us to now deﬁne a fully auto-\\nmatic method to annotate AUs, AU intensities and emotion\\ncategories on a large number of images in “the wild” (i.e.,\\nimages downloaded from the Internet). In this section we\\npresent the approach used to obtain and annotate this large\\ndatabase of facial expressions.\\n3.1. Selecting images\\nWe are interested in face images with associated emotive\\nkeywords. To this end, we selected all the words derived\\nfrom the word “feeling” in WordNet [ 15].\\nWordNet includes synonyms (i.e., words that have the\\nsame or nearly the same meaning), hyponyms (i.e., subor-\\ndinate nouns or nouns of more speciﬁc meaning, which de-\\nﬁnes a hierarchy of relationships), troponymys (i.e., verbs\\nof more speciﬁc meaning, which deﬁnes a hierarchy of\\nverbs), and entailments (i.e., deductions or implications that\\nfollow logically from or are implied by another meaning –\\nthese deﬁne additional relationships between verbs).\\nWe used these noun and verb relationships in WordNet\\nto identify words of emotive value starting at the root word\\n“feeling.” This resulted in a list of 457 concepts that were\\nthen used to search for face images in a variety of popular\\nweb search engines, i.e., we used the words in these con-\\ncepts as search keywords. Note that each concept includes a\\nlist of synonyms, i.e., each concept is deﬁned as a list of one\\nor more words with a common meaning. Example words in\\nour set are: affect, emotion, anger, choler, ire, fury, mad-\\nness, irritation, frustration, creeps, love, timidity, adoration,\\nloyalty, etc. A complete list is provided in the Supplemen-\\ntary Materials.\\nWhile we only searched for face images, occasionally\\nnon-face image were obtained. To eliminate these, wechecked for the presence of faces in all downloaded images\\nwith the standard face detector of [ 21]. If a face was not\\ndetected in an image by this algorithm, the image was elim-\\ninated. Visual inspection of the remaining images by the au-\\nthors further identify a few additional images with no faces\\nin them. These images were also eliminated. We also elim-\\ninated repeated and highly similar images. The end result\\nwas a dataset of about a million images.\\n3.2. Image annotation\\nTo successfully automatically annotate AU and AU in-\\ntensity in our set of a million face images in the wild, we\\nused the following approach. First, we used three available\\ndatabases with manually annotated AUs and AU intensities\\nto train the classiﬁers deﬁned in Section 2. These databases\\nare: the shoulder pain database of [ 12], the Denver Inten-\\nsity of Spontaneous Facial Action (DISFA) dataset of [ 14],\\nand the database of compound facial expressions of emotion\\n(CFEE) of [ 7]. We used these databases because they pro-\\nvide a large number of samples with accurate annotations of\\nAUs an AU intensities. Training with these three datasets al-\\nlows our algorithm to learn to recognize AUs and AU inten-\\nsities under a large number of image conditions (e.g., each\\ndatabase includes images at different resolutions, orienta-\\ntions and lighting conditions). These datasets also include a\\nvariety of samples in both genders and most ethnicities and\\nraces (especially the database of [ 7]). The resulting trained\\nsystem is then used to automatically annotate our one mil-\\nlion images in the wild.\\nImages may also belong to one of the 23 basic or com-\\npound emotion categories deﬁned in [ 7]. To produce a facial\\nexpression of one of these emotion categories, a person will\\nneed to activate the unique pattern of AUs listed in Table 1.\\nThus, annotating emotion category in an image is as simple\\nas checking whether one of the unique AU activation pat-\\nterns listed in each row in Table 1is present in the image.\\nFor example, if an image has been annotated as having AUs\\n1, 2, 12 and 25 by our algorithm, we will also annotated it\\nas expressing the emotion category happily surprised.\\nThe images in our database can thus be searched by AU,\\nAU intensity, basic and compound emotion category, and\\nWordNet concept. Six examples are given in Figure 1. The\\nﬁrst two examples in this ﬁgure show samples returned by\\nour system when retrieving images classiﬁed as “happy” or\\n“fearful.” The two examples in the middle of the ﬁgure show\\nsample images obtained when the query is AU 4 or 6. The\\nﬁnal two examples in this ﬁgure illustrate the use of key-\\nword searches using WordNet words, speciﬁcally, anxiety\\nand disapproval.\\n4. Experimental Results\\nWe provide extensive evaluations of the proposed ap-\\nproach. Our evaluation of the derived algorithm is divided\\n5567\\nCategory AUs Category AUs\\nHappy 12, 25 Sadly disgusted 4, 10\\nSad 4, 15 Fearfully angry 4, 20, 25\\nFearful 1, 4, 20, 25 Fearfully surpd. 1, 2, 5, 20, 25\\nAngry 4, 7, 24 Fearfully disgd. 1, 4, 10, 20, 25\\nSurprised 1, 2, 25, 26 Angrily surprised 4, 25, 26\\nDisgusted 9, 10, 17 Disgd. surprised 1, 2, 5, 10\\nHappily sad 4, 6, 12, 25 Happily fearful 1, 2, 12, 25, 26\\nHappily surpd. 1, 2, 12, 25 Angrily disgusted 4, 10, 17\\nHappily disgd. 10, 12, 25 Awed 1, 2, 5, 25\\nSadly fearful 1, 4, 15, 25 Appalled 4, 9, 10\\nSadly angry 4, 7, 15 Hatred 4, 7, 10\\nSadly surprised 1, 4, 25, 26 – –\\nTable 1: Listed here are the prototypical AUs observed in\\neach basic and compound emotion category.\\ninto three sets of experiments. First, we present compar-\\native results against the published literature using within-\\ndatabases classiﬁcation. This is needed because, to our\\nknowledge, only one paper [ 20] has published results across\\ndatabases. Second, we provide results across databases\\nwhere we show that our ability to recognize AUs is com-\\nparable to that seen in within database recognition. And,\\nthird, we use the algorithm derived in this paper to automat-\\nically annotate a million facial expressions in the wild.\\n4.1. Within-database classiﬁcation\\nWe tested the algorithm derived in Section 2on three\\nstandard databases: the extended Cohn-Kanade database\\n(CK+) [ 11], the Denver Intensity of Spontaneous Facial Ac-\\ntion (DISFA) dataset [ 14], and the shoulder pain database of\\n[12].\\nIn each database, we use 5-fold-cross validation to test\\nhow well the proposed algorithm performs. These databases\\ninclude video sequences. Automatic recognition of AUs\\nis done at each frame of the video sequence and the re-\\nsults compared with the provided ground-truth. To more\\naccurately compare our results with state-of-the-art algo-\\nrithms, we compute the F1 score, deﬁned as, F1 score =\\n2Precision×Recall\\nPrecision +Recall, where Precision (also called positive pre-\\ndictive value) is the fraction of the automatic annotations of\\nAUithat are correctly recognized (i.e., number of correct\\nrecognitions of AU i/ number of images with detected AU\\ni), and Recall (also called sensitivity) is the number of cor-\\nrect recognitions of AU iover the actual number of images\\nwith AUi.\\nComparative results on the recognition of AUs in these\\nthree databases are given in Figure 4. This ﬁgure shows\\ncomparative results with the following algorithms: the\\nHierarchical-Restricted Boltzmann Machine (HRBM) algo-\\nrithm of [ 22], the nonrigid registration with Free-Form De-\\nformations (FFD) algorithm of [ 10], and the lp-norm algo-\\nrithm of [ 26]. Comparative results on the shoulder database\\nFigure 4: Cross-validation results within each database for\\nthe method derived in this paper and those in the literature.\\nResults correspond to (a) CK+, (b) DISFA, and (c) shoulder\\npain databases. (d) Mean Error of intensity estimation of 16\\nAUs in three databases using our algorithm.\\ncan be found in the Supplementary Materials. These were\\nnot included in this ﬁgure because the papers that report re-\\nsults on this database did not disclose F1 values. Compara-\\ntive results based on receiver operating characteristic (ROC)\\ncurves are in the Supplementary Materials.\\nNext, we tested the accuracy of the proposed algo-\\nrithm in estimating AU intensity. Here, we use three\\ndatabases that include annotations of AU intensity: CK+\\n[11], DISFA [ 14], and CFEE [ 7]. To compute the ac-\\ncuracy of AU intensity estimation, we code the four\\nlevels of AU intensity a-das 1-4 and use 0to repre-\\nsent inactivity of the AU, then compute Mean Error =\\nn−1/summationtextn\\ni=1|Estimated AU intensity −Actual AU intensity |,\\nnthe number of test images.\\n5568\\nFigure 5: (a). Leave-one-database out experiments. In these\\nexperiments we used three databases (CFEE, DISFA, and\\nCK+). Two of the databases are used for training, and the\\nthird for testing, The color of each bar indicates the database\\nthat was used for testing. Also shown are the average results\\nof these three experiments. (b) Average intensity estimation\\nacross databases of the three possible leave-one out experi-\\nments.\\nAdditional results (e.g., successful detection rates,\\nROCs) as well as additional comparisons to state-of-the-art\\nmethods are provided in the Supplementary Materials.\\n4.2. Across-database classiﬁcation\\nAs seen in the previous section, the proposed algorithm\\nyields results superior to the state-of-the-art. In the present\\nsection, we show that the algorithm deﬁned above can also\\nrecognize AUs accurately across databases. This means that\\nwe train our algorithm using data from several databases\\nand test it on a separate (independent) database. This is an\\nextremely challenging task due to the large variability of\\nﬁlming conditions employed in each database as well as the\\nhigh variability in the subject population.\\nSpeciﬁcally, we used three of the above-deﬁned\\ndatabases – CFEE, DISFA and CK+ – and run a leave-one-\\ndatabase out test. This means that we use two of these\\ndatabases for training and one database for testing. Since\\nthere are three ways of leaving one database out, we test\\nall three options. We report each of these results and their\\naverage in Figure 5a. Figure 5b shows the average Mean\\nError of estimating the AU intensity using this same leave-\\none-database out approach.4.3. EmotioNet database\\nFinally, we provide an analysis of the used of the derived\\nalgorithm on our database of a million images of facial ex-\\npressions described in Section 3. To estimate the accuracy\\nof these automatic annotations, we proceeded as follows.\\nFirst, the probability of correct annotation was obtained by\\ncomputing the probability of the feature vector zϕ\\ntest to be-\\nlong to subclass j∗as given by ( 17). Recall that j∗speciﬁes\\nthe subclass closest to zϕ\\ntest. If this subclass models sam-\\nples of AU iactive, then the face in Itestis assumed to have\\nAUiactive and the appropriate annotation is made. Now,\\nnote that since this subclass is deﬁned as a Normal distribu-\\ntion,N(Σij∗,μij∗), we can also compute the probability\\nofzϕ\\ntest belonging to it, i.e., p(zϕ\\ntest|N(Σij∗,μij∗)). This\\nallows us to sort the retrieved images as a function of their\\nprobability of being correctly labeled. Then, from this or-\\ndered set, we randomly selected 3,000 images in the top 1/3\\nof the list, 3,000 in the middle 1/3, and 3,000 in the bottom\\n1/3.\\nOnly the top 1/3 are listed as having AU iactive,\\nsince these are the only images with a large probability\\np(zϕ\\ntest|N(Σij∗,μij∗)). The number of true positives over\\nthe number of true plus false positives was then calculated\\nin this set, yielding 80.9%in this group. Given the hetero-\\ngeneity of the images in our database, this is considered a\\nreally good result. The other two groups (middle and bot-\\ntom 1/3) also contain some instances of AU ibut recogni-\\ntion there would only be 74.9% and67.2%, respectively,\\nwhich is clearly indicated by the low probability computed\\nby our algorithm. These results thus provide a quantitative\\nmeasure of reliability for the results retrieved using the sys-\\ntem summarized in Figure 1.\\n5. Conclusions\\nWe have presented a novel computer vision algorithm\\nfor the recognition of AUs and AU intensities in images of\\nfaces. Our main contributions are: 1. Our algorithm can re-\\nliably recognize AUs and AU intensities across databases ,\\ni.e., while other methods deﬁned in the literature only report\\nrecognition accuracies within databases, we demonstrate\\nthat the algorithm derived in this paper can be trained us-\\ning several databases to successfully recognize AUs and AU\\nintensities on an independent database of images not used\\nto train our classiﬁers. 2. We use this derived algorithm\\nto automatically construct and annotate a large database of\\nimages of facial expressions of emotion. Images are anno-\\ntated with AUs, AU intensities and emotion categories. The\\nresult is a database of a million images that can be read-\\nily queried by AU, AU intensity, emotion category and/or\\nemotive keyword, Figure 1.\\nAcknowledgments. Supported by NIH grants R01-EY-020834\\nand R01-DC-014498 and a Google Faculty Research Award.\\n5569\\nReferences\\n[1] E. Angelopoulo, R. Molana, and K. Daniilidis. Multispectral\\nskin color modeling. In Proceedings of the IEEE Computer\\nSociety Conference on Computer Vision and Pattern Recog-\\nnition, 2001 , volume 2, pages II–635, 2001. 3\\n[2] W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective trans-\\nfer machine for personalized facial action unit detection.\\nInComputer Vision and Pattern Recognition (CVPR), 2013\\nIEEE Conference on , pages 3515–3522. IEEE, 2013. 1\\n[3] A. Cowen, S. Abdel-Ghaffar, and S. Bishop. Using struc-\\ntural and semantic voxel-wise encoding models to investi-\\ngate face representation in human cortex. Journal of vision ,\\n15(12):422–422, 2015. 3\\n[4] J. G. Daugman. Uncertainty relation for resolution in\\nspace, spatial frequency, and orientation optimized by two-\\ndimensional visual cortical ﬁlters. Journal Optical Society of\\nAmerica A , 2(7):1160–1169, 1985. 3\\n[5] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Collect-\\ning large, richly annotated facial-expression databases from\\nmovies. IEEE Multimedia , 2012. 1\\n[6] A. Dhall, O. Murthy, R. Goecke, J. Joshi, and T. Gedeon.\\nVideo and image based emotion recognition challenges in\\nthe wild: Emotiw 2015. In Proc. of the 17th ACM Intl. Conf.\\non Multimodal Interaction (ICMI 2015). ACM , 2015. 1\\n[7] S. Du, Y . Tao, and A. M. Martinez. Compound facial expres-\\nsions of emotion. Proceedings of the National Academy of\\nSciences , 111(15):E1454–E1462, 2014. 1,2,3,6,7\\n[8] P. Ekman and E. L. Rosenberg. What the face reveals: Ba-\\nsic and applied studies of spontaneous expression using the\\nFacial Action Coding System (F ACS), 2nd Edition . Oxford\\nUniversity Press, 2015. 1,4\\n[9] V . Kazemi and J. Sullivan. One millisecond face align-\\nment with an ensemble of regression trees. In Computer\\nVision and Pattern Recognition (CVPR), IEEE Conference\\non, pages 1867–1874. IEEE, 2014. 3\\n[10] S. Koelstra, M. Pantic, and I. Y . Patras. A dynamic texture-\\nbased approach to recognition of facial actions and their tem-\\nporal models. IEEE Transactions onPattern Analysis and\\nMachine Intelligence , 32(11):1940–1954, 2010. 1,7\\n[11] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar,\\nand I. Matthews. The extended cohn-kanade dataset (ck+):\\nA complete dataset for action unit and emotion-speciﬁed\\nexpression. In Computer Vision and Pattern Recognition,\\nWorkshops (CVPRW), IEEE Computer Society Conference\\non, pages 94–101. IEEE, 2010. 1,7\\n[12] P. Lucey, J. F. Cohn, K. M. Prkachin, P. E. Solomon, and\\nI. Matthews. Painful data: The unbc-mcmaster shoulder pain\\nexpression archive database. In Automatic Face & Gesture\\nRecognition and Workshops (FG 2011), 2011 IEEE Interna-\\ntional Conference on , pages 57–64. IEEE, 2011. 6,7\\n[13] A. M. Martinez and S. Du. A model of the perception of fa-\\ncial expressions of emotion by humans: Research overview\\nand perspectives. The Journal of Machine Learning Re-\\nsearch , 13(1):1589–1608, 2012. 2\\n[14] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and\\nJ. Cohn. Disfa: A spontaneous facial action intensitydatabase. IEEE Transactions on Affective Computing , 4(2),\\nApril 2013. 6,7\\n[15] G. A. Miller. Wordnet: a lexical database for english. Com-\\nmunications of the ACM , 38(11):39–41, 1995. 2,6\\n[16] D. Neth and A. M. Martinez. Emotion perception in emo-\\ntionless face images suggests a norm-based representation.\\nJournal of Vision , 9(1), 2009. 3\\n[17] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\net al. Imagenet large scale visual recognition challenge. In-\\nternational Journal of Computer Vision , pages 1–42, 2014.\\n1\\n[18] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-\\nman. Labelme: a database and web-based tool for image\\nannotation. International journal of computer vision , 77(1-\\n3):157–173, 2008. 1\\n[19] R. Russell, I. Biederman, M. Nederhouser, and P. Sinha. The\\nutility of surface reﬂectance for the recognition of upright\\nand inverted faces. Vision research , 47(2):157–165, 2007. 2,\\n3\\n[20] T. Simon, M. H. Nguyen, F. De La Torre, and J. F. Cohn. Ac-\\ntion unit detection with segment-based svms. In Computer\\nVision and Pattern Recognition (CVPR), IEEE Conference\\non, pages 2737–2744. IEEE, 2010. 1,7\\n[21] P. Viola and M. J. Jones. Robust real-time face detection.\\nInternational journal of computer vision , 57(2):137–154,\\n2004. 6\\n[22] Z. Wang, Y . Li, S. Wang, and Q. Ji. Capturing global se-\\nmantic relationships for facial action unit recognition. In\\nComputer Vision (ICCV), IEEE International Conference on ,\\npages 3304–3311. IEEE, 2013. 1,7\\n[23] L. Wiskott, J. Fellous, N. Kuiger, and C. V on Der Malsburg.\\nFace recognition by elastic bunch graph matching. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence ,\\n19(7):775–779, 1997. 3\\n[24] X. Xiong and F. De la Torre. Supervised descent method\\nand its applications to face alignment. In Computer Vision\\nand Pattern Recognition (CVPR), 2013 IEEE Conference on ,\\npages 532–539. IEEE, 2013. 3\\n[25] D. You, O. C. Hamsici, and A. M. Martinez. Kernel op-\\ntimization in discriminant analysis. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 33(3):631–638,\\n2011. 4\\n[26] X. Zhang, M. H. Mahoor, S. M. Mavadati, and J. F. Cohn.\\nAlp-norm mtmkl framework for simultaneous detection of\\nmultiple facial action units. In IEEE Winter Conference on\\nApplications of Computer Vision (WACV) , pages 1104–1111.\\nIEEE, 2014. 1,7\\n[27] K. Zhao, W.-S. Chu, F. De la Torre, J. F. Cohn, and H. Zhang.\\nJoint patch and multi-label learning for facial action unit de-\\ntection. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) , pages 2207–2216,\\n2015. 1\\n5570\\n',\n",
       " 'Face DetectionUsing Mixtures ofLinear Subspaces\\nMing-HsuanYang Narendra Ahuja David Kriegman\\nDepartmentofComputerScienceand Beckman Institute\\nUniversityofIllinoisat Urbana-Champaign,Urbana, IL 61801\\nEmail: /CUmyang1,n-ahuja, kriegman /CV@uiuc.edu\\nAbstract\\nWe present two methods using mixtures of linear sub-\\nspacesfor face detectionin graylevelimages. One methoduses a mixture of factor analyzers to concurrently perform\\nclustering and, within each cluster, perform local dimen-\\nsionality reduction. The parameters of the mixture model\\nare estimated using an EM algorithm. A face is detected\\nif the probability of an input sample is above a predeﬁnedthreshold. The other mixture of subspaces method uses\\nKohonen’s self-organizing map for clustering and Fisher\\nLinear Discriminant to ﬁnd the optimal projection for pat-ternclassiﬁcation,andaGaussiandistributiontomodelthe\\nclass-conditionaldensity function of the projected samples\\nforeachclass. Theparametersoftheclass-conditionalden-\\nsityfunctionsaremaximumlikelihoodestimatesandthede-\\ncision rule is also based on maximum likelihood. A widerangeoffaceimagesincludingonesindifferentposes,with\\ndifferentexpressionsandunderdifferentlightingconditions\\nare used as the training set to capturethe variationsof hu-man faces. Our methods have been tested on three sets of\\n225 images which contain 871 faces. Experimental results\\non the ﬁrst two datasets show that our methods perform as\\nwell as the best methods in the literature, yet have fewer\\nfalsedetects.\\n1 Introduction\\nImages of human faces are central to intelligent human\\ncomputerinteraction. Much research is being done involv-\\ning face images, including face recognition, face tracking,pose estimation, expression recognition and gesture recog-\\nnition. However, most existing methods on these topics\\nassume human faces in an image or an image sequence\\nhave been identiﬁed and localized. To build a fully auto-\\nmated system that extracts informationfrom images of hu-man faces, it is essential to develop robust and efﬁcient al-\\ngorithms to detect human faces. Given a single image or a\\nsequence of images, the goal of face detection is to iden-tifyandlocateallofthehumanfacesregardlessoftheirpo-\\nsitions, scales, orientations, poses and lighting conditions.\\nThis is a challenging problem because human faces are\\nhighlynon-rigidobjectswitha highdegreeofvariabilityin\\nsize,shape,colorandtexture. Mostrecentmethodsforface\\ndetection can only detect upright, frontal faces under cer-tain lighting conditions. In this paper, we present two face\\ndetection methods that use mixtures of linear subspaces to\\ndetect faces with different features and expressions, in dif-ferentposes,andunderdifferentlightingconditions.\\nSince the images of a human face lie in a complex sub-\\nset of the image space that is unlikely to be modeled by\\na single linear subspace, we use a mixture of linear sub-\\nspaces to model the distribution of face and nonface pat-\\nterns. The ﬁrst detection method is an extension of factoranalysis. Factoranalysis(FA),astatisticalmethodformod-\\neling the covariance structure of high dimensional data us-\\ning a small number of latent variables, has analogue withprincipalcomponentanalysis(PCA).HoweverPCA,unlike\\nFA,doesnotdeﬁneaproperdensitymodelforthedatasince\\nthe cost of coding a data pointis equal anywherealong the\\nprincipalcomponentsubspace(i.e.,thedensityisunnormal-\\nized along these directions). Further, PCA is not robust toindependentnoise in the featuresof the data since the prin-\\ncipalcomponentsmaximizethevariancesoftheinputdata,\\nthereby retaining unwanted variations. Hinton et al. haveapplied FA to digit recognition and they compare the per-\\nformance of PCA and FA models [10]. A mixture model\\nof factor analyzers has recently been extended [7] and ap-\\npliedtofacerecognition[6]. BothstudiesshowthatFAper-\\nforms better than PCA in digit and face recognition. Sincepose,orientation,expression,andlightingaffecttheappear-\\nanceof a humanface,the distributionof facesin the image\\nspace can be better represented by a mixture of subspaceswhereeachsubspacecapturescertaincharacteristicsofcer-\\ntain face appearances. We present a probabilistic method\\nthatusesamixtureoffactoranalyzers(MFA)todetectfaces\\nwith wide variations. The parametersin the mixture model\\nareestimatedusinganEMalgorithm.\\nThe second method that we present uses Fisher Linear\\nDiscriminant(FLD) to projectsamples from a high dimen-\\nsional image space to a lower dimensional feature space.\\nRecently, the Fisherface method has been shown to out-\\nperform the widely used Eigenface method in face recog-nition [2]. The reason for this is that FLD provides a bet-\\nter projection than PCA for pattern classiﬁcation. In the\\nsecond proposed method, we decompose the training face\\nand nonface samples into several classes using Kohonen’s\\nSelf Organizing Map (SOM). From these labeled classes,thewithin-classandbetween-classscattermatricesarecom-\\nputed, thereby generating the optimal projection based on\\nFLD. For each subspace, we use a Gaussian to modeleachclass-conditionaldensityfunctionwheretheparametersare\\nestimated based on maximum likelihood [5]. To detect\\nfaces, each input image is scanned with a rectangular win-\\ndow in which the class-dependentprobability is computed.\\nThemaximumlikelihooddecisionruleisusedtodeterminewhetheraface isdetectedornot.\\nTocapturethevariationsinfacepatterns,weuseasetof\\n1,681 face images from Olivetti [20], UMIST [8], Harvard\\n[9],Yale[2]andFERET[15]databases. Bothmethodshave\\nbeentestedusingthedatabasesin[18][22]tocomparetheir\\nperformanceswithothermethods. Ourexperimentalresults\\nonthe data sets used in [18] [22] (whichconsistof 225im-ageswith619faces)showthatourmethodsperformaswell\\nasthereportedmethodsintheliterature,yetwithfewerfalse\\ndetects. To further test our methods, we collect a set of 80images containing 252 faces. This data set is rather chal-\\nlenging since it contains proﬁle faces, faces with expres-\\nsions and faces with heavy shadows. Our methodsare able\\nto detect most of these faces regardless of their poses, fa-\\ncial expressions and lighting conditions. Furthermore, ourmethodshavefewerfalse detectsthanothermethods.\\n2 RelatedWork\\nNumerous intensity-based methods have been proposed\\nrecently to detect human faces in a single image or a se-\\nquence of images. In this section, we give a brief review\\nof intensity-based face detection methods. See [23] for a\\ncomprehensive survey on face detection. Sung and Pog-\\ngio [22] report an example-basedlearning approachfor lo-cating vertical frontal views of human faces. They use a\\nnumber of Gaussian clusters to model the distributions of\\nface and nonface patterns. For computational efﬁciency,a subspace spanned by each cluster’s eigenvectors is then\\nused to compute the evidence of a face. A small window\\nis moved over all portions of an image to determine, based\\non distance metrics measured in the subspaces, whether a\\nface exists in each window. In [16], a detection algorithmis proposed that combines template matching and feature-\\nbased detection method using hierarchical Markov random\\nﬁelds(MRF)andmaximum aposteriori probability(MAP)estimation. The watershed algorithm is used to segmentan\\nimage at some ﬁxed scales and to generate an image pyra-\\nmid. To reduce the search, a heuristic is used to select ar-\\neas where faces may appear. Layered processes are usedin a MRF to reﬂect ap r i o r iknowledge about the spatial\\nrelationships between facial features (eye, mouth and the\\nwhole face) which are identiﬁed by template matching and\\ngradient of intensity. Detection decision is based on MAP\\nestimation. ColmenarezandHuang[3] applyKullbackrel-ative informationfor maximaldiscriminationbetween pos-\\nitive and negative examples of faces. They use a family\\nof discrete Markov processes to model the face and back-ground patterns and estimate the density functions. De-\\ntection of a face is based on the likelihood ratio computed\\nduring training. Moghaddam and Pentland [12] propose a\\nprobabilistic method that is based on density estimation in\\na high dimensional space using an eigenspace decomposi-tion. In [18], Rowley et al. use an ensemble of neural net-\\nworksto learnface and nonfacepatternsforface detection.\\nSchneiderman et al. describe a probabilistic method basedonlocalappearanceandprincipalcomponentanalysis[21].\\nTheirmethodgivessomepreliminaryresultsonproﬁleface\\ndetection. Finally, hidden Markov models [17], higher or-\\nderstatistics[17],andsupportvectormachines(SVM)[13]\\n[14] have also been applied to face detection and demon-stratedsomesuccessindetectinguprightfrontalfacesunder\\ncertainlightingconditions.\\n3 Mixture ofFactorAnalyzers\\nIntheﬁrstmethod,weﬁtthemixturemodeloffactoran-\\nalyzers to the training samples using an EM algorithm and\\nobtain a distribution of face patterns. To detect faces, each\\ninputimageisscannedwitharectangularwindowinwhichthe probability of the current input being a face pattern is\\ncalculated. A face is detected if the probability is above\\na predeﬁned threshold. We brieﬂy describe factor analysisandamixtureoffactoranalyzersinthissection. Thedetails\\nofthesemodelscanbefoundin [1][7]./BF/BA/BD /BY /CP/CR/D8/D3/D6 /BT/D2/CP/D0/DD/D7/CX/D7\\nFactor analysis is a statistical model in which the ob-\\nserved vector is partitioned into an unobserved systematic\\npart and an unobserved error part. The systematic part istaken as a linear combination of a relatively small number\\nofunobservedfactorvariableswhile thecomponentsofthe\\nerror vector are considered as uncorrelatedor independent.\\nFromanotherpointofview,factoranalysisgivesa descrip-\\ntion of the interdependenceof a set of variablesin termsofthefactorswithoutregardtotheobservedvariability. Inthis\\nmodel, a/CS-dimensional real-valued observable data vector/DCis modeled using a /D4-dimensional vector of real-valued\\nfactors /DEwhere /D4is generally much smaller than /CS.T h e\\ngenerativemodelisgivenby:/DC /BP/A3 /DE /B7 /D9 (1)\\nwhere /A3isknownasthe factorloadingmatrix . Thefactors/DEa r ea s s u m e dt ob e /C6 /B4/BC /BN/C1 /B5distributed (zero-mean inde-\\npendent normals with unit variance). The /CS-dimensional\\nrandom variable /D9is distributed /C6 /B4/BC /BN /A9/B5where /A9is a di-\\nagonalmatrix,duetotheassumptionthattheobservedvari-ables are independent given the factors. According to this\\nmodel,/DCisthereforedistributedwithzeromeanandcovari-\\nance /A6/BP/A3 /A3\\n/CC/B7/A9. Thegoaloffactoranalysisistoﬁndthe/A3and /A9thatbest modelthe covariancestructureof /DC.T h e\\nfactor variables /DEmodel correlationsbetween the elements\\nof /DC, while the /D9variables account for independent noise\\nin each element /DC.T h e /D4factors play the same role as the\\nprincipalcomponentsinPCA,i.e.,theyareinformativepro-jections of the data. Given/A3and /A9, the expected value of\\nthefactorscanbecomputedthroughthelinearprojections:/BX /CJ /DE /CY /DC /CL/BP /AC/DC (2)/BX /CJ /DE/DE\\n/CC/CY /DC /CL/BP /C1 /A0 /AC /A3/B7 /AC/DC /DC\\n/CC/AC\\n/CC(3)\\nwhere /AC /BP/A3\\n/CC/A6\\n/A0 /BD./BF/BA/BE /C5/CX/DC/D8/D9/D6/CT /C5/D3 /CS/CT/D0\\nIn this section, we consider a mixture of /D1factor an-\\nalyzers (indexed by /CU/CY\\n/BN/CY /BP /BD /BN/BM/BM/BM/BN/D1) where each factor\\nanalyzer has the same number of /D4factors and each fac-\\ntoranalyzerhasa differentmean /AM/CY. The generativemodel\\nobeysthemixturedistribution:/C8 /B4 /DC /B5/BP\\n/D1/CG/CY /BP/BD\\n/CI/C8 /B4 /DC /CY /DE/BN /CU/CY\\n/B5 /C8 /B4 /DE /CY /CU/CY\\n/B5 /C8 /B4 /CU/CY\\n/B5 /CS/DE(4)\\nwhere/C8 /B4 /DE /CY /CU/CY\\n/B5/BP /C8 /B4 /DE /B5/BP /C6 /B4/BC /BN/C1 /B5 (5)/C8 /B4 /DC /CY /DE/BN /CU/CY\\n/B5/BP /C6 /B4 /AM/CY\\n/B7/A3/CY\\n/DE/BN /A9/B5 (6)\\nThe parametersof this mixture modelare /CU /B4 /AM/CY, /A3/CY\\n/B5\\n/D1/CY /BP/BD, /AP,/A9 /CVwhere /APis the vector of adaptable mixing proportions,/AP/CY\\n/BP /C8 /B4 /CU/CY\\n/B5. The latent variablesin this modelare the fac-\\ntors /DEand the mixture indicator variable /CU/CY,w h e r e /CU/CY\\n/BP/BD\\nwhenthedatapointisgeneratedbytheﬁrstfactoranalyzer.\\nGiven a set of training images, the EM algorithm [4] is\\nusedtoestimate /CU /B4 /AM/CY\\n/BN /A3/CY\\n/B5\\n/D1/CY /BP/BD, /AP, /A9 /CV. FortheE-stepofthe\\nEM algorithm, we need to compute expectations of all the\\ninteractions of the hidden variables that appear in the log\\nlikelihood,/BX /CJ /CU/CY\\n/DE /CY /DC/CX\\n/CL/BP /BX /CJ /CU/CY\\n/CY /DC/CX\\n/CL /BX /CJ /DE /CY /CU/CY\\n/BN/DC/CX\\n/CL (7)\\n/BX /CJ /CU/CY\\n/DE/DE\\n/CC/CY /DC/CX\\n/CL/BP /BX /CJ /CU/CY\\n/CY /DC/CX\\n/CL /BX /CJ /DE/DE\\n/CC/CY /CU/CY\\n/BN/DC/CX\\n/CL(8)\\nDeﬁning/CW/CX/CY\\n/BP /BX /CJ /CU/CY\\n/CY /DC/CX\\n/CL /BB /C8 /B4 /DC/CX\\n/BN/CU/CY\\n/B5/BP /AP/CY\\n/C6 /B4 /DC/CX\\n/A0 /AM/CY\\n/BN /A3/CY\\n/A3\\n/CC/CY\\n/B7/A9 /B5\\n(9)\\nandusingequations(2)and(6),weobtain/BX /CJ /CU/CY\\n/DE /CY /DC/CX\\n/CL/BP /CW/CX/CY\\n/AC/CY\\n/B4 /DC/CX\\n/A0 /AM/CY\\n/B5 (10)\\nwhere /AC/CY\\n/AH /A3\\n/CC/CY\\n/B4/A3/CY\\n/A3\\n/CC/CY\\n/B5\\n/A0 /BD. Similarly, using equations (3)\\nand(8),weobtain/BX /CJ /CU/CY\\n/DE/DE\\n/CC/CY /DC/CX\\n/CL/BP /CW/CX/CY\\n/B4 /C1 /A0 /AC/CY\\n/A3/CY\\n/B7 /AC/CY\\n/B4 /DC/CX\\n/A0 /AM/CY\\n/B5/B4 /DC/CX\\n/A0 /AM/CY\\n/B5\\n/CC/AC\\n/CC/CY\\n/B5\\n(11)\\nThe EM algorithm for mixture of factor analyzers can be\\nstatedasfollows:/AFE-step: Compute /BX /CJ /CU/CY\\n/CY /DC/CX\\n/CL, /BX /CJ /DE /CY /CU/CY\\n/BN/DC/CX\\n/CLand /BX /CJ /DE/DE\\n/CC/CY/CU/CY\\n/BN/DC/CX\\n/CLfor all data points /CXand mixture components/CY./AFM-step: Solveasetoflinearequationsfor /AP/CY, /A3/CY, /AM/CY\\nand /A9.\\nThe mixture of factor analyzersis essentially a reduced di-\\nmensionality mixture of Gaussians. Each factor analyzer\\nﬁts a Gaussian to a portion of the data, weighted by the\\nposteriorprobabilities, /CW/CX/CY. Since thecovariancematrixfor\\neach Gaussian is speciﬁed through the lower dimensionalfactorloadingmatrices,themodelhas/D1/D4/CS /B7 /CS,ratherthan/D1/CS /B4 /CS /B7/BD /B5 /BP /BEparametersdedicatedtomodelingcovariance\\nstructureinhighdimensions./BF/BA/BF /BW/CT/D8/CT/CR/D8/CX/D2/CV /BY /CP/CR/CT /C8 /CP/D8/D8/CT/D6/D2/D7\\nTo detectfaces, eachinputimageis scannedwith a rect-\\nangular window in which the probability of there being a\\nface pattern is estimated as given in equation(4). A face is\\ndetected if the probability is above a predeﬁned threshold.\\nIn order to detect faces of different scales, each input im-\\nageisrepeatedlysubsampledbyafactorof1.2andscannedthroughfor10iterations.\\n4 MixtureofLinearSpacesUsingFisherLin-\\nearDiscriminant\\nIn the second mixture model, we ﬁrst use Kohonen’s\\nself-organizing map [11] to divide the face and nonface\\nsamplesinto /CR/BDfaceclassesand /CR/BEnonfaceclasses,thereby\\ngenerating labels for the samples. Next, Fisher projection\\nis computed based on all /CR/BD\\n/B7 /CR/BEclasses to maximize the\\nratioofthe between-classscatter (variance)andthe within-classscatter(variance). Thenowlabeledtrainingsetispro-\\njected from a high dimensional image space to a lower di-\\nmensionalfeaturespace,andaGaussiandistributionisused\\nto model the class-conditional density function for each\\nclass where the parameters are estimated using the maxi-\\nmum likelihood principle. For detection, the conditional\\nprobability of each sample given each class is computedand the maximum likelihood principle is used to decide to\\nwhich class the sample belongs. In our experiments, the\\nreasonthatwechoose25faceand25nonfaceclassesisbe-\\ncause of the size of training set. If the number of classes\\nis too small, the clustering results may be poor. On theother hand, we may not have enough samples to estimate\\nthe class-conditional density function well if we choose a\\nlargenumberofclasses./BG/BA/BD /C4/CP/CQ /CT/D0/CX/D2/CV /CB/CP/D1/D4/D0/CT/D7 /CD/D7/CX/D2/CV /CB/C7/C5\\nIn applyingFisher Linear Discriminant to ﬁnd a projec-\\ntion, we need to know the class label of each training sam-\\nple. However,suchinformationisnotavailableinthetrain-\\ningsamples. Therefore,we useKohonen’sSelf-OrganizingMap [11] to divide face samples into a ﬁnite number of\\nclasses. In our experiments, we divide the face sample im-\\nages into 25 classes. After training, the ﬁnal weight vectorforeachnodeis thecentroidofthe class, i.e.,the prototype\\nvector, which corresponds to the prototype of each class.\\nThesameprocedureisappliedtononfacesamples. Figure1\\nshowstheprototypicalfaceofeachclass. Itisclearthatthe\\nsamplefaceimageswithdifferentposesandunderdifferentlightingconditions(intensityincreasesfromthelowerright\\ncornertotheupperleftcorner)havebeenclassiﬁedintodif-\\nferentclasses. NotethattheSOM algorithmalso placestheprototypes in the two dimensional feature map, shown in\\n1, in accordance with their topological relationships in the\\nimagespace. Inotherwords,prototypevectorscorrespond-\\ning to nearby points on the feature map grid have nearby\\nlocationsin thehighdimensionalimagespace(e.g.,nearbyprototypeshavesimilar intensityandpose)./BG/BA/BE /BY/CX/D7/CW/CT/D6 /C4/CX/D2/CT/CP/D6 /BW/CX/D7/CR/D6/CX/D1/CX/D2/CP/D2 /D8\\nWhile PCA is commonly used to project face patterns\\nfrom a high dimensional image space to a lower dimen-\\nsional feature space, a drawback of this approach is thatit deﬁnes a subspace such that it has the greatest variance\\nof the projected sample vectors among all the subspaces.\\nHowever, such projection is not suitable for classiﬁcationsinceitmaycontainprincipalcomponentswhichretainun-\\nwanted large variations. Therefore, the classes in the pro-\\njectedspacemaynotbewellclusteredandinsteadsmeared\\ntogether [2] [6] [10]. Fisher Linear Discriminant is an ex-\\nampleof a class speciﬁc methodthat ﬁndsthe optimalpro-jection for classiﬁcation. Rather than ﬁnding a projection\\nthat maximizes the projected variance, FLD determines a\\nprojection,/DE /BP /CF\\n/CC/BY/C4 /BW\\n/DC, that maximizes the ratio be-\\n/BY/CX/CV/D9/D6/CT /BD/BA /C8/D6/D3/D8/D3/D8 /DD/D4 /CT /D3/CU /CT/CP/CR/CW /CU/CP/CR/CT /CR/D0/CP/D7/D7/BA\\ntween the between-class scatter (variance) and the within-\\nclassscatter(variance). Consequently,classiﬁcationissim-pliﬁedin the projectedspace. Recently,it hasbeendemon-\\nstrated that the Fisherface method outperforms the Eigen-\\nfacemethodinfacerecognition[2].\\nConsidera/CR-classproblem,let thebetween-classscatter\\nmatrixbedeﬁnedas/CB/BU\\n/BP\\n/CR/CG/CX /BP/BD\\n/C6/CX\\n/B4 /AM/CX\\n/A0 /AM /B5/B4 /AM/CX\\n/A0 /AM /B5\\n/CC(12)\\nandthe within-classscattermatrixbedeﬁnedas/CB/CF\\n/BP\\n/CR/CG/CX /BP/BD\\n/CG/DC/CZ\\n/BE /CG/CX\\n/B4 /DC/CZ\\n/A0 /AM/CX\\n/B5/B4 /DC/CZ\\n/A0 /AM/CX\\n/B5\\n/CC(13)\\nwhere /AMis the mean of all samples, /AM/CXis the mean of class/CG/CX,a n d /C6/CXis the number of samples in class /CG/CX.T h e\\noptimal projection /CF/BY/C4 /BWis chosen as the matrix with or-\\nthonormalcolumnswhichmaximizestheratioofthe deter-\\nminant of the between-class scatter matrix of the projected\\nsamplestothedeterminantofthewithin-classscattermatrixoftheprojectedsampled,i.e.,/CF/BY/C4 /BW\\n/BP /CP/D6/CV /D1/CP/DC/DB\\n/CY /CF\\n/CC/CB/BU\\n/CF /CY/CY /CF\\n/CC/CB/CF\\n/CF /CY\\n/BP/CJ /DB/BD\\n/DB/BE\\n/BM/BM/BM /DB/D1\\n/CL(14)\\nwhere /CU /DB/CX\\n/CY /CX /BP/BD /BN /BE /BN/BM/BM/BM /BN/D1 /CVis thesetofgeneralizedeigen-\\nvectorsof /CB/BUand /CB/CF,correspondingto the /D1largestgen-\\neralized eigenvalues /CU /AL/CX\\n/CY /CX /BP /BD /BN /BE /BN/BM/BM/BM /BN/D1 /CV.H o w e v e r , t h e\\nrank of /CB/BUis /CR /A0 /BDor less because it is the sum of /CRma-\\ntrices of rank one or less. Thus, the upper bound on /D1is\\n/CR /A0 /BD[5]. See [2] for details about a method to overcome\\nsingularityproblemsin computing /CF/BY/C4 /BW./BG/BA/BF /BV/D0/CP/D7/D7/B9/BV/D3/D2/CS/CX/D8/CX/D3/D2/CP/D0 /BW/CT/D2/D7/CX/D8 /DD /BY /D9/D2/CR/D8/CX/D3/D2\\nOnce /CF/BY/C4 /BWis computed, the now labeled training set\\nis projected to the /CR /A0 /BDdimensional feature space, i.e.,/DE /BP /CF\\n/CC/BY/C4 /BW\\n/DC, and a Gaussian distribution is used to\\nmodel each class-conditional density (CCD) function, i.e.,/C8 /B4 /DE /CY /CG/CX\\n/B5/BP /C6 /B4 /AM/CG/CX\\n/BN /A6/CG/CX\\n/B5where /CX /BP/BD /BN/BM/BM/BM /BN/CR. The param-\\neters, /AI/CG/CX\\n/BP/B4 /AM/CG/CX\\n/BN /A6/CG/CX\\n/B5 /CVof each CCD are the maximum\\nlikelihoodestimates, i.e.,/CM /AM/CG/CX\\n/BP\\n/BD/CY /CG/CX\\n/CY\\n/CG/DE/CZ\\n/BE /CG/CX\\n/DE/CZ (15)\\nand/CM/A6/CG/CX\\n/BP\\n/BD/CY /CG/CX\\n/CY\\n/CG/DE/CZ\\n/BE /CG/CX\\n/B4 /DE/CZ\\n/A0 /CM /AM/CG/CX\\n/B5/B4 /DE/CZ\\n/A0 /CM /AM/CG/CX\\n/B5\\n/CC(16)/BG/BA/BG /BW/CT/D8/CT/CR/D8/CX/D2/CV /BY /CP/CR/CT /C8 /CP/D8/D8/CT/D6/D2/D7\\nEach input image is scanned with a rectangularwindow\\nto determine whether a face exists in the window or not.\\nThe decision rule for deciding whether an input windowcontainsa faceornotisbasedonmaximumlikelihood,/CG\\n/A3/BP /CP/D6/CV /D1/CP/DC/CG/CX\\n/C8 /B4 /DE /CY /CG/CX\\n/B5 (17)\\nTo detect faces of different scales, each input image is re-\\npeatedlysubsampledbyafactorof1.2andscannedthrough\\nfor10iterations.\\n5 Experiments\\nFortraining,weuseasetof1,681faceimages(collected\\nfrom Olivetti [20], UMIST [8], Harvard [9], Yale [2] andFERET[15]databases)whichhavewidevariationsinpose,\\nfacialexpressionandlightingcondition. Inthesecondmix-\\nture method, we start with 8,422 nonface examples from\\n400imagesoflandscapes,trees, buildings,etc. Althoughit\\nis extremely difﬁcult to collect a representative set of non-face examples,the bootstrapmethodsimilar to [22] is used\\nto include more nonface examples during training. Each\\nface sample is manually cropped and normalized such thatit is aligned vertically and its size is/BE/BC /A2 /BE/BCpixels. To\\nmake the detection method less sensitive to scale and ro-\\ntation variation, 10 face examples are generated from each\\noriginalsample. The imagesare producedbyrandomlyro-\\ntatingthe imagesbyup to /BD/BHdegreeswith scaling between/BK/BC/B1and /BD/BE/BC/B1. Thisproduces16,810facesamples.\\nWe test both methods on the three sets of images col-\\nlected by Rowley [18], Sung [22] and ourselves. In ourexperiments, a detected face is a successful detect is if the\\nsubimage contains eyes and mouth. Otherwise, it is a false\\ndetect. The detection rate is the ratio between the number\\nofsuccessfuldetectsandthenumberoffacesinthetestset.Table 1 shows the detection rates of our methods and the\\nreportedresults of severaldetectionmethodson the test set\\nin[18]. Experimentalresultsontestset1,whichconsistsof\\n125 images (483 faces) excluding 5 images of hand drawn\\nfaces, show that our methods have comparable detectionperformance with other methods, yet with fewer false de-\\ntects. Table 1 also shows the our experimental results on\\nthe test set of Sung and Poggio [22] which consists of 20imagesexcluding3imagesofline drawnfaces(136faces).\\nBothofourmethodsconsistentlyperformwellandhavefew\\nfalse detects.\\nTest set 3 consists of 80 images (252 faces), collected\\nfrom the World Wide Web, with different poses, expres-\\nsions and faces with heavy shadows. The detection ratesare/BK/BI /BM /BJ/B1and /BK/BK /BM /BE/B1for MFA and FLD-based methods.\\nThe number of false detects are /BG/BHand /BG/BC, respectively.\\nBothmethodsperformequallywell indetectingthesefacesthoughtheFLD-basedmethodperformsslightlybetterthan\\nthe ﬁrst one. Figures 2 and 3 show the results of our meth-\\nods on some test images. See the web page mentionedabovefor moreresults. Notice that there is a false detect in\\ntheupperleftcorneroftheimageinFigure2sinceonewin-\\ndowresemblesaface. Alsonoticethatourmethodscande-\\ntect,uptocertaindegree,proﬁlefacesandfaceswithheavy\\nshadows. However occluded, rotated faces or faces withsunglasses cannot be detected effectively by both methods\\ndue to lack of such examples in the training sets. None\\nof the existing detection methods cannot effectively detectthese types of faces except one recent method [19] seems\\nto able to detect rotated faces. Nevertheless, this method\\ncannotdetectoccludedfacesorfacewithheavyshadows.\\n6 DiscussionandConclusion\\nWe havedescribedmethodsusingmixtureof linearsub-\\nspaces methods to detect human faces regardless of their\\nposes, facial expressions and lighting conditions. Bothmethods ﬁnd better projection than PCA for pattern clas-\\nsiﬁcation,therebyfacilitatingdetectionoffaceandnonface\\npatterns. The ﬁrst methodﬁts a mixtureof factor analyzerstoestimatethedensityfunctionoffaceimages,andthesec-\\nondmethodusesSelf-OrganizingMaptopartitionthetrain-\\ning set into classes and Fisher Linear Discriminant to ﬁnd\\nthe optimal projection for classiﬁcation. Experimental re-\\nsultsonthreesetsofimagesdemonstratethatbothmethodsperform as well as the best algorithms in detecting upright\\nfrontalfaces,yetwith fewerfalse detects.\\nThe contributions of this paper can be summarized as\\nfollows. First, we introduce projection methods that per-\\n/CC /CP/CQ/D0/CT /BD/BA /BX/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1 /D8/CT/D7/D8 /D7/CT/D8 /BD /B4/BD/BE/BH /CX/D1/CP/CV/CT/D7 /DB/CX/D8/CW /BG/BK/BF /CU/CP/CR/CT/D7/B5 /CX/D2 /CJ/BD/BK/CL /CP/D2/CS /D8/CT/D7/D8/D7/CT/D8 /BE /B4/BE/BC /CX/D1/CP/CV/CT/D7 /DB/CX/D8/CW /BD/BF/BI /CU/CP/CR/CT/D7/B5 /CX/D2 /CJ/BE/BE/CL /B4/D7/CT/CT /D8/CT/DC/D8 /CU/D3 /D6 /CS/CT/D8/CP/CX/D0/D7/B5/BA\\nTest Set 1 Test Set 2\\nMethod DetectRate False Detects Detect Rate FalseDetects\\nMixtureoffactoranalyzers 92.3% 82 89.4% 3\\nFisherlineardiscriminant 93.6% 74 91.5% 1\\nDistribution-based[22] N/A N/A 81.9% 13\\nNeuralnetwork[18] 92.5% 862 90.3% 42\\nNaiveBayes[21] 93.0% 88 91.2% 12\\nKullbackrelativeinformation[3] 98.0% 12758 N/A N/A\\nSupportvectormachine[13] N/A N/A 74.2% 20\\n/BY/CX/CV/D9/D6/CT /BE/BA /CB/CP/D1/D4/D0/CT /CT/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D9/D7/CX/D2/CV/D1/CX/DC/D8/D9/D6/CT /D3/CU /CU/CP/CR/D8/D3 /D6 /CP/D2/CP/D0/DD/DE/CT/D6/D7 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1/D8/CW/D6/CT/CT /D8/CT/D7/D8 /D7/CT/D8/D7/BA /BX/DA/CT/D6/DD /CS/CT/D8/CT/CR/D8/CT/CS /CU/CP/CR/CT /CX/D7 /D7/CW/D3 /DB/D2/DB/CX/D8/CW /CP/D2 /CT/D2/CR/D0/D3/D7/CX/D2/CV /DB/CX/D2/CS/D3 /DB/BA\\n/BY/CX/CV/D9/D6/CT /BF/BA /CB/CP/D1/D4/D0/CT /CT/DC/D4 /CT/D6/CX/D1/CT/D2/D8/CP/D0 /D6/CT/D7/D9/D0/D8/D7 /D9/D7/CX/D2/CV/D1/CX/DC/D8/D9/D6/CT /D3/CU /D7/D9/CQ/D7/D4/CP/CR/CT/D7 /DB/CX/D8/CW /BY/CX/D7/CW/CT/D6 /C4/CX/D2/CT/CP /D6 /BW/CX/D7/B9/CR/D6/CX/D1/CX/D2/CP/D2/D8 /D3/D2 /CX/D1/CP/CV/CT/D7 /CU/D6/D3/D1 /D8/CW/D6/CT/CT /D8/CT/D7/D8 /D7/CT/D8/D7/BA /BX/DA/B9/CT/D6/DD /CS/CT/D8/CT/CR/D8/CT/CS /CU/CP/CR/CT /CX/D7 /D7/CW/D3 /DB/D2 /DB/CX/D8/CW /CP/D2 /CT/D2/CR/D0/D3/D7/CX/D2/CV/DB/CX/D2/CS/D3 /DB/BA\\nform better than PCA. Consequently, the classiﬁcation re-\\nsult in the linear subspace is better. Second, we apply mix-\\nture models such that the linear subspaces can better cap-\\nturethevariationsoffacepatterns. Althoughsomemethods[12][22]haveappliedmixturemodel,theyusePCAforpro-\\njection which is suboptimal for classiﬁcation in subspaces.\\nOn the other hand, it is not clear how SVM performs in\\nface detection since the study in [13] has applied SVM on\\na rather small test set with 136 faces. It will be of great in-teresttocompareourmethodswithSVM onalargetestset\\nsince SVM aims to ﬁnd the optimal hyperplane that min-\\nimizes the generalization error under the theoretical upperbounds.\\nAcknowledgments\\nD. Kriegman was supported in part by the Army Re-\\nseearchOfﬁceunderAROY-99-0006andtheNationalEye\\nInstitute.\\nReferences\\n[1] T. W. Anderson. An Introduction to Multivariate Statistical\\nAnalysis. John Wiley, New York, 1984.\\n[2] P.Belhumeur,J.Hespanha,andD.Kriegman.Eigenfacesvs.\\nﬁsherfaces: Recognition using class speciﬁc linear projec-\\ntion.IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , 19(7):711–720, 1997.\\n[3] A. J. Colmenarez and T. S. Huang. Face detection with\\ninformation-based maximum discrimination. In Proceed-\\ningsoftheIEEEComputerSocietyConferenceonComputer\\nVision andPatternRecognition , pages 782–787, 1997.\\n[4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum\\nlikelihood fromincompletedataviatheemalgorithm. Joru-\\nanl ofthe Royal Statistical Society , 39(1):1–38, 1977.\\n[5 ] R.O.Du d aan dP .E.Hart. Pattern Classiﬁcation and Scene\\nAnalysis. John Wiely, New York, 1973.\\n[6] B. J. Frey, A. Colmenarez, and T. S. Huang. Mixtures of\\nlocal subspaces for face recognition. In Proceedings of the\\nIEEEComputerSocietyConferenceonComputerVisionand\\nPatternRecognition , pages 32–37, 1998.\\n[7] Z. Ghahramani and G. E. Hinton. The em algorithm for\\nmixturesoffactoranalyzers. TechnicalReportCRG-TR-96-\\n1, Department of Computer Science, University of Toronto,\\n1996. Available at ftp://ftp.cs.toronto.edu/pub/zoubin/tr-96-1.ps.gz.\\n[8] D. B. Graham and N. M. Allinson. Characterizing virtual\\neigensignatures for general purpose face recognition. InH. Wechsler, P. J. Phillips, V. Bruce, F. Fogelman-Soulie,\\nand T.S.Huang, editors, Face Recognition: FromTheoryto\\nApplications , volume 163 of NATO ASI Series F, Computer\\nand Systems Sciences , pages 446–456. Springer,1998.\\n[9] P.Hallinan. ADeformable Model for Face Recognition Un-\\nderArbitraryLightingConditions . PhDthesis,HarvardUni-\\nversity, 1995.[10] G. E. Hinton, P. Dayan, and M. Revow. Modeling the man-\\nifolds of images of handwritten digits. IEEE Trans. Neural\\nNetworks , 8(1):65–74, 1997.\\n[11] T. Kohonen. Self Organizing Map . Springer, 1996.\\n[12] B. Moghaddam and A. Pentland. Probabilistic visual learn-\\ning for object recognition. IEEE Transactions on Pattern\\nAnalysis andMachine Intelligence , 19(7):696–710, 1997.\\n[13] E.Osuna, R. Freund, and F. Girosi. Training support vector\\nmachines: an application to face detection. In Proceedings\\nof the IEEE Computer Society Conference on Computer Vi-\\nsionand PatternRecognition , pages 130–136, 1997.\\n[14] C.Papageorgiou, M. Oren,and T.Poggio. Ageneralframe-\\nwork for object detection. In Proceedings of the FifthInter-\\nnational Conference on Computer Vision , pages 555–562,\\n1998.\\n[15] P. J. Phillips, H. Moon, S. Rizvi, and P. Rauss. The\\nferet evaluation. In H. Wechsler, P. J. Phillips, V. Bruce,\\nF. Fogelman-Soulie, and T. S. Huang, editors, Face Recog-\\nnition: From Theory to Applications , volume 163 of NATO\\nASI Series F, Computer and Systems Sciences , pages 244–\\n261. Springer, 1998.\\n[16] R. J. Qian and T. S. Huang. Object detection using hierar-\\nchical mrf and map estimation. In Proceedings of the IEEE\\nComputer Society Conference on Computer Vision and Pat-\\nternRecognition , pages 186–192, 1997.\\n[17] A. N. Rajagopalan, K. S. Kumar, J. Karlekar, R. Mani-\\nvasakan, and M. M. Patil. Finding faces in photographs. In\\nProceedings of the Sixth International Conference on Com-\\nputer Vision , pages 640–645, 1998.\\n[18] H.Rowley,S.Baluja,andT.Kanade. Neuralnetwork-based\\nface detection. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 20(1):23–38, 1998.\\n[19] H. Rowley, S. Baluja, and T. Kanade. Rotation invariant\\nneural network-based face detection. In Proceedings of the\\nIEEEComputerSocietyConferenceonComputerVisionand\\nPatternRecognition , pages 38–44, 1998.\\n[20] F. S. Samaria. Face Recognition Using Hidden Markov\\nModels. PhD thesis,University of Cambridge, 1994.\\n[21] H. Schneiderman and T. Kanade. Probabilistic modeling of\\nlocal appearance and spatial relationships for object recog-nition. In Proceedings of the IEEE Computer Society Con-\\nference on Computer Vision and PatternRecognition , pages\\n45–51, 1998.\\n[22] K.-K. Sung and T. Poggio. Example-based learning for\\nview-based human face detection. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 20(1):39–51,\\n1998.\\n[23] M.-H. Yang, N. Ahuja, and D. Kriegman. A survey on face\\ndetection methods. 2000. To be submitted.\\n',\n",
       " 'Face Expression Recognition with a 2-Channel \\nConvolutional Neural Network \\nDennis Hamester, Pablo Barros, Stefan Wermter \\nUniversity of Hamburg -Department of Informatics \\nVogt-Kolln-StraBe 30, 22527 Hamburg, Germany \\nhttp://www.informatik.uni-hamburg.de/wTMI \\n{hamester, barros, wermter} @informatik.uni-hamburg.de \\nAbstract-A new architecture based on the Multi-channel \\nConvolutional Neural Network (MCCNN) is proposed for rec\\xad\\nognizing facial expressions. Two hard-coded feature extractors \\nare replaced by a single channel which is partially trained in \\nan unsupervised fashion as a Convolutional Autoencoder (CAE). \\nOne additional channel that contains a standard CNN is left \\nunchanged. Information from both channels converges in a fully \\nconnected layer and is then used for classification. We perform \\ntwo distinct experiments on the JAFFE dataset (Ieave-one-out \\nand ten-fold cross validation) to evaluate our architecture. Our \\ncomparison with the previous model that uses hard-coded Sobel \\nfeatures shows that an additional channel of information with \\nunsupervised learning can significantly boost accuracy and re\\xad\\nduce the overall training time. Furthermore, experimental results \\nare compared with benchmarks from the literature showing \\nthat our method provides state-of-the-art recognition rates for \\nfacial expressions. Our method outperforms previously published \\nmethods that used hand-crafted features by a large margin. \\nI. INTRODUCTION \\nRecognizing human emotions has been the focus of at\\xad\\ntention in several areas from psychology and sociology to \\ncognitive and computer science. Emotions have been studied \\nfor many years, and are still one of the most challenging topics \\nin human psychological interactions. Through emotional state \\ndetermination, it is possible to enhance, highlight and under\\xad\\nstand better human interactions, actions and even feelings. \\nThe Artificial Intelligence community has studied and pro\\xad\\nposed approaches for automatic emotion recognition for the \\npast two decades [1]-[3]. Emotional states can be conveyed \\nby facial expressions, body posture, motion, language structure \\nor even from the environment [4]. Based on facial expression, \\nin one of the most comprehensive studies on understanding \\nhuman emotions, Ekman and Friesen [5] established six uni\\xad\\nversal emotions: disgust, fear, happiness, surprise, sadness \\nand anger. \\nOne of the most relevant characteristics of the presented \\nsystems is the use of a set of specific features, which constrains \\nthe solution to some rules. When used for facial expression \\nrecognition, the solutions based on hard-coded features show \\nvery good results, but lack the capability to be applied in a real\\xad\\nworld scenario because of the many restrictions based on en\\xad\\nvironment illumination, position of the subject, and skin color \\namong others [6]. To overcome this problem in the computer \\nvision area, the use of implicit features for image classification \\nled to some success in the past years [7]-[9]. Among implicit \\n978-1-4799-1959-8/15/$31.00 @2015 IEEE feature models, deep neural networks [10] were proposed be\\xad\\ncause they use the data themselves to learn the most significant \\naspects of the image. One of the advantages of deep neural \\nnetworks is their power of generalization, and once they are \\ntrained, the low computational cost to extract features and \\nclassify them. Among deep neural architectures, deep belief \\nnetworks [11] and Convolutional Neural Networks (CNN) [12] \\nare the ones which present the most promising results for a \\nwide range of applications [13]-[15]. \\nIn the past years, some approaches using deep neural \\nnetworks for facial expression recognition were proposed and \\nevaluated, for example by Kahou et al. [16]. In their work they \\nevaluate two experiments using CNNs. In the first experiment, \\nthey implement a standard CNN and train it with the Acted \\nFacial Expression in the Wild (AFEW) dataset [17]. In the sec\\xad\\nond experiment, they pre-train the model with the Toronto Face \\nDataset. After that, a softmax layer is trained with the AFEW \\ndataset. The networks presented in that approach implement \\nthe usual CNN model, adapting the number of layers and filters \\nfor each experiment executed. Both experiments were used for \\nthe recognition of six facial expressions, based on the universal \\nfacial expressions and one neutral expression. The idea of pre\\xad\\ntraining was shown to be successful, but a problem remains: \\nthe network is trained to learn specific features from the data \\nit is shown. If a fair amount of data is shown to the network, \\nit can have a significant capacity for generalization. If not, \\nthe network will not be able to learn proper features and will \\nnot be able to recognize images that were not shown during \\ntraining. \\nOur proposed model uses a Multi-channel Convolutional \\nNeural Network (MCCNN) architecture, proposed in previous \\nwork [18]. This network is able to learn with less data than the \\nusual deep learning models, and needs less effort for training. \\nOne of the most sensitive restrictions of the MCCNN is the use \\nof a pair of hard-coded Sobel-based layers which show good \\nresults when applied in a 3-channel topology. The proposed \\nmodel replaces both Sobel channels with a single channel \\nthat is trained separately and unsupervised as a Convolutional \\nAutoencoder (CAE). \\nUnsupervised feature extraction has become an attractive \\nalternative to hand-crafted features and can yield highly com\\xad\\npetitive results [19], [20]. The second channel in our proposed \\nmodel contains fixed filters that are learned by a CAE in a \\nseparate step. We train this layer on the Kyoto natural images \\ndataset [21] so that the filters are not specific to our facial \\nexpression recognition task. These filter weights are kept fixed \\nin all further task-specific training processes. \\nTo evaluate the proposed model on a face recognition task, \\na set of experiments is described using the JAFFE dataset [22]. \\nFirst, to find the most suitable model parameters, several \\nparameter exploration experiments are designed, executed and \\nevaluated. Using the selected parameters, two experiments \\nwith the JAFFE dataset are established. The results are col\\xad\\nlected, evaluated and discussed in this paper. Comparisons \\nwith benchmark results are also shown. \\nThe paper is structured as follows: section II introduces \\nour 2-channel architecture and describes preprocessing as well \\nas how both channels are trained. The methodology for our \\nexperiments is given in section III. Preliminary experiments \\nfor parameter optimization and the results are shown as well. \\nSection IV shows results from our main experiments and \\ncompares them to related work. Finally, a conclusion is given \\nin section V. \\nII. ARCHITECTURE \\nThe MCCNN architecture uses multiple independent Con\\xad\\nvolutional Neural Networks in parallel, but connects them in \\nthe last layer to extract and recognize patterns from images. In \\nprevious approaches, the use of three channels was explored, \\nand presented good results for hand posture [18] and motion \\nrecognition [23]. The concept behind this architecture is to \\nmake use of existing knowledge, here represented by a dif\\xad\\nferent channel, to diversify the input. In our previous work, \\neach channel received different information, usually Sobel\\xad\\nbased filters were applied to the image. The channels receiving \\nthe image after the application of the Sobel operators acted \\nas tuning for the layer receiving the raw image. At the end, \\nall channels were connected and able to extract different and \\nspecific information of the image. \\nThe Sobel-based layers were a solution to explore the edges \\nin two directions, horizontal and vertical. To extend this princi\\xad\\nple, our new proposed model (see Figure 1) replaces the hard\\xad\\ncoded Sobel-based layers by a layer containing Gabor-like \\nfilters. This increases the kind of information extracted from \\nthe image, specializing one channel with a strong multiple\\xad\\norientation edge detector. Following the multi-channel archi\\xad\\ntecture, the proposed model uses two channels, both of them \\nreceiving the same image. What differs in the channels is \\nthe implementation and training of the filters in the first \\nlayer of each channel. The first channel acts like a common \\nCNN training all the parameters in a supervised fashion. The \\nsecond channel uses pre-trained parameters, obtained by a \\nCAE, which learn Gabor-like filters. After pre-training, the \\nweights of this layer are fixed and not trained in the supervised \\nphase. The two channels are connected with a fully-connected \\nhidden layer that produces the output for a logistic regression \\nclassifier. All parameters of network are optimized at the same \\ntime. The final error is obtained with the output of the two \\nchannels, so that both second layers act like a bias for each other. Thus, the final classification layer in our proposed model \\ncombines information from both channels, a standard CNN \\nand a second channel whose weights are trained as CAE. \\nFigure 1 shows the architecture of the model which was used \\nin the experiments. \\nWhen training a CAE on natural images, the resulting filters \\nusually look like Gabor filters, which respond strongly to local \\nedge elements. They are thus similar to hard-coded Sobel \\nfilters used in our previous work. However, the Gabor-like \\nfilters here are much more diverse in the sense that there \\nare filters for many more specific orientations. Subsequent \\nlayers in our architecture receive explicit information about \\nthe existence of specific edges making classification later on \\neasier and more robust. \\nThis approach provides an important advantage over pure \\nsupervised methods besides a potential boost in recognition \\nperformance. After training a CAE once on a generic dataset \\nsuch as the Kyoto database [21], the network can be transferred \\nto several different tasks. Reusing a pre-trained network for the \\nfirst layer in a potentially deep architecture is a viable method \\nto speed up overall training time. \\nA. Channel 1 -Convolutional Neural Network \\nA Convolutional Neural Network is a set of pairs of \\nconvolution and max-pooling layers that enable the model \\nto extract and enhance implicit features of an image. When \\nstacked together, the first layers act like an edge enhancement \\nand allow to extract local features, which are passed to deeper \\nlayers that act as complex feature extractors. \\nEach convolutional layer contains a set of feature maps, \\nor filters, that extract features from a region of units using a \\nconvolution operation. Then, an additive bias is applied and \\nthe result is passed through a non-linear activation function. \\nThe value of a unit v;� in the position (x,y) at the n-th feature \\nmap in the c-th layer is given by \\nvXY = max (b + \"\"\"\"\\' �\" �\" whw v(x+h)(y+w) 0) (1) ne en � � � \\'Jm (,-l)m \\' \\nm h=O w=O \\nwhere max(-, 0) represents the rectified linear function, which \\nwas shown to be more suitable for training deep neural \\narchitectures [24], ben is the bias for the n-th feature map of \\nthe c-th layer, and m indexes over the set of features maps in \\nthe (i-I) layer connected to the current layer c. In the equation, \\nWnek is the weight of the connection between the unit (h,w) \\nwithin a region, or kernel, connected to the previous layer. Hi \\nand Wi are the height and width of the kernel. \\nIn the max-pooling layers, a region of the previous layer is \\nconnected to a unit in the current layer, reducing the dimen\\xad\\nsionality of the feature maps. Each max-pooling layer retains \\nonly the maximum value within its receptive field and passes \\nit to the next layer. This enhances invariance to scale and \\ndistortions of the input [9]. The parameters of a CNN could \\nbe learned either by a supervised approach tuning the filters in \\na training database [11], or by an unsupervised approach [19]. \\nThe proposed model uses the supervised approach. \\nCNN Channel \\nGabor-like Max-pooling \\nkernel \\nLayer 1 Conv-Kernel Max-pooling \\nLayer n connected regression \\nhidden classifier \\nlayer \\nFigure 1. Proposed architecture for a Multi-channel Convolutional Neural Network using 2 channels. The upper channel is a standard CNN. consisting of \\nconvolutional layers and max-pooling layers. The lower channel has the same topology. but Its first layer weights are tramed as a Convolutional Autoencod er. \\nB. Channel 2 -Convolutional Autoencoder \\nThe second channel in our proposed architecture consists of \\ntwo convolutional layers with max pooling in between. The \\ninput to this channel is the same as for the first channel. How\\xad\\never, the first layer is trained as a Convolutional Autoencoder \\nin an extra step, prior to supervised training of the whole \\nnetwork. This step is similar to pre-training found in other \\ndeep-learning methods [11]. However, we keep the weights \\nof the first layer fixed during the supervised learning phase, \\nwhereas pre-training is usually followed by supervised fine\\xad\\ntuning. \\nThe hidden layer uses rectified linear units as well and \\nactivations in the i-th feature map are given by: \\nYi = max (x * Wi + bi, 0) (2) \\nReconstructions are computed as linear combinations of Y i and \\nuse tied weights (W denotes the vertically and horizontally \\nflipped version of W): \\n(3) \\nA CAE with k feature maps is over-complete by a factor \\nof roughly kl. In order to prevent the network from learning \\ntrivial solutions like identity functions, the capacity of the \\nhidden layer must be regularized during training. We impose \\na very strong sparsity prior by regularizing the network using \\na Winner-Take-All (WTA) method, as described by Makhzani \\nand Frey [25]. For each feature map, the maximum value is \\ndetermined after applying the rectified linear function. This \\nvalue is retained, but all other values in the feature map are \\nset to zero. This can be thought of as multiplying each feature \\nmap with a different mask that leaves only one value intact \\nbefore computing the reconstructions. \\nThis approach is very efficient, especially in comparison \\nwith methods like contractive regularization [26]. Since the \\nWTA approach avoids computing additional gradients (or \\n1 The exact size of a feature maps depends on the border treatment of the \\nconvolution. Figure 2. Typical filters learned by our Convolutional Autoencoder with \\nWinner- Take-All regularization. \\nJacobian matrices) and relies only on determining the max\\xad\\nimum in each feature map. We also found this method to be \\nhighly effective at learning sparse codes while not introducing \\nmore hyper-parameters like trade-off weights in the case of \\ncontractive regularization or many other regularization tech\\xad\\nniques. WTA regularization is only applied during training \\nand dropped after the network is finalized and the weights \\nare frozen. Otherwise, the hidden layer would contain only \\na single non-zero value per feature map and be incapable \\nof properly transferring information about the whole image \\ntowards the next layer. \\nMakhzani and Frey [25] state that using the same weights \\nfor the hidden layer as well as for the reconstruction (tied \\nweights) hurts generalization. However, we use tied weights \\nas shown in eqs. (2) and (3) and observe no such problems \\nin any of our experiments. On the other hand, having two \\ndistinct weight matrices (or tensors in the case of convolutional \\nnetworks) doubles the number of model parameters and makes \\ntraining more difficult and more time consuming, which is why \\nwe decided to use tied weights in the first place. \\nThe CAE is trained on the Kyoto natural images dataset [21] \\nusing a fixed kernel of 11 x 11 pixels. Each image is pre\\xad\\nprocessed by convolving it with a Difference of Gaussian \\n(DoG) filter. DoG filters are effectively similar to ZCA whiten\\xad\\ning [27], without the need to learn the filter kernels first. \\nFurthermore, the shape of DoG filters is a good approximation \\nof ideal decorrelation filters for grayscale images [28]. Whiten\\xad\\ning enables our CAE to learn Gabor-like filters, otherwise \\nfilters would look more like principle components [29]. In \\nNeutral (NE) Happiness (HA) Sadness (SA) Surprise (SU) \\nAnger (AN) Disgust (01) Fear (FE) \\nFigure 3. Samples of all seven classes from the JAFFE dataset [22]. The \\ntwo-letter codes in parentheses are used throughout this paper to refer to the \\nclass. Each sample shows a different subject as well, out of the ten subjects \\nin the dataset. \\ncomparison to ZCA whitening, using DoG filters in the context \\nof convolutional networks also has the advantage that they can \\nbe applied directly to arbitrarily-sized images. The number \\nof features is not set a-priori but determined experimentally. \\nDetails are given in section III. Figure 2 shows 60 typical \\nGabor-like filters learned by our CAE. \\nIII. EXPERIMENTS \\nWe evaluate our architecture for facial expression recogni\\xad\\ntion on the well established JAFFE dataset [22]. The dataset \\nconsists of 213 images, showing ten Japanese women per\\xad\\nforming seven different facial expressions (neutral, happiness, \\nsadness, surprise, anger, disgust, and fear). For each com\\xad\\nbination of subject and facial expression, there are between \\ntwo and four examples present in the dataset. All images have \\nan equal resolution of 256 x 256 pixels and are in grayscale. \\nWe usually refer to the seven classes using common two-letter \\ncodes (i.e. NE for neutral) as shown in fig. 3. \\nEven though the JAFFE dataset was first published in \\n1998 [22], it is still relevant today and used as a benchmark for \\nfacial expression recognition throughout the literature [30]\\xad\\n[32]. It is a challenging dataset because there are only few \\nexamples per subject / expression, even more so if a separate \\ntesting set is excluded from training. Furthermore, the fact \\nthat every subject performs every expression allows for inter\\xad\\nesting experiments like for example the leave-one-out scheme \\ndescribed later in this section. \\nThe performance is analyzed with two different experiments \\nfollowing the methodology of Subramanian et al. [32]. In the \\nfirst experiment, a leave-one-out scheme is used to split the \\ndataset into training and testing subsets. One sample from each \\nsubject / expression combination is randomly taken out and \\nused for testing. In total, there are 143 training samples and \\n70 samples for testing [32]. This process is repeated 15 times \\nand performance measures are averaged. \\nIn the second experiment, we use ten-fold cross validation \\nto determine the training and testing sets. The whole dataset \\nis randomly split into ten almost equally sized subsets of \\nabout 21 samples each. Each subset is then used once for \\ntesting while the remaining nine subsets compose the training Table I \\nCLASSIFICATION FI-SCORES, STANDARD DEVIATIONS AND TRAINING \\nTIME FOR FIVE DIFFERENT NUMBERS OF FEATURE MAPS IN THE CAE \\nLAYER. THE TIME MEASURES A COMPLETE RUN OF TEN-FOLD CROSS \\nVALIDATION. \\nFeature maps 20 40 60 80 100 \\nMean FI -score 89.8% 91.2% 91.8% 93.7% 90.4% \\nStd. dev. 4.3% 9% 6.3% 2.6% 3.9% \\nTime (Minutes) 16.4 25 41.1 58.4 85.4 \\ndata. Finally, the perfonnance measures of all ten splits are \\naveraged. As with the first experiment, this process is repeated \\n15 times. \\nAs mentioned in section II, we optimize the number of \\nfeature maps in the CAE and both first layers in the 3-channel \\nSobel-based network. In both cases, we explore a range from \\n20 to 100 feature maps, in steps of 20. For this parameter \\nexploration we use the same ten-fold cross validation as it is \\nused in the second main experiment. For each parameter, the \\naverage Fl -score over all classes is detennined, of which in \\nturn the mean is computed after running all ten partitions in \\nthe cross validation. \\nTable I shows average Fl -scores for the explored range as \\nwell as standard deviations. The general trend is an increase \\nin performance as more feature maps are used, which is not \\nsurprising. However, the Fl -score drops to 90.4% on 100 \\nfeature maps, which might indicate that the large amount of \\nmodel parameters causes training to be more difficult. Even as \\nlow as 20 feature maps result in a classifier that achieves just \\nshort of 90% Fl -score. This is a very good result, considering \\nthat training a corresponding network takes less than two \\nminutes2. \\nFor our Sobel-based network, the results are shown in \\ntable II. Interestingly, a network that contains only 20 feature \\nmaps in its first layers performs better than all other parameters \\nwe tested. It is also noteworthy, that this particular network \\nperforms better than our equivalent CAE-based network (com\\xad\\npare table I), which is not true for any of the other parameter \\nvalues. In general, it seems the Sobel-based architecture has \\ndifficulties during training when too many feature maps and \\nconsequently too many model parameters are present. \\nIn order to determine the network parameters for our final \\nexperiments, we are not just taking into account the Fl -scores \\nof the presented ten-fold cross validation, but also the time \\nit takes to train a network. The goal is to find a suitable \\ncompromise between both criteria. Considering the training \\ntime is important here, because both main experiments are \\nrepeated 15 times for each network architecture (CAE-and \\nSobel-based). Tables I and II show that the training time \\ndepends roughly linear on the number of features and ranges \\nfrom as low as 16.4 minutes to more than 2 hours. Based on \\nthese considerations as well as the F1-scores discussed before, \\n2This value corresponds to training a single network, not ten-fold cross \\nvalidation, and excludes the time it took to train the CAE. \\nTable II \\nCLASSIFICATION FI-SCORES, STANDARD DEVIATIONS AND TRAINING \\nTIME FOR FIVE DIFFERENT NUMBERS OF FEATURE MAPS IN BOTH SOBEL \\nLAYERS. THE TIME MEASURES A COMPLETE RUN OF TEN-FOLD CROSS \\nVALIDATION. \\nFeature maps 20 40 60 80 100 \\nMean FI-score 93.4% 89.5% 88% 90.8% 88% \\nStd. dey. 2% 5.8% 6% 5.8% 5.7% \\nTime (Minutes) 2l.5 34.6 60.1 88.1 130.1 \\nwe choose 40 feature maps for both architectures and all \\nfollowing experiments. Choosing a value on the lower half of \\nthe range allows us repeat our main experiments often, because \\nthey can be trained quickly. On the other hand, both models \\nachieve good Fl -scores (about 90% and 91 %), which are well \\nwithin the average of the considered range here. Choosing the \\nsame number of feature maps for both models also has the \\nadvantage of better comparability. \\nOf course, our method requires determining several hyper\\xad\\nparameters. These include the number of layers in each \\nchannel, their number of feature maps and kernel sizes, as \\nwell as the size of the fully-connected layer that merges \\nboth channels. We concentrate our efforts here on the layer \\nthat is trained in an unsupervised fashion, mainly because \\nthe MCCNN architecture itself has already been explored in \\nprevious work [18]. The same parameter is optimized in our \\nSobel-based network in order to provide an equal ground for \\nall following experiments. \\nAll experiments as well as the CAE training were performed \\non a desktop machine with an Intel Xeon E5630 processor, \\ntwo Nvidia GeForce GTX 590 graphics cards and 24 GiB of \\nRAM. Theano was used to accelerate computations on both \\nGPUs [33], [34]. Training the CAE with 40 feature maps took \\nabout 15 minutes and comprised 20000 parameter updates. \\nTraining times for the whole network are given in Tables I \\nand II. \\nIV. RESULTS \\nA. Experiment 1 \\nWe first report the results of the leave-one-out experiment, \\naveraged over 15 runs. Table III shows the summary of our \\nresults, as well as the results reported by Subramanian et al. \\n[32]. For our experiment, an average accuracy of 95.8% was \\nobtained with a standard deviation of 1.6. These results present \\nan improvement of almost 8% when compared to the McFIS \\nmethod. When compared with the architecture using Sobel\\xad\\nbased filters, the results are almost 3% higher. While the CAE\\xad\\nbased network consists of 2 channels, the Sobel-based one is \\nimplemented with 3 channels. The training and recognition \\ntime are improved as less parameters need to be updated, while \\nat the same time, more diverse information is present. \\nA combined confusion matrix is shown in table IV. Each \\ncell in the matrix is computed as the average of the corre\\xad\\nsponding values of all confusion matrices. The low standard Table 1lI \\nRESULTS AND COMPARISON FOR THE LEAVE-ONE-OUT EXPERIMENT. \\nREPORTED ARE AVERAGE ACCURACIES AND STANDARD DEVIATIONS. \\nMethod Performance \\nCAE-based 95.8 ± 1.6 \\nSobel-based 93.1 ± l.6 \\nMcFIS [32] 87.6 ± 5.79 \\nTable IV \\nCOMBINED CONFUSION MATRIX OF 15 LEAVE-ON E-OUT EXPERIMENTS. \\nVALUES ARE GIVEN IN PERCENT AND DO NOT NECESSARILY ADD UP TO \\n100% DUE TO ROUNDING. \\nActual class \\nAN DI FE HA NE SA SU \\nAN 94 0 2.7 0 0.7 0 0 \\n\" DI 0 97.3 2.7 0 4.7 0 0 \\n.S: FE 0 0.7 90.7 0 0 0 0 t) \\n\\'6 HA 0 0 0 99.3 1.3 0 0 e \\n0-NE 0 2 0 0 93.3 0 4 \\nSA 6 0 0 0 0 100 0 \\nSU 0 0 4 0.7 0 0 96 \\ndeviations are visible here as well, as most values are either \\nzero or quite low. Especially noticeable are the results for the \\nclass fear (FE). The JAFFE dataset authors describe fear as \\nbeing difficult to express for Japanese people and consequently \\nexclude it from several experiments. This is consistent with our \\nresults here, as fear has the lowest accuracy (90.7%) among \\nall classes. \\nIn fig. 4 we show the best and worst confusion matrices \\nbased on the accuracy. In the best case we achieved an \\naccuracy of 98.6%. In this particular experiment, only a single \\nsample of the training set was misclassified (neutral instead of \\ndisgust). Subramanian et al. [32] report a best result of 94.5% \\naccuracy which is about 4% lower than ours. The right image \\nin fig. 4 shows the worst single experiment in which we still \\nachieved 92.9% accuracy. Here we can see that three classes \\n(happiness, sadness and surprise) were classified perfectly. All \\nother classes have only minor misclassification (at most 2 out \\nof 10). \\nB. Experiment 2 \\nThe second experiment uses ten-fold cross validation and \\nis repeated for 15 runs as well. A summary of the results is \\nshown in table V. The same table also shows results from the \\nliterature for comparison. We achieve an accuracy of 94.1 % \\nwith a standard deviation of 4.3. Our CAE-based MCCNN \\narchitecture achieves more than 5% better accuracy than the \\nMcFIS method [32] and is on average slightly better the \\nLinear Programming method presented by Feng et al. [35]. \\nThe improvement over the Sobel-based network is not as \\nhigh (2.1 %) but still consistent. It should be noted, that the \\nCAE-based network has one channel less than the Sobel-based \\nActual class Actual class \\nAN DI FE HA NE SA SU AN DI FE HA NE SA SU \\n10 10 \\nSU SU \\nSA 8 SA 8 \\n:: NE :: NE \\n.S: 6 .S: 6 \\nU HA U HA \\'i5 \\'i5 \\n£ FE 4 £ FE 4 \\nDI 2 DI 2 \\nAN AN \\n0 0 \\n(a) Best confusion matrix (b) Worst confusion matrix \\nFigure 4. Best (a) and worst (b) confusion matrices based on accuracies of a single leave-one-out experiment. \\nTable V \\nRESULTS AND COMPARISON FOR THE TEN-FOLD CROSS VALIDATION \\nEXPERIMENT. REPORTED ARE AVERAGE ACCURACIES AND STANDARD \\nDEVIATIONS. \\nMethod \\nCAE-based \\nSobel-based \\nMcFIS [32] \\nLinear Programming [35] Performance \\n94.1 ± 4.3 \\n92 ± 6.1 \\n89.05 ± 3.214 \\n93.8 \\nnetwork, whereas the remaining topology is identical. It might \\nthus be concluded that the proposed method is less expressive. \\nHowever as the experiments show, the Gabor-like filters in our \\nnetwork support classification very well. \\nInterestingly, the accuracy here is lower than for the leave\\xad\\none-out experiment reported earlier. At the same time, the \\nstandard deviation is much higher (4.3% vs. 1.6%). We believe \\nthis is a result of ambiguities in the dataset [22] and the \\nsmall (10%) testing set used here, compared to the previous \\nexperiment (about 33%). There is a higher chance that a certain \\nclass is represented by only ambiguous samples, which might \\naccount for the higher standard deviation as well as a slightly \\nlower average accuracy. \\nIn table VI, we show a combined confusion matrix of all \\nten-fold cross validation experiments. Compared to the first \\nexperiment (table IV), the higher standard deviation is visible \\nas many more cells are now greater than zero. However, many \\ncells outside the main diagonal are just slightly above zero \\nwhich means that most of the time very few sample were rnis\\xad\\nclassified. The results are consistent with the previous ones re\\xad\\ngardingf ear, which shows the lowest accuracy again (85.9%). \\nJudging by both experiments (tables IV and VI),jear is mostly \\nconfused with anger, disgust and surprise. \\nV. CONCLUSI ON \\nIn this paper, we presented a variant of the Multi-Channel \\nConvolutional Neural Network. The architecture is character-Table VI \\nCOMBINED CONFUSION MATRIX OF THE TEN-FOLD CROSS VALIDATION \\nEXPERIMENTS. VALUES ARE GIVEN IN PERCENT AND DO NOT \\nNECESSARILY ADD UP TO 100% DUE TO ROUNDING. \\nActual class \\nAN DI FE HA NE SA SU \\nAN 90.9 0.8 3 0 1.4 2.5 0 \\n:: DI 0 89.6 4.3 0.8 4.9 0 0 \\n0 \\n\\'B FE 1.1 4.8 85.9 0 0 0 1.7 \\n\\'i5 HA 0 2.4 0.5 96.3 3.7 0 0.9 � \\nc... NE 0 2.1 0.8 0 88 0 4.3 \\nSA 8.1 0.3 0.5 0.5 0 97.5 0 \\nSU 0 0 5 2.4 2 0 93.1 \\nized by multiple CNNs (channels), that first process infor\\xad\\nmation independently. All streams merge in a fully-connected \\nlayer, which is used for classification. Compared to previous \\nwork [18], hard-coded Sobel filters are replaced by a single \\nchannel, making the proposed model a 2-channel architecture. \\nThe weights of this new channel are trained as Convolutional \\nAutoencoder, which usually results in Gabor-like filters when \\ntrained on whitened natural images. This model is motivated \\nby two observations: First, using additional channels with \\ninput from edge detectors (Sobel) proved useful in previous \\nwork. Our Gabor-like filters provide qualitatively the same \\ninformation, but in a much more diverse scope, because \\nthe CAE has 40 different feature maps. Second, the effort \\nit takes to fully train a CAE-based network is lower than \\nthe previous Sobel-based network. Having one channel less \\ncertainly contributes to this, but also the fact that the CAE \\nlayer is reusable. The time it takes to train a CAE can easily \\nbe amortized by using it multiple times. \\nEvaluation of our model was done using the JAFFE dataset \\nand we followed the methodology of two experiments done \\nby Subramanian et al. [32]. Both experiments were performed \\non the proposed model as well as the Sobel-based model \\nFigure 5. Visualizations of four first-layer filters of the CNN channel. \\nfor comparison. The experiments and results show, that the \\nproposed model is a very viable method for facial expression \\nrecognition. It does not depend on any hand-crafted or task\\xad\\nspecific feature extraction but exploits unsupervised learning. \\nWe easily reach state-of-the-art recognition rates with minimal \\neffort. In the leave-one-out experiment, we achieve an average \\nof 95.8% accuracy, which is 8.2% higher than what Subrama\\xad\\nnian et al. [32] reported. The ten-fold cross validation leads \\nto an average accuracy of 94.1 %. \\nThe improvements over our previous Sobel-based model \\nrange between two and three percent. Thus, our new model \\ndoes not only perform better in terms of classification perfor\\xad\\nmance but is also faster to train, because it uses one channel \\nless. However, the first layer in the CAE channel has to be \\ntaken into account as well, when considering training time, \\nbecause it is trained in a separate step and the CAE training \\neffort pays off the more often it is reused. \\nFor future work, we would like to investigate what exact \\nrole each channel plays during the implicit feature extraction \\ntask. The CAE channel for example already contains diverse \\ninformation about the existence of edges. It seems intuitive \\nto assume, that the first channel (which is trained fully super\\xad\\nvised) would not take on the same filter shapes. In this sense, it \\nis still unclear in what way both channels influence each other \\nand whether they extract complementary information. Another \\npossibility might be that both channels learn qualitatively the \\nsame information and act as weak classifiers. In this case, the \\nfully-connected layer at the end might act as a boosting layer. \\nIn some very preliminary experiments to understand better \\nwhat our architecture learns, we visualized the first layer filters \\nof the channel, which is trained fully supervised (see fig. 5). \\nOn first sight and especially in comparison to the CAE filters \\n(compare Figure 2), the filter do not seem to have meaningful \\nstructure after learning. On the other hand, the filter do not \\njust contain white noise. A possibly better approach, that we \\nwant to pursue in the future, might be to visualize not only \\nfirst layer filters but also those in other layers. For example, \\nthe deconvolutional networks used by Zeiler and Fergus [36] \\ncould give valuable insights into our networks. However, \\neven without having analyzed the relationship between both \\nchannels yet, our model achieves state-of-the-art recognition \\nrates for face expressions and is very fast to train. \\nACKNOW LEDGEMENTS \\nThis work was partially supported by the State Graduate \\nFunding Program at the University of Hamburg and by CAPES \\nBrazilian Federal Agency for the Support and Evaluation of \\nGraduate Education (p.n.5951-13-5). \\nThe authors thank Dr. Cornelius Weber and Katja Kosters \\nfor their valuable suggestions when writing this paper. REFERENCES \\n[1] S. Morishima and H. Harashima. \"Facial expression synthesis based \\non natural voice for virtual face-to- face communication with machine\". \\nin, 1993 IEEE Virtual Reality Annual International Symposium, 1993. \\nSep. 1993. pp. 486-491. \\n[2] A. Colmenarez. B. Frey. and T. Huang. \"A probabilistic framework for \\nembedded face and facial expression recognition\", in Computer Vision \\nand Pattern Recognition, 1999. IEEE Computer Society Conference \\non .• vol. 1. 1999. \\n[3] B. Chu. S. Romdhani. and L. Chen. \"3D-Aided Face Recognition \\nRobust to Expression and Pose Variations\". in 2014 IEEE Conference \\non Computer Vision and Pattern Recognition (CVPR). Jun. 2014. \\npp. 1907-1914. \\n[4] M. Cabanac. \"What is emotion?\". Behavioural Processes. vol. 60. no. \\n2. pp. 69-83. Nov. 2002. \\n[5] P. Ekman and W. V. Friesen, \"Constants across cultures in the face and \\nemotion\". 1. Pers. Soc. Psycho!., vol. 17. no. 2, pp. 124-129. 1971. \\n[6] Z. Guo-Feng, W. Ke-Jun, Y Lei, and F. Gui-Xia. \"New research \\nadvances in facial expression recognition\", in Control and Decision \\nConference (CCDC), 2013 25th Chinese. May 2013, pp. 3403-3409. \\n[7] 1. Nagi, F. Ducatelle. G. Di Caro, D. Ciresan, U. Meier, A. Giusti. \\nF. Nagi. J. Schmidhuber. and L. Gambardella. \"Max-pooling convolu\\xad\\ntional neural networks for vision-based hand gesture recognition\", in \\n20ll IEEE International Conference on Signal and Image Processing \\nApplications (ICSIPA). Nov. 2011, pp. 342-347. \\n[8] S. Ji. W. Xu, M. Yang, and K. Yu. \"3D Convolutional Neural Networks \\nfor Human Action Recognition\", IEEE Trans. Pattern Anal. Mach. \\nIntell., vol. 35, no. 1. pp. 221-231, Jan. 2013. \\n[9] D. Ciresan, U. Meier, and J. Schmidhuber. \"Multi-column deep \\nneural networks for image classification\", in 2012 IEEE Conference \\non Computer Vision and Pattern Recognition (CVPR). Jun. 2012. \\npp. 3642-3649. \\n[10] Y Bengio, A. Courville, and P. Vincent, \"Representation Learning: \\nA Review and New Perspectives\". IEEE Trans. Pattern Anal. Mach. \\nIntell., vol. 35. no. 8, pp. 1798-1828. Aug. 2013. \\n[11] G. Hinton, S. Osindero, and Y Teh, \"A Fast Learning Algorithm for \\nDeep Belief Nets\", Neural Comput .• vol. 18. no. 7, pp. 1527-1554. \\nJul. 2006. \\n[12] Y LeCun, L. Bottou, Y Bengio, and P. Haffner, \"Gradient-based \\nlearning applied to document recognition\". Proc. IEEE. vol. 86, no. \\n11. pp. 2278-2324, Nov. 1998. \\n[l3] S. Lawrence. C. Giles. A. C. Tsoi. and A. Back. \"Face recognition: \\nA convolutional neural-network approach\". IEEE Trans. Neural Netw., \\nvol. 8, no. 1, pp. 98-113, Jan. 1997. \\n[l4] M. Khalil-Hani and L. S. Sung. \"A convolutional neural network ap\\xad\\nproach for face verification\". in 2014 International Conference on High \\nPetformance Computing Simulation (HPCS), Jul. 2014, pp. 707-714. \\n[l5] T. P. Kamowski. l. Are\\\\, and D. Rose. \"Deep Spatiotemporal Fea\\xad\\nture Learning with Application to Image Classification\". in 2010 \\nNinth International Conference on Machine Learning and Applications \\n(ICMLA). Dec. 2010, pp. 883-888. \\n[16] S. E. Kahou, C. Pal. X. Bouthillier. P. Froumenty. <;. Gtil<;:ehre, R. \\nMemisevic. P. Vincent, A. Courville. Y Bengio. R. C. Ferrari. M. \\nMirza, S. Jean. P.-L. Carrier. Y Dauphin. N. Boulanger-Lewandowski. \\nA. Aggarwal, J. Zumer, P. Lamblin, J.-P. Raymond, G. Desjardins. R. \\nPascanu, D. Warde-Farley. A. Torabi. A. Sharma. E. Bengio. M. Cote. \\nK. R. Konda, and Z. Wu, \"Combining Modality Specific Deep Neural \\nNetworks for Emotion Recognition in Video\", in Proceedings of the \\n15th ACM on International Conference on Multimodal Interaction. ser. \\nICMI \\'13, New York, NY, USA: ACM. 2013. pp. 543-550. \\n[17] A. Dhall. R. Goecke, S. Lucey. and T. Gedeon, \"Collecting Large, \\nRichly Annotated Facial-Expression Databases from Movies\", IEEE \\nMultimedia. vol. 19, no. 3, pp. 34-41, 2012. \\n[18] P. Barros, S. Magg, C. Weber. and S. Wermter. \"A Multichannel \\nConvolutional Neural Network for Hand Posture Recognition\". in \\nArtificial Neural Networks and Machine Learning -ICANN 2014, \\nS. Wermter, C. Weber, W. Duch, T. Honkela, P. Koprinkova-Hristova. \\nS. Magg, G. Palm, and A. E. P. Villa. Eds., ser. Lecture Notes in \\nComputer Science 8681. Springer International Publishing. Sep. 15, \\n2014, pp. 403-410. \\n[l9] M. Ranzato. F. J. Huang, Y-L. Boureau, and Y LeCun, \"Unsupervised \\nLearning of Invariant Feature Hierarchies with Applications to Object \\nRecognition\", in IEEE Conference on Computer Vision and Pattern \\nRecognition, 2007. CVPR \\'07, Jun. 2007, pp. 1-8. \\n[20] M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, \\nQ. Y. Le, and A. Y. Ng, \"Building high-level features using large \\nscale unsupervised learning\", presented at the Proceedings of the \\n29th International Conference on Machine Learning (ICML-12), 2012, \\npp. 81-88. \\n[21] E. Doi, T Inui, T-W. Lee, T Wachtler, and T J. Sejnowski, \"Spa\\xad\\ntiochromatic Receptive Field Properties Derived from Information\\xad\\nTheoretic Analyses of Cone Mosaic Responses to Natural Scenes\", \\nNeural Computation, vol. 15, no. 2, pp. 397-417, Feb. 1,2003. \\n[22] M. Lyons, S. Akamatsu, M. Kamachi, and 1. Gyoba, \"Coding facial \\nexpressions with Gabor wavelets\", in Third IEEE International Confer\\xad\\nence on Automatic Face and Gesture Recognition, 1998. Proceedings, \\nApr. 1998, pp. 200-205. \\n[23] P. Barros, G. I. Parisi, D. Jirak, and S. Wermter, \"Real-time Gesture \\nRecognition Using a Humanoid Robot with a Deep Neural Archi\\xad\\ntecture\", in Humanoid Robots (Humanoids), 2014 14th IEEE-RAS \\nInternational Coriference on, Nov. 2014, In Press. \\n[24] X. Glorot, A. Bordes, and Y. Bengio, \"Deep Sparse Rectifier Neural \\nNetworks\", presented at the International Conference on Artificial \\nIntelligence and Statistics, 2011, pp. 315-323. \\n[25] A. Makhzani and B. Frey, \"A Winner-Take-All Method for Training \\nSparse Convolutional Autoencoders\", ARXlVJ4092752 Cs, Sep. 9, \\n2014, arXiv: 1409.2752. \\n[26] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, \"Contrac\\xad\\ntive Auto-Encoders: Explicit Invariance During Feature Extraction\", \\npresented at the Proceedings of the 28th International Conference on \\nMachine Learning (lCML-II), 2011, pp. 833-840. \\n[27] A. J. Bell and T J. Sejnowski, \"The \"independent components\" of \\nnatural scenes are edge filters\", Vision Research, vol. 37, no. 23, \\npp. 3327-3338, Dec. 1997. \\n[28] M. Brown, S. Susstrunk, and P. Fua, \"Spatio-chromatic decorrela\\xad\\ntion by shift-invariant filtering\", in 2011 IEEE Computer Society \\nConference on Computer Vision and Pattern Recognition Workshops \\n(CVPRW), Jun. 2011, pp. 27-34. [29] A. Coates, A. Y. Ng, and H. Lee, \"An Analysis of Single-Layer \\nNetworks in Unsupervised Feature Learning\", presented at the In\\xad\\nternational Conference on Artificial Intelligence and Statistics, 2011, \\npp. 215-223. \\n[30] F. Cheng, J. Yu, and H. Xiong, \"Facial Expression Recognition in \\nJAFFE Dataset Based on Gaussian Process Classification\", IEEE \\nTrans. Neural Netw., vol. 21, no. 10, pp. 1685-1690, Oct. 2010. \\n[31] S. Liu, Y. Zhang, and K. Liu, \"Facial expression recognition under \\nrandom block occlusion based on maximum likelihood estimation \\nsparse representation\", in 2014 International foint Conference on \\nNeural Networks (]JCNN), Jul. 2014, pp. 1285-1290. \\n[32] K. Subramanian, S. Suresh, and R. Venkatesh Babu, \"Meta-Cognitive \\nNeuro-Fuzzy Inference System for human emotion recognition\", in The \\n20I2International foint Conference on Neural Networks (]JCNN), Jun. \\n2012, pp. 1-7. \\n[33] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. \\nDesjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \"Theano: A \\nCPU and GPU Math Compiler in Python\", presented at the Proceedings \\nof the 9th Python in Science Conference, 2010, pp. 3-10. \\n[34] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, l. Goodfellow, A. \\nBergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio, \"Theano: \\nNew features and speed improvements\", ARXIVI2JJ5590 Cs, Nov. 23, \\n2012, arXiv: 1211.5590. \\n[35] X. Feng, M. Pietikiiinen, and A. Hadid, \"Facial expression recognition \\nwith local binary patterns and linear programming\", Pattern Recognit. \\nImage Anal., vol. IS, no. 2, pp. 546-548, 2005. \\n[36] M. D. Zeiler and R. Fergus, \"Visualizing and Understanding Con\\xad\\nvolutional Networks\", in Computer Vision -ECCV 2014, D. Reet, \\nT Pajdla, B. Schiele, and T Tuytelaars, Eds., ser. Lecture Notes in \\nComputer Science 8689. Springer International Publishing, Jan. 1, \\n2014, pp. 818-833. \\n',\n",
       " 'IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003 117\\nFace Recognition Using Kernel Direct Discriminant\\nAnalysis Algorithms\\nJuwei Lu , Student Member, IEEE , Konstantinos N. Plataniotis , Member, IEEE , and\\nAnastasios N. Venetsanopoulos , Fellow, IEEE\\nAbstract— Techniques that can introduce low-dimensional\\nfeature representation with enhanced discriminatory power is ofparamount importance in face recognition (FR) systems. It is wellknown that the distribution of face images, under a perceivablevariation in viewpoint, illumination or facial expression, is highlynonlinear and complex. It is, therefore, not surprising that lineartechniques, such as those based on principle component analysis(PCA) or linear discriminant analysis (LDA), cannot providereliable and robust solutions to those FR problems with complexface variations. In this paper, we propose a kernel machine-based\\ndiscriminant analysis method, which deals with the nonlinearity\\nof the face patterns’ distribution. The proposed method alsoeffectively solves the so-called “small sample size” (SSS) problem,which exists in most FR tasks. The new algorithm has beentested, in terms of classification error rate performance, onthe multiview UMIST face database. Results indicate that theproposed methodology is able to achieve excellent performancewithonlyaverysmallsetoffeaturesbeingused,anditserrorrateis approximately 34% and 48% of those of two other commonlyused kernel FR approaches, the kernel-PCA (KPCA) and thegeneralized discriminant analysis (GDA), respectively.\\nIndexTerms— Facerecognition(FR),kerneldirectdiscriminant\\nanalysis (KDDA), linear discriminant analysis (LDA), principle\\ncomponent analysis (PCA), small sample size problem (SSS),\\nkernel methods.\\nI. INTRODUCTION\\nWITHIN the last decade, face recognition (FR) has found\\na wide range of applications, from identity authentica-\\ntion, access control, and face-based video indexing/browsing,\\ntohuman-computerinteraction/communication.Asaresult,nu-\\nmerous FR algorithms have been proposed, and surveys in thisarea can be found in [1]–[5]. Two issues are central to all thesealgorithms: 1) feature selection for face representation and 2)\\nclassification of a new face image based on the chosen feature\\nrepresentation[6].Thisworkfocusesontheissueoffeaturese-lection. The main objective is to find techniques that can in-\\ntroduce low-dimensional feature representation of face objects\\nwith enhanced discriminatory power. Among various solutionstotheproblem,themostsuccessfularethoseappearance-basedapproaches, which generally operate directly on images or ap-\\npearancesoffaceobjectsandprocesstheimagesastwo-dimen-\\nsional (2-D) holistic patterns, to avoid difficulties associatedwiththree-dimensional(3-D)modeling,andshapeorlandmarkdetection [5].\\nManuscript received December 12, 2001; revised July 1, 2002.\\nThe authors are with the Multimedia Laboratory, Edward S. Rogers, Sr.\\nDepartment of Electrical and Computer Engineering, University of Toronto,\\nToronto, ON M5S 3G4, Canada (e-mail: kostas@dsp.toronto.edu).\\nDigital Object Identifier 10.1109/TNN.2002.806629Principle component analysis (PCA) and linear discrimi-\\nnant analysis (LDA) are two classic tools widely used in the\\nappearance-based approaches for data reduction and feature\\nextraction. Many state-of-the-art FR methods, such as Eigen-faces [7] and Fisherfaces [8], are built on these two techniquesor their variants. It is generally believed that when it comes\\nto solving problems of pattern classification, LDA-based\\nalgorithms outperform PCA-based ones, since the formeroptimizes the low-dimensional representation of the objectswith focus on the most discriminant feature extraction while\\nthe latter achieves simply object reconstruction. However,\\nmany LDA-based algorithms suffer from the so-called “ small\\nsample size problem ” (SSS) which exists in high-dimensional\\npattern recognition tasks, where the number of available\\nsamples is smaller than the dimensionality of the samples.\\nThe traditional solution to the SSS problem is to utilize PCAconcepts in conjunction with LDA (PCA\\nLDA), as it was\\ndone for example in Fisherfaces [8]. Recently, more effective\\nsolutions, called direct LDA (D-LDA) methods, have been\\npresented [9], [10]. Although successful in many cases, linearmethods fail to deliver good performance when face patternsare subject to large variations in viewpoints, which results\\nin a highly nonconvex and complex distribution. The limited\\nsuccess of these methods should be attributed to their linearnature [11]. As a result, it is reasonable to assume that a bettersolution to this inherent nonlinear problem could be achieved\\nusing nonlinear methods, such as the so-called kernel machine\\ntechniques [12]–[15].\\nInthispaper,motivatedbythesuccessthatsupportvectorma-\\nchines (SVMs) [16]–[18], kernel PCA (KPCA) [19] and gen-\\neralized discriminant analysis (GDA) [20] have in pattern re-\\ngression and classification tasks, we propose a new kernel dis-criminantanalysisalgorithmforfacerecognition.Thealgorithmgeneralizes the strengths of the recently presented D-LDA and\\nthe kernel techniques while at the same time overcomes many\\nof their shortcomings and limitations. Therefore, the proposedalgorithm can be seen as an enhanced kernel D-LDA method(hereafterKDDA).FollowingtheSVMparadigm,wefirstnon-\\nlinearlymaptheoriginalinputspacetoanimplicithigh-dimen-\\nsional feature space, where the distribution of face patterns ishoped to be linearized and simplified. Then, a new variant ofthe D-LDA method is introduced to effectively solve the SSS\\nproblem and derive a set of optimal discriminant basis vectors\\nin the feature space.\\nThe rest of this paper is organized as follows. Since KDDA\\nis built on D-LDA and GDA, in Section II, we start the anal-\\nysisbybrieflyreviewingthetwolattermethods.Followingthat,\\n1045-9227/03$17.00 © 2003 IEEE\\n118 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003\\nKDDA is introduced and analyzed. The relationship of KDDA\\nto D-LDA and GDA is also discussed. In Section III, two setsofexperimentsarepresentedtodemonstratetheeffectivenessof\\nthe KDDA algorithm on highly nonlinear highly complex face\\npatterndistributions.KDDAiscompared,intermsoftheclassi-ficationerrorrateperformance,toKPCAandGDAonthemul-tiview UMIST face database. Conclusions are summarized in\\nSection IV.\\nII. M\\nETHODS\\nTheproblemtobesolvedisformallystatedasfollows:Aset\\noftraining face images is available. Each image is\\ndefinedasavectoroflength ,i.e., ,where\\nisthefaceimagesizeand denotesa -dimensional\\nrealspace.Itisfurtherassumedthateachimagebelongstooneof\\nclasses . The objective is to find a transformation\\n, based on optimization of certain separability criteria, which\\nproducesamapping ,with thatleadstoan\\nenhanced separability of different face objects.\\nA. GDA\\nFor solving nonlinear problems, the classic LDA has been\\ngeneralizedtoitskernelversion,namelyGDA[20].Let\\nbe a nonlinear mapping from the input space\\nto a high-dimensional feature space , where different classes\\nofobjectsaresupposedtobelinearlyseparable.Theideabehind\\nGDAistoperformaclassicLDAinthefeaturespace instead\\nof the input space .\\nLet and be the between- and within-class\\nscatter matrices in the feature space , respectively, expressed\\nas follows:\\n(1)\\n(2)\\nwhere\\n is the mean\\nof class\\n is the average of the\\nensemble, and is the element number in , which leads\\nto\\n . LDA determines a set of optimal discrim-\\ninant basis vectors, denoted by , so that the ratio of\\nthe between- and within-class scatters is maximized [21]. As-suming\\n, the maximization can be achieved\\nby solving the following eigenvalue problem:\\n(3)\\nThe feature space could be considered as a “linearization\\nspace” [22], however, its dimensionality could be arbitrarilylarge, and possibly infinite. Fortunately, the exact\\nis not\\nneeded and the feature space can become implicit by using\\nkernel methods, where dot products in are replaced with a\\nkernel function in the input space so that the nonlinear\\nmapping is performed implicitly in [23], [24].In FR tasks, the number of training samples, , is in most\\ncasesmuchsmallerthanthedimensionalityof (forLDA)or\\n(forGDA)leadingtoadegeneratedscattermatrix .Tra-\\nditionalmethods,forexampleGDAandFisherfaces[8],attempt\\ntosolvetheso-calledSSSproblembyusingtechniquessuchaspseudoinverseorPCAtoremovethenullspaceof\\n.How-\\never,ithasbeenrecentlyshownthatthenullspacemaycontain\\nthe most significant discriminant information [9], [10].\\nB. Direct LDA (D-LDA)\\nRecently, Chen et al.[9] and Yang et al.[10] proposed the\\nso-calleddirectLDA(D-LDA)algorithmthatattemptstoavoidthe shortcomings existing in traditional solutions to the SSS\\nproblem. The basic idea behind the algorithm is that the null\\nspace of\\nmay contain significant discriminant informa-\\ntion if the projection of is not zero in that direction, and\\nthat no significant information will be lost if the null space of\\nisdiscarded.Assuming,forexample,that andrepre-\\nsentthenullspacesof and ,respectively,thecom-\\nplement spaces of andcan be written as\\nand . Therefore, the optimal discriminant sub-\\nspace sought by the D-LDA algorithm is the intersection space\\n.\\nThe difference between Chen’s method [9] and Yang’s\\nmethod [10] is that Yang’s method first diagonalizes\\nto find when seek solution of (3), while Chen’s method\\nfirst diagonalizes to find . Although there is no\\nsignificant difference between the two approaches, it may beintractable to calculate\\nwhen the size of is large,\\nwhich is the case in most FR applications. For example,\\nthe size of and amounts to 10304 10304\\nfor face images of size 112 92 such as those used in our\\nexperiments. Fortunately, the rank of is determined by\\n, withthe number of image\\nclasses,usuallyasmallvalueinmostofFRtasks,e.g.,\\nin our experiments, resulting in .can be\\neasily foundby solving eigenvectorsof a19 19 matrix rather\\nthan the original 10304 10304 matrix through the algebraic\\ntransformationproposedin[7].Theintersectionspace\\ncan be obtained by solving the null space of projection of\\ninto, with the projection being a small matrix of\\nsize 19 19. For the reasons explained above, we proceed by\\nfirst diagonalizing the matrix instead of in the\\nderivation of the proposed here algorithm.\\nC. KDDA\\n1) Eigen-Analysis of in the Feature\\nSpace:Following the general D-LDA framework, we start\\nby solving the eigenvalue problem of , which can be\\nrewritten here as follows:\\n(4)\\nLUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 119\\nwhere , and . Since\\nthedimensionalityofthefeaturespace ,denotedas ,could\\nbearbitrarilylargeorpossiblyinfinite,itisintractabletodirectlycomputetheeigenvectorsofthe\\nmatrix .For-\\ntunately, the first most significanteigenvectors of\\n, which correspond to nonzero eigenvalues, can be indi-\\nrectly derived from the eigenvectors of the matrix (with\\nsize ) [7].\\nComputing , requires dot product evaluation in .\\nThis can be done in a manner similar to the one used in\\nSVM, KPCA, and GDA by utilizing kernel methods. For any\\n,weassumethatthereexistsakernelfunction\\nsuch that . The introduction of\\nthe kernel function allows us to avoid the explicit evaluation of\\nthe mapping. Any function satisfying Mercer’s condition canbe used as a kernel, and typical kernel functions include poly-nomial function, radial basis function (RBF) and multilayer\\nperceptrons [17].\\nUsing the kernel function, for two arbitrary classes\\nand\\n,a dot product matrix can be defined as\\nwhere\\n(5)\\nFor all of classes , we then define a kernel\\nmatrix\\n(6)\\nwhich allows us to express as follows:\\n(7)\\nwhere\\n ,isa matrixwith\\nterms all equal to: one, is a\\nblockdiagonalmatrix,and isa vectorwithallterms\\nequal to: (see Appendix I for a detailed derivation of\\n(7).).\\nLetand , be the th eigenvalue and cor-\\nresponding eigenvector of , sorted in decreasing order of\\neigenvalues. Since is\\nthe eigenvector of . In order to remove the null space\\nof , we only use its first eigenvectors:\\nwhere ,whose\\ncorresponding eigenvalues are greater than 0. It is not difficult\\nto see that , with ,a\\ndiagonal matrix.\\n2) Eigen-Analysis of in the Feature Space: Let\\n. Projecting and into the subspace\\nspannedby ,itcaneasilybeseenthat while\\ncan be expanded as\\n(8)Using the kernel matrix , a closed-form expression of\\ncan be obtained as follows:\\n(9)\\nwithanddefined in Appendix II along with the detailed\\nderivation of the expression in (9).\\nWe proceed by diagonalizing , a tractable\\nmatrix with size . Let be theth eigenvector of\\n, where , sorted in increasing order\\nof the corresponding eigenvalue . In the set of ordered\\neigenvectors, those that correspond to the smallest eigenvalues\\nmaximize the ratio in (3), and should be considered the mostdiscriminative features. Discarding the eigenvectors with the\\nlargest eigenvalues, the\\nselected eigenvectors are\\ndenoted as . Defining a matrix ,w e\\ncan obtain , with ,a\\ndiagonal matrix.\\nBased on the calculations presented above, a set of op-\\ntimal discriminant feature vectors can be derived through\\n. The features form a low-dimensional subspace\\nin,wheretheratioin(3)ismaximized.SimilartotheD-LDA\\nframework, the subspace obtained contains the intersection\\nspace shown in Section II-B. However, it is possible\\nthat there exist eigenvalues with in. To alleviate\\nthe problem, threshold values were introduced in [10], whereany value below the threshold\\nis promoted to (a very\\nsmall value). Obviously, performance heavily depends on the\\nheuristic evaluation of the parameter .\\nTo robustify the approach, we propose a modified Fisher’s\\ncriterion to be used instead of the conventional definition in\\n(3) when is singular. The newcriterion can be ex-\\npressed as\\n(10)\\nThe modified Fisher’s criterion of (10) has been proved to be\\nequivalent to the conventional one (3) in [25]. The expression\\nwhich is used in (10) instead of the\\ncan be shown to be nonsingular by the following\\nlemma.\\nLemma 1: Suppose is a real matrix of size\\n , and\\ncan be represented by where is a real matrix of\\nsize\\n .Then, ispositivedefinite,i.e., ,\\nwhereis a\\n identity matrix.\\nProof:Since isarealsymmetricmatrix.\\nFor any\\n nonzero real vector:\\n. According to [26], the\\nmatrix that satisfies the above conditions is positive\\ndefinite.\\nFollowing a procedure similar to can\\nbe expressed as , with\\n.Since and\\nsatisfies the conditions on discussed in Lemma 1,\\nis positive definite. As a result,\\nis nonsingular.\\n120 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003\\n3) Dimensionality Reduction and Feature Extraction: For\\nanyinputpattern ,itsprojectionintothesetoffeaturevectors,\\n, derived in Section II-C2, can be calculated by\\n(11)\\nwhere . Since\\n(12)\\nwe have\\n(13)\\nwhere isa\\nkernel vector.\\nCombining (11) and (13), we obtain\\n(14)\\nwhere\\nis a matrix which can be calculated of-\\nfline. Thus, through (14), a low-dimensional representation\\nonwith enhanced discriminant power, suitable for classifica-\\ntion tasks, has been introduced.\\n4) Comments: TheKDDAmethodimplementsanimproved\\nD-LDA in a high-dimensional feature space using a kernel ap-proach. Its main advantages can be summarized as follows.\\n1) KDDA introduces a nonlinear mapping from the input\\nspace to an implicit high-dimensional feature space,\\nwhere the nonlinear and complex distribution of patternsintheinputspaceis“linearized”and“simplified”sothatconventional LDA can be applied. It is not difficult to\\nsee that KDDA reduces to D-LDA for\\n. Thus,\\nD-LDA can be viewed as a special case of the proposedKDDA framework.\\n2) KDDAeffectivelysolvestheSSSprobleminthehigh-di-\\nmensional feature space by employing an improved\\nD-LDAalgorithm.UnliketheoriginalD-LDAmethodof[10], zero eigenvalues of the within-class scatter matrix\\nare never used as divisors in the improved one. In this\\nway, the optimal discriminant features can be exactlyextractedfrombothofinsideandoutsideof\\n’snull\\nspace.\\n3) In GDA, to remove the null space of , it is re-\\nquired to compute the pseudo inverse of the kernel ma-trix\\n, which could be extremely ill-conditioned when\\ncertain kernels or kernel parameters are used. Pseudoin-\\nversionisbasedoninversionofthenonzeroeigenvalues.\\nFig. 1. Some face samplesof one subject from the UMIST facedatabase.\\nDue to round-off errors, it is not easy to identify the truenull eigenvalues. As a result, numerical stability prob-\\nlems often occur [14]. However, it can been seen fromthe derivation of KDDA that such problems are avoidedin KDDA. The improvement can be observed also in ex-\\nperimental results reported in Figs. 4(a) and 5(a).\\nThe detailed steps for implementing the KDDA method are\\nsummarized in Fig. 6.\\nIII. E\\nXPERIMENTAL RESULTS\\nTwo sets of experiments are included in this paper to illus-\\ntrate the effectiveness of the KDDA algorithm. In all experi-ments reported here, we utilize the UMIST face database [27],\\n[28],amultiviewdatabase,consistingof575gray-scaleimages\\nof 20 subjects, each covering a wide range of poses from pro-filetofrontalviewsaswellasrace,genderandappearance.Allinput images are resized into 112\\n92, a standardized image\\nsize commonly used in FR tasks. The resulting standardized\\ninput vectors are of dimensionality . Fig. 1 depicts\\nsomesampleimagesofatypicalsubsetintheUMISTdatabase.\\nA. Distribution of Multiview Face Patterns\\nThe distribution of face patterns is highly nonconvex and\\ncomplex,especiallywhenthepatternsaresubjecttolargevaria-tionsinviewpointsasisthecasewiththeUMISTdatabase.Thefirstexperimentaimstoprovideinsightson howtheKDDA al-\\ngorithmlinearizes and simplifiesthe face patterndistribution.\\nFor the sake of simplicity in visualization, we only use a\\nsubset of the database, which contains 170 images of five ran-domly selected subjects (classes). Four types of feature bases\\nare generalized from the subset by utilizing the PCA, KPCA,\\nD-LDA, and KDDA algorithms, respectively. In the four sub-spacesproduced,twoarelinear,producedbyPCAandD-LDA,and two are nonlinear, produced by KPCA and KDDA. In the\\nsequence, all of images are projected onto the four subspaces.\\nFor each image, its projections in the first two most significantfeaturebases of each subspace arevisualized in Figs. 2and 3.\\nIn Fig. 2, the visualized projections are the first two most\\nsignificant principal components extracted by PCA andKPCA, and they provide a low-dimensional representationfor the samples, which can be used to capture the structure\\nof data. Thus, we can roughly learn the original distribution\\nof face samples from Fig. 2(a), which is nonconvex andcomplex as we expected based on the analysis presented inthe previous sections. In Fig. 2(b), KPCA generalizes PCA\\nto its nonlinear counterpart using a RBF kernel function:\\nLUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 121\\nFig.2. Distributionof170samplesoffivesubjectsinPCA-andKPCA-based\\nsubspaces. (A) PCA-based subspace /40 /26 /41. (B) KPCA-based subspace /40 /26\\n/41.\\nFig. 3. Distribution of 170 samples of five subjects in D-LDA- and\\nKDDA-based subspaces. (A) D-LDA-based subspace /40 /26 /41. (B)\\nKDDA-based subspace /40 /26 /41.\\nwith .H o w -\\never, it is hard to find any useful improvement for the purposeof pattern classification from Fig. 2(b). It can be concluded,therefore, that the low-dimensional representation obtained by\\nPCA like techniques, achieve simply object reconstruction,\\nand they are not necessarily useful for discrimination andclassification tasks [8], [29].\\nUnlike PCA approaches, LDA optimizes the low-dimen-\\nsional representation of the objects based on separability\\ncriteria. Fig. 3 depicts the first two most discriminant featuresextracted by utilizing D-LDA and KDDA, respectively. Simpleinspection of Figs. 2 and 3 indicates that these features out-\\nperform, in terms of discriminant power, those obtained using\\nPCA like methods. However, subject to limitation of linearity,some classes are still nonseparable in the D-LDA-based\\nsubspace as shown in Fig. 3(a). In contrast to this, we can\\nsee the linearization property of the KDDA-based subspace,as depicted in Fig. 3(b), where all of classes are well linearlyseparable when a RBF kernel with\\nis used.\\nB. Comparison With KPCA and GDA\\nThesecondexperimentcomparestheclassificationerrorrate\\nperformance of the KDDA algorithm to two other commonlyusedkernelFRalgorithms,KPCAandGDA.TheFRprocedureis completed in two stages:\\n1) Featureextraction.Theoveralldatabaseisrandomlypar-\\ntitionedintotwosubsets:thetrainingsetandtestset.Thetraining set is composed of 120 images: Six images per\\nperson are randomly chosen. The remaining 455 imagesare used to form the test set. There is no overlapping be-\\ntween the two. After training is over, both sets are pro-jected into the feature spaces derived from the KPCA,\\nGDA and KDDA methods.\\n2) Classification. This is implemented by feeding feature\\nvectors obtained in Step 1) into a nearest neighbor clas-sifier.Itshouldbenotedatthispointthat,sincethefocus\\ninthispaperisonfeatureextraction,asimpleclassifieris\\nalwayspreferedsothattheFRperformanceisnotmainlycontributed by the classifier but the feature selection al-gorithms. We anticipate that the classification accuracy\\nof all the three methods compared here will improve if a\\nmoresophisticatedclassifiersuchasSVMisusedinsteadof the nearest neighbor. However, such an experiment isbeyond the scope of this paper. To enhance the accuracy\\nof performance evaluation, the classification error rates\\nreported in this work are averaged over eight runs. Eachrun is based on a random partition of the database intothetrainingandtestsets.Followingtheframeworkintro-\\nducedin[30],[6],[31],theaverageerrorrate,denotedas\\n, is given as follows:\\n(15)\\nwhereisthenumberofruns, isthenumberofmis-\\nclassificationsforthe thrun,and isthenumberoftotal\\ntest samples of each run.\\nToevaluatetheoverallperformanceofthethreemethods,two\\ntypical kernel functions: namely the RBF and the polynomial\\nfunction, and a wide range of parameter values are tested. Sen-\\nsitivity analysis is performed with respect to the kernel param-eters and the number of used feature vectors\\n. Figs. 4 and 5\\ndepicttheaverageerrorrates ofthethreemethodscom-\\npared when the RBF and polynomial kernels are used.\\nThe only kernel parameter for RBF is the scale value .\\nFig.4(a)showstheerrorratesasfunctionsof withintherange\\nfrom to, when the optimal number of feature vec-\\ntors, ,isused.Theoptimalfeaturenumberisaresult\\noftheexistenceofthepeakingeffectinthefeatureselectionpro-cedure.Itiswellknownthattheclassificationerrorinitiallyde-clineswiththeadditionofnewfeatures,attainsaminimum,and\\nthenstartstoincrease[32].Theoptimalnumbercanbefoundby\\nsearching the number of used feature vectors that results in theminimalsummationoftheerrorratesoverthevariationrangeof\\n. In Fig. 4(a), is the value used for KPCA, while\\nis used for GDA and KDDA. Fig. 4(b) depicts the\\nerrorratesasfunctionsof withintherangefrom5to19,when\\noptimal is used. Similar to is defined as\\nthe scale parameter that results in the minimal summation of\\ntheerrorratesoverthevariationrangeof fortheexperiment\\ndiscussed here. In Fig. 4(b), a value is found for\\nKPCA, for GDA and for\\nKDDA.\\nAs such, the average error rates of the three methods with\\npolynomial kernel areshown\\nin Fig. 5. For the sake of simplicity, we only test the influence\\nof, while and are fixed. Fig. 5(a) depicts the\\n122 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003\\n(a) (b)\\nFig. 4. Comparison of error rates based on RBF kernel function. (a) Error rates as functions of /27. (b) Error rate as functions of /77.\\n(a) (b)\\nFig. 5. Comparison of error rates based on Polynomial kernel function. (a) error rates as functions of /97. (b) Error rate as functions of /77.\\nTABLE I\\nAVERAGEPERCENTAGES OF THE ERROR\\nRATE OFKDDA O VERTHOSE OFOTHERS\\nerror rates as functions of within the range from to\\n, where for KPCA, for GDA\\nand KDDA. Fig. 5(b) shows the error rates as functions of\\nwithin the range from 5 to 19 with for KPCA,\\nfor GDA and for KDDA,\\ndetermined similarly to and .\\nLetandbetheaverageerrorratesofKDDAandany\\noneofothertwomethodsrespectively,where .\\nFrom Figs. 4(b) and 5(b), we can obtain an interesting quan-\\ntity comparison: the average percentages of the error rate ofKDDA overthose of other methods by\\n. The\\nresultsaretabulatedinTableI.TheaverageerrorrateofKDDA\\nto KPCA and GDA are only about 34.375% and 47.765% re-\\nspectively.ItshouldbealsonotedthatFigs.4(a)and5(a)revealthenumericalstabilityproblemsexistinginpracticalimplemen-tations of GDA. Comparing the GDA performance to that of\\nKDDA we can easily see that the later is more stable and pre-dictable,resultinginacosteffectivedeterminationofparameter\\nvalues during the training phase.\\nIV. C\\nONCLUSION\\nAnewFRmethodhasbeenintroducedinthispaper.Thepro-\\nposed method combines kernel-based methodologies with dis-\\ncriminant analysis techniques. The kernel function is utilizedto map the original face patterns to a high-dimensional featurespace,wherethehighlynonconvexandcomplexdistributionof\\nfacepatternsislinearizedandsimplified,sothatlineardiscrim-\\ninant techniques can be used for feature extraction. The smallsample size problem caused by high dimensionality of mappedpatterns, is addressed by animproved D-LDA techniquewhich\\nexactly finds the optimal discriminant subspace of the feature\\nspace without any loss of significant discriminant information.ExperimentalresultsindicatethattheperformanceoftheKDDAalgorithm is overall superior to those obtained by the KPCA\\nor GDA approaches. In conclusion, the KDDA algorithm is a\\ngeneral pattern recognition method for nonlinearly feature ex-tractionfromhigh-dimensionalinputpatternswithoutsufferingfromtheSSSproblem.Weexpectthatinadditiontofacerecog-\\nnition, KDDA will provide excellent performance in applica-\\ntions where classification tasks are routinely performed, suchas content-based image indexing and retrieval as well as videoand audio classification.\\nLUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 123\\nFig. 6. KDDA pseudocode implementation.\\nAPPENDIX I\\nCOMPUTATION OF\\nExpanding ,w eh a v e\\n(16)\\nwhere\\n(17)\\nWedevelopeachtermof(17)accordingtothekernelmatrix\\nas follows:\\n•\\n•\\n•\\nApplying theabove derivationsinto (17), weobtain the(7).\\nAPPENDIX II\\nCOMPUTATION OF\\nExpanding ,w eh a v e\\n(18)\\n124 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003\\nwhere\\n(19)\\nFirst, expand the term\\n in (19), and\\nhave\\n(20)\\nWedevelopeachtermof(20)accordingtothekernelmatrix\\nas follows:\\n•\\n•\\n•\\n•\\nDefining\\n ,wecon-\\nclude\\n(21)\\nExpanding the term\\n in (19),we obtain\\n(22)\\nUsing the kernel matrix , the terms in (22) can be developed\\nas follows:\\n•\\n•\\n\\nLUet al.: FACE RECOGNITION USING KERNEL DIRECT DISCRIMINANT ANALYSIS ALGORITHMS 125\\n•\\n•\\nwhere\\n is a block diagonal\\nmatrix,and isa matrixwithtermsallequalto: .\\nDefining\\n , and\\nusing the above derivations, we conclude that\\n(23)\\nThus\\n(24)\\nACKNOWLEDGMENT\\nThe authors would like to thank Dr. D. Graham and Dr. N.\\nAllinson for providing the UMIST face database.\\nREFERENCES\\n[1] A. Samal and P. A. Iyengar, “Automatic recognition and analysis of\\nhuman faces and facial expressions: A survey,” Pattern Recognit. , vol.\\n25, pp. 65–77, 1992.\\n[2] D.Valentin,J.O.TooleHerveAbdiAlice,andG.W.Cottrell,“Connec-\\ntionistmodelsoffaceprocessing:Asurvey,” PatternRecognit. ,vol.27,\\nno. 9, pp. 1209–1230, 1994.\\n[3] R.Chellappa,C.L.Wilson,andS.Sirohey,“Humanandmachinerecog-\\nnition of faces: A survey,” Proc. IEEE , vol. 83, pp.705–740, 1995.\\n[4] S. Gong, S. J. McKenna,and A. Psarrou, Dynamic Vision From Images\\ntoFaceRecognition . Singapore:ImperialCollegePress,WorldScien-\\ntific, May 2000.\\n[5] M. Turk, “A random walk through eigenspace,” IEICE Trans. Inform.\\nSyst., vol. E84-D, no. 12, pp. 1586–1695, Dec. 2001.\\n[6] S. Z. Li and J. Lu, “Face recognition using the nearest feature line\\nmethod,” IEEE Trans. Neural Networks , vol. 10, pp. 439–443, Mar.\\n1999.\\n[7] M. A. Turk and A. P. Pentland, “Eigenfaces for recognition,” J. Cogn.\\nNeurosci. , vol. 3, no. 1, pp. 71–86, 1991.\\n[8] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\\nFisherfaces: Recognition using class specific linear projection,” IEEE\\nTrans.Pattern Anal. Machine Intell. , vol. 19, pp.711–720,July 1997.\\n[9] L.-F. Chen, H.-Y. M. Liao, M.-T. Ko, J.-C. Lin, and G.-J. Yu, “A new\\nLDA-based face recognition system which can solve the small sample\\nsize problem,” Pattern Recognit. , vol. 33, pp. 1713–1726, 2000.\\n[10] H. Yu and J. Yang, “A direct lda algorithm for high-dimensional data\\nwith application to face recognition,” Pattern Recognit. , vol. 34, pp.\\n2067–2070, 2001.[11] M. Bichsel and A. P. Pentland, “Human face recognition and the\\nface image set’s topology,” CVGIP: Image Understand. , vol. 59, pp.\\n254–261, 1994.\\n[12] B. Schölkopf, C. Burges, and A. J. Smola, Advances in Kernel\\nMethods—Support Vector Learning . Cambridge, MA: MIT Press,\\n1999.\\n[13] (2000). [Online]. Available: http://www.kernel-machines.org\\n[14] A. Ruiz and P. E. López de Teruel, “Nonlinear kernel-based statistical\\npatternanalysis,” IEEETrans.NeuralNetworks ,vol.12,pp.16–32,Jan.\\n2001.\\n[15] K.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf, “An\\nintroduction to kernel-based learning algorithms,” IEEE Trans. Neural\\nNetworks , vol. 12, pp. 181–201, Mar. 2001.\\n[16] C.CortesandV.N.Vapnik,“Supportvectornetworks,” MachineLearn. ,\\nvol. 20, pp. 273–297, 1995.\\n[17] V. N. Vapnik, The Nature of Statistical Learning Theory . New York:\\nSpringer-Verlag, 1995.\\n[18] B. Schölkopf, Support Vector Learning , Munich, Germany: Olden-\\nbourg-Verlag, 1997.\\n[19] B. Schölkopf, A. Smola, and K. R. Müller, “Nonlinear component\\nanalysis as a kernel eigenvalue problem,” Neural Comput. , vol. 10, pp.\\n1299–1319, 1999.\\n[20] G. Baudat and F. Anouar, “Generalized discriminant analysis using a\\nkernel approach,” Neural Comput. , vol. 12, pp.2385–2404, 2000.\\n[21] R. A. Fisher, “The use of multiple measures in taxonomic problems,”\\nAnn. Eugenics , vol. 7, pp. 179–188, 1936.\\n[22] M. A. Aizerman, E. M. Braverman, and L. I. Rozonoér, “Theoretical\\nfoundations of the potential function method in pattern recognition\\nlearning,” Automat. Remote Contr. , vol. 25, pp. 821–837, 1964.\\n[23] V. N.Vapnik, Statistical Learning Theory . New York: Wiley,1998.\\n[24] B. Scholkopf, S. Mika, C. J. C. Burges, P. Knirsch, K.-R. Muller, G.\\nRatsch, and A. J. Smola, “Input space versus feature space in kernel-\\nbasedmethods,” IEEETrans.NeuralNetworks ,vol.10,pp.1000–1017,\\nSept. 1999.\\n[25] K.Liu,Y.Q.Cheng,J.Y.Yang,andX.Liu,“Anefficientalgorithmfor\\nfoley-sammonoptimalsetofdiscriminantvectorsbyalgebraicmethod,”\\nInt. J. Pattern Recog. Artif. Intell. , vol. 6, pp. 817–829, 1992.\\n[26] R. A. Horn and C. R. Johnson, Matrix Analysis . Cambridge, MA:\\nCambridge Univ. Press, 1992.\\n[27] , D. Graham and N. Allinson. (1998). [Online]. Available: http://im-\\nages.ee.umist.ac.uk/danny/database.html\\n[28] D. B. Graham and N. M. Allinson, “Characterizing virtual eigensigna-\\ntures for general purpose face recognition,” in Face Recognition: From\\nTheorytoApplications,NATOASISeriesF,ComputerandSystemsSci-\\nences, H. Wechsler, P. J. Phillips, V. Bruce, F. Fogelman-Soulie, and T.\\nS. Huang, Eds., 1998, vol. 163, pp. 446–456.\\n[29] D. L. Swets and J. Weng, “Using discriminant eigenfeatures for image\\nretrieval,” IEEE Trans. Pattern Anal. Machine Intell. , vol. 18, pp.\\n831–836, Aug. 1996.\\n[30] S. Lawrence, C. Lee Giles, A. C. Tsoi, and A. D. Back, “Face recog-\\nnition: A convolutional neural network approach,” IEEE Trans. Neural\\nNetworks , vol. 8, pp. 98–113, Jan. 1997.\\n[31] M. Joo Er, S. Wu, J. Lu, and H. L. Toh, “Face recognition with radial\\nbasis function (RBF) neural networks,” IEEE Trans. Neural Networks ,\\nvol. 13, pp. 697–710, May 2002.\\n[32] S. J. Raudys and A. K. Jain, “Small sample size effects in statistical\\npattern recognition: Recommendations for practitioners,” IEEE Trans.\\nPattern Anal. Machine Intell. , vol. 13, pp. 252–264, May 1991.\\nJuwei Lu (S’00) received the B.Eng. degree in\\nelectrical engineering from Nanjing University of\\nAeronautics and Astronautics, China, in 1994 and\\nthe M.Eng. degree from the School of Electrical\\nand Electronic Engineering, Nanyang Technological\\nUniversity, Singapore, in 1999. Currently, he is pur-suing the Ph.D. degree in the Edward S. Rogers, Sr.\\nDepartmentofElectricalandComputerEngineering,\\nUniversityof Toronto, Toronto, ON, Canada.\\nFrom July 1999 to January 2001, he was with\\nthe Center for Signal Processing, Singapore, as a\\nResearchEngineer.Hisresearchinterestsincludemultimediasignalprocessing,\\nface detection and recognition, kernel methods, support vector machines,neural networks, and boosting technologies.\\n126 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 1, JANUARY 2003\\nKonstantinos N. Plataniotis (S’88–M’95) received\\ntheB.Eng.degreeincomputerengineeringandinfor-\\nmatics from University of Patras, Patras, Greece, in\\n1988andtheM.SandPh.Ddegreesinelectricalengi-\\nneeringfromFloridaInstituteofTechnology(Florida\\nTech),Melbourne, in1992 and 1994,respectively.\\nHe was with the Computer Technology Institute\\n(CTI), Patras, Greece from 1989 to 1991. He was a\\nPostdoctoral Fellow at the Digital Signal and Image\\nProcessing Laboratory, Department of Electrical\\nand Computer Engineering University of Toronto,\\nToronto, ON, Canada from November 1994 to May 1997. From September\\n1997toJune1999,hewasanAssistantProfessorwiththeSchoolofComputer\\nScience at Ryerson University, Toronto. He is now an Assistant Professor with\\nthe Edward S. Rogers Sr. Department of Electrical and Computer Engineeringat the University of Toronto, a Nortel Institute for Telecommunications\\nAssociate, and an Adjunct Professor in the Department of Mathematics,\\nPhysics and Computer Science at Ryerson University. His research interests\\ninclude multimedia signal processing, intelligent and adaptive systems, and\\nwireless communication systems.\\nDr. Plataniotis is a member of the Technical Chamber of Greece.\\nAnastasios N. Venetsanopoulos (S’66–M’69–\\nSM’79–F’88) received the diploma degree in\\nengineering from the National Technical University\\nof Athens (NTU), Athens, Greece, in 1965, and\\nthe M.S., M.Phil., and Ph.D. degrees in electrical\\nengineering from Yale University, New Haven, CT,in 1966, 1968, and 1969 respectively.\\nHe joined the Department of Electrical and\\nComputer Engineering of the University of Toronto,\\nToronto, ON, Canada, in September 1968, as a\\nLecturerandhewaspromotedtoAssistantProfessor\\nin 1970, Associate Professor in 1973, and Professor in 1981. He has served as\\nChairoftheCommunicationsGroupandAssociateChairoftheDepartmentof\\nElectrical Engineering. Between July 1997 and June 2001, he was Associate\\nChair: Graduate Studies of the Department of Electrical and ComputerEngineering, and was Acting Chair during spring term 1998–1999. In 1999, a\\nChair in Multimedia was established in the ECE Department, made possible\\nby a donation of $1.25 M from Bell Canada, matched by $1.0 M of university\\nfunds.HeassumedthepositionasInauguralChairholderinJuly1999,andtwo\\nadditional Assistant Professor positions became available in the same area.\\nSince July 2001, he has served as the 12th Dean of the Faculty of Applied\\nScienceandEngineeringoftheUniversityofToronto.Hewasonresearchleaveat the Imperial College of Science and Technology, the National Technical\\nUniversityofAthens,theSwissFederalInstituteofTechnology,theUniversity\\nof Florence and the Federal University of Rio de Janeiro, and has also served\\nas Adjunct Professor at Concordia University. He has served as lecturer in 138\\nshortcoursestoindustryandcontinuingeducationprogramsandasConsultant\\nto numerous organizations; he is a contributor to 29 books, a coauthor of\\nNonlinear Filters in Image Processing: Principles Applications (Boston,\\nMA: Kluwer, 1990), and Artificial Neural Networks: Learning Algorithms,\\nPerformance Evaluation and Applications , (Boston, MA: Kluwer, 1993),\\nFuzzy Reasoning in Information Decision and Control Systems , (Boston, MA:\\nKluwer, 1994), and Color Image Processing and Applications (New York:\\nSpringer-Verlag, 2000),and haspublished over700papers inrefereedjournals\\nand conference proceedings on digital signal and image processing and digital\\ncommunications.\\nProf. Venetsanopoulos has served as Chair on numerous boards, councils,\\nand technical conference committees of the IEEE, such as the Toronto Section\\n(1977–1979)andtheIEEECentralCanadaCouncil(1980–1982);hewasPres-\\nident of the Canadian Society for Electrical Engineering and Vice President ofthe Engineering Institute of Canada (EIC) (1983–1986). He was a Guest Ed-\\nitor or Associate Editor for several IEEE journals and the Editor of the Cana-\\ndianElectricalEngineeringJournal (1981–1983).HeisamemberoftheIEEE\\nCommunications, Circuits and Systems, Computer, and Signal Processing So-\\ncieties,aswellasamemberofSigmaXi,theTechnicalChamberofGreece,the\\nEuropeanAssociationofSignalProcessing,theAssociationofProfessionalEn-\\ngineersofOntario(APEO)andGreece.HewaselectedasaFellowoftheIEEE“for contributions to digital signal and image processing.” He is also a Fellow\\nof the EIC, and was awarded an Honorary Doctorate from the National Tech-\\nnical Universityof Athens, in October 1994.In October 1996,he was awarded\\nthe“ExcellenceinInnovationAward”oftheInformationTechnologyResearch\\nCentre of Ontario and Royal Bank of Canada, “for innovative work in color\\nimage processing and its industrial applications”. In November 2000, he be-\\ncameRecipientofthe“MillenniumMedalofIEEE.”InApril2001,hebecame\\naFellowoftheCanadianAcademyofEngineering,andonJuly1,2001,hewas\\nappointedasthetwelthDeanoftheFacultyofAppliedScienceandEngineering,University of Toronto.\\n',\n",
       " 'IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002 697\\nFace Recognition With Radial Basis Function (RBF)\\nNeural Networks\\nMeng Joo Er , Member, IEEE , Shiqian Wu , Member, IEEE , Juwei Lu , Student Member, IEEE , and\\nHock Lye Toh , Member, IEEE\\nAbstract— Ageneralandefficientdesignapproachusingaradial\\nbasis function (RBF) neural classifier to cope with small trainingsetsofhighdimension,whichisaproblemfrequentlyencounteredin face recognition, is presented in this paper. In order to avoidoverfittingandreducethecomputationalburden,facefeaturesarefirstextractedbytheprincipalcomponentanalysis(PCA)method.Then, the resulting features are further processed by the Fisher’slineardiscriminant(FLD)techniquetoacquirelower-dimensionaldiscriminantpatterns.Anovelparadigmisproposedwherebydatainformation is encapsulated in determining the structure and ini-tial parameters of the RBF neural classifier before learning takesplace.AhybridlearningalgorithmisusedtotraintheRBFneuralnetworkssothatthedimensionofthesearchspaceisdrasticallyre-duced in the gradient paradigm. Simulation results conducted onthe ORL database show that the system achieves excellent perfor-mance both in terms of error rates of classification and learning\\nefficiency.\\nIndex Terms— Face recognition, Fisher’s linear discriminant,\\nORLdatabase,principalcomponentanalysis,radialbasisfunction(RBF) neural networks,small training sets of high dimension.\\nI. INTRODUCTION\\nMACHINErecognitionofhumanfacefromstillandvideo\\nimages has become an active research area in the com-\\nmunities of image processing, pattern recognition, neural net-\\nworks and computer vision. This interest is motivated by wide\\napplications ranging from static matching of controlled format\\nphotographs such as passports, credit cards, driving licenses,and mug shots to real-time matching of surveillance video im-\\nages presenting different constraints in terms of processing re-\\nquirements[1].Althoughresearchersinpsychology,neuralsci-ences and engineering, image processing and computer vision\\nhaveinvestigatedanumberofissuesrelatedtofacerecognition\\nby human beings and machines, it is still difficult to design an\\nautomaticsystemforthistask,especiallywhenreal-timeidenti-\\nficationisrequired.Thereasonsforthisdifficultyaretwo-fold:1)Faceimagesarehighlyvariableand2)Sourcesofvariability\\ninclude individual appearance, three-dimensional (3-D) pose,\\nfacial expression, facial hair, makeup, and so on and these fac-\\ntors change from time to time. Furthermore,the lighting, back-\\nground, scale, and parameters of the acquisition are all vari-\\nManuscript received March 29, 1999; revised March 5, 2001 and December\\n5, 2001.\\nM.J.EriswiththeSchoolofElectricalandElectronicEngineering,Nanyang\\nTechnological University, Singapore 639798, Singapore.\\nS. Wu and H. L. Toh are with the Centre for Signal Processing, Innovation\\nCentre, Singapore 637722, Singapore.\\nJ.LuiswithDepartmentofElectricalandComputerEngineering,University\\nof Toronto, Toronto, ON M5S 3G4, Canada.\\nPublisher Item Identifier S 1045-9227(02)03984-X.ables in facial images acquired under real-world scenarios [1].\\nAs stated by Moses et al.[2], “The variations between the im-\\nagesofthesamefaceduetoilluminationandviewingdirectionare almost always larger than image variations due to changes\\nin the face identity.” This makes face recognition a great chal-\\nlenging problem. In our opinion, two issues are central to face\\nrecognition:\\n1) Whatfeaturescanbeusedtorepresentafaceunderenvi-\\nronmental changes?\\n2) How to classify a new face image based on the chosen\\nrepresentation?\\nFor 1), many successful face detection and feature extrac-\\ntion paradigms have been developed [3]–[12]. The frequently\\nused approaches are to use geometrical features, where the rel-ative positions and shapes of different features are measured\\n[3], [4]. At the same time, several paradigms have been pro-\\nposed to use global representation of a face, where all features\\nofafaceareautomaticallyextractedfromaninputfacialimage\\n[5]–[12]. It has been indicated in [4] that these algorithms withglobal encoding of a face are fast in face recognition. In [5],\\nsingular value decomposition (SVD) of a matrix was used to\\nextract features from the patterns. It has been illustrated that\\nsingular values of an image are stable and represent the alge-\\nbraic attributes of an image, being intrinsic but not necessarilyvisible. The eigenface approach of describing the features of a\\nface was presented in [6]. The key idea is to calculate the best\\ncoordinatesystemforimagecompression,inwhicheachcoordi-nateisactuallyanimagethatiscalledaneigenpicture.However,\\nthe eigenface paradigm, which uses principal component anal-\\nysis(PCA),yieldsprojectiondirectionsthatmaximizethetotal\\nscatteracrossallclasses,i.e.,acrossallfaceimages.Inchoosing\\nthe projection which maximizes the total scatter, the PCA re-tainsunwantedvariationscausedbylighting,facialexpression,\\nandotherfactors[7].Accordingly,thefeaturesproducedarenot\\nnecessarily good for discrimination among classes. In [7], [8],\\nthefacefeaturesareacquiredbyusingthefisherfaceordiscrim-\\ninanteigenfeatureparadigm.ThisparadigmaimsatovercomingthedrawbackoftheeigenfaceparadigmbyintegratingFisher’s\\nlineardiscriminant(FLD)criteria,whileretainingtheideaofthe\\neigenface paradigm in projecting faces from a high-dimension\\nimagespacetoasignificantlylower-dimensionalfeaturespace.\\nInstead of using statistical theory, neural-networks-based fea-tureextractionhasbeenreportedrecently[9]–[12].Thegoalof\\nface processing using neural networks is to develop a compact\\ninternal representation of faces, which is equivalent to featureextraction.Therefore,thenumberofhiddenneuronsislessthan\\nthatineitherinputoroutputlayers,whichresultsinthenetwork\\nencoding inputs in a smaller dimension that retains most of the\\nimportantinformation.Then,thehiddenunitsoftheneuralnet-\\n1045-9227/02$17.00 © 2002 IEEE\\n698 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nwork can serve as the input layer of another neural network to\\nclassify face images.\\nIn many pattern recognition systems, the methodology fre-\\nquentlyusedisthestatisticalapproach,wherebydecisiontheoryderivedfromstatisticsofinputpatternsisusedtodesignaclas-\\nsifier [13]. Although this paradigm has been successfully ap-\\nplied to solve various problems in pattern classification, it hasdifficulty in expressing structural information unless an appro-priatechoiceoffeaturesismadepossible.Furthermore,thisap-\\nproach requires much heuristic information to design a classi-\\nfier [14]. Neural-networks-based paradigms, as new means ofimplementing various classifiers based on statistical and struc-tural approach, have been proven to possess many advantages\\nforclassificationbecauseoftheirlearningabilityandgoodgen-\\neralization [9]–[12], [14]–[16]. Generally speaking, multilay-ered networks (MLNs), usually coupled with the backpropaga-\\ntion (BP) algorithm, are most widely used in face recognition\\n[9]. Yet, two major criticisms are commonly raised against theBP algorithm: 1) It is computationally intensive because of itsslow convergence speed and 2) there is no guarantee at all that\\nthe absolute minima can be achieved. On the other hand, RBF\\nneuralnetworkshaverecentlyattractedextensiveinterestsinthecommunity ofneuralnetworksforawiderangeofapplications[17]–[29]. The salient features of RBF neural networks are as\\nfollows.\\n• They are universal approximators [17].\\n• They possess the best approximation property [18].\\n• Their learning speed is fast because of locally tuned neu-\\nrons [19].\\n• They have more compact topology than other neural net-\\nworks [20].\\nNormally,RBFneuralnetworksarewidelyusedforfunction\\napproximation and pattern recognition wherein the pattern di-mensionintheseapplicationsisusuallysmall.Aspointedoutby\\nMoody and Darken [19], “RBF neural networks are best suited\\nforlearningtoapproximatecontinuousorpiecewisecontinuous,real-valued mapping where the input dimension is sufficientlysmall.” When RBF neural networks are implemented in face\\nrecognition,suchsystemspossessthefollowingcharacteristics:\\n• High dimension. For example, a 128\\n128 image will\\nhave 16384 features.\\n• Small sample sets. The sample patterns are very few for\\neach class, say, only one–ten images per person so that\\n(is the number of training patterns, is the\\nnumber of features), which is more severe than the case\\nshown in [16].\\nTherefore, face recognition is substantially different from clas-\\nsicalpatternrecognitionproblem,forinstance,characterrecog-nition [14], in which there are a limitednumber of classes with\\na large number of training patterns in each class. This situation\\nleads to the following challenges in designing an RBF neuralclassifier:\\n1)Overfitting problem . It has been indicated that if the di-\\nmension of the network input is comparable to the sizeof the training set, the system is liable to overfitting and\\nresult in poor generalization [16].\\n2)Overtraining problem . High dimension of the network\\ninputresultsincomplexoptimalprocessingandslowcon-vergence. Hence, it is likely to cause overtraining.\\nFig. 1. Schematic diagram of RBF neural classifier for small training sets of\\nhigh dimension.\\n3)Small-sample effect . It has been indicated that small\\nsample can easily contaminate the design and evaluation\\nof a proposed system [30]. For applications with a large\\nnumber of features and a complex classification rule, thetraining sample size must be quite large [30]. It has beenfurtherpointed out that thesample sizeneeds to increase\\nexponentially in order to have an effective estimate of\\nmultivariate densities as thedimension increases [31].\\n4)Singularproblem .If\\nislessthan ,thesamplecovari-\\nancematrixissingular,andthereforeunusableregardless\\nof the true value of the covariance matrix [32].\\nTo circumvent the aforementioned problems, a systematic\\nmethodologyforRBFneuralclassifierdesigntodealwithsmalltraining sets of high-dimensional feature vectors is presented,\\nas shown in Fig. 1. The proposed methodology comprises the\\nfollowing parts: 1) The number of input variables is reduced\\nthrough feature selection, i.e., a set of the most expressive fea-\\ntures is first generated by the PCA and the FLD is then imple-mented to generate a set of the most discriminant features so\\nthat different classes of training data can be separated as far\\nas possible and the same classes of patterns are compacted asclose as possible; 2) A new clustering algorithm concerning\\ncategory information of training samples is proposed so that\\nhomogeneous data could be clustered and a compact structure\\nof an RBF neural classifier with limited mixed data could be\\nachieved;3)Twoimportantcriteriaareproposedtoestimatetheinitial widths of RBF units which control the generalization of\\nRBFneuralclassifier;and4)Ahybridlearningalgorithmispre-\\nsented to train the RBF neural networks so that the dimension\\nof the search space is significantly reduced in the gradient par-\\nadigm.\\nThe rest of this paper is organized as follows. Section II\\npresents the architecture of RBF neural networks and the\\nrelated design problems when they are used as a classifier.\\nSection III provides the procedure of extracting face features.\\nIn Section IV, we propose a systemic approach for structuredetermination and initialization of RBF neural networks. A\\nhybrid learning algorithm is developed in Section V. Experi-\\nmental results are demonstrated in Section VI. In Section VII,we discuss some important issues concerning performances of\\nthe proposed approach and provide more insights into several\\nparadigms,whicharecloselyrelatedtoourproposedparadigm.\\nFinally, conclusions are drawn in Section VIII.\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 699\\nFig. 2. RBF neural networks.\\nII. RBF N EURALNETWORKS\\nAn RBF neural network, shown in Fig. 2, can be considered\\nas a mapping: .\\nLet bethe input vectorand be\\ntheprototypeoftheinputvectors.TheoutputofeachRBFunitis as follows:\\n(1)\\nwhere indicates the Euclidean norm on the input space.\\nUsually, the Gaussian function is preferred among all possibleradialbasisfunctionsduetothefactthatitisfactorizable.Hence\\n(2)\\nwhere isthewidthofthe thRBFunit.The thoutput\\nof an RBF neural network is\\n(3)\\nwhere ,is the weight or strength of the th re-\\nceptive field to the th output and is the bias of the th\\noutput. In order to reduce the network complexity, the bias isnot considered in the following analysis.\\nWecanseefrom(2)and(3)thattheoutputsofanRBFneural\\nclassifier are characterized by a linear discriminant function.\\nThey generate linear decision boundaries (hyperplanes) in theoutput space. Consequently, the performance of an RBF neuralclassifier strongly depends on the separability of classes in the\\n-dimensionalspacegeneratedbythenonlineartransformation\\ncarried out by the RBF units.\\nAccordingtoCover’stheoremontheseparabilityofpatterns\\nwherein a complex pattern classification problem cast in a\\nhigh-dimensionalspacenonlinearlyismorelikelytobelinearly\\nseparable than in a low-dimensional space [33], the numberof Gaussian nodes\\n, where is the dimension of input\\nspace. On the other hand, the increase of Gaussian units may\\nresult in poor generalization because of overfitting, especially,\\ninthecaseofsmalltrainingsets[16].Itisimportanttoanalyzethe training patterns for the appropriate choice of RBF hiddennodes.\\nGeometrically, the key idea of an RBF neural network is to\\npartition the input space into a number of subspaces which areintheformofhyperspheres.Accordingly,clusteringalgorithms(\\n-meansclustering,fuzzy -meansclusteringandhierarchical\\nclustering)whicharewidelyusedinRBFneuralnetworks[19],\\n(a)\\n(b)\\nFig.3. Two-dimensionpatternsandclustering:(a)conventionalclustering,(b)\\nclustering with homogeneous analysis.\\nFig. 4. Effect of Gaussian widths in clustering.\\n[21], are a logical approach to solve the problems [19], [22].\\nHowever, it should be noted that these clustering approachesare inherently unsupervised learning algorithms as no category\\ninformation about patterns is used. As an illustrative example,\\nconsider a simple training set\\nillustrated in Fig. 3. The\\nblackandwhitedatapointsreflectthecorrespondingvaluesas-sumed bythedependent variable\\n. Ifwesimply use -means\\nclustering approach without considering , two evident clus-\\ntersasshowninFig.3(a)areachieved.Thisbringsaboutsignif-icantmisclassificationinitially. Althoughtheclusteringbound-aries are modified in the subsequent learning phase, this could\\neasilyleadtoanundesiredandhighlydominantaveragingphe-\\nnomenon as well as to make the learning less effective [21].Topreservehomogeneousclusters,threeclustersasdepictedinFig. 3(b) should be created. In other words, a supervised clus-\\nteringprocedurewhichtakesintoconsiderationthecategoryin-\\nformation of training data should be considered.\\nWhile considering the category information of training pat-\\nterns, it should be emphasized that the class memberships are\\nnotonlydependedonthedistanceofpatterns,butalsodepended\\non the Gaussian widths. As illustrated in Fig. 4,\\nis near to\\nthe center of class in Euclidean distance, but we can select\\ndifferent Gaussian widths for each cluster so that the point\\nhas greater class membership to class than that to class .\\nTherefore, the use of class membership implies that we shouldpropose a supervised procedure to cluster the training patternsanddeterminetheinitialGaussianwidths,andthisworkwillbe\\nelaborated in Section IV.\\n700 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nIII. EXTRACTION OF FACEFEATURES\\nA. Principal Component Analysis (PCA)\\nLet a face image be a two-dimensional array of\\nintensity values. An image may also be considered as a vectorof dimension\\n. Denote the training set of face images by\\n, and we assume that each\\nimagebelongstooneof classes.Definethecovariancematrix\\nas follows [6], [13]:\\n(4)\\nwhere and\\n. Then, the eigenvalues and eigenvectors of the\\ncovariance are calculated. Let\\nbe the eigenvectors corresponding to the\\nlargest eigenvalues. Thus, for a set of original face images\\n, their corresponding eigenface-based feature\\ncan be obtained by projecting into the eigenface\\nspace as follows:\\n(5)\\nB. Fisher’s Linear Discriminant (FLD)\\nActually, the PCA paradigm does not provide any informa-\\ntion for class discrimination but dimension reduction. Accord-\\ningly, the FLD is applied to the projection of the set of training\\nsamples in the eigenface space\\n. The paradigm finds an optimal subspace for classifi-\\ncation in which the ratio of the between-class scatter and the\\nwithin-classscatterismaximized[7],[8],[13].Letthebetween-class scatter matrix be defined as\\n(6)\\nand the within-class scatter matrix be defined as\\n(7)\\nwhere isthemeanimageoftheensemble,\\nand is the mean image of the th class,\\nisthenumberofsamplesinthe thclass,and isthenumber\\nofclasses.Theoptimalsubspace, bytheFLDisdeter-\\nmined as follows [7], [8], [13]:\\n(8)\\nwhere is the set of generalized eigenvectors\\nofandcorresponding to the largest generalized\\neigenvalues , i.e.,\\n(9)Thus, the feature vectors for any query face images in the\\nmost discriminant sense can be calculated as follows:\\n(10)\\nRemarks:\\n1) From (7), we see .I n\\norder to prevent from becoming singular, the value\\nofshould be no more than .\\n2) Alsowecanseefrom(6)that .\\nAccordingly,thereareatmost nonzerogeneralized\\neigenvectors.Inotherwords,theFLDtransformsthe -di-\\nmensionspaceinto -dimensionspacetoclassify\\nclasses of objects.\\n3) It should be noted that the FLD is a linear transforma-\\ntion which maximizes the ratio of the determinant of the\\nbetween-class scatter matrix of the projected samples to\\nthe determinant of the within-class scatter matrix of theprojected samples. The results are globally optimal only\\nforlinearseparabledata.Thelinearsubspaceassumption\\nis violated for the face data that have great overlappings[34]. Moreover, the separability criterion is not directlyrelated to the classification accuracy in the output space\\n[34].\\n4) Several researchers have indicated that the FLD method\\nachieved the best performance on the training data, butgeneralized poorly to new individuals [35], [36].\\nTherefore, RBF neural networks, as a nonlinear alternative\\nwith good generalization, have been proposed for face classifi-cation. In the sequel, we will use the feature vectors\\ninstead\\nof their corresponding original data in Sections IV–VIII.\\nIV. STRUCTURE DETERMINATION AND INITIALIZATION OF RBF\\nNEURALNETWORKS\\nA. Structure Determination and Choice of Prototypes\\nFrom the point of view of face recognition, a set of optimal\\nboundaries between different classes should be estimated byRBF neural networks. Conversely, from the point of view ofRBF neural networks, the neural networks are regarded as a\\nmapping from the feature hyperspace to the classes. Each pat-\\nternisrepresentedbyarealvectorandeachclassisassignedfora suitable code. Therefore, we set:\\n• the number of inputs to be equal to that of features (i.e.,\\nthe dimension of the input space);\\n• the number of outputs to be equal to that of classes (see\\nFig. 2).\\nIt is cumbersome to select the hidden nodes. Different ap-\\nproaches revolving around increasing or decreasing the com-plexityofthearchitecturehavebeenproposed[19]–[28].Manyresearchers have illustrated that the number of hidden units de-\\npends on the geometrical properties of the training patterns as\\nwellasthetypeofactivationfunction[24].Nevertheless,thisisstill an open issue in implementing RBF neural networks. Ourproposed approach is as follows.\\n1) Initially, we set the number of RBF units to be equal to\\nthat of the output,\\n, i.e., we assume that each class\\nhas only one cluster.\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 701\\n2) For each RBF unit , , the center is se-\\nlectedasthemeanvalueofthesamplepatternsbelongingto class\\n, i.e.,\\n(11)\\nwhere isthethsamplebelongingtoclass andis\\nthe total number of training patterns in class .\\n3) Foranyclass , computetheEuclidean distance from\\nthe mean to the furthest point belonging to\\nclass, i.e.,\\n(12)\\n4) For any class :\\n• Calculatethedistance betweenthemeanof\\nclassand the mean of other classes as follows:\\n(13)\\n• Find\\n(14)\\n• Check the relationship between and,\\n.\\nCase1)Nooverlapping. If ,\\nclasshas no overlapping with other\\nclasses [see Fig. 5(a)].\\nCase2)Overlapping. If ,\\nclass has overlapping with other\\nclasses and misclassification may occurin this case. Fig. 5(b) represents thecase that\\nand\\n, while Fig. 5(c)\\ndepictsthecasethat\\nand .\\n5) Splitting Criteria:\\ni)Embody Criterion : If class is embodied in class\\ncompletely, i.e., and\\n, classwill besplit into two clus-\\nters, see Fig. 6.\\nii)Misclassified Criterion : If class contains many\\ndata of other classes (in the following experiment,thisimpliesthatifthenumberofmisclassifieddatain class\\nis more than one), then class will be\\nsplit into two clusters.\\nIf class satisfies one of the above conditions, class\\nwill be split into two clusters in which the centers are\\ncalculated based on their corresponding sample patterns\\naccording to (11).\\n6) Repeat(2)–(5),untilallthetrainingsamplepatternsmeet\\nthe above two criteria.\\nB. Estimation of Widths\\nEssentially, RBF neural networks overlap localized regions\\nformed by simple kernel functions to create complex decision\\n(a)\\n(b)\\n(c)\\nFig.5. Clustersanddistributionofsamplepatterns:(a) /100 /43 /100 /20 /100 /40 /107/59 /108 /41.\\n(b) /100 /43 /100 /62/100 /40 /107/59 /108 /41and /106 /100 /0 /100 /106 /60/100 /40 /107/59 /108 /41.(c) /100 /43 /100 /62/100 /40 /107/59 /108 /41\\nand /106 /100 /43 /100 /106/21 /100 /40 /107/59 /108 /41.\\nFig. 6. Splitting of one class into two clusters.\\nregions while the amount of overlapping is controlled by the\\nwidths of RBF units [22], [24]. If no overlapping occurs, the\\nsystemwill not givemeaningful outputsforinputs betweentheinputs for which the system is designed, i.e., the RBF units donot generalize well. However, if the widths are too large, the\\ninteraction of different classes will be great and the output be-\\nlonging to the class will not be so significant [25], while theoutput of other classes may be large so that it will lead to mis-classification greatly. Hence, our goal is to select the widths in\\nsuch a way that they would minimize overlapping of nearest\\nneighbors of different classes to preserve local properties, aswellasmaximizethegeneralizationabilityofthenetwork[25].Here, we present a general approach to select the widths of an\\nRBF neural classifier according to two criteria.\\n1)Majority Criterion : The majority criterion can be de-\\nscribed as follows: In any class, each datum should havemore than 50% confidence level for the class it belongs\\nto. The detailed calculations are presented as follows:\\nFirst,\\n, the distance from the mean to the furthest\\npointbelongingtoclass ,iscalculatedaccordingto(11)\\nand (12).\\n702 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nNext, define the width of class considering the\\nconfidence level as\\n(15)\\nwhereiscalledtheconfidencecoefficient,whichliesin\\nthe range .\\n2)Overlapping Criterion : The overlappingcriterion can be\\ndescribed as follows: For any class , the choice of\\nconsideringtheoverlappingofthenearestclass isdeter-\\nmined by\\n(16)\\nwhereisanoverlappingfactorthatcontrolstheoverlap\\nof different classes, is the minimum distance\\nbetweenthecenterofclass andcentersofotherclasses.\\nThen, the width of class is finally determined by\\n(17)\\nThekeyideaofthisapproachistoconsidernotonlythe\\nintra-data distribution butalso theinter-data variations.\\nIn order to efficiently determine the width , the pa-\\nrameter could beapproximately estimated as follows:\\n(18)\\nThe choice of is determined by the distribution\\nofsamplepatterns.Ifthedataarescattered\\nlargely, but the centers are close, a small should be\\nselected as demonstrated in Table IV. Normally, lies\\nin the range . The best values of and\\nare selected when the best performance is achieved fortraining patterns.\\nV. H\\nYBRIDLEARNING ALGORITHM\\nTheadjustmentofRBFunitparametersisanonlinearprocess\\nwhiletheidentificationofweight isalinearone.Though\\nwe can apply the gradient paradigm to find the entire set ofoptimal parameters, the paradigm is generally slow and likelyto become trapped in local minima. Here, a hybrid learning al-\\ngorithm, which combines the gradient paradigm and the linear\\nleast square (LLS) paradigm to adjust the parameters, is pre-sented.\\nA. Weight Adjustment\\nLet\\nandbe the number of inputs and outputs respec-\\ntively, and suppose that RBF units are generated according\\nto the above clustering algorithm for all training patterns. For\\nany input , theth output of the system is\\n(19)\\nor\\n(20)Given and , where\\nis the total number of sample patterns, is the target matrix\\nconsisting of “1’s” and “0’s” with exactly one per column that\\nidentifiestheprocessingunittowhichagivenexemplarbelongs,\\nfind an optimal coefficient matrix such that the\\nerror energy is minimized. This\\nproblem can be solved by the LLS method [15]\\n(21)\\nwhere isthetransposeof ,and isthe\\npseudoinverse of .\\nB. Modification of Parameters of RBF Units\\nHere, the parameters (centers and widths) of the prototypes\\nareadjustedbytakingthenegativegradientoftheerrorfunction\\n(22)\\nwhere andrepresent the th real output and the target\\noutput at the th pattern, respectively. The error rate for each\\noutput can be calculated readily from (22)\\n(23)\\nFor the internal nodes (center and width ), the error rate\\ncan be derived by the chain rule as follows [15]:\\n(24)\\n(25)\\nwhere isthecentralerrorrateofthe thinputvariable\\nofthethprototypeatthe thtrainingpattern, isthewidth\\nerror rate of the th prototype at the th pattern, is the\\nthinputvariableatthe thtrainingpatternand isthelearning\\nrate.\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 703\\nC. Learning Procedure\\nIn theforward pass, wesupplyinputdataandfunctional sig-\\nnals to calculate the output of theth RBF unit. Then, the\\nweight is modified according to (21). After identifying the\\nweight, the functional signals continue going forward till the\\nerror measure is calculated. In the backward pass, the errors\\npropagatefromtheoutputendtowardtheinputend.Keepingtheweightfixed,thecentersandwidthsofRBFnodesaremodifiedaccordingto(24)and(25).Thelearningprocedureisillustrated\\nin Table I.\\nRemarks:\\n1) IfwefixtheparametersoftheRBFunits,theweightsfound\\nby the LLS are guaranteed to be global optimum. Accord-\\ningly, the dimension of the search space is drastically re-duced in the gradient paradigm so that this hybrid learning\\nalgorithm converges much faster than the gradient descent\\nparadigm.\\n2) It is well known that the learning rate\\nis sensitive to the\\nlearning procedure. If is small, the BP algorithm will\\nclosely approximate the gradient path, but the convergence\\nspeed will be slow since the gradient must be calculatedmany times. On the other hand, if\\nis large, convergence\\nspeed will be very fast initially, but the algorithm will\\noscillate around the optimum value. Here, we propose an\\napproach sothat will bereduced gradually. Wecompute\\n(26)\\nwhere ,are maximum and minimum learning\\nrates, respectively, is the number of epochs, and is a\\ndescent coefficient which lies in the range .\\n3) As the widths are sensitive to the generalization of an RBF\\nneural classifier, a larger learning rate is adopted for width\\nadjustment than for center modification (twice as that for\\ncenter modification).\\n4) Asthesystemwithhighdimensionisliabletoovertraining,\\nthe early stop strategy in [24] is adopted.\\nVI. EXPERIMENTAL RESULTS\\nA. ORL Database\\nOur experiments were performed on the face database\\nwhich contains a set of face images taken between April 1992\\nand April 1994 at the Olivetti Research Laboratory (ORL)in Cambridge University, U.K. There are 400 images of 40individuals. For some subjects, the images were taken at\\ndifferenttimes,whichcontainquiteahighdegreeofvariability\\nin lighting, facial expression (open/closed eyes, smiling/non-smiling etc), pose (upright, frontal position etc), and facialdetails(glasses/noglasses).Alltheimagesweretakenagainsta\\ndark homogeneous background with the subjects in an upright,\\nfrontal position, with tolerance for some tilting and rotation ofup to 20\\n. The variation in scale is up to about 10%. All the\\nimages in the database are shown in Fig. 7.1\\nInthefollowingexperiments,atotalof200imageswereran-\\ndomly selected as the training set and another 200 images asthe testing set, in which each person has five images. Next, the\\n1The ORL database is available from http//www.cam-orl.co.uk/face-\\ndatabase.html.TABLE I\\nTWOPASSES IN THE HYBRIDLEARNING PROCEDURE\\nFig. 7. The ORL face database.\\ntrainingandtestingpatternswereexchangedandtheexperiment\\nwas repeated one more time. Such procedure was carried outseveral times.\\nB. Clustering Error Before Learning\\nThestructureofRBFneuralnetworksandparametersofpro-\\ntotypes are obtained according to the algorithm shown in Sec-tionIV.Inordertotesthowtheclusteringalgorithmworks,the\\ndata distributions on six simulations are illustrated in Table II\\nandthe misclusteringnumber based on differentdimensions offeature patterns in six runs of simulations before learning arelisted in Table III. We see from Table II that there are a total of\\nfiveclasseswhichareinwell-separateddistributionasdepicted\\ninFig.5(a)and235classesaredistributedasshowninFig.5(b)whenthedimensionofthefeaturevectorsis39.Theseparationof data becomes better and better as the dimension decreases.\\nCorrespondingly,theclusteringperformance isbetterwhenthe\\nnumberoffeaturevectorsreduces,asshowninTableIII.How-ever, as we will see later, it does not imply that the recognitionperformance will improve along with reduction in the dimen-\\nsion. We also find from Table III that the maximum misclus-\\n704 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nTABLE II\\nDATADISTRIBUTION BASED ON THE PROPOSED APPROACH BEFORELEARNING\\n(THERESULTSAREOBTAINEDBASED ON SIXSIMULATIONS )\\ntering number within a class for each simulation is one except\\nforone casewhich occurredwhenthe dimension is39. Wecanconclude that the clustering paradigm presented is good for allcases.\\nC. Error of Classification After Learning\\nAfterthestructureofRBFneuralnetworksandparametersof\\nprototypesareselected,thehybridlearningalgorithmpresentedin Section V is employed to train the network. One run of therecognition results is shown in Table IV.\\nFrom Table IV, we can see that:\\n1) If the information is sufficient (feature dimension is\\nlarger than 20), the results are stable in each case for\\ndifferent choice of initial parameters\\nandin terms\\nof the number of misclassifications. Otherwise, the errorrate will increase drastically.\\n2) On the other hand, it does not mean that more informa-\\ntion(dimensionislargerthan30)willresultinhigherper-\\nformance. The reason may be that high dimension willlead to complexity in structure and increase difficultyin learning. Moreover, the addition of some unimportant\\ninformation may become noise and degrade the perfor-\\nmance.Thebestresultsareachievedwhenthedimensionis 25–30.\\n3) Along with the increase in the feature dimension, the\\ntraining patterns have more overlapping, and a small\\nshould be selected.2\\nThe total results based on six simulations are summarized in\\nTables V and VI.\\nD. Comparisons With Other Approaches\\nRecently, a number of researchers use the ORL database to\\nverify their algorithms [11], [12], [29], [37]–[45]. Here, weadopt the same definition of average error rate,\\nused in\\n[11], [37], which is given by\\n(27)\\n2It should be noted that /12lies in the range /48 /58 /53 /20 /12/60 /49. Even the RMSE\\nis smaller in the case of /12/60 /48 /58 /53when the dimension is 39, the generalization\\nwill be very bad.TABLE III\\nCLUSTERING ERRORS FOR TRAININGPATTERNS BEFORELEARNING (THE\\nRESULTIS THESUM OFSIXSIMULATIONS )\\nTABLE IV\\nSPECIFIED PARAMETERS AND CLASSIFIED PERFORMANCE\\n* RMSE—Root Mean Squared Error\\n** NOM—Number of Misclassifications\\nTABLE V\\nCLASSIFIED PERFORMANCES ON SIXSIMULATIONS\\nTABLE VI\\nBESTPERFORMANCES ON 6SIMULATIONS (DIMENSION /114 /61/50 /53OR30)\\nwhereis the number of experimental runs, each one being\\nperformedonrandompartitioningofthedatabaseintotwosets,\\nisthenumberofmisclassificationsforthe thrun,and\\nis the number of total testing patterns of each run. Using the\\ncriterion of , comparisons with CNN [11], NFL [37] and\\nM-PCA [38] performed on the same ORL database are shown\\nin Table VII.\\nHere, the best value of for the CNN is based on three\\nruns of experiments, and the SOM size is 8 and 9. For NFL,\\nthe best error rate is obtained when the number of feature vec-\\ntors is 40, and the average error rate is evaluated based on fourrunsofexperiments.ForM-PCA,itwasreportedthattheoverall\\nperformance is the average of ten runs of experiments. For our\\nproposedparadigm,thebesterrorrateisbasedonsixruns,andthe feature dimension is 25 and 30, respectively. The face fea-tures are the same as [37], and the way to partition the training\\nset and query set isthe same as the methods in [11]and [37].\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 705\\nTABLE VII\\nERRORRATES OFDIFFERENT APPROACHES\\nSome other results recently performed on the ORL database\\narelistedinTableVIIIasreferences(theseresultsaretabulated\\nseparately from Table VII because we are not aware of how\\ntheirexperimentsareexactlyperformed).Itshouldbenotedthatsomeapproachesuseddifferentnumberoftrainingdata(forex-ample,onlyonetrainingpatternperpersonisusedin[39],[40],\\nand eight patterns per person in [29]); some results were eval-\\nuated based on the best performance of one run, such as [41],[42]; some experiments were performed based on part of thedatabase [40]. It is not clear how the experiments were carried\\noutandhowtheperformanceswereevaluatedin[12],[43]–[45].\\nItisnotfairtocomparetheperformancesunderdifferentexper-imental conditions.\\nVII. D\\nISCUSSION\\nInthispaper,ageneralandefficientapproachfordesigningan\\nRBF neural classifier to cope with high-dimensional problemsinfacerecognitionispresented.Forthetimebeing,manyalgo-rithms have been proposed to configure RBF neural networks\\nforvariousapplicationsincludingfacerecognition,asshownin\\n[19]–[29]. Here, we would like to provide more insights intothesealgorithmsandcomparetheirperformanceswithourpro-posed method.\\nA. Face Features, Classifiers, and Performances\\nHere, the face features are first extracted by the PCA par-\\nadigm so that the resulting data are compressed significantly.\\nThen, the information is further condensed via the FLD ap-proach. Corresponding to Tables II and III in which the pat-terns are obtained from the PCA\\nFLD, the data distribution\\nresultingfromthePCAandtheclusteringerrorsfortrainingpat-\\nternsbasedonourproposedapproacharetabulatedinTablesIXand X. Comparing Tables II and III with Tables IX and X, wehavethefollowingobservations:1)Classoverlappinggradually\\nreduces along with decrease in the number of feature vectors\\nfor the data resulting from both the PCA and the PCA\\nFLD\\nmethods; 2) For the data from the PCA, the clustering errorsincrease along with decrease in the feature dimension, but the\\nclusteringerrorsdecreaseforthedatafromthePCA\\nFLD;and\\n3) The data from the PCA FLD are still overlapping without\\ncompleteseparationunlessthefeaturedimensionislessthan20.However,theFLDindeedalleviatestheclassoverlappingasev-\\nidenced in comparing Tables IX and II.\\nDifferent face features are then used for testing by different\\nclassifiers. Figs. 8 and 9 illustrate the effect of data dimensionresultedfromthePCAandthePCA\\nFLDmethodsonperfor-\\nmance classified by the nearest neighbor method. We see thatTABLE VIII\\nOTHERRESULTSRECENTLY PERFORMED ON THE ORL DATABASE\\nTABLE IX\\nDATADISTRIBUTION RESULTED FROM THE PCA BASED ON THE PROPOSED\\nAPPROACH (THERESULTSAREOBTAINEDBASED ONSIXSIMULATIONS )\\nTABLE X\\nCLUSTERING ERRORS FOR TRAININGPATTERNS RESULTED FROM THE PCA\\n(THERESULTIS THESUM OFSIXSIMULATIONS )\\nmore information (more dimensions) result in higher perfor-\\nmanceinthePCA.However,theperformanceresultingfromthe\\nPCAFLDisnotmonotonicallyimprovedalongwithincrease\\ninthefeaturedimension,andthebestperformanceisalittlede-crease in PCA\\nFLD because of information loss.3. Table XI\\nillustratestheperformancesbyusingdifferentfacefeaturesand\\nclassifiers.\\nAs foreshadowed earlier, the FLD is a linear transformation\\nand the data resulting from this criterion are still heavily over-\\nlapping.Itisalsoindicatedin[34]thatthisseparabilitycriterion\\n3This is also indicated in several papers, for example, [35] and [36]\\n706 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nFig.8. ErrorrateasafunctionofpatterndimensioninPCA(thisistheaverage\\nresult of two runs).\\nFig. 9. Error rate as a function of pattern dimension in PCA /43FLD (this is\\nthe average result of two runs).\\nisnotdirectlyrelatedtotheclassificationaccuracyintheoutput\\nspace. Accordingly, nonlinear discriminant analysis is neces-sary for classification among which neural networks are one ofthe popular approaches [15].\\nThe advantage of neural classifiers over linear classifiers is\\nthattheycanreducemisclassificationsamongtheneighborhoodclasses as shown in Fig. 10. However, this ability will gradu-ally decrease along with increase in the feature dimension. We\\ncan seefromTableXI thatthe performancegainedbythe PCA\\nFLD is better than that obtained from the PCA. This is be-\\ncause the FLD can alleviate data overlapping, and reductionin the number of feature dimension moderates the architecture\\ncomplexity of the RBF neural classifier and reduces the com-\\nputational burden significantly in order to avoid overtrainingand overfitting. However, for those data falling into nonnearestclassesasshowninFig.10,theneuralclassifierstillcannotclas-\\nsify correctly.\\nIt has been reported in [11] that error rates of the multilayer\\nnetworks (MLNs) classifier are 41.2% and 39.6% respectivelyontheORLdatabasewhenthefeaturesareextractedbythePCA\\nandtheSOMrespectively.Ourproposedapproach,whichisdif-\\nferent from the MLN, wherein a particular supervised learningparadigm is employed, is a tremendous improvement over theresultsofMLN,CNN[11]andtheRBFmethodshownin[29].TABLE XI\\nPERFORMANCE COMPARISONS OF VARIOUSFACEFEATURES AND CLASSIFIERS\\nFig. 10. An RBF neural classifier versus a linear classifier.\\nB. Training Samples versus Performances\\nDue to the fact that there are very small sample patterns for\\neach face in the ORL database, and further, as mentioned inSectionIthatsimilaritiesbetweenthedifferentfaceimageswiththe same pose are almost always larger than those between the\\nsamefaceimagewithdifferentposes,thechoiceoftrainingdata\\nis consequently very crucial for generalization of RBF neuralnetworks. If thetraining dataare representativeof face images,thegeneralizationofRBFneuralclassifierimpliestointerpolate\\nthetesting data.Otherwise, it meansto predict thetesting data.\\nFrom the viewpoint of images, it is shown that the proposed\\napproach is not as sensitive to illumination (see Fig. 11), asother paradigms do [2], [6]. Usually, the proposed method also\\ndiscounts the variations of facial pose, expression and scale\\nwhen such variations are presented in the training patterns. Ifthe training patterns are not representative of image variationswhich appear during the testing phase, say, upward, then the\\nface turning up in the testing phase will not be recognized\\ncorrectly, as shown in Fig. 11. According to this principle,another database consisting of 600 face images of 30 individ-uals, which comprise different poses (frontal shots, upward,\\ndownward, up-right, down-left and so on), and high degree\\nof variability in facial expression, has been set up by us. Allthe images were taken under the same background with theresolution of 160\\n120 pixels. A total of 300 face images, in\\nwhich each person has ten images, were selected to represent\\ndifferentposesandexpressionsasthetrainingset.Another300images were used as the testing set. Our results demonstratedthat the success rate of recognition is 100%.\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 707\\n(a)\\n(b)\\nFig.11. Anexampleofincorrectclassification:(a)trainingimagesand(b)test\\nimages.\\nC. Initialization versus Performances\\n1) Selection of Gaussian Centers : Several paradigms have\\nbeen proposed for kernel location estimation. The simplest ap-proachistoselectfromthetrainingdataasshownin[20],[23],[25],[28].Othertypicaltechniquescanbefoundbyusingclus-\\nteringalgorithms[19],[21]ormedianoperation[26].Ifwealso\\nselectthesamesixgroupsoffacedataresultingfromthePCA\\nFLDwithafeaturedimensionof40,theinitialclusteringerrorsfor training data by other clustering algorithms are tabulated in\\nTable XII.\\nWecanseefromTableXIIthatmanydataaremisclassifiedby\\ntheunsupervised\\n-meansclusteringmethodandtheregression\\nclusteringmethod[23].Italsoimpliesthatthesedataaresignifi-\\ncantlyoverlapped.However,theclusteringerrorwillberemark-\\nablyreducedifthecategoryinformationaboutpatternsisused,forexample,theMRBFparadigm[26]showninTableXII,andour proposed method achieves the best clustering performance\\nas shown in Table III.\\n2) Determination of Gaussian Widths: The appropriate es-\\ntimationofwidthsofGaussianunitsisverysignificantforgen-eralization [20], [22], [24]–[29]. Frequently, the widths are se-\\nlectedbyheuristics[19],[21],[23],[29].Alsomanyresearchers\\nchoosethewidthsasthecommonvariance(CV)(i.e.,calculatedoverallsamplepatterns)[20],[33]ortheclass-relatedvariance\\n(CRV)(i.e.,calculatedoverallthepatternsbelongingtothecor-\\nrespondingclass)[24],[27].Recently,somenewmethodshavebeen proposed to estimate the widths, for example, the samplecovariance (i.e., the class-related variance) plus common co-\\nvariance (SCCC) [32], the minimum distance between cluster\\ncenters (i.e., using\\n) [25], the median operation (MO) [26],\\nor the evolutionary optimization [28]. If we use the same sixgroups of data resulting from the PCA\\nFLD, where the cen-\\ntersaredeterminedbyourproposedclusteringmethod,andthe\\nclusternumberisstill40,theinitialclusteringerrorsindifferentwidthschosenbydifferentmethodsaretabulatedinTableXIII.TableXIVillustratesthegeneralizationperformancefortesting\\npatterns performed on the ORL database.\\nItisshownfromTableXIIIthattheSCCCmethodisthebest\\nmethodtodescribethetrainingpatterns.However,amethodfor\\ngood description of training patterns does not imply that it has\\ngood generalization, as we see from Table XIV. On the otherhand, the testing errors before learning for the MO and CRVapproaches are very high (the total NOM’s are 95 and 117, re-TABLE XII\\nCLUSTERING ERRORS FOR TRAININGPATTERNS BY OTHERCLUSTERING\\nALGORITHMS (THERESULTIS THESUM OFSIXSIMULATIONS )\\nTABLE XIII\\nCLUSTERING ERRORS FOR TRAININGPATTERNS CONSIDERING WIDTHS\\nCHOSEN BY DIFFERENT METHODS(THERESULTIS THESUM OFSIX\\nSIMULATIONS )\\n* The widths are best chosen for each case\\nTABLE XIV\\nGENERALIZATION ERRORS FOR TESTINGDATA BYDIFFERENT INITIALWIDTHS\\n(THERESULTIS THESUM OFSIXSIMULATIONS )\\n* Theresults areobtained whenthefeature dimension is30\\nspectively).Buttheirfinalperformancesafterlearningarecom-\\nparabletootherparadigms(CV,SCCCand ).Theoretically,\\nthe final results should be the same regardless of initial param-\\netersif thelearningalgorithmisgoodenoughforoptimization.The discrepancies are mainly caused by overfitting and over-training due to small sets of patterns with high dimension.\\nTwo unsupervised algorithms with growing structure, i.e.,\\nORBF [23] and D-FNN [25] have been employed to test theORLdatabase.TablesXVandXVIillustratethegeneralizationresults for the first group data with different clusters. We see\\nthat the loss of category information will be at the cost of\\nmore clusters for the comparable performance. However, itshouldbenotedthattheincreaseofclustersislikelytoresultinoverfitting, as shown in Table XVI.\\n708 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nTABLE XV\\nGENERALIZATION ERRORS FOR TESTINGDATA BY THE ORBF M ETHOD\\nTABLE XVI\\nGENERALIZATION ERRORS FOR TESTINGDATA BY THE D-FNN M ETHOD\\nD. The Problem of Small Sample Sets\\nOne of the difficulties for neural networks to train in face\\nrecognition is the small sample data. This severe limitation al-\\nways results in poor generalization. An alternative paradigm to\\nimprove generalization is to adopt the technique of regulariza-tion [15], [33], i.e., to encourage smoother network mappingsby adding a penalty term\\nto the error function. But, the ap-\\npropriatechoiceofregularizationparameteristimeconsuming.\\nAnother direct method to improve generalization is to use\\nmore data patterns, i.e., by adding some patterns with noise[46].Accordingtotheprocedureproposedin[46],anothersetof\\ntraining patterns with noise randomly chosen from the uniform\\ndistribution is replenished. The learning algorithm is executedwith and without adding noise to the inputs. Our experimentsshow that if the variance of noise is small, there is no effect ongeneralization, whereas large variance of noise will deteriorate\\nthe performance.\\nHigh dimension may be one of the reasons that lead to poor\\ngeneralization.Asthevaluesineachdimensionvarygreatlyanddifferentfeatureshavedifferentinfluencesforfacerecognition,\\nauniformlydistributednoisemayaffectsomefeaturessubstan-\\ntiallyandhasnoinfluenceonsomeotherfeatures.Ontheotherhand,differentfeaturesmayhavedifferentweightsfordifferentface features. Therefore, normalization of the inputs should be\\ntaken when the noise is injected into the inputs.\\nAnotherreasonmaybeduetothepresuppositionthattheac-\\nquisitionofgeneralizationcapabilitybynoiseinjectionintotheinputsreliesontheassumptionthatthemappingfromtheinput\\nspace to the output space should be smooth [46]. For high-di-\\nmensional classifications, it is not easy to determine whetherthe assumption could be satisfied in advance.\\nVIII. C\\nONCLUSION\\nIt is well known that if the dimension of the network input\\nis comparable to the size of the training set, which is the usual\\ncaseinfacerecognition,thesystemwilleasilybringaboutover-\\nfitting and result in poor generalization. In this paper, a gen-eral design approach using an RBF neural classifier for facerecognitiontocopewithsmalltrainingsetsofhigh-dimensional\\nproblem is presented. Firstly, face features are first extracted\\nby the PCA. Then, the resulting features are further projectedinto the Fisher’s optimal subspace in which the ratio of the be-tween-classscatterandthewithin-classscatterismaximized.Anovelparadigm,wherebytrainingdatainformationisusedinthechoice of structure and parameters of RBF neural networks be-\\nforelearningtakesplace,ispresented.Finally,ahybridlearningalgorithm is proposed to train the RBF neural networks. Simu-\\nlation results show that the system achieves excellent perfor-\\nmancebothintermsoferrorratesofclassificationandlearningefficiency.\\nIn this paper, the feature vectors are only extracted from\\ngrayscale information. More features extracted from both\\ngrayscale and spatial texture information and a real-time\\nface detection and recognition system are currently underdevelopment.\\nA\\nCKNOWLEDGMENT\\nTheauthorswishtogratefullyacknowledgeDr.S.Lawrence\\nand Dr. C. L. Giles for providing details of their experimentson their CNN approach and Dr. A. Bors for his discussionsand comments. Also, the authors would like to express sincere\\nthankstotheanonymousreviewersfortheirvaluablecomments\\nthat greatly improved the quality of the paper.\\nR\\nEFERENCES\\n[1] R.Chellappa,C.L.Wilson,andS.Sirohey,“Humanandmachinerecog-\\nnition of faces: A survey,” Proc. IEEE ,vol. 83, pp. 705–740, 1995.\\n[2] Y. Moses, Y. Adini, and S. Ullman, “Face recognition: The problem of\\ncompensating for changes in illumination direction,” in Proc. EuroP.\\nConf. Comput. Vision , vol. A, 1994, pp. 286–296.\\n[3] R. Brunelli and T. Poggio, “Face recognition: Features versus tem-\\nplates,”IEEE Trans. Pattern Anal. Machine Intell. , vol. 15, pp.\\n1042–1053, 1993.\\n[4] P. J. Phillips, “Matching pursuit filters applied to face identification,”\\nIEEE Trans. ImageProcessing , vol. 7, pp. 1150–1164, 1998.\\n[5] Z. Hong, “Algebraic feature extraction of image for recognition,” Pat-\\ntern Recognition , vol. 24, pp. 211–219, 1991.\\n[6] M.A.TurkandA.P.Pentland,“Eigenfacesforrecognition,” J.Cognitive\\nNeurosci. , vol. 3, pp. 71–86, 1991.\\n[7] P.N.Belhumeur,J.P.Hespanha,andD.J.Kriegman,“Eigenfacesversus\\nfisherfaces: Recognition using class specific linear projection,” IEEE\\nTrans. Pattern Anal. Machine Intell. , vol. 19, pp. 711–720, 1997.\\n[8] D. L. Swets and J. Weng, “Using discriminant eigenfeatures for image\\nretrieval,” IEEE Trans. Pattern Anal. Machine Intell. , vol. 18, pp.\\n831–836, 1996.\\n[9] D.Valentin, H.Abdi,A.J.O’Toole, andG. W.Cottrell,“Connectionist\\nmodelsoffaceprocessing:ASurvey,” PatternRecognition ,vol.27,pp.\\n1209–1230, 1994.\\n[10] J.MaoandA.K.Jain,“Artificialneuralnetworksforfeatureextraction\\nandmultivariatedataprojection,” IEEETrans.NeuralNetworks ,vol.6,\\npp. 296–317, Mar. 1995.\\n[11] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, “Face recogni-\\ntion: A convolutional neural-network approach,” IEEE Trans. Neural\\nNetworks , vol. 8, pp. 98–113, Jan. 1997.\\n[12] S.-H. Lin, S.-Y. Kung, and L.-J. Lin, “Face recognition/detection by\\nprobabilistic decision-based neural network,” IEEE Trans. Neural Net-\\nworks, vol. 8, pp. 114–132, Jan. 1997.\\n[13] K. Fukunaga, Introduction to Statistical Pattern Recognition , 2nd\\ned. San Diego, CA: Academic Press, 1990.\\n[14] H. H. Song and S. W. Lee, “A self-organizing neural tree for large-set\\npattern classification,” IEEE Trans. Neural Networks , vol. 9, pp.\\n369–380, Mar. 1998.\\n[15] C.M.Bishop, NeuralNetworksforPatternRecognition ,NewYork:Ox-\\nford Univ. Press.\\n[16] J.L.YuanandT.L.Fine,“Neural-Networkdesignforsmalltrainingsets\\nofhighdimension,” IEEETrans.NeuralNetworks ,vol.9,pp.266–280,\\nJan. 1998.\\n[17] J. Park and J. Wsandberg, “Universal approximation using radial basis\\nfunctions network,” Neural Comput. , vol. 3, pp. 246–257, 1991.\\n[18] F. Girosi and T. Poggio, “Networks and the best approximation prop-\\nerty,”Biol. Cybern. , vol. 63, pp. 169–176, 1990.\\nERet al.: FACE RECOGNITION WITH RBF NEURAL NETWORKS 709\\n[19] J. Moody and C. J. Darken, “Fast learning in network of locally-tuned\\nprocessing units,” Neural Comput. , vol. 1, pp. 281–294, 1989.\\n[20] S. Lee and R. M. Kil, “A Gaussian potential function network with\\nhierarchically self-organizing learning,” Neural Networks , vol. 4, pp.\\n207–224, 1991.\\n[21] W. Pedrycz, “Conditional fuzzy clustering in the design of radial basis\\nfunction neural networks,” IEEE Trans. Neural Networks , vol. 9, pp.\\n601–612, July 1998.\\n[22] M.T.Musavi,W.Ahmed,K.H.Chan,K.B.Faris,andD.M.Hummels,\\n“On the training of radial basis function classifiers,” Neural Networks ,\\nvol. 5, pp. 595–603, 1992.\\n[23] S. Chen, C. F. N. Cowan, and P. M. Grant, “Orthogonal least squares\\nlearning algorithm for radial basis function network,” IEEE Trans\\nNeural Networks , vol. 2, pp. 302–309, 1991.\\n[24] G.BorsandM.Gabbouj,“Minimaltopologyforaradialbasisfunctions\\nneural networks for pattern classification,” Digital Processing , vol. 4,\\npp. 173–188, 1994.\\n[25] S.WuandM.J.Er,“Dynamicfuzzyneuralnetworks:Anovelapproach\\nto function approximation,” IEEE Trans. Syst, Man, Cybern , pt. B: Cy-\\nbern, vol. 30, pp. 358–364, 2000.\\n[26] A. G. Bors and I. Pitas, “Median radial basis function neural network,”\\nIEEE Trans. Neural Networks , vol. 7, pp.1351–1364, Sept. 1996.\\n[27] N.B.KarayiannisandG.W.Mi,“Growingradialbasisneuralnetworks:\\nMerging supervised and unsupervised learning with network growth\\ntechniques,” IEEE Trans. Neural Networks , vol. 8, pp. 1492–1506,\\nNov. 1997.\\n[28] A. Esposito, M. Marinaro, D. Oricchio, and S. Scarpetta, “Approxima-\\ntion of continuous and discontinuous mappings by a growing neural\\nRBF-based algorithm,” Neural Networks ,vol. 12,pp. 651–665,2000.\\n[29] E.-D.Virginia,“Biometricidentificationsystemusingaradialbasisnet-\\nwork,”in Proc34thAnnu.IEEEInt.CarnahanConf.SecurityTechnol. ,\\n2000, pp. 47–51.\\n[30] S. J. Raudys and A. K. Jain, “Small sample size effects in statistical\\npattern recognition: Recommendations for practitioners,” IEEE Trans.\\nPattern Anal. Machine Intell. , vol. 13, pp. 252–264, 1991.\\n[31] L.O.JimenezandD.A.Landgrebe,“Supervisedclassificationinhigh-\\ndimensionalspace:Geometrical,statistical,andasymptoticalpropertiesof multivariate data,” IEEE Trans. Syst, Man, Cybern. C , vol. 28, pp.\\n39–54, 1998.\\n[32] J.P.HoffbeckandD.A.Landgrebe,“Covariancematrixestimationand\\nclassificationwithlimitedtrainingdata,” IEEETrans.PatternAnal.Ma-\\nchine Intell. , vol. 18, pp. 763–767, 1996.\\n[33] S.Haykin, Neural Networks,A Comprehensive Foundation ,New York:\\nMacmillan, 1994.\\n[34] R. Lotlikar and R. Kothari, “Fractional-step dimensionality reduction,”\\nIEEE Trans.Pattern Anal.Machine Intell. , vol.22,pp. 623–627,2000.\\n[35] G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski,\\n“Classifyingfacialactions,” IEEETrans.PatternAnal.MachineIntell. ,\\nvol. 21, pp. 974–989, 1999.\\n[36] C. Liu and H.Wechsler, “Learning the facespace—Representation and\\nrecognition,” in Proc. 15th Int. Conf. Pattern Recognition , Spanish,\\n2000, pp. 249–256.\\n[37] S. Z. Li and J. Lu, “Face recognition using the nearest feature line\\nmethod,” IEEE Trans. Neural Networks , vol. 10, pp. 439–443, Mar.\\n1999.\\n[38] V. Brennan and J. Principe, “Face classification using a multiresolution\\nprincipal component analysis,” in Proc. IEEE Workshop Neural Net-\\nworks Signal Processing , 1998, pp. 506–515.\\n[39] K.-M.LamandH.Yan,“Ananalytic-to-holisticapproachforfacerecog-\\nnition based on a single frontal view,” IEEE Trans. Pattern Anal. Ma-\\nchine Intell. , vol. 20, pp. 673–686, 1998.\\n[40] T.Phiasai,S.Arunrungrusmi,andK.Chamnongthai,“Facerecognition\\nsystem with PCA and moment invariant method,” in Proc. IEEE Int.\\nSymp. Circuits Syst. , 2001, pp. II165–II168.\\n[41] Z.Jin,J.-Y.Yang,Z.-S.Hu,andZ.Lou,“Facerecognitionbasedonthe\\nuncorrelateddiscriminanttransformation,” PatternRecognition ,vol.34,\\npp. 1405–1416, 2001.\\n[42] A.S.TolbaandA.N.Abu-Rezq,“Combinedclassifiersforinvariantface\\nrecognition,”in Proc.Int.Conf.Inform.Intell.Syst. ,1999,pp.350–359.\\n[43] S.Eickeler,S.Mueller,andG.Rigoll,“Highqualityfacerecognitionin\\nJPEGcompressedimages,”in Proc.IEEEInt. Conf.ImageProcessing ,\\n1999, pp. 672–676.\\n[44] T. Tan and H. Yan, “Object recognitionusingfractal neighbordistance:\\nEventual convergence and recognition rates,” in Proc. 15th Int. Conf.\\nPattern Recognition , 2000, pp. 781–784.[45] B.-L. Zhang and Y. Guo, “Face recognition by wavelet domain asso-\\nciative memory,” in Proc. Int. Symp. Intell. Multimedia, Video, Speech\\nProcessing , 2001, pp. 481–485.\\n[46] K. Matsuoka, “Noise injection into inputs in back-propagation\\nlearning,” IEEETrans.Syst.,Man,Cybern. ,vol.22,pp.436–440,1992.\\nMeng Joo Er (S’82–M’85) received the B.Eng. and\\nM.Eng. degrees in electrical engineering from the\\nNational University of Singapore, Singapore, and\\nthe Ph.D. degree in systems engineering from the\\nAustralian National University, Canberra, in 1985,1988, and 1992, respectively.\\nFrom1987to1989,hewasasaResearchandDe-\\nvelopment Engineer in Chartered Electronics Indus-\\ntries Pte Ltd, Singapore, and a Software Engineer in\\nTelerate Research and Development Pte Ltd, Singa-\\npore,respectively.HeiscurrentlyanAssociateProfessorintheSchoolofElec-\\ntrical and Electronic Engineering (EEE), Nanyang Technological University(NTU).FromFebruarytoApril1998,hewasinvitedasaGuestResearcheratthe\\nPrecisionInstrumentDevelopmentCenterofNationalScienceCouncil,Taiwan.\\nHe served as a consultant to Growth Venture Pte Ltd., Singapore, from Feb-\\nruary1999toMay,2000.HewasinvitedasapanelisttotheIEEE-INNS-ENNS\\nInternational Conference on Neural Networks held from 24 to 27 July, 2000,Como, Italy. He has been Editor of IES Journal on Electronics and Computer\\nEngineering since 1995. He is currently a member of the editorial board of the\\nInternationalJournalof ComputerResearch and theguest editorof thespecial\\nissue on intelligent control of mobile robots. He has authored numerous pub-\\nlished works in his research areas of interest, which include control theory and\\napplications, robotics and automation, fuzzy logic and neural networks, artifi-\\ncialintelligence,biomedicalengineering,parallelcomputing,powerelectronicsand drives and digital signal processors applications.\\nDr.ErwastherecipientoftheInstitutionofEngineers,Singapore(IES)Pres-\\ntigiousPublication(Application)Awardin1996andtheIESPrestigiousPubli-\\ncation (Theory) Awardin 2001. Hereceivedthe Teacher of the Year Awardfor\\nthe School of EEE for his excellent performance in teaching in 1999. He was\\nalso a recipient of a Commonwealth Fellowship tenable at the University of\\nStrathclyde, U.K., from February to October 2000. He is a member of IES. Hehas served in these professional organizations in various capacities, including\\nChairman of Continuing Education Subcommittee of IEEE Control Chapter in\\n1993, Honorary Secretary of the IEEE Singapore Section from 1994 to 1996,\\nVice-Chairman of IEEE Singapore Section and Chairman of Continuing Edu-\\ncationSubcommittee,IEEESingaporeSectionfromMarch,1998toDecember1999. From February 1997 to February, 1999, he also served as a member of\\nthe Singapore Science Centre Board. Currently, he is serving as the First Vice-\\nChair of the Action Group on Educational Institutions, Uniform Groups and\\nYouth of Publication Education Committee on Family. He has also been very\\nactive in organizing international conferences. He was a member of the main\\norganizing committees of International Conference on Control, Automation,\\nRobotics and Vision (ICARCV) for four consecutive years, 1994, 1996, 1998,and 2000,and theAsianConference on ComputerVision(ACCV) in1995.He\\nwastheCochairmanofTechnicalProgrammeCommitteeandperson-in-charge\\nof invited sessions for ICARCV’96 and ICARCV’98. He was in charged of\\nInternational Liaison for ICARCV’2000. He is currently serving as the Tech-\\nnical Program Chair for ICARCV’2002. He has also served as a member of\\nthe International Scientific Committee of the International Conference on Cir-\\ncuits, Systems, Communications and Computers (CSC) since 1998. He was amember of the International Scientific Committee of a unique three confer-\\nences series on Soft Computing consisting of 2001 World Scientific and En-\\ngineering Society (WSES) International Conference on Neural Networks and\\nApplications, 2001 WSES International Conference on Fuzzy Sets and Fuzzy\\nSystems,and2001WSESInternationalConferenceonEvolutionaryComputa-tion.HehasalsobeeninvitedtoserveasGeneralChairofthe2001WSESCon-\\nference on Robotics, Distance Learning and Intelligent Communications Sys-\\ntems (RODLICS), International Conference on Speech, Signal and Image Pro-\\ncessing 2001 (SSIP’2001), International Conference on Multimedia, Internet,\\nVideo Technologies (MIV’2001) and International Conference on Simulation\\n(SIM’2001)organizedbyWSES.HehasbeenlistedinWho’sWhointheWorld\\nsince1998andwasnominatedManoftheYear1999bytheAmericanBiograph-ical Institute Board of International Research.\\n710 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 13, NO. 3, MAY 2002\\nShiqian Wu (M’02) received the B.S. and M.Eng.\\ndegrees, both from Huazhong University of Science\\nand Technology, Wuhan, China, in 1985 and 1988,\\nrespectively. He received the Ph.D. degree in elec-\\ntricalandelectronicengineeringfromNanyangTech-\\nnological University, Singapore, Singapore, in June2001.\\nFrom 1988 to 1997, he was a Lecturer and then\\nAssociate Professor in Huazhong University of\\nScience and Technology. Since August 2000, he\\nhas been with the Centre for Signal Processing,\\nSingapore. Currently, his research interests include neural networks, fuzzy\\nsystems, computer vision, face detection and recognition, and infrared image\\nanalysis. He has published more than 20 papers.\\nJuwei Lu (M’99–S’01) received the Bachelor of Electronic Engineering de-\\ngreefromNanjingUniversityofAeronauticsandAstronautics,China,in1994,\\nand the Master of Engineering degree from the School of Electrical and Elec-\\ntronic Engineering, Nanyang Technological University, Singapore, Singapore,\\nin 1999.Currently, he is pursuing the Ph.D. degree in the Department of Elec-\\ntricalandComputerEngineering,UniversityofToronto,Toronto,ON,Canada.\\nFrom July 1999 to January 2000, he was with the Centre for Signal Pro-\\ncessing, Singapore, Singapore, as a Research Engineer.\\nHockLyeToh (S’85–M’88)receivedtheB.Eng.de-\\ngreefromtheSchoolofElectricalandElectronicEn-\\ngineering,NanyangTechnologicalUniversity,Singa-\\npore, Singapore, in 1988.\\nHejoinedtheSignalProcessingLaboratoryofDe-\\nfence Science Organization in 1988. Prior to joiningCentre for Signal Processing as a Senior Research\\nEngineer in 1997, he was a Senior Engineer holding\\nGroupHeadappointment.From1988to1997,hehas\\nworkedonalgorithmdevelopmentprojectsonimage\\nenhancement and restoration, design, and develop-\\nment of real-time computation engine using high-performance digital signal\\nprocessors,andprogrammablelogicdevices.HewasProjectLeaderforthede-\\nvelopment of a few real-time embedded systems for defense applications. He\\nwasGroupHead,ImagingElectronicsGroupfrom1993andGroupHead,ImageProcessing Group from 1995. He was appointed Program Manager, Centre for\\nSignal Processing from 1998. He is spearheading a few research and develop-\\nment projects on embedded video processing and human thermogram analysis\\nfor multimedia, biometrics, and biomedical applications. He has led numerous\\nin-house and industry projects, filed a patent and submitted a few invention\\ndisclosures. His research interests are embedded processing core development,\\nthermogram video signal processing, and 3-D signal reconstruction.\\nMr. Toh is a member of the Institution of Engineers, Singapore.\\n',\n",
       " '98 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFace Recognition: A Convolutional\\nNeural-Network Approach\\nSteve Lawrence, Member, IEEE, C. Lee Giles, Senior Member, IEEE, Ah Chung Tsoi, Senior Member, IEEE,\\nand Andrew D. Back, Member, IEEE\\nAbstract— Faces represent complex multidimensional mean-\\ningful visual stimuli and developing a computational model forface recognition is difﬁcult. We present a hybrid neural-networksolution which compares favorably with other methods. Thesystem combines local image sampling, a self-organizing map\\n(SOM) neural network, and a convolutional neural network.\\nThe SOM provides a quantization of the image samples into atopological space where inputs that are nearby in the originalspace are also nearby in the output space, thereby providingdimensionality reduction and invariance to minor changes in theimage sample, and the convolutional neural network provides forpartialinvariance to translation,rotation, scale, anddeformation.The convolutional network extracts successively larger featuresin a hierarchical set of layers. We present results using theKarhunen–Lo `eve (KL) transform in place of the SOM, and\\na multilayer perceptron (MLP) in place of the convolutionalnetwork. The KL transform performs almost as well (5.3% errorversus 3.8%). The MLP performs very poorly (40% error versus3.8%). The method is capable of rapid classiﬁcation, requiresonly fast approximate normalization and preprocessing, andconsistently exhibits better classiﬁcation performance than theeigenfaces approach on the database considered as the numberof images per person in the training database is varied fromone to ﬁve. With ﬁve images per person the proposed methodand eigenfaces result in 3.8% and 10.5% error, respectively. Therecognizer provides a measure of conﬁdence in its output andclassiﬁcation error approaches zero when rejecting as few as 10%oftheexamples.Weuseadatabaseof400imagesof40individuals\\nwhich contains quite a high degree of variability in expression,\\npose,andfacialdetails.Weanalyzecomputationalcomplexityanddiscuss how new classes could be added to the trained recognizer.\\nIndex Terms— Facerecognition,convolutionalneuralnetworks,\\nself-organizing feature maps, Karhunen–Lo `eve transforms,\\nhybrid systems, access control, pattern recognition, imageclassiﬁcation.\\nI. INTRODUCTION\\nTHE REQUIREMENT for reliable personal identiﬁcation\\nin computerized access control has resulted in an in-\\ncreased interest in biometrics.1Biometrics being investigated\\nManuscript received January 1, 1996; revised June 13, 1996. This work\\nwas supported in part by the Australian Research Council (ACT) and the\\nAustralian Telecommunications and Electronics Research Board (SL).\\nS. Lawrence is with the NEC Research Institute, Princeton, NJ 08540 USA.\\nHe is also with the Department of Electrical and Computer Engineering,\\nUniversity of Queensland, St. Lucia, Australia.\\nC. L. Giles is with the NEC Research Institute, Princeton, NJ 08540 USA.\\nHe is also with the Institute for Advanced Computer Studies, University of\\nMaryland, College Park, MD 20742 USA.\\nA. C. Tsoi and A. D. Back are with the Department of Electrical and\\nComputer Engineering, University of Queensland, St. Lucia, Australia.\\nPublisher Item Identiﬁer S 1045-9227(97)00234-8.\\n1Physiological or behavioral characteristics which uniquely identify us.include ﬁngerprints [4], speech [7], signature dynamics [36],\\nand face recognition [8]. Sales of identity veriﬁcation products\\nexceed $100 million [29]. Face recognition has the beneﬁt ofbeing a passive, nonintrusive system for verifying personalidentity. The techniques used in the best face recognition\\nsystems may depend on the application of the system. We\\ncan identify at least two broad categories of face recognitionsystems.\\n1) We want to ﬁnd a person within a large database of\\nfaces(e.g.,inapolicedatabase).Thesesystemstypicallyreturn a list of the most likely people in the database[34]. Often only one image is available per person. It\\nis usually not necessary for recognition to be done in\\nreal-time.\\n2) We want to identify particular people in real-time (e.g.,\\nin a security monitoring system, location tracking sys-\\ntem, etc.), or we want to allow access to a group of\\npeople and deny access to all others (e.g., access to abuilding, computer, etc.) [8]. Multiple images per personare often available for training and real-time recognition\\nis required.\\nIn this paper, we are primarily interested in the second\\ncase.\\n2We are interested in recognition with varying facial\\ndetail, expression, pose, etc. We do not consider invariance to\\nhigh degrees of rotation or scaling—we assume that a minimalpreprocessing stage is available if required. We are interestedin rapid classiﬁcation and hence we do not assume that time is\\navailable for extensive preprocessing and normalization. Good\\nalgorithms for locating faces in images can be found in [37],[40], and [43].\\nThe remainder of this paper is organized as follows. The\\ndata we used is presented in Section II and related work\\nwith this and other databases is discussed in Section III.The components and details of our system are described inSections IV and V, respectively. We present and discuss our\\nresults in Sections VI and VII. Computational complexity is\\nconsidered in Section VIII, Section IX listsavenues forfurtherresearch, and we draw conclusions in Section X.\\nII. D\\nATA\\nWe have used the ORL database, which contains a set of\\nfaces taken between April 1992 and April 1994 at the Olivetti\\n2However,wehavenotperformedanyexperimentswherewehaverequired\\nthe system to reject people that are not in a select group (important, for\\nexample, when allowing access to a building).\\n1045–9227/97$10.00 \\uf8e91997 IEEE\\nLAWRENCE et al.: FACE RECOGNITION 99\\nFig. 1. The ORL face database. There are ten images each of the 40 subjects.\\nResearch Laboratoryin Cambridge, U.K.3There are ten differ-\\nentimagesof40distinctsubjects.Forsomeofthesubjects,theimages were taken at different times. There are variations infacial expression (open/closed eyes, smiling/nonsmiling), and\\nfacial details (glasses/no glasses). All the images were taken\\nagainst a dark homogeneous background with the subjects inan upright frontal position, with tolerance for some tilting androtation of up to about 20 degrees. There is some variation in\\nscale of up to about 10%. Thumbnails of all of the images are\\nshown in Fig. 1 and a larger set of images for one subject isshown in Fig. 2. The images are greyscale with a resolutionof 92\\n112.\\n3The ORL database is available free of charge, see http://www.cam-\\norl.co.uk/facedatabase.html.III. RELATEDWORK\\nA. Geometrical Features\\nManypeoplehaveexploredgeometricalfeaturebasedmeth-\\nods for face recognition. Kanade [17] presented an automaticfeature extraction method based on ratios of distances andreportedarecognitionrateofbetween45–75%withadatabase\\nof 20 people. Brunelli and Poggio [6] compute a set of\\ngeometrical features such as nose width and length, mouthposition, and chin shape. They report a 90% recognition rateon a database of 47 people. However, they show that a simple\\ntemplate matching scheme provides 100% recognition for the\\nsame database. Cox et al.[9] have recently introduced a\\nmixture-distance technique which achieves a recognition rate\\n100 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFig. 2. The set of ten images for one subject. Considerable variation can be seen.\\nof 95% using a query database of 95 images from a total\\nof 685 individuals. Each face is represented by 30 manually\\nextracted distances.\\nSystems which employ precisely measured distances be-\\ntweenfeaturesmaybemostusefulforﬁndingpossiblematches\\nin a large mugshot database.4For other applications, automatic\\nidentiﬁcation of these points would be required and the result-ing system would be dependent on the accuracy of the feature\\nlocation algorithm. Current algorithms for automatic location\\nof feature points do not provide a high degree of accuracy andrequire considerable computational capacity [41].\\nB. Eigenfaces\\nHigh-level recognition tasks are typically modeled with\\nmanystagesofprocessingasintheMarrparadigmofprogress-\\ning fromimagestosurfaces tothree-dimensional(3-D)models\\nto matched models [28]. However, Turk and Pentland [43]argue that it is likely that there is also a recognition processbased on low-level two-dimensional (2-D) image processing.\\nTheir argument is based on the early development and extreme\\nrapidity of face recognition in humans and on physiologicalexperiments in a monkey cortex which claim to have isolated\\nneurons that respond selectively to faces [35]. However, it is\\nnot clear that these experiments exclude the sole operation ofthe Marr paradigm.\\nTurk and Pentland [43] present a face recognition scheme in\\nwhichfaceimagesareprojectedontotheprincipalcomponents\\nof the original set of training images. The resulting eigenfaces\\nare classiﬁed by comparison with known individuals.\\nTurk and Pentland present results on a database of 16\\nsubjects with various head orientation, scaling, and lighting.\\nTheir images appear identical otherwise with little variationin facial expression, facial details, pose, etc. For lighting,orientation, and scale variation their system achieves 96%,\\n85%, and 64% correct classiﬁcation, respectively. Scale is\\nrenormalized to the eigenface size based on an estimate of thehead size. The middle of the faces is accentuated, reducingany negative affect of changing hairstyle and backgrounds.\\nIn Pentland et al.[33], [34] good results are reported on\\na large database (95% recognition of 200 people from a\\n4A mugshot database typically contains side views where the performance\\nof feature point methods is known to improve [8].database of 3000). It is difﬁcult to draw broad conclusions\\nas many of the images of the same people look very similar,and the database has accurate registration and alignment [30].In Moghaddam and Pentland [30], very good results are\\nreported with the FERET database—only one mistake was\\nmade in classifying 150 frontal view images. The system usedextensive preprocessing for head location, feature detection,and normalization for the geometry of the face, translation,\\nlighting, contrast, rotation, and scale.\\nSwets and Weng [42] present a method of selecting discrim-\\ninant eigenfeatures using multidimensional linear discriminantanalysis. They present methods for determining the most ex-\\npressive features (MEF) and the most discriminatory features\\n(MDF).Wearenotcurrentlyawareoftheavailabilityofresultswhich are comparable with those of eigenfaces (e.g., on the\\nFERET database as in Moghaddam and Pentland [30]).\\nIn summary, it appears that eigenfaces is a fast, simple,\\nand practical algorithm. However, it may be limited becauseoptimal performance requires a high degree of correlation be-\\ntween the pixel intensities of the training and test images. This\\nlimitationhasbeenaddressedbyusingextensivepreprocessingto normalize the images.\\nC. Template Matching\\nTemplate matching methods such as [6] operate by perform-\\ning direct correlation of image segments. Template matching\\nis only effective when the query images have the same scale,orientation, and illumination as the training images [9].\\nD. Graph Matching\\nAnother approach to face recognition is the well known\\nmethod of graph matching. In [21], Lades et al.present\\na dynamic link architecture for distortion invariant objectrecognition which employs elastic graph matching to ﬁndthe closest stored graph. Objects are represented with sparse\\ngraphs whose vertices are labeled with a multiresolution\\ndescription in terms of a local power spectrum, and whoseedges are labeled with geometrical distances. They presentgood results with a database of 87 people and test images\\ncomposed of different expressions and faces turned 15\\n. The\\nmatchingprocessiscomputationallyexpensive,takingroughly25 s to compare an image with 87 stored objects when using\\nLAWRENCE et al.: FACE RECOGNITION 101\\nFig. 3. A depiction of the local image sampling process. A window is stepped over the image and a vector is created at each location.\\na parallel machine with 23 transputers. Wiskott et al.[45] use\\nan updated version of the technique and compare 300 faces\\nagainst 300 different faces of the same people taken from theFERET database. They report a recognition rate of 97.3%. Therecognition time for this system was not given.\\nE. Neural-Network Approaches\\nMuch of the present literature on face recognition with\\nneural networks presents results with only a small number\\nof classes (often below 20). We brieﬂy describe a couple of\\napproaches.\\nIn [10] the ﬁrst 50 principal components of the images are\\nextracted and reduced to ﬁve dimensions using an autoassocia-\\ntive neural network. The resulting representation is classiﬁed\\nusing a standard multilayer perceptron (MLP). Good resultsare reported but the database is quite simple: the pictures aremanually aligned and there is no lighting variation, rotation,\\nor tilting. There are 20 people in the database.\\nA hierarchical neural network which is grown automatically\\nand not trained with gradient descent was used for facerecognition by WengandHuang [44].They report good results\\nfor discrimination of ten distinctive subjects.\\nF. The ORL Database\\nIn [39] a hidden Markov model (HMM)-based approach is\\nused for classiﬁcation of the ORL database images. The best\\nmodel resulted in a 13% error rate. Samaria also performedextensive tests using the popular eigenfaces algorithm [43] onthe ORL database and reported a best error rate of around 10%\\nwhen the number of eigenfaces was between 175 and 199.\\nWe implemented the eigenfaces algorithm and also observedaround 10% error. In [38] Samaria extends the top–downHMM of [39]with pseudo 2-DHMM’s. The errorrate reduces\\nto5%attheexpenseofhighcomputationalcomplexity—asin-\\ngle classiﬁcation takes 4 min on a Sun Sparc II. Samaria notesthat although an increased recognition rate was achieved thesegmentation obtained with the pseudo 2-D HMM’s appeared\\nquite erratic. Samaria uses the same training and test set sizes\\nas we do (200 training images and 200 test images with nooverlap between the two sets). The 5% error rate is the besterror rate previously reported for the ORL database that we\\nare aware of.\\nIV. S\\nYSTEMCOMPONENTS\\nA. Overview\\nIn the following sections we introduce the techniques which\\nform the components of our system and describe our motiva-tion for using them. Brieﬂy, we explore the use of local image\\nsampling and a technique forpartial lighting invariance, a self-\\norganizing map (SOM) for projection of the image samplerepresentation into a quantized lower dimensional space, theKarhunen-Lo `eve (KL) transform for comparison with the\\nSOM, a convolutional network (CN) for partial translation and\\ndeformation invariance, and an MLP for comparison with theconvolutional network.\\nB. Local Image Sampling\\nWe have evaluated two different methods of representing\\nlocal image samples. In each method a window is scannedover the image as shown in Fig. 3.\\n1) The ﬁrst method simply creates a vector from a\\nlocal window on the image using the intensityvalues at each point in the window. Let\\nbe\\nthe intensity at the th column and the th row of\\nthe given image. If the local window is a squareof sides\\nlong, centered on , then\\nthe vector associated with this window is simply\\n2) The second method creates a representation of the local\\nsample by forming a vector out of 1) the intensity of\\nthe center pixel and 2) the difference in intensity\\nbetween the center pixel and all other pixels withinthe square window. The vector is given by\\nThe resulting rep-\\nresentation becomes partially invariant to variationsin intensity of the complete sample. The degree of\\n102 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\ninvariance can be modiﬁed by adjusting the weight\\nconnected to the central intensity component.\\nC. The Self-Organizing Map\\n1) Introduction: Maps are an important part of both natural\\nandartiﬁcialneuralinformationprocessingsystems[2].Exam-\\nples of maps in the nervous system are retinotopic maps in thevisual cortex [31], tonotopic maps in the auditory cortex [18],and maps from the skin onto the somatosensoric cortex [32].\\nThe SOM, introduced by Kohonen [19], [20], is an unsuper-\\nvised learning process which learns the distribution of a set ofpatterns without any class information. A pattern is projectedfrom an input space to a position in the map—information is\\ncoded as the location of an activated node. The SOM is unlike\\nmost classiﬁcation or clusteringtechniques in thatit provides atopological ordering of the classes. Similarity in input patternsis preserved in the output of the process. The topological\\npreservation of the SOM process makes it especially useful\\nin the classiﬁcation of data which includes a large number ofclasses. In the local image sample classiﬁcation, for example,there may be a very large number of classes in which the\\ntransition from one class to the next is practically continuous\\n(making it difﬁcult to deﬁne hard class boundaries).\\n2) Algorithm: We give a brief description of the SOM\\nalgorithm, for more details see [20]. The SOM deﬁnes a\\nmapping from an input space\\nonto a topologically ordered\\nsetofnodes,usuallyinalowerdimensionalspace.Anexampleof a 2-D SOM is shown in Fig. 4. A reference vector in the\\ninput space,\\nis assigned to\\neach node in the SOM. During training, each input vector,\\nis compared to all of the obtaining the location of the\\nclosest match (given by where\\ndenotes the norm of vector The input point is mapped\\nto this location in the SOM. Nodes in the SOM are updatedaccording to\\n(1)\\nwhereis the time during learning and is theneighbor-\\nhood function , a smoothing kernel which is maximum at\\nUsually, , where andrepresent\\nthe location of the nodes in the SOM output space. is the\\nnode with the closest weight vector to the input sample and\\nrangesoverallnodes. approaches0as increases\\nand also as approaches A widely applied neighborhood\\nfunction is\\n(2)\\nwhere is a scalar valued learning rate and deﬁnes\\nthe width of the kernel. They are generally both monotonically\\ndecreasing with time [20]. The use of the neighborhood\\nfunction means that nodes which are topographically close inthe SOM structure are moved toward the input pattern alongwith the winning node. This creates a smoothing effect which\\nleadstoaglobalorderingofthemap.Notethat\\nshouldnot\\nbe reduced too far as the map will lose its topographical orderif neighboring nodes are not updated along with the closest\\nFig. 4. A 2-D SOM showing a square neighborhood function which starts\\nas /104 /99/105 /40 /116 /49 /41and reduces in size to /104 /99/105 /40 /116 /51 /41over time.\\nnode. The SOM can be considered a nonlinear projection of\\nthe probability density, [20].\\n3) Improving the Basic SOM: The original SOM is com-\\nputationally expensive due to the following.\\n1) In the early stages of learning, many nodes are adjusted\\nin a correlated manner. Luttrel [27] proposed a method,\\nwhich is used here, that starts by learning in a smallnetwork,anddoublesthesizeofthenetworkperiodicallyduring training. When doubling, new nodes are inserted\\nbetween the current nodes. The weights of the new\\nnodes are set equal to the average of the weights ofthe immediately neighboring nodes.\\n2) Each learning pass requires computation of the distance\\nof the current sample to all nodes in the network, which\\nis\\n. However, this may be reduced to\\nusing a hierarchy of networks which is created from theabove node doubling strategy.\\n5\\nD. KL Transform\\nThe optimal linear method6for reducing redundancy in\\na dataset is the KL transform or eigenvector expansion viaprinciple components analysis (PCA) [12]. PCA generates aset of orthogonal axes of projections known as the principal\\ncomponents, or the eigenvectors, of the input data distribution\\nin the order of decreasing variance. The KL transform isa well-known statistical method for feature extraction andmultivariate data projection and has been used widely in\\npattern recognition, signal processing, image processing, and\\ndata analysis. Points in an\\n-dimensional input space are\\nprojected into an -dimensional space, . The KL\\ntransform is used here for comparison with the SOM in the\\ndimensionality reduction of the local image samples. The KL\\ntransform is also used in eigenfaces, however in that case it\\n5This assumes that the topological order is optimal prior to each doubling\\nstep.\\n6In the least mean squared error sense.\\nLAWRENCE et al.: FACE RECOGNITION 103\\nFig. 5. A typical convolutional network.\\nFig. 6. A high-level block diagram of the system we have used for face recognition.\\nis used on the entire images, whereas it is only used on small\\nlocal image samples in this work.\\nE. Convolutional Networks\\nThe problem of face recognition from 2-D images is typ-\\nically very ill-posed, i.e., there are many models which ﬁt\\nthe training points well but do not generalize well to unseen\\nimages. In other words, there are not enough training points inthespacecreatedbytheinputimagesinordertoallowaccurateestimation of class probabilities throughout the input space.\\nAdditionally, for MLP networks with 2-D images as input,\\nthere is no invariance to translation or local deformation ofthe images [23].\\nConvolutional networks (CN) incorporate constraints and\\nachieve some degree of shift and deformation invariance using\\nthree ideas: local receptive ﬁelds, shared weights, and spatialsubsampling. The use of shared weights also reduces thenumber of parameters in the system aiding generalization.\\nConvolutional networks have been successfully applied to\\ncharacter recognition [3], [5], [22]–[24].\\nA typical convolutional network is shown in Fig. 5 [24].\\nThe network consists of a set of layers each of which contains\\none or more planes. Approximately centered and normalized\\nimages enter at the input layer. Each unit in a plane receivesinput from a small neighborhood in the planes of the previouslayer. The idea of connecting units to local receptive ﬁelds\\ndates back to the 1960’s with the perceptron and Hubel\\nand Wiesel’s [15] discovery of locally sensitive orientation-selective neurons in the cat’s visual system [23]. The weightsforming the receptive ﬁeld for a plane are forced to be equal\\nat all points in the plane. Each plane can be considered\\nas a feature map which has a ﬁxed feature detector that isconvolved with a local window which is scanned over theplanes in the previous layer. Multiple planes are usually used\\nin each layer so that multiple features can be detected. These\\nlayers are called convolutional layers. Once a feature hasbeen detected, its exact location is less important. Hence,the convolutional layers are typically followed by another\\nlayer which does a local averaging and subsampling operation\\n(e.g., for a subsampling factor of two:\\nwhere is the output of\\na subsampling plane at position andis the output of the\\nsame plane in the previous layer). The network is trained with\\nthe usual backpropagation gradient-descent procedure [13].A connection strategy can be used to reduce the number ofweights in the network. For example, with reference to Fig. 5,\\nLe Cunet al.[24] connect the feature maps in the second\\nconvolutional layer only to one or two of the maps in theﬁrst subsampling layer (the connection strategy was chosenmanually).\\nV. S\\nYSTEMDETAILS\\nThe system we have used for face recognition is a com-\\nbination of the preceding parts—a high-level block diagramis shown in Figs. 6 and 7 shows a breakdown of the varioussubsystems that we experimented with or discuss.\\nOur system works as follows (we give complete details of\\ndimensions etc. later).\\n1) For the images in the training set, a ﬁxed size window\\n(e.g., 5\\n5) is stepped over the entire image as shown\\nin Fig. 3 and local image samples are extracted at eachstep. At each step the window is moved by four pixels.\\n2) An SOM (e.g., with three dimensions and ﬁve nodes\\nper dimension,\\ntotal nodes) is trained on the\\nvectors from the previous stage. The SOM quantizesthe 25-dimensional input vectors into 125 topologically\\n104 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFig. 7. A diagram of the system we have used for face recognition showing alternative methods which we consider in this paper. The top “MLP style\\nclassiﬁer” represents the ﬁnal MLP style fully connected layer of the convolutional network. We have shown this decomposition of the convolutional\\nnetwork in order to highlight the possibility of replacing the ﬁnal layer (or layers) with a different type of classiﬁer. The nearest-neighbor style c lassiﬁer\\nis potentially interesting because it may make it possible to add new classes with minimal extra training time. The bottom MLP shows that the entireconvolutional network can be replaced with an MLP. We present results with either a SOM or the KL transform used for dimensionality reduction, and\\neither a convolutional neural network or an MLP for classiﬁcation.\\nordered values. The three dimensions of the SOM can\\nbe thought of as three features. We also experimented\\nwith replacing the SOM with the KL transform. In thiscase, the KL transform projects the vectors in the 25-dimensional space into a 3-D space.\\n3) The same window as in the ﬁrst step is stepped over\\nall of the images in the training and test sets. The localimage samples are passed through the SOM at each step,thereby creating new training and test sets in the output\\nspace created by the SOM. (Each input image is now\\nrepresented by three maps, each of which correspondsto a dimension in the SOM. The size of these maps isequal to the size of the input image (92\\n112) divided\\nby the step size (for a step size of four, the maps are\\n2328).\\n4) A convolutional neural network is trained on the newly\\ncreated training set. We also experimented with training\\na standard MLP for comparison.\\nA. Simulation Details\\nIn this section we give the details of one of the best\\nperforming systems.\\nFor the SOM, training is split into two phases as recom-\\nmended by Kohonen [20]—an ordering phase and a ﬁne-\\nadjustment phase. In the ﬁrst phase 100000 updates are\\nperformed, and in the second 50000 are performed. In the ﬁrstphase, the neighborhood radius starts at two-thirds of the sizeofthemapandreduceslinearlytoone.Thelearningrateduring\\nthis phase is:\\nwhereis the current update\\nnumber, and is the total number of updates. In the second\\nphase, the neighborhood radius starts at two and is reduced toone. The learning rate during this phase is .\\nThe convolutional network contained ﬁve layers excluding\\nthe input layer. A conﬁdence measure was calculated for eachclassiﬁcation:\\nwhere isthemaximumoutput\\nand is the second maximum output (for outputs which\\nhave been transformed using the softmaxtransformation\\nwhere are the original outputs, are the transformed\\noutputs, and is the number of outputs). The number of\\nplanes in each layer, the dimensions of the planes, and thedimensions of the receptive ﬁelds are shown in Table I. Thenetwork was trained with backpropagation [13] for a total of\\n20000 updates. Weights in the network were updated after\\neach pattern presentation, as opposed to batch update whereweights are only updated once per pass through the trainingset. All inputs were normalized to lie in the range\\n1t o\\n1. All nodes included a bias input which was part of the\\noptimization process. The best of ten random weight sets waschosen for the initial parameters of the network by evaluatingthe performance on the training set. Weights were initialized\\non a node by node basis as uniformly distributed random\\nnumbers in the range\\nwhere is the fan-in\\nof neuron [13]. Target outputs were 0.8 and 0.8 using the\\noutput activation function.7The quadratic cost function\\n7This helps avoid saturating the sigmoid function. If targets were set to the\\nasymptotes of the sigmoid this would tend to: 1) drive the weights to inﬁnity;\\n2) cause outlier data to produce very large gradients due to the large weights;\\nand 3) produce binary outputs even when incorrect—leading to decreased\\nreliability of the conﬁdence measure.\\nLAWRENCE et al.: FACE RECOGNITION 105\\nFig. 8. The learning rate as a function of the epoch number.\\nTABLE I\\nDIMENSIONS FOR THE CONVOLUTIONAL NETWORK.THECONNECTION\\nPERCENTAGE REFERS TO THE PERCENTAGE OF NODES IN THE PREVIOUSLAYER\\nTOWHICHEACHNODE IN THE CURRENTLAYERISCONNECTED —A VALUE\\nLESSTHAN100% R EDUCES THE TOTALNUMBER OF WEIGHTS IN THE NETWORK\\nANDMAYIMPROVEGENERALIZATION .THECONNECTION STRATEGY USEDHERE\\nISSIMILAR TO THAT USED BYLECUNET AL.[24]FORCHARACTER\\nRECOGNITION .HOWEVER,ASOPPOSED TO THE MANUALCONNECTION STRATEGY\\nUSED BYLECUNET AL.,THECONNECTIONS BETWEENLAYERS2AND3ARE\\nCHOSENRANDOMLY .ASA NEXAMPLE OF HOW THEPRECISECONNECTIONS CAN\\nBEDETERMINED FROM THE TABLE—THESIZE OF THE FIRST-LAYERPLANES(21\\n/226) ISEQUAL TO THE TOTALNUMBER OF WAYS OFPOSITIONING A\\n3 /23RECEPTIVE FIELD ON THE INPUT-LAYERPLANES(23 /228)\\nwas used. A search then converge learning rate schedule was\\nused8\\nwhere learning rate, initial learning rate 0.1,\\ntotal training epochs, current training epoch,\\nand The schedule is shown in Fig. 8.\\nTotal training time was around four hours on an SGI Indy100Mhz MIPS R4400 system.\\nVI. E\\nXPERIMENTAL RESULTS\\nWe performed various experiments and present the results\\nhere. Except when stated otherwise, all experiments were\\nperformed with ﬁve training images and ﬁve test images perperson for a total of 200 training images and 200 test images.There was no overlap between the training and test sets. We\\nnote that a system which guesses the correct answer would be\\nrightoneoutof40times,givinganerrorrateof97.5%.Forthe\\n8Relatively high learning rates are typically used in order to help avoid\\nslow convergence and local minima. However, a constant learning rate results\\nin signiﬁcant parameter and performance ﬂuctuation during the entire training\\ncycle such that the performance of the network can alter signiﬁcantly from\\nthe beginning to the end of the ﬁnal epoch. Moody and Darkin have proposed“search then converge” learning rate schedules. We have found that these\\nschedules still result in considerable parameter ﬂuctuation and hence we have\\nadded another term to further reduce the learning rate over the ﬁnal epochs (a\\nsimpler linear schedule also works well). We have found the use of learning\\nrate schedules to improve performance considerably.following sets of experiments, we vary only one parameter in\\neach case. The error barsshown in the graphs represent plus or\\nminusonestandarddeviationofthedistributionofresultsfromanumberofsimulations.\\n9Wenotethatideallywewouldliketo\\nhaveperformedmoresimulationsperreportedresult,however,\\nwe were limited in terms of computational capacity available\\nto us. The constants used in each set of experiments were:numberofclasses:40,dimensionalityreductionmethod:SOM,dimensions in the SOM: three, number of nodes per SOM\\ndimension: ﬁve, image sample extraction: original intensity\\nvalues, training images per class: ﬁve. Note that the constantsin each set of experiments may not give the best possibleperformance as the current best performing system was only\\nobtained as a result of these experiments. The experiments are\\nas follows.\\n1)Variation of the number of output classes —Table II and\\nFig. 9 show the error rate of the system as the number\\nof classes is varied from ten to 20 to 40. We made noattempt to optimize the system for the smaller numbersof classes. As we expect, performance improves with\\nfewer classes to discriminate between.\\n2)Variation of the dimensionality of the SOM —Table III\\nand Fig. 10 show the error rate of the system as thedimension of the SOM is varied from one to four. The\\nbest performing value is three dimensions.\\n3)Variation of the quantization level of the SOM —Table\\nIV and Fig. 11 show the error rate of the system asthe size of the SOM is varied from four to ten nodes\\nper dimension. The SOM has three dimensions in each\\ncase. The best average error rate occurs for eight or ninenodes per dimension. This is also the best average errorrate of all experiments.\\n4)Variation of the image sample extraction\\nalgorithm —Table V shows the result of using the\\ntwo local image sample representations described\\nearlier. We found that using the original intensity values\\ngave the best performance. We investigated altering theweight assigned to the central intensity value in thealternative representation but were unable to improve\\nthe results.\\n5)Substituting the SOM with the KL transform —Table VI\\nshows the results of replacing the SOM with the KL\\n9We ran multiple simulations in each experiment where we varied the\\nselection of the training and test images (out of a total of /49/48/33 /61 /53 /33/61/51 /48 /50 /52 /48\\npossibilities) and the random seed used to initialize the weights in the\\nconvolutional neural network.\\n106 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFig. 9. The error rate as a function of the number of classes. We did not modify the network from that used for the 40 class case.\\ntransform. We investigated using the ﬁrst one, two,\\nor three eigenvectors for projection. Surprisingly, thesystem performed best with only one eigenvector. The\\nbest SOM parameters we tried produced slightly better\\nperformance. The quantization inherent in the SOM\\ncould provide a degree of invariance to minor image\\nsample differences and quantization of the PCA projec-tions may improve performance.\\n6)Replacing the CN with an MLP —Table VII shows the\\nresults of replacing the convolutional network with anMLP. Performance is very poor. This result was ex-\\npected because the MLP does not have the inbuilt\\ninvariance to minor translation and local deformationwhich is created in the convolutional network using\\nthe local receptive ﬁelds, shared weights, and spatial\\nsubsampling. As an example, consider when a feature isshifted in a test image in comparison with the training\\nimage(s) for the individual. We expect the MLP to have\\ndifﬁculty recognizing afeature whichhas been shiftedincomparison to the training images because the weights\\nconnected to the new location were not trained for the\\nfeature.\\nTheMLPcontainedonehiddenlayer.Weinvestigated\\nthe following hidden layer sizes for the MLP: 20, 50,\\n100, 200, and 500. The best performance was obtainedwith 200 hidden nodes and a training time of two days.\\nThe learning rate schedule and initial learning rate were\\nthe same as for the original network. Note that thebest performing KL parameters were used while the\\nbest performing SOM parameters were not. We note\\nthat it may be considered fairer to compare against anMLPwithmultiplehiddenlayers[14],howeverselection\\nof the appropriate number of nodes in each layer is\\ndifﬁcult (e.g., we have tried a network with two hiddenlayers containing 100 and 50 nodes, respectively, which\\nresulted in an error rate of 90%).\\n7)The tradeoffbetweenrejectionthreshold andrecognition\\naccuracy—Fig. 12 shows a histogram of the recog-\\nnizer’s conﬁdence for the cases when the classiﬁeris correct and when it is wrong for one of the best\\nperforming systems. From this graph we expect that\\nclassiﬁcation performance will increase signiﬁcantly ifwe reject cases below a certain conﬁdence threshold.Fig. 13 shows the system performance as the rejection\\nthreshold is increased. We can see that by rejectingexamples with low conﬁdence we can signiﬁcantly in-\\ncrease the classiﬁcation performance of the system. If\\nwe consider a system which used a video camera totake a number of pictures over a short period, we couldexpect that a high performance would be attainable with\\nan appropriate rejection threshold.\\n8)Comparison with other known results on the same\\ndatabase—Table VIII shows a summary of the per-\\nformance of the systems for which we have results\\nusing the ORL database. In this case, we used a\\nSOM quantization level of eight. Our system is thebest performing system\\n10and performs recognition\\nroughly500timesfasterthanthesecondbestperforming\\nsystem—the pseudo 2-D HMM’s of Samaria. Fig. 14\\nshows the images which were incorrectly classiﬁed forone of the best performing systems.\\n9)Variation of the number of training images per person.\\nTable IX shows the results of varying the number of\\nimages per class used in the training set from one toﬁve for\\nand also for the eigen-\\nfaces algorithm. We implemented two versions of the\\neigenfaces algorithm—the ﬁrst version creates vectors\\nfor each class in the training set by averaging theresults of the eigenface representation over all imagesfor the same person. This corresponds to the algorithm\\nas described by Turk and Pentland [43]. However,\\nwe found that using separate training vectors for eachtraining image resulted in better performance. We foundthat using between 40 to 100 eigenfaces resulted in\\nsimilar performance. We can see that the\\nand\\nmethods are both superior to the eigenfaces\\ntechnique even when there is only one training imageper person. The SOM+CN method consistently performs\\nbetter than the\\nmethod.\\nVII. DISCUSSION\\nThe results indicate that a convolutional network can be\\nmore suitable in the given situation when compared with a\\n10The 4% error rate reported is an average of multiple simula-\\ntions—individual simulations have given error rates as low as 1.5%.\\nLAWRENCE et al.: FACE RECOGNITION 107\\nFig. 10. The error rate as a function of the number of dimensions in the SOM.\\nFig. 11. The error rate as a function of the number of nodes per dimension in the SOM.\\nFig. 12. A histogram depicting the conﬁdence of the classiﬁer when it turns out to be correct, and the conﬁdence when it is wrong. The graph suggests that\\nwe can improve classiﬁcation performance considerably by rejecting cases where the classiﬁer has a low conﬁdence.\\nTABLE II\\nERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH VARYINGNUMBER OF\\nCLASSES(SUBJECTS). EACHRESULTIS THEAVERAGE OF THREESIMULATIONS\\nstandard MLP. This correlates with the common belief that the\\nincorporation of prior knowledge is desirable for MLP style\\nnetworks (the CN incorporates domain knowledge regarding\\ntherelationshipofthepixelsanddesiredinvariancetoadegreeof translation, scaling, and local deformation).TABLE III\\nERRORRATE OF THE FACERECOGNITION SYSTEM WITH VARYING\\nNUMBER OF DIMENSIONS IN THE SELF-ORGANIZING MAP.EACH\\nRESULTGIVENIS THEAVERAGE OF THREESIMULATIONS\\nConvolutional networks have traditionally been used on raw\\nimages without any preprocessing. Without the preprocessing\\nwe have used, the resulting convolutional networks are larger,\\nmore computationally intensive, and have not performed aswell in our experiments [e.g., using no preprocessing and the\\n108 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFig. 13. The test set classiﬁcation performance as a function of the percentage of samples rejected. Classiﬁcation performance can be improved signi ﬁcantly\\nby rejecting cases with low conﬁdence.\\nFig. 14. Test images. The images with a thick white border were incorrectly classiﬁed by one of the best performing systems.\\nTABLE IV\\nERRORRATE OF THE FACERECOGNITION SYSTEM WITH VARYING\\nNUMBER OF NODES PER DIMENSION IN THE SOM. E ACH\\nRESULTGIVENIS THEAVERAGE OF THREESIMULATIONS\\nTABLE V\\nERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH VARYINGIMAGESAMPLE\\nREPRESENTATION .EACHRESULTIS THEAVERAGE OF THREESIMULATIONS\\nTABLE VI\\nERRORRA T EO FT H E FACERECOGNITION SYSTEM WITH LINEAR\\nPCAANDSOM F EATUREEXTRACTION MECHANISMS .\\nEACHRESULTIS THEAVERAGE OF THREESIMULATIONS\\nTABLE VII\\nERRORRATECOMPARISON OF THE VARIOUSFEATURE\\nEXTRACTION AND CLASSIFICATION METHODS.EACH\\nRESULTIS THEAVERAGE OF THREESIMULATIONS\\nsame CN architecture except initial receptive ﬁelds of 8 8\\nresulted in approximately two times greater error (for the case\\nof ﬁve images per person)].\\nFig. 15 shows the randomly chosen initial local image\\nsamples corresponding to each node in a 2-D SOM, and\\nthe ﬁnal samples to which the SOM converges. Scanning\\nacross the rows and columns we can see that the quantizedsamples represent smoothly changing shading patterns. This isthe initial representation from which successively higher level\\nfeatures are extracted using the convolutional network. Fig. 16\\nshows the activation of the nodes in a sample convolutionalnetwork for a particular test image.\\nLAWRENCE et al.: FACE RECOGNITION 109\\nTABLE VIII\\nERRORRATE OF THE VARIOUSSYSTEMS./49On a Sun Sparc II./50ONA NSGI\\nINDY MIPS R4400 100MHZ S YSTEM.ACCORDING TO THE SPECINT92\\nANDSPECFP92 R ATINGS AT http://hpwww.epﬂ.ch/bench/spec.html\\nTHESGI M ACHINEISAPPROXIMATELY THREETIMESFASTERTHAN\\nTHESUN SPARC II, M AKING THE SOM /43CN APPROXIMATELY 160\\nTIMESFASTER THAN THE PSEUDO2-D HMM’S FORCLASSIFICATION\\nTABLE IX\\nERRORRATE FOR THE EIGENFACES ALGORITHM AND THE /83/79 /77/43/67/78 AS THE\\nSIZE OF THE TRAININGSETISVARIED FROM ONE TOFIVEIMAGES PER PERSON.\\nAVERAGED OVERTWODIFFERENT SELECTIONS OF THE TRAINING AND TESTSETS\\nFig. 15. SOM image samples before training (a random set of imagesamples) and after training.\\nFig. 17 shows the results of sensitivity analysis in order to\\ndetermine which parts of the input image are most importantfor classiﬁcation. Using the method of Baluja and Pomerleau\\nas described in [37], each of the input planes to the convo-\\nlutional network was divided into 2\\n2 segments (the input\\nplanes are 23 28). Each of 168 (12 14) segments was\\nreplaced with random noise, one segment at a time. The test\\nperformance was calculated at each step. The error of the\\nnetwork when replacing parts of the input with random noisegives an indication of how important each part of the image isfor the classiﬁcation task. From the ﬁgure it can be observed\\nthat, as expected, the eyes, nose, mouth, chin, and hair regions\\nare all important to the classiﬁcation task.\\nCan the convolutional network feature extraction form the\\noptimal set of features? The answer is negative—it is unlikely\\nthat the network could extract an optimal set of features for all\\nimages. Although the exact process of human face recognitionis unknown, there are many features which humans may usebut our system is unlikely to discover optimally—e.g., 1)\\nknowledge of the 3-D structure of the face; 2) knowledge\\nof the nose, eyes, mouth, etc.; 3) generalization to glasses/noglasses, different hair growth, etc.; and 4) knowledge of facialexpressions.VIII. C\\nOMPUTATIONAL COMPLEXITY\\nThe SOM takes considerable time to train. This is not a\\ndrawback of the approach however, as the system can beextended to cover new classes without retraining the SOM.All that is required is that the image samples originally usedto train the SOM are sufﬁciently representative of the imagesamples used in new images. For the experiments we havereported here, the quantized output of the SOM is very similarif we train it with only 20 classes instead of 40. In addition,the KL transform can be used in place of the SOM with aminimal impact on system performance.\\nIt also takes a considerable amount of time to train a con-\\nvolutional network; how signiﬁcant is this? The convolutionalnetwork extracts features from the image. It is possible to useﬁxed feature extraction. Consider if we separate the convo-\\nlutional network into two parts: the initial feature extraction\\nlayers and the ﬁnal feature extraction and classiﬁcation layers.Given a well-chosen sample of the complete distribution offaces which we want to recognize, the features extracted fromthe ﬁrst section could be expected to also be useful for theclassiﬁcation of new classes. These features could then beconsidered ﬁxed features and the ﬁrst part of the network maynot need to be retrained when adding new classes. The pointat which the convolutional network is broken into two woulddepend on how well the features at each stage are useful forthe classiﬁcation of new classes (the larger features in theﬁnal layers are less likely to be a good basis for classiﬁcationof new examples). We note that it may be possible to replacethe second part with another type of classiﬁer—e.g., a nearest-neighborsclassiﬁer.Inthiscasethetimerequiredforretrainingthe system when adding new classes is minimal (the extractedfeature vectors are simply stored for the training images).\\nTo give an idea of the computational complexity of each\\npart of the system we deﬁne:\\nthe number of classes;\\nthe number of nodes in the SOM;\\nthe number of weights in the convolutional net-work;\\nthe number of weights in the classiﬁer;\\nthe number of training examples;\\nthe number of nodes in the neighborhood function;\\nthe total number of next nodes used to backpropa-gate the error in the CN;\\nthe total number of next nodes used to backpropa-gate the error in the MLP classiﬁer;\\nthe output dimension of the KL projection;\\nthe input dimension of the KL projection;\\nthe number of training samples for the SOM or theKL projection;\\nthe number of local image samples per image.\\nTables X and XI show the approximate complexity of the\\nvarious parts of the system during training and classiﬁcation.WeshowthecomplexityforboththeSOMandKLalternativesfor dimensionality reduction and for both the neural network(MLP) and a nearest-neighbors classiﬁer (as the lastpart of theconvolutional network—not as a complete replacement, i.e.,this is not the same as the earlier MLP experiments). We note\\n110 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\nFig. 16. A depiction of the node maps in a sample convolutional network showing the activation values for a particular test image. The input image\\nis shown on the left. In this case the image is correctly classiﬁed with only one activated output node (the top node). From left to right after the inputimage, the layers are: the input layer, convolutional layer 1, subsampling layer 1, convolutional layer 2, subsampling layer 2, and the output layer. The\\nthree planes in the input layer correspond to the three dimensions of the SOM.\\nFig. 17. Sensitivity to various parts of the input image. It can be observedthat the eyes, mouth, nose, chin, and hair regions are all important for the\\nclassiﬁcation. The\\n/122axis corresponds to the mean squared error rather than\\ntheclassiﬁcationerror(themeansquarederrorispreferablebecauseitvariesin\\na smoother fashion as the input images are perturbed). The image orientation\\ncorresponds to upright face images.\\nthat the constant associated with the log factors may increase\\nexponentiallyintheworstcase(cf.neighborsearchinginhigh-dimensional spaces [1]). We have aimed to show how thecomputational complexity scales according to the number of\\nclasses, e.g., for the training complexity of the MLP classiﬁer:\\nalthough\\nmay be larger than both and\\nscale roughly according to .TABLE X\\nTRAININGCOMPLEXITY . /107 /49AND /107 /51REPRESENT THE\\nNUMBER OF TIMES THE TRAININGSETISPRESENTED TO\\nTHENETWORK FOR THE SOMAND THECN, RESPECTIVELY\\nTABLE XI\\nCLASSIFICATION COMPLEXITY . /107 /50REPRESENTS\\nTHEDEGREE OF SHAREDWEIGHTREPLICATION\\nWith reference to Table XI, consider, for example, the main\\narchitecture in recognition mode. The complexity\\nof the SOM module is independent of the number of classes.The complexity of the CN scales according to the number of\\nweights in the network. When the number of feature maps in\\nthe internal layers is constant, the number of weights scalesroughly according to the number of output classes (the number\\nLAWRENCE et al.: FACE RECOGNITION 111\\nof weights in the output layer dominates the weights in the\\ninitial layers).\\nIn terms of computation time, the requirements of real-time\\ntasks varies. The system we have presented should be suitable\\nfor a number of real-time applications. The system is capable\\nof performing a classiﬁcation in less than half a second for 40classes. This speed is sufﬁcient fortasks such as access controland room monitoring whenusing40 classes.It is expected that\\nan optimized version could be signiﬁcantly faster.\\nIX. F\\nURTHERRESEARCH\\nWe can identify the following avenues for improving per-\\nformance.\\n1) More careful selection of the convolutional network\\narchitecture, e.g., by using the optimal brain damagealgorithm [25] as used by Le Cun et al.[24] to improve\\ngeneralization and speedup handwritten digit recogni-\\ntion.\\n2) More precise normalization of the images to account\\nfor translation, rotation, and scale changes. Any nor-malization would be limited by the desired recognition\\nspeed.\\n3) The various facial features could be ranked according\\nto their importance in recognizing faces and separate\\nmodules could be introduced for various parts of the\\nface, e.g., the eye region, the nose region, and themouth region (Brunelli and Poggio [6] obtain very goodperformance using a simple template matching strategy\\non precisely these regions).\\n4) An ensemble of recognizers could be used. These could\\nbe combined via simple methods such as a linear com-bination based on the performance of each network, or\\nvia a gating network and the expectation-maximization\\nalgorithm [11], [16]. Examination of the errors madeby networks trained with different random seeds and bynetworks trained with the SOM data versus networks\\ntrained with the KL data shows that a combination\\nof networks should improve performance (the set ofcommon errors between the recognizers is often muchsmaller than the total number of errors).\\n5) Invariancetoagroupofdesiredtransformationscouldbe\\nenhancedwiththeadditionofpseudo-datatothetrainingdatabase—i.e., the addition of new examples createdfrom the current examples using translation, etc. Leen\\n[26] shows that adding pseudodata can be equivalent\\nto adding a regularizer to the cost function where theregularizer penalizes changes in the output when theinput goes under a transformation for which invariance\\nis desired.\\nX. C\\nONCLUSIONS\\nWe have presented a fast, automatic system for face recog-\\nnition which is a combination of a local image sample rep-resentation, an SOM network, and a convolutional network\\nfor face recognition. The SOM provides a quantization of the\\nimage samples into a topological space where inputs that arenearby in the original space are also nearby in the outputspace, which results in invariance to minor changes in the\\nimage samples, and the convolutional neural network pro-vides for partial invariance to translation, rotation, scale, anddeformation. Substitution of the KL transform for the SOM\\nproduced similar but slightly worse results. The method is\\ncapable of rapid classiﬁcation, requires only fast approximatenormalization and preprocessing, and consistently exhibitsbetter classiﬁcation performance than the eigenfaces approach\\n[43] on the database considered as the number of images\\nper person in the training database is varied from one toﬁve. With ﬁve images per person the proposed method andeigenfaces result in 3.8% and 10.5% error, respectively. The\\nrecognizer provides a measure of conﬁdence in its output and\\nclassiﬁcation error approaches zero when rejecting as few as10% of the examples. We have presented avenues for furtherimprovement.\\nThere are no explicit 3-D models in our system, however\\nwe have found that the quantized local image samples usedas input to the convolutional network represent smoothlychanging shading patterns. Higher level features are con-\\nstructed from these building blocks in successive layers of\\nthe convolutional network. In comparison with the eigenfacesapproach, we believe that the system presented here is able tolearn more appropriate features in order to provide improved\\ngeneralization. The system is partially invariant to changes in\\nthe local image samples, scaling, translation and deformationby design.\\nA\\nCKNOWLEDGMENT\\nThe authors would like to thank I. Cox, S. Haykin, and the\\nanonymous reviewers for helpful comments, and the Olivetti\\nResearch Laboratory and Ferdinando Samaria for compiling\\nand maintaining the ORL database.\\nREFERENCES\\n[1] S. Arya and D. M. Mount, “Algorithms for fast vector quantization,”\\ninProc. DCC 93: Data Compression Conf., J. A. Storer and M. Cohn,\\nEds. Piscataway, NJ: IEEE Press, pp. 381–390.\\n[2] H.-U. Bauer and K. R. Pawelzik, “Quantifying the neighborhood preser-\\nvation of self-organizing feature maps,” IEEE Trans. Neural Networks,\\nvol. 3, pp. 570–579, 1992.\\n[3] Y. Bengio, Y. Le Cun, and D. Henderson, “Globally trained handwritten\\nword recognizer using spatial representation, space displacement neural\\nnetworks, and hidden Markov models,” in Advances in Neutral Infor-\\nmation Processing Systems 6 . San Mateo, CA: Morgan Kaufmann,\\n1994.\\n[4] J. L. Blue, G. T. Candela, P. J. Grother, R. Chellappa, and C. L. Wilson,\\n“Evaluation of pattern classiﬁers for ﬁngerprint and OCR applications,”\\nPattern Recognition, vol. 27, no. 4, pp. 485–501, Apr. 1994.\\n[5] L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. Jackel,\\nY. Le Cun, U. Muller, E. Sackinger, P. Simard, and V. N. Vapnik,\\n“Comparison of classiﬁer methods: A case study in handwritten digit\\nrecognition,” in Proc. Int. Conf. Pattern Recognition . Los Alamitos,\\nCA: IEEE Comput. Soc. Press, 1994.\\n[6] R. Brunelli and T. Poggio, “Face recognition: Features versus\\ntemplates,” IEEE Trans. Pattern Anal. Machine Intell., vol. 15, pp.\\n1042–1052, Oct. 1993.\\n[7] D. K. Burton, “Text-dependent speaker veriﬁcation using vector quan-\\ntization source coding,” IEEE Trans. Acoust., Speech, Signal Process.,\\nvol. ASSP-35, pp. 133, 1987.\\n[8] R. Chellappa, C. L. Wilson, and S. Sirohey, “Human and machine\\nrecognition offaces: A survey,” Proc.IEEE, vol.83, pp.5, pp.705–740,\\n1995.\\n[9] I. J. Cox, J. Ghosn, and P. N. Yianilos, “Feature-based face recognition\\nusing mixture-distance,” in Computer Vision and Pattern Recognition .\\nPiscataway, NJ: IEEE Press, 1996.\\n112 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 8, NO. 1, JANUARY 1997\\n[10] D. DeMers and G. W. Cottrell, “Nonlinear dimensionality reduction,” in\\nAdvances in Neural Information Processing Systems 5 , S. J. Hanson, J.\\nD.Cowan, and C. Lee Giles, Eds. San Mateo,CA: Morgan Kaufmann,1993, pp. 580–587.\\n[11] H. Drucker, C. Cortes, L. Jackel, Y. Le Cun, and V. N. Vapnik,\\n“Boosting and other ensemble methods,” Neural Computa., vol. 6, pp.\\n1289–1301, 1994.\\n[12] K. Fukunaga, Introduction to Statistical Pattern Recognition , 2nd ed.\\nBoston, MA: Academic, 1990.\\n[13] S.Haykin, NeuralNetworks,AComprehensiveFoundation . NewYork:\\nMacmillan, 1994.\\n[14]\\n, private communication, 1996.\\n[15] D. H. Hubel and T. N. Wiesel, “Receptive ﬁelds, binocular interaction,\\nand functional architecture in the cat’s visual cortex,” J. Physiol., vol.\\n160, pp. 106–154, 1962.\\n[16] R.A.Jacobs,“Methodsforcombiningexperts’probabilityassessments,”\\nNeural Computa., vol. 7, pp. 867–888, 1995.\\n[17] T. Kanade, “Picture processing by computer complex and recognition\\nof human faces,” Ph.D. dissertation, Kyoto Univ., Japan, 1973.\\n[18] H. Kita and Y. Nishikawa, “Neural-network model of tonotopic map\\nformation based on the temporal theory of auditory sensation,” inProc. World Congr. Neural Networks, WCNN 93, vol. II. Hillsdale,\\nNJ: Lawrence Erlbaum, pp. 413–418.\\n[19] T. Kohonen, “The self-organizing map,” Proc. IEEE, vol. 78, pp.\\n1464–1480, 1990.\\n[20]\\n,Self-Organizing Maps . Berlin: Springer-Verlag, 1995.\\n[21] M.Lades,J.C.Vorbr¨ uggen,J.Buhmann,J.Lange,C.vonderMalsburg,\\nRolf P. W¨ urtz, and W. Konen, “Distortion invariant object recognition\\nin the dynamic link architecture,” IEEE Trans. Comput., vol. 42, pp.\\n300–311, 1993.\\n[22] Y. Le Cun, “Generalization and network design strategies,” Dep. Com-\\nput. Sci., Univ. Toronto, Canada, Tech. Rep. CRG-TR-89-4, 1989.\\n[23] Y. Le Cun and Y. Bengio, “Convolutional networks for images, speech,\\nandtimeseries,”in TheHandbookofBrainTheoryandNeuralNetworks,\\nM. A. Arbib, Ed. Cambridge, MA: MIT Press, 1995, pp. 255–258.\\n[24] Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. Howard, W.\\nHubbard,and L.Jackel,“Handwrittendigitrecognitionwithabackprop-\\nagation neural network,” in Advances in Neural Information Processing\\nSystems 2 , D. S. Touretzky, Ed. San Mateo, CA: Morgan Kaufmann,\\n1990, pp. 396–404.\\n[25] Y. Le Cun, J. S. Denker, and S. A. Solla, “Optimal brain damage,”\\ninNeural Information Processing Systems , vol. 2, D. S. Touretzky, Ed.\\nSan Mateo, CA: Morgan Kaufmann, 1990, pp. 598–605.\\n[26] T. K. Leen, “From data distributions to regularization in invariant\\nlearning,” Neural Computa., vol. 3, no. 1, pp. 135–143, 1991.\\n[27] S. P. Luttrell, “Hierarchical self-organizing networks,” in Proc. 1st\\nIEEConf.ArtiﬁcialNeuralNetworks . London: British Neural Network\\nSoc., 1989, pp. 2–6.\\n[28] D. Marr, Vision. San Francisco, CA: W. H. Freeman, 1982.\\n[29] B. Miller, “Vital signs of identity,” IEEE Spectrum, pp. 22–30, Feb.\\n1994.\\n[30] B. Moghaddam and A. Pentland, “Face recognition using view-based\\nand modular eigenspaces,” in Automat. Syst. Identiﬁcation Inspection of\\nHumans, vol. 2257, 1994.\\n[31] K. Obermayer, G. G. Blasdel, and K. Schulten, “A neural-network\\nmodel for the formation and for the spatial structure of retinotopicmaps, orientation, and ocular dominance columns,” in Artiﬁcial Neural\\nNetworks, T. Kohonen, K. M ¨akisara, O. Simula, and J. Kangas, Eds.\\nAmsterdam, Netherlands: Elsevier, 1991, pp. 505–511.\\n[32] K. Obermayer, H. Ritter, and K. Schulten, “Large-scale simulation of\\na self-organizing neural network: Formation of a somatotopic map,”\\ninParallel Processing in Neural Systems and Computers , R. Eck-\\nmiller, G. Hartmann, and G. Hauske, Eds. Amsterdam, Netherlands:North–Holland, 1990, pp. 71–74.\\n[33] A. Pentland, B. Moghaddam, and T. Starner, “View-based and modular\\neigenspaces for face recognition,” in Proc. IEEE Conf. Comput. Vision\\nPattern Recognition, 1994.\\n[34] A. Pentland, T. Starner, N. Etcoff, A. Masoiu, O. Oliyide, and M. Turk,\\n“Experiments with eigenfaces,” in Proc. Looking at People Wkshp, Int.\\nJoint Conf. Artiﬁcial Intell. , Chamberry, France, 1993.\\n[35] D. J. Perret, E. T. Rolls, and W. Caan, “Visual neurones responsive\\nto faces in the monkey temporal cortex,” Experimental Brain Res., vol.\\n47, pp. 329–342, 1982.\\n[36] Y. Y. Qi and B. R. Hunt, “Signature veriﬁcation using global and grid\\nfeatures,” Pattern Recognition, vol. 27, no. 12, pp. 1621–1629, Dec.\\n1994.\\n[37] H. A. Rowley, S. Baluja, and T. Kanade, “Human face detection in\\nvisual scenes,” School Comput. Sci., Carnegie Mellon Univ., Pittsburgh,\\nPA, Tech. Rep. CMU-CS-95-158, July 1995.\\n[38] F. S. Samaria, “Face recognition using hidden Markov models,” Ph.D.\\ndissertation, Trinity College, Univ. Cambridge, Cambridge, U.K., 1994.[39] F. S. Samaria and A. C. Harter, “Parameterization of a stochastic model\\nfor human face identiﬁcation,” in Proc. 2nd IEEE Wkshp. Applicat.\\nComput. Vision, Sarasota, FL, 1994.\\n[40] K.-K. Sung and T. Poggio, “Learning human face detection in cluttered\\nscenes,” in Computer Analysis of Images and Patterns , G. Goos, J.\\nHartmonis, and J. van Leeuwen, Eds. New York: Springer–Verlag,\\n1995, pp. 432–439.\\n[41] K. Sutherland, D. Renshaw, and P. B. Denyer, “Automatic face recog-\\nnition,” in Proc. 1st Int. Conf. Intell. Syst. Eng . Piscataway, NJ: IEEE\\nPress, 1992, pp. 29–34.\\n[42] D. L. Swets and J. J. Weng, “Using discriminant eigenfeatures for image\\nretrieval,” IEEE Trans. Pattern Anal. Machine Intell. , to appear, 1996.\\n[43] M. Turk and A. Pentland, “Eigenfaces for recognition,” J. Cognitive\\nNeurosci. , vol. 3, pp. 71–86, 1991.\\n[44] J. Weng, N. Ahuja, and T. S. Huang, “Learning recognition and\\nsegmentation of 3-D objects from 2-D images,” in Proc. Int. Conf.\\nComput. Vision, ICCV 93 , 1993, pp. 121–128.\\n[45] L. Wiskott, J.-M. Fellous, N. Kr¨ uger, and C. von der Malsburg, “Face\\nrecognition and gender determination,” in Proc. Int. Wkshp. Automat.\\nFace Gesture Recognition, Z¨urich, Switzerland, 1995.\\nSteve Lawrence (M’96) received the B.Sc. and\\nB.Eng. degrees summa cum laude in 1993 from\\nthe Queensland University of Technology, (QUT)\\nAustralia. He submitted his Ph.D. thesis in August\\n1996 at the University of Queensland, Australia.\\nHe is presently working as a Scientist at the NEC\\nResearch Institute in Princeton, NJ. His research in-terests include neural networks, pattern recognition,\\nintelligent agents, and information retrieval.\\nHis awards include a university medal, a QUT\\naward for excellence, ATERB and APA priority\\nscholarships, QEC and Telecom Australia Engineering prizes, and three prizes\\nin successive years of the Australian Mathematics Competition.\\nC. Lee Giles (S’80–M’80–SM’95) received the\\nB.A. degree from Rhodes College, Memphis, TN,\\nthe B.S. degree in engineering physics from the\\nUniversity of Tennessee, Knoxville, in 1969, theM.S. degree in physics from the University of\\nMichigan,AnnArbor,in1974,andthePh.D.degree\\nin optical sciences from the University of Arizona,\\nTucson, in 1980.\\nPreviously, he was a Program Manager at the\\nAir Force Ofﬁce of Scientiﬁc Research in Washing-\\nton, D.C. where he initiated and managed research\\nprograms in neural networks and in optics in computing and processing.\\nBefore that, he was a Research Scientist at the Naval Research Laboratory,Washington, D.C., and an Assistant Professor of Electrical and Computer\\nEngineering at Clarkson University. During part of his graduate education\\nhe was a Research Engineer at Ford Motor Scientiﬁc Research Laboratory.\\nHe is a Senior Research Scientist in Computer Sciences at NEC Research\\nInstitute, Princeton, NJ, and an Adjunct Associate Professor at the Institute for\\nAdvancedComputerStudiesattheUniversityofMaryland,CollegePark,MD.\\nHis research interests include neural networks, machine learning, and artiﬁcial\\nintelligence; hybrid systems and integrating neural networks with intelligent\\nsystems and agents; applications of intelligent methods to signal processing,communications, computing, networks, time series, pattern recognition, and\\noptical computing and processing. He has published more than 80 journal and\\nconference papers and book chapters in these areas.\\nDr. Giles serves on many related conference program committees and has\\nhelpedorganizemanyrelatedmeetingsandworkshops.Hehasbeenanadvisor\\nand reviewer to government and university programs in both neural networks\\nand in optical computing and processing. He has served or is currently servingon the editorial boards of IEEE T\\nRANSACTIONS ON KNOWLEDGE AND DATA\\nENGINEERING , IEEE T RANSACTIONS ON NEURALNETWORKS,JournalofParallel\\nand Distributed Computing, Neural Networks, Neural Computation, Optical\\nComputing and Processing, Applied Optics, and Academic Press. He is a\\nmember of AAAI, ACM, INNS, and OSA.\\nLAWRENCE et al.: FACE RECOGNITION 113\\nAh Chung Tsoi (S’70–M’72–M’84–SM’90), was\\nborn in Hong Kong. He received the Higher\\nDiploma in electronic engineering, Hong Kong\\nTechnical College, in 1969, and the M.Sc. degree\\nin electronic control engineering and the Ph.D.\\ndegree in control engineering from the Universityof Salford in 1970 and 1972, respectively.\\nFrom 1972 to 1974 he worked as Senior\\nResearch Fellow at the Inter-University Institute of\\nEngineering Control, University College of North\\nWales, Bangor, Wales. From 1974 to 1977, he\\nworked as a Lecturer at the Paisley College of Technology, Renfrewshire,\\nScotland. From 1977 to 1984, he worked as a Senior Lecturer at theUniversity of Auckland, New Zealand. From 1985 to 1990, he worked\\nas a Senior Lecturer at the University College, University of New South\\nWales. He is presently a Professor in Electrical Engineering at the University\\nof Queensland, Australia. His research interests include aspects of neural\\nnetworks and their application to practical problems, adaptive signal\\nprocessing, and adaptive control.\\nAndrew D. Back (M’96) received the B.Eng. de-\\ngree with distinction from Darling Downs Institute\\nof Advanced Edutcation (now the University of\\nSouthern Queensland), Australia, in 1985, and the\\nPh.D. degree from the University of Queensland,\\nAustralia, in 1992.\\nHe was a Research Fellow with the Defence\\nScience and Technology Organization (DSTO) from\\n1988 to 1992, and a Research Scientist with the\\nDSTO in 1993. In 1995 he was a Visiting Scientist\\nat the NEC Research Institute, Princeton, NJ. He\\nis currently a Frontier Researcher with the Brain Information Processing\\nGroup in the Frontier Research Program at the Institute of Physical andChemical Research (RIKEN) in Japan. He has published about 30 papers.\\nHis research interests include nonlinear models for system identiﬁcation, time\\nseries prediction, and blind source separation.\\nDr. Back was awarded an Australian Research Council (ARC) Postdoctoral\\nResearchFellowshipintheDepartmentofElectricalandComputerEnginering\\nat the University of Queensland for the period from 1993–1996. He has orga-\\nnized workshops for the Neural Information Processing Systems Conference\\nin 1994, 1995, and 1996 in area of neural networks for signal processing.\\n',\n",
       " 'Facial Emotion Detection Using Deep Learning\\nAkriti Jaiswal, A. Krishnama Raju, Suman Deb\\nDepartment of Electronics Engineering\\nSVNIT Surat, India\\n{nainajaiswal96, krishnamraju995 }@gmail.com, sumandeb@eced.svnit.ac.in\\nAbstract —Human Emotion detection from image is one of\\nthe most powerful and challenging research task in social\\ncommunication. Deep learning (DL) based emotion detectiongives performance better than traditional methods with image\\nprocessing. This paper presents the design of an artiﬁcial\\nintelligence (AI) system capable of emotion detection through\\nfacial expressions. It discusses about the procedure of emotiondetection, which includes basically three main steps: face detec-\\ntion, features extraction, and emotion classiﬁcation. This paper\\nproposed a convolutional neural networks (CNN) based deeplearning architecture for emotion detection from images. The\\nperformance of the proposed method is evaluated using two\\ndatasets Facial emotion recognition challenge (FERC-2013) andJapaness female facial emotion (JAFFE). The accuracies achieved\\nwith proposed model are 70.14 and 98.65 percentage for FERC-\\n2013 and JAFFE datasets respectively.\\nIndex T erms —Artiﬁcially intelligence (AI), Facial emotion\\nrecognition (FER), Convolutional neural networks (CNN), Rec-\\ntiﬁed linear units (ReLu), Deep learning (DL).\\nI. I NTRODUCTION\\nEmotion is a mental state associated with the nervous sys-\\ntem associated with feeling, perceptions, behavioral reactions,and a degree of gratiﬁcation or displeasure [1]. One of thecurrent application of artiﬁcial intelligence (AI) using neuralnetworks is the recognition of faces in images and videos for\\nvarious applications. Most techniques process visual data and\\nsearch for general pattern present in human faces in images orvideos. Face detection can be used for surveillance purposes bylaw enforcers as well as in crowd management. In this paper,we present a method for identifying seven emotions such as\\nanger, disgust, neutral fear, happy, sad, and surprise using fa-\\ncial images. Previous research used deep-learning technologyto create models of facial expressions based on emotions toidentify emotions [2]. The typical human computer interaction(HCI) lacks users emotional state and loses a great deal ofinformation during the process of interaction. Comparatively,\\nusers are more efﬁcient and desired by emotion-sensitive HCI\\nsystems [3]. Now a days, interest in emotional computing hasincreased with the increasing demands of leisure, commerce,physical and psychological well being, and education relatedapplications. Because of this, several products of emotionallysensitive HCI systems have been developed over the past\\nseveral years, although the ultimate solution for this researchﬁeld has not been suggested [4].II. R\\nELATED WORK\\nA. Human Facial Expressions\\nThe universality of facial expressions and the body language\\nis a key feature of human interaction. Charles Darwin al-ready published on globally common facial expressions in thenineteenth century, which play an important role in nonverbalcommunication [4]. In 1971, Ekman Friesen declared facialbehaviors to be correlated uniformly with speciﬁc emotions[5]. Apparently humans but also animals, produce speciﬁcmuscle movements that belong to a certain mental state. Peopleinterested in research on emotion classiﬁcation via speechrecognition are referred to Nicholson et al. [6].\\nB. Image Classiﬁcation T echniques\\nImage classiﬁcation system generally consists of feature\\nextraction followed by classiﬁcation stage. Fasel and Luet-tin provided an extensive overview of the analytical featureextractors and neural network approaches for recognition offacial expression [7]. It may be concluded that both approacheswork approximately equally well by the time of writing, atthe beginning of the twenty-ﬁrst century. However, given thecurrent availability of training data and computational power,the expectation is that the performance of models based onneural networks can be substantially improved. Several recentmilestones are set out below.\\n•Krizhevsky and Hinton give a landmark publication onthe automatic image classiﬁcation in general [8]. Thiswork shows a deep neural network that resembles the hu-man visual cortex’s functionality. A model to categorizeobjects from pictures is obtained using a self-developedlabeled array of 60,000 images over 10 classes, usingthe CIFAR-10 dataset. Another important outcome of theresearch is the visualization of the ﬁlters in the network,so that how the model breaks down the images can beassessed.\\n•In 2010, the launch of the annual Imagenet challenges [9]boosted work on the classiﬁcation of images, and sincethen the belonging gigantic collection of labeled data isoften used in publications . In a later work by Krizhevskyet al. [10], the ImageNet LSVRC-2010 contest trains anetwork of 5 convolutional, 3 max pooling, and 3 fullyconnected layers with 1.2 million high-resolution images.\\n•In particular, with regard to facial expression recogni-tion, Lv et al. [11] present a network of deep beliefs2020 International Conference for Emerging Technology (INCET) \\nBelgaum, India. Jun 5-7, 2020\\n978-1-7281-6221-8/20/$31.00 ©2020 IEEE 1\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. \\nprimarily for use with the JAFFE and expanded Cohn-\\nKanade (CK+) databases. The results are comparable tothe accuracy obtained 95 percentage on the same databaseby other methods, such as support vector machine (SVM)and learning vector quantization (LVQ).\\n•The dataset used now is the Facial Expression Recog-nition Challenge (FERC-2013) [2], organized deep net-work, obtained an average accuracy of 67.02 percentageon emotion classiﬁcation.\\nIII. P\\nROPOSED MODEL\\nA. Emotion Detection Using Deep Learning\\nIn this paper we use the deep learning (DL) open library\\n“Keras” provided by Google for facial emotion detection, byapplying robust CNN to image recognition [12]. We usedtwo different datasets and trained with our proposed networkand evaluate its validation accuracy and loss accuracy. Imagesextracted from given dataset which have facial expressions forseven emotions, and we detected expressions by means of anemotion model created by a CNN using deep learning. Wehave changed a few steps in CNN as compared to previousmethod using a keras library given by Google and alsomodiﬁed CNN architectutre which give better accuracy . Weimplemented emotion detection using keras with the proposednetwork.\\nB. CNN Architecture\\nThe networks are program on top of keras, operating\\non Python, using the keras learn library. This environmentreduces the code’s complexity, since only the neuron layers\\nneed to be formed, rather than any neuron. The software\\nalso provides real-time feedback on training progress andperformance, and makes the model after training easy tosave and reuse. In CNN architecture initially we have toextract input image of 48*48*1 from dataset FERC-2013.The network begins with an input layer of 48 by 48 which\\nmatches the input data size parallelly processed through two\\nsimilar models that is functionality in deep learning, andthen concatenated for better accuracy and getting features ofimages perfectly as shown in Fig.1 which is our proposedmodel, Model-A . There are two submodels for the extraction\\nof CNN features which share this input and both have\\nsame kernel size. The outputs from these feature extraction\\nsub-models are ﬂattened into vectors and concatenated intoone long vector matrix and transmitted to a fully connectedlayer for analysis before a ﬁnal output layer allows forclassiﬁcation.This models contains convolutional layer with 64 ﬁlters each\\nwith size of [3*3], followed by a local contrast normalization\\nlayer, maxpooling layer, followed by one more convolutionallayer, max pooling, ﬂatten respectively. After that weconcatenate two similar models and linked to a softmaxoutput layer which can classify seven emotions. We use\\ndropout of 0.2 for reducing over-ﬁtting. It has been appliedto the fully connected layer and all layers contain units ofrectiﬁed linear units (ReLu) activation function.First we are passing our input image to convolutional layer\\nwhich consists of 64 ﬁlters each of size 3 by 3, after thatit passes through local contrast normalization can removeaverage from neighbourhood pixels leads to get quality offeature maps, followed by ReLu activation function. Maximumpooling is used to reduce spatial dimension reduction soprocessing speed will increase. We are using concatenation forgetting features of images (eyes, eyebrows, lips, mouth etc)perfectly so that prediction accuracy improved as comparedto previous model. Furthermore, it is followed by fullyconnected layer and softmax for classifying seven emotions.A second layer of maxpooling is added to reduce the numberof dimensionality. Here, we use batch normalization, dropout,ReLu activation function, categorical cross entropy loss,adam optimizer, softmax activation function in ouput layerfor seven emotion classiﬁcation.\\nIn JAFEE dataset, input image size is adjusted to that\\n128*128*3. The network starts with an input layer of 128by 128 which matches the input data size parallely processedthrough two similar models as shown in Fig.1. Furthermore,it is concatenated and pass through one more softmax layerfor emotion classiﬁcation and all procedure is same as above.\\nInModel-B , previously proposed by Correa et al. [2], the\\nnetwork starts with a 48 by 48 input layer, which matchesthe size of the input data. This layer is preceded by oneconvolutional layer, a local contrast normalization layer, andone layer of maxpooling, respectively. Two more convolutionallayers and one fully connected layer, connected to a softmaxoutput layer, complete the network. Dropout has been appliedto the fully connected layer and all layers contain units of\\nReLu.\\nIV . E\\nXPERIMENT DETAILS\\nWe develop a network based on the concepts from [12],\\n[13] and [14] to assess the two models ( Model-A and Model-\\nB) mentioned above on their emotion detection capability.\\nThis section describes the data used for training and testing,explains the details of the used data sets and evaluates theresults obtained using two different datasets with two models.\\nA. Datasets\\nNeural networks, and particularly deep networks, needs\\nlarge amounts of training data. In addition, the choice ofimages used for the training is responsible for a large part ofthe eventual model’s performance. It means the need for a dataset that is both high quality and quantitative. Several datasetsare available for research to recognize emotions, ranging froma few hundred high resolution photos to tens of thousandsof smaller images. The two, we will be debating in thiswork, are the Japanese Female Face Expression (JAFFE) [15],Facial Expression Recognition Challenge (FERC-2013) [16]which contains seven emotions like anger, surprise, happy, sad,disgust, fear, neutral.\\n2\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. \\nFig. 1: Network Architecture\\nThe datasets primarily vary in the amount, consistency,\\nand cleanness of the images. For example, the FERC-2013collection has about 32,000 low-resolution images. It can alsobe noted that the facial expressions in the JAFFE (i.e. further\\nextended as CK+ ) are posed (i.e. clean), while the FERC-2013\\nset displays “in the wild” emotions. This makes it harder tointerpret the images from the FERC 2013 set, but given thelarge size of the dataset, a model’s robustness can be beneﬁcialfor the diversity.\\nB. Training Details\\nWe train the network using GPU for 100 epochs to ensure\\nthat the precision converges to the optimum. The network willbe trained on a larger set than the one previously describedin an attempt to improve the model even more. Training willtake place with 20,000 pictures from the FERC-2013 datasetinstead of 9,000 pictures. The FERC-2013 database also usesnewly designed veriﬁcation (2000 images) and sample sets(1000 images). It shows number of emotions in the ﬁnal testingand validation set after training and testing our model. Theaccuracy will be higher on all validation and test sets than inprevious runs, emphasizing that emotion detection using deepconvolutional neural networks can improve the performanceof a network with more information.C. Results using Proposed Model\\nIn emotion detection we are using three steps, i.e., face\\ndetection, features extraction and emotion classiﬁcation usingdeep learning with our proposed model which gives betterresult than previous model. In the proposed method, computa-tion time reduces, validation accuracy increases and loss alsodecreases, and further performance evaluation achieved which\\ncompares our model with previous existing model. We testedour neural network architectures on FERC-2013 and JAFFE\\ndatabase which contains seven primary emotions like sad, fear,happiness, angry, neutral, surprised, disgust.\\nFig.2 shows the proportions of detected emotions in a single\\nimage of FER dataset. Fig.2(a) shows the image, whereasthe detected emotion proportions are shown in Fig.2(b). It\\nis clearly observable that neutral has higher proportion than\\nother emotions. That means, the emotion detected for thisimage (in Fig.2(a)) is neutral. Similarly, Fig.3 show anotherimage and corresponding emotion proportions. From Fig.3(b),it is observable that happy emotion has higher proportion thanothers. That suggests that image of Fig.3(a) detects happy\\nemotions.\\nSimilarly, performance is evaluated for all the test images\\nof the dataset. We have achieved 95 percentage for happy, 75\\n3\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. \\nFig. 2: (a) Image, (b) Proportion of emotions.\\nFig. 3: (a) Image, (b) Proportion of emotions.\\npercentage for neutral, 69 percentage for sad, 68 percentage\\nfor surprise, 63 percentage for disgust, 65 percentage for fearand 56 percentage for angry. On an average we are gettingaverage accuracy of 70.14 percentage using our proposedmodel.\\nThe confusion matrix of classiﬁcation accuracy is shown in\\nTABLE I. We get an average validation accuracy of 70.14percentage using our proposed model in facial emotiondetection using FER dataset.\\nFig.4 shows the result of test sample related to surprise\\nemotion from JAFFE dataset, and our proposed model alsopredicted the same emotion with reduced computation timeas compared to previous existing model B. Similarly, perfor-\\nmance is evaluated for all the test samples of JAFFE dataset.When we are using JAFFE dataset we are getting validationaccuracy of 98.65 percentage which is better than previous\\nresult and it takes less computational time per step.\\nFig. 4: Prediction of emotion.\\nV . PERFORMANCE EV ALUATION\\nIn FER dataset we train on 32,298 samples which is validate\\non 3589 samples, and in JAFFE dataset we train 833 samples,which is validate on 148 samples for calculation of validationaccuracy, validation loss, computational time per step upto to\\n4\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. \\nTABLE I: Confusion Matrix ( %) for emotion detection using proposed model\\nEmotions Angry Sad Happy Disgust Fear Neutral Surprise\\nAngry 56 12 3 9 8 11 1\\nSad 10 69 2 6 9 2 2\\nHappy 0 0 95 0 0 3 2\\nDisgust 7 13 0 63 8 5 4\\nFear 9 8 3 2 65 10 3\\nNeutral 2 1 8 1 7 75 6\\nSurprise 7 3 11 0 3 8 68\\nAverage accuracy = 70.14 ( %)\\nTABLE II: Qualitative assessment of our proposed model for emotion detection\\nModel (Dataset) Validation accuracy ( %) Validation loss Computation time per step (msec.)\\nProposed Model A (FERC-2013) 70.14 1.7577 16 msec.\\nModel B (FERC-2013) 67.02 2.0389 45 msec.\\nProposed Model A (JAFFE) 98.65 0.1694 284 msec.\\nModel B (JAFFE) 97.97 0.1426 462 msec.\\n100 and 50 epochs respectively shown in TABLE II. The aim\\nof the training step is to determine the correct conﬁgurationparameters for the neural network which are: number of nodesin the hidden layer (HL), rate of learning (LR), momentum\\n(Mom), and epoch (Ep). Different combinations of theseparameters have been tested to ﬁnd out how to achieve thebetter recognition rate.From Table II, it is observed that our proposed model shows\\n70.14%average accuracy compared to the 67.02 %average\\naccuracy reported in model B FOR FER dataset. In this case ofJAFFE database, we achieved average accuracy 98.65 %which\\nis also higher than model B.\\nVI. C\\nONCLUSION\\nIn this paper, we have proposed a deep learning based\\nfacial emotion detection method from image. We discussour proposed model using two different datasets, JAFFE andFERC-2013. The performance evaluation of the proposed\\nfacial emotion detection model is carried out in terms ofvalidation accuracy, computational complexity, detection rate,learning rate, validation loss, computational time per step. We\\nanalyzed our proposed model using trained and test sampleimages, and evaluate their performance compare to previous\\nexisting model. Results of the experiment show that the modelproposed is better in terms of the results of emotion detectionto previous models reported in the literature. The experimentsshow that the proposed model is producing state-of-the-art\\neffects on both two datasets.\\nR\\nEFERENCES\\n[1] S. Li and W. Deng, “Deep facial expression recognition: A survey,”\\narXiv preprint arXiv:1804.08348, 2018.[2] E. Correa, A. Jonker,M.Ozo, andR.Stolk, “Emotion recognition using\\ndeep convolutional neural networks,” Tech. Report IN4015, 2016.\\n[3] Y . I. Tian, T. Kanade, and J.F.Cohn, “Recognizing action units for facial\\nexpression analysis,” IEEE Transactions on pattern analysis and machine\\nintelligence, vol. 23, no. 2, pp. 97–115, 2001.\\n[4] C. R. Darwin. The expression of the emotions in man and animals. John\\nMurray, London, 1872.\\n[5] P. Ekman and W. V . Friesen. Constants across cultures in the face and\\nemotion. Journal of personality and social psychology, 17(2):124, 1971.\\n[6] J. Nicholson, K. Takahashi, and R. Nakatsu. Emotion recognition in\\nspeech using neural networks. Neural computing applications, 9(4):\\n290–296, 2000.\\n[7] B. Fasel and J. Luettin. Automatic facial expression analysis: a survey.\\nPattern recognition, 36(1):259–275, 2003.\\n[8] A. Krizhevsky and G. Hinton. Learning multiple layers of features from\\ntiny images, 2009.\\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A\\nlarge-scale hierarchical image database. In Computer Vision and Pattern\\nRecognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255.\\nIEEE, 2009.\\n[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. In Advances in neural infor-\\nmation processing systems, pages 1097–1105, 2012.\\n[11] Y . Lv, Z. Feng, and C. Xu. Facial expression recognition via deep\\nlearning. In Smart Computing (SMARTCOMP), 2014 InternationalConference on, pages 303–308. IEEE, 2014.\\n[12] TFlearn. Tﬂearn: Deep learning library featuring a higher-level api for\\ntensorﬂow. URL http://tﬂearn.org/.\\n[13] Open Source Computer Vision Face detection using haar cascades. URL\\nhttp://docs.opencv.org/master/d7/d8b/tutorialpyfacedetection.html.\\n[14] P. J. Werbos et al., “Backpropagation through time: what it does and\\nhow to do it,” Proceedings of the IEEE, vol. 78, no. 10, pp. 1550–1560,\\n1990.\\n[15] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I.\\nMatthews. The extended cohn-kanade dataset (ck+): A complete dataset\\nfor action unit and emotion-speciﬁed expression. In Computer Visionand Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer\\nSociety Conference on, pages 94–101. IEEE, 2010.\\n[16] Kaggle. Challenges in representation learning: Facial expression recog-\\nnition challenge, 2013.\\n5\\nAuthorized licensed use limited to: University of Wollongong. Downloaded on August 13,2020 at 17:55:09 UTC from IEEE Xplore.  Restrictions apply. \\n',\n",
       " '2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zak ho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n70 \\n 978-1-5386 -9343 -8/19/$31.00 ©2019 IEEE  Facial Expression C lassification Based on  SVM, \\nKNN and MLP  Classifiers  \\nHivi Ismat Dino  \\nDept. of Computer Science  \\nUniversity of Zakho  \\nDuhok, Iraq  \\nhivi.dino@gmail.com  Maiwan Bahjat Abdulrazzaq  \\nDept. of Computer Science  \\nUniversity Of Zakho  \\nDuhok,Iraq  \\nmaiwan. abdulrazzaq@uoz.edu.krd\\n \\nAbstract—Facial Expression Recognition (FER) has been an \\nactive topic of papers that were researched during 1990s till now, \\naccording to its importance, FER has achieved an extremely role \\nin image processing area. FER typically pe rformed in three \\nstages include, face detection, feature extraction and \\nclassification. This paper presents an automatic system of face \\nexpression recognition  which is able to  recognize all eight  basic \\nfacial expressions which are (normal, h appy, angry,  contempt,  \\nsurprise, sad, fear and disgust) while many FER systems were \\nproposed for recognizing only some of face expressions. F or \\nvalidating the method, the Extended Cohn -Kanade (CK+) \\ndataset is used. The presented  method uses Viola -Jones \\nalgorithm fo r face  detection. Histogram of Oriented G radients \\n(HOG) is used as a descriptor for feature extraction from the \\nimages of expressive faces. Principal Component A nalysis (PCA) \\napplied to reduce dimensionality of the Features, to obtaining the \\nmost significant fea tures. Finally, the presented method used  \\nthree different classifiers which are Support Vector Machine \\n(SVM), K -Nearest N eighbor (KNN) and Multilayer Perceptron \\nNeural N etwork (MLPNN) for classifying the facial expressions \\nand the results of them are compa red. The experimental  results \\nshow that the presented method provides  the recognition rate \\nwith 93.53 % when using SVM classifier, 82.97% when using \\nMLP classifier and 79.97% when using KNN classifier which \\nrefers that the presented method provides better r esults while \\nusing SVM as a classifier . \\n Keywor ds—face expression recognition, histogram of \\noriented gradients , principle component analysis, support \\nvector machine, k -nearest neighbor, multi -layer perceptron  \\nI. INTRODUCTION  \\nFacial recognition has been one o f the significant and \\ninterest areas in the field of secu rity and biometric since \\n1990s, naturally humans communicate their opinions with \\nothers, also can see reactions through emotions and intentions, \\ntherefore it is important in natural communication  [1]. Several \\nadvance d research  produced in terms of face tracking and \\ndetection, the mechanisms of feature extraction and the \\ntechniques of expression classification  [2]. Human have an \\nexceptionally face recognition system, the brain of human is \\nso sophisticated in such a way that it is able to recognize \\ndifferent faces with very f ast changes in appearance. \\nResearchers have dependa bly been stunned by the capacity of \\nhuman brain to recognize face in  multiple different situations, \\nmany efforts were made to reproduce the human brain system. \\nBased on this idea, many algorithms were produced for face \\nrecognition  and Face detection  [3].  The most pop ular approache s doing facial expression \\nanalysis are Support Vector M achine  (SVM ) [4], Artificial \\nNeural N etwork ( ANN ) [5], Active Appeara nce Model \\n(AAM) [6] and etc . In this paper  the utilized  classifiers are  \\nSVM, Multilayer Perceptron (MLP) [7] and KNN  [8]. SVM is \\na good algorithm that used to create a model for Facial \\nExpre ssion classification, it tries  to find a hyper plane in an N -\\ndimensional space that separate the classes of data points , with \\naim to find a plane that has the maximum margin  separation, \\nthe productivity of recognition relies on th e me thod of feature \\nextraction [9]. MLP is one of the most common technique of \\nneural network which is based on backpropagation algorithm \\n[10]. KNN is a non-parametric  method in pattern recognition, \\nit is the simplest algorithm among all machine learning \\nalgorithms [11]. \\nThere are three approaches for recognizing faces with \\ndifferent face expressions  [12] which are : feature based -\\ntechniques techniques, model -based techniques  [13] and \\ncorrelat ion-based techn iques.  Feature -based technique is \\nclassified into geometric feature -based, appearance fea ture-\\nbased and movement feature -based method . The most \\ncommon used approaches of feature -based techniques are \\ngeometric based and appearance -based metho ds. In the \\ngeometric al feature -based technique, geometric parameters \\nsuch as curves, pointers …etc., which are used to represent the \\nimage locating different face parts like eyes, ears, nose, \\nmouth … etc. Also, by estimating their relative width, position , \\nthe geometric shapes construct ed for representing these \\nparameters. According to the method of appearance based , the \\nimportance is disposed to pixel values instead of the shape of \\nfeature components or relative distance. In this method there \\nare many differ ent parameters of pixels used such as \\nhistogram, intensity  and etc. The corresponding values are \\nperformed in a template as a 2D array. The combination of \\nthese two approaches is performed in some algorithms for \\ngetting a good rate of recognition . \\nThere ar e three main steps for Face expression recognition: \\n(1) face detection, (2) feature extraction, (3)  expression \\nrecognition. Different alg orithms are used for each step.  For \\nvalidating the Face Expression Recognition (FER) many \\ndataset were used such as Coh n Kanade (CK), Extended Cohn \\nKanade (CK+), Yale, Jaffe and many other.  In this paper CK+ \\ndataset is used to validate the presented method due to its been \\none of the most common dataset for FER system, it released in \\n2000 for the purpose to promote the rese arch in to an \\nautomatic system of expression detection [14]. In FER system, \\n2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n71 \\n Face detection  [15] is considered as a first step for any \\ntechniques of  image processing. This step is required to \\nnormalize our data , in other words the image we get has been \\ntaken under different requirements and conditions using \\ndifferent equipment. The image is varying in size and also has \\nbeen affected by many disruptions such as backgrounds, noise, \\nillumination variations …etc. thus; the first step is image \\nstandardization in FER whic h includes enhancement, resizing, \\nnoise removal of the input image. These steps are done in \\npreprocessing stage. In addition, many techniques of FER \\nutilize the gray scale images instead of color images, this \\nconversion also counts as a preprocessing step.  Precisely, in \\npreprocessing the images are standardized. The most prevalent \\ntechniques utilized for face detection are Viola -Jones  [16], \\nEdge detection, Kirsch, Laplaci an, Sobel, Canny, prewitt etc.  \\nIn this work we used Viola -Jones algorithm for face \\ndetection, which was proposed by Paul V iola and  Michael \\nJones in 2001 [17]. Generally, dimension reduction loses \\ninformation, PCA tends to reduce that  information loss. PCA \\nis a standard method utilized in signal processing and \\nstatistical pattern recognition for data reduction. Most of times \\nthe pattern contains much redundant information, PCA used to \\nmap it in to a feature vector in order to get rid f rom redundant \\ninformation and obtain only important information. The role \\nof those extracted features is to identify the input patterns.  \\nHistogram of Oriented Gradients (HOG) is a n image shape \\ndescriptor that can counts how many time the occurrence of \\ngradient orientations in special portions of an image and \\nwhich used for th e principle of object detection , also by \\ninstinct useful to form the shape of the facial muscles by edge \\nanalysis  [18].  Principle Component Analysis ( PCA ) method \\nused widely to reduce the number of features in  image in order \\nto obtain an available covering image that is coverin g about \\n99% of the varia tion [19]. PCA technique is applied  in this \\npaper to reduce the dimensionality of HOG features. Finally,  \\nSVM, MLP and KNN methods are utilized as a classifier for \\nexpression r ecognition.  \\nThe main goal of the presented method is to recognize all \\neight  facial expressions which are (normal, happy, sad, angry, \\ncontempt, surprised, fear and disgust) while many modern \\nFER system were produced for recognizing only s ome face \\nexpression s. The presented research focused on the \\ninvestigation techniques that are able in  increasing  both the \\ncomputational efficiency and recognition accuracy by \\napplying va rious classification techniques and comparing their \\nresults.  \\nSection II of the paper give s an overview of the related \\nwork. Section III describes the steps of proposed system. \\nSection IV presents the experimental results and discussion. \\nFinally, section V shows the conclusion of the paper.  \\nII. RELATE WORKS  \\nReference  [20] has proposed a novel me thod of FER using \\nequable Principle Component Analysis (EPCA) as a \\nrepresentation of expression features and Linear Regression \\nClassification (LRC) [21] is implement ed as a classifiers for \\nexpressions. EPCA deals with the original image by \\npreserving the useful information and reducing the feature vector’s dimension of data. LRC compacts as a linear \\nregression problem with the face recognition problem. In this \\npaper t he reproduction experiment was commanded on the \\nYale and JAFFE Database of face. The authors proposed the \\nmethod of PCA  for extracting the quantity of the face feature \\nthat has a great ability in improving the generalization \\nperformance and the accuracy of  the feature vector. According \\nto this paper the proposed method is the improved method of \\nPCA that works through changing the feature vectors for \\nproducing  the transformation of high dimensional data in to \\nthe low dimensional data. Each class has an avera ge value that \\nis an image of linear increase. The authors found that the \\nclassifier of LRC method is very effective for identifying the \\nother expressions . The recognition rate gained 89.1% on Yale \\ndatabase which is applied on five expressions while t he \\nrecognition rate gained 91.1 % on JAFFE database which \\napplied on eight expressions.  \\nReference [22] presented a FER system to recognize six \\nface emotions. It utilized th e method of cascade regression \\ntree for feature extraction and used three machine learning \\nalgorithms for classification which are SVM, NN and logistic \\nregression and compared their results. The presented method \\napplied on CK+ dataset. The recognition accu racy gained \\n77.06% while using logistic regression classifier, 80% while \\nusing NN classifier and 89% on SVM.  \\nReference [23] described both PCA and kernel -based PCA \\napproach in FER system for three facial expressions of 3D \\nface images.  KNN cla ssifier is used to recognize the face \\nexpressions.  Imperial College London dataset utilized for \\nvalidating the system. The experimental results show the \\ndemonstration of kernel PCA being outperforming PCA by \\nobtaining 77.29% of rate while PCA obtained on ly 52.69% of \\nrecognition rate.  \\nReference  [24] proposed a new recognition method of face \\nexpression recognition using the combination of Local \\nDirectional Number Pattern (LDN) and Loca l Tetra Pattern \\n(LTrP). This method uses Modified Decision Based \\nUnsymmetrical Median Filter (MDBUTMF) for noise \\nremoval. LDN descriptor is organized based on the descriptor \\nof eight Gaussian edge. The authors used the Japanese female \\nfacial expression dat abase , the used expressions are (disgust, \\nsmile, sad and surprise). The proposed method encrypts the \\nrelationship between the indicated pixel and its neighbors. \\nLDPN utilizes the stable directional information against noise \\nrather than intensity for coding  the various patterns from the \\ntexture of face. LTrP technique uses the center pixel direction \\nto characterize the spatial frame of the local texture. LTrP \\nmethod encrypts the image based on the pixel direction which  \\nis calculated by vertical and horizonta l derivatives. To display \\nthe proposed face expressions, the proposed technique goes \\nthrough mask generation and edge detection of Gaussian; also \\nafter processing LDN and LTrP process, MLDN histogram \\nand LTrP histogram are calculate for training up the fac e \\nexpressions by using the SVM classifier . In experimental \\nresults, it shows that the accuracy of FER was higher than \\n90%.  \\nIn [25] the authors suggested a dynamic feature -based \\nmethod of facial expression for video of face sequences  based \\n2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n72 \\n on a low dimensional feature space through extracting a \\nPyramid representation of uniform Temporal Local binary \\nPattern (PTLBPu2)using orthogonal planes (XT and YT). The \\nproposed method is then used for selecting the most \\ndiscriminating sub regions.  Feature space is reduced that is \\nabout to be intended on low -dimensional feature space \\nthorough applying the PCA method. C4.5 algorithm and SVM \\nclassifier is used for facial expression classification. \\nExperiments proceeded on MMI and CK+ databases which \\nare from famous databases of facial expression. The \\nexperimental result shows that the accuracy of recognition rate \\nobtained 92% with uncontrolled environment.  \\nTABLE I.  COMPARISON SUMMERY OF RELATED WO RKS \\nLiterature  Feature \\nExtraction  Classifier   \\nEmotion  \\nNo. Datase t Pros  Cons  \\nZhu et \\nal.[20]  EPCA  LRC  7 Yale \\nand \\nJAFFE  \\n Error free \\nrecognition  \\nachievement  Pollution \\nproblem in \\nface \\nrecogni tion  \\nBilkhu et \\nal.[22] Cascade \\nRegression  SVM  \\nNN \\nLogistic \\nregression  6 Ck+ Suitable for \\nreal time \\napplications  Less \\neffective \\nrecognition \\nperformance  \\nPeter et al . \\n[23] PCA and \\nKernel \\nPCA  KNN  4 Imperial  \\nCollege \\nLondon  Efficient for \\n3D face \\nrecognition  Less \\naccuracy  \\nEmmanuel \\nand Revina \\n[24]  LDN \\n+LTrP  SVM  4 JAFFE  Robust in \\nnoisy \\nremoval  Less \\nnumber of \\nrecognized \\nemotion  \\nAbdallah et \\nal. [25] LDT  \\nfeature \\nspace  SVM  6 CK+  More \\nReliable  Less \\naccuracy  \\n       \\nResearchers in the related  papers used various methods of \\nfeature extraction and classification wi th different numbers of \\nfacial expressions. Our system is able to recognize all eight  \\nbasic facial expressions while  researchers in [22-25] is able to \\nrecognize fe wer number of facial expressions and with lower \\nrecognition rate compared with our method.  Table I show the \\ncomparison between our system and others in accuracy and \\nmethods been used.  \\nIII. METHODOLOGY  \\n       In this paper, The Extended Cohn -Kanade (CK+) \\nDataset  was used. The dataset contains different emotions for \\neach person. Dataset is consisting of image sequences \\nconverted from video frame. There are many missing label \\nsequences in the dataset. The labeled emotion was considered \\nand used in ou r experiment. First image frame used as a natural \\nemotion, last image frame used as a labeled emotion in the \\ndataset.  Our work is the process for identifying the human’s \\nexpressions with eight  emotions whi ch are: neutral, anger , \\ndisgust, fear, happy, sadness and surprise . Fig .1. shows  the \\nblock diagram of the presented  method.  \\n \\nFig. 1. Block diagram for the presented method  \\nA. Preprocessing  \\n      This step is considered as a first step for any techniques of  \\nimage processing.  In this work we used Viola -Jones algorithm \\nfor face detec tion. The original images  were digitized into \\neither 640x490 or 640x480 pixel arrays with  8- bit gray -scale \\nor 24 -bit color values.   \\n                \\n  \\nFig. 2. Some emotions on detected face  \\nViola -Jones is one of the most popular face detection \\nalgorithms because o f having a robust and real time \\ncharacteristic in face detection. The algorithm goes through \\nfour stages which are Haar features selection  [26], creating an \\nintegral image, ad boost training and cascading classifiers. \\nAfter face detection, the images were normalized and resized \\nto 256 by 256 -pixel  arrays  in gray-scale . Table 2  show t he tot al \\nemotion labeled and face detected.  Fig.2. show some emotions \\non detected faces.  To calculate the Haar transform of an array \\nof n samples:  \\n\\uf0b7 Treat the array as  n/2 pairs called  (a, b)  \\n\\uf0b7 Calculate  (a + b) / sqrt(2)  for each pair, these values \\nwill be the firs t half of the output array.  \\n\\uf0b7 Calculate  (a - b) / sqrt(2)  for each pair, these values \\nwill be the second half.  \\n\\uf0b7 Repeat the process on the first half of the array.  \\n(the array length should be a power of two)  \\n2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n73 \\n TABLE II.  TOTAL DETECTED FACE E MOTIONS  \\nValue  Count  Percent  \\nneutral 317 50.0%  \\nanger  45 7.1%  \\ncontempt  18 2.8%  \\nDisgust  55 8.7%  \\nfear 24 3.8%  \\nhappy  68 10.7%  \\nsadness  26 4.1%  \\nsurprise  81 12.8%  \\nTotal  634 100.0  \\n \\nB. Feature extraction  \\nThis is the most significant stage in FER. The efficiency of \\nFER is depending on the t echniques used in this stage. The \\nmajor techniques of feature extraction are Local Binary \\nPattern (LBP), Gabor filter, Principal Component Analysis \\n(PCA) etc. In this work we used HOG algorithm for feature \\nextraction which was proposed by Dalal and Higgs  [27].  \\n                    \\n  \\nFig. 3. Extracted HOG features from a grayscale input image  \\nThe basic idea behind the HOG descriptor is that the shape \\nand appearance of local object inside the image can be \\ncharac terized by the allocation of gradients intensity or edge \\ndirections. HOG is the combination of gradient direction \\nhistograms. To improve the accuracy, local histograms are \\ncapable to be construct -normalized which is done by choosing \\nthe wider region on ima ge which is called a block and \\ncalculating its measure of intensity,  after that the produced \\nvalue is used to normalize all cells inside the block. Fig.3. \\nshows  the visualization of Extract histogra m of oriented \\ngradients (HOG).                         \\nFeatures  on face Construct -normalization produces better \\ninvariant to shadowing and illumination. HOG is a better \\ndescriptor than others because of its being operated on local \\ncells and it is static for photometric and geometric \\ntransformation inspire of bein g object orientation.  The total \\nnumber of extracted HOG features is 8100, it is hug, and \\ntherefore dimension reduction was essential.            \\nC. Feature Reduction  \\n      Generally, dimension reduction loses information; PCA \\ntends to reduce that information l oss. PCA is a standard \\nmethod utilized in signal processing and statistical pattern recognition for data reduction. Most of times the pattern \\ncontains much redundant information, PCA used to map it in \\nto a feature vector in order to get rid from redundant \\ninformation and obtain only important information. The role \\nof those extracted features is to identify the input patterns. The \\nface image of 256x256 pixel size in two -dimension produce \\n8100  HOG features, which can be considered as one -\\ndimension vector of feature . PCA  used on HOG feature to \\nreduce the dimensionality, we suggest to keep 90% of original \\ninformation. The final feature size was 247 for each sample.  \\nIV. EXPERIMENTAL RESULTS  \\nA. Dataset  \\nThe proposed method was validated using CK+ dataset, \\nCK+ dataset con sist of the facial emotions of 210 adults, they \\nwere between 18 to 50 years old, 69% of members were \\nfemale, 13% Afro -American, 81% Euro -American, and 6% \\nother gatherings. CK+ consist of 539 image sequences of \\nfacial expressions from 123 subjects, among th em 327 \\nsequences have eight  emotion labels, image’s resolution \\nis640x490 or 640x480 . After preprocessing, there were 634 \\nlabeled images with eight emotion  [14]. \\nB. Classifier  \\nIn this stage the extracted features from face images are \\nclassified using a specific classifier to the respective classes of \\nface expressions. The most widely used classifier is support \\nvector machine (SVM) and Neural Networks (NN), which are \\nused for classification. In this work we used three classifier \\nwhich they are SVM, MLP and K -nearest neighbor KNN with \\ncomparing their results.  \\nV. RESULTS  \\nIn the experiment, 10 -fold Cross -validation method is  used \\nas model evaluation . The dataset is divided into 10- subsets  \\nwith ten -time e valuation, each time the model uses nine -\\nsubsets  as a training data, and use one remained  subset in \\ntesting phase, then compute the average of the  output results \\nin ten tests. U sually  it is preferred method because it gives a \\nmodel the opportunity to train on multiple train -test splits. \\nThis gives better estimation  of how well the model will \\nperform on unseen data.   \\nTo illustrate the results of Tables 2, 3 and 4 . The first \\nattribute is the s pecificity , which is  the true negative rate of \\nthe considered class. The s pecificity  highest rate  value  is (1)  in \\ndisgust  emotion for SVM  classifier , also in fear , sadness  and \\nsurprise emotions  for K -NN classifier . The specificity  lowest \\nrate value  is (0.6309) in neutral  emotion for K -NN classifier. \\nThe second attribute is the Recall . The Recall commonly \\ncalled Sensitivity, corresponds to the true positive rate of the \\nconsider class. The Recall highest rate value i s (1) in happy  \\nemotion for K -NN classifier. The Recall lowest rate value is \\n(0.0385 ) in sadness  emotion for K -NN classifier. The third \\nattribute is the False Positives (FP) Rate  which mean  \\nincorrectly classified . The value of FP is (0) when take (1) in \\n2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n74 \\n Specificity . The Last attribute is the F-Measure  which is \\nthe harmonic mean  of precision and recall . The F-Measure  \\nhighest rate value is ( 0.9853 ) in happy  emotion for SVM \\nclassifie r while the F-Measure  lowest rate value is ( 0.0741 ) in \\nsadness  emotion for K -NN classifier.  \\nThe correct classification rate is 93.53% using the SVM \\nclassifier. The MLP correct classification rate was 82.97%, \\nand the K -NN correct classification rate was 79.97%. The \\naccuracy and performance average weights of each classifier \\nfor all emotions  are given in table 2, 3 and 4.  \\nTABLE III.  PERFORMANCE DETAILS F OR SVM  CLASSIFIER WITH 10-\\nFOLD CROSS -VALIDATION  \\nSVM accuracy = 93.89%.  \\nLabel  Specificity  Recall  FP-Rate  Precision  F-Measure  \\nneutral  0.9117  0.9874  0.0883  0.9179  0.9514  \\nanger  0.9932  0.8889  0.0068  0.9091  0.8989  \\ncontempt  0.9935  0.6111  0.0065  0.7333  0.6667  \\ndisgust  1.0000  0.9455  0.0000  1.0000  0.9720  \\nfear 0.9984  0.7083  0.0016  0.9444  0.8095  \\nhappy  0.9982  0.9853  0.0018  0.985 3 0.9853  \\nsadness  0.9984  0.6538  0.0016  0.9444  0.7727  \\nsurprise  0.9964  0.9383  0.0036  0.9744  0.9560  \\nAverage  0.9862  0.8398  0.0138  0.9261  0.8765  \\nTABLE IV.  PERFORMANCE DETAILS F OR K-NN CLASSIFIER WITH 10-\\nFOLD CROSS -VALIDATION   \\nK-NN accuracy = 79.97%  \\nLabel  Specificity  Recall  FP-Rate  Precision  F-Measure  \\nneutral  0.6309  0.9937  0.3691  0.7292  0.8411  \\nanger  0.9983  0.0667  0.0017  0.7500  0.1224  \\ncontempt  0.9984  0.1111  0.0016  0.6667  0.1905  \\ndisgust  0.9983  0.7818  0.0017  0.9773  0.8687  \\nfear 1.0000  0.1250  0.0000  1.0000  0.2222  \\nhappy 0.9876  1.0000  0.0124  0.9067  0.9510  \\nsadness  1.0000  0.0385  0.0000  1.0000  0.0741  \\nsurprise  1.0000  0.8889  0.0000  1.0000  0.9412  \\nAverage  0.9517  0.5007  0.0483  0.8787  0.5264  \\nTABLE V.  PERFORMANCE DETAILS F OR MLP  CLASSIFIER WITH 10-\\nFOLD CROSS -VALIDATION  \\nNN accuracy = 8 2.97%  \\nLabel  Specificity  Recall  FP-Rate  Precision  F-Measure  \\nneutral  0.7539  0.9811  0.2461  0.7995  0.8810  \\nanger  0.9847  0.5778  0.0153  0.7429  0.6500  \\ncontempt  0.9919  0.1111  0.0081  0.2857  0.1600  \\ndisgust  0.9983  0.7818  0.0017  0.9773  0.8687  \\nfear 0.9984  0.4583  0.0016  0.9167  0.6111  \\nhappy  0.9894  0.8676  0.0106  0.9077  0.8872  \\nsadness  0.9918  0.4231  0.0082  0.6875  0.5238  \\nsurprise  0.9946  0.7778  0.0054  0.9545  0.8571  \\nAverage  0.9629  0.6223  0.0371  0.7840  0.6799  \\n       \\nExperimental results show that our method provides be tter \\naccuracy comparing to the results in research [28] which used \\nthe same data set, it used facial landmarks to extract features \\nand applied different clas sifiers, in this research the \\ncombination of three dataset used CK, Jaffe and PSL to train and test data, the proposed method tested on two emotions and \\nobtained 65.35% of accuracy with SVM classifier, 70.87% \\naccuracy with KNN classifier. Researchers in pa per [29] \\nemployed a Deep Convolution Neural Network (DCNN)  in \\norder to extra ct deeper features for automatic emotion \\ndetection, KNN classifier and tested on their system, the \\nrecognition rate was 77.27%  on CK+ dataset. Researchers in \\n[22] obtained 80.0% and 89.0% accuracy with  SVM and  \\nneural network . With KNN classifier [23] enhanced the \\nrecognition rate to 77.29%.  \\nTABLE VI.  THE COMPARISON OF REC OGNITION RATE OF PRE SENTED \\nMETHOD WITH STATE -OF-ART % \\nLiterature  Dataset  Classifier  Accuracy  \\n[22] CK+  SVM  \\nNN 80.0%  \\n89.0%  \\n[23] CK+  KNN  77.29  \\n[28] CK+Jaffe+PSL  SVM  \\nKNN  65.35%  \\n70.87%  \\n[29] CK+  KNN  77.27%  \\nours CK+  SVM  \\nNN \\nKNN  93.53%  \\n82.57%  \\n79.97%  \\n \\nVI. CONCLUSION  \\nIn this research  FER system  based on Histograms of \\noriented gradients  (HOG) using various machine learning \\nalgorithms was presented . The Extended Cohn -Kanade (CK+) \\ndataset used as good data recourse to exam the classi fication \\nof human Facial  Expression . Principal  component analysis \\n(PCA) used to reduce the dimensionality of the features. This \\npaper presents a system with ability to recognize all eight  \\nbasic face expressions. 10-fold validation is used to train and \\ntest the classifiers. The t hree classifiers were utilized to \\nperform the classification of facial expressions , SVM, NN and \\nK-NN. The experiments results show that SVM proven to be \\nbetter classifier with 93.53 % accuracy  of correct classification \\nrate. In future,  we can use our own novel dataset which is in \\ncollecting progress and test the presented method with \\ndifferent machine learning algorithms  to provide better \\naccuracy . \\n       REFERENCES  \\n \\n[1] T. Gehrig and H. K. Ekenel, \"A common framewor k for real -time \\nemotion recognition and facial action unit detection,\" in Computer \\nVision and Pattern Recognition Workshops (CVPRW), 2011 IEEE \\nComputer Society Conference on, 2011, pp. 1 -6. \\n[2] S. H. Abdurrahim, S. A. Samad, and A. B. Huddin, \"Review on th e \\neffects of age, gender, and race demographics on automatic face \\nrecognition,\" The Visual Computer, vol. 34, pp. 1617 -1630, 2018.  \\n[3] S. L. Fernandes and G. J. Bala, \"A Study on Face Recognition Under \\nFacial Expression Variation and Occlusion,\" in Proceed ings of the \\nInternational Conference on Soft Computing Systems, 2016, pp. 371 -\\n377. \\n[4] G. Lei, X. -h. Li, J. -l. Zhou, and X. -g. Gong, \"Geometric feature based \\nfacial expression recognition using multiclass support vector machines,\" \\nin 2009 IEEE Internationa l Conference on Granular Computing, 2009, \\npp. 318 -321. \\n2019 International Conference on Advanced Science and Engineering (ICOASE),  \\nUniversity of Zakho, Duhok Polytechnic University, Kurdistan Region, Iraq  \\n75 \\n [5] M.-P. Loh, Y. -P. Wong, and C. -O. Wong, \"Facial expression recognition \\nfor e -learning systems using Gabor wavelet & neural network,\" in null, \\n2006, pp. 523 -525. \\n[6] C. Martin, U. Werner, and H. -M. G ross, \"A real -time facial expression \\nrecognition system based on active appearance models using gray \\nimages and edge images,\" in Automatic Face & Gesture Recognition, \\n2008. FG\\'08. 8th IEEE International Conference on, 2008, pp. 1 -6. \\n[7] P. Burkert, F. Trie r, M. Z. Afzal, A. Dengel, and M. Liwicki, \\n\"Dexpression: Deep convolutional neural network for expression \\nrecognition,\" arXiv preprint arXiv:1509.05371, 2015.  \\n[8] I. T. Meftah, N. Le Thanh, and C. B. Amar, \"Emotion recognition using \\nKNN classification for user modeling and sharing of affect states,\" in \\nInternational Conference on Neural Information Processing, 2012, pp. \\n234-242. \\n[9] M. Abdulrahman and A. Eleyan, \"Facial expression recognition using \\nsupport vector machines,\" in 2015 23nd Signal Processing an d \\nCommunications Applications Conference (SIU), 2015, pp. 276 -279. \\n[10] H. Boughrara, M. Chtourou, C. B. Amar, and L. Chen, \"Facial \\nexpression recognition based on a mlp neural network using constructive \\ntraining algorithm,\" Multimedia Tools and Applicatio ns, vol. 75, pp. \\n709-731, 2016.  \\n[11] S. Li, W. Deng, and J. Du, \"Reliable crowdsourcing and deep locality -\\npreserving learning for expression recognition in the wild,\" in \\nProceedings of the IEEE Conference on Computer Vision and Pattern \\nRecognition, 2017, p p. 2852 -2861.  \\n[12] I. M. Revina and W. S. Emmanuel, \"A survey on human face expression \\nrecognition techniques,\" Journal of King Saud University -Computer and \\nInformation Sciences, 2018.  \\n[13] C.-L. Huang and Y. -M. Huang, \"Facial expression recognition using \\nmodel -based feature extraction and action parameters classification,\" \\nJournal of visual communication and image representation, vol. 8, pp. \\n278-290, 1997.  \\n[14] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. \\nMatthews, \"The extended cohn -kanade dataset (ck+): A complete \\ndataset for action unit and emotion -specified expression,\" in Computer \\nVision and Pattern Recognition Workshops (CVPRW), 2010 IEEE \\nComputer Society Conference on, 2010, pp. 94 -101. \\n[15] D. Chawla and M. C. Trivedi, \"A comparat ive study on face detection \\ntechniques for security surveillance,\" in Advances in Computer and \\nComputational Sciences, ed: Springer, 2018, pp. 531 -541. \\n[16] T. PAUL, U. A. SHAMMI, M. U. AHMED, R. RAHMAN, S. \\nKOBASHI, and M. A. R. AHAD, \"A Study on Face Dete ction Using \\nViola -Jones Algorithm in Various Backgrounds, Angles and Distances,\" \\nInternational Journal of Biomedical Soft Computing and Human \\nSciences: the official journal of the Biomedical Fuzzy Systems \\nAssociation, vol. 23, pp. 27 -36, 2018.  \\n[17] Á. P. P ertierra, A. B. G. González, J. T. Lafuente, and A. de Luis \\nReboredo, \"Communication Skills Personal Trainer Based on Viola -Jones Object Detection Algorithm,\" in International Conference on \\nIntelligent Data Engineering and Automated Learning, 2018, pp. 722 -\\n729. \\n[18] P. Carcagnì, M. Del Coco, M. Leo, and C. Distante, \"Facial expression \\nrecognition and histograms of oriented gradients: a comprehensive \\nstudy,\" SpringerPlus, vol. 4, p. 645, 2015.  \\n[19] C. Y. Yong, R. Sudirman, and K. M. Chew, \"Facial expression \\nmonitoring system using pca -bayes classifier,\" in Future Computer \\nSciences and Application (ICFCSA), 2011 International Conference on, \\n2011, pp. 187 -191. \\n[20] Y. Zhu, X. Li, and G. Wu, \"Face expression recognition based on \\nequable principal component analy sis and linear regression \\nclassification,\" in Systems and Informatics (ICSAI), 2016 3rd \\nInternational Conference on, 2016, pp. 876 -880. \\n[21] Y. Zhu, C. Zhu, and X. Li, \"Improved principal component analysis and \\nlinear regression classification for face rec ognition,\" Signal Processing, \\nvol. 145, pp. 175 -182, 2018.  \\n[22] M. S. Bilkhu, S. Gupta, and V. K. Srivastava, \"Emotion Classification \\nfrom Facial Expressions Using Cascaded Regression Trees and SVM,\" \\nin Computational Intelligence: Theories, Applications an d Future \\nDirections -Volume II, ed: Springer, 2019, pp. 585 -594. \\n[23] M. Peter, J. -L. Minoi, and I. H. M. Hipiny, \"3D Face Recognition using \\nKernel -based PCA Approach,\" in Computational Science and \\nTechnology, ed: Springer, 2019, pp. 77 -86. \\n[24] W. R. S. Em manuel and I. M. Revina, \"Face expression recognition \\nusing integrated approach of Local Directional Number and Local Tetra \\nPattern,\" in Control, Instrumentation, Communication and \\nComputational Technologies (ICCICCT), 2015 International Conference \\non, 201 5, pp. 707 -711. \\n[25] T. B. Abdallah, R. Guermazi, and M. Hammami, \"Facial -expression \\nrecognition based on a low -dimensional temporal feature space,\" \\nMultimedia Tools and Applications, vol. 77, pp. 19455 -19479, 2018.  \\n[26] R. S. Stanković and B. J. Falkowski , \"The Haar wavelet transform: its \\nstatus and achievements,\" Computers & Electrical Engineering, vol. 29, \\npp. 25 -44, 2003.  \\n[27] N. Dalal and B. Triggs, \"Histograms of oriented gradients for human \\ndetection,\" in international Conference on computer vision &  Pattern \\nRecognition (CVPR\\'05), 2005, pp. 886 --893. \\n[28] K. Lawrence, R. Campbell, and D. Skuse, \"Age, gender, and puberty \\ninfluence the development of facial emotion recognition,\" Frontiers in \\npsychology, vol. 6, p. 761, 2015.  \\n[29] K. Shan, J. Guo, W. You , D. Lu, and R. Bie, \"Automatic facial \\nexpression recognition based on a deep convolutional -neural -network \\nstructure,\" in 2017 IEEE 15th International Conference on Software \\nEngineering Research, Management and Applications (SERA), 2017, \\npp. 123 -128. \\n \\n \\n \\n',\n",
       " 'IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1685\\n2) When 1 ≤i≤N1and j>N1\\nG/prime(αi)+G/prime(γ−sαi)=di−s−αi/D (33)\\nαnew\\ni=Dκαold\\ni\\nDκ+1\\n+Dyi[f(xj)−f(xi)−yj+diyi]\\nDκ+1.(34)\\n3) When 1 ≤i,j≤N1\\nG/prime(αi)+G/prime(γ−sαi)=di−djs\\n+sγ/D−2αi/D (35)\\nαnew\\ni=Dκαold\\ni\\nDκ+2\\n+Dyi[f(xj)−f(xi)−djyj+diyi+yjγ/D]\\nDκ+2.\\n(36)\\n4) When 1 ≤j≤N1andi>N1\\nG/prime(αi)+G/prime(γ−sαi)=1−djs\\n+sγ/D−αi/D (37)\\nαnew\\ni=Dκαold\\ni\\nDκ+1\\n+Dyi[f(xj)−f(xi)−djyj+yi+yjγ/D]\\nDκ+1.\\n(38)\\nFinally, we clip αnew\\nito ensure it is in the range of [U,V].\\nThe value of αnew\\njis given by\\nαnew\\nj=αold\\nj+yiyj(αold\\ni−αnew\\ni). (39)\\nREFERENCES\\n[1] V . Vapnik, The Nature of Statistical Learning Theory .N e wY o r k :\\nSpringer, 1995.\\n[2] P. Williams, S. Li, J. Feng, and S. Wu, “A geometrical method to improve\\nperformance of the support vector machine,” IEEE Trans. Neural Netw. ,\\nvol. 18, no. 3, pp. 942–947, May 2007.\\n[3] K. Kobayashi and F. Komaki, “Info rmation criteria for support vector\\nmachines,” IEEE Trans. Neural Netw. , vol. 17, no. 3, pp. 571–577, May\\n2006.\\n[4] Q. Tao, D. Chu, and J. Wang, “Recursive support vector machines for\\ndimensionality reduction,” IEEE Trans. Neural Netw. , vol. 19, no. 1, pp.\\n189–193, Jan. 2008.\\n[5] S. Decherchi, S. Ridella, R. Zunino, P. Gastaldo, and D. Anguita, “Using\\nunsupervised analysis to constrain generalization bounds for support\\nvector classiﬁers,” IEEE Trans. Neural Netw. , vol. 21, no. 3, pp. 424–\\n438, Mar. 2010.\\n[6] D. D. Cox and R. L. Savoy, “Func tional magnetic resonance imaging\\n(fMRI) ‘brain reading’: Detecting and classifying distributed patterns offMRI activity in human visual cortex,” Neuroimage , vol. 19, no. 2, pp.\\n261–270, 2003.\\n[7] S. LaConte, S. Strother, V . Cherkassky, J. Anderson, and X. Hu,\\n“Support vector machines for tempor al classiﬁcation of block de-\\nsign fMRI data,” Neuroimage , vol. 26, no. 2, pp. 317–329,\\n2005.\\n[8] S. Li, D. Ostwald, M. Giese, and Z. Kourtzi, “Flexible coding for\\ncategorical decisions in the human brain,” J. Neurosci. , vol. 27, no. 45,\\npp. 12321–12330, 2007.\\n[9] K. A. Norman, S. M. Polyn, G. J. Detre, and J. V . Haxby, “Beyond\\nmind-reading: Multi-voxel pattern analysis of fMRI data,” Trends Cogn.\\nSci., vol. 10, no. 9, pp. 424–430, Sep. 2006.\\n[10] T. Mitchell, R. Hutchinson, R. Nicu lescu, F. Pereira, X. Wang, M. Just,\\nand S. Neman, “Learning to decode cognitive states from brain images,”Mach. Learn. , vol. 57, nos. 1–2, pp. 145–175, Oct.–Nov. 2004.\\n[11] Z. Wang, A. Childress, J. Wang, and J. Detre, “Support vector machine\\nlearning based fMRI data group analysis,” Neuroimage , vol. 36, no. 4,\\npp. 1139–1151, Jul. 2007.\\n1045-9227/$26.00 © 2010 IEEE[12] K. J. Friston, A. P. Holmes, K. J. Worsley, J. Poline, C. D. Frith, and\\nR. S. J. Frackowiak, “Statistical parametric maps in functional imaging:A general linear approach,” Human Brain Map. , vol. 2, no. 11, pp. 189–\\n210, 1995.\\n[13] M. Montemurro, R. Senatore, and S. Panzeri, “Tight data-robust\\nbounds to mutual information combining shufﬂing and model selec-tion techniques,” Neural Computat. , vol. 19, no. 11, pp. 2913–2957,\\nNov. 2007.\\n[14] C. Bishop, Neural Networks for Pattern Recognition . New York: Oxford\\nUniv. Press, 1995.\\n[15] L. Liang, V . Cherkassky, and D. R ottenberg, “Spatial SVM for feature\\nselection and fMRI activation detection,” in Proc. Int. Joint Conf. Neural\\nNetw. , 2006, pp. 1463–1469.\\n[16] A. Graf, F. Wichmann, H. Bulthöff, and B. Schölkopf, “Classiﬁcation\\nof faces in man and machine,” Neural Computat. , vol. 18, no. 1, pp.\\n143–165, Jan. 2006.\\n[17] B. Schölkopf and A. Smola, Learning with Kernels: Support Vector\\nMachines, Regularization, Optimization and Beyond . Cambridge, MA:\\nMIT Press, 2002.\\n[18] J. C. Platt, “Fast training of support vector machines using sequential\\nminimal optimization,” in Advances in Kernel Methods: Support Vector\\nLearning , B. Schölkopf, C. Burgess, and A. Smola, Eds. Cambridge,\\nMA: MIT Press, 1999, pp. 185–208.\\n[19] L. Glass, “Moire effect from random dots,” Nature , vol. 223, no. 5206,\\npp. 578–580, Aug. 1969.\\n[20] D. Ostwald, J. M. Lam, S. Li, and Z. Kourtzi, “Neural coding of global\\nform in the human visual cortex,” J. Neurophysiol. , vol. 99, no. 5, pp.\\n2456–2469, May 2008.\\n[21] J. Duncan, “An adaptive coding model of neural function in pre-\\nfrontal cortex,” Nat. Rev. Neurosci. , vol. 2, no. 11, pp. 820–829, Nov.\\n2001.\\n[22] E. K. Miller, “The prefrontal cortex and cognitive control,” Nat. Rev.\\nNeurosci. , vol. 1, no. 1, pp. 59–65, Oct. 2000.\\n[23] N. Sigala and N. K. Logothetis, “Vi sual categorization shapes feature\\nselectivity in the primate temporal cortex,” Nature , vol. 415, no. 6869,\\npp. 318–320, Jan. 2002.\\n[24] S. Li, S. D. Mayhew, and Z. Kourtzi, “Learning shapes the representation\\nof behavioral choice in the human brain,” Neuron , vol. 62, no. 3, pp.\\n441–452, May 2009.\\nFacial Expression Recognition in JAFFE Dataset Based\\non Gaussian Process Classiﬁcation\\nFei Cheng, Jiangsheng Yu, and Huilin Xiong\\nAbstract — The Gaussian process (GP) approaches to clas-\\nsiﬁcation synthesize Bayesian methods and kernel techniques,which are developed for the purpose of small sample analysis.Here we propose a GP model and investigate it for the facialexpression recognition in the Japanese female facial expressiondataset. By the strategy of leave-one-out cross validation, theaccuracy of the GP classiﬁers reaches 93.43% without any featureselection/extraction. Even when tested on all expressions of anyparticular expressor, the GP classiﬁer trained by the other sam-ples outperforms some frequently used classiﬁers signiﬁcantly.In order to survey the robustness of this novel method, therandom trial of 10-fold cross validations is repeated many timesto provide an overview of recognition rates. The experimentalresults demonstrate a promising performance of this application.\\nManuscript received October 26, 2009; revised July 21, 2010; accepted July\\n29, 2010. Date of publication August 19, 2010; date of current version October\\n6, 2010. This work was supported in part by Peking University through the\\n985 Project, in part by the Beijing Natural Science Foundation under Grant048SG/46810707-001 and Grant 4032013, and in part by the Natural ScienceFoundation of China through Project 60775008.\\nF. Cheng is with the Department of Mathematics, Beijing Jiaotong Univer-\\nsity, Beijing 100044, China (e-mail: fcheng@bjtu.edu.cn).\\nJ. S. Yu is with the Department of Computer Science and Technology, Key\\nLaboratory of High Conﬁdence Softwar e Technologies, Ministry of Education,\\nPeking University, Beijing 100871, China (e-mail: yujs@pku.edu.cn).\\nH. L. Xiong is with the Department o f Automation, Shanghai Jiaotong\\nUniversity, Shanghai 200240, China (e-mail: hlxiong@sjtu.edu.cn).\\nDigital Object Identiﬁer 10.1109/TNN.2010.2064176\\n1686 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010\\nIndex Terms — Classiﬁcation, facial expression recognition,\\nGaussian process model, kernel method.\\nI. I NTRODUCTION\\nFacial expression recognition (FER) is always one of the\\nmost challenging issues in the research of artiﬁcial intelligenceand pattern recognition [1]. It also has many potential appli-\\ncations to human-computer interaction, data-driven animation,\\nimage retrieval, human emotio n analysis, medical treatment,\\netc. The statistical approach es of machine learning to FER\\nare mainly the training of cla ssiﬁers and the extraction of\\nsemantic-level attributes [2 ] from a set of labeled digital\\nimages of facial expressions. Due to the problem of high\\ndimensionality and relatively small sample in FER, manytechniques for the reduction of dimensionality and the feature\\nselection/extraction [3], such as wavelet transform [4], princi-\\npal component analysis [5], independent component analysis(ICA) [6], kernel canonical correlation analysis [7], etc., have\\nbeen tested in the last two decades [8], as well as various\\nclassiﬁcation methods, for instance, artiﬁcial neural network(ANN) [9], support vector machine (SVM) [10], [11], Fisher\\ndiscriminant analysis [12], and locally linear embedding [13].\\nThe combinations of feature extraction and classiﬁcation\\nhave been tried in many ways, such as Gabor ﬁlter and ICA\\ncombined with SVMs of distinct kernels [6]. The reported\\naccuracy of FER reported in the literature is about 90%, or\\neven better. For instance, [11] showed a recognition rate of95.71% using a combination of 2-D-LDA and an SVM classi-\\nﬁer. With previous feature selection/extraction engines, some\\nwell-designed classiﬁers promise to yield good performance.The goal of this brief is to show that the Gaussian process\\n(GP) approach is able to give good results, close to the state\\nof the art, even without any feature selection/extraction.\\nAmong the frequently used classiﬁers, ANNs and SVMs\\noften achieve good performance in practice and play the roleof baseline whenever comparisons are made. It has been\\nproved that the two-layer perceptrons are capable of simulating\\nany continuous function on a compact domain [14], [15].\\nIf the hidden layer is composed of inﬁnitely many hidden\\nunits, Neal showed in [16] that the distribution of functionsgenerated by such neural networks tends to GP for a broad\\nclass of prior distributions over the parameters. With the\\nhelp of kernel methods, the separability of the nonlinearlytransformed observations is likely to be improved in a higher\\ndimensional space. And the “kernel trick” makes the nonlinear\\nmap not involved explicitly in the algorithms, which facilitates\\nthe training and testing pro cedures. Recently, there has been\\ngrowing interest in applying the kernel methods in machinelearning to the data analysis [17]. GPs, as a Bayesian kernel\\nmethod [18], have become a promising scheme for the problem\\nof classiﬁcation/regression in recent years. Together with themodels of SVMs and Bayesian neural networks [19], GP mod-\\nels offer more and more desirable applications [20], [21], [22].\\nThis brief focuses on the GP approach to FER. Section II\\nintroduces the proposed method, and Section III describes the\\nJapanese female facial expression (JAFFE) dataset and thenpresents the experiments of GPs with polynomial kernels and\\nGaussian radial basis function (RBF) kernels by three versionsof leave-one-out cross validation (LOO CV). Moreover, the\\nGP classiﬁer is compared with some other frequently usedclassiﬁers, such as SVM, k-nearest neighbor ( k-NN), naive\\nBayes, classiﬁcation tree, etc., for the capability of recognizing\\nall expressions of a new person whose images are not inthe training set. In order to examine the robustness of the\\nproposed model, the novel method is tested by sufﬁciently\\nmany independent random trials of 10-fold cross validations.\\nAll the results show a promising performance in such applica-\\ntions. Finally, some further investigations and conclusions arementioned in Section IV .\\nII. C\\nLASSIFICATION MODEL OF GAUSSIAN PROCESS\\nGiven the training data of x1,..., xn∈ Rdwith the\\ncorresponding target labels t=(t1,..., tn)T,t h ea i mo f\\nBayesian multiple classiﬁcation of a new input xnew∈\\nRdis the calculation of the conditional probability of\\np(tnew=k|x1,..., xn,t,xnew),o rb r i e ﬂ y p(tnew=k|D,t),\\nwhere k=1,2,..., mdenote the class labels and D=\\n(x1,..., xn,xnew)d×(n+1)is the data matrix of observations.\\nFor the binary classiﬁcation, k=0,1 are often adopted as the\\nclass labels.\\nFor each sample point x, it is assumed that there is\\nan associated hidden variable y(x)such that the vector of\\nyn+1=(y1,..., yn,ynew)T=(y(x1) ,..., y(xn),y(xnew))T\\nis normally distributed. That is, y(x)is a GP prior over\\nfunctions in the following Bayesian inference.\\nWithout loss of generality, let yn+1∼Nn+1(0,Kn+1),\\nwhere the covariance matrix Kn+1is deﬁned by a parame-\\nterized kernel function κ(·,·), which guarantees that Kn+1\\nis positive deﬁnite. The kernel approach to determining the\\ncovariance matrix is a highlighted contribution of GP model to\\nBayesian classiﬁcation. The common kernel functions include,\\nfor instance, polynomial kernel κ(u,v)=(θ1uTv+θ2)θ3,\\nhyperbolic tangent kernel κ(u,v)=tanh(uTv+θ),G a u s s i a n\\nRBFκ(u,v)=exp{−θ/bardblu−v/bardbl2}, etc. The choice of parame-\\nters is concerned to the kernel learning [23].\\nThe covariance matrix Kn+1=/parenleftBig\\nKnk\\nkTc/parenrightBig\\nis well deﬁned, in\\nwhich the n×nsubmatrix Kn=(κ(xi,xj))n×n+σ2I,t h e\\nvector k=(κ(xnew,x1) ,...,κ( xnew,xn))T∈Rnand the real\\nnumber cinKn+1isc=κ(xnew,xnew)+σ2,s ot h a t Kn+1\\nis positive deﬁnite. The model relies on the fact that the GP\\nis uniquely determined by the covariance matrix whenever the\\nmean is given [20], [24].\\nBy the well-known predictive distribution [25], p(tnew=\\nk|D,t), the probability of xnew in the k-th class can be\\npredicted by means of the following equation :\\np(tnew=k|D,t)=/integraldisplay\\nRp(tnew=k|ynew)p(ynew|D,t)dynew (1)\\nwhere p(tnew=k|ynew)is further speciﬁed by\\np(tnew=k|ynew)=softmax (ynew)\\n=exp(ynew)[exp(ynew)+n/summationdisplay\\nj=1exp(yj)]−1.\\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1687\\nIn particular, the binary classiﬁcation uses\\np(tnew=1|ynew)=sigmoid (ynew)=[1+exp(−ynew)]−1.\\nWhile the posterior distribution of p(ynew|D,t)in (1) is\\nhard to be expressed explicitly in a closed form because of\\nthe intractable integral in the equation\\np(ynew|D,t)=/integraldisplay\\nRnp(ynew|yn)p(yn|D,t)dyn (2)\\nsome numerical techniques, such as sampling methods [26],\\n[27], variational approximations [28], etc., are optional to get\\nthe acceptable replacement of p(ynew|D,t). In this brief, the\\nLaplace’s method [29], [30] is adopted to yield its Gaussianapproximation. The process is introduced brieﬂy as follows.\\nThe item p(y\\nnew|yn)in (2) is determined by the assumption\\nyn+1∼Nn+1(0,Kn+1). More precisely, by the fundamental\\nproperty of multivariate normal distribution [31], we get\\nthe conditional distribution of ynew|yn∼N(kTK−1\\nnyn,c−\\nkTK−1\\nnk).\\nThe item p(yn|D,t)in (2) can be derived from the fact\\np(yn|D,t)∝p(yn|D)p(t|D,yn),w h e r e yn|D∼Nn(0,Kn)\\nandp(t|D,yn)=/producttextn\\nj=1p(tj|D,yn). For the binary classiﬁca-\\ntion, p(tj|D,yn)=stj\\nj(1−sj)1−tj,w h e r e sj=sigmoid (yj).\\nThe extension to the multiclass situation is straightforward.\\nLetting ∇lnp(yn|D,t)=0, we get the stationary point\\nof log-likelihood ln p(yn|D,t)to be yn=Kn(t−s),w h e r e\\ns=(s1,..., sn)T. By the Laplace’s method [30], [32], the\\ndistribution of yn|D,tmay be approximated by a Gaussian\\ndistribution N n(Kn(t−s),A−1),w h e r e Ais the negative\\nHessian matrix of ln p(yn|D,t)\\nA=− ∇2lnp(yn|D,t)=W+K−1\\nn\\nin which W=diag[s1(1−s1) ,..., sn(1−sn)]is a diagonal\\nmatrix. Consequently\\nynew|D,t∼N(kT(t−s),c−kT(W−1+Kn)k). (3)\\nBy Monte Carlo techniques, the numerical computation\\nof (1) is implemented in the way of p(tnew=k|D,t)≈\\n1/r/summationtextr\\nj=1sigmoid (y(j)\\nnew),w h e r e y(1)\\nnew,..., y(r)\\nneware randomly\\nsampled from the distribution of (3).\\nIII. FER E XPERIMENTS IN JAFFE D ATASET\\nThe basic emotional expressions of concern include anger,\\ndisgust, fear, happiness, sadness, and surprise. For conve-\\nnience, we use their two letter acronyms of “an,” “di,” “fe,”\\n“ha,” “sa,” and “su” in the following text. As a benchmarkdatabase, the JAFFE dataset consists of 213 grayscale images\\nof seven expressions (neutral plus six basic emotional expres-\\nsions) from 10 female expressors, whose names are “ka,” “kl,”\\n“km,” “kr,” “mk,” “na,” “nm,” “tm,” “uy,” and “ym” for short.\\nEach image size is of 256 ×256 pixels and each expressor has\\n2–4 samples for each expression.\\nAt the ﬁrst step of the experimental procedure, the original\\nimages are cropped into 128 ×128 to reduce the inﬂuences\\nof background as usual. The cropping remains the central partof facial expression and no more complicated techniques of\\nlocating eyebrows and mouth are utilized in the preprocessing.\\nFig. 1. Cropped samples of anger, disgus t, fear, happiness, neutral, sadness,\\nand surprise expressions in turn.\\nFig. 2. In the ﬁrst line, the shape of mouth varies widely in the anger and\\nfear expressions. In the second line, th e difference between anger and sadness\\nis not signiﬁcant.\\nAnd then, the cropped images are further resized to 64 ×64\\nso that the dimension of each observation is reduced to 4096.\\nAfter the naive feature selection, all images are normalized\\nsuch that the range of pixel intensity is 0 to 255. Without\\nany class-based feature selection/extraction, the ﬁrst two steps\\nhave nothing to do with the labeled class information in thedata. Fig. 1 shows seven different facial expressions from two\\npersons in the JAFFE dataset.\\nIt is assumed in the GP models that the adjacent obser-\\nvations convey information about each other. For instance,the kernel of κ(u,v)=θ\\n0exp{(−1/2)(u−v)TM(u−v)}+\\nθd+1uTvis often used to deﬁne the covariance matrix Kn+1,\\nwhere M=diag(θ1,...,θ d)and the parameters are θ=\\n(θ0,θ1,...,θ d,θd+1)T. The parameters in the covariance ma-\\ntrixKn+1are determined by the maximum likelihood estimate\\nofθ, whose time complexity is O(n3)by Laplacian approx-\\nimation. In consideration of the efﬁciency of evaluation, we\\nskip over the time-consuming learning of complicated kernels\\nand only adopt the speciﬁc polynomial and Gaussian RBF\\nkernels in the experiments. It is believed that the performance\\nshould improve if the training of kernels is involved in thewhole learning procedure.\\nFirstly, the performance of GP classiﬁers deﬁned by the\\npolynomial kernel κ(u,v)=(θ\\n1uTv+θ2)θ3and Gaussian\\nRBF kernel κ(u,v)=exp{−θ/bardblu−v/bardbl2}is simply examined\\nby the closed test: that is, the classiﬁer is tested on the training\\nset itself. The recognition rate of such test is 100% for many\\nassignments of θ. For instance, θ1=θ2=1,θ3=2,3,4a n d\\nθ=0.1.\\nIn the JAFFE dataset, the same expression of one person\\nmay differ greatly in different samples, or distinct expressions\\nmay not be very distinguishable (see Fig. 2). To test the GPclassiﬁer, we design three versions of LOO CV . The ﬁrst\\nversion is the traditional one, which tests one image each time\\nand trains the classiﬁer on the remaining data. Since thereis at least one properly tagged example in the training set\\nthat looks close to the one under prediction, some researchers\\n1688 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010\\nTABLE I\\nCONFUSION MATRI X OF THE FIRST VERSION OF LOO CV\\nPrediction an di fe ha ne sa su total\\na n 2 9 000010 3 0\\nd i12 60 0 0 2 0 2 9f e 00 2 7 0050 3 2\\nh a0 0 03 10 0 0 3 1n e 0000 2 9 103 0\\ns a 00100 3 0 03 1\\nsu 0 0 0 1 2 0 27 30\\nTABLE II\\nASSOCIA TED STATISTICS OF SENSITIVITY ,SPECIFICITY ,ETC.,OFGP\\nCLASSIFICATION MODEL BY THE FIRST VERSION OF LOO CV\\nan di fe ha ne sa su\\nSensitivity 0.97 1.00 0.97 0.97 0.94 0.77 1.00\\nSpeciﬁcity 0.99 0.98 0.97 1.00 0.99 0.99 0.98\\nPos. pred. value 0.97 0.90 0.84 1.00 0.97 0.97 0.90\\nNeg. pred. value 0.99 1.00 0.99 0.99 0.99 0.95 1.00\\nPrevalence 0.14 0.12 0.13 0.15 0.15 0.18 0.13\\nDetection rate 0.14 0.12 0.13 0.15 0.14 0.14 0.13\\nPrevalence 0.14 0.14 0.15 0.15 0.14 0.15 0.14\\nFig. 3. As shown in Table I, there are nine samples misclassiﬁed to the classof “sadness.” They are 1 anger, 2 disgust, 5 fear, and 1 neural samples.\\nsuggest a more rigorous examination so that the images of\\nthe same person with the same expression are not considered\\nin the training set. Therefore, in the second version of LOO\\nCV , “one” means one expression of a particular person. More\\nstrictly, the third version leaves one person out of the trainingset and this strategy examines the capability of predicting all\\nexpressions of one new expressor based on the experience\\nabout the others.\\nUsing the parameterized Gaussian RBF kernel, the overall\\naccuracy of the ﬁrst version of LOO CV is 0 .9343. The\\nconfusion matrix of prediction is shown in Table I and some\\nother statistics for the evaluation are shown in Table II.The low sensitivity of “sadness” is caused by nine misclas-\\nsiﬁed samples illustrated in Fig. 3. Taking all the positive\\nanger and disgust expressions for example, Fig. 4 shows thepredictions of GP models on them.\\nSeveral frequently used classiﬁers, such as SVM, k-NN,\\nnaive Bayes, classiﬁcation tree, etc., are also tested by the\\nthird version of LOO CV . For each person, the accuracy ofFER based on a speciﬁc classiﬁer is recorded in Table III.\\nThe SVM in Table III uses a Gaussian RBF kernel with\\nparameter 0 .05, which may be improved by the further kernel\\nlearning. Among the tested classiﬁers, the GP model achieves\\nthe best performance, especia lly on the person named “mk.”\\nThe most interesting phenomenon is that the result of the\\nsecond version of LOO CV on any expression of a par-\\nticular person is not so good. For instance, the GP modelmisclassiﬁes 10 expressions of “mk” and the recognition\\nrate is only 52%. In the errors, three “ne” expressions are\\nProbabilit y0.10 0.15 0.20 0.25\\nan di fe ha ne sa s u\\n0.10 0.15 0.20 0.25\\nan di fe ha ne sa s u\\nFig. 4. Left panel illustrates the predicted results of the 30 positive anger\\nsamples by GP classiﬁers in the LOO CV , in which the solid line is the meanprediction. Similarly, the right panel i s for the 29 positive disgust samples.\\nfrequency0 50 100 150\\n02468 1 0 1 2 1 4\\n0.6 0.7 0.8\\nreco gnition rate0.9 1.0 0.83 0.85 0.87\\nreco gnition rate0.89\\nFig. 5. Left panel is the histogram of all 600 accuracies of FER in 60\\nindependent trials of 10-fold cross va lidations. The right panel demonstrates\\nthe empirical distribution of evaluations, with the mean recognition rate of86.89% and the standard deviation of 0 .01439204.\\nlabeled as “sa” and vice versa. Compared to that of the\\nthird version of LOO CV , the performance of the second\\nversion is worse, even when the classiﬁer is trained by\\nmore image samples of “mk.” The implies that the training\\nexpressions of “mk” substantially impairs the experience ofidentifying the testing expression of “mk” based on the other\\nsamples. In our experiments, similar things happen to the other\\nexpressors.\\nIn order to test the average performance, especially the\\nrobustness, of the GP model, the classiﬁer is investigatedby the 10-fold cross validations. For each trial of 10-fold\\ncross validation, the dataset is randomly partitioned into 10\\nsubsets of the same size, and then the procedure of “one for\\ntesting and the others for training” is repeated 10 times in\\nparallel. The accuracy of one trial is the arithmetic mean ofthe 10 recognition rates. After repeating such random trial\\n60 times independently, it is easy to survey the empirical\\ndistribution of those accuracies. The histograms of accuraciesof the GP model with a Gaussian RBF kernel (in which,\\nparameter =0.1) are shown in Fig. 5.\\nDue to the random partition in a multifold cross validation,\\nthe performance of classiﬁcation may vary greatly. That is\\nwhy the 10-fold cross validation is required to be repeated\\nmany times independently. A well-designed classiﬁer shouldreach a high accuracy on average, as well as a small deviation\\nof accuracies. We pick out one random trial of 10-fold\\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010 1689\\nTABLE III\\nRECOGNITION PERFORMANCE OF SOME FREQUENTLY USED CLASSIFIERS TESTED BY THE THIRD VERSION OF LOO CV\\nka kl km kr mk na nm tm uy ym\\nGaussian process 0.5652 0.5000 0.6364 0.5500 0.7619 0.6190 0.4000 0.3333 0.4762 0.6818\\nSVM 0.2609 0.1364 0.1364 0.1500 0.1429 0.1429 0.1500 0.1429 0.1429 0.1818\\n3-NN 0.3913 0.3182 0.4545 0.1000 0.5714 0.2857 0.2500 0.3810 0.2857 0.5455\\n5-NN 0.3043 0.4545 0.6364 0.3000 0.6667 0.2381 0.2000 0.3810 0.1905 0.5000\\nNaive Bayes 0.3478 0.6364 0.2273 0.1000 0.4286 0.3333 0.3500 0.4762 0.4286 0.6819\\nClassiﬁcation tree 0.1739 0.3636 0.3636 0.3500 0.2381 0.1905 0.3500 0.0952 0.4286 0.1818\\nC4.5 decision tree 0.1739 0.3636 0.5000 0.3000 0.4286 0.4286 0.2500 0.2857 0.2857 0.2727\\nTABLE IV\\nCONFUSION MATRI X OF THE WORST CASE OF FER T ESTED BY ONE\\nRANDOM TRIAL OF 10-F OLD CROSS VALIDA TION\\nP r e d i c t i o na nd if eh an es as ut o t a l\\na n 2000000 2\\nd i 1300000 4f e 0020001 3\\nh a 0001000 1n e 0000220 4\\ns a 0000030 3\\ns u 0010003 4\\ncross validation, with best accuracy of 0 .9524 and the worst\\naccuracy of 0 .7619. As illustrated in Fig. 5, the case of an\\naccuracy less than 0 .8 happens infrequently. For the worst case\\nof this 10-fold cross validation, the confusion matrix is listed\\nin Table IV.\\nIV . C ONCLUSION\\nIn this brief, we investigated the GP classiﬁcation model\\nfor the identiﬁcation of seven f acial expressions in the JAFFE\\ndataset. Three strategies of LOO CV were proposed to testthe performance of FER. The experiment of traditional LOO\\nCV achieved the recognition rate of 93 .43%, which shows that\\nGP classiﬁer accomplished success in a small sample analysis.\\nThe second version of LOO CV extracted all images of one\\nexpression of a particular person as testing set in each trial, andtrained the classiﬁer on the left samples. The third version of\\nLOO CV seemed stricter, which leaves one person out of the\\ntraining set and tests all expressions of the excluded person.We discover the interesting fact that, for each expressor, the\\nFER rate of the second version of LOO CV is worse, instead\\nof better, than that of the third version of LOO CV . In the\\nexperiments of FER of a new person, the GP approach to FER\\nworks much better than some other frequently used classiﬁers,such as SVM, k-NN, naive Bayes, and classiﬁcation tree.\\nBy many repeated 10-fold cross validations, we found\\nthat the recognition rate of a GP model had a very small\\ndeviation, that is, the novel method is robust in classiﬁcation.\\nSince the ﬁnal decision of a GP model depends upon the\\npredicted probabilities, a soft decision based on the descend-ing ordering of the estimates becomes feasible. Some more\\nexperiments on the capability of the proposed system to recog-\\nnize a completely unknown expression are being carried out.What is more, the exploration of kernel learning and the\\napplication of the GP model to a more authoritative and largerdatabase of FER, like the AR database of Purdue University,\\nwould also be worthy to consider in further work.\\nA\\nCKNOWLEDGMENT\\nThe authors would like to thank the anonymous reviewers\\nfor their valuable suggestions and helpful comments on the\\noriginal manuscript.\\nREFERENCES\\n[1] P. Ekman and W. Friesen, Unmasking the Face . Englewood Cliffs, NJ:\\nPrentice-Hall, 1975.\\n[2] C. F. Shan, S. G. Gong, and P. W. McOwan, “Facial expression\\nrecognition based on local binary patterns: A comprehensive study,”Image Vis. Comput. , vol. 27, no. 6, pp. 803–816, May 2009.\\n[3] A. K. Jain, P. W. Duin, and J. Mao, “Statistical pattern recognition:\\nAr e v i e w , ” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1,\\npp. 4–37, Jan. 2000.\\n[4] M. J. Lyons, J. Budynek, and S. Akamatsu, “Automatic classiﬁcation of\\nsingle facial images,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21,\\nno. 12, pp. 1357–1362, Dec. 1999.\\n[5] A. J. Calder, A. M. Burton, and P. Miller, “A principals component\\nanalysis of facial expressions,” Vis. Res. , vol. 41, no. 9, pp. 1179–1208,\\nApr. 2001.\\n[6] I. Buciu, C. Kotropoulos, and I. P itas, “ICA and gabor representation\\nfor facial expression recognition,” IEEE ICIP , vol. 2, nos. 14–17, pp.\\n855–858, Sep. 2003.\\n[7] W. M. Zheng, X. Y . Zhou, C. R. Zou, and L. Zhao, “Facial expression\\nrecognition using kernel canonical correlation analysis (KCCA),” IEEE\\nTrans. Neural Netw. , vol. 17, no. 1, pp. 233–238, Jan. 2006.\\n[8] B. Fasel and J. Luettin, “Automatic facial expression analysis: A survey,”\\nPattern Recogni. , vol. 36, no. 1, pp. 259–275, Jan. 2003.\\n[9] Y . Tian, T. Kanade, and J. F. Cohn, “Recognizing action units for facial\\nexpression analysis,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23,\\nno. 2, pp. 97–115, Feb. 2001.\\n[10] C. F. Chuang and F. Y . Shih, “Recognizing facial action units using\\nindependent component analysis and support vector machine,” Pattern\\nRecognit. , vol. 39, no. 9, pp. 1795–1798, 2006.\\n[11] F. Y . Shih, C. F. Chuang, and P. S. P. Wang, “Performance comparisons\\nof facial expression recognition in JAFFE database,” Int. J. Pattern\\nRecognit. Artiﬁcial Intell. , vol. 22, no. 3, pp. 445–459, 2008.\\n[12] T. Zhao, Z. Z. Liang, D. Zhang, and Q. Zou, “Interest ﬁlter vs. interest\\noperator: Face recognition using ﬁsher linear discriminant based on\\ninterest ﬁlter representation,” Pattern Recognit. Lett. , vol. 29, no. 13,\\npp. 1849–1857, Oct. 2008.\\n[13] D. Liang, J. Yang, Z. L. Zheng, and Y . C. Chang, “A facial expression\\nrecognition system based on supervised locally linear embedding,”\\nPattern Recognit. Lett. , vol. 26, no. 15, pp. 2374–2389, Nov. 2005.\\n[14] B. D. Ripley, Pattern Recognition and Neural Networks . Cambridge,\\nU.K.: Cambridge Univ. Press, 1996.\\n[15] C. M. Bishop, Neural Networks for Pattern Recognition . London, U.K.:\\nOxford Univ. Press, 1995.\\n[16] R. M. Neal, “Bayesian learning for neural networks,” in Lecture Notes\\nin Statistics , vol. 118. New York: Springer-Verlag, 1996, p. 204.\\n[17] T. Hofmann, B. Schölkoft, and A. J. Smola, “Kernel methods in machine\\nlearning,” Ann. Statist. , vol. 36, no. 3, pp. 1171–1220, 2008.\\n[18] A. J. Smola and B. Schölkopf , “Bayesian kernel methods,” in Advanced\\nLectures on Machine Learning: Machine Learning Summer School ,\\nS. Mendelson and A. J. Smola, Eds. New York: Spring-Verlag, 2002,\\npp. 65–117.\\n1690 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 21, NO. 10, OCTOBER 2010\\n[19] J. Lampinen and A. Vehtari, “Bayesian approach for neural networks-\\nreview and case studies,” Neural Netw. , vol. 14, no. 3, pp. 257–274, Apr.\\n2001.\\n[20] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine\\nLearning . Cambridge, MA: MIT Press, 2006.\\n[21] H. Nickisch and C. E. Rasmussen, “Approximations for binary Gaussian\\nprocess classiﬁcation,” J .M a c h .L e a r n .R e s . , vol. 9, pp. 2035–2078, Oct.\\n2008.\\n[22] C. K. I. Williams and D. Barber, “Bayesian classiﬁcation with Gaussian\\nprocesses,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 12,\\npp. 1342–1351, Dec. 1998.\\n[23] B. Schölkoft and A. J. Smola, Learning with Kernels . Cambridge, MA:\\nMIT Press, 2002.\\n[24] W. Liu, J. C. Principe, and S. Haykin, Kernel Adaptive Filtering:\\nA Comprehensive Introduction . New York: Wiley, 2010.\\n[25] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data\\nAnalysis , 2nd ed. Boston, MA: Chapman & Hall, 2004.\\n[26] C. P. Robert and G. Casella, Monte Carlo Statistical Methods , 2nd ed.\\nNew York: Springer-Verlag, 2004.\\n[27] R. M. Neal, “Monte Carlo implementation of Gaussian process models\\nfor Bayesian regression and classiﬁcation,” Dept. Comput. Sci., Univ.\\nToronto, ON, Canada, Tech. Rep. CRG-TR-97-2, 1997.\\n[28] T. Jaakkola and M. I. Jordan, “Bayesian parameter estimation via\\nvariational methods,” Statist. Comput. , vol. 10, no. 1, pp. 25–37, Jan.\\n2000.\\n[29] L. Tierney and J. Kadane, “Accu rate approximations for posterior\\nmoments and marginal densities,” J. Amer. Statist. Assoc. , vol. 81, no.\\n393, pp. 82–86, Mar. 1986.\\n[30] M. A. Tanner, Tools for Statistical Inference-Methods for the Exploration\\nof Posterior Distributions and Likelihood Functions , 3rd ed. New York:\\nSpringer-Verlag, 1996.\\n[31] C. R. Rao, Linear Statistical Inference and its Applications .N e wY o r k :\\nWiley, 1973.\\n[32] C. M. Bishop, Pattern Recognition and Machine Learning .N e wY o r k :\\nSpringer-Verlag, 2006.\\nContinuous Attractors of Lotka–Volterra Recurrent\\nNeural Networks with Inﬁnite Neurons\\nJiali Yu, Zhang Yi, and Jiliu Zhou\\nAbstract — Continuous attractors of Lotka–Volterra recurrent\\nneural networks (LV RNNs) with inﬁnite neurons are studied inthis brief. A continuous attractor is a collection of connected\\nequilibria, and it has been recognized as a suitable model\\nfor describing the encoding of continuous stimuli in neuralnetworks. The existence of the continuous attractors dependson many factors such as the connectivity and the externalinputs of the network. A continuous attractor can be stable orunstable. It is shown in this brief that a LV RNN can possessmultiple continuous attractors if the synaptic connections and theexternal inputs are Gussian-like in shape. Moreover, both stableand unstable continuous attractors can coexist in a network.Explicit expressions of the continuous attractors are calculated.Simulations are employed to illustrate the theory.\\nIndex Terms — Continuous attractors, Lotka–Volterra recur-\\nrent neural networks, stable, unstable.\\nManuscript received June 12, 2009; revised July 11, 2010; accepted July 31,\\n2010. Date of publication September 8, 2010; date of current version October\\n6, 2010. This work was supported in part by the National Science Foundation\\nof China under Grant 60970013, the National Basic Research Program ofChina under Program 973, under Grant 2011CB302201, and the FundamentalResearch Funds for the Central U niversities under Grant ZYGX2009J102.\\nJ. Yu is with the Institute for Infocomm Research, Agency for Science\\nTechnology and Research, 138632, Singapore (e-mail: jlyu@i2r.a-star.edu.sg).\\nZ. Yi and J. Zhou are with the College of Computer Science,\\nSichuan University, Chengdu 610065, China (e-mail: zhangyi@scu.edu.cn;\\nzhoujl@scu.edu.cn).\\nColor versions of one or more of the ﬁgures in this brief are available online\\nat http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TNN.2010.2067224I. I NTRODUCTION\\nThe Lotka–V olterra (LV) model of recurrent neural networks\\n(RNNs) was ﬁrst proposed in [1]. It was derived from theconventional membrane dynamics of neurons with a sigmoid\\nresponse function and its dynamic properties were analytically\\nstudied. The LV RNNs have found successful applications in\\nwinner-take-all, winner-share-all, and k-winner-take-all prob-\\nlems (see [2]–[5]). A recent interesting work on LV RNNs canbe found in [6], in which, LV RNNs were successfully used\\nfor implementing the competitive layer model. The model of\\nLV RNNs can be looked upon as a special model of the well-known Grossberg neural network model [7], [8].\\nRecently, there has been increasing interest in multista-\\nbility analysis for neural networks [9]–[11]. In multistability\\nanalysis, the networks are allowed to have multiple attractors\\n[12]. There are two kinds of attractors: discrete attractorsand continuous attractors. Continuous attractors are suitable to\\ndescribe the encoding of continuous external stimuli, such as\\nthe orientation, the moving direction, and the spatial location\\nof objects, or those continuous features that underlie the\\ncategorization of complicated objects [13]–[15]. It is knownthat bump behaviors are observed everywhere in the cortex,\\nand continuous attractors with Gaussian shape have been\\nwidely studied [14]–[17]. There are several computationallydesirable properties with continuous attractors. For example,\\nthey can encode continuous stimuli efﬁciently, track objects\\nsmoothly, and act as a link between neurodynamics andcognitive behaviors. This brief proposes to study continuous\\nattractors of LV RNNs.\\nContinuous attractors have been studied in two different\\nclasses of RNNs. In the ﬁrst class, the networks are with\\nﬁnite neurons. Networks of this class are analyzed in ﬁnite\\ndimensional space. Line attractors have been found in some\\nnetworks of this class [18], [19]. The theory of line attractorsof linear RNNs has been used successfully to explain how the\\nbrain can keep the eyes still [20]. A line attractor is a special\\nkind of continuous attractors, it is embedded in some 1-Dmanifolds. In the second class, the RNNs are with continuous\\ndistribution of inﬁnite neurons. The networks of second class\\nhave been widely studied by many authors (see [21]–[23],[14], [15], [17]). The networks in the second class should\\nbe studied in inﬁnite dimensional space. Recently, it was\\nreported in [13] that continuous attractors can be designed by\\ntuning the external inputs to networks that have a connectivity\\nmatrix with Toeplitz symmetry. It should be pointed out thatthese two classes of networks are different in dynamics from\\na mathematical point of view. This brief studies continuous\\nattractors of the networks that belong to the second class.\\nA continuous attractor is a set of connected equilibria and\\nit can be stable or unstable. If each point of a continuousattractor is stable, then the continuous attractor is said to be\\nstable; otherwise, it is said to be unstable. A stable continuous\\nattractor has the property that any trajectory starting from apoint close to it will remain close to it forever, i.e., the stable\\ncontinuous attractor “attracts” trajectories around it. On the\\nother hand, for any unstable continuous attractor, there must\\nexist a trajectory that cannot be “attracted” by it. In this brief,\\n1045-9227/$26.00 © 2010 IEEE\\n',\n",
       " '172 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nFacial Expression Recognition in Image Sequences\\nUsing Geometric Deformation Features\\nand Support Vector Machines\\nIrene Kotsia and Ioannis Pitas , Senior Member, IEEE\\nAbstract— In this paper, two novel methods for facial expres-\\nsion recognition in facial image sequences are presented. Theuser has to manually place some of Candide grid nodes to facelandmarks depicted at the ﬁrst frame of the image sequenceunder examination. The grid-tracking and deformation systemused, based on deformable models, tracks the grid in consecutivevideo frames over time, as the facial expression evolves, until theframe that corresponds to the greatest facial expression intensity.The geometrical displacement of certain selected Candide nodes,deﬁned as the difference of the node coordinates between the ﬁrstand the greatest facial expression intensity frame, is used as aninput to a novel multiclass Support Vector Machine (SVM) system\\nof classiﬁers that are used to recognize either the six basic facial\\nexpressions or a set of chosen Facial Action Units (FAUs). Theresults on the Cohn–Kanade database show a recognition accu-racy of 99.7% for facial expression recognition using the proposedmulticlass SVMs and 95.1% for facial expression recognitionbased on FAU detection.\\nIndex Terms— Candide grid, Facial Action Coding S (FACS),\\nFacial Action Unit (FAU), facial expression recognition, machinevision, pattern recognition, Support Vector Machines (SVMs).\\nI. I NTRODUCTION\\nDURING the past two decades, facial expression recog-\\nnition has attracted a signiﬁcant interest in the scientiﬁc\\ncommunity, as it plays a vital role in human centered interfaces.Many applications, such as virtual reality, video-conferencing,\\nuser proﬁling, and customer satisfaction studies for broadcast\\nand web services, require efﬁcient facial expression recognitionin order to achieve the desired results. Therefore, the impact offacial expression recognition on the above-mentioned applica-\\ntion areas is constantly growing.\\nSeveral research efforts have been done regarding facial ex-\\npression recognition. The facial expressions under examinationwere deﬁned by psychologists as a set of six basic facial expres-\\nsions (anger, disgust, fear, happiness, sadness, and surprise) [1].\\nIn order to make the recognition procedure more standardized, aset of muscle movements known as Facial Action Units (FAUs)\\nthat produce each facial expression, was created, thus forming\\nManuscript received May 5, 2005; revised July 2, 2006. This work was sup-\\nported by the research project 01ED312 “Use of Virtual Reality for training\\npupils to deal with earthquakes” ﬁnanced by the Greek Secretariat of Research\\nand Technology and the ”SIMILAR” European Network of Excellence on Mul-\\ntimodal Interfaces of the IST Programme of the European Union. The associate\\neditor coordinating the review of this manuscript and approving it for publica-tion was Dr. Beniot Macq.\\nThe authors are with the Department of Informatics, Aristotle University of\\nThessaloniki, 54124 Thessaloniki, Greece (e-mail: ekotsia@aiia.csd.auth.gr;\\npitas@aiia.csd.auth.gr).\\nDigital Object Identiﬁer 10.1109/TIP.2006.884954the so-called Facial Action Coding System (FACS) [2]. These\\nFAUs are combined in order to create the rules responsible forthe formation of facial expressions as proposed in [3].\\nA. Facial Expression Recognition\\nA survey on the research made regarding facial expression\\nrecognition can be found in [4] and [5]. The approaches re-ported regarding facial expression recognition can be distin-guished in two main directions, the feature-based ones and the\\ntemplate-based ones, according to the method they use for fa-\\ncial information extraction. The feature-based methods use tex-ture or geometrical information as features for expression infor-mation extraction. The template-based methods use 3-D or 2-D\\nhead and facial models as templates for expression information\\nextraction.\\n1) Feature-Based Approaches: Facial feature detection and\\ntracking is based on active InfraRed illumination in [6], in\\norder to provide visual information under variable lighting and\\nhead motion. The classiﬁcation is performed using a DynamicBayesian Network (DBN).\\nA method for static and dynamic segmentation and classiﬁca-\\ntion of facial expressions is proposed in [7]. For the static case,\\na DBN is used, organized in a tree structure. For the dynamicapproach, multi level Hidden Markov Models (HMMs) classi-ﬁers are employed.\\nThe system proposed in [8] automatically detects frontal\\nfaces in the video stream and classiﬁes them in seven classesin real time: neutral, anger, disgust, fear, joy, sadness, andsurprise. An expression recognizer receives image regions\\nproduced by a face detector and then a Gabor representation\\nof the facial image region is formed to be later processed by abank of SVMs classiﬁers.\\nGabor ﬁlters are also used in [9] for facial expression recogni-\\ntion. Facial expression images are coded using a multiorienta-\\ntion, multiresolution set of Gabor ﬁlters which are topograph-ically ordered and aligned approximately with the face. Thesimilarity space derived from this facial image representation is\\ncompared with one derived from semantic ratings of the images\\nby human observers. The classiﬁcation is performed by com-paring the produced similarity spaces.\\nThe images are ﬁrst transformed using a multiscale, multi-\\norientation set of Gabor ﬁlters in [10]. The grid is then regis-tered with the facial image region either automatically, usingelastic graph matching [11] or by manual clicking on ﬁducial\\nface points. The amplitude of the complex valued Gabor trans-\\nform coefﬁcients are sampled on the grid and combined into a\\n1057-7149/$25.00 © 2006 IEEE\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 173\\nsingle vector, called a Labeled Graph Vector (LGV). The clas-\\nsiﬁcation is performed using the distance of the LGV from each\\nfacial expression cluster center. Gabor features are used for fa-\\ncial feature extraction given a set of ﬁducial points in [12]. The\\nclassi ﬁcation is performed using Bayes, SVMs, Adaboost, and\\nlinear programming classi ﬁers.\\nA Neural Network (NN) is employed to perform facial ex-\\npression recognition in [13]. The features used can be either\\nthe geometric positions of a set of ﬁducial points on a face or\\na set of multiscale and multiorientation Gabor wavelet coef ﬁ-\\ncients extracted from the facial image at the ﬁducial points. The\\nrecognition is performed by a two layer perceptron NN. A con-\\nvolutional NN was used in [14]. The system developed is robustto face location changes and scale variations. Feature extrac-tion and facial expression classi ﬁcation were performed using\\nneuron groups, having as input a feature map and properly ad-\\njusting the weights of the neurons for correct classi ﬁcation. A\\nmethod that performs facial expression recognition is presentedin [15]. Face detection is performed using a Convolutional NN,\\nwhile the classi ﬁcation is performed using a rule-based algo-\\nrithm. Optical ﬂow is used for facial region tracking and facial\\nfeature extraction in [16]. The facial features are inserted ina Radial Basis Function (RBF) NN architecture that performs\\nclassi ﬁcation. The Discrete Cosine Transform (DCT) is used\\nin [17], over the entire face image as a feature detector. Theclassi ﬁcation is performed using a one-hidden layer feedfor-\\nward NN.\\nA feature selection process that is based on principal compo-\\nnent analysis (PCA) is proposed in [18]. A decision tree-basedclassi ﬁer that uses successive projections onto more precise rep-\\nresentation subspaces, is employed. The image pixels are used\\nin [19] as input to PCA and Linear Discriminant Analysis (LDA)\\nto reduce the original feature space dimensionality. The resultedfeatures are lexicographically ordered and concatenated to a fea-ture vector, which is used for classi ﬁcation according to the\\nnearest neighbor rule.\\nThe approach followed in [20] uses structured and geomet-\\nrical features of a user sketched expression model. The classi ﬁ-\\ncation is performed using Linear Edge Mapping (LEM). Expres-\\nsive face modelling, using an Active Appearance Model (AAM)\\nis employed in [21]. The facial model is constructed based oneither three or one PCA. The classi ﬁcation is performed in the\\nspace of AAM.\\n2) Model Template-Based Approaches: Two methods for fa-\\ncial expression recognition are proposed in [22], based on a 3-Dmodel enriched with muscles and skin. The ﬁrst method esti-\\nmates facial muscle actuations from optical ﬂow data. The clas-\\nsiﬁcation is performed according to its similarity to the classical\\npatterns of muscle actuation. The second method uses the clas-sical patterns of muscle actuation to generate the classical pat-tern of motion energy associated with each facial expression,\\nthus resulting in a set of simple facial expression “detectors, ”\\neach of which looks for the particular space-time pattern of mo-tion energy associated with each facial expression.\\nA face model, de ﬁned as a point-based model composed of\\ntwo 2-D facial views (frontal and pro ﬁle views) is used in [3].\\nThe deformation of facial features is extracted from both thefrontal and pro ﬁle views and its correspondence with the FAUsis established. The facial expression recognition is performed\\nbased on a set of decision rules.\\nA 3-D facial model is proposed in [23]. Anatomically-based\\nmuscles are added to it. A Kalman ﬁlter in correspondence\\nwith optical ﬂow computation are used to extract muscle action\\nin order to form a new model of facial action, the so-called\\n.\\nA 3-D facial model used for facial expression recognition is\\nalso proposed in [24]. First, the head pose is estimated in a facialvideo sequences. Subsequently, face images are warped onto aface model with canonical face geometry, then they are rotated\\nto frontal ones, and are projected back onto the image plane.\\nPixels brightness is linearly rescaled and resulting images areconvolved with a bank of Gabor kernels. The Gabor representa-tions are then channelled to a bank of SVMs to perform facial\\nexpression recognition.\\nB. FAU-Based Facial Expression Recognition\\nFor FAUs detection, the approaches followed were also fea-\\nture based. Many techniques for FAUs recognition are proposed\\nin [25]. PCA, Independent Component Analysis (ICA), LocalFeatures Analysis (LFA), LDA, Gabor wavelet representations,and Local Principal Components (LPC) are investigated more\\nthoroughly.\\nA group of FAUs is detected in [26]. The facial feature\\ncontours are adjusted and both permanent and transient facialfeatures changes are automatically detected and tracked in the\\nimage sequence. The facial parameters are then fed into two\\nNN classi ﬁers, one for the upper face and one for the lower\\nface.\\nFAUs detection is also investigated in [27]. Facial expression\\ninformation extraction is performed either by using optical ﬂow,\\nor by facial feature point tracking. The extracted information isused as an input in a HMMs system that has as an output upperface expressions at the forehead and brow regions.\\nHMMs are also used in [28]. Dense optical ﬂow extraction\\nis used to track ﬂow across the entire face image, after the\\ninput image sequence is aligned. Facial feature tracking of asmall set of preselected features is performed and high-gradient\\ncomponent detection uses a combination of horizontal, vertical,\\nand diagonal line and edge feature detectors to detect and trackchanges in standard and transient facial lines and furrows. Theresults from the above system are fed to a HMMs system to\\nperform facial expression recognition.\\nA NN is employed for FAUs detection in [29]. The geometric\\nfacial features (including mouth, eyes, brows, and cheeks) areextracted using multistate facial component models. After ex-\\ntraction, these features are represented parametrically. The re-\\ngional facial appearance patterns are captured using a set of mul-tiscale and multiorientation Gabor wavelet ﬁlters at speci ﬁc lo-\\ncations. The classi ﬁcation is performed using a back-propaga-\\ntion NN.\\nIn the current paper, two novel fast feature-based methods\\nare proposed that use SVMs classi ﬁers for recognizing dynamic\\nfacial expressions either directly or by ﬁrst detecting the FAUs.\\nSVMs were chosen due to their good performance in various\\npractical pattern recognition applications [30] –[33], and their\\nsolid theoretical foundations. A novel class of SVMs, which\\n174 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nincorporates statistic information about the classes under exami-\\nnation, is also proposed. The classi ﬁcation on both cases (facial\\nexpression recognition using multiclass SVMs or based on FAU\\ndetection) is performed using only geometrical information,\\nwithout taking into consideration any facial texture information.\\nLet us consider a video shot containing a face, whose facial\\nexpression evolves from a neutral state ( ﬁrst frame) to a fully\\nexpressed state (last frame). The proposed method is based on\\nmapping and tracking the facial model Candide onto the videoframes. The proposed facial expression recognition system issemi-automatic, in the sense that the user has to manually place\\nsome of the Candide grid nodes [34] on face landmarks depicted\\nat the ﬁrst frame of the image sequence under examination. The\\ntracking system allows the grid to follow the evolution of thefacial expression over time till it reaches its highest intensity,\\nproducing at the same time the deformed Candide grid at each\\nvideo frame. A subset of the Candide grid nodes is chosen, thatpredominantly contribute to the formation of the facial defor-mations described by the FACS. The geometrical displacement\\nof these nodes, de ﬁned as the difference of each node coordi-\\nnates at the ﬁrst and the last frame of the facial image sequence,\\nis used as an input to a SVMs classi ﬁer (either the classical or\\nthe proposed one). When facial expression recognition using\\nmulticlass SVMs is performed, the SVMs system consists of a\\nsix-class SVMs classi ﬁer, each class representing one of the six\\nbasic facial expressions (anger, disgust, fear, happiness, sadness,and surprise). When FAU-based facial expression recognition\\nis .performed, 8 or 17 FAUs are chosen that corresponds to the\\nnew empirically derived facial expressions rules and to the rulesproposed in [3]. Thus, the recognition system used is composedof a bank of two-class SVMs, each one detecting the presence\\nor absence of a particular FAU that corresponds to a speci ﬁc\\nfacial expression. The experiments were performed using theCohn –Kanade database and the results show that the proposed\\nnovel facial expression recognition system can achieve a recog-\\nnition accuracy of 99.7% or 95.1%, when recognizing six basic\\nfacial expressions on the Cohn –Kanade database by the multi-\\nclass SVMs approach or by the FAU detection-based approach,respectively.\\nSummarizing, the contributions of this paper are as follows.\\n•The presentation of a real-time system able to correctly\\nclassify facial expressions and FAUs, taking under consid-eration only geometrical displacement information based\\non the standard and well known Candide grid [35], con-\\ntrary to other approaches that use their own models withouthaving explicitly de ﬁned them [7], [9], [23], [36], [37].\\n•The introduction of a new class for multiclass SVMs clas-\\nsiﬁcation, based on the extension of the approach described\\nin [30].\\n•The presentation of a new set of empirical rules for facial\\nexpression recognition using FAUs, as well as a simpli ﬁed\\nCandide model whose nodes correspond to the above men-\\ntioned FAUs.\\nOur system is different from the method proposed in [38] as:\\n•it uses a general and well known model (Candide facial\\ngrid) for tracking and information extraction, and not an\\narbitrary grid that the author chose, not having been prop-erly de ﬁned for public use;•a method for FAU recognition and facial expression recog-\\nnition through the FAUs appearing in a facial grid is alsopresented;\\n•a novel modi ﬁed SVMs system is proposed and used to\\nsolve the facial expression recognition system, proving atthe same time that its performance greatly outperforms themaximum margin SVMs approach [39].\\nThis paper is organized as follows. The system used for facial\\nexpression classi ﬁcation is described in Section II. The facial\\nexpression rules used for the synthesis of the six basic facial ex-pressions as proposed in [3] are presented in Section II-C. The\\nmodi ﬁed SVMs for a two-class and multiclass problem are pre-\\nsented in Sections III and Section IV, respectively. The data-base used for the experiments and their description for bothapproaches are presented in Section V-A. The newly proposed\\nrules for the simpli ﬁed Candide grid and the facial expressions\\nare described in Section V-B. The accuracy rates achieved forwhen the chosen subset of FAUs was chosen as well as whenfacial expression recognition was attempted using multiclass\\nSVMs are shown in Sections V-C and E, respectively. Conclu-\\nsions are drawn in Section VI.\\nII. S\\nYSTEM DESCRIPTION\\nThe facial expression recognition system is composed of two\\nsubsystems: one for Candide grid node information extractionand one for grid node information classi ﬁcation. The grid node\\ninformation extraction is performed by a tracking system, while\\nthe grid node information classi ﬁcation is performed by a SVMs\\nsystem. The ﬂow diagram of the proposed system is shown in\\nFig. 1.\\nA. Tracking System Initialization\\nThe initialization procedure is performed in a semi-auto-\\nmatic way in order to attain reliability and robustness of theinitial grid displacement. The facial wireframe model used in\\nthe tracking procedure is the well-known Candide wireframe\\nmodel [34], contrary to the other approaches that use their ownmodels without having explicitly de ﬁned them. Candide is a pa-\\nrameterized face mask speci ﬁcally developed for model-based\\ncoding of human faces. A frontal and a pro ﬁle view of the\\nmodel can be seen in Fig. 7. The low number of its trianglesallows fast face animation with moderate computing power.\\nIn the beginning, the Candide wireframe grid is randomly\\nplaced on the facial image depicted at the ﬁrst frame. The grid is\\nin its neutral state. The user has to manually select a number ofpoint correspondences that are matched against the facial fea-tures of the actual face image. Future research involves the au-\\ntomatical placement of the grid on the face, using elastic graph\\nmatching algorithms. The most signi ﬁcant nodes (around the\\neyes, eyebrows and mouth) should be chosen, since they are re-sponsible for the formation of facial deformations modelled by\\nFACS. It has been empirically determined that ﬁve to eight node\\ncorrespondences are enough for a good model ﬁtting. These\\ncorrespondences are used as the driving power which deformsthe rest of the model and matches its nodes against face image\\npoints. The result of the initialization procedure, when seven\\nnodes (four for the inner and outer corner of the eyes and threefor the upper lip) are placed by the user, can be seen in Fig. 2.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 175\\nFig. 1. System architecture for facial expression recognition in facial videos.\\nFig. 2. Result of initialization procedure when seven Candide nodes are placed by the user on a facial image.\\nB. Model-Based Tracking\\nWireframe node tracking is performed by a pyramidal variant\\nof the well-known Kanade –Lucas –Tomasi (KLT) tracker [40].\\nThe loss of tracked features is handled through a model defor-mation procedure that increases the robustness of the tracking\\nalgorithm.\\nThe algorithm, initially ﬁts and subsequently tracks the Can-\\ndide facial wireframe model in video sequences containing theformation of a dynamic human facial expression from the neu-\\ntral state to the fully expressive one. The facial features are\\ntracked in the video sequence using a variant of KLT tracker[40]. If needed, model deformations are performed by mesh ﬁt-\\nting at the intermediate steps of the tracking algorithm. Such\\ndeformations provide robustness and tracking accuracy.\\nThe facial model is assumed to be a deformable 2-D mesh\\nmodel. The facial model elements (springs) are assumed to havea certain stiffness. The driving forces that are needed, i.e., the\\nforces that deform the model, are determined from the point\\ncorrespondences between the facial model nodes and the faceimage features. Each force is de ﬁned to be proportional to the\\ndifference between the model nodes and their correspondingmatched feature points on the face image. If a node correspon-\\ndence is lost, the new node position is the result of the grid defor-mation. This solves a major problem of feature-based trackingalgorithms, the gradual elimination of features points with re-\\nspect to time. In the modi ﬁed tracking algorithm, the incorpo-\\nration of the deformation step enables the tracking of featuresthat would have been lost otherwise. The tracking algorithm\\nprovides a dynamic facial expression model for each video se-\\nquence, which is de ﬁned as a series of frame facial expression\\nmodels, one for each video frame.\\nAn example of the deformed frame facial expression models\\nproduced for each one of the six basic facial expressions can be\\nseen in Fig. 3\\nC. Grid Node Displacement Extraction\\nIn the proposed approach, the facial expression classi ﬁcation\\nis performed based only on geometrical information, withouttaking directly into consideration any facial texture information.\\nThe geometrical displacement information of the grid node co-\\nordinates is used either for facial expression recognition usingmulticlass SVMs or for FAU-based facial expression recogni-tion. In the FAU-based recognition, the activated FAUs should\\n176 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nFig. 3. Example of the deformed Candide grids for each one of the six facial expressions.\\nbe detected in the grid, before employing them to produce one\\nof the six basic facial expressions using a set of rules that maps\\nthem to facial expressions.\\nLet\\n be the video database that contains the facial mage se-\\nquences. In the case of facial expression recognition using mul-ticlass SVMs, it is clustered into six different classes\\n,\\n, each one representing one of six basic facial expres-\\nsions (anger, disgust, fear, happiness, sadness, and surprise). Inthe case of FAU-based facial expression recognition, for everyFAU, the database is clustered into two different classes\\n,\\n1, 2 for the\\n th FAU,\\n . The ﬁrst class,\\n ,\\nrepresents the presence of the FAU under examination at thegrid being processed, while the second one,\\n, represents its\\nabsence.\\nThe geometrical information used for facial expression recog-\\nnition is the displacement of one node\\n ,d eﬁned as the dif-\\nference of the\\n th grid node coordinates at the ﬁrst and the fully\\nformed expression facial video frame\\nand\\n(1)\\nwhere\\n ,\\n are the\\n ,\\ncoordinate displacement of the\\nth node in the\\n th image, respectively.\\n is the total number ofnodes (\\n for the Candide model) and\\n is the number\\nof the facial image sequences. This way, for every facial image\\nsequence in the training set, a feature vector\\n is created, called\\ngrid deformation feature vector containing the geometrical dis-\\nplacement of every grid node\\n(2)\\nhaving\\n dimensions. We assume that each\\ngrid deformation feature vector\\n belongs to one\\nof the six facial expression classes\\n ,\\n (for facial\\nexpression recognition using multiclass SVMs) and activates anumber of FAUs (for FAUs detection-based facial expression\\nrecognition).\\nFacial expressions can be described as combinations of FAUs,\\nas proposed in [3]. As can be seen in the original rules (Table I),the FAUs that are necessary for fully describing all facial ex-\\npressions are FAUs 1, 2, 4, 5, 6, 7, 9, 10, 12, 15, 16, 17, 20, 23,\\n24, 25, and 26. Therefore, these 17 FAUs are responsible for de-scribing face deformations according to FACS. The operators\\n,\\nrefer to the logical AND, OR operations, respectively. There-\\nfore, FAUs can be easily used for facial expression recognition.\\nIn the following, we will formulate the SVMs-based classi ﬁca-\\ntion problems used for FAUs detection in the grid and for facialexpression recognition.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 177\\nTABLE I\\nFACIAL EXPRESSION SYNTHESIS RULES AS PROPOSED IN [3]\\nIII. FAU D ETECTION USING SVM S\\nHere, we shall describe two such classi ﬁcation algorithms.\\nOne is based on the well-known SVMs [39] and the other oneis a modi ﬁed version of SVMs as proposed in [30].\\nA. Two Class SVMs FAU Detection\\nIn our approach in order to detect the activated FAUs, the\\ngrid deformation feature vector\\nis\\nused as an input to 17 two class SVMs systems, each one\\ndetecting a speci ﬁc FAU. Each SVMs system, uses the grid\\nnodes geometrical displacements to decide whether a speci ﬁc\\nFAU is activated at the grid under examination or not. The\\nth SVM\\n is trained with the examples in\\nas positive ones and all\\nother examples\\n as\\nnegative ones.\\nIn order to train the\\n th SVMs network, the following mini-\\nmization problem has to be solved [41]\\n(3)\\nsubject to the separability constraints\\nwhere\\n is the bias for the\\n th SVM,\\n is\\nthe slack variable vector and\\n is the term that penalizes the\\ntraining errors.\\nAfter solving the optimization problem (3) subject to the\\nseparability constraints (4) [39], [42], the function that decides\\nwhether the\\n th FAU is activated by a test displacement feature\\nvector\\n is\\n(4)\\nwhere\\n is an arbitrary dimensional Hilbert space [43] and\\n. In this formulation, a nonlinear mapping\\nhas been used for a high-dimensional feature mapping for ob-\\ntaining a linear SVMs system in which it should be\\n .This mapping is de ﬁned by a positive kernel function,\\n ,\\nspecifying an inner product in the feature space and satisfying\\nthe Mercer condition [39], [42]\\n(5)\\nThe functions used as SVMs kernels were the\\n degree polyno-\\nmial function\\n(6)\\nand the Radial Basis Function (RBF) kernel\\n(7)\\nwhere\\n is the spread of the Gaussian function.\\nB. Modiﬁed Two Class SVMs\\nThe other classi ﬁer tested for FAU detection is based on a\\nmodi ﬁed two class SVMs formulation proposed in [30]. The\\napproach in [30] was motivated by the fact that the Fisher ’s\\ndiscriminant optimization problem for two classes is a con-\\nstraint least-squares optimization problem [30], [44], [45]. Theproblem of minimizing the within-class variance has been re-formulated so that it can be solved by constructing the optimal\\nseparating hyperplane for both separable and nonseparable\\ncases. The modi ﬁed SVMs [30] has been applied successfully\\nin order to weight the elastic graph nodes local similarity valueaccording to their corresponding discriminant power for frontal\\nface veri ﬁcation. It has been shown that it outperforms the\\nclassical\\n1SVMs approach. More details about the motivations\\nof this modi ﬁed SVMs class can be found in [30].\\n1) Linear Case: In order to form the optimization problem\\nof the SVMs proposed in [30], we should de ﬁne the within class\\nscatter matrix of the training set in (8), shown at the bottom ofthe page, where\\nand\\n are the mean vectors of the classes\\nand\\n , respectively. In this approach, we assume that the within\\nscatter matrix\\n is invertible (which is true in our case, since\\n1The term classical SVMs refers to the maximal margin SVMs proposed in\\n[39]\\n(8)\\n178 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nthe dimensionality of the vector\\n is classically smaller than\\nthe number of available training examples). The optimizationproblem of the modi ﬁed SVMs is [30]\\n(9)\\nsubject to the separability constraints (4) (here, we refer to the\\nlinear case where\\n ). The solution of the optimization\\nproblem (9) subject to the constraints (4) is given by the saddle\\npoint of the Lagrangian\\n(10)\\nwhere\\n and\\n are the vec-\\ntors of Lagrangian multipliers for the constraints (4). The vector\\ncan be derived from the Kuhn –Tucker (KT) conditions [30]\\n(11)\\nInstead of ﬁnding the saddle point of the Lagrangian (10), we\\nﬁnd the maximization point of the Wolf dual problem [30]\\n(12)\\nsubject to\\n(13)\\nThe above optimization problem can be solved using optimiza-\\ntion packages like [46] or using the “quadprog ”function of\\nMATLAB [47].\\nThe linear decision function that decides whether the\\n th FAU\\nis activated in the geometrical displacement vector\\n , or not, is\\n(14)\\n2) Nonlinear Case: The nonlinear multiclass decision\\nsurfaces can be created in the same manner as the two class\\nnonlinear decision surfaces that have been proposed in [30].\\nThat is, in the dual Wolf problem the term\\n is\\nemployed. Assuming that the within scatter matrix is invertible,\\nthis term can be written as\\n .\\nApplying the nonlinear function\\n to the vectors\\n,w eh a v e\\n[30]. Then, we can apply\\nkernel functions in (15) as\\n(15)\\nThe corresponding nonlinear decision function that detects the\\nth FAU in the geometrical displacement vector\\n is given by\\n(16)\\nIV . F ACIAL EXPRESSION RECOGNITION USING\\nMULTICLASS SVM S\\nFor facial expression recognition using multiclass SVMs, the\\ngrid deformation feature vector\\n is used as an input to\\na multi class SVMs system [48]. Six classes were considered\\nfor the experiments, each one representing one of the basic fa-\\ncial expressions (anger, disgust, fear, happiness, sadness, andsurprise). The SVMs system, classi ﬁes the set of the grid ge-\\nometrical displacements to one of the six basic facial expres-\\nsions. More speci ﬁcally, the grid deformation vectors\\n,\\n, are used as an input to the SVMs system. The output\\nof the SVMs system is a label that classi ﬁes the grid deforma-\\ntion under examination to one of the six basic facial expressions.\\nIn this section, we will also show how the two class SVMs\\ndescribed in Section III-B2 can be extended to multiclass classi-ﬁcations problems using the multiclass SVMs formulation pre-\\nsented in [39], [49], and [50]. In the experimental results section,\\nwe will show that the modi ﬁed multiclass SVMs outperforms\\nthe ones proposed in [39], [49], and [50].\\nA. Multiclass SVMs\\nA brief conversation about the optimization problem of the\\nmulticlass SVMs will be given below. The interested reader can\\nrefer to [39], [41] [49], [50] and the references therein for for-\\nmulating and solving multiclass SVMs optimization problems.\\nThe training data are\\nwhere\\nthe are grid deformation vectors and\\n are the\\nfacial expression labels of the feature vector. The multiclass\\nSVMs problem solves only one optimization problem [49]. Itconstructs six facial expressions rules, where the\\nth function\\nseparates training vectors of the class\\n from the\\nrest of the vectors, by minimizing the objective function\\n(17)\\nsubject to the constraints\\n(18)\\nis the function that maps the deformation vectors to a higher\\ndimensional space, where the data are supposed to be linearly ornear linearly separable.\\nis the term that penalizes the training\\nerrors. The vector\\n is the bias vector and\\n\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 179\\nis the slack variable vector. Then, the de-\\ncision function is\\n(19)\\nUsing this procedure, a test grid deformation feature vector is\\nclassi ﬁed to one of the six facial expressions using (19). Once\\nthe six-class SVMs system is trained, it can be used for testing,i.e., for recognizing facial expressions on new facial imagesequences. For the solution of the optimization problem (17)\\nsubject to the constraints (18) someone can refer to [39], [49],\\nand [50].\\nB. Modi ﬁed Class of Multiclass SVMs\\nIn this section, a novel multiclass SVMs method extending\\nthe constraint optimization problem in (9), is proposed. Thisnovel multiclass SVMs method is the generalization of the twoclass modi ﬁed SVMs described in Section III-B.\\n1) Linear Case: Let that the within class scatter matrix of\\nour grid deformation feature vectors\\nis deﬁned as\\n(20)\\nwhere six is the number of facial expression classes and\\n is the\\ngeometrical displacement vector for the class\\n . In this section,\\nwe assume that the within class scatter matrix\\n is invertible,\\nwhich holds in our case, since classically for our deformation the\\nfeature vector dimension is smaller than the available training\\nexamples.\\nThe modi ﬁed constraint optimization problem is\\n(21)\\nsubject to the separability constraints in (18) (in the linear case\\n). The solution of the above constraint optimization\\nproblem can be given by ﬁnding the saddle point of the La-\\ngrangian\\n(22)\\nwhere\\n and\\nare the Lagrangian multipliers\\nfor the constraints (18) with\\n(23)and constraints\\n(24)\\nThe Lagrangian (22) has to be maximized with respect to\\nand\\n and minimized with respect to\\n and\\n . In order to pro-\\nduce a more compact equation form, let us de ﬁne the following\\nvariables:\\nand\\nif\\nif\\n .(25)\\nAfter a series of manipulations shown in Appendix I, the search\\nof the saddle point of the Lagrangian (22) is reformulated to themaximization of the Wolf dual problem\\n(26)\\nwhich is a quadratic function in terms of\\n with the linear\\nconstraints\\n(27)\\nThe above optimization problem can be solved using optimiza-\\ntion packages like [49]. The corresponding decision hyperplane\\nis\\n(28)\\nas is detailed in Appendix I.\\n2) Nonlinear Case: The nonlinear multiclass decision sur-\\nfaces can be created in the same manner as the two class non-\\nlinear decision surfaces that have been proposed in [30] and aredescribed in Section III-B2. That is, we exploit the fact thatthe term\\ncan be written in terms of dot products as\\n. Then, we can apply kernels in (26)\\nas\\n(29)\\nthe corresponding decision surface is\\n(30)\\n180 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nFig. 4. Example of each facial expression for a poser from the Cohn –Kanade\\ndatabase.\\nV. E XPERIMENTAL RESULTS\\nA. Database Description\\nThe Cohn –Kanade database [2] was used for the facial\\nexpression recognition in six basic facial expressions classes\\n(anger, disgust, fear, happiness, sadness, and surprise). This\\ndatabase is annotated with FAUs. These combinations of FAUswere translated into facial expressions according to [3], in\\norder to de ﬁne the corresponding ground truth for the facial\\nexpressions. All the subjects were taken under consideration to\\nform the database for the experiments.\\nIn Fig. 4, a sample of an image for every facial expression for\\none poser from this database, is shown.\\nThe most usual approach for testing the generalization perfor-\\nmance of a SVMs classi ﬁer, is the leave-one cross-validation ap-\\nproach. The leave-one out method [7] was used in order to make\\nmaximal use of the available data and produce averaged classi ﬁ-\\ncation accuracy results. The term leave-one out cross-validation,\\ndoes not correspond to the classic leave-one-out de ﬁnition here,\\nas a variant of leave-one-out is used (i.e., leave 20% out) for the\\nformation of the test dataset. However, the procedure followed\\nwill be called leave-one-out from now on. More speci ﬁcally, all\\nimage sequences contained in the database are divided into six\\nclasses, each one corresponding to one of the six basic facial ex-\\npressions to be recognized. Neutral state is not considered as a\\nclass, as the system tries to recognize the fully expressed facial\\nexpression starting from the neutral state. Five sets containing20% of the data for each class, chosen randomly, were created.\\nOne set containing 20% of the samples for each class is used for\\nthe test set, while the remaining sets form the training set. After\\nthe classi ﬁcation procedure is performed, the samples forming\\nthe testing set are incorporated into the current training set, anda new set of samples (20% of the samples for each class) is ex-\\ntracted to form the new test set. The remaining samples create\\nthe new training set. This procedure is repeated ﬁve times. A di-\\nagram of the leave-one-out cross-validation method can be seen\\nin Fig. 5. The average classi ﬁcation accuracy is the mean value\\nof the percentages of the correctly classi ﬁed facial expressions.\\nFig. 5. Diagram of leave-one-out method.\\nThe accuracy achieved for each facial expression is averaged\\nover all facial expressions and does not provide any information\\nwith respect to a particular expression. The confusion matrices\\n[10] have been computed to handle this problem. The confusion\\nmatrix is a\\n matrix containing the information about the\\nactual class label\\n (in its columns) and the label obtained\\nthrough classi ﬁcation\\n (in its rows). The diagonal entries of\\nthe confusion matrix are the rates of facial expressions that are\\ncorrectly classi ﬁed, while the off-diagonal entries correspond\\nto misclassi ﬁcation rates. The abbreviations an, di, fe, ha, sa,\\nandsurepresent anger, disgust, fear, happiness, sadness, and\\nsurprise, respectively.\\nB. Representative FAU and Grid Node Selection\\nThe rules proposed in [3] require the detection of 17 FAUs.\\nThe use of so many FAUs makes the rules sensitive to false FAUdetection or rejection. In order to simplify the rules, a small\\nset of rules are proposed for facial expression classi ﬁcation that\\nyield better performance in the experiments performed.\\nFrom all the FAUs appearing in the facial expression descrip-\\ntion rules, many describe two or more facial expressions. Those\\nthat appear once in every facial expression rule are chosen to\\ndescribe uniquely the facial expressions under examination. Forexample, FAU 26 appears in every facial expression. Thus, its\\npresence is irrelevant when de ﬁning a facial expression, as no\\nfacial expression could be speci ﬁed. Therefore, a FAU that ex-\\nists in only one facial expression rule should be speci ﬁed for\\neach facial expression. Where it is not possible, a unique com-\\nbination of FAUs should be de ﬁned instead.\\n•For facial expression anger, the FAUs that appear once\\nare the FAUs 23 and 24. The rest FAUs that participate in\\nthe facial expression rule are observed two or more times.\\nFAUs 23 and 24 do not participate in the rest of facial ex-\\npressions rules for the other ﬁve facial expressions. Anger\\nshould, therefore, be de ﬁned by the appearance of those\\ntwo FAUs.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 181\\nFig. 6. Eight most representative FAUs used for facial expression recognition.\\nFig. 7. PFEG according to FACS, used for the experiments.\\n•For the disgust facial expression, the FAU that appears only\\nonce is the FAU 9. It is also uniquely observed in disgust\\nrules, thus making it appropriate for disgust classi ﬁcation.\\n•For the fear facial expression, the FAU that appears only\\nonce is the FAU 20. Since it appears only in fear facial ex-\\npression rule, it will be the only one taken under consider-\\nation when recognizing fear.\\n•For the happiness facial expression, the FAU that appears\\nonly once is the FAU 12. However, in Fig. 6, it can be seen\\nthat FAUs 12 and 16 appear the same. Therefore, facialexpression happiness will be recognized if FAUs 12 and\\n16 exist (both of them).\\n•For the sadness facial expression, the FAU that appears\\nonly once is the FAU 15. Since it appears only the in sad-\\nness facial expression rule, it will be the only one taken\\nunder consideration when recognizing sadness.\\n•For the surprise facial expression, the FAUs that appear\\nonly once are FAUs 2, 5. Since they appear only in the\\nsurprise facial expression rule, they will be the ones taken\\nunder consideration when recognizing surprise.\\nThe deformed Candide grid produced by the grid-tracking al-\\ngorithm [51] that corresponds to the greatest intensity of the fa-cial expression shown, contains 104 nodes. Only some of thesenodes are important for facial expression recognition. For ex-\\nample, nodes on the outer face contour do not contribute muchto facial expression recognition. Thus, a subset of 62 nodes\\nis chosen that controls the facial deformations. The grid that\\nis composed of these nodes can be seen in Fig. 7. From thispoint onward, this grid will be called Primary Facial Expres-\\nsion Grid (PFEG) . Fig. 6 presents the eight FAUs chosen as the\\nmost representative for each facial expression. The FAU de ﬁni-\\ntion image, as provided by Ekman and Friesen [52], is depicted,as well as its application to a poser from the Cohn –Kanade data-\\nbase used in our experiments.\\nC. FAUs Detection\\nIn this section, only FAUs detection is described. The method\\nfollowed was the application of either the classical two class\\nSVMs (described in Section III-A) or the modi ﬁed two class\\nSVMs (described in Section III-B). The accuracy rates obtainedfor FAUs detection using RBF and polynomial functions as ker-nels, for both the classical two class SVMs as well as the mod-\\niﬁed two class SVMs and the original set of FAUs (FAUs 1, 2,\\n4, 5, 6, 7, 9, 10, 12, 15, 16, 17, 20, 23, 24, 25, and 26, 17 FAUstotal) proposed in [3], are presented in Fig. 8. The equivalentFAUs detection accuracies obtained when using our proposed\\n182 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nFig. 8. Accuracy rates obtained for the set of 17 FAUs detection.\\nFig. 9. Accuracy rates obtained for the set of eight FAUs detection.\\nset of rules (corresponding to FAUs 5, 9, 12, 15, 16, 20, 23, and\\n24, eight FAUs total), are presented in Fig. 9.\\n1) FAUs Detection Using Candide Grid: The FAUs detection\\naccuracy was measured as the percentage of the correctly rec-ognized FAUs. The ground truth for FAUs was provided by theCohn –Kanade database annotation. The achieved FAUs detec-\\ntion accuracy when the set of 17 FAUs was under examination\\nand the modi ﬁed two class SVMs and Candide grid were used,\\nwas equal to 82.7%. The equivalent FAUs detection accuracyrate, when our set of FAUs (subset of the 17 set of FAUs) was\\ntaken under consideration was equal to 93.5%.\\n2) FAUs Detection Using PFEG Grid: The achieved FAUs\\nrecognition accuracy when the original set of FAUs was underexamination and the modi ﬁed two class SVMs and PFEG grid\\nwere used, was equal to 84.7%. The equivalent FAUs recogni-\\ntion accuracy when our set of FAUs was taken under considera-tion was equal to 94.5%. The detection accuracy achieved by theproposed method for FAUs detection is quite satisfactory, when\\ncompared with the state of the art facial expression recognition\\nperformance for the Cohn –Kanade database [26]. More specif-\\nically, in [26], the recognition accuracy achieved was equal to95,6%, when using the Cohn –Kanade database. The detected\\nFAUs can be separated in two groups, those of upper and those\\nof lower face. The accuracies achieved were 95.4% and 95.6%,respectively, although the classi ﬁcation method followed was\\nnot the leave-one-out procedure, as used in this paper.D. Facial Expression Recognition Using the Detected FAUs\\nIn this section, facial expression recognition from the already\\ndetected FAUs is performed, either using the original set of rulesas proposed in [3], or the newly proposed one. When the originalset of FAUs (17 FAUs total) was used to describe the six basic\\nfacial expressions, and the Candide grid, taking under consider-\\nation 104 nodes was applied, the facial expression recognitionaccuracy achieved was equal to 87.9%. When the chosen FAUssubset (FAUs 5, 9, 12, 15, 16, 20, 23, and 24, eight FAUs total)\\nwas used to describe the six basic facial expressions, and the\\nCandide grid was applied, the equivalent recognition accuracyachieved was equal to 92.5%.\\nRegarding the application of PFEG grid (taking under consid-\\neration 62 grid nodes), the recognition accuracy obtained for the\\nsix basic facial expressions when the original set of 17 FAUs wasused, was equal to 93.75%. The equivalent recognition accu-racy achieved when the PFEG grid and the proposed set of eight\\nFAUs were used, was equal to 95.1%. Thus, when the FAUs an-\\nnotation is available, the equivalent depicted facial expressioncan be recognized with an recognition accuracy of 95.1%, ifonly the FAUs 5, 9, 12, 15, 16, 20, 23, and 24, are taken into\\nconsideration.\\nThe accuracy rates obtained for facial expression recognition\\nfrom the detected FAUs using the proposed set of rules (eightFAUs) and applying the PFEG grid are presented in Fig. 10.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 183\\nFig. 10. Accuracy rates obtained for facial expression recognition from the detected FAUs using the PFEG grid.\\nTABLE II\\nCONFUSION MATRICES FOR FAU D ETECTION -BASED FACIAL EXPRESSION RECOGNITION\\nWHEN USING THE CANDIDE AND PFEG G RID AND THE PROPOSED SET OF FAU R ULES\\nTABLE III\\nCONFUSION MATRICES FOR FACIAL EXPRESSION RECOGNITION USING MULTICLASS\\nSVM SWHEN APPLYING THE MODIFIED SVM S TO THE CANDIDE AND PFEG G RID\\nTheﬁrst confusion matrix shown in Table III presents the results\\nobtained while using the Candide grid and the new set of rules\\nproposed. As can be seen, the most ambiguous facial expressionwas disgust, since it was misclassi ﬁed the most times (as anger\\nand then sadness). The facial expressions that follow, are anger\\nand sadness, with a similar misclassi ﬁcation rate. The second\\nconfusion matrix shown in Table II presents the results obtainedwhile using the PFEG grid and the new set of rules proposed.As can be seen, the most ambiguous facial expression remaineddisgust, since it was misclassi ﬁed most times, followed by anger\\nand then sadness.\\nE. Facial Expression Recognition Using Multiclass SVMs\\nIn this section, facial expression recognition directly from the\\ngrid nodes displacements is described. The method followed\\nwas the application of either the classical six class SVMs (de-scribed in Section IV-A) or the modi ﬁed six class SVMs (de-\\nscribed in Section IV-B).\\n184 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nFig. 11. Accuracy rates obtained for facial expression recognition using multiclass SVMs.\\n1) Facial Expression Recognition Using Candide Grid:\\nWhen the classical six class SVMs were applied to the classicCandide grid, taking under consideration 104 nodes, the facial\\nexpression recognition accuracy achieved was equal to 91.4%.\\nThe equivalent facial expression recognition accuracy, whenthe modi ﬁed six class SVMs were used, was equal to 98.2%.\\nTherefore, the introduction of the modi ﬁed six class SVMs\\nincreases the recognition accuracy by 6.8%. The ﬁrst confusion\\nmatrix shown in Table III presents the results obtained whileapplying the modi ﬁed six class SVMs to the Candide grid. As\\ncan be seen, the most ambiguous facial expression was anger,\\nbeing misclassi ﬁed as sadness or disgust.\\n2) Facial Expression Recognition Using PFEG Grid: When\\nthe classical six class SVMs were applied to the PFEG grid,\\ntaking under consideration 62 nodes, the facial expression\\nrecognition accuracy achieved was equal to 95.75%. The equiv-alent facial expression recognition accuracy when the modi ﬁed\\nsix class SVMs were used, was equal to 99.7%. Therefore,\\nthe introduction of the modi ﬁed six class SVMs increases the\\nrecognition accuracy by 3.95%. The second confusion matrix\\nshown in Table III, presents the results obtained while applyingthe modi ﬁed six class SVMs to the PFEG grid. As can be\\nseen, the most ambiguous facial expression remains anger,\\nsince it was the only one being misclassi ﬁed as another facial\\nexpression (sadness). The recognition accuracies obtained\\nfor facial expression recognition using six class SVMs, RBF,and polynomial functions as kernels for both the classical as\\nwell as the modi ﬁed six class SVMs, are presented in Fig. 11.\\nPolynomial kernels offer better recognition performance.\\nThe recognition accuracy achieved by the proposed method\\nfor facial expression recognition is better than any other re-ported in the literature so far for the Cohn –Kanade database,\\nat least according to the authors knowledge. More speci ﬁcally,\\nin [7] the recognition accuracy achieved was equal to 74.5%\\nfor the Cohn –Kanade database and leave-one-out approach,\\nwhile in [8], it was 90.7%. Generally speaking, the best fa-\\ncial expression recognition accuracy reported so far, is equal\\nto 96.1% [53].\\nIn order to understand if the proposed modi ﬁed class of\\nSVMs approach is statistically signi ﬁcant better than the clas-\\nsical SVMs approach, the McNemar ’s [54] has been used.\\nMcNemar ’s test is a null hypothesis statistical test based ona Bernoulli model. If the resulting\\n-value is below a desired\\nsigni ﬁcance level (for example, 0.02), the null hypothesis is re-\\njected and the performance difference between two algorithmsis considered to be statistically signi ﬁcant. Using this test, it\\nhas been veri ﬁed that the modi ﬁed class of SVMs outperforms\\nthe other tested classi ﬁers in the demonstrated experiments at a\\nsigni ﬁcant level less that\\n.\\nThe experiments indicated that for both approaches, the whole\\nsystem is fast enough to perform almost real-time facial expres-\\nsion recognition, on a PC having an Intel Centrino (1.5 GHz)\\nprocessor with 1-GB RAM memory, since it is able to classify\\nexpressions at a rate of 20 frames per second during testing.\\nVI. C ONCLUSION\\nTwo novel methods for facial expression recognition using\\nSVMs for facial expression recognition are proposed in thispaper. A novel class for SVMs classi ﬁers that incorporates sta-\\ntistical information of the classes under examination, is also pro-\\nposed. The user initializes some of the Candide grid nodes on thefacial image depicted at the ﬁrst frame of the image sequence.\\nThe tracking system used, based on deformable models, tracksthe facial expression as it evolves over time, by deforming theCandide grid, eventually producing the grid that correspondsto the facial expression ’s greatest intensity classically depicted\\nat the last facial video frame. Only Candide nodes that in ﬂu-\\nence the formation of FAUs are used in our system.Their ge-ometrical displacement, de ﬁned as their coordinate difference\\nbetween the last and the ﬁrst frame, is used as an input to the\\nSVMs system (either the classical one or the modi ﬁed one). In\\nthe case of facial expression recognition, this system is com-posed of one six-class SVMs, one for each one of the six basicfacial expressions (anger, disgust, fear, happiness, sadness, andsurprise) to be recognized. When FAUs detection-based facialexpression recognition is attempted, the SVMs system consistsof eight one-class SVMs, one for each one of the eight chosenFAUs used. The proposed methods, achieve a facial expressionrecognition accuracy of 99.7% and 95.1%, respectively. Theachieved accuracy for facial expression recognition using mul-ticlass SVMs (99.7%) is better than any other reported in the lit-erature so far for the Cohn –Kanade database, at least according\\nto the authors knowledge.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 185\\nAPPENDIX\\nWOLF DUAL PROBLEM FOR THE MODIFIED MULTICLASS\\nSVM S:In order to ﬁnd the optimum separating hyperplanes for\\nthe optimization problem (21) subject to the constraints (18), we\\nhave to de ﬁne the saddle point of the Langragian (22).\\nIn the saddle point, the solution should satisfy the KT condi-\\ntions for\\n(31)\\n(32)\\nand\\n(33)\\nSubstituting (31) back into (22), we obtain\\n(34)\\nAdding the constraint (33), the terms in\\n disappear. Consid-\\nering the two terms in\\n , only\\nand\\n(35)\\nbut from (32), we have\\n(36)so\\n and the two terms cancel, giving\\n(37)\\nSince\\n ,w eh a v e\\nbut\\n so\\n(38)\\nwhich is a quadratic function in terms of alpha with linear\\nconstraints\\n(39)\\nand\\n(40)\\nThis gives the decision function\\n(41)\\nor equivalently\\n\\n186 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 16, NO. 1, JANUARY 2007\\nREFERENCES\\n[1] P. Ekman and W. V. Friesen , Emotion in the Human Face . Engle-\\nwood Cliffs, NJ: Prentice-Hall, 1975.\\n[2] T. Kanade, J. Cohn, and Y. Tian, “Comprehensive database for facial\\nexpression analysis, ”inProc. IEEE Int. Conf. Face and Gesture Recog-\\nnition , Mar. 2000, pp. 46 –53.\\n[3] M. Pantic and L. J. M. Rothkrantz, “Expert system for automatic anal-\\nysis of facial expressions, ”Image Vis. Comput. , vol. 18, no. 11, pp.\\n881–905, Aug. 2000.\\n[4]—— ,“Automatic analysis of facial expressions: The state of the art, ”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 12, pp. 1424 –1445,\\nDec. 2000.\\n[5] B. Fasel and J. Luettin, “Automatic facial expression analysis: A\\nsurvey, ”Pattern Recognit. , vol. 36, no. 1, pp. 259 –275, 2003.\\n[6] Y. Zhang and Q. Ji, “Active and dynamic information fusion for facial\\nexpression understanding from image sequences, ”IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 27, no. 5, pp. 699 –714, May 2005.\\n[7] I. Cohen, N. Sebe, S. Garg, L. S. Chen, and T. S. Huanga, “Facial\\nexpression recognition from video sequences: temporal and static\\nmodelling, ”Comput. Vis. Image Understand. , vol. 91, pp. 160 –187,\\n2003.\\n[8] M. S. Bartlett, G. Littlewort, I. Fasel, and J. R. Movellan, “Real time\\nface detection and facial expression recognition: Development and ap-\\nplications to human computer interaction, ”inProc. Conf. Computer\\nVision and Pattern Recognition Workshop , Madison, WI, Jun. 16 –22,\\n2003, vol. 5, pp. 53 –58.\\n[9] M. J. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial\\nexpressions with G abor wavelets, ”inProc. 3rd IEEE Int. Conf. Auto-\\nmatic Face and Gesture Recognition , 1998, pp. 200 –205.\\n[10] M. J. Lyons, J. Budynek, and S. Akamatsu, “Automatic classi ﬁcation\\nof single facial images, ”IEEE Trans. Pattern Anal. Mach. Intell. , vol.\\n21, no. 12, pp. 1357 –1362, Dec. 1999.\\n[11] L. Wiskott, J. Fellous, N. Kr üger, and C. v. d. Malsburg, “Face recog-\\nnition by elastic bunch graph matching, ”IEEE Trans. Pattern Anal.\\nMach. Intell. , vol. 19, no. 7, pp. 775 –779, Jul. 1997.\\n[12] G. Guo and C. R. Dyer, “Learning from examples in the small sample\\ncase: Face expression recognition, ”IEEE Trans. Syst., Man, Cybern.\\nB, Cybern. , vol. 35, no. 3, pp. 477 –488, Jun. 2005.\\n[13] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, “Comparison be-\\ntween geometry-based and G abor-wavelets-based facial expression\\nrecognition using multi-layer perceptron, ”inProc. 3rd IEEE Int. Conf.\\nAutomatic Face and Gesture Recognition , Nara, Japan, Apr. 14 –16,\\n1998, pp. 454 –459.\\n[14] B. Fasel, “Multiscale Facial Expression Recognition Using Convolu-\\ntional Neural Networks, ”Tech. Rep., IDIAP, 2002, .\\n[15] M. Matsugu, K. Mori, Y. Mitari, and Y. Kaneda, “Subject independent\\nfacial expression recognition with robust face detection using a convo-\\nlutional neural network, ”Neural Netw. , vol. 16, no. 5 –6, pp. 555 –559,\\nJun.-Jul. 2003.\\n[16] M. Rosenblum, Y. Yacoob, and L. S. Davis, “Human expression\\nrecognition from motion using a radial basis function network archi-\\ntecture, ”IEEE Trans. Neural Netw. , vol. 7, no. 5, pp. 1121 –1138,\\nSep. 1996.\\n[17] L. Ma and K. Khorasani, “Facial expression recognition using con-\\nstructive feedforward neural networks, ”IEEE Trans. Syst., Man, Cy-\\nbern. B, Cybern. , vol. 34, no. 3, pp. 1588 –1595, Jun. 2004.\\n[18] S. Dubuisson, F. Davoine, and M. Masson, “A solution for facial\\nexpression representation and recognition, ”Signal Process.: Image\\nCommun. , vol. 17, no. 9, pp. 657 –673, Oct. 2002.\\n[19] X.-W. Chen and T. Huang, “Facial expression recognition: A clus-\\ntering-based approach, ”Pattern Recognit. Lett. , vol. 24, no. 9 –10, pp.\\n1295 –1302, Jun. 2003.\\n[20] Y. Gao, M. Leung, S. Hui, and M. Tananda, “Facial expression recog-\\nnition from line-based caricatures, ”IEEE Trans. Syst., Man, Cybern.\\nA: Syst. Humans , vol. 33, no. 3, pp. 407 –412, May 2003.\\n[21] B. Abboud, F. Davoine, and M. Dang, “Facial expression recognition\\nand synthesis based on an appearance model, ”Signal Process.: Image\\nCommun. , vol. 19, no. 8, pp. 723 –740, 2004.\\n[22] I. A. Essa and A. P. Pentland, “Facial expression recognition using a\\ndynamic model and motion energy, ”presented at the Int. Conf. Com-\\nputer Vision, Cambrdige, MA, Jun. 20 –23, 1995.\\n[23] —— ,“Coding, analysis, interpretation, and recognition of facial ex-\\npressions, ”IEEE Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp.\\n757–763, Jul. 1997.[24] M. S. Bartlett, G. Littlewort, B. Braathen, T. J. Sejnowski, and J. R.\\nMovellan, “An approach to automatic analysis of spontaneous facial\\nexpressions, ”presented at the 5th IEEE Int. Conf. Automatic Face and\\nGesture Recognition, Washington, DC, 2002.\\n[25] G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski,\\n“Classifying facial actions, ”IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 21, no. 10, pp. 974 –989, Oct. 1999.\\n[26] Y. L. Tian, T. Kanade, and J. F. Cohn, “Recognizing action units for\\nfacial expression analysis, ”IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 23, no. 2, pp. 97 –115, Feb. 2001.\\n[27] J. J. Lien, T. Kanade, J. Cohn, and C. C. Li, “Automated facial ex-\\npression recognition based on facs action units, ”inProc. 3rd IEEE\\nInt. Conf. Automatic Face and Gesture Recognition , Apr. 1998, pp.\\n390–395.\\n[28] J. J. Lien, T. Kanade, J. F. Cohn, and C. Li, “Detection, tracking, and\\nclassi ﬁcation of action units in facial expression, ”J. Robot. Auton.\\nSyst., Jul. 1999.\\n[29] Y. L. Tian, T. Kanade, and J. Cohn, “Evaluation of G abor wavelet-\\nbased facial action unit recognition in image sequences of increasing\\ncomplexity, ”inProc. 5th IEEE Int. Conf. Automatic Face and Gesture\\nRecognition , 2002, pp. 229 –234.\\n[30] A. Tefas, C. Kotropoulos, and I. Pitas, “Using support vector machines\\nto enhance the performance of elastic graph matching for frontal face\\nauthentication, ”IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23, no. 7,\\npp. 735 –746, Jul. 2001.\\n[31] H. Drucker, W. Donghui, and V. Vapnik, “Support vector machines\\nfor spam categorization, ”IEEE Trans. Neural Netw. , vol. 10, no. 5, pp.\\n1048 –1054, Sep. 1999.\\n[32] A. Ganapathiraju, J. Hamaker, and J. Picone, “Applications of support\\nvector machines to speech recognition, ”IEEE Trans. Signal Process. ,\\nvol. 52, no. 8, pp. 2348 –2355, Aug, 2004.\\n[33] M. Pontil and A. Verri, “Support vector machines for 3D object recog-\\nnition, ”IEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 6, pp.\\n637–646, Jun. 1998.\\n[34] M. Rydfalk, “CANDIDE: A Parameterized Face, ”Tech. Rep.,\\nLinkoping Univ., 1978.\\n[35] F. Dornaika and F. Davoine, “Simultaneous facial action tracking and\\nexpression recognition using a particle ﬁlter,”presented at the IEEE\\nInt. Conf. Computer Vision, Beijing, China, Oct. 17 –20, 2005.\\n[36] S. B. Gokturk, C. Tomasi, B. Girod, and J.-Y. Bouguet, “Model-based\\nface tracking for view-independent facial expression recognition, ”in\\nProc. 5th IEEE Int. Conf. Automatic Face and Gesture Recognition ,\\nCambridge, U.K., May 2002, pp. 287 –293.\\n[37] M. Malciu and F. Preteux, “Tracking facial features in video sequences\\nusing a deformable model-based approach, ”Proc. SPIE , vol. 4121, pp.\\n51–62, 2000.\\n[38] P. Michel and R. Kaliouby, “Real time facial expression recognition\\nin video using support vector machines, ”inProc. 5th Int. Conf. Multi-\\nmodal interfaces , Vancouver, BC, Canada, 2003, pp. 258 –264.\\n[39] V. Vapnik , Statistical learning theory . New York: Wiley, 1998.\\n[40] J. Y. Bouguet, “Pyramidal Implementation of the Lucas-Kanade Fea-\\nture Tracker, ”Tech. Rep., Intel Corporation, Microprocessor Research\\nLabs, 1999.\\n[41] C. W. Hsu and C. J. Lin, “A comparison of methods for multiclass\\nsupport vector machines, ”IEEE Trans. Neural Netw. , vol. 13, no. 2,\\npp. 415 –425, Mar. 2002.\\n[42] C. J. C. Burges, “A tutorial on support vector machines for pattern\\nrecognition, ”Data Mining Knowl. Disc. , vol. 2, no. 2, 1998.\\n[43] B. Scholkopf, S. Mika, C. Burges, P. Knirsch, K.-R. Muller, G. Ratsch,\\nand A. Smola, “Input space vs. feature space in kernel-based methods, ”\\nIEEE Trans. Neural Netw. , vol. 10, no. 5, pp. 1000 –1017, Sep. 1999.\\n[44] K.-R. Muller, S. Mika, G. Ratsch, K. Tsuda, and B. Scholkopf, “An\\nintroduction to kernel-based learning algorithms, ”IEEE Trans. Neural\\nNetw. , vol. 12, no. 2, pp. 181 –201, Mar. 2001.\\n[45] R. Duda and P. Hart , Pattern Classi ﬁcation and Scene Analysis .N e w\\nYork: Wiley, 1973.\\n[46] S. Gunn, “Support vector machines for classi ﬁcation and regression, ”\\nMP-TR-98-05, Image, Speech, Intell. Syst. Group, Univ. Southampton,\\n1998.\\n[47] MATLAB User ’s Guide . Natick, MA: The MathWorks, Inc.,\\n1994 –2001.\\n[48] L. Bottou, C. Cortes, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y.\\nLeCun, U. Muller, E. Sackinger, P. Simard, and V. Vapnik, “Compar-\\nison of classi ﬁer methods: A case study in handwritting digit recogni-\\ntion,”inProc. Int. Conf. Pattern Recognition , 1994, pp. 77 –87.\\nKOTSIA AND PITAS: FACIAL EXPRESSION RECOGNITION IN IMAGE SEQUENCES 187\\n[49] J. Weston and C. Watkins, “Multi-Class Support Vector Machines, ”\\nTech. Rep. CSD-TR-98-04, 2004.\\n[50] J. Weston and C. Watkins, “Multi-class support vector machines, ”pre-\\nsented at the ESANN, Brussels, Belgium, 1999.\\n[51] S. Krinidis and I. Pitas, “2-D physics-based deformable shape models:\\nExplicit governing equations, ”inProc. 1st Int. Workshop on Interactive\\nRich Media Content Production: Architectures, Technologies, Applica-\\ntions, Tools , Lausanne, Switzerland, Oct. 16 –17, 2003, pp. 43 –55.\\n[52] P. Ekman and W. V. Friesen, “FACS —Facial Action Coding System, ”\\n1978 [Online]. Available: http://www-2.cs.cmu.edu/afs/cs/project/\\nface/www/facs.htm\\n[53] F. Dornaika and F. Davoine, “View- and texture-independent facial ex-\\npression recognition in videos using dynamic programming, ”presented\\nat the IEEE Int. Conf. Image Processing Genova, Italy, Sep. 11 –14,\\n2005.\\n[54] L. Gillick and S. Cox, “Some statistical issues in the comparison of\\nspeech recognition algorithms, ”inProc. ICASSP , 1989, pp. 532 –535.\\nIrene Kotsia received the B.S. degree from the\\nDepartment of Informatics, Aristotle University of\\nThessaloniki, Thessaloniki, Greece, in 2002, where\\nshe is currently pursuing the Ph.D. degree in the\\nDepartment of Informatics.\\nShe is currently a Researcher and Teaching Assis-\\ntant. Her current research interests lie in the areas offacial expression recognition from static images and\\nimage sequences, as well as in the area of graphics\\nand animation.\\nIoannis Pitas (SM’94) received the Diploma of\\nelectrical engineering and the Ph.D. degree in\\nelectrical engineering from the Aristotle University\\nof Thessaloniki, Thessaloniki, Greece, in 1980 and\\n1985, respectively.\\nSince 1994, he has been a Professor at the De-\\npartment of Informatics, Aristotle University of\\nThessaloniki, where he served as Scienti ﬁc Assis-\\ntant, Lecturer, Assistant Professor, and Associate\\nProfessor in the Department of Electrical and Com-\\nputer Engineering from 1980 to 1993. He served as a\\nVisiting Research Associate at the University of Toronto, Toronto, ON, Canada;\\nthe University of Erlangen-Nuernberg, Nuernberg, Germany; and the Tampere\\nUniversity of Technology, Tampere, Finland. He also served as Visiting\\nAssistant Professor at the University of Toronto and Visiting Professor at theUniversity of British Columbia, Vancouver, BC, Canada. He was a Lecturer in\\nshort courses for continuing education. He has published over 380 papers and\\ncontributed to 13 books in his areas of interest. He is the co-author of the books\\nNonlinear Digital Filters: Principles and Applications (Norwell, MA: Kluwer,\\n1990), 3-D Image Processing Algorithms (New York: Wiley, 2000), Nonlinear\\nModel-Based Image/Video Processing and Analysis (New York: Wiley, 2001),\\nand author of Digital Image Processing Algorithms and Applications (New\\nYork: Wiley, 2000). He is also the Editor of the book Parallel Algorithms\\nand Architectures for Digital Image Processing, Computer Vision and Neural\\nNetworks (New York: Wiley, 1993). His current interests are in the areas of\\ndigital image and video processing and analysis, multidimensional signal\\nprocessing, watermarking, and computer vision.\\nDr. Pitas has been member of the European Community ESPRIT Parallel\\nAction Committee. He has also been an invited speaker and/or member of the\\nprogram committee of several scienti ﬁc conferences and workshops. He was\\nan Associate Editor of the IEEE T\\nRANSACTIONS ON CIRCUITS AND SYSTEMS ,\\nthe IEEE T RANSACTIONS ON NEURAL NETWORKS , the IEEE T RANSACTIONS ON\\nIMAGE PROCESSING , the EURASIP Journal on Applied Signal Processing , and\\nCo-Editor of Multidimensional Systems and Signal Processing andSignal Pro-\\ncessing . He was the General Chair of the 1995 IEEE Workshop on Nonlinear\\nSignal and Image Processing, Technical Chair of the 1998 European Signal Pro-\\ncessing Conference, and General Chair of IEEE International Conference on\\nImage Processing 2001.\\n',\n",
       " \"1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n1\\nFacial Expression Recognition in Video with\\nMultiple Feature Fusion\\nJunkai Chen, Zenghai Chen, Zheru Chi, Member, IEEE and Hong Fu,\\nAbstract—Video based facial expression recognition has been a long standing problem and attracted growing attention recently. The\\nkey to a successful facial expression recognition system is to exploit the potentials of audiovisual modalities and design robust features\\nto effectively characterize the facial appearance and conﬁguration changes caused by facial motions. We propose an effective\\nframework to address this issue in this paper. In our study, both visual modalities (face images) and audio modalities (speech) are\\nutilized. A new feature descriptor called Histogram of Oriented Gradients from Three Orthogonal Planes (HOG-TOP) is proposed to\\nextract dynamic textures from video sequences to characterize facial appearance changes. And a new effective geometric feature\\nderived from the warp transformation of facial landmarks is proposed to capture facial conﬁguration changes. Moreover, the role of\\naudio modalities on recognition is also explored in our study. We applied the multiple feature fusion to tackle the video-based facial\\nexpression recognition problem under lab-controlled environment and in the wild, respectively. Experiments conducted on the extended\\nKohn-Kanada (CK+) database and the Acted Facial Expression in Wild (AFEW) 4.0 database show that our approach is robust in\\ndealing with video-based facial expression recognition problem under lab-controlled environment and in the wild compared with the\\nother state-of-the-art methods.\\nIndex Terms—Facial expression recognition; Multiple feature fusion; HOG-TOP; Geometric warp feature; Acoustic feature.\\nF\\n1 I NTRODUCTION\\nFACIAL expression, as a powerful nonverbal channel,\\nplays an important role for human beings to convey\\nemotions and transmit messages. Automatic facial expres-\\nsion recognition (AFEC) can be widely applied in many\\nﬁelds such as medical assessment, lie detection and human\\ncomputer interaction [1]. AFEC has attracted great interest\\nin the past two decades. However, facial expression analysis\\nis a very challenging task because facial expressions caused\\nby facial muscle movements are subtle and transient [2]. To\\ncapture and represent these movements is a key issue to be\\naddressed in facial expression analysis.\\nTwo main streams of facial expressions analysis are\\nwidely adopted in the current research and development.\\nOne stream is to detect facial actions. The study reported\\nin [3], [4] showed that each facial expression contains a u-\\nnique group of facial action units. The Facial Action Coding\\nSystem (FACS), which was ﬁrst proposed by Ekman and\\nFriesen in 1978 [5] and then enhanced in 2002 [6], is the\\nbest known system developed for human beings to describe\\nfacial actions. Another stream of facial expression analysis is\\nto carry out facial affect (emotion) recognition directly. Most\\nresearchers deal with the recognition task of six universal\\nemotions: happy, sad, fear, disgust, angry and surprise [7].\\nMany efforts have been made for facial expression recog-\\nnition. The methodologies used are commonly categorized\\n\\x0fJunkai Chen and Zheru Chi are with the Department of Electronic and\\nInformation Engineering, The Hong Kong Polytechnic University, Hong\\nKong. E-mail: Junkai.Chen@connect.polyu.hk, chi.zheru@polyu.edu.hk\\n\\x0fZenghai Chen is with the School of Electrical and Electronic Engi-\\nneering, Nanyang Technological University, Singapore. E-mail: zeng-\\nhaichen@ntu.edu.sg\\n\\x0fHong Fu is with the Department of Computer Science, Chu Hai College\\nof Higher Education, Hong Kong. E-mail: hongfu@chuhai.edu.hkinto appearance based methods and geometry based meth-\\nods [8]. An appearance based method applies feature de-\\nscriptors to model facial texture changes. A geometry based\\nmethod captures facial conﬁgurations in which a set of facial\\nﬁducial points is used to characterize the face shape.\\nPrevious works mainly focused on static and single\\nface image based facial expression recognition. Recently,\\nfacial expression recognition in video has attracted great\\ninterest. Compared with a static image, a video sequence\\ncan not only provide spatial appearance but also include\\nfacial motions and accompanied speech. The key to solve\\nthe problem of video based facial expression recognition is\\nto exploit the representation capability of multi modalities\\n(e.g. visual and audio information) and design robust fea-\\ntures to effectively characterize the facial appearance and\\nconﬁguration changes caused by facial muscular activities.\\nTo achieve this goal, we propose an effective frame-\\nwork based on multiple feature fusion for facial expression\\nrecognition in video. We explore the potentials of visual\\nmodalities (face images) and audio modalities (speech) in\\nour study. In addressing visual modalities, we extend the\\nHistograms of Oriented Gradients (HOG) [9] to temporal\\nThree Orthogonal Planes (TOP), inspired by a temporal\\nextension of Local Binary Patterns, LBP-TOP [10]. The pro-\\nposed HOG-TOP is used to characterize facial appearance\\nchanges. We show that HOG-TOP performs as well as LBP-\\nTOP for facial expression recognition. In addition, compared\\nwith LBP-TOP , HOG-TOP is more compact and effective\\nto characterize facial appearance changes. Moreover, an\\neffective geometric warp feature derived from the warp\\ntransformation of facial landmarks is proposed to capture\\nfacial conﬁguration changes. We show that the proposed\\ngeometric warp feature is more effective compared with\\nother proposed geometric features [11], [12]. We also ex-\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n2\\nInput video  \\nFace sequences  \\nFacial landmarks  Facial \\nconfiguration \\nchanges  \\nAcoustic feature  \\nFeatures pool  Geometric \\nfeature \\n… \\nDynamic textures (HOG-TOP) \\nAcoustic \\nKernel  \\nGeometric \\nKernel  Appearance \\nKernel  Multiple kernel SVM for \\nfacial expression recognition  \\nin the wild  \\nMultiple kernel SVM for   \\nlab-controlled  \\nfacial expression recognition  \\nFig. 1  \\nFig. 1: Block diagram of our proposed framework. Geometric features coupled with dynamic textures (HOT-TOP) are used\\nto deal with lab-controlled facial expression recognition while acoustic features and dynamic textures (HOG-TOP) are fused\\nto tackle facial expression recognition in the wild.\\nplore the role of audio modalities on affect recognition. We\\nﬁnd that audio modalities can provide some complemen-\\ntary information, especially for facial expression recognition\\nin the wild. We further apply a multiple feature fusion\\nmethod to deal with facial expression recognition under\\nlab-controlled environment and in the wild, respectively. As\\nshown in Fig. 1, geometric features coupled with dynamic\\ntextures (HOT-TOP) are used to deal with lab-controlled fa-\\ncial expression recognition. Acoustic features and dynamic\\ntextures (HOG-TOP) are fused to tackle facial expression\\nrecognition in the wild.\\nOur contributions are summarized as follows:\\n1. We develop a framework which can effectively tackle\\nfacial expression recognition in video. A multiple feature\\nfusion method is used to deal with facial expression\\nrecognition under lab-controlled environment and in the\\nwild, respectively.\\n2. We propose a new feature descriptor HOG-TOP to\\ncharacterize facial appearance changes and a new effective\\ngeometric feature to capture facial conﬁguration changes.\\n3. We show that multiple features can make different\\ncontributions and can achieve better performance than\\nthe individual features applied alone. We also show that\\nmultiple feature fusion can enhance the discriminative\\npower of multiple features.\\nThe remainder of the paper is organized as follows. In\\nSection 2, we review some related work. Section 3 presents\\nour proposed approach. Experimental results and discus-\\nsions are presented in Section 4. The paper is concluded in\\nSection 5.2 R ELATED WORK AND MOTIVATION\\n2.1 Static Image Based Methods\\nMany researchers apply static image based models to han-\\ndle facial expression problem. One or several peak frames\\nare usually selected for extracting appearance or geometric\\nfeatures. For instance, the methods reported in [11], [12], [13]\\napplied the facial landmarks to characterize the whole face\\nshape. And the method [14] measured the displacements\\nof several selected candidate ﬁducial points. Bag of Words\\n(BoW) based on the multi-scale dense SIFT features were\\napplied to represent facial appearance textures in [15]. Local\\nFisher discriminant analysis (LFDA) was used for feature\\nextraction in [16]. The method [17] applied Gabor ﬁlters\\nto extract facial movement features. A novel framework\\nfor expression recognition by using appearance features of\\nselected facial patches was proposed in [18]. However, au-\\ntomatically picking out key frames from a video sequence is\\nusually difﬁcult. The methods [19], [20] attempted to classify\\nevery frame ﬁrst and adopt a voting strategy to label the\\nvideo sequence. It is necessary to extract features from each\\nframe. LBP was applied in [19]. Pyramid of Histograms of\\nOriented Gradients (PHOG) and Local Phase Quantization\\n(LPQ) features were used in [20].\\n2.2 Dynamic Texture Based Methods\\nThere exists a drawback for a static image based method:\\nextracting features from an individual frame fails to utilize\\ndynamic information which is important to describe facial\\nmotions. Dynamic texture based methods can effectively\\ndeal with this problem. Dynamic texture based methods\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n3\\nattempt to simultaneously model the spatial appearance\\nand dynamic motions in a video sequence. Zhao et al. [10]\\nproposed LBP-TOP , a temporal extension of local binary\\npatterns, for facial expression analysis in video. A facial\\ncomponent LBP-TOP was proposed in [21]. A Local Gabor\\nBinary Patterns from Three Orthogonal Planes (LGBP-TOP)\\nwas proposed in [22]. A SpatioTemporal Local Monogenic\\nBinary Pattern (STLMBP) feature descriptor was proposed\\nin [23], [24]. In addition, Long et al. [25] employed Inde-\\npendent Component Analysis (ICA) to learn spatiotemporal\\nﬁlters from videos, and then extracted dynamic textures\\nusing the learned ﬁlters. Chew et al. [26] employed sparse\\ntemporal representation to model the temporal dynamics of\\nfacial expressions in video. Li et al. [27] developed a dy-\\nnamic Bayesian network to simultaneously and coherently\\nrepresent the facial evolvement at different levels.\\n2.3 Audiovisual Based Methods\\nStatic image based methods or dynamic texture based\\nmethods only rely on visual modalities. However, audio\\nor speech is also important for human beings to convey\\nemotions and intentions. Audio modalities can provide\\nsome complementary information in addition to visual\\nmodalities. Recently, audiovisual based methods for affect\\nrecognition have attracted growing attention from the affec-\\ntive computing community. A number of approaches have\\nbeen proposed to combine audio and visual modalities for\\naffect recognition (e.g. [28], [29], [30], [31]). A comprehensive\\nsurvey can be found in [32]. Acoustic features extracted\\nfrom voice or speech and visual features extracted from\\nface images are combined to tackle this problem. For ex-\\nample, voice and lip activity were used in [33]. Face images\\nand speech were employed in [34]. The methods reported\\nin [35], [36] applied several feature descriptors such as SIFT,\\nHOG, PHOG etc. to encode face images and combined them\\nwith acoustic features to recognize facial expression in the\\nwild.\\n2.4 Motivation\\nWe can see that feature extraction plays a center role on\\naffect recognition in video. Designing an effective feature\\nis important and meaningful. LBP-TOP is widely used for\\nmodeling dynamic textures. However, there are two limita-\\ntions of LBP-TOP . One is the high dimensionality. The size of\\nLBP-TOP coded using a uniform pattern is 59\\x023[10]. More-\\nover, although LBP-TOP is robust to deal with illumination\\nchanges, it is insensitive to facial muscle deformations. In\\nthis work, we propose a new feature called HOG-TOP ,\\nwhich is more compact and effective to characterize facial\\nappearance changes. More details on HOG-TOP can be\\nfound in Section 3.1.\\nIn addition, conﬁguration and shape representations\\nplay an important role in human vision for the perception\\nof facial expressions [37]. We believe that previous works\\nhave not yet fully exploited the potentials of conﬁgura-\\ntion representations. Characterizing face shape [11], [12] or\\nmeasuring displacements of ﬁducial points [14], [38] only\\nare not sufﬁcient to capture facial conﬁguration changes,\\nespecially the subtle non-rigid changes. In this work, we\\nintroduce a more robust geometric feature to capture facial\\nx \\ny t \\nX-Y X-T \\nY-T \\nFig. 2  Fig. 2: The textures in XY, XT and YT planes.\\nY X T \\nX-T \\nY-T X-Y \\nFig. 3  \\nFig. 3: The HOG from Three Orthogonal Planes (TOPs).\\nconﬁguration changes. More discussion on our proposed\\ngeometric feature is given in Section 3.2.\\n3 M ETHODOLOGY\\nThis section presents the details of our proposed approach.\\nWe introduce the three types of features and multiple feature\\nfusion employed in our study.\\n3.1 Histogram of Oriented Gradients from Three Or-\\nthogonal Planes\\nHistograms of oriented gradients (HOG) [9] were ﬁrst pro-\\nposed for human detection. The basic idea of HOG is that\\nlocal object appearance and shape can often be characterized\\nrather well by the distribution of local intensity gradients or\\nedge directions. HOG is sensitive to object deformations.\\nFacial expressions are caused by facial muscle movements.\\nFor example, mouth opening and raised eyebrows will gen-\\nerate a surprise facial expression. These movements could\\nbe regarded as types of deformations. HOG can effectively\\ncapture and represent these deformations [39]. However,\\nthe original HOG is limited to deal with a static image.\\nIn order to model dynamic textures from a video sequence\\nwith HOG, we extend HOG to 3-D to compute the oriented\\ngradients on three orthogonal planes XY, XT, and YT (TOP),\\ni.e. HOG-TOP . The proposed HOG-TOP is used to charac-\\nterize facial appearance changes.\\nA video sequence includes three orthogonal directions,\\ni.e. X, Y, and T (time) directions. The XY plane provides spa-\\ntial appearance, and XT and YT planes record temporal or\\nmotion information. Fig. 2 illustrates the textures extracted\\nfrom the three orthogonal planes. In our study, we compute\\nthe distributions of oriented gradients of each plane and\\nobtain HOG features, namely HOG-XY, HOG-XT and HOG-\\nYT, as shown in Fig. 3. Each point in a video sequence\\nincludes three orthogonal neighborhoods lying on XY, XT\\nand YT planes, respectively. We ﬁrst compute the gradients\\nalong X, Y and T directions with a 3\\x023Sobel mask. The gra-\\ndient orientations are deﬁned as \\x12XY=tan\\x001(GY=GX),\\n\\x12XT=tan\\x001(GT=GX),\\x12YT=tan\\x001(GT=GY), where\\nGX,GY, andGTare the gradients along the X, Y and T\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n4\\ndirections, respectively. These angles are quantized into K\\n(K is 9 in our work) orientation bins with a range of 0\\x0e\\x00360\\x0e\\nor0\\x0e\\x00180\\x0e.\\nWe enumerate the appearance of these gradient orien-\\ntations and obtain a histogram in each plane. The three\\nhistograms are concatenated to form a global description\\nwith the spatial and temporal features. Fig. 3 shows that the\\nthree histograms from the three planes are combined into a\\nsingle one. The HOG-TOP computation algorithm is shown\\nin Algorithm 1.\\nAlgorithm 1 Compute the HOG-TOP .\\nInput: Video sequence V , which contains Nframes with\\nthe same width and height.\\nOutput: The histograms of oriented gradients from three\\northogonal plans (HOG-TOP).\\nAlgorithm:\\nGet the number of frames N, frame width Wand height\\nH.\\nfort= 2 :N\\x001do\\nforx= 2 :W\\x001do\\nfory= 2 :H\\x001do\\nget the local patch in XY, XT, and YT planes.\\nPxy=V(x\\x001 :x+ 1;y\\x001 :y+ 1;t);\\nPxt=V(x\\x001 :x+ 1;y;t\\x001 :t+ 1);\\nPyt=V(x;y\\x001 :y+ 1;t\\x001 :t+ 1);\\nCompute the gradients GX,GY, andGT; and\\ngradient orientations \\x12XY,\\x12XT,\\x12YT. Quantize the\\n\\x12XY,\\x12XT,\\x12YTinto one of 9 bins. Get a histogram\\nin each plan, i.e. HOG-XY, HOG-XT and HOG-YT.\\nend for\\nend for\\nend for\\nNormalize the HOG-XY, HOG-XT and HOG-YT respec-\\ntively. Concatenate the three histograms into a long his-\\ntogram.\\nLBP-TOP computes the difference of a pixel with respect\\nto its neighborhood, making LBP-TOP robust in dealing\\nwith illumination changes. HOG-TOP computes the orient-\\ned gradients of a pixel, which is more sensitive to object\\ndeformations [9]. Facial expressions are caused by facial\\nmuscle movements, which can be regarded as types of\\nmuscle deformations. HOG-TOP is therefore more effective\\nto characterize facial appearance changes than LBP-TOP .\\nAnother advantage of HOG-TOP is the feature dimen-\\nsionality. Compared with LBP-TOP , the size of HOG-TOP is\\nmuch smaller than that of LBP-TOP . The size of LBP-TOP\\ncoded using a uniform pattern is 59\\x023[10], [40]. and the\\nsize of HOG-TOP quantized into 9 bins is 9\\x023, which is\\nmuch more compact than that of LBP-TOP .\\nIn order to utilize local spatial information, a block-based\\nmethod is introduced in our study, as shown in Fig. 4. We\\ncan divide the image sequence into many blocks and extract\\nthe HOG-TOP features from each block. The HOG-TOP\\nfeatures of all the blocks can be concatenated to represent\\nthe whole sequence. In our experiments, the face is ﬁrst\\ncropped from the original image and resized to 128\\x02128.\\nWe partition the face image into 8\\x028blocks with each block\\nhaving a size of 16\\x0216. The number of bins is set to 9 with\\n… Features of one block  \\nFeatures extracted from the whole sequence.  \\nFig. 4  Fig. 4: The HOG-TOP features extracted from each block are\\nconcatenated together to represent the whole sequence.\\nan angle range of 0\\x0e\\x00180\\x0e.\\n3.2 Geometric Warp Feature\\nIn this section, we introduce a more robust geometric feature\\nnamely geometric warp feature, which is derived from the\\nwarp transform of the facial landmarks. Facial expressions\\nare caused by facial muscle movements. These movements\\nresult in the displacements of the facial landmarks. Here we\\nassume that each face image consists of many sub-regions.\\nThese sub-regions can be formed with triangles with their\\nvertexes located at facial landmarks, as shown in Fig. 5. The\\ndisplacements of facial landmarks cause the deformations\\nof the triangles. We propose to utilize the deformations to\\nrepresent facial conﬁguration changes.\\nFacial expression can be considered as a dynamic process\\nincluding onset, peak and offset. We consider the displace-\\nment of the corresponding facial landmarks between onset\\n(neutral face) and peak (expressive face). Given a set of\\nfacial landmarks s= (x1;y1;x2;y2;:::xn;yn), where (xi;yi)\\ndenote the coordinates of the i-th facial landmark. These\\nfacial landmarks make up the mesh of a face, as shown in\\nFig. 5.\\nAs we can see, there are many small triangles in the face,\\nand each triangle is determined by three facial landmarks.\\nFacial muscle movements cause the deformations of the\\ntriangles when a neutral face transforms to an expressive\\nface. We consider a pixel (x;y)which lies in a triangle\\n\\x01ABC belonging to the neutral face and the corresponding\\npixel (u;v)lies in a triangle \\x01A0B0C0belonging to the\\nexpressive face, as shown in Fig. 6. From [41], we know that\\nthe pixel (x;y)can be expressed with a linear combination\\nof the three vertexes.\\n\\x14\\nx\\ny\\x15\\n=\\x14\\nx1\\ny1\\x15\\n+\\x151\\x14\\nx2\\x00x1\\ny2\\x00y1\\x15\\n+\\x152\\x14\\nx3\\x00x1\\ny3\\x00y1\\x15\\n(1)\\nAnd the coefﬁcients \\x151;\\x152can be obtained as\\n\\x151=(x\\x00x1)(y3\\x00y1)\\x00(y\\x00y1)(x3\\x00x1)\\n(x2\\x00x1)(y3\\x00y1)\\x00(y2\\x00y1)(x3\\x00x1)(2)\\n\\x152=(x2\\x00x1)(y\\x00y1)\\x00(y2\\x00y1)(x\\x00x1)\\n(x2\\x00x1)(y3\\x00y1)\\x00(y2\\x00y1)(x3\\x00x1)(3)\\nThe point (u;v)in the triangle \\x01A0B0C0of the expres-\\nsive face can be deﬁned with the three vertexes and \\x151;\\x152,\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n5\\nFig. 5  \\nFig. 5: Facial landmarks describe the shape of a face.\\nFig. 6  \\n11()u ,v\\n22()u ,v\\n33()u ,v\\n11()x , y\\n33()x , y\\n22()x , y\\nA\\nC\\nB\\n'A\\n'C\\n'B\\n()x,y\\n()u,v\\nFig. 6: A pixel (x;y)in a triangle \\x01ABC of the neutral face\\nis transformed to another pixel (u;v)in a triangle \\x01A0B0C0\\nof the expressive face.\\n\\x14\\nu\\nv\\x15\\n=\\x14\\nu1\\nv1\\x15\\n+\\x151\\x14\\nu2\\x00u1\\nv2\\x00v1\\x15\\n+\\x152\\x14\\nu3\\x00u1\\nv3\\x00v1\\x15\\n(4)\\nCombining Eq. (2) with Eq. (3), Eq. (4) can be rewritten\\nas:\\n\\x14\\nu\\nv\\x15\\n=\\x14\\na1+a2x+a3y\\na4+a5x+a6y\\x15\\n(5)\\nEach pair of triangles between the neutral face and the\\nexpressive face can deﬁne a unique transform and each\\nafﬁne transform is determined by 6 parameters a1;a2;:::;a 6.\\nWe compute the 6 parameters for each warp transform\\nand concatenate all the parameters as a long global feature\\nvector, which is used to characterize facial conﬁguration\\nchanges. We will show by experiments that the proposed\\ngeometric warp feature is more effective than the other\\ngeometric features [11], [14], [38].\\n3.3 Acoustic Feature\\nVisual modalities (face images) and audio modalities\\n(speech) can both convey the emotions and intentions of\\nhuman beings. Audio modalities also provide some useful\\nclues for affect recognition in video. For instance, with voice\\nsignal, the method [42] proposed an enhanced autocorrela-\\ntion (EAC) feature for emotion recognition in video.\\nOne successful acoustic feature extraction is to obtain\\nthe time series of multiple paralinguistic descriptors and\\nthen using pooling operations on each time series to ex-\\ntract feature vectors. Schuller et al. [43] showed how to\\ncompute the acoustic features by taking 21 functionals of 38\\nlow level descriptors and their ﬁrst regression coefﬁcients.\\nThe 38 low-level descriptors shown in Table 1 are ﬁrst\\nextracted and smoothed by simple moving average low-\\npass ﬁltering. After that, 21 functionals are employed and 16\\nzero-information features are eliminated. Finally, two single\\nfeatures: the number of onsets (F0) and turn duration are\\nadded. A total of 1,582 acoustic features are extracted fromTABLE 1: Acoustic features: 38 low level descriptors along\\nwith their ﬁrst regression coefﬁcients and 21 functional-\\ns [43].\\nDescriptors Functionals\\nPCM loudness Position max./min.\\nMFCC (0-14) Arithmetic Mean\\nlog Mel Freq. Band (0-7) skewness, kurtosis\\nLSP Frequency (0-7) lin. regression coeff.\\nF0 lin. regression error\\nF0 Envelope quartile\\nVoicing Prob. quartile range\\nJitter local percentile\\nJitter consec. frame pairs percentile range\\nShimmer local up-level time\\neach video. These acoustic features include energy/spectral\\nLow Level Descriptors (LLD) (top 6 items in Table 1) and\\nvoice related LLD (bottom 4 items in Table 1).\\nWe explore the representation ability of acoustic features\\nfor affect recognition in our study. Experiments show that\\naudio modalities (speech) can provide useful complemen-\\ntary information in addition to visual modalities. The visual\\nfeatures coupled with acoustic features can achieve better\\nperformance for facial expression recognition in the wild.\\n3.4 Multiple Feature Fusion\\nFeatures from different modalities can make different con-\\ntributions. Traditional SVM concatenates different features\\ninto a single feature vector and built a single kernel for all\\nthese different features. However, constructing a kernel for\\neach type of features and integrating these kernels optimally\\ncan enhance the discriminative power of these features.\\nThe study in [44] showed that using multiple kernels with\\ndifferent types of features can improve the performance of\\nSVM. A multiple kernel SVM is designed to learn both the\\ndecision boundaries between data from different classes and\\nthe kernel combination weights through a single optimiza-\\ntion problem [45].\\nGiven a training set with labeled samples D=\\nf(xi;yi)jxi2Rn;yi2 f\\x00 1;1ggN\\ni=1. A decision line is\\nobtained by solving the following primal optimization prob-\\nlem,\\nmin\\nw;b1\\n2kwk2\\ns:t: yi(wxi+b)\\x151;i= 1;2;:::N(6)\\nIn general, we solve the dual form of the primal opti-\\nmization problem. The dual formulation of the traditional\\nsingle kernel SVM optimization problem is given by\\nmax\\n\\x0b2\\n4NX\\ni=1\\x0bi\\x001\\n2NX\\ni=1NX\\nj=1\\x0bi\\x0bjyiyjKij3\\n5\\ns:t:NX\\ni=1\\x0biyi= 0;0\\x14\\x0bi\\x14C(7)\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n6\\nwhereKijis the kernel matrix, and Kij=k(xi;xj), here\\nk(\\x01;\\x01)is the kernel function and xi;xjare the feature vec-\\ntors.\\nMultiple kernel fusion applies a linear combination of\\nmultiple kernels to substitute for the single kernel. In our\\nstudy, we adopt the formulation proposed in [46] in which\\nthe kernel is actually a convex combination of basis kernels:\\nKij=MX\\ni=1\\x0cmkm(xi;xj)\\ns:t: \\x0cm\\x150;MX\\ni=1\\x0cm= 1(8)\\nWe apply a multiple kernel fusion framework to deal\\nwith facial expression recognition under lab-controlled en-\\nvironment and in the wild, respectively, as shown in Fig. 1.\\nHOG-TOP and acoustic feature are optimally fused to han-\\ndle the problem of facial expression recognition in the wild,\\nwhile HOG-TOP and geometric warp feature are combined\\nto tackle the problem of facial expression recognition under\\nlab-controlled environment.\\nIn the followings, we detail how to ﬁnd an optimal\\ncombination of HOG-TOP and acoustic feature for facial\\nexpression recognition in the wild. It can be easily extended\\nto the problem of facial expression recognition under lab-\\ncontrolled environment.\\nWe denote the dynamic texture HOG-TOP as xand\\nacoustic feature as z, then we have\\nKij=\\x0ck1(xi;xj) + (1\\x00\\x0c)k2(zi;zj) (9)\\nwith 0\\x14\\x0c\\x141, whereKis the kernel matrix, k1(\\x01;\\x01);k 2(\\x01;\\x01)\\nare the basis kernels. The basis kernels could be linear\\nkernel, radial basis function (RBF) kernel and polynomial\\nkernel, etc. We need to learn the kernel weight \\x0cand\\ncoefﬁcients \\x0b. In our study, we construct a linear kernel for\\neach type of feature and build a two-step method to search\\nfor the optimal values of \\x0cand\\x0b. We set two nested iterative\\nloops to optimize both the classiﬁer and kernel combination\\nweights. In the outer loop, we adopt the grid search to ﬁnd\\nthe kernel weight \\x0c. In the inner iteration, a solver of SVM\\n(LIBSVM [47] is used in our work) is implemented by ﬁxing\\nthe kernel weight \\x0cto ﬁnd the coefﬁcients \\x0b. Then given a\\nnew sample which contains visual feature HOG-TOP xand\\nacoustic feature z, the predict label y can be obtained by\\ny=sgn(NX\\ni=1yi\\x0bi(\\x0ck 1(xi;x) + (1\\x00\\x0c)k2(zi;z)) +b)(10)\\nIn our work, the one-vs-one method is employed to deal\\nwith the multiclass-SVM problem and we adopt the max-\\nwin voting strategy to do the classiﬁcation. Finally, the \\x0c\\nvalue and\\x0bvalues with the highest overall classiﬁcation\\naccuracy in the validation data set are obtained as the\\noptimal kernel weight and coefﬁcients.\\n4 E XPERIMENTAL RESULTS AND DISCUSSIONS\\n4.1 Data sets\\nIn order to evaluate our methods, we conduct the experi-\\nments on three public data sets: the Extended Cohn-Kanade(CK+) data set [11], GEMEP-FERA 2011 data set [19] and the\\nActed Facial Expression in Wild (AFEW) 4.0 data set [48]. We\\nﬁrst give a brief description of the three data sets.\\nTheExtended Cohn-Kanade (CK+) data set contains 593\\nimage sequences from 123 subjects. The face images in the\\ndatabase are lab-controlled. The image sequences vary in\\nduration from 10 to 60 frames. In total, 327 of 593 image\\nsequences have emotion labels and each is categorized into\\none of the following seven emotion classes: anger (An), con-\\ntempt (Co), disgust (Di), fear (Fe), happiness (Ha), sadness\\n(Sa) and surprise (Su). Each image sequence changes from\\nthe onset (the neutral frame) to the peak (the expressive\\nframe). In addition, the X-Y coordinates of 68 facial land-\\nmark points were given for each image in the database. The\\nlandmark points of key frames within each video sequence\\nwere manually labelled, while the remaining frames were\\nautomatically aligned using the AAM ﬁtting algorithm [41].\\nTheGEMEP-FERA 2011 data set contains 289 sequences\\nof 10 actors, who are trained by a professional director. It\\nis divided into a training set of 155 sequences and a test\\nset of 134 sequences. Each sequence is categorized into the\\nfollowing ﬁve emotions: anger (An), fear (Fe), happiness\\n(Ha), relief (Re) and sadness (Sa). Only the training set\\nprovides emotion labels. This database is more challenging\\nthan the CK+ database, since there are head movements and\\ngesture variations in image sequences.\\nTheActed Facial Expression in Wild (AFEW) 4.0 data\\nsetincludes video clips collected from different movies\\nwhich are believed to be close to real world conditions.\\nThe database splits into a training set, a validation set\\nand a test set. There are 578 video clips in the training\\nset. The validation and test sets have 383 video clips and\\n407 video clips, respectively. Each video clip belongs to\\none of the seven categories: anger (An), disgust (Di), fear\\n(Fe), happiness (Ha), neutral (Ne), sadness (Sa), and sur-\\nprise (Su). This database provides original video clips and\\naligned face sequences. They applied the model proposed\\nin [49] to extract the faces from video clips and aligned\\nthe faces. Different from the CK+ and GEMEP-FERA 2011\\ndata sets, facial expressions in AFEW 4.0 are more natural\\nand spontaneous. The variations in illumination, pose and\\nbackground in image sequences increase the complexity of\\nfacial expression analysis.\\nFig. 7 shows the selected image sequences from the three\\ndatabases. The ﬁrst row is the face images from the CK+\\ndatabase, which are frontal-view and lab-controlled faces.\\nThe middle row shows the images from the GEMP-FERA\\n2011 database; there exist head movements and gesture vari-\\nations. The bottom row is an image sequence from AFEW\\n4.0 database. We can see that the background is complex\\nand there exist illumination changes and pose variations.\\n4.2 Feature Extraction\\nIn our experiments, three types of features are employed,\\nnamely HOG-TOP , geometric warp feature and acoustic\\nfeature.\\nIn extracting HOG-TOP from image sequences, each face\\nimage is ﬁrst cropped and resized to 128\\x02128. The resized\\nface image is then partitioned into 8\\x028blocks with a size\\nof16\\x0216. The bin number is set to 9 with an angle range\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n7\\nFig. 7  \\nFig. 7: The selected image sequences from the three databases. From top to bottom: CK+, GEMEP-FERA2011 and AFEW\\n4.0.\\nof0\\x0e\\x00180\\x0e. In each block, we can obtain a HOG-TOP with\\na dimension of 3\\x029 = 27 . We then concatenate the HOG-\\nTOP of the 8\\x028blocks into a long feature vector with a\\ndimension of 3\\x029\\x028\\x028 = 1728 .\\nFacial landmarks are used to compute the geometric\\nwarp features. We compute the warp transform of facial\\nlandmarks between the neutral face and an expressive face.\\nEach face contains 68 facial landmarks. These facial land-\\nmarks divide the face into many non-overlap sub regions\\nby Delaunay triangulation. In our work, we take 109 pair\\nof triangles (the smallest number of triangles available in\\nface images). Each pair of triangles between the neutral\\nface and an expressive face can deﬁne a unique transform\\nand each afﬁne transform is determined by six parameters\\n(see Section 3.2). The warp transform coefﬁcients are ﬁnally\\nconcatenated as a feature vector of 6\\x02109 = 654 elements\\nto represent the geometric warp feature.\\nThe acoustic features with a length of 1582 used in our\\nwork are provided by the database [34], [48]. The acoustic\\nfeatures are extracted by applying the open-source Emotion\\nAffect Recognition (openEAR) toolkit [50] backend OpenS-\\nMILE [51].\\n4.3 Experimental Results\\n4.3.1 A Comparison of HOG-TOP and LBP-TOP\\nWe ﬁrst compare the performance of HOG-TOP proposed\\nin our work with LBP-TOP proposed in [10]. When we\\ncompute the LBP-TOP features, we take the general settings\\nadopted in most reported works. The resized face image is\\npartitioned into 4\\x024blocks. The LBP-TOP is coded with a\\nuniform pattern. The LBP-TOP histogram of each block is a\\nfeature vector of 3\\x0259 = 177 elements. The length of the\\nfeature vector consists of 4\\x024blocks is 3\\x0259\\x024\\x024 = 2832 .\\nThere are 327 image sequences with emotion labels be-\\nlonging to 118 subjects in the CK+ database. We follow the\\nprotocol proposed in [11] and take the leave-one-subject-out\\ncross validation strategy. Each time the samples from one\\nsubject are used for testing and the remaining samples from\\nall other subjects are used for training. In order for each\\nsubject to be evaluated once, we carry out 118 validations.\\nThe classiﬁcation accuracy acquired on the CK+ database by\\nusing two types of features is shown Table 2.\\nWe also compare the performance of the two features in\\nthe GEMEP-FERA 2011 database. Since only the emotionTABLE 2: The classiﬁcation accuracy of LBP-TOP and HOG-\\nTOP on the CK+ database (%).\\nLBP-TOP HOG-TOP\\nAnger 75.6 88.9\\nContempt 88.9 66.7\\nDisgust 93.2 94.9\\nFear 80.0 76.0\\nHappiness 98.5 95.6\\nSadness 78.6 67.9\\nSurprise 92.8 97.6\\nOverall 89.3 89.6\\nTABLE 3: The classiﬁcation accuracy of LBP-TOP and HOG-\\nTOP on the GEMEP-FERA 2011 database (%).\\nLBP-TOP HOG-TOP\\nAnger 56.2 43.7\\nFear 26.7 36.7\\nJoy 58.1 61.3\\nRelief 51.6 54.8\\nSad 74.2 74.2\\nOverall 53.6 54.2\\nlabels of training set are publicly available, we do the\\nevaluation on the training set. There are seven subjects in\\nthe training set. We adopt the leave-one-subject-out strategy\\nand carry out seven cross validations. Table 3 shows the\\nperformance obtained by applying the two features.\\nAs for the AFEW 4.0 database, we utilize the training\\nset to train an SVM classiﬁer and test the classiﬁer on the\\nvalidation set. The database provided a baseline method [34]\\nwhich employed LBP-TOP to represent the dynamic tex-\\ntures of the video sequence and trained an SVM with non-\\nlinear RBF kernel for emotion classiﬁcation. The accuracy\\nacquired on the AFEW 4.0 database by applying two types\\nof features is shown in Table 4.\\nWe use the overall accuracy to evaluate the performance.\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n8\\nTABLE 4: The classiﬁcation accuracy of LBP-TOP and HOG-\\nTOP on validation set of the AFEW 4.0 database (%).\\nLBP-TOP HOG-TOP\\nNeutral 19.0 58.7\\nAnger 50.0 73.4\\nDisgust 25.0 22.5\\nFear 15.2 4.3\\nHappiness 57.1 60.3\\nSadness 16.4 4.9\\nSurprise 21.7 2.2\\nOverall 30.6 35.8\\nThe overall accuracy is deﬁned as\\nOacc=PN\\nn=1PK\\nk=1mnkPN\\nn=1PK\\nk=1Mnk(11)\\nwhereKis the number of classes, Nis the number of cross\\nvalidation folds, mnkis the number of correctly predicted\\nsamples of the k-th class in the n-th fold, and Mnkdenotes\\nthe total samples of the k-th class in the n-th fold. The\\nclassiﬁcation rate of each individual facial expression (k -th\\nclass) isPN\\nn=1mnkPN\\nn=1Mnk.\\nFrom the experimental results, we can see that the over-\\nall classiﬁcation accuracy obtained by using HOG-TOP on\\nthe CK+ database and GEMEP-FERA 2011 database is 89.6%\\nand 54.2%, respectively. It is competitive with the result of\\n89.3% and 53.6% obtained by applying LBP-TOP on the two\\ndatabases. While the overall classiﬁcation rate of HOG-TOP\\non the AFEW 4.0 database is 35.8%, which is better than\\n30.6% obtained by using LBP-TOP , meaning that HOG-TOP\\nis more robust in capturing the subtle facial appearance\\nchanges in the wild. In addition, HOG-TOP with a length of\\n1728 is more compact than LBP-TOP with a length of 2832.\\nWe further examine the conﬁdence intervals (the variances\\nacross cross-validation folds) of two feature sets. Since each\\nfold in the CK+ database contains several samples only\\n(118 folds (subjects) all together), the variance across cross-\\nvalidation folds is very large and therefore it is not very\\nmeaningful to report the variances for this data set. On the\\nother hand, the training set and validation set are ﬁxed in\\nthe AFEW 4.0 data set and therefore no variance cross the\\nfolds for this data set can be reported. We only compare\\nthe variances of the two feature sets on the GEMEP-FERA\\n2011 database. The variances of HOG-TOP and LBP-TOP are\\n12.7% and 12.6%, respectively, which are comparable with\\neach other. We also compare the computational speeds of\\nthe two features under the 64-bit Win 7 operating system\\nwith a Core i7 CPU. We computed the two features with\\nMatlab 8.2. The computation time depends on the block size\\nand sequence duration. With the same block size (16 \\x0216)\\nand sequence duration (11 frames), the computation time of\\nHOG-TOP and LBP-TOP is 0.027s and 0.042s, respectively,\\nshowing the computational efﬁciency of HOG-TOP .TABLE 5: The comparison results of different geometric\\nfeatures on the CK+ database (%). (GWF is our proposed\\ngeometric warp feature).\\nGWF [11] [14] [38]\\nAnger 86.7 35.0 75.6 62.2\\nContempt 94.4 25.0 – 72.2\\nDisgust 96.6 68.4 88.2 86.4\\nFear 36.0 21.7 76.0 56.0\\nHappiness 98.5 98.4 97.1 91.3\\nSadness 75.0 4.0 89.3 39.3\\nSurprise 96.4 100.0 98.7 95.2\\nOverall 89.0 66.7 87.5 79.2\\nTABLE 6: The classiﬁcation accuracy obtained by using four\\ndifferent feature sets on the CK+ database (%).\\nHOG-TOP Geometric\\nFeatureHybrid\\nFeature IHybrid\\nFeature II\\nAnger 88.9 86.7 95.6 100.0\\nContempt 66.7 94.4 94.4 94.4\\nDisgust 94.9 96.6 94.9 96.6\\nFear 76.0 36.0 52.0 84.0\\nHappiness 95.6 98.5 98.5 100.0\\nSadness 67.9 75.0 78.6 78.6\\nSurprise 97.6 96.4 96.4 98.8\\nOverall 89.6 89.0 91.4 95.7\\n4.3.2 Facial Expression Recognition Under Lab-controlled\\nEnvironment\\nWe develop a model which combines HOG-TOP and ge-\\nometric warp to handle the problem of facial expression\\nrecognition under lab-controlled environment. We evalu-\\nate the following different feature sets: geometric warp\\nfeature, dynamic appearance feature (HOG-TOP), hybrid\\nfeature I and hybrid feature II. Hybrid feature I denotes the\\nfeature vector of concatenating HOG-TOP and geometric\\nwarp feature directly and hybrid feature II is the optimal\\ncombination of the HOG-TOP and geometric warp feature.\\nWe ﬁrst compare our proposed geometric warp feature\\nwith the other geometric features on the CK+ data set.\\nAll the methods take the leave-one-subject-out cross vali-\\ndation. The comparison results are shown in Table 5. The\\nmethod [11] applied a set of facial landmarks to character-\\nize the face shape. The relative distance of eight selected\\nﬁducial points are measured to represent the geometric\\nfeature in [14]. The shifts of the facial landmarks between\\nthe neutral face and the expressive face are computed to\\nrepresent the geometric feature in [38]. We can see that\\nour proposed geometric warp feature achieves a superior\\nperformance compared with the other geometric features,\\nmeaning that the geometric warp feature is more effective\\nto capture facial conﬁguration changes.\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n9\\nFig. 8 An Co Di Fe Ha Sa Su \\nAn 0.87  0.00  0.06  0.00  0.00  0.07  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.06  0.00  \\nDi 0.01  0.00  0.97  0.02  0.00  0.00  0.00  \\nFe 0.00  0.00  0.04  0.36  0.28  0.04  0.28  \\nHa 0.00  0.00  0.00  0.01  0.99  0.00  0.00  \\nSa 0.14  0.00  0.07  0.04  0.00  0.75  0.00  \\nSu 0.00  0.02  0.00  0.00  0.00  0.02  0.96  An Co Di Fe Ha Sa Su \\nAn 0.89  0.02  0.02  0.02  0.00  0.04  0.00  \\nCo 0.17  0.67  0.00  0.05  0.05  0.00  0.06  \\nDi 0.01  0.00  0.95  0.02  0.00  0.00  0.02  \\nFe 0.00  0.00  0.04  0.76  0.08  0.12  0.00  \\nHa 0.00  0.00  0.00  0.01  0.96  0.00  0.03  \\nSa 0.21  0.00  0.00  0.07  0.00  0.68  0.04  \\nSu 0.00  0.01  0.00  0.00  0.00  0.01  0.98  (a) (b) \\n(a)\\nFig. 8 An Co Di Fe Ha Sa Su \\nAn 0.87  0.00  0.06  0.00  0.00  0.07  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.06  0.00  \\nDi 0.01  0.00  0.97  0.02  0.00  0.00  0.00  \\nFe 0.00  0.00  0.04  0.36  0.28  0.04  0.28  \\nHa 0.00  0.00  0.00  0.01  0.99  0.00  0.00  \\nSa 0.14  0.00  0.07  0.04  0.00  0.75  0.00  \\nSu 0.00  0.02  0.00  0.00  0.00  0.02  0.96  An Co Di Fe Ha Sa Su \\nAn 0.89  0.02  0.02  0.02  0.00  0.04  0.00  \\nCo 0.17  0.67  0.00  0.05  0.05  0.00  0.06  \\nDi 0.01  0.00  0.95  0.02  0.00  0.00  0.02  \\nFe 0.00  0.00  0.04  0.76  0.08  0.12  0.00  \\nHa 0.00  0.00  0.00  0.01  0.96  0.00  0.03  \\nSa 0.21  0.00  0.00  0.07  0.00  0.68  0.04  \\nSu 0.00  0.01  0.00  0.00  0.00  0.01  0.98  (a) (b) (b)\\nAn Co Di Fe Ha Sa Su \\nAn 0.96  0.00  0.04  0.00  0.00  0.00  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.00  0.06  \\nDi 0.03  0.00  0.95  0.00  0.02  0.00  0.00  \\nFe 0.00  0.00  0.04  0.52  0.24  0.04  0.16  \\nHa 0.00  0.00  0.00  0.01  0.99  0.00  0.00  \\nSa 0.14  0.00  0.03  0.04  0.00  0.79  0.00  \\nSu 0.00  0.02  0.00  0.00  0.00  0.02  0.96  (c) \\nAn Co Di Fe Ha Sa Su \\nAn 1.00  0.00  0.00  0.00  0.00  0.00  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.00  0.06  \\nDi 0.01  0.00  0.97  0.00  0.00  0.02  0.00  \\nFe 0.00  0.00  0.04  0.84  0.12  0.00  0.00  \\nHa 0.00  0.00  0.00  0.00  1.00  0.00  0.00  \\nSa 0.21  0.00  0.00  0.00  0.00  0.79  0.00  \\nSu 0.00  0.01  0.00  0.00  0.00  0.00  0.99  (d) \\nFig. 8 (c)\\nAn Co Di Fe Ha Sa Su \\nAn 0.96  0.00  0.04  0.00  0.00  0.00  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.00  0.06  \\nDi 0.03  0.00  0.95  0.00  0.02  0.00  0.00  \\nFe 0.00  0.00  0.04  0.52  0.24  0.04  0.16  \\nHa 0.00  0.00  0.00  0.01  0.99  0.00  0.00  \\nSa 0.14  0.00  0.03  0.04  0.00  0.79  0.00  \\nSu 0.00  0.02  0.00  0.00  0.00  0.02  0.96  (c) \\nAn Co Di Fe Ha Sa Su \\nAn 1.00  0.00  0.00  0.00  0.00  0.00  0.00  \\nCo 0.00  0.94  0.00  0.00  0.00  0.00  0.06  \\nDi 0.01  0.00  0.97  0.00  0.00  0.02  0.00  \\nFe 0.00  0.00  0.04  0.84  0.12  0.00  0.00  \\nHa 0.00  0.00  0.00  0.00  1.00  0.00  0.00  \\nSa 0.21  0.00  0.00  0.00  0.00  0.79  0.00  \\nSu 0.00  0.01  0.00  0.00  0.00  0.00  0.99  (d) \\nFig. 8 (d)\\nFig. 8: The confusion matrices obtained by using the four feature sets on the CK+ database: (a) HOG-TOP , (b) geometric\\nfeature, (c) hybrid feature I and (d) hybrid feature II. (An: Anger, Co: Contempt, Di: Disgust, Fe: Fear, Ha: Happiness, Sa:\\nSadness and Su: Surprise).\\nWe further evaluate hybrid feature I and hybrid feature\\nII with the leave-one-subject-out cross validation on the CK+\\ndatabase and compare the performance with that obtained\\nby applying geometric feature and HOG-TOP alone. Table 6\\nshows the classiﬁcation accuracy obtained by the four differ-\\nent feature sets. Fig. 8 shows the confusion matrices of using\\nthe four different feature sets. We can see that the emotions\\n”disgust”, ”happiness” and ”surprise” have higher classiﬁ-\\ncation rates than the other emotions, indicating that these\\nthree emotions are easier to distinguish than the others. We\\nalso note that hybrid feature I (91.4%) and hybrid feature II\\n(95.7%) outperform the geometric warp feature (89.0%) and\\nHOG-TOP (89.6%) applied individually. We can conclude\\nthat different features (hybrid feature I) can provide com-\\nplementary information and multiple feature fusion (hybrid\\nfeature II) can further enhance the discriminative ability of\\nthe combined features .\\nWe also compare our method with the other meth-\\nods. All the methods we compared follow the baseline\\nmethod [11] and take the leave-one-subject-out cross vali-\\ndation. The methods [11], [12] combined geometric feature\\nand appearance feature and trained an SVM to perform the\\nclassiﬁcation. In [21], a weighted component-based feature\\ndescriptor to extract dynamic appearance feature was u-\\ntilized and multiple kernel learning was applied to train\\nthe SVM for recognition. A sparse temporal representation\\nclassiﬁer was proposed for facial expression recognition\\nin [26]. The method in [24] applied spatiotemporal local\\nmonogenic binary pattern (STLMBP) feature to handle the\\nproblem of facial expression recognition.\\nAs can be seen in Table 7, the HOG-TOP (89.6%) and ge-\\nometric feature (89.3%) proposed in our method can achieve\\na competitive performance compared with SPTS+CAPP [11]\\n(88.38%), CLM [12] (82.4%) and STLMBP [24] (88.4%). It\\ndemonstrates the effectiveness of our proposed features.\\nThe hybrid feature II as the optimal combination of HOG-\\nTOP and geometric feature achieves a superior performance\\ncompared with the other methods tested, showing the effec-\\ntiveness of the multiple feature fusion.\\n4.3.3 Facial Expression Recognition in the Wild\\nHOG-TOP and acoustic feature are fused to tackle the\\nproblem of facial expression recognition in the wild. We ﬁrstTABLE 7: Performance comparison with other methods on\\nCK+ database.\\nMethod Accuracy (%)\\nHOG-TOP 89.6\\nGeometric Feature 89.3\\nHybrid Feature II 95.7\\nSPTS+CAPP [11] 88.4\\nCLM [12] 82.4\\nSTLMBP [24] 88.4\\nSTR [26] 94.9\\nCFD [21] 93.2\\nTABLE 8: The classiﬁcation accuracy obtained by using four\\nfeature sets on validation set of the AFEW 4.0 database (%).\\nHOG-TOP Acoustic\\nFeatureHybrid\\nFeature IHybrid\\nFeature II\\nNeutral 58.7 57.1 65.1 69.8\\nAnger 73.4 64.1 75.0 76.6\\nDisgust 22.5 15.0 12.5 17.5\\nFear 4.3 26.1 8.7 15.2\\nHappiness 60.3 34.9 57.1 63.5\\nSadness 4.9 14.7 13.1 9.8\\nSurprise 2.1 0.0 4.4 2.1\\nOverall 35.8 32.9 37.6 40.2\\nevaluate our method on the validation set. Four feature sets\\nare explored: HOG-TOP only, acoustic feature only, hybrid\\nfeature I and hybrid feature II. Hybrid feature I concatenates\\nthe HOG-TOP and acoustic feature directly. Hybrid feature\\nII is the optimal combination of the HOG-TOP and acoustic\\nfeature.\\nTable 8 shows the classiﬁcation accuracy obtained by\\napplying four different feature sets. The corresponding con-\\nfusion matrices are shown in Fig. 9. We can see that the\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n10\\nFig. 9 An Di Fe Ha Ne Sa Su \\nAn 0.73  0.05  0.06  0.02  0.14  0.00  0.00  \\nDi 0.23  0.23  0.08  0.13  0.33  0.03  0.00  \\nFe 0.52  0.11  0.04  0.11  0.17  0.04  0.00  \\nHa 0.05  0.06  0.08  0.60  0.19  0.02  0.00  \\nNe 0.16  0.03  0.05  0.10  0.59  0.08  0.00  \\nSa 0.31  0.13  0.00  0.08  0.43  0.05  0.00  \\nSu 0.26  0.02  0.07  0.09  0.50  0.04  0.02  (a) (b) \\nAn Di Fe Ha Ne Sa Su \\nAn 0.64  0.03  0.05  0.13  0.11  0.02  0.03  \\nDi 0.18  0.15  0.00  0.18  0.35  0.10  0.05  \\nFe 0.33  0.02  0.26  0.13  0.15  0.02  0.09  \\nHa 0.24  0.03  0.11  0.35  0.27  0.00  0.00  \\nNe 0.08  0.10  0.06  0.14  0.57  0.03  0.02  \\nSa 0.02  0.15  0.11  0.23  0.30  0.15  0.05  \\nSu 0.17  0.09  0.13  0.17  0.35  0.09  0.00  \\n(a)\\nFig. 9 An Di Fe Ha Ne Sa Su \\nAn 0.73  0.05  0.06  0.02  0.14  0.00  0.00  \\nDi 0.23  0.23  0.08  0.13  0.33  0.03  0.00  \\nFe 0.52  0.11  0.04  0.11  0.17  0.04  0.00  \\nHa 0.05  0.06  0.08  0.60  0.19  0.02  0.00  \\nNe 0.16  0.03  0.05  0.10  0.59  0.08  0.00  \\nSa 0.31  0.13  0.00  0.08  0.43  0.05  0.00  \\nSu 0.26  0.02  0.07  0.09  0.50  0.04  0.02  (a) (b) \\nAn Di Fe Ha Ne Sa Su \\nAn 0.64  0.03  0.05  0.13  0.11  0.02  0.03  \\nDi 0.18  0.15  0.00  0.18  0.35  0.10  0.05  \\nFe 0.33  0.02  0.26  0.13  0.15  0.02  0.09  \\nHa 0.24  0.03  0.11  0.35  0.27  0.00  0.00  \\nNe 0.08  0.10  0.06  0.14  0.57  0.03  0.02  \\nSa 0.02  0.15  0.11  0.23  0.30  0.15  0.05  \\nSu 0.17  0.09  0.13  0.17  0.35  0.09  0.00  (b)\\n(c) (d) \\nFig. 9 An Di Fe Ha Ne Sa Su \\nAn 0.75  0.06  0.02  0.02  0.11  0.03  0.02  \\nDi 0.23  0.13  0.00  0.23  0.38  0.05  0.00  \\nFe 0.41  0.11  0.09  0.15  0.20  0.04  0.00  \\nHa 0.11  0.03  0.06  0.57  0.19  0.02  0.02  \\nNe 0.08  0.14  0.00  0.06  0.65  0.06  0.00  \\nSa 0.21  0.23  0.03  0.07  0.33  0.13  0.00  \\nSu 0.15  0.11  0.07  0.15  0.48  0.00  0.04  An Di Fe Ha Ne Sa Su \\nAn 0.77  0.06  0.02  0.02  0.11  0.03  0.00  \\nDi 0.28  0.18  0.00  0.15  0.33  0.08  0.00  \\nFe 0.39  0.09  0.15  0.15  0.20  0.02  0.00  \\nHa 0.08  0.05  0.06  0.63  0.16  0.02  0.00  \\nNe 0.10  0.08  0.00  0.08  0.70  0.05  0.00  \\nSa 0.21  0.18  0.00  0.07  0.44  0.10  0.00  \\nSu 0.22  0.04  0.07  0.15  0.50  0.00  0.02  (c)\\n(c) (d) \\nFig. 9 An Di Fe Ha Ne Sa Su \\nAn 0.75  0.06  0.02  0.02  0.11  0.03  0.02  \\nDi 0.23  0.13  0.00  0.23  0.38  0.05  0.00  \\nFe 0.41  0.11  0.09  0.15  0.20  0.04  0.00  \\nHa 0.11  0.03  0.06  0.57  0.19  0.02  0.02  \\nNe 0.08  0.14  0.00  0.06  0.65  0.06  0.00  \\nSa 0.21  0.23  0.03  0.07  0.33  0.13  0.00  \\nSu 0.15  0.11  0.07  0.15  0.48  0.00  0.04  An Di Fe Ha Ne Sa Su \\nAn 0.77  0.06  0.02  0.02  0.11  0.03  0.00  \\nDi 0.28  0.18  0.00  0.15  0.33  0.08  0.00  \\nFe 0.39  0.09  0.15  0.15  0.20  0.02  0.00  \\nHa 0.08  0.05  0.06  0.63  0.16  0.02  0.00  \\nNe 0.10  0.08  0.00  0.08  0.70  0.05  0.00  \\nSa 0.21  0.18  0.00  0.07  0.44  0.10  0.00  \\nSu 0.22  0.04  0.07  0.15  0.50  0.00  0.02  (d)\\nFig. 9: The confusion matrices obtained by using the four feature sets on the validation set of AFEW 4.0 database: (a) HOG-\\nTOP , (b) acoustic feature, (c) hybrid feature I and (d) hybrid feature II. (An: Anger, Di: Disgust, Fe: Fear, Ha: Happiness,\\nNe: Neutral, Sa: Sadness and Su: Surprise).\\nTABLE 9: Performance comparison with other methods on\\nthe test set of the AFEW 4.0 database.\\nMethod Accuracy (%)\\nHybrid Feature II (our method) 45.2\\nLBP-TOP + Voice [34] 33.7\\nLip activity + Voice [33] 35.3\\nSTLMBP [23] 41.5\\nEAC [42] 40.1\\nELM [52] 44.2\\nclassiﬁcation rates are much lower than the results shown\\nin Table 6. Different from facial expressions under lab-\\ncontrolled environment in which the actors or subjects can\\npose distinguished facial expressions, the facial expressions\\nin the wild may be more subtle. The factors including head\\nmovements, pose variations etc. also increase classiﬁcation\\ndifﬁculties. And sometimes, several facial expressions in the\\nwild may appear together, which makes a facial expression\\nto be confused with other expressions. We observe that\\nthe classiﬁcation rate of emotion ”surprise” is the lowest.\\nFrom the confusion matrices shown in Fig. 9, we ﬁnd\\nthe emotion ”surprise” is mostly misclassiﬁed as emotions\\n”anger”, ”happiness” and ”neutral”. The emotions ”anger”\\nand ”neutral” have higher recognition accuracies than the\\nother emotions. Hybrid feature I and hybrid feature II\\noutperform the HOT-TOP and acoustic feature used indi-\\nvidually, indicating that two feature sets are complementary\\nwith each other. Hybrid feature II achieves a superior perfor-\\nmance compared with hybrid feature I, demonstrating that\\nthe effectiveness of the multiple feature fusion in dealing\\nwith the facial expression recognition problem in the wild.\\nWe further apply hybrid feature II which achieves the\\nbest performance on the validation set (the kernel weights\\nof HOG-TOP and acoustic feature are 0.73 and 0.27) to\\nevaluate the test set. The overall recognition accuracy on\\nthe test set is 45.2%. Table 9 shows the results compared\\nwith the other methods. The baseline method [34] combined\\nLBP-TOP and the acoustic feature. Lip activity was incorpo-\\nrated with voice in [33] to tackle the emotion recognition\\nFig. 8  35.8 \\n32.9 37.6 40.2 \\nHOG-TOP Audio Feature Decision-level\\nFusionFeature-level\\nFusionOverall Accuracy (%)  \\n35.8  \\n32.9  37.6  40.2  \\nHOG-TOP Audio Feature Decision-level\\nFusionFeature-level\\nFusion (hybrid\\nfeautre II)Overall Accuracy (%)  Fig. 10: The comparison results of different methods on the\\nvalidation set of AFEW 4.0 database.\\nproblem. The method [23] used dynamic textures only and\\nthe method [42] applied the voice only. The method [35]\\nemployed audiovisual feature for emotion recognition. We\\ncan see that our method (45.2%) improves signiﬁcantly com-\\npared with the baseline method [34] and the method [33],\\nwith an improvement of about 11% and 10%, respectively.\\nOur method is also better than [23] (41.5%) and EAC [42]\\n(40.1%). Compared with the method [52] (44.2%), our per-\\nformance is still competitive. Moreover, we applied our\\nmethod to participate in the second emotion recognition\\nin the wild challenge (EmotiW 2014) [34] and achieved the\\nsecond runner-up award.\\n4.3.4 Decision-level fusion vs Feature-level fusion\\nThe multiple feature fusion applied in our work is a kind\\nof feature-level fusion method. Another technique, namely\\ndecision-level fusion, is also widely used in computer vision\\ncommunity to deal with multiple sets of features. In a\\npreliminary study, we explore the effectiveness of the two\\ntechniques for facial expression recognition in video. A pre-\\nliminary experiment is conducted on AFEW 4.0 database.\\nAs we have mentioned above, we employ the one-vs-one\\ntechnique to tackle the multiclass-SVM problem, and use\\nmax-win voting strategy to conduct the classiﬁcation. For\\ndecision-level fusion, we ﬁrst apply the HOG-TOP and a-\\ncoustic feature separately and then saved the predict results,\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n11\\nTABLE 10: The parameters used for extracting HOG-TOP .\\nImage Size Block Size Overlap Blocks\\n96\\x0296 12 \\x0212 No 8\\x028\\n96\\x0296 24 \\x0224 Half 7\\x027\\n128\\x02128 16 \\x0216 No 8\\x028\\n128\\x02128 32 \\x0232 Half 7\\x027\\ni.e. the number of votes for each class of the two features,\\nrespectively. After that, we add the votes obtained by each\\nindividual feature together and based on the combined\\nvotes, we carry out the max-win voting strategy again\\nto make the ﬁnal decision. The overall classiﬁcation rate\\nis computed as the performance of decision-level fusion\\nmethod.\\nFig. 10 shows the results of the different methods tested.\\nThe overall accuracy of HOG-TOP , acoustic feature and\\nfeature-level fusion (hybrid feature II) shown in Fig. 10 is the\\nsame as shown in Table 8. From the experimental results, we\\nﬁnd that feature-level fusion outperforms the decision-level\\nfusion method, although they both utilize the same multiple\\nsets of features. We also ﬁnd that the improvement acquired\\nby decision-level fusion over individual features sets is not\\nas signiﬁcant as that achieved by feature-level fusion.\\n4.3.5 The Effect of Block Size on HOG-TOP\\nWe further explore the representation ability of HOG-TOP\\nwith different block sizes, from 12\\x0212to32\\x0232. Table\\n10 shows the parameters used for extracting HOG-TOP . The\\nblocks with a small size (12 and 16) are not overlapped and\\nlarge blocks (24\\x0224and32\\x0232) are half overlapped. We\\nemploy HOG-TOP on the CK+ database and the AFEW 4.0\\ndatabase. Experiment results are shown in Tables 11 and 12.\\nWe can see that the HOG-TOP with various block sizes\\nachieve the similar overall accuracy. We can therefore con-\\nclude that HOG-TOP is robust to scales. We can further\\nexamine the experimental results. For facial expressions\\nunder lab-controlled environment (Table 11), HOG-TOP\\nwith a small size (12) is more effective to recognize the\\nfacial expressions ”fear” and ”contempt” which have subtle\\nfacial muscle activities. A small block size is more robust to\\ncapture local subtle appearance changes than a large block\\nsize. For facial expressions in the wild (Table 12), HOG-TOP\\nwith a large size (24 and 32) achieves a superior performance\\nfor facial expression ”surprise”, indicating that HOG-TOP\\nwith a large block size is more robust to distinguish this\\nexpression from others in the wild. Table 12 also shows that\\nHOG-TOP with various block sizes outperforms the LBP-\\nTOP (30.6%) for facial expression recognition in the wild.\\n4.4 Discussion\\nFrom the experimental results reported above, we can see\\nthat our proposed framework can efﬁciently handle the\\nproblem of facial expression recognition in video. Facial\\nexpressions under lab-controlled environment are different\\nfrom those in the wild which are more natural and spon-\\ntaneous. We propose two approaches to tackle the two\\ndifferent facial expression recognition problems. The twoTABLE 11: The performance of HOG-TOP with various\\nblock sizes on the CK+ database (%).\\n12\\x0212 24 \\x0224 16 \\x0216 32 \\x0232\\nAnger 84.4 80.0 88.9 84.4\\nContempt 72.2 66.7 66.7 66.7\\nDisgust 93.2 93.2 94.9 96.6\\nFear 88.0 80.0 76.0 72.0\\nHappiness 95.7 95.7 95.7 97.1\\nSadness 64.3 64.3 67.9 60.7\\nSurprise 96.4 96.4 97.6 97.6\\nOverall 89.3 87.8 89.6 88.7\\nTABLE 12: The performance of HOG-TOP with various\\nblock sizes on the validation set of AFEW 4.0 database (%).\\n12\\x0212 24 \\x0224 16 \\x0216 32 \\x0232\\nNeutral 54.0 38.1 58.7 46.0\\nAnger 71.9 71.9 73.4 73.4\\nDisgust 20.0 15.0 22.5 20.0\\nFear 6.50 15.2 4.30 13.0\\nHappiness 57.1 63.5 60.3 60.3\\nSadness 4.90 9.80 4.90 9.80\\nSurprise 2.20 13.0 2.20 8.70\\nOverall 34.2 35.2 35.8 36.0\\napproaches both apply HOG-TOP , indicating that facial ap-\\npearance plays an important role for both facial expression\\nrecognition problems. Compared with LBP-TOP , HOG-TOP\\nis more compact and effective to characterize facial appear-\\nance changes. Facial conﬁguration changes also provide use-\\nful clues for facial expression analysis. The facial landmarks\\ncan be located exactly on a face image under lab-controlled\\nenvironment, representing the facial conﬁguration changes\\ncaused by facial muscle movements. We propose a new\\neffective geometric feature based on warp transform of\\nfacial landmarks and the proposed geometric warp feature\\nis robust to capture facial conﬁguration changes. On the\\nother hand, it is very challenging to locate facial landmarks\\non face images in the wild. However, the speech also plays\\nan important role on affect recognition. Instead of using\\ngeometric feature, acoustic feature is employed for facial\\nexpression recognition in the wild. Experimental results\\nshow that different features can make different contribution-\\ns to facial expression recognition and the multiple feature\\nfusion can enhance the discriminative ability of the multiple\\nfeatures. We also note that for facial expression recognition\\nin the wild, although our method outperforms the baseline\\nmethod, the performance is in general not as good as that\\nin facial expression recognition under lab-controlled envi-\\nronment. Facial expression recognition in the wild is much\\nmore challenging and it will be one of our future research\\nfocuses.\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n12\\n5 C ONCLUSION\\nVideo based facial expression recognition is a challenging\\nand long standing problem. In this paper, we exploit the\\npotentials of audiovisual modalities and propose an effec-\\ntive framework with multiple feature fusion to handle this\\nproblem. Both the visual modalities (face images) and audio\\nmodalities (speech) are utilized in our study. A new feature\\ndescriptor called Histogram of Oriented Gradients from\\nThree Orthogonal Planes (HOG-TOP) is proposed to extract\\ndynamic textures from video sequences to characterize fa-\\ncial appearance changes. Experiments conducted on three\\npublic databases (CK+, GEMEP-FERA 2011, AFEW4.0) have\\nshown that HOG-TOP performs as well as a widely used\\nfeature LBP-TOP in representing dynamic textures from\\nvideo sequences. Moreover, HOG-TOP is more effective\\nto capture subtle facial appearance changes and robust in\\ndealing with facial expression recognition in the wild. In\\naddition, HOG-TOP is more compact. In order to capture\\nfacial conﬁgure changes, we introduce an effective geomet-\\nric feature deriving from the warp transform of the facial\\nlandmarks. Realizing that voice is another powerful way\\nfor human beings to transmit message, we also explore\\nthe role of speech and employ the acoustic feature for\\naffect recognition in video. We applied the multiple feature\\nfusion to deal with facial expression recognition under lab-\\ncontrolled environment and in the wild. Experiments con-\\nducted on two facial expression datasets, CK+ and AFEW\\n4.0, demonstrate that our approach can achieve a promising\\nperformance in facial expression recognition in video.\\nACKNOWLEDGMENTS\\nThe work reported in this paper was partially supported\\nby a research grant from the Natural Science Foundation of\\nChina (NSFC) grant (Project Code: 61473243).\\nREFERENCES\\n[1] R. A. Calvo and S. D’Mello, ”Affect Detection An Interdisci-\\nplinary Review of Models, Methods, and Their Applications,”\\nIEEE Transactions on Affective Computing, vol. 1, pp. 18-37, 2010.\\n[2] Y. l. Tian, T. Kanade, and J. F. Cohn, ”Recognizing action units for\\nfacial expression analysis,” IEEE Transactions on Pattern Analysis\\nand Machine Intelligence, vol. 23, pp. 97-115, 2001.\\n[3] K. Scherer and P . Ekman, ”Handbook of Methods in Nonverbal\\nBehavior Research,” UK: Cambridge Univ. Press, 1982.\\n[4] J. F. Cohn and P . Ekman, ”Measuring facial action,” 2005.\\n[5] P . Ekman and W. V . Friesen, ”Facial Action Coding System: A\\nTechnique for the Measurement of Facial Movement,” Consulting\\nPsychologists Press, 1978.\\n[6] P . Ekman, W. V . Friesen, and J. C. Hager, ”Facial Action Coding\\nSystem: The Manual on CD ROM. A Human Face,” 2002.\\n[7] P . Ekman, ”An argument for basic emotions,” Cognition & Emo-\\ntion, vol. 6, pp. 169-200, 1992.\\n[8] S. Z. Li and A. K. Jain, ”Handbook of face recognition,” springer,\\n2011.\\n[9] N. Dalal and B. Triggs, ”Histograms of Oriented Gradients for\\nHuman Detection,” IEEE Conference on Computer Vision and Pat-\\ntern Recognition, 2005, pp. 886-893.\\n[10] G. Zhao and M. Pietikainen, ”Dynamic texture recognition using\\nlocal binary patterns with an application to facial expressions,”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol.\\n29, pp. 915-928, 2007.\\n[11] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews, ”The Extended Cohn-Kanade Dataset (CK+) A\\ncomplete dataset for action unit and emotion-speciﬁed expres-\\nsion,” IEEE Conference on Computer Vision and Pattern Recognition\\nWorkshops (CVPRW), 2010, pp. 94-101.[12] S. W. Chew, P . Lucey, S. Lucey, J. Saragih, J. F. Cohn, and\\nS. Sridharan, ”Person-independent facial expression detection\\nusing constrained local models,” IEEE International Conference on\\nAutomatic Face & Gesture Recognition and Workshops, 2011, pp. 915-\\n920.\\n[13] S. Taheri, P . Turaga, and R. Chellappa, ”Towards view-invariant\\nexpression analysis using analytic shape manifolds,” IEEE Inter-\\nnational Conference on Automatic Face & Gesture Recognition and\\nWorkshops, 2011, pp. 306-313.\\n[14] A. Saeed, A. Al Hamadi, R. Niese, and M. Elzobi, ”Effective\\ngeometric features for human emotion recognition,” IEEE 11th\\nInternational Conference on Signal Processing (ICSP), 2012, pp. 623-\\n627.\\n[15] K. Sikka, T. Wu, J. Susskind, and M. Bartlett, ”Exploring bag of\\nwords architectures in the facial expression domain,” in Computer\\nVision-ECCV Workshops and Demonstrations, 2012, pp. 250-259.\\n[16] Y. Rahulamathavan, R. C. W. Phan, J. A. Chambers, and\\nD. J. Parish, ”Facial Expression Recognition in the Encrypted\\nDomain Based on Local Fisher Discriminant Analysis,” IEEE\\nTransactions on Affective Computing, vol. 4, pp. 83-92, 2013.\\n[17] L. Zhang and D. Tjondronegoro, ”Facial expression recognition\\nusing facial movement features,” IEEE Transactions on Affective\\nComputing, vol. 2, pp. 219-229, 2011.\\n[18] S. Happy and A. Routray, ”Automatic facial expression recogni-\\ntion using features of salient facial patches,” IEEE Transactions on\\nAffective Computing, vol. 6, pp. 1-12, 2015.\\n[19] M. F. Valstar, B. Jiang, M. Mehu, M. Pantic, and K. Scherer, ”The\\nﬁrst facial expression recognition and analysis challenge,” IEEE\\nInternational Conference on Automatic Face & Gesture Recognition\\nand Workshops, 2011, pp. 921-926.\\n[20] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, ”Emotion\\nrecognition using PHOG and LPQ features,” IEEE International\\nConference on Automatic Face & Gesture Recognition and Workshops,\\n2011, pp. 878-883.\\n[21] X. Huang, G. Zhao, M. Pietikainen, and W. Zheng, ”Expression\\nRecognition in Videos Using a Weighted Component-Based Fea-\\nture Descriptor,” in Proceedings of the 17th Scandinavian conference\\non Image analysis, 2011, pp. 569-578.\\n[22] T. R. Almaev and M. F. Valstar, ”Local Gabor Binary Patterns\\nfrom Three Orthogonal Planes for Automatic Facial Expression\\nRecognition,” in Affective Computing and Intelligent Interaction\\n(ACII), 2013, pp. 356-361.\\n[23] X. Huang, Q. He, X. Hong, G. Zhao, and M. Pietikainen, ”Im-\\nproved Spatiotemporal Local Monogenic Binary Pattern for Emo-\\ntion Recognition in The Wild,” in ACM International Conference on\\nMultimodal Interaction, 2014, pp. 514-520.\\n[24] X. Huang, G. Zhao, W. Zheng, and M. Pietikainen, ”Spatio\\ntemporal Local Monogenic Binary Patterns for Facial Expression\\nRecognition,” IEEE Signal Processing Letters, vol. 19, pp. 243-246,\\n2012.\\n[25] F. Long, T. Wu, J. R. Movellan, M. S. Bartlett, and G. Littlewort,\\n”Learning spatiotemporal features by using independent compo-\\nnent analysis with application to facial expression recognition,”\\nNeurocomputing, vol. 93, pp. 126-132, 2012.\\n[26] S. W. Chew, R. Rana, P . Lucey, S. Lucey, and S. Sridharan, ”Sparse\\nTemporal Representations for Facial Expression Recognition,” in\\nAdvances in Image and Video Technology, 2012, pp. 311-322.\\n[27] Y. Li, S. Wang, Y. Zhao, and Q. Ji, ”Simultaneous Facial Feature\\nTracking and Facial Expression Recognition,” IEEE Transactions\\non Image Processing, vol. 22, pp. 2559-2573, 2013.\\n[28] J. Chen, Z. Chen, Z. Chi, and H. Fu, ”Emotion Recognition in\\nthe Wild with Feature Fusion and Multiple Kernel Learning,” in\\nACM International Conference on Multimodal Interaction, 2014, pp.\\n508-513.\\n[29] S. E. Kanou, C. Pal, X. Bouthillier, P . Froumenty, ?. Gl?ehre and\\nR. Memisevic, et al., ”Combining modality speciﬁc deep neural\\nnetworks for emotion recognition in video,” in Proceedings of\\nthe 15th ACM on International conference on multimodal interaction,\\n2013, pp. 543-550.\\n[30] M. Liu, R. Wang, S. Li, S. Shan, Z. Huang, and X. Chen, ”Com-\\nbining Multiple Kernel Methods on Riemannian Manifold for E-\\nmotion Recognition in the Wild,” in ACM International Conference\\non Multimodal Interaction, 2014, pp. 494-501.\\n[31] Y. Kim, H. Lee, and E. M. Provost, ”Deep learning for robust\\nfeature generation in audiovisual emotion recognition,” in IEEE\\nInternational Conference on Acoustics, Speech and Signal Processing\\n(ICASSP), 2013, pp. 3687-3691.\\n1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2593719, IEEE\\nTransactions on Affective Computing\\n13\\n[32] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, ”A Survey\\nof Affect Recognition Methods Audio, Visual, and Spontaneous\\nExpressions,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 31, pp. 39-58, 2009.\\n[33] F. Ringeval, S. Amiriparian, F. Eyben, K. Scherer, and B. Schuller,\\n”Emotion Recognition in the Wild Incorporating Voice and Lip\\nActivity in Multimodal Decision-Level Fusion,” in ACM Interna-\\ntional Conference on Multimodal Interaction, 2014, pp. 473-480.\\n[34] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon, ”Emotion\\nRecognition In The Wild Challenge 2014: Baseline, Data and Pro-\\ntocol,” in ACM International Conference on Multimodal Interaction,\\n2014, pp. 461-466.\\n[35] B. Sun, L. Li, T. Zuo, Y. Chen, G. Zhou, and X. Wu, ”Combining\\nMultimodal Features with Hierarchical Classiﬁer Fusion for E-\\nmotion Recognition in the Wild,” in ACM International Conference\\non Multimodal Interaction, 2014, pp. 481-486.\\n[36] K. Sikka, K. Dykstra, S. Sathyanarayana, G. Littlewort, and\\nM. Bartlett, ”Multiple kernel learning for emotion recognition in\\nthe wild,” in Proceedings of the 15th ACM on International conference\\non multimodal interaction, 2013, pp. 517-524.\\n[37] A. Martinez and S. Du, ”A model of the perception of facial\\nexpressions of emotion by humans Research overview and\\nperspectives,” The Journal of Machine Learning Research, vol. 13,\\npp. 1589-1608, 2012.\\n[38] J. Chen, Z. Chen, Z. Chi, and H. Fu, ”Dynamic texture and\\ngeometry features for facial expression recognition in video,” in\\nIEEE International Conference on Image Processing (ICIP), 2015, pp.\\n4967-4971.\\n[39] T. Gritti, C. Shan, V . Jeanne, and R. Braspenning, ”Local features\\nbased facial expression recognition with face registration errors,”\\nin8th IEEE International Conference on Automatic Face & Gesture\\nRecognition, 2008, pp. 1-8.\\n[40] T. Ojala, M. Pietikainen, and T. Maenpaa, ”Multiresolution gray-\\nscale and rotation invariant texture classiﬁcation with local bi-\\nnary patterns,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 24, pp. 971-987, 2002.\\n[41] I. Matthews and S. Baker, ”Active appearance models revisited,”\\nInternational Journal of Computer Vision, vol. 60, pp. 135-164, 2004.\\n[42] S. Meudt and F. Schwenker, ”Enhanced Autocorrelation in Real\\nWorld Emotion Recognition,” in ACM International Conference on\\nMultimodal Interaction, 2014, pp. 502-507.\\n[43] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers and\\nC. A. M \\x7fuller, et al., ”The INTERSPEECH 2010 paralinguistic\\nchallenge,” in INTERSPEECH, 2010, pp. 2794-2797.\\n[44] M. G \\x7fonen and E. Alpaydin, ”Multiple Kernel Learning Algo-\\nrithms,” The Journal of Machine Learning Research, vol. 12, pp. 2211-\\n2268, 2011.\\n[45] G. R. Lanckriet, N. Cristianini, P . Bartlett, L. E. Ghaoui, and\\nM. I. Jordan, ”Learning the Kernel Matrix with Semi-Deﬁnite\\nProgramming,” The Journal of Machine Learning Research, vol. 5,\\npp. 27-72, 2004.\\n[46] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet,\\n”SimpleMKL,” Journal of Machine Learning Research, vol. 9, pp.\\n2491-2521, 2008.\\n[47] C.-C. Chang and C.-J. Lin, ”LIBSVM: a library for support vector\\nmachines,” ACM Transactions on Intelligent Systems and Technology\\n(TIST) ,vol. 2, 2011.\\n[48] Abhinav Dhall, Roland Goecke, Simon Lucey, and T. Gedeon, ”A\\nsemi-automatic method for collecting richly labelled large facial\\nexpression databases from movies,” IEEE Multimedia, 2012.\\n[49] X. Zhu and D. Ramanan, ”Face detection, pose estimation and\\nlandmark localization in the wild,” in IEEE Conference on Com-\\nputer Vision and Pattern Recognition , 2012, pp. 2879-2886.\\n[50] F. Eyben, M. Wollmer, and B. Schuller, ”OpenEAR - Introducing\\nthe munich open-source emotion and affect recognition toolkit,”\\nin3rd International Conference onAffective Computing and Intelligent\\nInteraction and Workshops, 2009, pp. 1-6.\\n[51] Florian Eyben, Martin W \\x7follmer, and B. Schuller, ”Opensmile: the\\nmunich versatile and fast open-source audio feature extractor,” in\\nProceedings of the 18th ACM international conference on Multimedia,\\n2010, pp. 1459-1462.\\n[52] H. Kaya and A. A. Salah, ”Combining Modality-Speciﬁc Extreme\\nLearning Machines for Emotion Recognition in the Wild,” in\\nACM International Conference on Multimodal Interaction, 2014, pp.\\n487-493.Junkai Chen received his Bachelor and Master degrees from North-\\nwestern Polytechnic University and Xi’an Jiaotong University in 2010\\nand 2013, respectively. He is currently a Ph.D. candidate with Depart-\\nment of Electronic and Information Engineering, The Hong Kong Poly-\\ntechnic University. His research interests include pattern recognition,\\ncomputer vision and machine learning.\\nZenghai Chen received his B.Eng. in Department of Electronics and\\nCommunication Engineering from Sun Y at-set University, Guangzhou in\\n2009, and Ph.D. degree in Department of Electronic and Information\\nEngineering from The Hong Kong Polytechnic University, Hong Kong\\nin 2014. He is now a postdoctoral fellow in School of Electrical and\\nElectronic Engineering, Nanyang Technological University, Singapore.\\nHis research interests include pattern recognition, computer vision and\\nmachine learning.\\nZheru Chi received his B.Eng. and M.Eng. degrees from Zhejiang\\nUniversity in 1982 and 1985, respectively, and his Ph.D. degree from\\nthe University of Sydney in March 1994, all in electrical engineering.\\nBetween 1985 and 1989, he was on the Faculty of the Department\\nof Scientiﬁc Instruments at Zhejiang University. He worked as a Se-\\nnior Research Assistant/Research Fellow in the Laboratory for Imaging\\nScience and Engineering at the University of Sydney from April 1993\\nto January 1995. Since February 1995, he has been with the Hong\\nKong Polytechnic University, where he is now an Associate Professor\\nin the Department of Electronic and Information Engineering. Since\\n1997, he has served on the organization or program committees for\\na number of international conferences. He was an associate editor for\\nIEEE Transactions on Fuzzy Systems between 2008 and 2010, and is\\ncurrently an editor for International Journal of Information Acquisition.\\nHis research interests include image processing, pattern recognition,\\nand computational intelligence. Dr Chi has authored/co-authored one\\nbook and 11 book chapters, and published more than 190 technical\\npapers.\\nHong Fu received her Bachelor and Master degrees from Xian Jiao tong\\nUniversity in 2000 and 2003, and Ph.D. degree from the Hong Kong\\nPolytechnic University in 2007. She is now an Associate Professor in\\nDepartment of Computer Science, Chu Hai College of Higher Educa-\\ntion, Hong Kong. Her research interests include eye tracking, computer\\nvision, pattern recognition, and artiﬁcial intelligence.\\n\",\n",
       " 'Facial Expression Recognition Using Deep\\nConvolutional Neural Networks\\nDinh Viet Sang\\nHanoi University of Science\\nand Technology\\nEmail: sangdv@soict.hust.edu.vnNguyen Van Dat\\nHanoi University of Science\\nand Technology\\nEmail: datnguyenvan844@gmail.comDo Phan Thuan\\nHanoi University of Science\\nand Technology\\nEmail: thuandp@soict.hust.edu.vn\\nAbstract —Facial expressions convey non-verbal information\\nbetween humans in face-to-face interactions. Automatic facial\\nexpression recognition, which plays a vital role in human-machine\\ninterfaces, has attracted increasing attention from researchers\\nsince the early nineties. Classical machine learning approaches\\noften require a complex feature extraction process and produce\\npoor results. In this paper, we apply recent advances in deep\\nlearning to propose effective deep Convolutional Neural Networks\\n(CNNs) that can accurately interpret semantic information avail-\\nable in faces in an automated manner without hand-designing\\nof features descriptors. We also apply different loss functions\\nand training tricks in order to learn CNNs with a strong\\nclassiﬁcation power. The experimental results show that our\\nproposed networks outperform state-of-the-art methods on the\\nwell-known FERC-2013 dataset provided on the Kaggle facial\\nexpression recognition competition. In comparison to the winning\\nmodel of this competition, the number of parameters in our\\nproposed networks intensively decreases, that accelerates the\\noverall performance speed and makes the proposed networks\\nwell suitable for real-time systems.\\nI. I NTRODUCTION\\nEver since computers were invented, people have wanted\\nto build artiﬁcially intelligent (AI) systems that are mentally\\nand/or physically equivalent to humans. In the past decades,\\nthe increase of generally available computational power pro-\\nvided a helping hand for developing fast learning machines,\\nwhereas the Internet supplied an enormous amount of data\\nfor training. Among a lot of advanced machine learning\\ntechniques that have been developed so far, deep learning is\\nwidely considered as one of the most promising techniques to\\nmake AI machines approaching human-level intelligence.\\nFacial expression recognition is the process of identifying\\nhuman emotion based on facial expressions. Humans are natu-\\nrally capable of recognizing emotions. In fact, children, which\\nare only 36 hours old, can interpret some very basic emotions\\nfrom faces. In older humans, this ability is considered one\\nof the most important social skills. There is a universality in\\nfacial expressions of humans in expressing certain emotions.\\nHuman develop similar muscular movements belonging to\\na certain mental state, despite their place of birth, race,\\neducation, etcetera. Therefore, if properly being modelled, this\\nuniversality can be a convenient feature in human-machine\\ninteraction: a well trained system can understand emotions,\\nindependent of who the subject is.Automated facial expression recognition has numerous prac-\\ntical applications such as psychological analysis, medical\\ndiagnosis, forensics (lie-detection), studying effectiveness of\\nadvertisement and so on. The ability to read facial expressions\\nand then recognize human emotions provides a new dimension\\nto human-machine interactions, for instance, smile detector\\nin commercial digital cameras or interactive advertisements.\\nRobots can also beneﬁt from automated facial expression\\nrecognition. If robots can predict human emotions, they can\\nreact upon this and have appropriate behaviors.\\nIn this paper, we adopt deep learning technique and propose\\neffective architectures of Convolutional Neural Networks to\\nsolve the problem of facial expression recognition. We also\\napply different loss functions associated with supervised learn-\\ning and several training tricks in order to learn CNNs with a\\nstrong discriminative power. We show that Multiclass SVM\\nloss works better than cross-entropy loss (combining with\\nsoftmax function) in facial expression recognition. Besides, the\\nevaluation of the test sets using multiple crops with different\\nscales and rotations can yield an accuracy boost compared to\\nsingle crop evaluation.\\nFacial expression recognition competition FERC-2013 [1]\\nwas held in 2013 by Kaggle. The winner is RBM team [14]\\nwith the accuracy of 69.4% on public test set and 71.2% on\\nprivate test set. To the best of our knowledge, this is the state-\\nof-the-art on the FERC-2013 dataset so far. The experiments\\nshow that our proposed method achieves better accuracy than\\ntheir results on both two test sets.\\nThe rest of the paper is organized as follows. In section II\\nwe brieﬂy summarize some related work on facial expression\\nrecognition. In section III we describe our proposed CNN\\narchitectures. Our experiments and evaluation are shown in\\nsection IV . The conclusion is in section V with some discus-\\nsion for the future work.\\nII. R ELATED WORK\\nClassical approaches for facial expression recognition are\\noften based on Facial Action Coding System (FACS) [3],\\nwhich involves identifying various facial muscles causing\\nchanges in facial appearance. It includes a list of Action\\nUnits (AUs). Cootes et al. [15] propose a model based on an\\napproach called the Active Appearance Model [2]. Given input2017 9th International Conference on Knowledge and Systems Engineering(KSE)\\n978-1-5386-3576-6/17/$31.00 c\\r2017 IEEE 130\\nimage, preprocessing steps are performed to create over 500\\nfacial landmarks. From these landmarks, the authors perform\\nPCA algorithm [12] and derive Action Units (AUs). Finally,\\nthey classify facial expressions using a single layered neural\\nnetwork.\\nWith the fast growth of deep learning, the state-of-the-art in\\nmany computer vision tasks has been considerably improved.\\nIn image classiﬁcation, there are some well-known deep CNNs\\ncan be mentioned as follows. The ﬁrst network we want to\\nmention is AlexNet [8], the winner of ImageNet ILSVRC\\nchallenge in 2012. This network has a very similar architecture\\nto LeNet [10], but is deeper and bigger, and its convolutional\\nlayers stack on top of each other. Previously it is common\\nto have only a single convolutional layer followed by a pool\\nlayer. The next CNNs are called VGGNet [11], the runner-\\nup in ILSVRC 2014. One important property of VGGNet is\\nthat there are many convolutional layers with small ﬁlter size\\n3×3that stack on top of each other instead of using a single\\nconvolutional layer with larger ﬁlter size as in previous CNN\\ngenerations. The winner of ILSVRC 2015 is ResNet [5] which\\ncan be characterized by skip connections and heavy use of\\nbatch normalization [6]. Recent improved variants of ResNet\\ncalled Wide ResNet [17] or ResNeXt [16] also demonstrate\\ntheir impressive power in image classiﬁcation tasks.\\nDeep learning allows us to extract facial features in an\\nautomated manner without requiring manual design of feature\\ndescriptors. There have been some studies that employ CNNs\\nto address the problem of facial expression recognition. Gudi\\net al. [4] propose a CNN model to recognize gender, race,\\nage and emotion from facial images. The network includes 3\\nconvolutional layers (with large ﬁlter size), 2 fully connected\\nlayers, where the ﬁrst has 3072 neurons and the second is\\nthe output layer with 7 neurons. There is only one maxpool\\nlayer after the ﬁrst convolutional layer. The softmax activation\\nfunction is applied to output layer and cross-entropy loss\\nfunction is used in training process. In the task of emotion\\nrecognition from faces, Tang and his RBM team [14] set\\nthe state-of-the-art on the Kaggle FERC-2013 dataset. Their\\nnetwork is pretty similar with Gudi’s architecture [4]. The\\ndifference is that Tang et al. use multi-class SVM loss instead\\nof cross-entropy loss, and exploit augmentation techniques to\\ncreate more data for training.\\nIII. O UR PROPOSED NETWORK ARCHITECTURES\\nIn this section, we will propose several different CNN\\narchitectures for facial expression recognition problem. In\\nall architectures, the input image size is ﬁxed to 42x42x1.\\nArchitectures are composed of convolution layers, pooling\\nlayers, and fully connected layers. After each convolutional\\nlayer and fullly connected layer (except the output layer),\\nthe ReLU [8] activation function is applied. The output layer\\nconsists of 7 neurons corresponding to 7 emotional labels:\\nangry, disgust, fear, happy, sad, surprise and neutral. It is\\npossible to optionally use a softmax layer right after the output\\nlayer if one wants to use the cross-entropy loss function in thetraining phase. However, if the multi-class SVM loss function\\nis used in the training phase, the softmax layer can be omitted.\\nA. The BKStart architecture\\nFirstly, we try to reconstruct the winning model of the\\nKaggle facial expression competition proposed by Tang et al.\\nin [14] to conduct some experiments with its modiﬁcations.\\nSince the details of the winning model was not fully described\\nin [14], so we try to make the reconstructed model, called\\nBKStart, as close to the origin as possible.\\nTABLE I: BKStart architecture\\nInput: 42x42x1\\nConv2d: 5x5x32, stride: 1 + ReLU\\nMax Pooling: 3x3, stride: 2\\nConv2d: 4x4x32, stride: 1 + ReLU\\nAverage Pooling: 3x3, stride: 2\\nConv2d: 5x5x64, stride: 1 + ReLU\\nAverage Pooling: 3x3, stride: 2\\nFC: 3072 + ReLU\\nFC: 7\\nThe BKStart architecture is described in the table I. This\\narchitecture consists of 3 convolutional layers with (size,\\nnumber of ﬁlters) is ( 5×5, 32), ( 4×4, 32), ( 5×5, 64)\\nrespectively and the stride is 1. The ﬁrst pooling layer is a max\\npooling layer with a ﬁlter size of 3×3and the stride is 2. The\\nnext two pooling layers are the average pooling layers with a\\nﬁlter size of 3×3and the stride is 2. The next one is a fully\\nconnected layer with 3072 neurons. Passing this layer, from\\neach input image we obtain a 3072-d vector, which is a high-\\nlevel feature descriptor representing the input image. Finally,\\nthis vector is passed to the last fully connected layer with 7\\nneurons, which, in turn, outputs a 7-d vector s= (s1,s2,..,s 7)\\nrepresenting class scores towards 7 kinds of emotions.\\nWhen the multi-class SVM loss is used in the training\\nphase, the class label corresponding to the highest score is\\nimmediately chosen as the ﬁnal answer. In other case, when\\nthe cross-entropy loss is used in the training phase, the softmax\\nactivation function is then applied to the score vector s, and\\nit in turn outputs a new 7-d vector u= (u1,u2,...,u 7), that\\ncan be deﬁned as follows:\\nui=esi\\n/summationtext7\\nk=1esk,i= 1,2,..., 7. (1)\\nThus, we ﬁnally obtain the vector of 7 elements that\\nrepresents the probability distribution over 7 emotion classes.\\nThe class label corresponding to the highest probability will\\nbe then chosen as the ﬁnal answer.\\nB. Our proposed VGG-like CNNs\\nAs mentioned above, VGG is the runner-up in ILSVRC\\n2014. VGG is characterized by its simplicity, using only small\\nconvolutional layers stacked on top of each other in increasing\\ndepth. Unlike conventional CNNs, VGG consists of blocks,\\neach of which contain one to four convolutional layers. The\\nconvolutional layers often have a very small ﬁlter size of 3×3,131\\nand the stride is 1. The convolutional layers in the same block\\nhave the same number of ﬁlters. And the number of ﬁlters in\\neach convolutional layer in this block is double or equal to the\\nnumber of ﬁlters in each convolutional layer of the previous\\nblock. Each following block is a max pooling layer which\\noften has a ﬁlter size of 2×2with the stride 2.\\nInspired by the simple designing idea of VGG, we propose\\ndifferent effective CNNs to tackle the problem of facial\\nexpression recognition. Since FERC-2013 dataset is much\\nsmaller than ImageNet dataset, we propose some changes to\\navoid overﬁtting phenomenon. In fact, we strongly reduce the\\nnumber of ﬁlters in all convolutional and fullly connected\\nlayers. Our four proposed architectures, called BKVGG8,\\nBKVGG10, BKVGG12 and BKVGG14, are shown in Table II,\\none per column. All of these architectures follow the general\\ndesigning principles of VGG. They differ from each other\\nonly in depth: from 8 layers in BKVGG8 (5 convolutional\\nand 3 fully connected layers) up to 14 layers in BKVGG14\\n(11 convolutional and 3 fully connected layers).\\nFor example, BKVGG12 is described in more details as\\nfollows. Overall, BKVGG12 consists of four blocks. The ﬁrst\\nblock has two 3×3convolutional layers, the number of\\nﬁlters is 32, the stride is 1. The second block has two 3×3\\nconvolutional layers, the number of ﬁlters is 64, the stride is 1.\\nThe third block has two 3×3convolutional layers, the number\\nof ﬁlters is 64, the stride is 1. The fourth block has three 3×3\\nconvolutional layers, the number of ﬁlters is 64, the stride is\\n1. After each block except the last one, there is a max pooling\\nlayer with ﬁlter size 2×2and the stride is 2. After the four\\nblocks, there are two fully connected layers with 256 neurons\\nper each layer. Finally, the output layer is a fully connected\\nlayer with 7 neurons associated with 7 emotion classes. As\\nmentioned previously, one can add a softmax layer after the\\noutput layer, and then use the cross-entropy loss function to\\ntrain the models. Nevertheless, if one use the multi-class SVM\\nloss during the training process, the softmax layer can be\\nskipped.\\nIn Table III we show the number of parameters for each ar-\\nchitecture. Despite of a large depth, the number of parameters\\nin our proposed architectures is much smaller than the one in\\nBKStart.\\nC. Implementation\\n1)Data preprocessing :The preprocessing step is quite\\nsimple: ﬁrstly normalizing data per image, and then normal-\\nizing data per pixel.\\n•Normalizing data per image: ﬁrstly, we subtract from each\\nimage the mean value over that image and then set the\\nstandard deviation of the image to 3.125.\\n•Normalizing data per pixel: ﬁrstly, we compute the mean\\nimage over the training set. Each pixel in the mean image\\nis computed from the average of all corresponding pixels\\n(i.e.with the same coordinates) across all training images.\\nFor each training image, we then subtract from each pixel\\nits mean value, and then set the standard deviation of each\\npixel over all training images to 1.TABLE II: Our proposed network architectures. The depth\\nincreases from the left (BKVGG8) to the right (BKVGG14),\\nwith more layers being added (the added layers are showed\\nin bold). Convolutional layer parameters are denoted by\\n“conv/angbracketleftﬁlter size/angbracketright-/angbracketleftﬁlter number/angbracketright” . ReLU activation function\\nis not showed for the sake of brevity.\\nConvNet Conﬁguration\\nBKVGG8 BKVGG10 BKVGG12 BKVGG14\\n8 layers 10 layers 12 layers 14 layers\\nInput ( 42×42×1)\\nconv3-32 conv3-32conv3-32\\nconv3-32conv3-32\\nconv3-32\\nmaxpool\\nconv3-64 conv3-64conv3-64\\nconv3-64conv3-64\\nconv3-64\\nmaxpool\\nconv3-128conv3-128\\nconv3-128conv3-128\\nconv3-128conv3-128\\nconv3-128\\nconv3-128\\nmaxpool\\nconv3-256\\nconv3-256conv3-256\\nconv3-256\\nconv3-256conv3-256\\nconv3-256\\nconv3-256conv3-256\\nconv3-256\\nconv3-256\\nconv3-256\\nFC-256\\nFC-256\\nFC-7\\nTABLE III: Number of parameters\\nArchitecture Number of parameters\\nBKStart (reconstructed 7.17 M\\nFERC-2013 winning model [14])\\nBKVGG8 3.40 M\\nBKVGG10 4.14 M\\nBKVGG12 4.19 M\\nBKVGG14 4.92 M\\nPreprocessing step is applied to both training data and test\\ndata.\\n2)Data augmentation :Due to the small amount of training\\ndata, we apply data augmentation techniques to increase the\\namount of training samples in order to avoid overﬁtting and\\nimprove recognition accuracy. For each image, we perform the\\nfollowing successive transforms:\\n•Mirror the image with a probability of 0.5.\\n•Rotate the image with a random angle from -45 to 45 (in\\ndegrees).\\n•Rescale the image, whose original size in the FERC-2013\\ndataset is 48×48, to a random size in the range from\\n42×42to54×54.\\n•Take a random crop of size 42×42from the rescaled\\nimage.\\nFig. 1 illustrates some examples of the data augmentation\\nresults. In the next section, we will show that the data aug-\\nmentation signiﬁcantly improves the accuracy of the models.\\n3)Training :In the training phase, we minimize the loss\\nfunction by using mini-batch gradient descent with momentum\\nand the back-propagation algorithm [9]) . The batch size is132\\nFig. 1: Examples of the data augmentation. The left is original\\nimages. The right is after preprocessing and augmentation.\\n256, the momentum is 0.9. To avoid overﬁtting, we apply the\\ndropout technique [13] to the fully connected layers (except\\nfor the output one) with a dropout probability of 0.5. During\\ntraining phase, we use a strategy that decrease the learning rate\\n10 times if the training loss stops improving. The experiments\\nshow that the learning rate is often decreased about 5 times,\\nand the training phase is often ﬁnished after about 1400\\nepochs.\\nAll biases are initialized by zero. All weights are randomly\\ninitialized from a Gaussian distribution with zero mean and\\nthe following standard deviation:\\nδ=/radicalbigg2\\nnInput, (2)\\nwherenInput is the number of weights of each neuron. For\\nconvolutional layer, nInput = ﬁlter size×ﬁlter size×the\\ndepth of the previous layer. For the fully connected layer,\\nnInput = the number of neurons in the previous layer.\\nFor each iteration, the next 256 images are taken from the\\ntraining data. After performing data augmentation, we will\\nhave a batch of 256 augmented images, each of which has\\nsize of 42×42. These images are then fed to the network\\nfor training. After each epoch, the training data is randomly\\nshufﬂed.\\nAs mentioned above, we try to use different loss functions:\\ncross-entropy and L2 multi-class SVM.\\nLet us assume that:\\n•Nis the number of images in the training data;\\n•Cis the number of emotion classes ( C= 7 in FERC-\\n2013 dataset);\\n•siis the vector of class scores corresponding to i-th\\nimage;\\n•liis the correct class label of i-th image;\\n•yiis the one-hot encoding of the correct class label of\\ni-th image (yi(li) = 1 );\\n•/hatwideyiis the probability distribution over the emotion classes\\nofi-th image that can be adopted by applying the softmax\\nfunction to si.The cross-entroy loss function is deﬁned as follows:\\nH=−1\\nNN/summationdisplay\\ni=1C/summationdisplay\\nj=1yi(j)log(/hatwideyi(j)), (3)\\nwhere:\\n•yi(j)∈{0,1}indicates whether jis the correct label of\\ni-th image;\\n•/hatwideyi(j)∈[0,1]expresses the probability that jis the\\ncorrect label of i-th image.\\nMeanwhile, the L2 multi-class SVM loss function can be\\ndeﬁned as follows:\\nL=1\\nNN/summationdisplay\\ni=1/summationdisplay\\nj/negationslash=limax(0,si(j)−si(li) + 1)2, (4)\\nwhere:\\n•si(j)indicates the score of class jin thei-th image;\\n•si(li)deﬁnes the score of true label liin thei-th image.\\nNote that in (3) and (4), for simplicity, we ignore regularized\\nterms such as L2 weight decay penalty.\\nIn practice, the cross-entropy loss (combining with softmax\\nactivation function) and L2 multi-class SVM loss are usually\\ncomparable. Different people have different opinions on which\\nis better.\\nFig. 2: Training accuracy (orange curve) and validation accu-\\nracy (blue curve)\\n4)Testing :We divide the original dataset into a training\\nset and a validation set. During the training phase, we monitor\\nthe validation accuracy and save the model state with the\\nhighest accuracy on the validation set. The trained model is\\nthen applied to the test sets to estimate the ﬁnal accuracy.\\nFig. 2 shows us the accuracy on the training set and validation\\nset during training process.\\nIn the test phase, having a trained network and an input\\nimage of size 48x48, we use multiple crops in two ways to\\npredict the true class label:\\n1) Method Eval1 : we take 10 crops of size 42x42 of the\\nimage (5 crops and their mirrors).\\n2) Method Eval2 : We ﬁrst rescale the image to various sizes\\n48×48,50×50,52×52, and 54×54. For each size,\\nwe rotate the image with different angles -45, -33.75,\\n-22.5, -11.25, 0, 11.25, 22.5, 33.75, 45 (in degrees). For\\neach angle, take 18 crops of size 42×42of the image\\n(9 crops and their mirrors).133\\nIn both methods Eval1 andEval2 , for each crop, we com-\\npute scores over the classes (in case of L2 multi-class SVM\\nloss) or estimate the class probability distribution (in case of\\ncross-entropy loss). The ﬁnal resulting vector is computed\\nby averaging over the results of all crops. The class label\\nassociated with the highest value in the ﬁnal vector is selected\\nas the predicted label.\\nIn the section IV , we show that the second method Eval2\\ngives better results than the ﬁrst one Eval1 .\\nIV. E XPERIMENT\\nA. Datasets\\nWe conduct experiments on the FERC-2013 dataset, which\\nis provided on the Kaggle facial expression competition [1].\\nThe dataset consists of 35,887 gray images of 48x48 resolu-\\ntion. Kaggle has divided into 28,709 training images, 3589\\npublic test images and 3589 private test images. Each image\\ncontains a human face that is not posed (in the wild). Each\\nimage is labeled by one of seven emotions: angry, disgust,\\nfear, happy, sad, surprise and neutral. Some images of the\\nFERC-2013 dataset are showed in Fig. 3.\\nFig. 3: FERC-2013 dataset\\nB. Experiment setup\\nOur experiments have been conducted using Python\\nprograming-language on the computer with the following\\nspeciﬁcations: Intel Xeon E5-2650 v2 Eight-Core Processor\\n2.6GHz 8.0GT/s 20MB, Ubuntu Operating System 14.04 64\\nbit, 32GB RAM.\\nC. Result and Evaluation\\n1)The affection of data augmentation :Table IV shows\\nthe accuracy of BKStart model with softmax activation func-\\ntion and cross-entropy loss function on the test sets in two\\ncases: with and without data augmentation. One can see\\nthat data augmentation signiﬁcantly improves the model’s\\naccuracy. In the public test set the accuracy increases from\\n61.0% to 68.3%. In the private test set the accuracy increases\\nfrom 60.6% to 69.5%. We use data augmentation by default\\nin all next experiments.TABLE IV: The accuracy of BKStart with/without data aug-\\nmentation\\nArchitectureData\\naugmen-\\ntationTest\\nmethodPublic\\ntest\\naccuracy\\n(%)Private\\ntest\\naccuracy\\n(%)\\nBKStart + softmax +\\ncross-entropyNo Eval1 61.0 60.6\\nYes Eval1 68.3 69.5\\nTABLE V: The accuracy when applying Eval1 test method\\nandEval2 test method.\\nArchitectureData\\naugmen-\\ntationTest\\nmethodpublic\\ntest\\naccuracy\\n(%)private\\ntest\\naccuracy\\n(%)\\nBKStart + softmax +\\ncross-entropyYesEval1 68.3 69.5\\nEval2 69.2 70.4\\nBKVGG8 + softmax\\n+ cross-entropyYesEval1 66.4 68.2\\nEval2 67.6 69.1\\n2)The affection of test methods :In section III, we\\nproposed two test methods called Eval1 andEval2 . Table V\\nshows us the accuracy of some proposed architectures on the\\ntest sets when applying these test methods. We can see that\\non both architectures BKStart and BKVGG8, method Eval2\\ngives better result. Thereby, we use the test method Eval2 in\\nthe remaining experiments.\\n3)The affection of loss function :Table VI shows the\\naccuracy on test sets when applying two types of loss function:\\ncross-entropy and L2 multi-class SVM. One can see that on\\nboth architectures BKStart and BKVGG8, the L2 multi-class\\nSVM loss achieves better result. So we believe that the L2\\nmulti-class SVM loss works better in case of facial expression\\nrecognition.\\n4)Comparison between different models :As mentioned\\nabove, in the next experiments, all proposed architectures are\\nset to use data augmentation, test method Eval2 and L2 multi-\\nclass SVM loss. Table VII shows the accuracy of all proposed\\narchitectures on FERC-2013 dataset. One can see that the\\naccuracy gradually increases from BKVGG8 to BKVGG12,\\nbut decreases with BKVGG14. The reason could be that from\\nTABLE VI: The accuracy when applying cross-entropy loss\\nand L2 multi-class SVM loss\\nArchitectureData\\naugmen-\\ntationtest\\nmethodpublic\\ntest\\naccuracy\\n(%)private\\ntest\\naccuracy\\n(%)\\nBKStart + softmax +\\ncross-entropyYes Eval2 69.2 70.4\\nBKStart + L2 SVM Yes Eval2 69.7 70.9\\nBKVGG8 + softmax\\n+ cross-entropyYes Eval2 67.6 69.1\\nBKVGG8 + L2 SVM Yes Eval2 68.7 70.2134\\nTABLE VII: Accuracy of proposed network architectures on\\nthe FERC-2013 dataset\\nArchitecture Public test accuracy (%) Private test accuracy (%)\\nBKStart 69.7 70.9\\nBKVGG8 68.7 70.2\\nBKVGG10 69.5 70.4\\nBKVGG12 71.0 71.9\\nBKVGG14 70.6 71.4\\nTABLE VIII: Compare our results with top 4 teams on Kaggle\\ncompetition [1]\\nArchitecturePublic test\\naccuracy (%)Private test\\naccuracy (%)\\nSIFT+MKL [7] ( Radu +\\nMarius + Cristi )67.3 67.5\\nCNN ( Maxim Milakov ) 68.2 68.8\\nCNN ( team Unsupervised ) 69.1 69.3\\nCNN+SVM Loss [14]\\n(team RBM )69.4 71.2\\nBKStart 69.7 70.9\\nBKVGG14 70.6 71.4\\nBKVGG12 71.0 71.9\\nBKVGG8 to BKVGG12 the model becomes more and more\\ncomplex thanks to increasing depth and can ﬁt better the\\ndataset. Nevertheless, BKVGG14 is too deep and becomes\\nover-complex towards the dataset. It causes the overﬁtting\\nphenomenon, that results in the drop of recognition accuracy.\\nTable VIII presents our results in comparison with the top\\nfour teams on the Kaggle competition [1]. The results of the\\nRBM winning team are 69.4% on the public test set and 71.2%\\non the private test set, which set the state-of-the-art on the\\nFERC-2013 dataset so far. The experiments show that our\\nproposed networks BKVGG14 and BKVGG12 outperforms\\nRBM team on both public test set and private test set, while\\nmaintaining considerably higher performance speed thanks to\\nmuch smaller number of parameters (Table VII).\\nTable IX presents the confusion matrix of recognition result\\nachieved by BKVGG12 architecture. High accuracies are\\nobtained with happy (89.76%), surprised (82.45%), and neutral\\n(73.16%) classes. In fact, they are the most distinguishable\\nemotions for human. The angry, fearful, sad classes are more\\noften confused together, since they share many similar expres-\\nsions. Finally, the disgusted class get a pretty good accuracy\\n69.09%, despite the low amount of disgusted samples in the\\ntraining set.\\nTABLE IX: Confusion matrix of BKVGG12 architecture\\nTrue\\nLabelPredicted\\nLabel Angry Disgusted Fearful Happy Sad Surprised Neutral\\nAngry 63.14 0.41 9.98 2.24 15.27 0.81 8.15\\nDisgusted 18.18 69.09 0.00 3.64 3.64 3.64 1.82\\nFearful 11.36 0.00 51.89 2.27 20.08 6.82 7.58\\nHappy 1.48 0.00 1.59 89.76 2.96 1.37 2.84\\nSad 7.74 0.00 8.92 3.87 61.78 0.84 16.84\\nSurprised 1.44 0.24 6.01 4.81 2.16 82.45 2.88\\nNeutral 3.51 0.16 4.15 3.04 15.34 0.64 73.16V. C ONCLUSION\\nIn this paper, inspired by the designing principles of VGG,\\nwe propose effective architectures of CNNs to tackle the prob-\\nlem of facial expression recognition. The proposed networks\\nare composed of a stack of convolutional blocks. Each block\\nhas a few 3×3convolutional layers followed a max pooling\\nlayers. Despite having much smaller number of parameters,\\nour proposed networks outperform the winning model of the\\nKaggle competition. The results prove the power of small\\nﬁlter and very deep network in classiﬁcation tasks. We also\\nshow that L2 multi-class SVM loss is preferable than cross-\\nentropy loss in facial expression recognition. Additionally, data\\naugmentation is shown to be an important trick in training deep\\nneural networks.\\nVI. A CKNOWLEDGMENTS\\nThis research is funded by Vietnam National Foundation for\\nScience and Technology Development (NAFOSTED) under\\ngrant number 102.01-2017.12.\\nREFERENCES\\n[1] Challenges in respresentation learning: Facial expres-\\nsion recognition challenge. https://www.kaggle.com/c/\\nchallenges-in-representation-learning-facial-expression-recognition-challenge,\\n2013.\\n[2] T. F. Cootes, C. J. Taylor, et al. Statistical models of appearance for\\ncomputer vision, 2004.\\n[3] P. Ekman and E. L. Rosenberg. What the face reveals: Basic and applied\\nstudies of spontaneous expression using the Facial Action Coding System\\n(FACS) . Oxford University Press, USA, 1997.\\n[4] A. Gudi. Recognizing semantic features in faces using deep learning.\\narXiv preprint arXiv:1512.00743 , 2015.\\n[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , pages 770–778, 2016.\\n[6] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. arXiv preprint\\narXiv:1502.03167 , 2015.\\n[7] R. T. Ionescu, M. Popescu, and C. Grozea. Local learning to improve bag\\nof visual words model for facial expression recognition. In Workshop\\non challenges in representation learning, ICML , 2013.\\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. In Advances in neural\\ninformation processing systems , pages 1097–1105, 2012.\\n[9] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten\\nzip code recognition. Neural computation , 1(4):541–551, 1989.\\n[10] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning\\napplied to document recognition. Proceedings of the IEEE , 86(11):2278–\\n2324, 1998.\\n[11] K. Simonyan and A. Zisserman. Very deep convolutional networks for\\nlarge-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.\\n[12] L. Sirovich and M. Kirby. Low-dimensional procedure for the charac-\\nterization of human faces. Josa a , 4(3):519–524, 1987.\\n[13] N. Srivastava. Improving neural networks with dropout . PhD thesis,\\nUniversity of Toronto, 2013.\\n[14] Y . Tang. Deep learning using support vector machines. CoRR,\\nabs/1306.0239 , 2, 2013.\\n[15] H. Van Kuilenburg, M. Wiering, and M. Den Uyl. A model based method\\nfor automatic facial expression recognition. In European Conference on\\nMachine Learning , pages 194–205. Springer, 2005.\\n[16] S. Xie, R. Girshick, P. Doll ´ar, Z. Tu, and K. He. Aggregated\\nresidual transformations for deep neural networks. arXiv preprint\\narXiv:1611.05431 , 2016.\\n[17] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv\\npreprint arXiv:1605.07146 , 2016.135\\n',\n",
       " 'Facial Expression Recognition\\nUsing Facial Movement Features\\nLigang Zhang, Student Member ,IEEE , and Dian Tjondronegoro\\nAbstract —Facial expression is an important channel for human communication and can be applied in many real applications. One\\ncritical step for facial expression recognition (FER) is to accurately extract emotional features. Current approaches on FER in staticimages have not fully considered and utilized the features of facial element and muscle movements, which represent static and\\ndynamic, as well as geometric and appearance characteristics of facial expressions. This paper proposes an approach to solve this\\nlimitation using “salient” distance features, which are obtained by extracting patch-based 3D Gabor features, selecting the “salient”\\npatches, and performing patch matching operations. The experimental results demonstrate high correct recognition rate (CRR),\\nsignificant performance improvements due to the consideration of facial element and muscle movements, promising results under face\\nregistration errors, and fast processing time. Comparison with the state-of-the-art performance confirms that the proposed approach\\nachieves the highest CRR on the JAFFE database and is among the top performers on the Cohn-Kanade (CK) database.\\nIndex Terms —Facial expression analysis, feature evaluation and selection, computer vision, Gabor filter, Adaboost.\\nÇ\\n1I NTRODUCTION\\nFACIAL expression recognition (FER) has been dramati-\\ncally developing in recent years, thanks to advance-\\nments in related fields, especially machine learning, imageprocessing, and human cognition. Accordingly, the impact\\nand potential usage of automatic FER have been growing in\\na wide range of applications, including human-computerinteraction, robot control, and driver state surveillance.However, to date, robust recognition of facial expressionsfrom images and videos is still a challenging task due to thedifficulty in accurately extracting the useful emotionalfeatures. These features are often represented in differentforms, such as static, dynamic, point-based geometric, orregion-based appearance.\\nFacial movement features , which include feature position\\nand shape changes, are generally caused by the movementsof facial elements and mus cles during the course of\\nemotional expression. The facial elements, especially keyelements, will constantly change their positions whensubjects are expressing emotions. As a consequence, thesame feature in different images usually has differentpositions, as shown in Fig. 1a. In some cases, the shape ofthe feature may also be distorted due to the subtle facial\\nmuscle movements. For example, the mouth in the first two\\nimages in Fig. 1b presents different shapes from that in thethird image. Therefore, for any feature representing acertain emotion, the geometry-based position and appear-ance-based shape normally change from one image toanother image in image databases, as well as in videos. This\\nkind of movement features represents a rich pool of bothstatic and dynamic characteristics of expressions, which\\nplay a critical role for FER.\\nThe vast majority of the past work on FER does not take\\nthe dynamics of facial movement features into account [1].\\nSome efforts have been made in capturing and utilizing\\nfacial movement features, and almost all of them are video-based. These efforts try to adopt either geometric features of\\nthe tracked facial points (e.g., shape vectors [2], facial\\nanimation parameters [3], distance and angular [4], andtrajectories [5]), or appearance difference between holisticfacial regions in consequent frames (e.g., optical flow [6],\\nand differential-AAM [7]), or texture and motion changes in\\nlocal facial regions (e.g., surface deformation [8], motionunits [9], spatiotemporal descriptors [10], animation units\\n[11], and pixel differences [12]). Although they achieved\\npromising results, these approaches often require accuratelocation and tracking of facial points, which remainsproblematic [13]. In addition, it is still an open question as\\nto how to learn the grammars in defining dynamic features\\nand handle ambiguities in the input data [14]. On the otherhand, image-based FER techniques provide an alternative\\nway to recognize emotions based on appearance-based\\nfeatures in a single image, and are important for thesituation where only several images are available fortraining and testing. However, to the best of our knowl-\\nedge, no research has been reported on image-based FER\\nthat considers facial movement features.\\nIn this paper, we aim for improving the performance of\\nFER by automatically capturing facial movement features in\\nstatic images based on distance features. The distances areobtained by extracting “salient” patch-based Gabor features\\nand then performing patch matching operations. Patch-\\nbased Gabor features have shown excellent performance in\\novercoming position, scale, and orientation changes [15],\\n[16], [17], as well as extracting spatial, frequency, and\\norientation information [18]. They also show a greatIEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011 219\\n.The authors are with the Faculty of Science and Technology, Queens-\\nland University of Technology, 126 Margaret Street, Brisbane CBD,Queendsland 4001, Australia. E-mail: ligang.zhang@student.qut.edu.au,\\ndian@qut.edu.au.\\nManuscript received 2 Sept. 2010; revised 7 Feb. 2011; accepted 18 Apr. 2011;\\npublished online 16 May 2011.Recommended for acceptance by R. Calvo.\\nFor information on obtaining reprints of this article, please send e-mail to:\\ntaffc@computer.org, and reference IEEECS Log NumberTAFFC-2010-09-0066.Digital Object Identifier no. 10.1109/T-AFFC.2011.13.\\n1949-3045/11/$26.00 /C2232011 IEEE Published by the IEEE Computer Society\\nadvantage over the commonly used fiducial point-based\\nGabor [19], [20], [21], [22], [23], graph-based Gabor [24], anddiscrete Fourier transform [25] features in capturing\\nregional information. Although other appearance-basedfeatures, such as local binary patterns (LBP) [26], [27],\\n[10], Haar [28], and histograms of oriented gradients (HOG)\\n[29], have shown good performance in FER, they lack thecapacity of capturing facial movement features with highaccuracy. This is due to the fact that these appearance-basedfeatures are based on statistic values (e.g., histogramsimilarity) extracted from subregions; therefore, theyproduce similar results even when facial features move abit from the original position. On the other hand, Gaborfeatures have the capacity to accurately capture movementinformation and have been proven to be robust, even in thecase of face misalignment [30].\\nThe idea of patch matching operations has been used to\\nbuild features for object recognition [15], [16] and actionclassification [17], which remain robust when there arechanges in position, scale, and orientation. To fit for thepurpose of FER, we define the matching area and matchingscale to restrict the operations within a suitable space. Bymatching patch-based Gabor features in this space, multi-distance values are obtained. The minimum distance ischosen as the final feature for emotion classification. In this\\nway, one patch, which varies in its position, scale, and\\nshape, can still be captured provided that it is locatedwithin the defined matching space. To show the effective-ness of using the proposed distance features, we demon-strate the high performance on two widely used databases,significant improvements due to the consideration of facialmovement features, and promising results under faceregistration errors.\\nThe remainder of the paper is organized as follows:\\nSection 2 describes the proposed framework, while thedetails of building distance features and “salient” featureselection are explained in Sections 3 and 4, respectively.Section 5 represents the recognition and speed performance,and demonstrates comparison with the state-of-the-artperformance. Finally, conclusions are drawn in Section 6.\\n2P ROPOSED FRAMEWORK\\nFig. 2 illustrates the proposed framework, which iscomposed of preprocessing, training, and test stages. Atthe preprocessing stage, by taking the nose as the centerand keeping main facial components inclusive, facialregions are manually cropped from database images andscaled to a resolution of 48/C348pixels. No more processing\\nis conducted to imitate the results of real face detectors.Then, multiresolution Ga bor images are attained byconvolving eight scale, four-orientation Gabor filters with\\nthe scaled facial regions. During the training stage, a wholeset of patches is extracted by moving a series of patcheswith different sizes across the training Gabor images. Then,\\na patch matching operation is proposed to convert the\\nextracted patches to distance features. To capture facialmovement features, the matching area and matching scaleare defined to increase the matching space, whereas theminimum rule is used to find the best matching feature inthis space. Based on the converted distance features, a setof “salient” patches is selected by Adaboost. At the test\\nstage, the same patch matching operation is performed on\\na new image using the “salient” patches. The resultingdistance features are fed into a multiclass support vectormachine (SVM) to recognize six basic emotions, includinganger (AN), disgust (DI), fear (FE), happiness (HA),sadness (SA), and surprise (SU).\\nThe rest of this section gives an introduction of Gabor\\nfilters and SVM. The details of building distance features\\nand feature selection are explained in Sections 4 and 5,\\nrespectively.\\nIn this paper, 2D Gabor filter [31] is adopted and it can be\\nmathematically expressed as:\\nFðx;yÞ¼exp /C0\\nX2þ/C132Y2\\n2/C272/C18/C19\\n/C2cos2/C25\\n/C21X/C18/C19\\n;\\nX¼xcos/C18þysin/C18;Y¼/C0xsin/C18þycos/C18;ð1Þ\\nwhere /C18is the orientation, /C27the effective width, /C21the\\nwavelength, and /C13¼0:3the aspect ratio. Instead of the\\nwidely used five scales, eight scales (5:2:19 pixels) areadopted here to test the results using a larger number ofscales. The values of the rest of the parameters are set basedon [15] due to the high reported performance. As a result,four orientations ( /C045, 90, 45, 0 degrees) are used.\\nSVM [32] is one of the most widely used machine\\nlearning algorithms for classification problems. This paper\\ndirectly uses the LIBSVM [33] implementation of SVMswith four different kernels, including linear, polynomial,radial basis function (RBF), and sigmoid. The six-emotion-class problem is solved by the one-against-the-rest strategy.\\n3B UILDING DISTANCE FEATURES\\nFig. 3 depicts the two main processes to build distancefeatures: feature extraction and patch matching operation.Feature extraction aims to collect a set of discriminating 3Dpatches for all emotions, whereas the patch matching220 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011\\nFig. 2. The proposed framework.\\nFig. 1. Facial movement features. (a) Feature position (left mouth\\ncorner) changes. (b) Feature shape (mouth) changes. Facial regions are\\nmanually cropped from two subjects, “KA” and “KL,” on the JAFFE\\ndatabase.\\noperation converts these patches to distance features which\\ncan capture facial movement features.\\n3.1 Patch-Based Feature Extraction\\nThe upper parts of Figs. 3 and 4 show the feature extraction\\nalgorithm, which is comprised of four steps: First, all training\\nimages are classified into 10 sets. For each emotion, each\\nGabor scale, and each patch size, one Gabor image is\\nrandomly selected from all the images of emotion Ek\\n( F i g .3 a ) .S e c o n d ,g i v e no n ep a t c h Pawith a size of\\nPj/C3Pj/C3Onum, move this patch across the row and column\\npixels of this Gabor image (Fig. 3b), a set of 3D patches can be\\nextracted (Fig. 4 line-2), one example is shown in Fig. 3c.Third, the matching area and matching scale are recorded\\n(Fig. 4 lines-3 and -4, details explained in Section 3.2). Finally,a patch set is constituted by combining the extracted patchesof all emotions, all scales, and all patch sizes (Fig. 3d).\\nTo reduce the feature dimension and increase the\\nprocessing speed, we only extract a part of all patches by\\nmoving the patch P\\nawith a step. As indicated by line-1 in\\nFig. 4, the moving steps are set to 1, 2, 3, and 4 correspondingto four patch sizes of 2/C32/C34,4/C34/C34,6/C36/C34, and 8/C38/C34.\\nGiven 48/C348facial images, eight-scale, and four-orientation\\nGabor filters, the final set contains 148,032 patches.\\nAs indicated and investigated by Zhao and Pietikainen\\n[10], current patch-based approaches only concentrate on\\nthe location information of the selected patches, wherebyone location is shared by all emotions and only the locationinformation is preserved after feature selection. Thus, theuseful multiresolution information contained in thesepatches is discarded. On the contrary, our approachreserves both the location and multiresolution informationof patches for recognition, resulting in an equal set ofpatches for each emotion.\\n3.2 Patch Matching Operation\\nAs shown in the lower part in Fig. 3, the patch matchingoperation comprises of four steps for each patch and eachtraining image: First, the matching area and matching scaleare defined to provide a bigger matching space (Figs. 3e and3f). Second, the distances are obtained by matching thispatch with all patches within its matching space in atraining image (Fig. 3h). This step takes two patches as\\ninputs and yields one distance value based on a distance\\nmetric. Third, the minimum distance is chosen as thedistance feature of this patch in the training image (Fig. 3h).Finally, the distance features of all patches are combinedinto a final set with 148,032 elements (Fig. 3i).\\n3.2.1 Matching Area and Matching Scale Definition\\nThe matching area and matching scale are used to\\naccurately capture the position and scale changes causedby facial feature movements. The idea of them stems fromZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 221\\nFig. 3. Building distance features. (a) One scale is selected from eight-scale Gabor images. (b) Patches are extracted across all rows and columns in\\nthe selected scale image. (c) One extracted patch Pa. (d) Extracted patch set. (e) Defined matching scale. (f) Defined matching area. (g) One\\nmatching area. (h) Distance calculation. (i) Distance feature set.\\nFig. 4. Pseudocode of building distance features. (1) Defining moving\\nsteps. (2) Extracting patches. (3) Recording matching area. (4) Record-ing matching scale.\\nthe observation that the position and scale of one feature do\\nnot move or change a lot in different facial images oncethese images are roughly located by a face detector. Thus,\\nthe invariance to position and scale changes can be\\naccomplished by defining a bigger area and a larger scalefor each patch when performing patch matching.\\nFig. 5 illustrates the matching area Area of a patch P\\na\\nwith a size of Pj/C3Pj/C3Onum, while Fig. 3e shows the\\nmatching scales that are drawn in a gray color. In this\\npaper, the Area is set to two times of Pain width and height,\\nbut with the same orientation number Onum and center\\npoint. That is, Area ¼ð2/C3PjÞ/C3ð2/C3PjÞ/C3Onum. The match-\\ning scale is the same with that of Pabecause the cropped\\nfacial regions generally belong to the same scale. However,\\nit is flexible to increase the matching scale in the case of\\nlarge scale variations.\\n3.2.2 Distance Metric Definition\\nThe distance metric is used to compute the similarity\\nbetween two patches. Several metrics have been adopted in\\nprevious work, such as Gaussian-like euclidean [34] and\\nnormalized dot-product [17]. In this paper, four metrics,\\nincluding dense L1ðDL 1Þ, dense L2ðDL 2Þ, sparse L1ðSL1Þ,\\nand sparse L2ðSL2Þ, are used due to the computational\\nsimplicity, and they can be mathematically expressed as\\nDL 1:kPb/C0Pck¼1\\nPj/C3Pj/C3OnumXPj\\ni¼1XPj\\nj¼1XOnum\\no¼1/C13/C13Pijo\\nb/C0Pijo\\nc/C13/C13;\\nð2Þ\\nDL 2:kPb/C0Pck\\n¼1\\nPj/C3Pj/C3Onumﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nXPj\\ni¼1XPj\\nj¼1XOnum\\no¼1/C0\\nPijo\\nb/C0Pijo\\nc/C12vuut;ð3Þ\\nSL\\n1:kPb/C0Pck\\n¼1\\nPj/C3PjXPj\\ni¼1XPj\\nj¼1maxXOnum\\no¼1Pijo\\nb/C0maxXOnum\\no¼1Pijo\\nc !\\n;ð4Þ\\nSL2:kPb/C0Pck\\n¼1\\nPj/C3Pjﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\\nXPj\\ni¼1XPj\\nj¼1maxXOnum\\no¼1Pijo\\nb/C0maxXOnum\\no¼1Pijo\\nc !2vuut;ð5Þ\\nwhere, P\\nbandPcrepresent two patches, Pijorepresents the\\npixel values in the ith row, jth column, and oth orientationof the patches. The distances are normalized by dividing the\\nnumber of pixels in the patches. The dense distances can be\\nconceived as taking into account all orientations of each\\npixel in one patch, whereas the sparse distances only\\nconsider the dominant orientation [16] of each pixel in one\\npatch. Therefore, they differ in representing all features or\\nonly dominant features of all orientations.\\n4P ATCH-BASED FEATURE SELECTION AND\\nANALYSIS\\nIn this section, we use Adaboost for discriminative (called\\n“salient” here) patch selection on the Japanese female facial\\nexpression (JAFFE) and CK databases. To give a deeperunderstanding of the selected “salient” patches and provideuseful information on the design of Gabor filters and feature\\nextraction algorithms, we also present a description on their\\nposition, number, size, scale, and overlap distributions.\\n4.1 Databases\\nThe JAFFE database [35] contains 213 gray images of sevenfacial expressions (six basic + neutral) poses of 10 Japanesefemales. Each image has a resolution of 256/C3256pixels.\\nEach object has three or four frontal face images for each\\nexpression and their faces are approximately located in themiddle of the images. All images have been rated on six\\nemotion adjectives by 60 subjects.\\nThe Cohn-Kanade AU coded facial expression (CK)\\ndatabase [36] is one of the most comprehensive bench-\\nmarks for facial expression tests. The released portion ofthis database includes 2,105 digitized image sequencesfrom 182 subjects ranged in age from 18 to 30 years. Sixty-\\nfive percent are female; 15 percent are African-American\\nand 3 percent Asian or Latino. Six basic expressions werebased on descriptions of pro totypic emotions. Image\\nsequences from neutral to target display were digitized\\ninto 640/C3480or 490 pixel arrays with eight-bit precision\\nfor gray scale values.\\nIn this paper, all the images of six basic expressions from\\nthe JAFFE database are used. For the CK database, 1,184\\nimages that represent one of the six expressions are selected,\\nfour images for each expression of 92 subjects. The images arechosen from the last image (peak) of each sequence, then oneevery two images. The images of 10 subjects in the JAFFE\\ndatabase are classified into 10 sets, each of which includes\\nimages of one subject. Similarly, all images in the CKdatabase are classified into 10 similar sets and all images\\nof one subject are included in the same set.\\n4.2 “Salient” Patch Selection\\nThe feature extraction step produces a feature set contain-\\ning 148,032 patches. To reduce the feature dimension andthe redundant information, it is necessary to select a subsetof “salient” patches. In this paper, the widely used and\\nefficiency proven boosting algorithm—Adaboost [37]—is\\nused for “salient” patch selection.\\nSince Adaboost was designed to solve two-class pro-\\nblems, in this research the one-against-the-rest strategy is\\nused to solve the six-emotion-class problem. The training\\nprocess stops when the empirical error is below 0.0001 with\\nan initial error of 1. This setting is inspired by the stopping\\ncondition in [20] that there is no training error and the\\ngeneralization error becomes flat. For the training set, the222 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011\\nFig. 5. Matching area. (a) One patch Pa. (b) The corresponding matching\\narea “ Area ” in a new image.\\nJAFFE database includes all database images, whereas the\\nCK database is only composed of the peak frames.\\n4.3 Position Distribution of “Salient” Patches\\nThe position distribution of the “salient” patches demon-\\nstrates the most important facial areas for each emotion. In\\nFig. 6, the patches are distributed over different Gabor\\nscales, and they are drawn in one scale image for a simpleand clear demonstration. Based on this figure, we can see\\nthat the positions are distributed differently over six\\nemotions. However, most of these patches for all emotionstend to concentrate on the areas around the mouth and\\neyes. For sadness and surprise, the “salient” patches on\\nJAFFE focus on the eye areas, while those on CK focus onthe mouth area. For the remaining four emotions, they have\\nsimilar distributions between two databases. As shown in\\nthe second and third rows in Fig. 6, the positions of the“salient” patches in our work and those of the point-based\\n“salient” features in [38] for the same emotion tend to focus\\non the same areas. This suggests that there exist the same“salient” areas for each emotion regardless of use of point-\\nbased or patch-based Gabor features. However, the overall\\nnumber of the “salient” patches is much less than that of thepoint-based “salient” features (177 versus 538).\\n4.4 Number and Size Distributions of “Salient”\\nPatches\\nThe number and size distributions can provide useful hints\\non the number of patches for different emotions and howto choose suitable patch sizes during feature extraction. As\\nseen in Fig. 7, two databases have a similar overall number\\nof the “salient” patches. Among six emotions, fear andsadness need the largest numbers of patches to achieve thepreset recognition accuracy, whereas surprise requires the\\nleast number. Within four patch sizes, the size 4/C34takes a\\nsignificant proportion of the overall number of the“salient” patches.\\nOn the other hand, there are also some differences\\nbetween two databases. The number for anger on JAFFE is\\nmuch less than that on CK, while the number for disgust onJAFFE is much bigger than that on CK. Moreover, four patch\\nsizes are evenly distributed among six emotions on JAFFE,\\nbut the patch size 2/C32takes a significant proportion of the\\noverall number of the “salient” patches on CK. This reflects\\nthat emotions in JAFFE images need big sizes of patches to\\nrepresent useful information, whereas those in CK imagesonly require small sizes of patches. The reason may be thatthe emotions in CK are more distinct than those in JAFFE.\\n4.5 Scale Distribution of “Salient” Patches\\nThe scale distribution is a very important factor for\\ndetermining the scale number of Gabor filters. As observed\\nin Fig. 8, the “salient” patches of two databases areunevenly distributed across eight scales. JAFFE emphasizes\\nthe eighth scale and CK focuses on the fourth scale. For both\\ndatabases, the higher scales (fourth to eighth) contain morepatches than the lower scales (first to third). Therefore, the\\nemotional information is distributed across all scales with\\nan emphasis on the higher scales, which confirms Little-wort’s argument that a wider range of spatial frequencies,\\nparticularly high frequencies, could potentially improve\\nperformance [20].\\n4.6 Overlap Distribution of “Salient” Patches\\nTable 1 demonstrates the characteristics of number, size,\\nscale, emotion pair of the overlapping patches, which areselected as “salient” patches more than one time. As can beseen, the JAFFE database has a larger number of theoverlapping patches than the CK database (eight versusthree). As for the patch size, 4/C34dominates the over-\\nlapping patches on JAFFE, while 2/C32takes most of these\\npatches on CK. With respect to the patch scale, the patchesof JAFFE tend to distribute on the sixth, seventh, and eighth\\nscales, whereas those of CK are all included by the fourth\\nscale. The patch size and scale distributions again revealZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 223\\nFig. 6. Position distribution of the selected “salient” patches for six\\nemotions. The first and second rows show positions of the selectedpatches in the proposed approach on the JAFFE and CK databases,\\nrespectively, whereas the third row reveals positions of the selected\\npoint-based Gabor features in [38] on the CK database.\\nFig. 7. Number and size distributions of the selected “salient” patches for six emotions on (a) the JAFFE and (b) the CK databases. Note that Onumis\\nfour for all patch sizes of Pj/C3Pj/C3Onumand it is not shown.\\nthat JAFFE needs larger patches than CK. For the emotion\\npair, the majority of the overlapping patches are shared bythe same emotion. As shown in Fig. 9, the overlapping\\npatches are mainly distributed over disgust and anger.\\n5E XPERIMENTAL RESULTS\\nIn this section, we present the recognition and computa-\\ntional performance of the proposed approach. The perfor-mance obtained with and without matching area is also\\ncompared. Finally, a performance comparison with pre-\\nvious approaches is conducted.\\n5.1 Recognition Performance\\n5.1.1 JAFFE Database\\nThe performance results are obtained by averaging the\\ncorrect recognition rate (CRR) of all sets in 10 leave-one-set-out cross validations. Table 2 shows the results obtainedusing four SVMs and four distances. From this table, we cansee that the proposed approach performs the best with aCRR of 92.93 percent using DL\\n2and linear SVM. Regardingthe performance of distances, DL 2achieves higher CRRs than\\nthe other three distances for all SVMs. When L1is used, sparse\\ndistances outperform dense distances for linear, RBF, and\\nsigmoid SVMs. On the contrary, when L2is used, dense\\ndistances outperform sparse distances for all SVMs (note thatthe CRR of DL\\n2and sigmoid SVM is not shown). For both\\nsparse and dense distances, L2performs better than L1for all\\nSVMs. Among four SVMs, linear and RBF outperformpolynomial and sigmoid for all distances. More exactly, thebest performance is obtained by linear, which is followed by\\nRBF, whereas sigmoid ranks the lowest.\\nTable 3 demonstrates the confusion matrix of six\\nemotions using DL\\n2and linear SVM. Observed from this\\ntable, disgust and surprise belong to the most difficult facial\\nexpressions to be correctly recognized with the same CRR\\nof 90.00 percent, whereas anger is the easiest one with aCRR of 96.67 percent. Regarding the misrecognition rate,\\nanger contributes the most; as a result, it has a major\\nnegative impact on the overall performance. The emotionthat follows in misrecognition rate is fear.224 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011\\nFig. 8. Scale distribution of the selected “salient” patches for six emotions on (a) the JAFFE and (b) the CK databases.\\nFig. 9. Position distribution of the overlapping patches for six emotions.\\nThe upper row is for JAFFE and the lower row is for CK.TABLE 1\\nOverlapping Patches on the JAFFE and CK Databases\\nThe figures before parentheses stand for the number of overlapped\\npatches, while content in parentheses indicates patch sizes, scales, and\\nemotion pairs. As an instance of emotion pairs, “1(FE-SA)” means onepatch is shared by fear and sadness.TABLE 2\\nCRRs of Six Emotions on the JAFFE Database\\nTABLE 3\\nConfusion Matrix of Six Emotions on the JAFFE Database\\n\\n5.1.2 CK Database\\nThe CRRs using four SVMs and four distance metrics are\\nshown in Table 4, in which the proposed approach obtainsthe highest CRR of 94.48 percent using DL\\n2and RBF SVM.\\nRegarding the performance of distances, DL 2keeps the\\nhighest CRRs for all SVMs (note that the CRR of DL 2and\\nsigmoid SVM is not shown). Moreover, dense distances\\nhave a higher overall performance than sparse distances.This reflects that emotional information in the CK images isdistributed over all orientations rather than the dominantorientation of Gabor features. As for SVMs, RBF performsthe best for dense distances, while linear performs the bestfor sparse distances. This confirms with the results in [20]\\nthat RBF and linear perform better than polynomial on the\\nCK database.\\nTable 5 shows the confusion matrix of six emotions using\\nDL\\n2and RBF SVM. As can be seen, surprise performs the\\nbest, with a CRR of 100 percent, the following one ishappiness, with a CRR of 98.07 percent. On the other hand,anger is the most difficult facial expression to be correctlyrecognized, with a CRR of only 87.10 percent. Theperformance of surprise and anger on CK contrasts with\\nthat on JAFFE, in which surprise and anger are the most\\ndifficult and easiest emotions, respectively. The reason\\nprobably is that surprise images on CK are often character-ized as an exaggerated “open mouth,” while those onJAFFE are normally with a “close or slightly open mouth.”It can be seen from Fig. 6 that the selected patches for CKfocus on the mouth region, but those for JAFFE are mainly\\ndistributed around the eyes regions. Similarly, anger\\nimages are better expressed by the patches selected in theeye regions on JAFFE than those selected in the whole faceon CK. Among six emotions, anger and sadness contributemost to the misrecognition rate.\\n5.2 Performance versus Number of Patches\\nWe also test the relationship between the performance andthe number of patches. Table 6 shows the error thresholds\\nused to control the number of the patches selected by\\nAdaboost. These thresholds are set based on our observa-tion that the empirical errors of Adaboost decrease with afactor of 10 and the numbers are evenly distributed betweendecimal intervals. For instance, the number of errorsbetween 0.01 and 0.02 is similar to that between 0.003 and0.004. Accordingly, 38 groups of features are obtained by\\nselecting patches with empirical errors bigger than the\\ncorresponding error thresholds.\\nFor the JAFFE database, as can be seen from Fig. 10, the\\nproposed approach achieves the highest CRR of 93.48 percentusing DL\\n2and linear SVM when the error threshold equals to\\n0.0001 and the number of patches equals to 185. The overallperformance of four distances grows up rapidly at theZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 225\\nTABLE 4\\nCRRs of Six Emotions on the CK Database\\nTABLE 5\\nConfusion Matrix of Six Emotions on the CK Database\\nTABLE 6\\nError Thresholds Used to Control the Number of Patches\\nFig. 10. Recognition performance versus the number of patches on the JAFFE database using (a) linear and (b) RBF SVMs.\\nstarting stage; however, it begins to level off when the\\nnumber of patches exceeds 150 for linear and 80 for RBF. For\\nthe overall performance of SVMs, linear performs better than\\nRBF for all distances. Regarding the overall performance of\\ndistances, for both linear and RBF, the best performance is\\nachieved by DL 2, which is followed by SL2. On the other\\nhand, SL1andDL 1rank the last two.\\nFor the CK database, seen in Fig. 11, the proposed\\napproach obtains the highest CRR of 94.48 percent using\\nDL 2and RBF SVM when the error threshold is 0 and the\\nnumber of patches is 180. This implies that a performance\\nimprovement can still be achieved using a larger number of\\npatches. Similarly to that on JAFFE, the CRR grows up rapidly\\nat the starting stage and L2outperforms L1for both linear and\\nRBF. On the other hand, the CRR reaches the plateau with a\\nquicker speed than that on JAFFE and DL 1performs better\\nthan SL1. Moreover, the performance difference between\\nlinear and RBF is smaller than that on JAFFE.\\n5.3 Matching Area versus No Matching Area\\nTo evaluate the performance improvement rising from the\\nuse of facial movement features, we compare the perfor-\\nmance of with and without matching area. In the latter case,\\nthe distance features are obtained by performing subtraction\\nbetween two patches at the exact same position. Therefore,\\nthe resulting features do not include the information of\\nfeature movements. Fig. 12 shows the comparison results\\nobtained when the error threshold of Adaboost is 0. The\\nclassifiers of JAFFE and CK are linear and RBF SVMs,respectively. As can be seen, for the JAFFE database, the\\nrecognition performance of the proposed approach using\\nfour distances is greatly boosted due to the use of matchingarea. There is a CRR increase of 11.41 percent using DL\\n2. For\\nthe CK database, the CRRs of DL 1and DL 2are improved\\nabout 2.5 percent due to the use of matching area, while the\\nperformance of SL1andSL2does not benefit from matching\\narea. Considering the highest CRR of four distances, we can\\nsee that taking facial movement features into account helps\\nto improve the recognition performance.\\n5.4 Perforamnce under Registration Errors\\nTo test the performance of the proposed approach usingimages with registration errors, we add uniform random\\nnoises into the coordinate of the top-left corner and the scale\\nof each face region produced by the widely used Viola-Jones face detector [39]. The noises are controlled so that\\nboth the coordinate and the image scale randomly change\\nwithin a range of ½/C0/C11%;/C11%/C138(/C11/C26½1;2;3;4;5;6/C138) of face\\nwidth. In addition, we also include neutral images in the\\nexperiment. In videos, the starting and ending frames for an\\nexpression can be determined by classifying the current\\nframe into “neutral” or “emotion.” In this way, two sets of\\ndatabase images with different levels of noises are created.\\nFig. 13 shows sample images with 3 and 6 percent errors\\nfrom the JAFFE and CK databases. After scaling the\\nsimulated images into 48/C348, there will be a maximum of\\n/C11pixels ð48/C32/C3/C11%Þchanges of position and scale for a\\nlevel of /C11percent errors.226 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011\\nFig. 12. Recognition accuracy (percent) obtained with and without matching area (a) on the JAFFE database using linear SVM (b) and on the CK\\ndatabase using RBF SVM.\\nFig. 11. Recognition performance versus the number of patches on the CK database using (a) linear and (b) RBF SVMs.\\nTo handle scale changes, the matching scale is set to\\ninclude two neighbor scales and the scale itself. In each ofthe 10 sets cross validation, nonerror images in nine sets areused for training, while the error-simulated images of theone set left are used for testing. This is important for the realsituation that only ideally registered images are availablefor the training. To testify the usefulness of matching scale,we give the results of using both matching area andmatching scale (AreaScale), as well as only using matchingarea (Area). Note that SL\\n1is used here. We also compare\\nthe results with using point-based four orientations andeight scales Gabor features + Adaboost feature selector +RBF SVM classifier (point-based Gabor).\\nFig. 14 shows the performance of three approaches under\\nthe simulated errors. As expected, all approaches sufferdecreasing performances using a larger percentage oferrors. The two proposed approaches achieve higher overall\\nperformances than the approach using point-based Gabor\\nfeatures, for both JAFFE and CK. This again demonstratesthe advantage of patch-based Gabor over point-basedGabor features in terms of the performance under faceregistration errors. Using both matching area and scale(AreaScale) performs better than using matching area only(Area) for both the databases. At the error level of 4 percent,which can be considered as larger than the errors producedby real face detectors, AreaScale still keeps a CRR of 69.5and 83.9 percent for JAFFE and CK, respectively. Therefore,the proposed approach achieves promising results underthe simulated registration errors.5.5 Computational Time Performance\\nFig. 15 illustrates the average computational time at three\\nstages, including Gabor images (Gab), patch matching (PM),\\nand classification (SVM). The program was developed byMatlab 7.6.0 under a laptop configuration of core duo\\n1.66 GHz CUP and 2 GB memory. The proposed approach\\nachieves a speed of 0.1258 seconds per image for the JAFFEdatabase (using DL\\n2and linear) and 0.1185 seconds per\\nimage for the CK database (using DL 2and RBF). Thus, a\\nreal-time processing is expected if our approach could be\\ndeveloped by time efficient languages, such as C, C++.Among three stages, computing Gabor images takes the\\nbiggest proportion of the overall time, while the classifica-\\ntion requires the least amount of time.\\n5.6 Comparison with State-of-the-Art Performance\\nTable 7 demonstrates the r esults compared with the\\nreported results of the benchmarked approaches. Theseapproaches are selected because they produced the state-of-\\nthe-art performance using a similar testing strategy and the\\nsame databases. In [40], only the recognition results of theleave-one-subject-out strategy is used here as this strategy is\\nmore similar to our leave-one-set-out cross validations. The\\nrecognition results in [41] were obtained by removing twoJAFFE images named “KR.SR3.79” and “NA.SU1.79.”ZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 227\\nFig. 14. Recognition performance under face registration errors on (a) the JAFFE and (b) the CK databases.Fig. 15. The used time (in seconds) per image at three stages. “Gab”\\nand “PM” indicate the stages of Gabor image and patch matching, “J”\\nand “C” indicate using the JAFFE and CK databases, “D” and “S” standfor using dense and sparse distances, “L1” and “L2” represent L\\n1and\\nL2distances, respectively. The time is obtained using the SVM type with\\nthe highest CRR for each distance.\\nFig. 13. Sample images with errors simulated by uniform random noisesranged (a) [–3 percent, 3 percent] and (b) [–6 percent, 6 percent] of facewidth.\\nAs shown in Table 7, the proposed approach outperforms\\nall nine benchmarked approaches [23], [26], [40], [41], [42],[43], [44], [45], [46] when the JAFFE database is used, and\\nthree out of four benchmarked approaches [20], [21], [26],\\n[46] when the CK database is used. When six emotions areused, the CRR of the proposed approach is 9.73 and 15.88\\npercent higher than those in [42] and [43], respectively, on the\\nJAFFE database, as well as 1.88 percent higher than that\\nobtained using LBP features in [26] on the CK database. The\\nresult of the proposed approach on the CK database is 0.62percent lower than the result obtained using the boosted-LBP\\nfeatures in [26] and 1.39 percent lower than the result in [46].\\nHowever, the work [26] normalized the face based onmanually labeled eye locations and improved the results\\nby optimizing the SVM parameters. While the proposed\\napproach does not involve normalization of face regions anduses the default parameters in LIBSVM. Another difference\\nis that the database images in the proposed approach\\nrepresent bigger emotional intensity than those in [26]. Tobe specific, the proposed approach collects images using a\\n“every two images from the peak frame” strategy, while [26]\\njust used the three peak frames. Wong and Cho [46] obtained\\nthe results based on five-fold cross validations and five\\nexpressions; therefore it used more training images andclassified less emotions compared to our approach.\\n6C ONCLUSION AND FUTURE WORK\\nThis paper explores the issue of facial expression recogni-\\ntion using facial movement features. The effectiveness ofthe proposed approach is testified by the recognitionperformance, computational time, and comparison withthe state-of-the-art performance. The experimental resultsalso demonstrate significant performance improvements\\ndue to the consideration of facial movement features and\\npromising performance under face registration errors.\\nThe results indicate that patch-based Gabor features show\\na better performance over point-based Gabor features interms of extracting regional features, keeping the positioninformation, achieving a better recognition performance, andrequiring a less number. Different emotions have different\\n“salient” areas; however, the majority of these areas are\\ndistributed around mouth and eyes. In addition, these“salient” areas for each emotion seem to not be influencedby the choice of using point-based or using patch-basedfeatures. The “salient” patches are distributed across all\\nscales with an emphasis on the higher scales. For both theJAFFE and CK databases, DL\\n2performs the best among four\\ndistances. As for emotion, anger contributes most to the\\nmisrecognition. The JAFFE database requires larger sizes of\\npatches than the CK database to keep useful information.\\nThe proposed approach can be potentially applied into\\nmany applications, such as patient state detection, driverfatigue monitoring, and intelligent tutoring system. In our\\nfuture work, we will extend our approach to a video-based\\nFER system by combining patch-based Gabor features withmotion information in multiframes. Recent progress onaction recognition [47] and face recognition [48] has laid afoundation for using both appearance and motion features.\\nACKNOWLEDGMENTS\\nThe authors would like to thank Nicki Ridgeway for\\nproviding the Cohn-Kanade AU-Coded Facial Expression\\nDatabase and the providers of the JAFFE database.\\nREFERENCES\\n[1] Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang, “A Survey of\\nAffect Recognition Methods: Audio, Visual, and SpontaneousExpressions,” IEEE Trans. Pattern Analysis and Machine Intelligence,\\nvol. 31, no. 1, pp. 39-58, Jan. 2009.\\n[2] T. Yan, C. Jixu, and J. Qiang, “A Unified Probabilistic Framework\\nfor Spontaneous Facial Action Modeling and Understanding,”\\nIEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 2,\\npp. 258-273, Feb. 2010.\\n[3] P.S. Aleksic and A.K. Katsaggelos, “Automatic Facial Expression\\nRecognition Using Facial Animation Parameters and MultistreamHMMs,” IEEE Trans. Information Forensics and Security, vol. 1,\\nno. 1, pp. 3-11, Mar. 2006.\\n[4] D. Hamdi, V. Roberto, S.A. Ali, and G. Theo, “Eyes Do Not Lie:\\nSpontaneous versus Posed Smiles,” Proc. Int’l Conf. Multimedia,\\npp. 703-706, 2010.\\n[5] T.-H. Wang and J.-J.J. Lien, “Facial Expression Recognition System\\nBased on Rigid and Non-Rigid Motion Separation and 3D PoseEstimation,” J. Pattern Recognition, vol. 42, no. 5, pp. 962-977, 2009.\\n[6] M. Yeasin, B. Bullot, and R. Sharma, “Recognition of Facial\\nExpressions and Measurement of Levels of Interest from Video,”IEEE Trans. Multimedia, vol. 8, no. 3, pp. 500-508, June 2006.\\n[7] Y. Cheon and D. Kim, “Natural Facial Expression Recognition\\nUsing Differential-AAM and Manifold Learning,” Pattern Recogni-\\ntion, vol. 42, pp. 1340-1350, 2009.\\n[8] F. Tsalakanidou and S. Malassiotis, “Real-Time 2D+3D Facial\\nAction and Expression Recognition,” Pattern Recognition, vol. 43,\\npp. 1763-1775, 2010.\\n[9] I. Cohen, N. Sebe, A. Garg, L.S. Chen, and T.S. Huang, “Facial\\nExpression Recognition from Video Sequences: Temporal andStatic Modeling,” Computer Vision and Image Understanding, vol. 91,\\npp. 160-187, 2003.\\n[10] G. Zhao and M. Pietikainen, “Boosted Multi-Resolution Spatio-\\ntemporal Descriptors for Facial Expression Recognition,” Pattern\\nRecognition Letters, vol. 30, pp. 1117-1127, 2009.\\n[11] F. Dornaika and F. Davoine, “Simultaneous Facial Action Tracking\\nand Expression Recognition in the Presence of Head Motion,” Int’l\\nJ. Computer Vision, vol. 76, pp. 257-281, 2008.\\n[12] A. Kapoor, W. Burleson, and R.W. Picard, “Automatic Prediction\\nof Frustration,” Int’l J. Human-Computer Studies, vol. 65, pp. 724-\\n736, 2007.\\n[13] L. Peng and S.J.D. Prince, “Joint and Implicit Registration for Face\\nRecognition,” Proc. IEEE Conf. Computer Vision and Pattern\\nRecognition, pp. 1510-1517, 2009.\\n[14] T. Huang, A. Nijholt, M. Pantic, and A. Pentland, “Human\\nComputing and Machine Understanding of Human Behavior: ASurvey,” Artifical Intelligence for Human Computing, vol. 4451,\\npp. 47-71, 2007.228 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 2, NO. 4, OCTOBER-DECEMBER 2011\\nTABLE 7\\nComparison with State-of-the-Art Performance\\nThe figures in parentheses stand for the number of the testing facial\\nexpressions.\\n[15] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio,\\n“Robust Object Recognition with Cortex-Like Mechanisms,” IEEE\\nTrans. Pattern Analysis and Machine Intelligence, vol. 29, no. 3,\\npp. 411-426, Mar. 2007.\\n[16] J. Mutch and D.G. Lowe, “Multiclass Object Recognition with\\nSparse, Localized Features,” Proc. IEEE CS Conf. Computer Vision\\nand Pattern Recognition, pp. 11-18, 2006.\\n[17] H. Jhuang, T. Serre, L. Wolf, and T. Poggio, “A Biologically\\nInspired System for Action Recognition,” Proc. IEEE 11th Int’l\\nConf. Computer Vision, pp. 1-8, 2007.\\n[18] L. Zhen, L. Shengcai, H. Ran, M. Pietikainen, and S.Z. Li, “Gabor\\nVolume Based Local Binary Pattern for Face Representation and\\nRecognition,” Proc. IEEE Eighth Int’l Conf. Automatic Face and\\nGesture Recognition, pp. 1-6, 2008.\\n[19] L. Wiskott, J.M. Fellous, N. Kuiger, and C. von der Malsburg,\\n“Face Recognition by Elastic Bunch Graph Matching,” IEEE Trans.\\nPattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 775-779,\\nJul. 1997.\\n[20] G. Littlewort, M.S. Bartlett, I. Fasel, J. Susskind, and J. Movellan,\\n“Dynamics of Facial Expression Extracted Automatically fromVideo,” Image and Vision Computing, vol. 24, pp. 615-625, 2006.\\n[21] H.Y. Chen, C.L. Huang, and C.M. Fu, “Hybrid-Boost Learning for\\nMulti-Pose Face Detection and Facial Expression Recognition,”Pattern Recognition, vol. 41, pp. 1173-1185, 2008.\\n[22] S. Hoch, F. Althoff, G. McGlaun, and G. Rigoll, “Bimodal Fusion\\nof Emotional Data in an Automotive Environment,” Proc. IEEE\\nInt’l Conf. Acoustics, Speech, and Signal Processing, pp. 1085-1088,\\n2005.\\n[23] G. Guo and C.R. Dyer, “Learning from Examples in the Small\\nSample Case: Face Expression Recognition,” IEEE Trans. Systems,\\nMan, and Cybernetics, Part B: Cybernetics, vol. 35, no. 3, pp. 477-488,\\nJune 2005.\\n[24] S. Zafeiriou and I. Pitas, “Discriminant Graph Structures for Facial\\nExpression Recognition,” IEEE Multimedia, vol. 10, no. 8, pp. 1528-\\n1540, Dec. 2008.\\n[25] T. Xiang, M.K.H. Leung, and S.Y. Cho, “Expression Recognition\\nUsing Fuzzy Spatio-Temporal Modeling,” Pattern Recognition,\\nvol. 41, pp. 204-216, 2008.\\n[26] C. Shan, S. Gong, and P.W. McOwan, “Facial Expression\\nRecognition Based on Local Binary Patterns: A ComprehensiveStudy,” Image and Vision Computing, vol. 27, pp. 803-816, 2009.\\n[27] Z. Guoying and M. Pietikainen, “Dynamic Texture Recognition\\nUsing Local Binary Patterns with an Application to FacialExpressions,” IEEE Trans. Pattern Analysis and Machine Intelligence,\\nvol. 29, no. 6, pp. 915-928, June 2007.\\n[28] P. Yang, Q. Liu, and D.N. Metaxas, “Boosting Encoded Dynamic\\nFeatures for Facial Expression Recognition,” Pattern Recognition\\nLetters, vol. 30, pp. 132-139, 2009.\\n[29] C. Orrite, A. Gan ˜a´n, and G. Rogez, “HOG-Based Decision Tree\\nfor Facial Expression Classification,” Proc. Fourth Iberian Conf.\\nPattern Recognition and Image Analysis, pp. 176-183, 2009.\\n[30] S. Shiguang, G. Wen, C. Yizheng, C. Bo, and Y. Pang, “Review the\\nStrength of Gabor Features for Face Recognition from the Angle ofIts Robustness to Mis-Alignment,” Proc. 17th Int’l Conf. Pattern\\nRecognition, vol. 1, pp. 338-341, 2004.\\n[31] D. Gabor, “Theory of Communication,”\\nJ. Institution of Electrical\\nEngineers—Part III: Radio and Comm. Eng., vol. 93, pp. 429-441,\\n1946.\\n[32] C. Cortes and V. Vapnik, “Support-Vector Networks,” Machine\\nLearning, vol. 20, pp. 273-297, 1995.\\n[33] C.C. Chang and C.J. Lin, “LIBSVM: A Library for Support Vector\\nMachines, 2001,” http://www.csie.ntu.edu.tw/cjlin/libsvm,2001.\\n[34] T. Serre, L. Wolf, and T. Poggio, “Object Recognition with Features\\nInspired by Visual Cortex,” Proc. IEEE CS Conf. Computer Vision\\nand Pattern Recognition, vol. 2, pp. 994-1000, 2005.\\n[35] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding Facial\\nExpressions with Gabor Wavelets,” Proc. IEEE Third Int’l Conf.\\nAutomatic Face and Gesture Recognition, pp. 200-205, 1998.\\n[36] T. Kanade, J.F. Cohn, and T. Yingli, “Comprehensive Database for\\nFacial Expression Analysis,” Proc. IEEE Fourth Int’l Conf. Automatic\\nFace and Gesture Recognition, pp. 46-53, 2000.\\n[37] Y. Freund and R.E. Schapire, “A Decision-Theoretic General-\\nization of On-Line Learning and an Application to Boosting,”J. Computer and System Sciences, vol. 55, no. 1, pp. 119-139, 1997.[38] M.S. Bartlett, G. Littlewort, I. Fasel, and J.R. Movellan, “Real Time\\nFace Detection and Facial Expression Recognition: Developmentand Applications to Human Computer Interaction,” Proc. Compu-\\nter Vision and Pattern Recognition Workshop, p. 53, 2003.\\n[39] P. Viola and M.J. Jones, “Robust Real-Time Face Detection,” Int’l\\nJ. Computer Vision, vol. 57, pp. 137-154, 2004.\\n[40] M. Kyperountas, A. Tefas, and I. Pitas, “Salient Feature and\\nReliable Classifier Selection for Facial Expression Classification,”Pattern Recognition, vol. 43, pp. 972-986, 2010.\\n[41] C. Zhengdong, S. Bin, F. Xiang, and Z. Yu-Jin, “Automatic\\nCoefficient Selection in Weighted Maximum Margin Criterion,”\\nProc. 19th Int’l Conf. Pattern Recognition, pp. 1-4, 2008.\\n[42] W. Yuwen, L. Hong, and Z. Hongbin, “Modeling Facial Expres-\\nsion Space for Recognition,” Proc. IEEE/RSJ Int’l Conf. Intelligent\\nRobots and Systems, pp. 1968-1973, 2005.\\n[43] Z. Wenming, Z. Xiaoyan, Z. Cairong, and Z. Li, “Facial Expression\\nRecognition Using Kernel Canonical Correlation Analysis(KCCA),” IEEE Trans. Neural Networks, vol. 17, no. 1, pp. 233-\\n238, Jan. 2006.\\n[44] Y. Horikawa, “Facial Expression Recognition Using KCCA with\\nCombining Correlation Kernels and Kansei Information,” Proc.\\nInt’l Conf. Computational Science and Its Applications, pp. 489-498,\\n2007.\\n[45] J. Bin, Y. Guo-Sheng, and Z. Huan-Long, “Comparative Study of\\nDimension Reduction and Recognition Algorithms of DCT and\\n2DPCA,” Proc. Int’l Conf. Machine Learning and Cybernetics, pp. 407-\\n410, 2008.\\n[46] J.-J. Wong and S.-Y. Cho, “A Face Emotion Tree Structure\\nRepresentation with Probabilistic Recursive Neural NetworkModeling,” Neural Computing and Applications, vol. 19, pp. 33-54,\\n2010.\\n[47] K. Schindler and L. van Gool, “Action Snippets: How Many\\nFrames Does Human Action Recognition Require?” Proc. IEEE\\nConf. Computer Vision and Pattern Recognition, pp. 1-8, 2008.\\n[48] A. Hadid and M. Pietikainen, “Combining Appearance and\\nMotion for Face and Gender Recognition from Videos,” Pattern\\nRecognition, vol. 42, pp. 2818-2827, 2009.\\nLigang Zhang is currently working toward the\\nPhD degree at the Queensland University of\\nTechnology, Brisbane, Australia. His researchinterests include face-related technologies (e.g.,face expression recognition and face recogni-tion), affective computing, affective contentanalysis, pattern recognition, and computervision. He is a student member of the IEEE.\\nDian Tjondronegoro is an associate profes-\\nsor on the Faculty of Science and Technology\\nat Queensland University of Technology. He\\nleads the Mobile Multimedia Research Groupand has published more than 60 refereedarticles in the field. His research interestsinclude video analysis, summarization, andvisualization, multichannel content analysis,mobile applications, and interaction design.\\n.For more information on this or any other computing topic,\\nplease visit our Digital Library at www.computer.org/publications/dlib.ZHANG AND TJONDRONEGORO: FACIAL EXPRESSION RECOGNITION USING FACIAL MOVEMENT FEATURES 229\\n\\n',\n",
       " '1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 1 \\n \\nAbstract—Facial expression recognition (FER) has long \\nbeen a challenging task in computer vision. In this paper, \\nwe propose a novel method, named Deep Comprehensive \\nMulti-patches Aggregation Convolutional Neural Networks \\n(DCMA-CNNs), to solve FER problem. The proposed \\nmethod is a deep-based framework, which mainly consists \\nof two branches of Convolutional Neural Network (CNN). \\nOne branch extracts local features from image patches \\nwhile the other extracts holistic features from the whole \\nexpressional image. In our model, local features depict \\nexpressional details and holistic features characterize the \\nhigh-level semantic information of an expression. We \\naggregate both local and holistic features before making \\nclassification. These two types of hierarchical features \\nrepresent expressions in different scales. Compared with \\nmost current methods with single type of feature, our \\nmodel can represent expressions more comprehensively. \\nAdditionally, in the training stage, a novel pooling strategy \\nnamed Expressional Transformation-invariant pooling \\n(ETI-pooling) is proposed for handling nuisance variations, \\nsuch as rotations, variant illuminations, etc. Extensive \\nexperiments are conducted on the famous CK+ and JAFFE \\nexpression datasets, where the recognition results obtained \\nby our model are superior to most existing FER methods.  \\n \\nIndex Terms —Convolutional neural network, expressional \\ntransformation-invariant, facial expression recognition , feature \\naggregation. \\nI. INTRODUCTION  \\nACIAL  expressions are important carriers for human to \\nconvey emotions in communications. Study on nonverbal \\ncommunication [1] reveals that 55% of a person’s emotional or \\nintentional information is conveyed through facial expressions. \\nRecently, researches on emotion al analysis have made great \\nachievements. On one hand, development of neuroscience and \\ncognitive science well propel the progress of emotional \\nanalysis. On the other hand, technical advance in computer \\nvision and machine learning makes applications related to \\nemotional analysis available to the public. As an important \\nsubfield of emotional analysis, researches on facial expression \\nrecognition (FER) develop quickly as well. Applications based \\non FER can be found in many cases, such as human-computer \\ninteraction system [2], multimedia [3], surveillance [4] and driver safety [5]. \\nSystematical studies on facial expression analysis can date \\nback to the work of Ekman et al. [6]. The main target of facial \\nexpression analysis is to establish a system that can \\nautomatically classify different expressions. In general, facial \\nexpressions can be categorized into six basic expressions [7], \\nwhich include anger, disgust, fear, happiness, sadness and \\nsurprise. Therefore, the primary task of current expressional \\nanalysis is to classify these six basic expressions. \\nPrevious methods on FER can be categorized into two \\ngroups: detecting facial actions that are related to a specific \\nexpression or making classification based on the extracted \\nimage features. In Facial Action Coding System (FACS) [8], \\nexpressions are encoded by Action Units (AUs), which refer to \\nsome tiny but discriminable facial muscle changes. Thus, \\nresearchers usually convert FER problem to the task of AU \\ndetection. Some other methods represent expressions by some \\nhand-crafted patterns or features, which can be utilized to train \\nan expression classifier. However, some intractable problems \\nare inevitable for these methods. For example, when encoding \\nexpressions by AUs, it’s hard for researchers to accurately \\ndetect every AU in an image as facial muscle moves are \\nsometimes difficult to be track ed. It is also hard to design a type \\nof feature that can adapt to different environments. Variations \\nsuch as illuminations and image rotations can weaken the \\nrepresentative capacity of hand-crafted features. \\nIn recent years, Convolutional Neural Network (CNN) has \\nachieved great success in the field of computer vision. It can \\nadaptively adjust its convolutional kernels of each layer to \\nobtain some desired features, which makes it adapt to various \\nclassification problems without much prior knowledge. In \\ngeneral, the output of CNN, i.e. the feature maps of the last \\nlayer, will be treated as the high-level semantic concept to \\nrepresent the input. In FER problem, previous studies [31], [32] \\nhave revealed that expressional changes usually occur on some \\nsalient facial regions such as neighborhood of mouth, eyes and \\nnose. This implies that details of local facial regions can be \\ndiscriminative for expressional recognition. However, most \\ncurrent CNN-based methods on FER only extract features from \\nthe whole expressional image. These methods emphasize the \\nintegrality of a facial expression but ignore the information of \\nlocal details. In the other words, typical CNN can ’t take full \\nadvantage of recognition-effective information encoded in Siyue Xie and Haifeng Hu   Facial Expression Recognition Using \\nHierarchical Features with Deep Comprehensive \\nMulti -Patches Aggregation Convolutional \\nNeural Networks  \\nF \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 2 \\nexpressional images. Therefore, we attempt to make some \\nmodification in commonly used CNN architecture for \\nimproving performance on FER problem. \\nIn this paper, we present a novel framework, named Deep \\nComprehensive Multi-Patches Aggregation Convolutional \\nNeural Networks (DCMA-CNNs), for solving the FER \\nproblem. Fig.1 illustrates the flowchart of our method. The \\nframework consists of two individual CNN branches: one \\nextracts holistic features from the whole image and the other \\nextracts local features from some overlapped image patches. \\nHolistic features aim to represent the integrity of an expression \\nwhile local features focus on describing details on local regions, \\nwhich can indirectly indicate some active expressional regions \\non the face. These two separated branches represent an \\nexpressional image from two different scales, each of which is \\ncomplementary to the other. Compared with current works, \\nwhich mostly characterize expressions using single type of \\nfeatures, DCMA-CNNs can represent an expressional image \\nmore comprehensively by aggregating two types of hierarchical \\nfeatures. In addition, we improve the TI-pooling [9] and \\npropose the Expressional Transformation-invariant pooling \\n(ETI-pooling) strategy for handling nuisance variations such as \\nilluminations, poses and noises. ETI-pooling enhances the \\ndiscriminative ability of our model by fusing a certain number \\nof features that from the same class. This pooling strategy is \\napplied to modify the structure of typical CNN and well \\nimprove the recognition performance. Extensive experiments \\nare conducted on the CK+ and JAFFE dataset and classification \\nresults demonstrate the effectives of the proposed method. \\nContributions of our model can be summarized as follow: \\n(1) A novel two-branch framework is proposed to solve the \\nchallenging FER problem. One branch is implemented \\nto extract local features from image patches, which \\nhighlights the detailed information of facial expressions. \\nSalient patches and active regions related to FER \\nproblem can be determined and visualized based on the \\nextracted local features. \\n(2) We incorporate both the holistic and local features in to \\nour model. These two types of hierarchical features \\nrepresent images in different scales. By aggregating \\nthese features, we can obtain the image representation with more discriminative power, which significantly \\nimproves the classification performance. \\n(3) We modify typical CNN structure with the proposed \\nETI-pooling, which can distinguish expression-sensitive \\nelements from extracted features . Owing to the \\nmodification, our model can be more robust to some \\nvariations such as illuminations, image rotations, noises, \\netc. \\nThe remainder of this paper is organized as follow. We make \\na brief review of some existing works on FER in section II. The \\ndetails of DCMA-CNNs will be specialized in Section III. In \\nSection IV, experiments and result analysis are presented. We \\nconclude our method in the Section V.  \\nII. RELATED W ORKS  \\nExpression recognition has long been a challenging problem \\nin emotional analysis. Conventional approaches can be \\ngenerally categorized into two groups: AU-based and \\nfeature-based approaches. Recently, methods based on \\ndeep-learning algorithms make great achievements in \\nexpression recognition, which is regarded as an effective \\nalternative to solve FER problem.  \\nAU-based methods attempt to detect expression-related AUs \\non a facial image, which are inspired by the studies on FACS \\n[10]. Researchers can recognize an expression according to the \\ncombination of detected AUs [11], [12], [13], [14], [15]. Some \\nother methods even focus on 3D AU detection [16], [17]. As for \\nfeature-based method, hand-crafted patterns or features are \\nusually used to represent an expressional image. Geometric \\nrelationships among facial organs or landmark points are \\ntypical features to represent expressions [ 19], [20], [21], [ 22]. \\nSome image operators , such as Local Binary Patterns (LBP) \\n[23], Local Description Patterns (LDP) [ 24], Gabor-features \\n[25] Local Phase Quantization (LPQ) [ 26], [27] or Scale \\nInvariant Feature Transform(SIFT) [ 28], are used in expression \\nanalysis as they can extract significant information from \\nimages. Moreover, some methods divide images into patches \\nwith different scales. Features extracted from these patches can \\nhighlight some detailed information of local facial regions [ 29], \\n[30], [31], which significantly improve the recognition \\nperformance. To enhance the comprehensive representation \\nFig. 1  Flowchart of the proposed method  \\n\\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 3 \\nability, methods like [32 ] even fuse different types of features \\nbefore making classification. \\nIn recent years, deep-based algorithms have been applied to \\nexpression recognition. Zhao et al.  [18] propose the Deep \\nRegion and Multi-label Learning (DRML) algorithm to \\nconduct AUs detection, which addresses  the problem of Region \\nlearning (RL) and multi-label learning (ML). Li et al . [36] \\nextract Gabor-wavelet features from images to train a deep \\nnetwork for FER task. Mollahosseini et al.  [37] construct a \\nCNN with inception layers. Their experiments are conducted \\non seven standard face datasets and obtained comparable \\nresults. Liu et al.  [38] model a system named Boosted Deep \\nBelief Network (BDBN) to classify different expressions. They \\ndivide expressional images into patches. Some patches with \\nhigh discriminative power will be selected and combined to \\ntrain a strong classifier.  \\nSome works aggregate or fuse different types of features \\nthrough deep network to produce a comprehensive \\nrepresentation, which usually results in a better recognition \\nresult than using single type of features. For example, \\nMajumder et al.  [32] extract LBP features as well as facial \\ngeometrical features from expressional images. These two \\ntypes of features are finally fused by a 2-layer autoencoder. \\nHamester et al.  [39] construct 2-channel architecture for feature \\nextraction, which utilizes CNNs and an autoencoder to extract \\nfeatures. Jung et al. [40] train a CNN and a deep neural network \\nindependently and combine them through fully connected \\nlayers with joint fine-tuning.  However, it should be noted that \\nmost existing deep-based methods merely focus on extracting \\nhigh-level semantic concept of expressions but ignore \\nfine-grained information in local facial regions. Different from \\nsome existing works, in this paper, we present an individual \\nCNN branch to extract local features, which highlights the \\nimportance of local detailed information in expression analysis.  III. PROPOSED METHOD  \\n The proposed model is based on a two-branch CNN \\nframework, as shown in Fig.2. In order to extract \\nvariation-robust features, we propose the ETI-pooling strategy \\nto modify the typical structure of CNN, which forms the CNN \\nmodule. A CNN module is a basic unit for feature extraction in \\nthe proposed DCMA -CNNs model , and it helps extract local \\nand holistic features from an input image.  \\nIn the following subsections, we briefly review the structure \\nof typical CNN (Section III.A) and then describe the special \\ncase of obtain ing the desired CNN module by adopting \\nETI-pooling (Section III.B). The obtained CNN module will be \\nused to construct the DCMA-CNNs model (Section III.C). \\nAdditionally, a salient patch learning algorithm is presented \\nbased on the proposed DCMA-CNNs model (Section III.D). \\nThis algorithm can indicate some active facial regions that are \\nrelevant to changes in expression.   \\nA. Convolutional Neural Network \\nCNN has been successfully applied in many tasks related to \\ncomputer vision in the recent years. Typically, a CNN \\ncomprises stacks of multiple convolutional layers and pooling \\nlayers. In general, the features generated by the deeper layers \\ncan represent the content of a larger region in the input image. \\nFeature maps yielded by the last layer can be treated as the \\nFig. 2  Framework of DCMA -CNN s \\nFig. 3  A  typical structure of CNN.  \\n\\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 4 \\nrepresentation of the input. The typical structure of a CNN is \\nshown in Fig.3. \\nIn our model, the CNN module has a fo ur-layer network \\nstructure . Convolutional layers and pooling layers are stack ed \\nup alternately to forms the backbone of the CNN module. The \\noutput feature maps in the final layer serve as the high-level \\nsemantic concept of the input, and they are used for the \\nprocessing as described in the following subsections. \\nB. CNN with Expressional Transformation-invariant Pooling \\nIn the stage of feature extraction, variations such as \\nillumination or image rotation can degrade the representative \\ncapacity of the extracted features. One approach commonly \\nused to address this problem is dataset augmentation. However, \\nthis approach has some side-effects. Models trained with data \\naugmentation still need to learn feature representations \\nseparately for different variations. Extra transformation of data \\ncan mislead models to learn some useless information from \\nnoises samples or wrong labels [9]. \\nIn order to reduce the negative impact of image variations \\nand to overcome the disadvantages of data augmentation, we \\nintroduce a novel feature-fusing strategy, namely, Expressional \\nTransformation-invariant pooling (ETI-pooling), to modify the \\ntypical CNN structure. It follows the concept of TI-pooling [9], \\nwhich was proposed to handle one specific variation in images. \\nTI-pooling accumulates all responses of the extracted features \\nand uses their maximal value. Such processing allows for more \\nefficient data usage and can help discover “canonical ” \\ninstances, leading to the generation of variation-independent \\nand transformation-invariant features.  \\nUnlike TI-pooling, our ETI-pooling scheme was extended to \\nhandle different variations simultaneously. In our method, \\ndifferent variations can be regarded as different transformations \\nfor images in the same class. For example, for two images with \\nexpressions of anger but different illuminations, one image can \\nbe regarded as a transformation of the other. We can \\naccumulate the responses of all samples from the same class \\ninstead of treating them individually. Such processing not only \\nallows TI-pooling to learn from “canonical ” instances, but also \\nyields features that are robust to different variations. \\nThe main structure of ETI-pooling is illustrated in Fig.4. To \\nextract invariant features from expressional images, we train \\nthe network using samples from the transformation sets, which consist of a certain number of expressional images with the \\nsame class label. In the training stage, transformation sets are \\ntreated as the input of the networks. Each image of a \\ntransformation set will be fed into a channel, i.e., an \\nexpressional image corresponds to a channel of the CNN \\nmodule. In each channel, the input image will pass through a \\nCNN, whose interior structure is similar to that shown in Fig.3. \\nThe output of each channel comprises vectorized features that \\nare denoted via a concatenated feature vector (refer to Fig.3). In \\nthe training stage, weight-sharing is implemented among these \\nparallel channels, indicating that the model only requires the \\nsame amount of memory as one CNN. The output features of \\nevery individual channel will finally be fused to generate a \\nfeature vector. \\nThe concatenated feature vector of the k-th (𝑘 = 1,2, … , 𝑀)  \\nchannel is denoted as 𝒛𝑘=(𝑧1𝑘, 𝑧2𝑘, … , 𝑧 𝑡𝑘, … , 𝑧𝑁𝑘), where N is \\nthe dimension of the vector and 𝑧𝑡𝑘 ( 𝑡 = 1,2, … , 𝑁)  is the t-th \\nelement of the vector. The vector obtained after ETI-pooling \\ncan be denoted as 𝒛 = (𝑧 1, 𝑧2, … , 𝑧 𝑡, … , 𝑧 𝑁). The non-linear \\noperator conducts an element-wise fusion operation across all \\nM channels, formulated as follows: \\n𝑧𝑡=𝑚𝑎𝑥  (𝑧𝑡1, 𝑧𝑡2, … , 𝑧 𝑡𝑀)                    （3） \\nThis operation only responds to the maximal element in the \\nsame position among different feature vectors, which lowers \\nthe probability of propagating an odd variation-related feature \\nto the following parts. In the other words, ETI-pooling attempt \\nto improve the classification performance by highlighting the \\nmost discriminative feature elements that are beneficial to most \\ninstances. Such non-linear operations increase the competition \\namong different channels, inducing networks to learn \\nadaptively the expression-discriminative features and to \\nsuppress the back propagation of information related to \\nnuisance variations . Unlike the max-pooling strategy, ETI- \\npooling considers all instances in a transformation set. This \\nmakes it efficiently utilize all available and effective \\ninformation concealed in multiple expressional images. By \\ntraining the network using transformation sets, the network can \\nlearn invariant features across variations. This is beneficial to \\nclassification as described below.  \\nC. DCMA- CNNs for Classification \\nMany existing deep-based models for facial expressional \\nanalysis are built on single-branch CNN [36], [37], [42] , \\nwhich usually focus es on extracting features from a whole \\nimage but ignores detailed descriptions. However, some \\nstudies [21], [29], [31] validated the effectiveness of local \\nexpressional features in solving the FER problem. This means \\nthat both holistic and local features are important for \\nexpressional analysis. Some recent methods fuse features from \\ndifferent representative spaces [39], [32], [40] or aggregate \\ndifferent features [43], [44] that achieved satisfactory \\nperformance in different classification tasks. It is reasonable \\nthat fused or aggregated features contain more \\nclassification-effective information than a single type of \\nfeatures. Motivated by these works, we attempt ed to \\nincorporate both holistic and local information and aggregate \\nthem for improving the representation ability of our model. \\nAs shown in Fig. 2, the DCMA-CNNs consist of two \\nFig. 4  Modified CNN structure with ETI -pooling (a CNN module).  \\n\\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 5 \\nbranches. One extracts holistic features from input images, \\nwhile the other focuses on local feature extraction. The output \\nfeatures obtained from the two branches are aggregated and \\nused in expression recognition task.  \\nIn the branch of holistic feature extraction, images from a \\ntransformation set are treated as the input of the CNN module. \\nThe output of the branch is the holistic feature vector, which \\nrepresents a set of images with different transformations. In \\nthe other branch, original images are equally divided into \\nimage patches. Individual CNN modules are implemented to \\nextract the local features with respect to each patch. All the \\nsteps of local feature extraction are the same as those of the \\nholistic feature extraction except for some detailed \\nconfiguration of CNN modules. Although both branches are \\nbased on CNN, the difference in input data results in \\ncomplementary features with different semantic concepts. The \\nlocal branch focuses on extracting patch-specific semantic \\nconcepts, while the holistic branch concentrates on extracting \\nan abstract representation from the whole image. Aggregation \\nof these two branches not only increases the number of \\nextracted features, but also improves the representative ability \\nof our model. \\nIn this paper, we denote the holistic feature vector as 𝒗ℎ0, and \\nthe m-th local feature vector as 𝒗𝑙𝑚, where 𝑚 = 1,2, … , 𝐿 , L is \\nthe number of local feature vectors, which equals the number of \\nimage patches. The aggregated feature vector 𝒗𝑎 is obtained by \\nconcatenating holistic feature vector and all local feature \\nvectors, which can be formulated as follows: \\n𝒗𝑎= (𝒗ℎ0; 𝒗𝑙1; 𝒗𝑙2; … ; 𝒗𝑙𝑚; … ; 𝒗𝑙𝐿)                  (4) \\nThe aggregated feature vector will be fed into a \\nfully-connected layer for feature fusion. A classifier is followed \\nby the fully-connected layer, which maps the output of the \\nprevious layer to an expression class. In our method, we choose \\nsoftmax as the classifier, which can be formulated as shown \\nbelow: \\n𝑓(𝒛 𝒋) =  𝑒𝒛𝒋\\n∑𝑒𝒛𝒊𝑖                                      (5) \\nEach element of equation (5) corresponds to a unique \\nexpression class. The class with the maximal element value is \\nregarded the predicted class . Thus, the loss function of our \\nmodel can be formulated as follows: \\n𝐿 =1\\n2‖𝒚𝒑− 𝒚𝒈‖2=1\\n2∑ (𝑦𝑐𝑝− 𝑦𝑐𝑔)2𝐶\\n𝑐=1              (6) \\nwhere 𝒚𝒑 is the predicted label; 𝒚𝒈 is the ground truth label; \\n𝑦𝑐𝑝and 𝑦𝑐𝑔 are the c-th element of 𝒚𝒑 and 𝒚𝒈 respectively. C is \\nthe total class number of expressions. All related parameters of \\nthe method can be updated through back-propagation using \\nthe stochastic gradient descent (SGD). \\nD. Learning Salient Patches for Expression Analysis \\nIntuitively, expressions can be regarded as regional changes \\nin the appearance where the changes are triggered by some \\nfacial muscles. This means that analysis of expressions can be \\nassociated with limited facial regions. Some previous works on \\nFER are based on the study of local regions [ 29], [31], [32]. In \\nthis subsection , we define the salient patches as the image patches that make large contribution to the expression \\nrecognition task. Because local regions can be represented by \\nthe extracted local features, we can detect the se salient patches \\nby analyzing the features extracted from them. \\n In classification tasks, features with large norms are much \\nmore discriminative than random features [ 43]. Therefore, we \\ncan try to detect salient patches according to the L2-norm value \\nof the extracted local feature vectors. We assume that the \\ndiscriminative power of a local feature vector is proportional to \\nthe magnitude of its L2-norm.  \\nBased  on this assumption, we can bypass some patches with \\na small norm but preserve patches with a large norm. The \\nprocedure of learning salient patches is implemented in two \\nsteps in the training stage: For the first step, all feature vectors \\nare involved in the classification task. As the training loss \\nconverges, the second step is initiated to learn salient the \\npatches and to continue fine- tuning  the model. The general \\nprocedure of selecting salient patches is summarized in \\nAlgorithm 1. We record the norm of each local feature vector in \\neach iteration and compute the Averaged Cumulative \\nSummation (ACS) of the norm of each vector every T iterative \\nepochs. According to our assumption, vectors with larger ACS \\nplay a more important role in recognition. The vector with the \\nsmallest ACS contributes the least; in other words, we can \\nbypass this patch and exclude it in the next iteration when \\ncontinu ing fine-tuning. By successively removing irrelevant \\npatches from the input, we can optimize the model and increase \\nthe representation efficiency. \\nIV. EXPERIMENTAL EVALUATION  \\nIn this section, a number of experiments are carried out on \\ntwo publicly available facial expression datasets: the Extended \\nCohn-Kanade (CK+) dataset [45] and the Japanese Female \\nFacial Expression (JAFFE) database [46 ]. Experiments are in \\nsupport of the following objectives: \\n(1) Evaluate the performance of DCMA-CNNs on FER task \\nby the recognition accuracy and compare it with some \\ncompeting conventional approaches as well as \\ndeep-based methods.  \\n(2) Investigate the role of aggregated features and ETI- \\npooling in solving FER problem. \\nAlgorithm 1  Leaning Salient Patc hes for Expression Analysis  \\nInput : Localized feature vectors set Φ={v1,v2,…,vm,…,vN} \\ncorresponds to N patches, Salient patches number P, bypass period T  \\n1: Initialize averaged cumulative  norm summation set \\nΩ={𝒔1,𝒔2, … ,𝒔𝑚, … ,𝒔𝑁} for N patches, 𝒔𝑚= 0,𝑚= 1,2, … , 𝑁; \\n2: Begin training until the training loss converges; \\n3: While number of elements in 𝛷 has not met P , do \\n   for 𝑡= 1:𝑇 do \\n      Fine-tune the model with localized feature vectors in Φ; \\n      for each  element in  Ω do \\n           𝒔𝑚←𝒔𝑚+1\\n𝑇‖𝒗𝑚‖2; \\n      end for each \\n   end for \\n   𝑘=𝑎𝑟𝑔𝑚𝑖𝑛 𝑚𝒔𝑚; \\n   Φ←Φ−𝒗𝑘; \\n   Ω←Ω−𝒔𝑘; \\nend while \\n4: Output : Localized feature vectors set Φ \\n \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 6 \\nFig. 5  Example of the six prototypic expressions.  \\n(a) the CK+ dataset   \\n(b) the JAFFE database \\n(left to right: anger, disgust, fear, happiness, sadness, surprise).  \\n(a) \\n(b) (3) Investigate the effect of salient patch learning algorithm \\nand the impact on recognition accuracy after bypassing \\nsome irrelevant patches.  \\nA. Experimental Data \\nExperiments are conducted on CK+ and JAFFE datasets. For \\neach dataset, six classes of basic expressions, i.e., anger, disgust, \\nfear, happiness, sadness and surprise, are chosen as the target of \\nour classification task.  \\nThe CK+ dataset contains 327 expression-labeled image \\nsequences from 123 subjects. All sequences are from the \\nneutral face to the peak expression. Some samples of the CK+ \\ndataset are shown in Fig.5 (a). 309 sequences with one of the \\nsix prototypical expressions are selected to conduct \\nexperiments. For each chosen sequence, the last 3 frames with \\npeak expression are collected to form the training and testing \\nsets, i.e., 927 images are involved in our experiments. \\nThe JAFFE database contains 213 images of 7 facial \\nexpressions posed by 10 Japanese females. 183 images with six \\nbasic expressions are chosen for the experiment. Fig.5 (b) \\nshows some samples of the JAFFE database. As for JAFFE \\ndataset, data augmentation is applied for improving recognition \\nperformance. Images are horizontally flipped, which obtains \\nthe corresponding mirror image. Each image is rotated by the \\nangles of 5º  clockwise and counterclockwise. Furthermore, \\nGaussian noise with a zero mean and 0.01 variance is added to \\nthe original image. Thus, the sample set of experiments on \\nJAFFE database is largely extended, which contains 915 facial \\nexpression images. \\nTo simplify the recognition task, human faces in all selected \\nimages are detected by the Viola-Jones faces detector [47 ]. \\nFacial regions are cropped from images and resized to the scale of 60×60. These resized facial expression images are then \\nequally divided into partially overlapped patches, which forms \\nthe sample set of the branch of local feature extraction.  \\nB. Experimental Settings \\n In our experiment, two individual branches are set with \\ndifferent CNN structural configuration. Table I shows the \\ndetailed configuration of the CNN module in each branch. The \\nstride of the convolutional layer and the max-pooling layer is \\nset as 1. The convolutional operation is conducted without \\npadding. When applying the ETI-pooling, we set 3 parallel \\nchannels in the model. \\nIn the training stage, the learning rate α decays in an \\nexponential form, which can be formulated as: \\n𝛼 =  𝛼 0× 0. 95⌊𝑖\\n10⌋                                  (7) \\nwhere 𝛼0= 0.1  is the initial value, i indicates the i-th iterative \\nepoch of the training stage and symbol ⌊𝑥⌋ refers to the largest \\ninteger that is smaller than 𝑥. In our experiment, images are \\ndivided into 3 × 3  patches with overlap. The overlap ratio \\nbetween two adjacent patches is 0.5.  \\nTo evaluate the performance of the proposed method, \\n10-fold cross-validation is applied to all the experiments (i.e. \\nimages are randomly divided into ten equal-sized subsets, nine \\nfor training and one for testing). To fairly compare with other \\nmethods, in CK+, all ten subsets are subject-independent. As \\nfor the JAFFE database, an original image and its augmented \\nimages can only be allocated either to training or testing sets. \\nThe final results are reported by averaging the recognition \\naccuracy of all ten folds experiments. \\nC. Expression Recognition Results \\n1) Experiments on the CK+ Dataset \\n Table II is the confusion matrix obtained by DCMA-CNNs \\nmodel, which shows the detailed classification result of all six \\nclasses of expressions. To evaluate the performance of the \\nDCMA-CNNs model, we compare the recognition accuracy of \\nour methods with several competitive approaches, including \\nconventional methods (LAP [ 48], MPSD [ 49] and MCSPL [ 29]) \\nand deep-based methods (DTAN [ 50], DTGN [50] and \\n3DCNN-DAP [51]). In addition, we reproduce the model of the \\nwork of Khorrami et al.  [56], which is a recently-proposed \\ndeep-based model and is denoted as Khor-net in the following. \\nTable III shows the result of these comparative studies.  \\nTABLE  I \\nNetwork Configuration \\n  Layer1  Layer2  Layer3  Layer4  Layer5  Layer6  \\nHolistic Branch  type Input  Convolutional  Max Pooling  Convolutional  Max Pooling  Fully -connected  \\nsize - 5×5×6 2×2 5×5×18 2×2 \\n8424 ×6 Localized  \\nBranch  type Input  Convolutional  Max Pooling  Convolutional  Max Pooling  \\nsize - 3×3×6 2×2 3×3×18 2×2 \\n \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 7 \\nFrom Table II and Table III, we can observe the following. \\n(1) The proposed method achieves an averaged recognition \\naccuracy of 93.46% in the CK+ dataset. DCMA-CNNs \\nperforms the best when recognizing the expression of \\nsurprise. Some samples of fear and sadness are \\nmisclassified as anger. This is because expressions of \\nfear and anger have some similar actions in local facial \\nregions, such as fall of eyebrow and rise of the upper \\neyelid. In FACS, these two expressions have shared \\nAUs. The same phenomenon appears when dealing with \\nsadness and anger. This may be due to the reason that \\nonly 75 samples of fear and 84 samples of sadness are \\nused for training, which is far less than that of other four \\nexpressions.    \\n(2) The performance of DCMA -CNNs is far better than \\nthese  conventional methods. LAP and MCSPL extract \\nLBP features to represent expressions while MPSD \\nutilizes SIFT features. All these features are artificially \\npredefined and incapable to be adaptive to some \\ncomplex environments. Compar ed with these methods, \\nour deep- based model learns features adaptively, which \\nare more robust than hand- crafted features.  \\n(3) DCMA- CNNs outperforms the competitive deep \\nmodels. The deep -learning based methods such as \\nDTAN and DTGN utilize one -branch architecture to \\nextract desired features while our method extracts \\nfeatures with an additional branch, which leads to a \\nmore comprehensive representation of expressions. The \\nresult indicates that the branch of local feature extraction \\nreally benefits expression classification.  \\n(4) DCMA -CNNs outperforms Khor- net. This is because \\nKhor -net is based on the structure of typical CNN while \\nour model adopt ETI-pooling and feature aggregation schemes . Thus our DCMA -CNNs model can capture \\nmore detailed information and is robust to different \\nvariations. Note that the recognition accuracy of \\nKhor -net in Table III is different from its reported result \\nin [56]. Although we use the same dataset (CK+), \\nvalidation settings (10 -fold cross -validation) and \\nnetwork architecture as [56], images used in each fold of \\nexperiments can be different as training and testing \\nsamples are randomly selected from dataset. Such \\ndifference results in the fluctuation of the final \\nrecognition performance.  \\n2) Experiments on the JAFFE Database \\nDetails of classification result are shown in the confusion \\nmatrix in Table IV. In Table V, we compare the proposed \\nDCMA-CNNs model with some other existing works.  \\nFrom Table IV and Table V, some conclusions can be \\nsummarized as the following.  \\n(1) The proposed method achieves an averaged recognition \\naccuracy of 94.75% in the JAFFE database. The best \\naveraged recognition accuracy occurs when recognizing \\nthe expression of anger, which reaches the accuracy of \\n97.04%. The averaged recognition accuracy on fear is \\nrelatively lower than all others but still reach 88.89%. In \\nthe experiment, nearly 5% samples of fear expressions \\nare misclassified as surprise. A reasonable explanation is \\nthat the expression of fear and surprise act similarly in \\nseveral facial regions such as the inner brows and the \\nupper eyelids. We can also find that these two \\nexpressions are coded by some identical AUs in FACS.  \\n(2) The proposed DCMA -CNNs model outperforms all \\nother competitive methods. In Table V , some methods \\nusing hand -crafted features are involved in comparison \\nbut the recognition accuracy of them is far lower than \\nthe proposed model, which validates the effectiveness of \\nTable  II \\nConfusion Matrix of DCMA-CNNs on CK+ (%) \\n(An: anger, Di: disgust, Fe: fear, Ha: happiness, Sa: sadness, Su: surprise) \\n An Di Fe Ha Sa Su \\nAn 90.74  0 0 0 9.26 0 \\nDi 1.85 95.06  1.85 0 1.23 0 \\nFe 11.11  0 74.07  3.70 0 11.11  \\nHa 0 2.12 3.17 94.71  0 0 \\nSa 12.35  1.23 1.23 0 81.48  3.70 \\nSu 1.39 0 0.46 0.46 0 97.69  \\n \\nTable III  \\nPerformance Comparison with Existing Works on the CK+ Dataset (%) \\nMethod  Accuracy  \\nLAP [48]  88.26  \\nMPSD [49]  88.52  \\nKhor -Net [56]  91.25  \\nMCSPL [29]  91.53  \\nDTAN [50]  91.44  \\nDTGN [50]  92.35  \\n3DCNN -DAP [51]  92.40  \\nDCMA -CNNs (proposed)  93.46  \\n \\nTable IV  \\nConfusion Matrix of DCMA-CNNs on JAFFE (%) \\n An Di Fe Ha Sa Su \\nAn 97.04  2.22 0 0 0 0.74 \\nDi 3.97 93.65  0.79 0 0.79 0.79 \\nFe 2.78 1.39 88.89  0.69 1.39 4.86 \\nHa 0.69 0 0.69 95.14  0.69 2.78 \\nSa 2.22 3.70 2.22 0 90.37  1.48 \\nSu 0 0.74 1.48 2.22 0 95.56  \\n \\nTable V  \\nPerformance Comparison with Existing Works on the JAFFE Database \\n(%) \\nMethod  Accuracy  \\nMcFIS [52]  87.60 \\nLPTP [53]  90.20 \\nSRC+LBP [54]  90.30 \\nSRC+Gabor [54]  91.21  \\nSRC+LPQ [54]  91.67  \\nSFPL [31]  91.80 \\nLSDP [55]  92.30 \\nDCMA -CNNs (proposed)  94.75  \\n \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 8 \\ndeep -based features.  \\n(3) The proposed method is robust to small variations. \\nImages with variations such as rotations and noises are \\ninvolved in the experiment on JAFFE but DCMA -CNNs \\ncan still correctly classify most expressions, which \\ndemonstrates the robustness of the proposed method.  \\n3) Experiments across Databases \\nTo evaluate the generalization capability of our model, we \\nconduct an experiment on DCMA-CNNs across different \\ndatabases. To be specific, we train DCMA-CNNs with the data \\nfrom the JAFFE databases and validate it on the CK+ dataset.  \\nNote that there are few works had conducted cross-databases \\ntesting on facial expression recognition. It is hard to make a full \\nand fair comparison with other models. Here we compared our \\nmodel with the work of da Silva et al.  [57]. To make a fair \\ncomparison, our experimental settings and preprocessing \\nfollow the work of da Silva et al.  [57]. The results are shown in \\nTable VI . From the table, we can observe that DCMA-CNNs is \\nstill comparable to the work of [57], which demonstrate the \\neffectiveness of our model.  \\nD. Comparison Studies on DCMA-CNNs Properties \\nIn our method, we aggregat e two types of features and \\nintroduce the ETI-pooling into the model. To assess these two \\nproperties, we conduct some experiments on the CK+ dataset to \\nevaluate their effect on recognition. \\n1) The Effects of Feature Aggregation \\nWe construct another two models to take on the evaluation \\ntask. The model only utilizes holistic features to make \\nclassification is denoted as HFCNN. The model that recognizes \\nexpressions only with local features is denoted as LFCNN. The \\nrecognition performances of these two models are listed in \\nTable VII. \\nFrom Table VII, we can observe that the recognition \\naccuracy of DCMA-CNNs is much higher than HFCNN and LFCNN, which means that feature aggregation really improve \\nexpression recognition. This is reasonable as holistic features or \\nlocal features only focus on representing expressional \\ninformation with a specific scale. The improvement on \\nrecognition accuracy by aggregation indicates that these two \\nkinds of features are complementary to each other. \\n2) The Effects of ETI-pooling \\nIn this experiment, t he model without ETI-pooling is denoted \\nas DCMA-CNNs-WTE. In other words, in this model, the CNN \\nmodule is replaced by a typical CNN structure as shown in \\nFig.3. We compare the performance of DCMA-CNNs-WTE \\nwith the proposed model. The recognition result is shown in \\nTable VIII. \\nFrom Table VIII, we can see that the proposed model \\nperforms better than DCMA-CNNs-WTE. This can be owed to \\nthe parallel structure and the non-linear operator of \\nETI-pooling. These two operations help distinguish \\ndiscriminative elements in the feature vector and suppress \\nnuisance variations collaboratively. The improvement in \\nrecognition accuracy demonstrates the effectiveness of \\nETI-pooling.  \\nE. Experiments on Learning Active Regions \\nFollowing the assumptions and the learning algorithm we \\nsuggested in the Section III.D, we attempt to make a selection \\nof expressional patches in our experiments. The period of \\nselecti on is 30 epochs in the experiments on both the CK+ \\ndataset and JAFFE database, i.e., we bypass an image patch \\nevery 30 iterative epochs during the procedure of fine-tuning. \\nExperiments follow the principle of 10-fold cross-validation \\nand we finally retain 5 patches in the end of each fold of \\nexperiment. We count the times that a patch is retained  after all \\nten folds of experiments. The more times a patch is retained, the \\nmore importantly it acts in the expressional analysis. Note that \\nan expressional image is divided by the scale of 3× 3 with 50% \\noverlap area between two adjacent patches, an image can be \\nseen as being equivalently divided into 4× 4 disjoint regions in \\nthe visualization task. Each patch is composed of four regions. \\nRegions in the retained patch can be regarded as active regions \\nwhereas regions in bypassed patches are regarded as less-active \\nregions. \\nWe visualize the salient patches and their corresponding \\nactive regions in Fig.7. The intensity of red color of a region is \\nproportional to the times it is retained in the 10-fold \\ncross-validation experiments. We can observe that in both the \\nCK+ dataset and the JAFFE database, the “most active ” regions \\nare located around some primary facial organs such as eyes, \\nnose and mouse. Some marginal regions are with low intensity, \\nwhich means that these “less-active ” regions and their \\ncorresponding patches contribute less to expression recognition. \\nDistribution of active regions roughly accord with the intuition \\nof places where actions of an expression should take place in \\nface.  \\nThe recognition performance after bypassing less-active \\npatches is listed in Table IX. We denote DCMA-CNNs-5p as \\nthe model trained with five retained patches. We can observe \\nthat the averaged recognition accuracy of DCMA-CNNs-5p in \\nthe CK+ dataset is slightly lower than the accuracy of \\nDCMA-CNNs. This is reasonable as DCMA-CNNs can make \\nuse of more local information for making classification. \\nTable VII  \\nRecognition Accuracy on CK+ with Different Types of Features (%) \\n Accuracy  \\nDCMA -CNNs  93.46  \\nHFCNN  90.67  \\nLFCNN  88.67  \\n \\nTable VIII  \\nEffect of ETI-pooling on CK+ Dataset (%) \\n Accuracy  \\nDCMA-CNN s 93.46  \\nDCMA-CNN s-WTE  92.67  \\n \\nTable IX  \\nRecognition Accuracy of DC MA-CNNs with and without Patches \\nExclusion in CK+ and JAFFE (%) \\n CK+  JAFFE  \\nDCMA -CNNs  93.46  94.75  \\nDCMA -CNNs -5p 92.10  94.75  \\n \\nTable VI  \\nGeneralization performance of DCMA-CNNs across databases (trained on \\nJAFFE, validated on CK+) (%) \\n Accuracy  \\nda Silva et al.  [57] 48.20  \\nDCMA -CNNs  46.28  \\n \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 9 \\nHowever, these two models achieve the same averaged \\nrecognition accuracy in the JAFFE database, which implies that \\nthe retained five patches have contained most discriminative \\ninformation of expressions. To sum up, the proposed model and \\nthe salient patch learning algorithm help distinguish sensitive \\npatches that are related to the FER problem. \\nWe conduct another experiment to verify the assumption \\nmentioned in Section III.D, i.e., patches with larger ACS plays \\na more important role in FER. To be specific, w e prune the \\nlocal branch of DCMA-CNNs. Each pruned DCMA-CNNs can \\nextract features from only one specific patch. Th us we can \\ntotally construct nine different pruned models corresponding to \\neach different patch . The recognition performance of each \\nnetwork is shown in Fig.8, where we sort all patches by the \\nmagnitude of ACS ( in ascending order). From the figure, we \\ncan observe that models with the patch that have larger ACS \\ntend to have a better recognition performance. In both the CK+ \\nand JAFFE dataset, the model with the largest patch ACS \\nachieves the highest recognition accuracy. This verifies our \\nassumption that patches with larger ACS can be more important \\nin FER task. \\nV. CONCLUSIONS  \\nIn this paper, a novel method, named DCMA-CNNs, is \\nproposed for facial expression recognition. The architecture of \\nour proposed model consists of two individual CNN branches. \\nOne of the branches conducts holistic features extraction on the \\nwhole expressional images while the other extracts local \\nfeatures from segmented expressional image patches.  These \\ntwo types of hierarchical features depict an expressional image \\nin two different scales and being complementary to each other. \\nExpressional images represented by these two types of features \\ncan be more discriminative than many existing works. We \\naggregate the holistic and local features to yield fused features \\nand classification is conducted based on the fused features. \\nFurthermore, we modify the typical CNN structure with the \\nproposed ETI-pooling strategy, which reduces the impact of \\nnuisance variations in classification tasks. W e additionally \\nproposed a method to learn salient expressional image patches \\nbased on the L2-norm of local feature vectors and visualize the \\nactive regions relevant to expression changes. The selected \\nsalient patches are regarded as important parts for facia l expression recognition. Extensive experiments on two public ly \\navailable expression datasets (the CK+ dataset and the JAFFE \\ndatabase) demonstrate the effectiveness of our proposed \\nmethod on FER task. The classification result of our \\nDCMA-CNNs model on these two datasets outperforms many \\nother competitive works, which include both conventional and \\ndeep based methods.  \\nACKNOWLEDGEMENT  \\nThis work was supported in part by the National Natural \\nScience Foundation of China (61673402, 61273270, \\n60802069), the Natural Science Foundation of Guangdong \\nProvince (2017A030311029, 2016B010123005, \\n2017B090909005), the Science and Technology Program of \\nGuangzhou of China (201704020180, 201604020024), and the \\nFundamental Research Funds for the Central Universities of \\nChina. \\nREFERENCES  \\n[1] G. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA: \\nAldine, 2007. \\n[2] A. Ryan, J. F. Cohn, S. Lucey, J. Saragih , P. Lucey , F. De la Torre, and A. \\nRossi, “Automated facial expression recognition system,” in Proc. 43rd \\nAnnu. Int. Carnahan Conf. Security Technol., Zurich, Switzerland, 2009, \\npp. 172 –177. \\n[3] A. Vinciarelli, M. Pantic, and H. Bourlard, “Social signal processing: \\nSurvey of an emerging domain,” Image Vis. Comput., vol. 31, no. 1, pp. \\n1743 –1759, 2009. \\n[4] Q. Wang, K. Jia, and P. Liu, “Design and Implementation of Remote \\nFacial Expression Recognition Surveillance System Based on PCA and \\nKNN Algorithms ”. In Intelligent Information Hiding and Multimedia \\nFig. 7  Visualization  of significant patches and the corresponding active \\nregions following the learning strategy of salient patches. The intensity of \\nred color is proportional to the total times a region is retained in \\nexperiments.  \\n(a) Example on the CK+ dataset.  \\n(b) Example on the JAFFE database.  \\n(a) \\n (b) \\nFig. 8  Recognition Performance with respect to different patches.  \\n(a) Performance on the CK+ dataset. \\n(b) Performance on the JAFFE database.  \\n(a) \\n(b) \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 10 \\nSignal Processing (IIH-MSP), 2015 International Conference on IEEE , \\nAdelaide, Australia , 2015, pp. 314-317. \\n[5] E. Vural et al ., “Automated drowsiness detection for improved \\ndriversafety comprehensive databases for facial expression analysis,” in \\nProc. Int. Conf. Autom. Technol., vol. 1. Istanbul, Turkey, 2008, pp. 96 –\\n105. \\n[6] P. Ekman and W. Friesen, “The Facial Action Coding System: A \\nTechniquefor the Measurement of Facial Movement ”, Santa Clara, CA, \\nUSA: Consulting Psychologists Press, 1978. \\n[7] C. E. Izard, The Face of Emotion, vol. 1. New York, NY, USA: \\nAppleton-Century-Crofts, 1971. \\n[8] P. Ekman, W. V. F riesen, and J. C. Hager, “FACS Manual,” Salt  Lake \\nCity, UT, USA: A Human Face, May 2002. \\n[9] D. Laptev, N. Savinov, J. M. Buhmann, M. Pollefeys, \"Ti-pooling: \\nTransformation-invariant pooling for feature learning in convolutional \\nneural networks\", Proc. Int. Conf. Comput. Vis. Pattern Recognit., pp. \\n289-297, 2016. \\n[10] P. Ekman and W. Friesen, “The Facial Action Coding System: A \\nTechnique for the Measurement of Facial Movement ”. Santa Clara, CA, \\nUSA:   Consulting Psychologists Press, 1978. \\n[11] Y. Tian, T. Kanade, and J. F. Cohn, “Recognizing upper face action units  \\nfor facial expression analysis,” in Proc. Int. Conf. Comput. Vis. Pattern  \\nRecognit., 2000, pp. 294 –301. \\n[12] Y. Tian, T. Kanade, and J. F. Cohn, “Recognizing action units for facial  \\nexpression analysis,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 23,   no. \\n2, pp. 97 –115, Feb. 2001. \\n[13] Y. Tong, W. Liao, and Q. Ji, “Facial action unit recognition by exploiting  \\ntheir dynamic and semantic relationships,” IEEE Trans. Pattern Anal.   \\nMach. Intell., vol. 29, no. 10, pp. 1683 –1699, Oct. 2007. \\n[14] M. S. Bartlett et al ., “Fully automatic facial action recognition in \\nspontaneous behavior,” in Proc. 7th Int. Conf. Autom. Face Gesture \\nRecognit., Southampton, U.K., 2006, pp. 223 –230. \\n[15] J. F. Cohn, L. Reed, Z. Ambadar, J. Xiao, and T. Moriyama, “Automatic  \\nanalysis and recognition of brow actions and head motion in spontaneous \\nfacial behavior,” in Proc. IEEE Int. Conf. Syst., Man, Cybern., 2004,   pp. \\n610– 616. \\n[16] G. Sandbach, S. Zafeiriou, and M. Pantic, “Binary p attern analysis for 3D \\nfacial action unit detection,” in Proc. Brit. Mach. Vis. Conf., Surrey,   U.K., \\n2012, pp. 119.1 –119.12. \\n[17] G. Sandbach, S. Zafeiriou, M. Pantic, and D. Rueckert, “Recognition of  \\n3D facial expression dynamics,” Image Vis. Comput., vol. 30 , no. 10, pp. \\n762– 773, Oct. 2012. \\n[18] Zhao, Kaili, Wen- Sheng Chu, and Honggang Zhang. “Deep Region  and \\nMulti- Label Learning for Facial Action Unit Detection.” In  Proceedings \\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. \\n3391-3399. 2016. \\n[19] H. Kobayashi, F. Hara, \"Facial interaction between animated 3d face \\nrobot and human beings\", Proceedings of the International Conference on \\nSystems Man and Cybernetics, pp. 3732-3737, 1997 \\n[20] G. Gao, K. Jia, B. Jiang , “An Automatic Geometric Features Extrac ting \\nApproach for Facial Expression Recognition Based on Corner Detection” , \\nIntelligent Information Hiding and Multimedia Signal Processing \\n(IIH-MSP), 2015 International Conference on. IEEE, pp. 302-305, 2015. \\n[21] A. Majumder, L. Behera, and V. K. Subramanian, \"Emotion recognition \\nfrom geometric facial features using self-organizing map\", Pattern \\nRecognition, vol. 47, no. 3, pp. 1282-1293, 2014. \\n[22] Mao, Q., Rao, Q., Yu, Y., & Dong, M. \"Hierarchical Bayesian Theme \\nModels for Multipose Facial Expression Recognition.\" IEEE \\nTransactions on Multimedia vol. 19, no.4, pp. 861-873 , 2017.  \\n[23] M. Huang, Z. Wang, Z. Ying, \"A new method for facial expression \\nrecognition based on sparse representation plus LBP\", Proc. IEEE Int. \\nCongress Image Signal Process., pp. 1750-1754, 2010. \\n[24] T. Jabid, M. Kabir, and O. Chae, “Robust facial expression recognition  \\nbased on local directional pattern,” ETRI J., vol. 32, pp. 784– 794, 2010. \\n[25] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. \\nMovellan, “Recognizing facial expression: M achine learning and \\napplication to spontaneous behavior,” in Proc. IEEE Comput. Soc. Conf. \\nComput. Vis. Pattern Recog., 2005, pp. 568 –573. \\n[26] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, “Emotion recognition  \\nusing PHOG and LPQ features,” in Proc. IEEE Int.  Conf. Autom. Face \\nGesture Recog. Workshops, 2011, pp. 878 –883. \\n[27] A. Tawari, M. M. Trivedi, \"Face expression recognition by cross modal \\ndata association\", IEEE Transactions on Multimedia, vol. 15, no. 7, pp. \\n1543-1552, [28] Zhang, T., Zheng, W., Cui, Z., Zong, Y., Yan, J., & Yan, K. “A Deep \\nNeural Network-Driven Feature Learning Method for Multi-view Facial \\nExpression Recognition ”. IEEE Transactions on Multimedia, vol. 18, \\nno.12, pp. 2528-2536, 2016. \\n[29] L. Zhong, Q. Liu, P. Yang, J. Huang, D. N. Metaxas, \"Learning multiscale \\nactive facial patches for expression analysis\", IEEE Trans. Cybern., vol. \\n45, no. 8, pp. 1499-1510, Aug. 2015. \\n[30] S. L. Happy, Aurobinda Routray, \"Robust facial expression classification \\nusing shape and appearance features\", Advances in Pattern Recognition \\n(ICAPR) 2015 Eighth International Conference on, pp. 1-5, 2015. \\n[31] S. Happy, A. Routray, \"Automatic facial expression recognition using \\nfeatures of salient facial patches\", IEEE Trans. Affect. Comput., vol. 6, no. \\n1, pp. 1-12, Jan –Mar. 2015. \\n[32] A. Majumde r, L. Behera, and V. K. Subramanian, “Automatic  facial \\nexpression recognition system using deep network-based data fusion,” \\nIEEE Trans. Cybern., 99 (2016), pp. 1- 12. \\n[33] B. Girshick, J. Donahue, T. Darrell, J. Malik, \"Rich feature hierarchies for \\naccurate object detection and semantic segmentation\", Proc. IEEE Conf. \\nComput. Vis. Pattern Recognit. (CVPR), pp. 580-587, Jun. 2014. \\n[34] K. He, X. Zhang, S. Ren, J. Sun, \"Deep residual learning for image \\nrecognition\", Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), \\n2016.  \\n[35] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. \\nOzair et al ., Generative adversarial nets. In Advances in Neural \\nInformation Processing Systems, pp. 2672-2680, 2014. \\n[36] J. Li, E. Lam, \"Facial expression recognition using deep neural networks\", \\nIEEE International Conference on Imaging Systems and Techniques (IST) \\n2015, pp. 1-6, Sept 2015. \\n[37] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper in  facial \\nexpression recognition using deep neural networks,” in 2016  IEEE \\nWinter Conference on Applications of Computer Vision (WACV), pp. 1 –\\n10, IEEE, 2016. \\n[38] P. Liu, S. Han, Z. Meng, Y. Tong, \"Facial expression recognition via a \\nboosted deep belief network\", Computer Vision and Pattern Recognition \\n(CVPR) 2014 IEEE Conference on, pp. 1805-1812, 2014. \\n[39] D. Hamester, P. Barros, S. Wermter, \"Face expression recognition with a \\n2-channel Convolutional Neural Network\", International Joint \\nConference on Neural Networks (IJCNN), pp. 1-8, 2015.  \\n[40] H. Jung, S. Lee, J. Yim, S. Park, J. Kim, \"Joint fine-tuning in deep neural \\nnetworks for facial expression recognition\", Proc. Int. Conf. Comput. Vis., \\npp. 2982-2991, Dec. 2015. \\n[41] D. Ciresan, U. Meier, and J. Schmidhuber. ‘Multi -column deep neural \\nnetworks for image classification’, 2012 IEEE Conference on Computer \\nVision and Pattern Recognition (CVPR). Jun. 2012. pp. 3642-3649. \\n[42] Xu, M., Cheng, W., Zhao, Q., Ma, L., Xu, F.: Facial expression \\nrecognition based on transfer learning from deep convolutional networks. \\nIn: 2015 11th International Conference on Natural Computation (ICNC), \\npp. 702 –708. IEEE (2015). \\n[43] A. Babenko, V. Lempitsky, \"Aggregating deep convolutional features for \\nimage retrieval\", Proc. Int. Conf. Comput. Vis., pp. 1269- 1277, 2015. \\n[44] X. Lu, Z. Lin, X. Shen, R. Mech, J. Z. Wang, \"Deep multi-patch \\naggregation network for image style aesthetics and quality estimation\", \\nICCV, 2015. \\n[45] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar, “The  \\nextended Cohn-Kanade dataset (CKþ ): A complete dataset for action unit \\nand emotion- specified expression,” in Proc. IEEE Int. Conf. Comput. Vis. \\nPattern Recog., 2010, pp. 94 –101. \\n[46] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial \\nexpressions with gabor wavelets,” in Proc. IEEE Int. Conf. Autom. Face \\nGesture Recog., 1998, pp. 200 –205. \\n[47] P. Viola, M. Jones, \"Robust Real-Time Face Detection\", Int\\'l J. Computer \\nVision, vol. 57, no. 2, pp. 137-154, May 2004. \\n[48] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas, \\n“Learning active facial patches for expression analysis,” in Proc.  IEEE \\nConf. Comput. Vis. Pattern Recog., 2012, pp. 2562 –2569 \\n[49] S. Taheri, Q. Qiang, R. Chellappa, \"Structure-preserving sparse \\ndecomposition for facial expression analysis\", IEEE transactions on \\nimage processing: a publication of the IEEE Signal Processing Society, \\nvol. 23, no. 8, pp. 3590, 2014. \\n[50] H. Jung, S. Lee, J. Yim, S. Park, J. Kim, \"Joint fine-tuning in deep neural \\nnetworks for facial expression recognition\", Proc. Int. Conf. Comput. Vis., \\npp. 2982- 2991, Dec. 2015. \\n[51] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning \\ndeformable facial action parts model for dynamic expression analysis.  In \\nACCV, 2014, pages 1749 –1756. IEEE, 2014. 2, 5, 7 \\n1520-9210 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2018.2844085, IEEE\\nTransactions on Multimedia\\n> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) < \\n 11 \\n[52] K. Subramanian, S. Suresh, and R. Venkatesh Babu, \"Meta-Cognitive \\nNeuro-Fuzzy Inference System for human emotion recognition\", in The \\n20I2 International Joint Conference on Neural Networks (IJCNN), Jun. \\n20 12, pp. 1- 7. \\n[53] J. A. R. Castillo A. R. Rivera and O. Chae, “Recognition of face \\nexpressions using local prin cipal texture pattern,” in Image Processing \\n(ICIP), 2012 19th IEEE International Conference on, september 2012. \\n[54] S. H. Lee, K. Plataniotis, Y. M. Ro, \"Intra-class variation reduction using \\ntraining expression images for sparse representation based facial \\nexpression recognition\", IEEE Transactions on Affective Computing, pp. \\n1, 2014. \\n[55] J. A. R. Castillo, A. R. Rivera, and O. Chae, “Facial expression \\nrecognition based on local sign directional pattern,” in Image Processing \\n(ICIP), 2012 19th IEEE International Conference on, september 2012. \\n[56] Khorrami, Pooya, Thomas Paine, and Thomas Huang. \"Do deep neural \\nnetworks learn facial action units when doing expression recognition?.\" \\nProceedings of the IEEE International Conference on Computer Vision \\nWorkshops. 2015. \\n[57] da Si lva, FlĂĄvio Altinier Maximiano, and Helio Pedrini. \"Effects of \\ncultural characteristics on building an emotion classifier through facial \\nexpression analysis.\" Journal of Electronic Imaging 24.2 (2015): \\n023015-023015. \\n \\n \\n',\n",
       " 'IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 233\\nLetters __________________________________________________________________________________________\\nFacialExpressionRecognitionUsingKernelCanonical\\nCorrelationAnalysis(KCCA)\\nWenming Zheng, Xiaoyan Zhou, Cairong Zou , Member, IEEE , and\\nLiZhao\\nAbstract—Inthiscorrespondence,weaddressthefacialexpressionrecog-\\nnitionproblemusingkernelcanonicalcorrelationanalysis(KCCA).Fol-\\nlowingthemethodproposedbyLyons et al.[7]andZhang et al.[8],we\\nmanuallylocate34landmarkpointsfromeachfacialimageandthencon-vertthesegeometricpointsintoalabeledgraph(LG)[7]vectorusingthe\\nGaborwavelettransformationmethodtorepresentthefacialfeatures.On\\ntheotherhand,foreachtrainingfacialimage,thesemanticratingsde-\\nscribingthebasicexpressionsarecombinedintoasix-dimensionalsemantic\\nexpressionvector.LearningthecorrelationbetweentheLGvectorandthe\\nsemanticexpressionvectorisperformedbyKCCA.Accordingtothiscor-\\nrelation,weestimatetheassociatedsemanticexpressionvectorofagiven\\ntestimageandthenperformtheexpressionclassiﬁcationaccordingtothis\\nestimatedsemanticexpressionvector.Moreover,wealsoproposeanim-\\nprovedKCCAalgorithmtotacklethesingularityproblemoftheGram\\nmatrix.TheexperimentalresultsontheJapanesefemalefacialexpression\\ndatabaseandtheEkman’s“PicturesofFacialAffect”databaseillustrate\\ntheeffectivenessoftheproposedmethod.\\nIndex Terms—Facialexpressionrecognition(FER),generalizeddiscrimi-\\nnantanalysis(GDA),kernelcanonicalcorrelationanalysis(KCCA),kernel\\nmethod.\\nI. INTRODUCTION\\nAutomatic facial expression recognition (FER) could be traced back\\nto the preliminary work of Suwa et al. [15] in 1978 and gained much\\npopularity starting with the pioneering work of Mase and Pentland [16]\\nin the 1990s. More recently, facial expression analysis has become a\\nvery hot research topic in computer vision and pattern recognition, and\\nvarious approaches have been proposed to this goal. For literature sur-veys, see [12]–[14]. The contribution in this correspondence is two\\nfold: 1) we present an effective facial expression analysis and recog-\\nnition method based on kernel canonical correlation analysis (KCCA),\\nand 2) we propose an improved algorithm for KCCA to tackle the sin-\\ngularity problem of the Gram matrix.\\nFollowing the work done by Lyons et al. [6], [7] and Zhang et al.\\n[8], and being motivated by the kernel-based machine learning algo-\\nrithms[18],weﬁrstmanuallylocate34landmarkpointsfromeachfacial\\nimage and then convert these geometric points into a labeled graph (LG)\\n[7] vector using the Gabor wavelet transformation method [20], [21] to\\nrepresent the facial features. On the other hand, a semantic expression\\nManuscript received March 16, 2004; revised November 6, 2004. This work\\nwas supported in part by the Jiangsu Natural Science Foundations under Grants\\nBK2005407 and BK2005122 and in part by the Natural Science Foundations\\nunder Grant 60503023.\\nW. Zheng is with the Research Center for Learning Science, Southeast\\nUniversity, Nanjing, Jiangsu 210096, China (e-mail: wenming_zheng@\\nseu.edu.cn).\\nX. Zhou is with the Department of Electrics Engineering, Nanjing Univer-\\nsity of Information Science and Technology, Nanjing, Jiangsu 210044, China(e-mail: xiaoyan_zhou@nuist.edu.cn).\\nC. Zou and L. Zhao are with the Engieneering Research Center of Information\\nProcessing and Application, Southeast University, Nanjing, Jiangsu 210096,\\nChina (e-mail: cairong@seu.edu.cn; zhaoli@seu.edu.cn).\\nDigital Object Identiﬁer 10.1109/TNN.2005.860849vector consisting of the semantic ratings of each training facial image\\nis used as the semantic features. Learning the correlation between the\\nLG vector and the semantic expression vector is performed by KCCA.\\nAccording to this correlation, we estimate the associated semantic ex-\\npression vector of a given test image and then perform the expression\\nclassiﬁcation according to this estimated semantic expression vector.\\nCanonical correlation analysis (CCA) is a powerful method to corre-\\nlate the linear relation between two multidimensional variables. How-ever, if there is nonlinear correlation between the two variables, linear\\nCCA may not correctly correlate this relationship. KCCA is a nonlinear\\nextension of CCA via the “kernel trick” to overcome this drawback\\n[10], [11]. Although the idea of kernelizing CCA is not new, all the\\nprevious algorithms of KCCA [2], [3] tackle the singularity problem of\\nthe Gram matrix by simply adding a regularization to the Gram matrixsuch that the Gram matrix becomes invertible [2], [3]. In this correspon-\\ndence, we propose an improved KCCA algorithm based on eigenvalue\\ndecomposition approach rather than the regularization method to over-come this problem.\\nThe remainder of this correspondence is organized as follows: we\\nderive the KCCA formulations in Section II, and the facial expression\\nclassiﬁcation formulations based on KCCA in Section III. Experiments\\nare conducted in Section IV, and conclusion are given in Section V.\\nII. KCCA\\nGiven two centered random multivariables\\n/120 /50 /82/110\\nand /121 /50 /82/110\\n,\\nthe goal of CCA is to ﬁnd a pair of directions /33/33/33 /120and /33/33/33 /121such that\\nthe correlation /26 /40 /120 /59 /121 /41between the two projections /33/33/33/84\\n/120 /120and /33/33/33/84\\n/121 /121is\\nmaximized [1], where\\n/26 /40 /120 /59 /121 /59 /33/33/33 /120 /59/33 /33/33 /121 /41\\n/61/69\\n /33/33/33/84\\n/120 /120/121/84/33/33/33 /121\\n/69 /102 /33/33/33/84/120 /120/120/84 /33/33/33 /120 /103\\n /69\\n /33/33/33/84/121 /121/121/84 /33/33/33 /121\\n/61/33/33/33/84\\n/120 /69 /102 /120/121/84/103 /33/33/33 /121\\n/33/33/33/84/120 /69 /102 /120/120/84 /103 /33/33/33 /120\\n /33/33/33/84/121 /69 /102 /121/121/84 /103 /33/33/33 /121/58 (1)\\nSuppose that /102 /120 /105 /103 /105 /61/49 /59 /46/46/46 /59/78and /102 /121 /105 /103 /105 /61/49 /59 /46/46/46 /59/78are /78observations of /120\\nand /121respectively. Then the goal of CCA is equivalent to ﬁnding /33/33/33 /120\\nand /33/33/33 /121that maximize\\n/26 /40 /120 /59 /121 /59 /33/33/33 /120 /59/33 /33/33 /121 /41/61/33/33/33/84\\n/120 /88/89/84/33/33/33 /121\\n/33/33/33/84/120 /88/88/84/33/33/33 /120\\n /33/33/33/84/121 /89/89/84/33/33/33 /121(2)\\nwhere\\n/88 /61/91 /120 /49 /120 /50 /1/1/1 /120 /78 /93 /59 /89 /61/91 /121 /49 /121 /50 /1/1/1 /121 /78 /93 /58\\nLet /120be mapped into a Hilbert space /70through a nonlinear mapping /8\\n/8/58 /82/110\\n/33 /70 /59 /120 /33 /8/40 /120 /41 /58\\nAlso, we suppose that /8/40 /120 /41is centered in /70. For a method to center\\n/8/40 /120 /41, see [11]. From (2), we obtain that the correlation function in /70\\ncan be formulated as\\n/26 /40/8/40 /120 /41 /59 /121 /59 /33/33/33 /8/40 /120 /41 /59/33 /33/33 /121 /41\\n/61/33/33/33/84\\n/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121\\n/33/33/33/84\\n/8/40 /120 /41/8/40 /88 /41/40/8/40 /88 /41/41/84 /33/33/33 /8/40 /120 /41\\n /33/33/33/84/121 /89/89/84/33/33/33 /121/58\\n1045-9227/$20.00 © 2006 IEEE\\n234 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006\\nTherefore, the goal of KCCA is equivalent to ﬁnding /33/33/33 /8/40 /120 /41and /33/33/33 /121that\\nmaximize /33/33/33/84\\n/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121under the constraints\\n/33/33/33/84\\n/8/40 /120 /41 /8/40 /88 /41/40/8/40 /88 /41/41/84/33/33/33 /8/40 /120 /41 /61 /33/33/33/84\\n/121 /89/89/84/33/33/33 /121 /61/49 (3)\\nwhere\\n/8/40 /88 /41/61 /91 /8 /40 /120 /49 /41/8 /40 /120 /50 /41 /1/1/1 /8/40 /120 /78 /41/93 /58 (4)\\nAccording to [17], there exists direction /11/11/11 /8/40 /120 /41such that\\n/33/33/33 /8/40 /120 /41 /61/40 /8 /40 /88 /41/41 /11/11/11 /8/40 /120 /41 /58 (5)\\nAssume that /107 /40 /120 /105 /59 /120 /106 /41is a kernel function that can be expressed as\\nthe dot product form on the Hilbert space /70\\n/40 /107 /41 /105/106 /61 /107 /40 /120 /105 /59 /120 /106 /41/61 /104 /8/40 /120 /105 /41 /59 /8/40 /120 /106 /41 /105 /61/40 /8 /40 /120 /105 /41/41/84/8/40 /120 /106 /41(6)\\nwhere /104 /8/40 /120 /105 /41 /59 /8/40 /120 /106 /41 /105stands for the dot product of /8/40 /120 /105 /41and /8/40 /120 /106 /41.\\nFrom (4) and (6), we obtain that the /78 /2 /78matrix /75 /61\\n/40/40 /107 /41 /105/106 /41 /105 /61/49 /59 /46/46/46 /59/78 /59 /106 /61/49 /59 /46/46/46 /59/78(also known as the Gram matrix) can be\\nwritten as\\n/75 /61/40 /8 /40 /88 /41/41/84/8/40 /88 /41 /58 (7)\\nFrom (5), (3), and (7), we obtain\\n/33/33/33/84\\n/8/40 /120 /41 /8/40 /88 /41 /89/84/33/33/33 /121 /61 /11/11/11/84\\n/8/40 /120 /41 /75/89/84/33/33/33 /121 (8)\\n/33/33/33/84\\n/121 /89/89/84/33/33/33 /121 /61 /33/33/33/84\\n/8/40 /120 /41 /8/40 /88 /41/40/8/40 /88 /41/41/84/33/33/33 /8/40 /120 /41\\n/61 /11/11/11/84\\n/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /61/49 /58 (9)\\nThus, solving the KCCA problem is equivalent to ﬁnding /11/11/11 /8/40 /120 /41and\\n/33/33/33 /121that maximize /11/11/11/84\\n/8/40 /120 /41 /75/89/84/33/33/33 /121under the constraints\\n/33/33/33/84\\n/121 /89/89/84/33/33/33 /121 /61 /11/11/11/84\\n/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /61/49 /58\\nThe corresponding Lagrangian is\\n/76\\n /11/11/11 /8/40 /120 /41 /59/33 /33/33 /121 /59/21 /59 /22\\n/61 /11/11/11/84\\n/8/40 /120 /41 /75/89/84/33/33/33 /121 /0 /21\\n /11/11/11/84\\n/8/40 /120 /41 /75/75 /11/11/11 /8/40 /120 /41 /0 /49\\n /50\\n/0 /22\\n /33/33/33/84\\n/121 /89/89/84/33/33/33 /121 /0 /49\\n /50 /58\\nTaking derivatives with respect to /11/11/11 /8/40 /120 /41and /33/33/33 /121and set to zeros, we\\nobtain\\n/64/76\\n/64/11 /11/11 /8/40 /120 /41/61 /75/89/84/33/33/33 /121 /0 /21 /75/75 /11/11/11 /8/40 /120 /41 /61 /48 (10)\\n/64/76\\n/64/33 /33/33 /121/61 /89/75 /11/11/11 /8/40 /120 /41 /0 /22 /89/89/84/33/33/33 /121 /61 /48 /58 (11)\\nFrom (9)–(11), we obtain\\n/22 /61 /21/59 /33 /33/33 /121 /61/40 /89/89/84/41/0 /49/89/75\\n/22/11/11/11 /8/40 /120 /41(12)\\n/75/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/75/75 /11/11/11 /8/40 /120 /41 /58 (13)\\nIf the Gram matrix /75has full rank, then (13) can be rewritten as\\n/75/0 /49/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/11/11/11 /8/40 /120 /41 /58 (14)\\nHowever, in most cases, /75is often singular because it is a centered\\nGram matrix [2]. To overcome this problem, one may add a regular-\\nization matrix /78 /75 /73to /75and then resolve the following generalized\\neigenequation (14), [2], [3], [5]\\n/40 /75 /43 /78 /75 /73 /41/0 /49/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41 /61 /21/50/11/11/11 /8/40 /120 /41 (15)\\nwhere /73is the identity matrix and /78 /75is a small constant. The drawback\\nof this approach is that the selection of the regularization to the Grammatrix could inﬂuence the performance of KCCA. In what follows, we\\nwill propose an improved algorithm for KCCA based on eigenvalue de-\\ncomposition to avoid the regularization to the Gram matrix. In fact, the\\nlargest eigenvalue of (13) gives the maximum of the following Rayleigh\\nquotient [4]\\n/21/50/61/11/11/11/84\\n/8/40 /120 /41 /75/89/84/40 /89/89/84/41/0 /49/89/75 /11/11/11 /8/40 /120 /41\\n/11/11/11/84\\n/8/40 /120 /41/75/75 /11/11/11 /8/40 /120 /41/58 (16)\\nLet /87 /61 /89/84/40 /89/89/84/41/0 /49/89, then (16) can be rewritten as\\n/21/50/61/11/11/11/84\\n/8/40 /120 /41 /75/87/75 /11 /8/40 /120 /41\\n/11/11/11/84\\n/8/40 /120 /41/75/75 /11/11/11 /8/40 /120 /41/58 (17)\\nThus, solving KCCA is equivalent to ﬁnding /11/11/11 /8/40 /120 /41that maximizes the\\nRayleigh quotient in (17), which is equivalent to solving the same op-timal problem as that of generalized discriminant analysis (GDA) [4].\\nTherefore, we can adopt the same eigenvalue decomposition approach\\nas that used in GDA for the Gram matrix to system (17). Moreover,\\nnote that there exists the degenerate eigenvalue problem in solve system\\n(17), we propose to use the method used in the modiﬁed generalized\\ndiscriminant analysis (MGDA) [9] algorithm to tackle this problem.\\nIII. P\\nATTERN CLASSIFICATION USING KCCA\\nLet /102 /40 /33/33/33/105\\n/8/40 /120 /41 /59/33 /33/33/105\\n/121 /41 /103/116\\n/105 /61/49be the /116pair directions of KCCA, and\\n/26 /49 /59 /46/46/46 /59/26 /116the /116corresponding correlation values. Let /97 /105and /98 /105be the\\nprojections of the variables /8/40 /120 /41and /121onto the projection vectors\\n/33/33/33/105\\n/8/40 /120 /41and /33/33/33/105\\n/121, respectively. Thus, we get\\n/97 /105 /61\\n /33/33/33/105\\n/8/40 /120 /41\\n/84\\n/8/40 /120 /41 /59/98 /105 /61\\n /33/33/33/105\\n/121\\n/84\\n/121 /40 /105 /61/49 /59 /46/46/46 /59/116 /41 /58\\nSuppose that /26 /105 /25 /49. Then, we can obtain that /97 /105and /98 /105are proximately\\nlinearly correlated; thus, we can assume that there exists coefﬁcient /107 /105\\nand constant /34 /105such that\\n/98 /105 /61 /107 /105 /97 /105 /43 /34 /105 /40 /105 /61/49 /59 /46/46/46 /59/116 /41 /58 (18)\\nNote that both /98 /105and /97 /105are centered projections. Thus, according to\\nregression analysis [19], the coefﬁcients /107 /105and the constant /34 /105can be\\ncalculated using the observations of /8/40 /120 /41and /121, as follows:\\n/107 /105 /61/78\\n/106 /61/49\\n/33/33/33/105\\n/8/40 /120 /41\\n/84\\n/8/40 /120 /106 /41\\n /33/33/33/105\\n/121\\n/84\\n/121 /106\\n/78\\n/106 /61/49\\n/33/33/33/105\\n/8/40 /120 /41\\n/84\\n/8/40 /120 /106 /41\\n/50\\n(19)\\n/34 /105 /61/49\\n/78/78\\n/106 /61/49\\n/33/33/33/105\\n/121\\n/84\\n/121 /106\\n/0/107 /105\\n/78/78\\n/106 /61/49\\n/33/33/33/105\\n/8/40 /120 /41\\n/84\\n/8/40 /120 /106 /41/61 /48 /58 (20)\\nLet /8/40 /120 /116/101/115 /116 /41be a test sample and /121 /116/101/115/116the corresponding observa-\\ntion of /121. Let /97 /116/101/115/116be the projection of /8/40 /120 /116/101/115/116 /41onto the directions\\n/33/33/33/49\\n/8/40 /120 /41 /59 /46/46/46 /59/33 /33/33/116\\n/8/40 /120 /41, and /98 /116/101/115/116the projection of /121 /116/101/115/116onto the directions\\n/33/33/33/49\\n/121 /59 /46/46/46 /59/33 /33/33/116\\n/121. Then, we get\\n/97 /116/101/115/116 /61\\n /97/49\\n/116/101/115/116 /59 /46/46/46 /59/97/116\\n/116/101/115/116\\n/61\\n /33/33/33/49\\n/8/40 /120 /41 /59 /46/46/46 /59/33 /33/33/116\\n/8/40 /120 /41\\n/84/8/40 /120 /116/101/115/116 /41/61 /80/84\\n/120 /75 /116/101/115/116 (21)\\n/98 /116/101/115/116 /61\\n /98/49\\n/116/101/115/116 /59 /46/46/46 /59/98/116\\n/116/101/115/116\\n/61\\n /33/33/33/49\\n/121 /59 /46/46/46 /59/33 /33/33/116\\n/121\\n/84/121 /116/101/115/116 /61 /80/84\\n/121 /121 /116/101/115/116 (22)\\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 235\\nFig. 1. Example of the 34 landmark points selected from a facial image.\\nwhere\\n/80 /120 /61\\n /11/11/11/49\\n/8/40 /120 /41 /59 /46/46/46 /59/11 /11/11/116\\n/8/40 /120 /41\\n/75 /116/101 /115 /116 /61/40 /8 /40 /88 /41/41/84/8/40 /120 /116/101/115/116 /41\\n/80 /121 /61\\n /33/33/33/49\\n/121 /59 /46/46/46 /59/33 /33/33/116\\n/121\\n /58\\nThe projection /121 /116/101/115/116in (22) can be solved by\\n/121 /116/101/115/116 /61\\n /80 /121 /80/84\\n/121\\n/0 /49\\n/80 /121 /98 /116/101/115/116 /58 (23)\\nIf /80 /121 /80/84\\n/121is singular, we add a regularization /34 /73on /80 /121 /80/84\\n/121such that\\n/80 /121 /80/84\\n/121 /43 /34 /73is a nonlinear matrix. Then (23) can rewritten by\\n/121 /116/101/115/116 /61\\n /80 /121 /80/84\\n/121 /43 /34 /73\\n/0 /49\\n/80 /121 /98 /116/101/115/116 /58 (24)\\nThus, for a given test observation /120 /116/101/115/116, we can estimate the corre-\\nsponding observation /121 /116/101/115/116using (21), (18), (19), (20) and (24). Let\\n/121/105\\n/116/101/115/116denote the /105th element of /121 /116/101/115/116, then the index of the most matched\\nexpression class of the test image is signed by\\n/99/3/61/97 /114 /103 /109 /97 /120\\n/105/121/105\\n/116/101/115/116 /58 (25)\\nIV. E XPERIMENTS\\nWe will use the Japanese female facial expression (JAFFE) database\\n[6]–[8] and Ekman’s “Pictures of Facial Affect” database [22], respec-\\ntively, to conduct the FER based on KCCA. In the preprocessing stage,\\nwe manually locate 34 landmark points from each facial image by re-\\nferring to the method in [7]. Fig. 1 illustrates an example of the 34\\nlandmark points.\\nSimilar with Lyons et al. [7], after locating the 34 landmark points,\\nwe use the Gabor wavelet representation of each facial image at the\\nlandmark points to represent the facial features of each image, where\\nall of the wavelet convolution values (magnitudes) at these landmarkpoints are combined into a 1020-dimensional LG vector. The Gabor\\nkernel is deﬁned as follows [7], [20], [21]:\\n/32 /117/59 /118 /40 /122 /41/61/107 /107 /117/59/118 /107/50\\n/27/50/101/120 /112\\n /0/107 /107 /117/59/118 /107/50/107 /122 /107/50\\n/50 /27/50\\n/2\\n /101/120/112/40 /105 /107 /117/59/118 /1 /122 /41 /0 /101/120/112\\n /0/27/50\\n/50\\n(26)\\nFig. 2. Semantic ratings (1: happiness; 2: sadness; 3: surprise; 4: anger; 5:\\ndisgust; 6: fear) of the facial image in Fig. 1, where the left ﬁgure illustrates\\nthe semantic ratings from psychologists, whereas the right one illustrates the\\nsemantic ratings estimated by the KCCA method.\\nwhere /117and /118represent the orientation and scale of the Gabor kernels,\\nand /107 /117/59/118is deﬁned as\\n/107 /117/59/118 /61 /107 /118 /101/120/112/40 /105/30 /117 /41 (27)\\nwhere /107 /118 /61 /25/61 /50/118/40 /118 /50/102 /49 /59 /46/46/46 /59 /53 /103 /41and /30 /117 /61 /25/117 /61 /54/40 /117 /50/102 /48 /59 /46/46/46 /59 /53 /103 /41.\\nIt is notable that both JAFFE database and Ekman’s expression\\ndatabase provide semantic ratings evaluated by psychologists foreach facial image. These semantic ratings are used for quantitatively\\nevaluating the six basic expressions (happiness, sadness, surprise,\\nanger, disgust, and fear). In this experiment, the semantic ratings of\\neach image are combined into a six-dimensional semantic expression\\nvector for expression analysis.\\nLet\\n/120and /121, respectively, denote the LG vector and semantic expres-\\nsion vector. The goal of the KCCA-based facial expression analysis\\nmethod is to estimate the associated value of /121for a given observation\\nof /120using (24) and then perform the facial expression classiﬁcation\\ntask using (25). We use the “leave-one-image-out” and “leave-one-sub-\\nject-out” cross validation strategies, respectively, to perform the experi-\\nment. In the “leave-one-image-out” strategy, one facial image is used as\\nthe testing data and the other ones as the training data. This is repeated\\nfor all of the possible trials and the recognition results are averaged as\\nthe ﬁnal recognition rate. In the “leave-one-subject-out” strategy, the\\nimages belonging to one subject are used as the testing data and the\\nremainder ones as the training data. This is also repeated for all of thepossible trials until all the subjects are used as the testing data. The ex-\\nperiment results are also averaged as the ﬁnal recognition rate.\\nA. FER on JAFFE Facial Expression Database\\nWe use the JAFFE database to test the performance of the proposed\\nmethod in this experiment. There are 213 images covering seven\\ncategories of facial expressions (neutral, happiness, sadness, surprise,anger, disgust, and fear) in JAFFE database. Except for the neutral\\nexpression images, there are 183 images in this database. The original\\nimages are all sized 256\\n/2256 pixels with a 256-level gray scale.\\nFor a new test facial image, we locate the 34 landmark points and\\nthen get the LG vector. The corresponding semantic ratings of each\\nfacial image are estimated using (24), and then perform the expressionclassiﬁcation according to (25). Fig. 2 illustrates the semantic ratings\\nof the facial image in Fig. 1, where the left ﬁgure illustrates the\\nsemantic ratings evaluated from psychologists whereas the right onethe semantic ratings estimated from our experiment (both ﬁgures are\\nnormalized into the scope [0, 1]). From Fig. 2, we can see that the\\n236 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006\\nTABLE I\\nEXPERIMENTAL RESULTS ON JAFFE D ATABASE USING\\nTHE“LEAVE-ONE-IMAGE -OUT”CROSS VALIDATION\\nWITHSEMANTIC INFORMATION USED\\nTABLE II\\nEXPERIMENTAL RESULTS ON JAFFE D ATABASE USING\\nTHE“LEAVE-ONE-SUBJECT -OUT”CROSS VALIDATION\\nWITHSEMANTIC INFORMATION USED\\nright ﬁgure approximately coincides with the left one. Based on the\\nsemantic ratings in Fig. 2, we can classify this facial image into sadness\\ncategory. The experimental results based the “leave-one-image-out”\\nand the “leave-one-subject-out” cross validation strategies are shown\\nin Table I and Table II, respectively, where the kernel functions are\\ndeﬁned as:\\nMonomial kernel: /107 /40 /120/59 /121 /41/61 /40 /104 /120/59 /121 /105 /41/100, where /100is the monomial\\ndegree; Gaussian kernel: /107 /40 /120/59 /121 /41/61/101 /120 /112 /40 /0/107 /120 /0 /121 /107/50/61/27 /41, where /27is\\nthe parameter of the kernel function.\\nTo further test the performance of the proposed method, we set the\\nvalues of the semantic expression vector of each training image by re-\\nferring to the class label information: Assume that /121 /105is the semantic\\nexpression vector corresponding to the /105th training image /120 /105, and let\\n/121 /105/107denote the /107thelement of /121 /105.I f /120 /105belongs to class /106, then we set\\n/121 /105/106 /61/49; else /121 /105/107 /61/48 /40 /107 /54/61 /106 /41. In this case, the directions /33/33/33 /8/40 /120 /41of\\nKCCA could be equivalent to the discriminant vectors of GDA. For the\\ncomparison purpose, we also conduct the same expression classiﬁca-\\ntion task using the GDA method, where the nearest neighbor classiﬁer\\nis used for classiﬁcation. Table III and Table IV show the experimental\\nresults.\\nFrom Tables I–IV, we can see that the KCCA method achieves much\\nbetter performance than the CCA method for the FER task. When the\\nclass label information of each facial image is used for constructing\\nthe semantic expression vector, the recognition rate of KCCA can be\\nachieved as high as 98.36% for the “leave-one-image-out” cross val-\\nidation experiment and 77.05% for the “leave-one-subject-out” crossvalidation experiment. Additionally, it is notable that the best recog-\\nnition results in Tables I and II are, respectively, lower than those in\\nTables III and IV. The reason could be attributed to the fact that the se-mantic expression vectors used in Tables III and IV consider the class\\nlabel information; thus, it will be advantageous when used for the clas-\\nsiﬁcation task. The drawback of doing this, however, is that it may not\\nquantitatively estimate all the basic expressions from the facial image.\\nBesides, although KCCA and GDA are equivalent when the semanticexpression vector of each facial image consists of the class label infor-TABLE III\\nEXPERIMENTAL RESULTS ON JAFFE D ATABASE USING\\nTHE“LEAVE-ONE-IMAGE -OUT”CROSS VALIDATION\\nWITHCLASS LABEL INFORMATION USED\\nTABLE IV\\nEXPERIMENTAL RESULTS ON JAFFE D ATABASE USING\\nTHE“LEAVE-ONE-SUBJECT -OUT”CROSS VALIDATION\\nWITHCLASS LABEL INFORMATION USED\\nmation in KCCA, our experimental results also show that KCCA may\\nachieve better performance than GDA. The reason could be attributed\\nto the degenerate eigenvalue problem of GDA [9]. Due to the degen-\\nerate eigenvalue problem, GDA becomes instable and could not achievethe best recognition result [9]. The improved KCCA algorithm we pro-\\nposed in this correspondence, however, can overcome this problem and,\\nthus, could obtain better results.\\nB. FER on Ekman’s Facial Expression Database\\nIn this experiment, we use Ekman’s “Pictures of Facial Affect” data-\\nbase [22] to further test the performance of the proposed FER method.\\nThis database contains 110 images consisting of six male and eight fe-male subjects. We only choose 96 images (excluding all the 14 neutral\\nimages) covering all the 14 persons with six basic facial expressions\\nas the training data. The experiment scheme is designed to be sim-ilar with the one in Section A. In the ﬁrst example of the experiment,\\nwe use the semantic ratings provided by the Ekman’s database to gen-\\nerate the semantic expression vectors, whereas the second example uses\\nthe class label information of each image to generate the semantic ex-\\npression vectors. The “leave-one-image-out” cross validation and the\\n“leave-one-subject-out” cross validation are also respectively adoptedin both examples. The best experimental results of the two examples\\nare shown in Tables V and VI, respectively. Moreover, in Fig. 3 we\\nalso show the plot of recognition rates of KCCA and GDA as functions\\nof gaussian kernel parameter\\n/27when the “leave-one-subject-out” cross\\nvalidation with class label information is used for constructing the se-mantic expression vector.\\nV. D\\nISCUSSION AND CONCLUSION\\nThe FER problem based on KCCA has been addressed in this cor-\\nrespondence. The correspondence shows that KCCA is a very effec-\\nIEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006 237\\nFig. 3. Recognition rates as functions of the Gaussian kernel parameter /27.\\nTABLE V\\nEXPERIMENTAL RESULTS ON EKMAN’S DATABASE\\nWITHSEMANTIC INFORMATION USED\\nTABLE VI\\nEXPERIMENTAL RESULTS ON EKMAN’S DATABASE\\nWITHCLASS LABEL INFORMATION USED\\ntive method for correlating the nonlinear relationship between the fa-\\ncial features and the associated semantic features, which also provideus an effective technique to predict the semantic expression informa-\\ntion of a facial image. The experimental results on JAFFE database and\\nEkman’s facial expression database are encouraging. In addition, fromthe experimental results, we note that the performance of the proposed\\nmethod is closely related to selection of the semantic expression vec-\\ntors used for training KCCA. Although the best classiﬁcation perfor-mance in the experiment using JAFFE database is obtained when the\\nclass label information is used for constructing the semantic expressionvectors, a major drawback of doing this is that these semantic expres-\\nsion vectors convey relatively less basic expression information. Thus,\\nthey may not produce better results when used for quantitatively pre-dicting all the six basic expressions. To improve the performance of the\\nsemantic ratings-based approach, a natural and better way could resort\\nto obtain more accurate semantic ratings of each facial image to con-struct the corresponding semantic expression vector. Moreover, in this\\ncorrespondence we also provide an improved algorithm for KCCA to\\novercome the singularity problem of the Gram matrix and the degen-\\nerate eigenvalue problem of solving the eigenequation of KCCA.\\nA\\nCKNOWLEDGMENT\\nThe authors would like to thank the anonymous reviewers for their\\nvaluable comments. They would also like to thank Dr. M. J. Lyons for\\nproviding the JAFFE facial expression database and the semantic rating\\ndata of the database.\\nREFERENCES\\n[1] H. Hotelling, “Relations between two sets of variates,” Biometrika , vol.\\n28, pp. 321–377, 1936.\\n[2] F. R. Bach and M. I. Jordan, “Kernel independent component analysis,”\\nJ. Mach. Learn. Res. , vol. 3, pp. 1–48, 2002.\\n[3] P. L. Lai and C. Fyfe, “Kernel and nonlinear canonical correlation anal-\\nysis,”Int. J. Neural Syst. , vol. 10, no. 5, pp. 365–377, 2000.\\n[4] G. Baudat and F. Anouar, “Generalized discriminant analysis using a\\nkernel approach,” Neural Comput. , vol. 12, pp. 2385–2404, 2000.\\n[5] T. V. Gestel, J. A. K. Suykens, J. De Brabanter, B. De Moor, and J. Van-\\ndewalle, “Kernel canonical correlation analysis and least square support\\nvector machines,” in Proc. Int. Conf. Artiﬁcial Neural Networks , 2001,\\npp. 384–389.\\n[6] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial ex-\\npressions with gabor wavelets,” in Proc. 3rd IEEE Int. Conf. Automatic\\nFace and Gesture Recognition , Japan, Apr. 1998, pp. 200–205.\\n[7] M. Lyons, J. Budynek, and S. Akamatsu, “Automatic classiﬁcation of\\nsingle facial images,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21,\\nno. 12, pp. 1357–1362, Dec. 1999.\\n[8] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, “Comparison\\nbetween geometry-based and Gabor-wavelets-based facial expressionrecognition using multi-layer perceptron,” in Proc. 3rd IEEE Int.\\nConf. Automatic Face and Gesture Recognition , Nara, Apr. 1998, pp.\\n454–459.\\n[9] W. Zheng, L. Zhao, and C. Zou, “A modiﬁed algorithm for generalized\\ndiscriminant analysis,” Neural Comput. , vol. 16, no. 6, pp. 1283–1297,\\n2004.\\n[10] V. Vapnik, The Nature of Statistical Learning Theory . New York:\\nSpringer, 1995.\\n[11] B. Schölkopf, A. Smola, and K. R. Müller, “Nonlinear component\\nanalysis as a kernel eigenvalue problem,” Neural Comput. , vol. 10, pp.\\n1299–1319, 1998.\\n[12] B. Fasel and J. Luettin, “Automatic Facial Expression Analysis: A\\nSurvey,”Pattern Recognit. , vol. 36, pp. 259–275, 2003.\\n[13] M. Pantic and L. J. M. Rothkrantz, “Automatic analysis of facial expres-\\nsions: The state of the art,” IEEETrans.PatternAnal.Mach.Intell. , vol.\\n22, no. 12, pp. 1424–1445, Dec. 2000.\\n[14] M. Pantic and L. J. M. Rothkrantz, “Toward an affect-sensitive mul-\\ntimodal human-computer interaction,” Proc. IEEE , vol. 91, no. 9, pp.\\n1370–1390, Sep. 2003.\\n[15] M. Suwa, N. Sugie, and K. Fujimora, “A preliminary note on pattern\\nrecognition of human emotional expression,” in Proc.4thInt.JointConf.\\nPattern Recognition , 1978, pp. 408–410.\\n[16] K. Mase and A. Pentland, “Recognition of facial expression from optical\\nﬂow,”Trans. IEICE , pp. 3474–3483, 1991.\\n[17] D. R. Hardoon, S. Szedmark, and J. S. Taylor, Canonical Correlation\\nAnalysis: An Overview With Application to Learning Methods, Tech.Rep. CSD-TR-03-02, 2003.\\n[18] B. Schölkopf and A. J. Smola, Learning With Kernels . Cambridge,\\nMA: MIT Press, 2001.\\n[19] J. M. Lattin, J. D. Carroll, and P. E. Green, Analyzing multivariate\\ndata. Paciﬁc Grove, CA: Brooks/Cole, 2003.\\n238 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 17, NO. 1, JANUARY 2006\\n[20] L. Wiskott, J. M. Fellous, N. Krüger, and C. von der Malsburg, “Face\\nrecognition by elastic bunch graph matching,” IEEETrans.PatternAnal.\\nMach. Intell. , vol. 19, no. 7, pp. 775–779, Jul. 1997.\\n[21] C. Liu and H. Wechsler, “Independent component analysis of Gabor fea-\\ntures for face recognition,” IEEETrans.NeuralNetw. , vol. 14, no. 4, pp.\\n919–928, Aug. 2003.\\n[22] P. Ekman and W. V. Friesen, “Pictures of Facial Affect,” in Human In-\\nteraction Laboratory . San Francisco, CA: Univ. California Medical\\nCenter, 1976.\\nStudyonModelingofMultispectralEmissivityand\\nOptimizationAlgorithm\\nChunling Yang, Yong Yu, Dongyang Zhao, and Guoliang Zhao\\nAbstract—Target’sspectralemissivitychangesvariously,andhowtoob-\\ntaintarget’scontinuousspectralemissivityisadifﬁcultproblemtobewell\\nsolvednowadays.Inthisletter,anactivation-function-tunableneuralnet-\\nworkisestablished,andamultistepsearchingmethodwhichcanbeusedto\\ntrainthemodelisproposed.Theproposedmethodcaneffectivelycalculate\\ntheobject’scontinuousspectralemissivityfromthemultispectralradiation\\ninformation.Itisauniversalmethod,whichcanbeusedtorealizeon-line\\nemissivitydemarcation.\\nIndex Terms—Neuralnetworks,optimizationalgorithm,spectralemis-\\nsivity,tunableactivationfunction.\\nI. INTRODUCTION\\nThe emissivity is not only dependent on the inherence of the object to\\nbe measured, but also lies with the physical state of the object’s surface,\\nsuch as surface smoothness, material granularity, temperature, radiantangle, wavelength, and so on. A great of error exists in the emissivity\\nobtained by experienced methods because of the factors listed above.\\nAt present, this problem has not been well solved. Classical emissivitymeasuring method, such as calorimetry, the thermal return reﬂection\\nmethod and radiometry [1]–[3] need to install some assistant equipment\\naround the object to be detected and they cannot realize the on-linedemarcation. Thus their applications are conﬁned.\\nMultispectral thermometry is an improved method in recent years\\nfor obtaining radiation temperature and emissivity, which can sepa-\\nrate emissivity and the real temperature from the object’s radiation\\ninformation. We can use this new method to overcome the aboveproblem. However, current multispectral thermometries usually rely\\non an assumption model [4]–[6] and an experienced formula. If the\\nformula cannot describe the object to be measured accurately, the\\nerror will exist in the measurement results of emissivity and the\\nreal temperature. It restricts the further development of multispectral\\nthermometry.\\nThis letter has established an activation- function-tunable neural net-\\nwork (AFT-CNNE) and a multistep searching method used to train the\\nmodel in order to calculate object’s continuous spectral emissivity. This\\nManuscript received July 11, 2004 ; revised December 17, 2004.\\nC. Yang is with the Department of Electrical Engineering, Harbin Institute of\\nTechnology, Harbin, Heilongjiang 150001, China, and also with the Department\\nof Automation, Harbin Engineering University, Harbin, Heilongjiang 150001,\\nP.R. China (e-mail: yangcl1@hit.edu.cn).\\nY. Yu, D. Zhao, and G. Zhao are with the Department of Electrical Engi-\\nneering, Harbin Institute of Technology, Harbin, Heilongjiang 150001, China.\\nDigital Object Identiﬁer 10.1109/TNN.2005.860837\\nFig. 1. Neural network structure.\\nmodel does not require the assumption of a ﬁxed emissivity formula,\\nand can realize on-line demarcation, and can also get very high preci-\\nsion for any object.\\nII. CNNE M ODEL BASED ON FIXED ACTIVATION FUNCTION\\nFirst of all, we need to use the neural network to establish the map-\\nping relationship between emissivity and wavelength, that is /34 /61 /34 /40 /21 /41.\\nAs we cannot get the sample data of wavelength and emissivity from\\nthe actual measurement, so it is impossible to get the functional rela-tionship model of\\n/34directly from the BP network. Therefore, we com-\\nbine the radiant temperature measurement theory with neural network\\nto establish the combined neural network model. In this way, we canget the weight coefﬁcients of the mapping relationship for\\n/34 /61 /34 /40 /21 /41.\\nNow we introduce the speciﬁc steps as follows.\\nA. Identiﬁcation of the Nonlinear Relationship Between Wavelength\\nand Emissivity\\nIt has been proved in theory that a forward neural network with a\\nhidden layer can approximate any nonlinear curve deﬁned in a com-pact set in\\n/82/110with any precision. Since there is a nonlinear mapping\\nrelationship between the spectral emissivity and the wavelength of the\\nobject, a single hidden layer neural network is used to represent this\\nrelationship as shown in Fig. 1. The network has a structure with one\\ninput and one output, with /107nodes. The input and output signals are\\ndenoted by /21and /34, respectively. The hidden layer errors are denoted\\nby /98 /49 /49 /59/98 /49 /50 /59 /46/46/46 /59/98 /49 /107, respectively, and the output layer error is denoted\\nby /98 /50. According to the theory of neural network, the activation func-\\ntion /102 /40 /58 /41of output and hidden layers can be either the same or different.\\nSuppose the activation function is the same denoted by /102 /40 /58 /41, then the\\nrelationship between the wavelength /21and the emissive /34can be ob-\\ntained from Fig. 1 as follows.\\n/34 /61 /102\\n/107\\n/109 /61/49/86 /109 /102 /40 /87 /109 /21 /43 /98 /49 /109 /41\\n /43 /98 /50\\n /58 (1)\\nSince the sample data of wavelength and emissivity cannot be ob-\\ntained by using multispectral thermometry, the weights in above neural\\nnetwork cannot be calculated, which means the network has to be im-\\nproved. Since the brightness temperature of each spectrum can be mea-\\nsured, a mathematical model can be constructed based on the bright-\\nness temperature so as to obtain the best approximation to the weights\\nof the object’s emissivity, and then the nonlinear mapping relationshipbetween the emissivity and the wavelength can be achieved. The CNNE\\nmodel below can realize above purpose.\\nB. Construction of the Combined Neural Network Model\\nLet the number of channels of the multispectral radiation ther-\\nmometer be\\n/110, then, according to the theory of radiation thermometry,\\nthe relationship among the real object temperature /84, the brightness\\n1045-9227/$20.00 © 2006 IEEE\\n',\n",
       " '0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 1\\nFacial Expression Recognition using Local\\nGravitational Force Descriptor based Deep\\nConvolution Neural Networks\\nKarnati Mohan, Ayan Seal, Senior Member IEEE , Ondrej Krejcar, Anis Yazidi, Senior Member IEEE\\nAbstract —An image is worth a thousand words, hence a face\\nimage illustrates extensive details about the speciﬁcation, gender,\\nage, and emotional states of mind. Facial expressions play an im-\\nportant role in community-based interactions and are often used\\nin the behavioral analysis of emotions. Recognition of automatic\\nfacial expressions from a facial image is a challenging task in the\\ncomputer vision community and admits a large set of applications\\nsuch as driver safety, human-computer interactions, health care,\\nbehavioral science, video conferencing, cognitive science, and\\nothers. In this work, a deep-learning-based scheme is proposed\\nfor identifying the facial expression of a person. The proposed\\nmethod consists of two parts. The former one ﬁnds out local\\nfeatures from face images using a local gravitational force de-\\nscriptor while in the latter part, the descriptor is fed into a novel\\ndeep convolution neural networks model (DCNN). The proposed\\nDCNN has two branches. The ﬁrst branch explores geometric\\nfeatures such as edges, curves, and lines whereas holistic features\\nare extracted by the second branch. Finally, the score-level fusion\\ntechnique is adopted to compute the ﬁnal classiﬁcation score.\\nThe proposed method along with twenty-ﬁve state-of-the-art\\nmethods is implemented on ﬁve benchmark available databases\\nnamely, Facial Expression Recognition 2013, Japanese Female\\nFacial Expressions, Extended CohnKanade, Karolinska Directed\\nEmotional Faces, and Real-world Affective Faces. The databases\\nconsist of seven basic emotions viz., neutral, happiness, anger,\\nsadness, fear, disgust, and surprise. The proposed method is\\ncompared with existing approaches using four evaluation metrics\\nnamely, accuracy, precision, recall, and f1-score. The obtained\\nresults demonstrate that the proposed method outperforms all\\nstate-of-the-art methods on all the databases.\\nIndex Terms —Facial expression recognition, local gravitational\\nforce descriptor, deep convolution neural networks, softmax\\nclassiﬁcation, score-level fusion.\\nI. I NTRODUCTION\\nKarnati Mohan is with PDPM Indian Institute of Information Technology,\\nDesign and Manufacturing, Jabalpur, 482005, India E-mail: 1811011@iiit-\\ndmj.ac.in.\\nAyan Seal is with PDPM Indian Institute of Information Technology, Design\\nand Manufacturing, Jabalpur, 482005, India and Center for Basic and Applied\\nScience, Faculty of informatics and management, University of Hradec\\nKralove, Rokitanskeho 62, 500 03 Hradec Kralove, Czech Republic E-mail:\\nayanseal30@ieee.org.\\nOndrej Krejcar is with Center for Basic and Applied Science, Faculty of\\ninformatics and management, University of Hradec Kralove, Rokitanskeho 62,\\n500 03 Hradec Kralove, Czech Republic Email: ondrej.krejcar@uhk.cz. He\\nis also associated with Malaysia-Japan International Institute of Technology\\n(MJIIT), Universiti Teknologi Malaysia, Jalan Sultan Yahya Petra, 54100\\nKuala Lumpur, Malaysia.\\nAnis Yazidi is with the Research Group in Applied Artiﬁcial Intelligence,\\nOslo Metropolitan University, 460167, NorwayEmail:anisy@oslomet.no.AFFECTIVE computing is a ﬁeld of study that attempts to\\ndevelop instruments/devices and systems that can iden-\\ntify, interpret, process, and simulate human affects. Nowadays\\nit has got a considerable amount of attention towards the\\nresearch communities in the ﬁelds of artiﬁcial intelligence and\\ncomputer vision owing to its noticeable academic and commer-\\ncial applications such as human-computer interaction (HCI),\\nvirtual reality, health care, deceptive detection, multimedia,\\naugmented reality, driver safety, surveillance, etc. Generally,\\ncomputational models of human expression process affective\\nstate and they are of two types namely, decision-making\\nmodels and predictive models. The former one accounts for\\nthe effect of expression whereas the latter one can identify the\\nstate of the emotion. Models of non-verbal expression of dif-\\nferent forms of facial expressions deduced from speech, body\\ngesture, and physiological signals provide valuable sources for\\naffective computing. Interested readers are referred to [1] to\\nknow more about various methods, models, and applications\\nof affective computing. In this study, computer-based facial\\nexpression recognition (FER) is considered due to its ability to\\nmimic human coding skills. FER is indispensable in affective\\ncomputing. Facial expression is an essence of non-verbal com-\\nmunication to express the internal behaviors in interpersonal\\nrelations. Moreover, it is a sentiment analysis technology\\nthat uses biometric to automatically recognize seven basic\\nemotions, namely, neutral (NE), anger (AN), disgust (DI),\\nfear (FE), happiness (HA), sadness (SA), and surprise (SU)\\nfrom still images or videos. Although a considerable amount\\nof works was conducted for developing instruments to access\\nemotions, recognizing human expressions is still a challenging\\ntask that is affected by deﬁnite circumstances especially when\\nperformed in the wild. Some of the notable difﬁculties associ-\\nated with FER are as follows: a) when the difference between\\ntwo facial expressions is small then it is difﬁcult to distinguish\\nthem with high precision [2], b) generally, the expression of\\na particular facial emotion by different people is not the same\\ndue to the inter-person variability and their face biometric\\nshapes [3]. All the recent studies focus on FER methods that\\ncan be categorized into two groups viz., hand-crafted feature-\\nbased methods, and deep-learning feature-based methods. The\\nformer one is also divided into appearance-based features and\\ngeometric features. Appearance-based methods rely on various\\nstatistics of the pixels’ values within the face image. Examples\\ninclude Gobar wavelets [4], Haar wavelet [5], Local Binary\\nPattern (LBP) [6], [7], Histogram of oriented gradients (HOG)\\n[7], [8], Histogram of bunched intensity values (HBIV) [9],\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 2\\nDynamic Bayesian Network (DBN), [10], etc. On the other\\nhand, geometric features are obtained by transforming the\\nimage into geometric primitives such as corner or minutiae\\npoints [11], edges, and curves [12]. This is accomplished, for\\nexample, by locating unique features such as eyes, mouth,\\nnose, and chin, and measuring their relative position [13],\\n[14], width, and perhaps other parameters. However, extracting\\ndistinctive features based on traditional methods is limited to\\nthe human experience, so it is difﬁcult to acquire as well\\nas it is arduous to achieve better performance on large data.\\nTraditional approaches are not up to the real FER application\\nrequirements and they also require high computational cost\\nand space [15].\\nOver the past few years, feature extraction from image data\\nusing deep convolution neural networks (DCNN) has gained\\npopularity in various computer vision tasks. By virtue of using\\nDCNN, many breakthroughs were achieved for image clas-\\nsiﬁcation problems especially face related recognition tasks\\n[16]–[18]. It is observed that DCNN has outperformed the\\ntraditional methods with hand-crafted features in recent years\\n[19]–[21]. DCNN is able to extract hypothetical features from\\na low-level to a high-level of facial images with the help\\nof several non-linear connections [16]. Furthermore, DCNN\\ncan extract useful unique features by solving several issues\\ncaused by traditional methods. In [22], a DCNN for FER was\\ndesigned to provide better discrimination ability by combining\\nthe central loss function and the veriﬁcation recognition model.\\nIn another work [23], a conditional generative adversarial net-\\nwork was presented to increase the size of the data and DCNN\\nused for the facial expression classiﬁcation [24]. Fathallah et\\nal. discussed a recognition algorithm based on the geometry\\ngroup model [25].\\nA. Motivation and contribution\\nIt is clear from the literature that most of the existing works\\nperform reasonably well on databases having images which\\nwere captured in controlled lab environments. However, these\\nworks do not yield satisfactory results on more challenging\\nand real-time databases consisting of images with greater\\nvariations. Thus, there is a need to improve the performance\\nof a FER system. The performance of a FER system relies on\\nfeature engineering. Engineering new features from existing\\nones can improve the performance of a system. This motivates\\nus to work further in this direction. It is also clear from\\nstate-of-the-art methods that most of the works related to\\nFER tasks are based on edge information [12], [26]–[30]\\nbecause it varies in individual expression. Important features\\nsuch as corners, lines, curves can be extracted from edges\\nof an image. Edges are signiﬁcant local changes of intensity\\nin an image. In the recent past, various DCNN models were\\nexploited to extract hypothetical deep features for developing\\nFER systems. However, the number of features is quite large.\\nSometimes deep features may lead to overﬁtting. Moreover,\\nthe extraction of deep features is time-consuming and it\\nrequires powerful resources. Furthermore, only a small fraction\\nof these overwhelming numbers of features are used. On the\\nother hand, edge detection using gradient captures the smallchange in the x and y directions, which are known as gradients.\\nThe gradient is a vector that has a certain magnitude (M)\\nand direction (D). The M would be higher when there is a\\nsharp change in intensity, such as around the edges. The M\\nprovides information about edge strength. On the other hand,\\nthe D is always perpendicular to the D of the edge. The D\\nrepresents the geometric structure of the image. So, in the\\nﬁrst step of the proposed method, edge descriptor based on\\nGravitational Force (GF) [31] is adopted because it uses sur-\\nrounding pixel information instead of considering the adjacent\\npixels difference in x and y directions while computing M and\\nD images. However, the proposed system does not depend\\nonly on local edge information, but it also depends on holistic\\nfeatures. Thus, in the second step, M and D images are fed into\\na novel DCNN to extract useful information. The proposed\\nDCNN consists of two branches, the ﬁrst one consists of\\nshallow DCNN and extracts the local features whereas the\\nsecond one fetches the holistic features from M and D images\\nas it consists of major DCNN. Finally, a score-level fusion\\ntechnique is adopted on classiﬁcation results obtained from\\nM and D images to get ﬁnal results. The overview of the\\nproposed method is shown in Fig. 1. The performance of the\\nproposed method is compared with twenty-ﬁve state-of-the-\\nart methods. All the methods are implemented on ﬁve bench-\\nmark databases namely, Facial Expression Recognition 2013\\n(FER2013) [32], Japanese Female Facial Expressions (JAFFE)\\n[33], Extended CohnKanade (CK+) [34], Karolinska Directed\\nEmotional Faces (KDEF) [35], and Real-world Affective Faces\\n(RAF) [36], [37]. To measure the efﬁciencies of all the\\nmethods including the proposed one, four classiﬁcation metrics\\nviz., accuracy, precision, recall, and f1-score are considered for\\nthe quantitative evaluation. Empirical outcomes illustrate that\\nthe proposed method defeats all the twenty-ﬁve state-of-the-art\\nmethods.\\nThe rest of the work is organized as follows: In Section\\nII, a review of earlier works related to FER is conducted.\\nThe proposed method is described in Section III. Experimental\\nresults and discussion are presented in Section IV. Finally,\\nSection V concludes the work.\\nII. R ELATED WORK\\nAll the methods in the FER task can be categorized\\ninto two groups based on feature extraction techniques, viz.\\nhand-crafted features, and deep-learning features. This section\\npresents them brieﬂy. Mainly two steps, namely feature ex-\\ntraction, and classiﬁcation are associated with the FER task.\\nConventional features such as Gobar wavelets [4], curves\\n[12], Scale Invariant Feature Transform [21], HOG [8], LBP\\n[6], minutiae points [11], Haar wavelet [5], HBIV [9], DBN\\n[10], and edges [38] were exploited with advanced domain\\ncomprehension in the ﬁrst step. In the second step, support\\nvector machine (SVM) [39], feed-forward neural network\\n[40], and extreme learning machine [41] were adopted for\\nclassiﬁcation. In [42], Chen et al. offered a feature de-\\nscriptor called HOG from Three Orthogonal Planes (HOG-\\nTOP) to extract dynamic textures from video sequences to\\ncharacterize facial appearance changes. However, hand-crafted\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 3\\n \\n \\n       M \\n        D \\n    DCNN  \\n    DCNN  \\n         Pre-processing  \\n Feature extraction & c lassification  \\n Score level \\nfusion  \\nFinal s core  \\n \\nInput i mage  \\nNE \\n AN  \\n DI \\n FE \\n HA \\n SA \\n SU \\n Fusion  \\n        Rotation by +50 \\n       Rotation by -50 \\n       Horizontal flip \\n       Gaussian  noise \\nFig. 1. An overview of the proposed FER scheme\\nfeatures based methods have limited performance in real-\\nlife applications, especially for FER tasks. In recent years,\\nit is observed from the literature that deep-learning-based\\nmethods are superior to hand-crafted features based methods\\nfor FER tasks [17], [18], [43], [44]. Shallow and Deep CNN\\nconsidered for extraction on gray-scale images and classiﬁed\\nusing softmax classiﬁers on FER2013 [45]. In [46], the DCNN\\nframework and Softmax were considered for feature extraction\\nand classiﬁcation respectively on the FER2013 database [32].\\nIn [47], Orozco presented Alexnet, VGG19, and ResNet based\\ntransfer learning methods for the FER task. In [16], Sun et\\nal. presented a DCNN model and DeepID features for face\\nrecognition. In another work, Sun et al. considered the Siamese\\nnetwork to increase the efﬁciency of the FER task [48].\\nBarsoum et al. discussed the VGG13 network for the FER\\ntask on the FER+ database in [49]. A weighted mixture deep\\nneural network considered for the FER task and it consists\\nof two channels, one of them was used to extract the facial\\nexpression features on gray-scale images with partial VGG16\\nframework. On the other hand, features are extracted on LBP\\nimages with shallow DCNN on JAFFE [33] and CK+ [34]\\ndatabases further, softmax classiﬁers were used to classify\\nthe extracted features and then combined obtained outputs\\nfrom both the channels using weighted fusion in [50]. In [51],\\nfeatures were extracted from pre-trained VGG19 architecture\\non the ImageNet database for the FER task and SVM used\\nfor expression classiﬁcation on JAFFE and CK+ databases. In\\n[52], three DCNN sub-networks were considered and trained\\nindependently on FER2013 and AffectNet database [53], fur-\\nther ensembled three networks using weighted fusion. Fur-\\nthermore larger weights were assigned to the network, which\\nobtained higher recognition accuracy. Moreover, appearance-\\nbased features were extracted using DCNN and obtained\\nfeatures were fused with geometric feature-based DCNN in\\nthe hierarchical constitution according to [54]. However, ap-\\npearance features were extracted on LBP images likewise\\ngray-scale images were considered for geometric feature-based\\nnetworks on JAFFE and CK+ databases. In [55], ensembledResNet50 and VGG16 frameworks were utilized to extract\\nfacial features and classify individual expressions on the\\nKDEF database [35]. Hasani and Mahoor presented a DCNN\\nframework which consists of 3D Inception-ResNet layers\\nfollowed by an LSTM unit that together extract the spatial and\\ntemporal relations from facial images (3D Inception-ResNet +\\nlandmarks) [56]. Geometric and regional local binary pattern\\nfeatures were merged by autoencoders followed by Kohonen\\nself-organizing map (SOM)-based classiﬁer (Autoencoders +\\nSOM) to recognize facial expressions [57]. Kim et al. [58],\\nconsidered a Spatio-temporal feature representation learning\\nfor solving the FER problem by encoding the characteristics of\\nfacial expressions using DCNN and long short-term memory\\n(LSTM) (Spatio-temporal feature + LSTM). Pons and Masip\\nconsidered ensembles of DCNNs for solving the FER problem\\n[59]. Garcia and Ramirez presented a DCNN for classifying\\ntwo facial expressions viz., happy and sad only [60]. Meng\\net al. [61] and Liu et al. [62] worked on identity-aware FER\\nmodels. In [61], Meng et al. used two identical DCNN streams\\nto jointly estimate various expressions and identity features\\n(IACNN) to ﬁnd relief inter-subject variations initiated by\\npersonal attributes for the FER task. On the other hand, Liu et\\nal. [62] employed deep metric learning (2B (N+M)Softmax)\\nto jointly optimize a deep metric and softmax loss. In [63],\\nAlam et al. resorted to a sparse-deep simultaneous recurrent\\nnetwork (S-DSRN) for the FER problem and incorporated\\na dropout rate to the model. Benitez-Quiroz et al. [64],\\npresented a FER system based on discriminant color features\\nand a Gabor transform-based algorithm (Color features +\\nGabor transform) to gain invariance to the timing of facial\\naction units (AUs) changes. In [65], a model called deep\\ncomprehensive multi-patches aggregation convolutional neural\\nnetworks (DCMA-CNNs) was presented. It had two branches.\\nOne branch extracted holistic features whereas the other\\nbranch obtained local features from segmented expressional\\nimage patches. Then both feature vectors were combined\\nto classify expressions using DCNN with ETI-pooling. In\\n[66], Zhang et al. developed a broad learning system for\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 4\\nFER. A multilevel DCNN was developed to extract mid-\\nlevel and high-level features within facial images to solve\\nthe FER problem (Ensemble of MLCNNs) [67]. In [68], an\\nattentional DCNN named a Deep-Emotion to tackle the FER\\nproblem was devised. In [69], a deep AUs graph network\\nwas presented based on a psychological mechanism. In the\\nﬁrst step, the face image is divided into small key areas\\nusing segmentation techniques. Further, these key areas are\\nthen converted into corresponding AU-related facial expression\\nregions. Second, from these regions, local appearance features\\nwere extracted for further AUs analysis. Then, considering\\nAU related regions as vertices and distance between every\\ntwo landmarks as edges, AUs facial graph is constructed to\\nrepresent expressions. Finally, to learn hybrid features for FER\\nadjacency matrices of the facial graph are put into a graph-\\nbased convolutional neural network to combine the local-\\nappearance and global-geometry information. In [7], Kopaczka\\net al. presented a high-resolution thermal facial image database\\nfor the FER task. Besides, they extend existing approaches for\\ninfrared landmark detection with a head pose estimation for\\nimproved robustness and analyze the performance of a deep-\\nlearning method on this task.\\nIII. P ROPOSED METHOD\\nThis section presents a brief overview of an edge descriptor\\nof an image using GF followed by a detailed description of\\nour proposed DCNN for the FER task.\\nA. An edge descriptor\\nIn [70], Roy et al. stated that each pixel value of an image\\nis parallel to a universal body and therefore it is considered\\nas a mass of the body. The GF of an image is employed by\\nthe central pixel on its adjacent pixels. The Law of Universal\\nGravitation states that everybody mass (m 1) attracts every\\nother body mass (m 2) in the universe by a force pointing in a\\nstraight line (d ) between the centers-of-mass of both bodies,\\nand this force, GF, is proportional to the masses of the bodies\\nand inversely proportional to the square of their separation.\\nMathematically, GF is computed using Eq. 1.\\nGF=G(m1\\x02m2)\\nd2; (1)\\nwhere G is gravitational constant and its value is 6:67259 \\x02\\n10\\x0011. Let A be a gray-scale image. Let us consider a center\\npixel,Ac, of a local 3mask. It means Acis surrounded\\nby eight neighboring pixels, Ai. It is clear from the law of\\nuniversal gravitation that all the eight neighboring pixels, Ai,\\nexert forces on Ac. Thus, the force exerted on Acby theith\\nneighboring pixel can be represented by GFic. Then the x\\nand y components of GFicareGFicx=GFic\\x02sin\\x1e and\\nGFicy=GFic\\x02cos\\x1e respectively when GFicis at an angle\\nof\\x1ew. r. t. x-axis. The GFicxandGFicycan be computed\\nusing Eq. 2 and Eq. 3 respectively. Figure 2 shows an input\\nimage and edge strengths i.e. Ms in x and y directions.\\nGFicx=NX\\ni=1(GAc\\x03Ai\\nd2\\nic\\x02sin\\x1e ic); (2)GFicy=NX\\ni=1(GAc\\x03Ai\\nd2\\nic\\x02cos\\x1e ic); (3)\\nwhere N is the total number of neighboring pixels of a mask\\nandd2\\nicis squared Euclidean distance between the ithpixel and\\nthe center pixel. The GFicMandGFicDofGFicare calculated\\nby Eqs. 4 and 5 respectively.\\nGFicM=q\\n(GFicx)2+ (GF icy)2 (4)\\nGFicD=tan\\x001\\x12GFicy\\nGFicx\\x13\\n(5)\\nEqs. 4 and 5 can be used repeatably by considering every\\npixel as a center pixel to ﬁnd out M and D of gradients of a\\ngray-scale image, A.\\n \\n \\n  \\n  \\n           a) \\n            b) \\n            c) \\nFig. 2. a) A sample gray-scale image from JAFFE database b) M in x\\ndirection, and c) M in y direction\\nB. The architecture of the proposed DCNN\\nDCNNs learn features automatically and tend to describe\\nthe aimed task more accurately due to the parameters learning\\nby back-propagation from the loss function of the aimed task.\\nExisting DCNN based models such as VGG-16 and VGG-19,\\nare built on a single branch sequentially connected with con-\\nvolutional layers, and usually focus on homogeneously scaled\\nreceptive ﬁelds and ignore detailed edge information. Thus,\\nthey lack gathering adequate features of spatial structure for\\nfacial appearance. To address this problem multi convolutional\\nnetworks were introduced. In this section, we introduce a novel\\nDCNN for the FER task. The architecture of the proposed\\nDCNN is shown in Fig. 3. It consists of two branches. The\\nﬁrst branch is able to extract signiﬁcant local features like\\nedges, lines, curves, and corners from the M and D of an\\nimage as shallow DCNN is designed. On the other hand,\\nthe second branch is responsible for extracting the holistic\\nfeatures, which can differentiate one expression from others\\nsince major DCNN is considered. Since M and D of an image\\nare considered, the proposed DCNN is able to extract the\\nfeatures which are relevant to the individual expressions. The\\nﬁrst branch of DCNN consists of three convolutional layers,\\nwhich are connected sequentially, namely two max-pooling,\\none average pooling, and zero-padding, these are connected se-\\nquentially. On the other hand, the second branch of the DCNN\\nnetwork contains ﬁve convolutional, three max-pooling, one\\naverage-pooling, and up-sampling layer. Moreover, these two\\nbranches are concatenated and forwarded to the two dense\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 5\\n         \\n \\n      Final output \\nL1 \\nL11 \\n L12 \\n L13\\n L14 \\n L15 \\n L16 \\nL11 \\n L12 \\n L13\\n L14 \\n L15 \\n L16 \\nL1 \\nL21 \\nL22 \\n L23 \\n L24 \\n L25 \\n L26 \\nL27 \\n L28 \\nL29 \\nL21 \\nL22 \\n L23 \\n L24 \\n L25 \\n L26 \\nL27 \\n L28 \\nL29 \\nF \\n FC1 \\nFC2 \\nScore  \\n                                            Feature e xtraction  \\n         Classification  \\n Score level f usion  \\n \\n \\nMagnitude  \\n Direction  \\nFig. 3. The detailed DCNN architecture\\nlayers for the classiﬁcation of facial expressions. The detailed\\ndescription of each layer and its parameters are shown in Fig.\\n4. Moreover, to capture the enriched contextual information\\nﬁlters 5\\x025and4\\x024are employed. These ﬁlters allow the\\nnetwork to learn true edge variations. The biggest advantage\\nof convolutional layers is that it is able to extract the features\\nautomatically, kthconvolutional layer consists of nkfeature\\nmaps, denoted as Fk\\np, wherep= 1;2;3;:::nkandkrepresents\\na particular convolutional layer. Each feature map i.e, Fk\\x001\\nq,\\nwhereq= 1; 2;3;::::nk\\x001from the (k\\x001)thconvolutional\\nlayer is convolved with the ﬁlter Wk\\npqand biasbk\\npis added.\\nFurther, convolved feature maps are fed into the non-linear\\nactivation function Rectiﬁed Linear Unit (ReLu). Equation 6\\nshows how we can obtain a convolved feature map, Fk\\np.\\nFk\\np= \\x01 nk\\x001X\\nq=1Fk\\x001\\nq\\x03Wk\\npq+bk\\np!\\n;p= 1;2;3;:::nk;(6)\\nwhere \\x03indicates convolution operation. The responsibility\\nof an activation function is to rework the weighted sum of\\ninput from one node to a different activated nodes. Here, ReLU\\nis adopted because it can reduce the value of cost/loss function\\nby mitigating the vanishing gradient problem to some extent.\\nIt can compute faster and better performance on complex\\ndatabases [71]. The mathematical representation of the ReLU\\nactivation function is shown in Eq. 7. Max-pooling is applied\\nto convolved feature maps obtained by Eq. 6 to defeat the\\noverﬁtting problem by providing an abstracted style of the\\nrepresentation of the convolved feature maps. Max-pooling\\ncalculates the utmost value of every patch from each feature\\nmap to spotlight the foremost presented feature within thepatch. It also reduces the number of parameters in order to\\nmake the model simple. Moreover, it provides translation,\\nrotation, and scale-invariant feature maps.\\n\\x01 nk\\x001X\\nq=1Fk\\x001\\nq\\x03Wk\\npq+bk\\np!\\n=\\n=(0 if\\nPnk\\x001\\nq=1Fk\\x001\\nq\\x03Wk\\npq+bk\\np<0\\nPnk\\x001\\nq=1Fk\\x001\\nq\\x03Wk\\npq+bk\\np otherwise\\n(7)\\nThe feature maps, FM16andFM29, obtained from the\\nlayersL16andL29are concatenated. Then the concatenated\\nfeature maps FM16;FM29are ﬂattened by ﬂattening layer,\\nF, before feeding them as input, FC0into the ﬁrst fully-\\nconnected layer. The output, FC1, of the ﬁrst fully connected\\nlayer, is fed into the second fully connected layer to generate\\nFC2as output. The mathematical operation involved in two\\nfully connected layers is denoted by Eq. 8.\\nFCi=Wi\\x03FCi\\x001+bi;i= 1;2 (8)\\nwhereWiandbiare the weight and bias of the ithfully\\nconnected layer. It is observed from the experiments that\\nthe overﬁtting problem arises in both fully connected layers\\nas a number of learnable parameters are associated with\\nthem. In this work, the dropout technique is adopted to\\nresolve the overﬁtting problem that occurred in both the fully\\nconnected layers. The output of the second fully connected\\nlayer,FC2, is further fed into the softmax layer. The softmax\\nlayer consists of seven neurons and produces a probability\\nvector, ^y= [ ^y1;^y2;^y3;^y4;^y5;^y6;^y7]. The probability vector\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 6\\n     Layer              Type                                                                             Network Parameters  \\n         L1        Convolution  Filter size: 5 x 5       Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L11           Pooling                    Pool size: 3 x 3                                 Stride: 3 x 3          Padding: Valid      Pooling Type: Max -Pooling  \\n         L12        Convolution  Filter size: 4 x 4        Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L13           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  \\n         L14        Convolution  Filter size: 5 x 5       Fil ter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L15           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Vali d      Pooling Type: Average -Pooling  \\n         L16          padding                    Pad size: 4  x 4                                          -                                 -                   Padding  Type: Zero -padding  \\n         L21        Convolution  Filter size: 5 x 5       Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L22           Pooling                    Pool size: 5  x 5                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  \\n         L23        Convolution  Filter size: 4 x 4        Filter number: 32           Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L24           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  \\n         L25        Convolution  Filter size: 5 x 5       Filter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L26           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Valid      Pooling Type: Max -Pooling  \\n         L27        Convolution  Filter size: 5 x 5       Filter number: 64            Stride: 1 x 1          Padding: Same     Activation F unctio n = ReLU  \\n         L28           Pooling                    Pool size: 3 x 3                                 Stride: 2 x 2           Padding: Vali d      Pooling Type: Average -Pooling  \\n         L29        Up-sampling                    Sample  size: 4  x 4                                     -                                 -                  Sample Type: Up -sampling  \\n      L16, L29        Early Fusion                                                               Type:           ----     Concatenation  \\n         F          Flatten                                                                         -----------------------  \\n         FC1    Full Connection                           Number of Neurons = 1024                                                                   Dropout = 0.3  \\n         FC2    Full Connection                           Number of Neurons = 1024                                                                   Dropout = 0.3  \\n      Score         Output                                                                    Number of Neurons = 7                                                                 \\n Final Output   Score level Fusion                                                              Type:          -----     Late Score Level Fusion  \\n \\nFig. 4. Various parameters used in the proposed DCNN architecture shown in Fig. 3 and their values\\nconsists of seven probability values as seven classes of facial\\nexpressions are considered in this study. The kthprobability\\nvalue is obtained by Eq. 9.\\n^yk=eFC2\\nk\\nP7\\nk=1eFC2\\nk;k= 1;2;::::; 7: (9)\\n1) Network training: The proposed network trains inde-\\npendently on M and D of gradients of facial images and\\nestimates the probability of each class. The proposed network\\nweights are initialized using glorot uniform method and adam\\noptimization is employed to prevent local optimum [72], along\\nwith learning rate decay is introduced to advance the training\\neffect. In adam optimization, initial learning rate and learning\\nrate decay are assigned as 0.00001 and 1e-4 respectively.\\nCategorical cross-entropy is exploited to ﬁnd the loss for\\nmulticlass classiﬁcation and it is a measure to quantify the\\nerror of our model. The categorical cross-entropy is computed\\nusing Eq. 10.\\n (y;^y) =\\x001\\n77X\\nj=1yjlog ^yj; (10)\\nwhere,y(= [y1;y2;y3;y4;y5;y6;y7])is one-hot encoding\\nvector of the actual labels. Batch size 16 is considered while\\ntraining the proposed DCNN since the network can occupy\\nless memory in our system and 60 epochs are considered for\\nJAFFE, CK+, KDEF, and RAF databases while 200 epochs\\nare used for FER2013.\\n2) Score-level fusion: To estimate the ﬁnal prediction of\\nseven basic expressions, the score-level fusion technique [73]\\nis performed on M and D of gradients. Mathematically, score-\\nlevel fusion is done by Eq. 11.SFi= arg max\\ncNX\\nj=1\\x0bjPijc; (11)\\nwherecindicates the various expressions, and that irepresents\\nthe input sample, and Nindicates the two different modalities,\\nin this study MandDof the input image, are considered as\\ntwo different modalities. Pijccould be a prediction probability\\nto belong to a class cfor input sample iof modality j.\\nThe worth of \\x0bjis chosen by searching values from 0 to\\n5 with a step size of 0.2. Score-level fusion is simpler to\\nweigh individual scores of modalities and it gave a better\\nperformance on the FER task.\\nIV. E XPERIMENTAL RESULTS AND DISCUSSION\\nA. Environment settings\\nIn this study, the Keras framework and Anaconda de-\\nvelopment platform are considered for training and testing\\nthe proposed model. Python language is used since many\\nof the deep-learning libraries are developed using it. The\\nspeciﬁcations of the system are reported in Table I.\\nTABLE I\\nSPECIFICATION OF THE SYSTEM\\nName Parameter\\nGPU RAM 16GB\\nGraphics processor NVIDIA Quadro P5000\\nCuda cores 2560\\nMemory interface 256-bit\\nMemory type GDDR5X\\nBandwidth 288.5 GB/s\\nLanguage Python\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 7\\nB. Database description\\nIn this work, ﬁve well-known benchmark databases namely,\\nFER2013 [32], JAFFE [33], CK+ [34], KDEF [35], and RAF\\n[36], [37] are considered for the evaluation of the proposed\\nnetwork because these databases contain seven basic universal\\nfacial expressions namely, NE, AN, DI, FE, HA, SA, and SU.\\nThe upper part of Table II describes the statistical information\\nof the aforementioned databases.\\nTABLE II\\nSTATISTICAL INFORMATION OF THE DATABASES\\nDatabase\\nImage SizeNumber of imagesTotalNE\\nAN DI FE HA SA SU\\nBefore\\nAugmentation\\nFER2013 48\\x0248 6183\\n4916 545 5089 8977 6069 3993 35772\\nJ\\nAFFE 256\\x02256 30 30 29 32 31 31 30 213\\nCK+ 256\\x02256 50\\n47 61 24 59 28 62 331\\nKDEF 256\\x02256 70\\n70 70 70 70 70 70 490\\nRAF 100\\x02100 3204\\n867 877 355 5957 2460 1463 15183\\nAfter\\nAugmentation\\nFER2013 256\\x02256 6183\\n4916 2725 5089 8977 6069 3993 37952\\nJ\\nAFFE 256\\x02256 150 150 145 160 155 155 150 1065\\nCK+ 256\\x02256 250\\n235 305 120 295 140 310 1655\\nKDEF 256\\x02256 350\\n350 350 350 350 350 350 2450\\nRAF 256\\x02256 3204\\n4335 4385 1755 5957 2460 1463 23559\\nC. Data augmentation\\nHaving a large database is important for measuring the\\nperformance of a DCNN model. Moreover, we can prevent a\\nDCNN model from learning irrelevant features because irrele-\\nvant or partially relevant features can negatively impact model\\nperformance. However, the performance can be improved to\\nsome extent by augmenting the data we already have. A\\nDCNN can be invariant to translation, viewpoint, size, or\\nillumination. So, some of the image processing techniques like\\nthe rotation of images by 5\\x0eclockwise and anti-clockwise\\ndirections, horizontal ﬂip, and adding Gaussian noise are\\nconsidered to extend the total number of images of seven facial\\nexpressions of JAFFE, CK+, and KDEF databases to increase\\nthe diversity of these databases. On the other hand, augmenta-\\ntion is done on DI facial expression of FER2013 and AN, DI,\\nand FE facial expressions of RAF only due to their imbalance\\nclasses. The lower half of Table II describes the statistical\\ninformation of ﬁve databases after data augmentation. Further,\\neach database is divided into three parts such as training,\\nvalidation, and testing. A tenfold cross-validation technique\\nis adopted for all the experiments to evaluate the performance\\nof the proposed method. In other words, out of ten subsets,\\n8 subsets are used for training, 1 subset is considered for\\nvalidation, and the rest of the one subset is adopted for testing.\\nThe average classiﬁcation results are reported in this study.\\nTable III shows the number of facial images used in the\\ntraining, validation, and testing processes for each fold.\\nD. Experimental results using M\\nIn the ﬁrst experiment, only M followed by the proposed\\nDCNN is considered. In other words, the upper half of the\\nproposed model is only used for training and validation. So,\\nthe upper half of the model is implemented in the above men-\\ntioned ﬁve databases. Figure 5 shows training and validation\\nperformances with respect to the epoch on each database. TheTABLE III\\nTRAIN ,VALIDATION ,AND TEST SPLIT FOR THE FIVE DATABASES\\nDatabase Train Validation Test\\nFER2013 30362 3795 3795\\nJAFFE 851 107 107\\nCK+ 1323 166 166\\nKDEF 1960 245 245\\nRAF 18847 2356 2356\\n \\n \\n \\n \\n \\n \\n  \\n    a) \\n    b) \\n     c) \\n    d) \\n     e) \\nFig. 5. Training and validation performances using M of gravitational force\\non ﬁve databases namely, a) FER2013, b) JAFFE, c) CK+, d) KDEF, and e)\\nRAF\\naverage testing accuracy and average loss on each database\\nare reported in Table IV. It is clear from Table IV that results\\nare very good on three databases namely, JAFFE, CK+, and\\nKDEF. However, the results on FER2013 and RAF databases\\nare relatively poor, but these could be accepted.\\nTABLE IV\\nTESTING ACCURACY AND LOSS WHEN MOF GRAVITATIONAL FORCE IS\\nUSED\\nDatabase Testing Accuracy Loss\\nFER2013 77.10% 0.4204\\nJAFFE 95.63% 0.1602\\nCK+ 97.99% 0.0627\\nKDEF 96.36% 0.1485\\nRAF 82.33% 0.3820\\nE. Experimental results using D\\nIn the second experiment, only D followed by the proposed\\nDCNN is adopted. In other words, the lower half of the\\nproposed model is only considered for training and validation.\\nThus, the lower half of the model is executed on the above\\nmentioned ﬁve databases. Figure 6 shows training and valida-\\ntion accuracies with respect to iteration on each database. The\\naverage testing accuracy and average loss on each database are\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 8\\nTABLE V\\nTESTING ACCURACY AND LOSS WHEN DOF GRAVITATIONAL FORCE IS\\nCONSIDERED\\nDatabase Testing Accuracy Loss\\nFER2013 76.27% 0.4589\\nJAFFE 94.38% 0.1988\\nCK+ 96.79% 0.0685\\nKDEF 94.12% 0.2040\\nRAF 79.27% 0.3910\\nnoted in Table V. It is observed from Table V that the results\\nfollow the same trend as Table IV. However, the performance\\nis deteriorated when D is used followed by the proposed\\nDCNN.\\n \\n \\n \\n \\n \\n \\n  \\n    a) \\n    b) \\n     c) \\n    d) \\n     e) \\nFig. 6. Training and validation performances using D of gravitational force\\non ﬁve databases namely, a) FER2013, b) JAFFE, c) CK+, d) KDEF, and e)\\nRAF\\nF . Experimental results using both M and D\\nIn the third experiment, the complete model depicted in Fig.\\n3 is used. The proposed model is run on ﬁve databases. Figures\\n7 and 8 display the intermediate features maps obtained by\\nthe proposed DCNN. Figures 9, 10, 11, 12, and 13 show\\nthe one out of ten confusion matrices and the classiﬁcation\\nreport obtained from the given confusion matrix on FER2013,\\nJAFFE, CK+, KDEF, and RAF databases respectively. The\\nreported classiﬁcation report of each database is obtained by\\naveraging the classiﬁcation reports of 10-folds separately. The\\naverage accuracy obtained by the proposed model on ﬁve\\ndatabases is reported in the last row of Table VIII.\\nG. Model Analysis\\nThe proposed method consists of two steps. Extraction of\\nedge information using the GF feature descriptor is done in\\nthe ﬁrst step, whereas the proposed DCNN tunes the edge\\n \\n \\n \\n \\n \\nFig. 7. Feature maps of the proposed DCNN, the primary row represents 1st\\nbranch followed by the 2ndbranch before the secondary pooling layer\\n \\n \\n \\n \\n \\nFig. 8. Feature maps of proposed DCNN, the primary row represents 1st\\nbranch and followed by 2ndbranch before the secondary pooling layer\\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE       \\n39         \\n0          \\n0         \\n1         \\n0          \\n0         \\n0 \\n        \\nAN                \\n0        \\n40         \\n0         \\n1         \\n0         \\n0         \\n0 \\n      \\nDI        \\n0         \\n0        \\n43         \\n0         \\n0         \\n1         \\n0 \\n                                                                                                                                                                                                              \\nFE        \\n0         \\n0         \\n0        \\n55         \\n0         \\n1         \\n0 \\n      \\nHA        \\n0         \\n0         \\n0         \\n0        \\n64         \\n1         \\n0 \\n      \\nSA        \\n0         \\n0         \\n2         \\n0         \\n0        \\n50         \\n0 \\n      \\nSU        \\n0         \\n0         \\n0         \\n3         \\n0         \\n0        \\n56 \\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE     \\n460                              \\n 32          \\n  2          \\n 13          \\n 35          \\n 63          \\n  6 \\n        \\nAN                 \\n 31         \\n362          \\n 14          \\n 28          \\n 28          \\n 26          \\n  9 \\n      \\nDI         \\n  2          \\n  3        \\n232          \\n 15          \\n 14          \\n  3          \\n  4 \\n                                                                                                                                                                                                              \\nFE         \\n 32          \\n 34          \\n  9         \\n342          \\n 17          \\n 39          \\n 36 \\n      \\nHA         \\n 30          \\n 19          \\n  3          \\n 14         \\n794          \\n 20          \\n 18 \\n      \\nSA         \\n 70          \\n 32          \\n  9          \\n 45          \\n 29         \\n416          \\n  6 \\n      \\nSU         \\n  7          \\n  7          \\n  5          \\n 17          \\n  8          \\n  5         \\n350 \\n \\n   \\n \\nClass es         \\n      \\nAccuracy                              \\n      \\nPrecision                   \\n    \\nRecall                    \\n        \\nF1-Score                    \\n            \\nNE  \\n   0.76   \\n    0.73    \\n  0.76   \\n    0.75  \\n             \\nAN  \\n   0.74  \\n    0.73   \\n  0.74    \\n     0.74  \\n             \\nDI  \\n   0.85   \\n    0.82   \\n  0.89   \\n     0.85  \\n                                                                                                                                                                                                              \\nFE  \\n   0.67   \\n    0.75  \\n  0.68  \\n     0.71  \\n            \\nHA  \\n   0.88   \\n    0.85   \\n  0.88   \\n     0.86  \\n             \\nSA  \\n   0.69   \\n    0.73   \\n  0.76   \\n     0.75  \\n                \\nSU  \\n   0.89     \\n    0.88   \\n  0.89   \\n     0.88  \\n \\nFig. 9. Performance in terms of confusion matrix and classiﬁcation report on\\nFER2013 database\\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE       \\n39         \\n0          \\n0         \\n1         \\n0          \\n0         \\n0 \\n        \\nAN                \\n0        \\n40         \\n0         \\n1         \\n0         \\n0         \\n0 \\n      \\nDI        \\n0         \\n0        \\n43         \\n0         \\n0         \\n1         \\n0 \\n                                                                                                                                                                                                              \\nFE        \\n0         \\n0         \\n0        \\n55         \\n0         \\n1         \\n0 \\n      \\nHA        \\n0         \\n0         \\n0         \\n0        \\n64         \\n1         \\n0 \\n      \\nSA        \\n0         \\n0         \\n2         \\n0         \\n0        \\n50         \\n0 \\n      \\nSU        \\n0         \\n0         \\n0         \\n3         \\n0         \\n0        \\n56 \\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE           \\n 15                              \\n  0          \\n  0          \\n  0          \\n  0          \\n  0          \\n  2 \\n        \\nAN                 \\n  0         \\n 15          \\n  0          \\n  0          \\n  0          \\n  0          \\n  0 \\n      \\nDI         \\n  0          \\n  0          \\n 19          \\n  0          \\n  0          \\n  0          \\n  0 \\n                                                                                                                                                                                                              \\nFE         \\n  0          \\n  0          \\n  0         \\n 12          \\n  0          \\n  0          \\n  0 \\n      \\nHA         \\n  0          \\n  0          \\n  0          \\n  0         \\n 11          \\n  0          \\n  0 \\n      \\nSA         \\n  0          \\n  0          \\n  0          \\n  0          \\n  0         \\n 18          \\n  1 \\n      \\nSU         \\n  0          \\n  0          \\n  0          \\n  0          \\n  0          \\n  0         \\n 14 \\n \\n   \\n \\nClass es         \\n      \\nAccuracy                              \\n      \\nPrecision                   \\n    \\nRecall                    \\n        \\nF1-Score                    \\n            \\nNE  \\n   1.00   \\n    0.94    \\n  1.00   \\n    0.97  \\n             \\nAN  \\n   1.00   \\n    0.96   \\n  1.00    \\n     0.98  \\n             \\nDI  \\n   0.961   \\n    1.00   \\n  0.92   \\n     0.96  \\n                                                                                                                                                                                                              \\nFE  \\n   1.00  \\n    1.00   \\n  1.00   \\n     1.00  \\n            \\nHA  \\n   1.00   \\n    0.86   \\n  1.00   \\n     0.91  \\n             \\nSA  \\n   0.903   \\n    0.96   \\n  0.84   \\n     0.90  \\n                \\nSU  \\n   1.00     \\n    1.00   \\n  1.00   \\n     1.00  \\n \\nFig. 10. Performance in terms of confusion matrix and classiﬁcation report\\non JAFFE database\\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE       \\n39         \\n0          \\n0         \\n1         \\n0          \\n0         \\n0 \\n        \\nAN                \\n0        \\n40         \\n0         \\n1         \\n0         \\n0         \\n0 \\n      \\nDI        \\n0         \\n0        \\n43         \\n0         \\n0         \\n1         \\n0 \\n                                                                                                                                                                                                              \\nFE        \\n0         \\n0         \\n0        \\n55         \\n0         \\n1         \\n0 \\n      \\nHA        \\n0         \\n0         \\n0         \\n0        \\n64         \\n1         \\n0 \\n      \\nSA        \\n0         \\n0         \\n2         \\n0         \\n0        \\n50         \\n0 \\n      \\nSU        \\n0         \\n0         \\n0         \\n3         \\n0         \\n0        \\n56 \\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE           \\n 26                              \\n  1          \\n  0          \\n  0          \\n  0          \\n  0          \\n  0 \\n        \\nAN                 \\n  0         \\n 17          \\n  1          \\n  0          \\n  0          \\n  0          \\n  0 \\n      \\nDI         \\n  1          \\n  0          \\n 29          \\n  0          \\n  0          \\n  0          \\n  0 \\n                                                                                                                                                                                                              \\nFE         \\n  1          \\n  0          \\n  0         \\n 20          \\n  0          \\n  0          \\n  0 \\n      \\nHA         \\n  0          \\n  0          \\n  0          \\n  0         \\n 29          \\n  0          \\n  0 \\n      \\nSA         \\n  0          \\n  0          \\n  0          \\n  0          \\n  0         \\n 12          \\n  0 \\n      \\nSU         \\n  0          \\n  0          \\n  0          \\n  0          \\n  0          \\n  0         \\n 29 \\n \\n   \\n \\nClass es         \\n      \\nAccuracy                              \\n      \\nPrecision                   \\n    \\nRecall                    \\n        \\nF1-Score                    \\n            \\nNE  \\n   0.933   \\n    1.00    \\n  0.90   \\n    0.95  \\n             \\nAN  \\n   1.00   \\n    0.98   \\n  1.00    \\n     0.99  \\n             \\nDI  \\n   1.00   \\n    1.00   \\n  1.00   \\n     1.00  \\n                                                                                                                                                                                                              \\nFE  \\n   0.954   \\n    1.00   \\n  0.90   \\n     0.95  \\n            \\nHA  \\n   1.00   \\n    0.98   \\n  1.00   \\n     0.99  \\n             \\nSA  \\n   1.00   \\n    0.88   \\n  1.00   \\n     0.94  \\n                \\nSU  \\n   1.00     \\n    0.98   \\n  1.00   \\n     0.99  \\n \\nFig. 11. Performance in terms of confusion matrix and classiﬁcation report\\non CK+ database\\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE       \\n39         \\n0          \\n0         \\n1         \\n0          \\n0         \\n0 \\n        \\nAN                \\n0        \\n40         \\n0         \\n1         \\n0         \\n0         \\n0 \\n      \\nDI        \\n0         \\n0        \\n43         \\n0         \\n0         \\n1         \\n0 \\n                                                                                                                                                                                                              \\nFE        \\n0         \\n0         \\n0        \\n55         \\n0         \\n1         \\n0 \\n      \\nHA        \\n0         \\n0         \\n0         \\n0        \\n64         \\n1         \\n0 \\n      \\nSA        \\n0         \\n0         \\n2         \\n0         \\n0        \\n50         \\n0 \\n      \\nSU        \\n0         \\n0         \\n0         \\n3         \\n0         \\n0        \\n56 \\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE           \\n 36                              \\n  0          \\n  0          \\n  0          \\n  0          \\n  0          \\n  0 \\n        \\nAN                 \\n  0         \\n 30          \\n  1          \\n  2          \\n  0          \\n  0          \\n  0 \\n      \\nDI         \\n  0          \\n  1          \\n 29          \\n  0          \\n  0          \\n  0          \\n  1 \\n                                                                                                                                                                                                              \\nFE         \\n  1          \\n  0          \\n  0         \\n 35          \\n  0          \\n  1          \\n  1 \\n      \\nHA         \\n  0          \\n  0          \\n  0          \\n  0         \\n 44          \\n  0          \\n  0 \\n      \\nSA         \\n  2          \\n  0          \\n  0          \\n  0          \\n  0         \\n 28          \\n  0 \\n      \\nSU         \\n  0          \\n  0          \\n  0          \\n  2          \\n  0          \\n  0         \\n 31 \\n \\n   \\n \\nClass es         \\n      \\nAccuracy                              \\n      \\nPrecision                   \\n    \\nRecall                    \\n        \\nF1-Score                    \\n            \\nNE  \\n   0.975   \\n    0.97    \\n  0.95   \\n    0.96  \\n             \\nAN  \\n   0.975   \\n    1.00   \\n  0.98    \\n     0.99  \\n             \\nDI  \\n   0.977   \\n    0.96   \\n  0.98   \\n     0.97  \\n                                                                                                                                                                                                              \\nFE  \\n   0.982   \\n    0.92   \\n  0.96   \\n     0.94  \\n            \\nHA  \\n   0.984   \\n    1.00   \\n  0.98   \\n     0.99  \\n             \\nSA  \\n   0.961   \\n    0.91   \\n  0.94   \\n     0.92  \\n                \\nSU  \\n   0.949     \\n    1.00   \\n  0.95   \\n     0.97  \\n \\nFig. 12. Performance in terms of confusion matrix and classiﬁcation report\\non KDEF database\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 9\\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE       \\n39         \\n0          \\n0         \\n1         \\n0          \\n0         \\n0 \\n        \\nAN                \\n0        \\n40         \\n0         \\n1         \\n0         \\n0         \\n0 \\n      \\nDI        \\n0         \\n0        \\n43         \\n0         \\n0         \\n1         \\n0 \\n                                                                                                                                                                                                              \\nFE        \\n0         \\n0         \\n0        \\n55         \\n0         \\n1         \\n0 \\n      \\nHA        \\n0         \\n0         \\n0         \\n0        \\n64         \\n1         \\n0 \\n      \\nSA        \\n0         \\n0         \\n2         \\n0         \\n0        \\n50         \\n0 \\n      \\nSU        \\n0         \\n0         \\n0         \\n3         \\n0         \\n0        \\n56 \\n \\n        \\nNE       \\nAN       \\nDI       \\nFE       \\nHA       \\nSA       \\nSU \\n      \\nNE           \\n218                              \\n  7          \\n 10          \\n  3          \\n 20          \\n 45          \\n 17 \\n        \\nAN                 \\n  2         \\n396          \\n 12          \\n  1          \\n 14          \\n  3          \\n  2 \\n      \\nDI         \\n 11          \\n  7         \\n401          \\n  2          \\n 11          \\n  4          \\n  3 \\n                                                                                                                                                                                                              \\nFE         \\n  4          \\n  3          \\n  3         \\n160          \\n  2          \\n  4          \\n  0 \\n      \\nHA         \\n  20          \\n 12          \\n 10          \\n  4         \\n539          \\n  9          \\n  3 \\n      \\nSA         \\n 30          \\n 12          \\n 12          \\n  7          \\n 15         \\n169          \\n  2 \\n      \\nSU         \\n 20          \\n  2          \\n  2          \\n  5          \\n  4          \\n  3         \\n111 \\n \\n   \\n \\nClass es         \\n      \\nAccuracy                              \\n      \\nPrecision                   \\n    \\nRecall                    \\n        \\nF1-Score                    \\n            \\nNE  \\n   0.697   \\n    0.68    \\n  0.69   \\n    0.68  \\n             \\nAN  \\n   0.940   \\n    0.90   \\n  0.94    \\n     0.92  \\n             \\nDI  \\n   0.927   \\n    0.88   \\n  0.93   \\n     0.90  \\n                                                                                                                                                                                                              \\nFE  \\n   0.916   \\n    0.89   \\n  0.90   \\n     0.89  \\n            \\nHA  \\n   0.906  \\n    0.89   \\n  0.90   \\n     0.89 \\n             \\nSA  \\n   0.684   \\n    0.66   \\n  0.67   \\n     0.66  \\n                \\nSU  \\n   0.765     \\n    0.78   \\n  0.71   \\n     0.74  \\n \\nFig. 13. Performance in terms of confusion matrix and classiﬁcation report\\non RAF database\\ninformation to extract local as well as holistic features in the\\nsecond step. In [31], Roy and Bhattacharjee already showed\\nthat the GF feature descriptor performs better than other edge\\ninformation extraction techniques. So, we have not conducted\\nthe same experiment again. However, the performance of\\nthe GF feature extractor is compared with two well-known\\ntexture measures named as Local Binary Pattern (LBP) and\\nGabor face descriptor in this study. A face image of size\\n256\\x02256 pixels is fed as an input to the above mentioned\\nthree descriptors separately and they produce a feature vector\\nof size 1\\x0265536 as an output. Then the obtained feature\\nvector is mapped into a d-dimensional polyhedron to get a\\npoint, where the value of d is 256\\x02256 = 65536 . This process\\nwould be repeated for all the face images of a database. We\\nwill get a d-dimensional polyhedron at the end of this process,\\nwhere a face image would be represented as a point. A feature\\ndescriptor is not good enough if the points of two classes\\nare highly overlapping. When the points of the two classes\\nare highly overlapping then the accuracy would decrease.\\nHere, the overlap is computed based on compactness and\\nseparation. Compactness and separation deﬁne the quality of\\nclustering results. A cluster has good compactness when points\\nare close to each other and good separation when clusters do\\nnot overlap. In other words, the ideal values of compactness\\nand separation are zero and inﬁnity respectively. Initially, k-\\nmeans is applied to the points to divide them into k= 7\\nclusters as 7 basic expressions are considered in this study.\\nThe values of compactness and separation are computed for\\nthe three above mentioned feature descriptors on ﬁve databases\\nthat are reported in Table VI.\\nTABLE VI\\nCOMPARISON OF GRAVITATIONAL FORCE DESCRIPTOR WITH OTHER\\nDESCRIPTORS\\nDatabaseLBP\\nGobar Filter Gravitational Force\\ncompactness\\nseparation compactness separation compactness separation\\nFER2013\\n2.1007 11.1001 1.6723 13.3321 0.3792 17.2005\\nJ\\nAFFE 2.5133 16.9478 0.3947 18.3839 0.2783 32.0319\\nCK+\\n0.9196 24.2100 0.4641 22.5207 0.2941 33.2091\\nKDEF\\n1.7509 18.0879 0.1164 11.9973 0.0976 28.9084\\nRAF\\n1.4702 7.8901 0.9493 9.4510 0.2301 13.0923\\nIt is noticed from Table VI that the value of compactness of\\nthe GF is less as compared to LBP and Gabor face descriptor.\\nOn the other hand, the value separation of the GF is the\\nhighest among the three feature descriptors. Thus, we can\\nconclude that the GF is better compared to LBP and Gabor\\nface descriptor. In other words, the feature vector is more\\ninformative and is able to distinguish a facial expression fromothers when GF is used. The same experiment is conducted\\nafter extracting features by the proposed DCNN and other\\nstate-of-the-art models and the values of compactness and\\nseparation are noted in Table VII.\\nTwo conclusions can be drawn from Table VII: GF and the\\nproposed DCNN jointly generates features, which have more\\ndistinctive capabilities than the features produced by the GF\\nalone as the value of compactness is less and the value of\\nseparation is high. We can thus state that GF followed by the\\nproposed DCNN is better than the state-of-the-art methods for\\nthe same reason.\\nH. Comparative results\\nIn the last experiment, we provide comparative results\\nagainst twenty-ﬁve state-of-the-art algorithms, for example,\\nHOG-TOP [42], Shallow CNN [45], Major CNN [45], Shallow\\nCNN on LBP images [50], Shallow CNN on gray-scale images\\n[50], Partial VGG16 [50], Weighted mixture of double channel\\n[50], Weighted fusion of three subnetworks [52], Appearance-\\nbased CNN on LBP images [54], Fusion of appearance and\\ngeometric features [54], 3D Inception-ResNet + landmarks\\n[56], Autoencoders + SOM [57], Spatio-temporal feature\\n+ LSTM [58], Ensemble DCNNs [59], DCNN for binary\\nclassiﬁcation [60], IACNN [61], 2B(N+M)Softmax [62], S-\\nDSRN [63], Color features + Gabor transform [64], DCMA-\\nCNNs [65], Broad Learning [66], Ensemble MLCNNs [67],\\nDeep-Emotion [68], VGG19 [47], and ResNet150 [47], on\\nﬁve publicly available databases. However, the comparison is\\ndone based on average recognition accuracy only. Some of the\\nabove-mentioned methods were implemented on videos. Few\\nworks considered less number of classes. So, we change a few\\nof these algorithms for this study by keeping the overall archi-\\ntecture the same. Table VIII shows the average classiﬁcation\\naccuracies achieved by the above-said state-of-the-art methods.\\nIt is clear from Table VIII that the proposed model defeats\\nall the twenty-ﬁve existing methods on ﬁve databases and it\\nhappens due to the use of GF-based local edge features along\\nwith holistic features extracted by the proposed DCNN, which\\nis our main focus. However, all the methods are also compared\\nbased on training and testing times. Generally, the training\\ntime of a method depends on the size of the network, size\\nof the input, number of epochs, number of folds, and others.\\nIn this study, all the models are implemented according to\\ntheir respective speciﬁcations. However, we consider 10-folds\\ncross-validation and 60 epochs while training the proposed\\nmethod on all the databases except FER2013. The proposed\\nmethod is trained for 200 epochs for the FER2013 database\\nonly. The training and testing times required by all the state-\\nof-the-art methods including the proposed method on all the\\nﬁve databases are reported in Table 8. However, testing time\\nfor one-fold cross-validation is noted only in Table 8. Testing\\ntime per image (TTPI) is the same for all the images of a\\ndatabase as their size is equal. However, it varies from one\\nmethod to another. It is clear from Table 8 that the proposed\\nDCNN takes an average training and testing time across all\\nthe databases except the FER2013 database. The proposed\\nmethod takes about 960 minutes to train the proposed method\\nfor FER2013, which is quite large.\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 10\\nTABLE VII\\nFEATURE EXTRACTION ANALYSIS OF ALL THE COMPARED METHODS AND PROPOSED ONE FOR ALL THE FIVE DATABASES\\nMethodFER2013\\nJAFFE CK+ KDEF RAF\\ncompactness\\nseparation compactness separation compactness separation compactness separation compactness separation\\nHOG-T\\nOP [42] 6.3401 11.9010 11.4307 18.1801 9.1060 24.1107 12.4987 18.2312 5.9019 9.2809\\nShallo\\nw CNN [45] 5.2903 13.6850 13.1721 16.8956 10.1040 21.1652 12.9472 17.9843 2.4212 13.5219\\nMajor\\nCNN [45] 3.2760 16.0395 7.2167 19.9009 4.3400 36.6180 9.8661 23.2107 1.0192 15.9305\\nShallo\\nw CNN on LBP images [50] 5.0081 12.5627 3.3133 23.6127 4.9092 34.4410 8.3283 25.9169 2.8358 13.9102\\nShallo\\nw CNN on gray-scale images [50] 2.0843 18.8401 2.1050 27.1873 2.0969 41.9901 6.3166 28.0498 1.1023 15.8001\\nP\\nartial VGG16 [50] 2.1037 18.9823 1.6430 31.0262 3.0974 38.5981 6.0116 29.0963 4.6783 10.3281\\nW\\neighted mixture of double channel [50] 0.1702 20.9437 2.0051 27.9642 1.1268 46.0533 4.2403 31.1106 1.7281 14.1421\\nW\\neighted fusion of three subnetworks [52] 3.0103 15.6149 2.7613 26.2167 3.1943 40.9165 7.2389 26.9901 2.3929 13.6780\\nAppearance\\nbased CNN on LBP images [54] 7.9823 10.6843 2.9124 26.8913 1.8412 43.0935 8.9982 22.9307 1.5816 14.4389\\nFusion\\nof appearance and geometric features [54] 5.8124 12.0844 2.0962 27.6893 2.0082 42.2190 8.6826 24.2411 1.0741 16.0381\\n3D\\nInception-ResNet + landmarks [56] 2.1862 19.0125 1.9863 28.3013 2.5137 40.9810 3.9198 32.0250 2.4927 13.4617\\nAutoencoders\\n+ SOM [57] 1.9810 19.5213 1.8047 29.1677 2.4176 41.0869 3.9826 32.6810 1.9074 14.4837\\nSpatio-temporal\\nfeature + LSTM [58] 2.3814 18.9046 2.5913 26.9033 5.1905 32.1890 4.3001 31.0241 2.6481 14.0102\\nEnsemble\\nDCNNs [59] 5.6321 12.8610 12.1622 17.4612 8.9567 26.6010 11.1725 19.4607 2.4217 13.6550\\nDCNN\\nfor binary classiﬁcation [60] 7.1027 11.0913 12.8907 17.9917 6.0230 30.3851 7.9604 26.0021 3.0429 12.3489\\nIA\\nCNN [61] 2.9467 19.8437 6.3401 21.2157 1.8102 43.1880 9.6007 24.0420 1.8489 14.1013\\n2B(N+M)Softmax\\n[62] 2.7890 19.5246 5.8804 21.9672 4.0801 36.7013 4.2509 30.9103 3.0134 12.6581\\nS-DSRN\\n[63] 2.0133 19.0081 2.2081 27.8534 3.0162 39.4693 3.8964 32.8158 1.7618 14.1023\\nColor\\nfeatures + Gabor transform [64] 2.6448 18.6162 6.0182 20.6420 3.9180 36.9401 4.8321 30.9903 2.5001 13.2978\\nDCMA-CNNs\\n[65] 1.8844 19.8916 1.8441 30.8757 2.4503 41.2251 3.7987 32.6380 1.0127 16.4190\\nBroad\\nLearning [66] 8.9162 10.2761 1.9962 29.0123 5.2629 31.0829 2.0312 38.1023 3.7320 11.8902\\nEnsemble\\nMLCNNs [67] 1.2491 20.0617 1.8001 29.8962 2.4984 41.0012 2.6512 33.8138 0.9326 16.8721\\nDeep-Emotion\\n[68] 2.5193 19.2013 1.9125 28.9102 2.2630 41.8230 4.3612 31.0883 2.5216 14.1852\\nV\\nGG19 [47] 1.1347 20.1933 1.3237 30.9817 1.4866 44.1805 4.3791 31.1990 2.9987 12.9029\\nResNet150\\n[47] 0.1672 20.0347 2.4340 27.1583 2.8682 38.0113 7.5981 26.0894 2.4901 13.4823\\nPr\\noposed method 0.0916 23.4823 0.1525 36.3913 0.2760 51.9503 0.0844 46.5627 0.0727 17.1048\\nTABLE VIII\\nCLASSIFICATION ACCURACY (%) AND EXECUTION TIME IN SECONDS ON FIVE DATABASES VIZ . FER2013, JAFFE, CK+, KDEF, AND RAF BY VARIOUS\\nMETHODS\\nClassiﬁcation\\nAccuracy (%) on Five Databases Execution Time in Minutes\\nSl.\\nNo. Method FER2013 JAFFE CK+ KDEF RAF TTPITraining Time Testing Time for all the Images\\nFER2013\\nJAFFE CK+ KDEF RAF FER2013 JAFFE CK+ KDEF RAF\\n1.\\nHOG-TOP [42] 52 60 65 55 53 0.4500 220.0 86.0 92.33 115.0 165.0 2.1623 1.2124 1.3264 1.1576 2.1529\\n2.\\nShallow CNN [45] 55 50 61 55 70 0.1812 5.5 1.0 1.5 2.5 4.0 0.9992 0.8845 0.8985 0.9001 0.9845\\n3.\\nMajor CNN [45] 64 68 85 67 78 0.1872 9.0 1.5 2.0 2.5 6.0 1.1060 0.8883 0.9127 0.9168 1.0921\\n4.\\nShallow CNN on LBP images [50] 54 88 83 70 71 0.1801 75.0 8.33 16.33 25.0 41.66 1.0721 0.8821 0.8991 0.9008 0.9698\\n5.\\nShallow CNN on gray-scale images [50] 72 92 94 78 78 0.1801 75.0 8.33 16.33 25.0 41.66 1.0721 0.8821 0.8991 0.9008 0.9698\\n6.\\nPartial VGG16 [50] 72 96 91 78 57 0.4309 441.66 150.0 158.33 175.0 316.66 1.9348 1.4508 1.5238 1.5523 1.8900\\n7.\\nWeighted mixture of double channel [50] 75 92 97 81 75 0.4699 516.66 158.33 174.99 200.0 358.32 2.2602 1.9665 1.9789 1.9965 2.1056\\n8.\\nWeighted fusion of three subnetworks [52] 63 90 91 74 70 0.8956 150.0 9.63 10.0 16.6 70.0 2.2012 1.9804 1.9912 1.9989 2.1020\\n9.\\nAppearance based CNN on LBP images [54] 50 90 95 66 77 0.3945 105.0 10.0 20.0 30.0 70.0 1.3563 1.1230 1.1321 1.1394 1.2953\\n10.\\nFusion of appearance and geometric features [54] 53 90 94 69 78 0.4612 280.0 70.0 110.0 135.0 210.0 2.3906 1.9023 1.9984 2.0102 2.2134\\n11.\\n3D Inception-ResNet + landmarks [56] 72 93 93 83 70 0.8926 340.0 110.0 140.0 186.0 213.0 2.3472 1.9245 1.8612 1.9946 2.1980\\n12.\\nAutoencoders + SOM [57] 73 93 94 84 76 0.3298 240.0 85.0 96.66 110.0 153.33 2.1089 1.1426 1.1623 1.1982 1.9036\\n13.\\nSpatio-temporal feature + LSTM [58] 71 90 82 81 73 0.4329 460.0 205.0 215.33 240.0 340.0 3.1652 2.6743 2.8628 2.9845 3.1107\\n14.\\nEnsemble DCNNs [59] 53 55 67 58 70 0.4917 420.0 200.0 220.0 300.0 360.0 3.8138 2.5827 2.6296 2.7013 3.4238\\n15.\\nDCNN for binary classiﬁcation [60] 50 56 73 70 68 0.1725 103.33 24.66 28.0 37.33 75.0 1.6239 1.3469 1.4430 1.4523 1.5426\\n16.\\nIACNN [61] 68 75 95 67 74 0.3946 1033.0 600.0 633.0 700.0 866.66 2.1623 1.8920 1.9165 1.9730 2.0935\\n17.\\n2B(N+M)Softmax [62] 67 78 87 81 69 0.2814 265.0 80.0 85.33 93.33 165.0 1.1721 0.9643 0.9892 0.9946 1.0021\\n18.\\nS-DSRN [63] 72 92 92 83 75 0.1927 980.0 320.0 340.0 380.33 460.0 2.3042 1.9756 1.9878 2.0262 2.1040\\n19.\\nColor features + Gabor transform [64] 66 77 86 80 70 0.3218 240.0 98.66 100.0 115.33 180.66 2.8794 2.2167 2.4510 2.5503 2.7165\\n20.\\nDCMA-CNNs [65] 73 95 93 83 78 0.3129 340.0 160.0 180.0 190.0 230.0 1.2439 1.1092 1.1123 1.1706 1.2034\\n21.\\nBroad Learning [66] 44 93 81 89 64 0.1023 7.12 1.0 1.5 2.0 4.5 0.4812 0.3101 0.3323 0.3812 0.4102\\n22.\\nEnsemble MLCNNs [67] 74 94 93 85 79 0.5612 88.33 23.33 25.0 28.33 63.33 2.3024 1.9823 1.9986 2.0145 2.1876\\n23.\\nDeep-Emotion [68] 70 93 94 81 72 0.2908 316.0 41.6 50.0 66.66 241.66 1.0982 0.8943 0.9165 0.9346 0.9978\\n24.\\nVGG19 [47] 74 95 96 81 60 0.6128 45.5 22.0 23.0 26.5 34.5 2.2123 1.9821 1.9901 2.0981 2.1823\\n25.\\nResNet150 [47] 75 91 89 72 70 0.7123 130.0 54.0 67.0 76.5 105.5 3.1190 2.5981 2.6180 2.7833 2.9009\\n26. Pr\\noposed method 78 98 98 96 83 0.3039 960.0 200.0 240.0 320.0 400.0 2.0974 1.6919 1.7346 1.7983 1.9029\\nV. C ONCLUSION\\nIt is clear from the empirical results that the proposed\\nmethod can efﬁciently handle the problem of FER using\\nstatic/still images. Facial expressions under lab-controlled en-\\nvironments are different from those in the wild, which are more\\nnatural and spontaneous. So, three databases namely, JAFFE,\\nCK+, and KDEF, developed in a lab-controlled environment\\nare considered in this work. This study also adopted two\\ndatabases viz., FER2013, and RAF built in the wild to demon-\\nstrate the efﬁcacy of the proposed method over state-of-the-art\\nmethods. A novel DCNN framework is introduced to extract\\nholistic features for identifying facial expression. However,\\nbefore the use of the proposed DCNN model, a GF-based\\nedge descriptor is adopted to fetch the low-level local features.\\nThe GF-based edge descriptor produces two intermediate local\\nfeatures namely, M and D. At the end of the proposed DCNN\\nmodel, a softmax classiﬁer is used to compute the probability\\nvalues in favor of either seven facial expressions. Finally,\\na score-level fusion technique is employed to combine theoutputs obtained by the proposed model using M and D. The\\nproposed method achieves an average recognition accuracy of\\n78%, 98%, 98%, 96%, and 83% for FER2013, JAFFE, CK+,\\nKDEF, and RAF respectively. Empirical results demonstrate\\nthat local as well as holistic features can together enhance\\nthe FER task. Experimental results also illustrate that the\\nproposed method outperforms twenty-ﬁve baseline methods\\nby considering the average time. However, the performance\\nis generally not as good as that in FER under lab-controlled\\nenvironment, which deserves further study. Moreover, it is\\nworth investigating to deploy the proposed model in some\\nreal-life applications.\\nThis work is partially supported by the project “Pre-\\ndiction of diseases through computer assisted diagnosis\\nsystem using images captured by minimally-invasive and\\nnon-invasive modalities”, Computer Science and Engineer-\\ning, PDPM Indian Institute of Information Technology, De-\\nsign and Manufacturing, Jabalpur India (under ID: SPARC-\\nMHRD-231). This work is also partially supported by the\\nproject IT4Neuro(degeneration), reg. nr. CZ.02.1.01/0.0/0.0/18\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 11\\n069/0010054, by the project “Smart Solutions in Ubiquitous\\nComputing Environments”, Grant Agency of Excellence, Uni-\\nversity of Hradec Kralove, Faculty of Informatics and Man-\\nagement, Czech Republic (under ID: UHK-FIM-GE-2020);\\nproject at Universiti Teknologi Malaysia (UTM) under Re-\\nsearch University Grant V ot-20H04, Malaysia Research Uni-\\nversity Network (MRUN) V ot 4L876 and the Fundamental\\nResearch Grant Scheme (FRGS) V ot5F073 supported under\\nMinistry of Education Malaysia for the completion of the\\nresearch.\\nREFERENCES\\n[1] R. A. Calvo and S. D’Mello, “Affect detection: An interdisciplinary\\nreview of models, methods, and their applications,” IEEE Transactions\\non affective computing, vol. 1, no. 1, pp. 18–37, 2010.\\n[2] C. A. Corneanu, M. O. Sim ´on, J. F. Cohn, and S. E. Guerrero,\\n“Survey on rgb, 3d, thermal, and multimodal approaches for facial\\nexpression recognition: History, trends, and affect-related applications,”\\nIEEE transactions on pattern analysis and machine intelligence, vol. 38,\\nno. 8, pp. 1548–1568, 2016.\\n[3] P. Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter, S. Gruss,\\nand H. C. Traue, “Automatic pain assessment with facial activity\\ndescriptors,” IEEE Transactions on Affective Computing, vol. 8, no. 3,\\npp. 286–299, 2016.\\n[4] C. Liu and H. Wechsler, “Gabor feature based classiﬁcation using the\\nenhanced ﬁsher linear discriminant model for face recognition,” IEEE\\nTransactions on Image processing, vol. 11, no. 4, pp. 467–476, 2002.\\n[5] A. Seal, S. Ganguly, D. Bhattacharjee, M. Nasipuri, and D. K. Basu,\\n“Thermal human face recognition based on haar wavelet transform and\\nseries matching technique,” in Multimedia Processing, Communication\\nand Computing Applications. Springer, 2013, pp. 155–167.\\n[6] D. Bhattacharjee, A. Seal, S. Ganguly, M. Nasipuri, and D. K. Basu,\\n“A comparative study of human thermal face recognition based on haar\\nwavelet transform and local binary pattern,” Computational intelligence\\nand neuroscience, vol. 2012, 2012.\\n[7] M. Kopaczka, R. Kolk, J. Schock, F. Burkhard, and D. Merhof, “A ther-\\nmal infrared face database with facial landmarks and emotion labels,”\\nIEEE Transactions on Instrumentation and Measurement, vol. 68, no. 5,\\npp. 1389–1401, 2018.\\n[8] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\\ndetection,” in 2005 IEEE computer society conference on computer\\nvision and pattern recognition (CVPR’05), vol. 1. IEEE, 2005, pp.\\n886–893.\\n[9] A. Seal, D. Bhattacharjee, M. Nasipuri, C. Gonzalo-Martin, and\\nE. Menasalvas, “Histogram of bunched intensity values based thermal\\nface recognition,” in Rough Sets and Intelligent Systems Paradigms.\\nSpringer, 2014, pp. 367–374.\\n[10] S. Onta ˜n´on, J. L. Monta ˜na, and A. J. Gonzalez, “A dynamic-bayesian\\nnetwork framework for modeling and evaluating learning from observa-\\ntion,” Expert Systems with Applications, vol. 41, no. 11, pp. 5212–5226,\\n2014.\\n[11] A. Seal, S. Ganguly, D. Bhattacharjee, M. Nasipuri, and D. K. Basu,\\n“Automated thermal face recognition based on minutiae extraction,”\\narXiv preprint arXiv:1309.1000, 2013.\\n[12] Y . Gao, M. K. Leung, S. C. Hui, and M. W. Tananda, “Facial expression\\nrecognition from line-based caricatures,” IEEE Transactions on Systems,\\nMan, and Cybernetics-Part A: Systems and Humans, vol. 33, no. 3, pp.\\n407–412, 2003.\\n[13] M. D. Cordea, E. M. Petriu, and D. C. Petriu, “Three-dimensional head\\ntracking and facial expression recovery using an anthropometric muscle-\\nbased active appearance model,” IEEE transactions on instrumentation\\nand measurement, vol. 57, no. 8, pp. 1578–1588, 2008.\\n[14] Z. Xu, H. R. Wu, X. Yu, K. Horadam, and B. Qiu, “Robust shape-\\nfeature-vector-based face recognition system,” IEEE Transactions on\\nInstrumentation and Measurement, vol. 60, no. 12, pp. 3781–3791, 2011.\\n[15] J. Li, D. Zhang, J. Zhang, J. Zhang, T. Li, Y . Xia, Q. Yan, and L. Xun,\\n“Facial expression recognition with faster r-cnn,” Procedia Computer\\nScience, vol. 107, pp. 135–140, 2017.\\n[16] Y . Sun, X. Wang, and X. Tang, “Deep learning face representation from\\npredicting 10,000 classes,” in Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, 2014, pp. 1891–1898.[17] P. Liu, S. Han, Z. Meng, and Y . Tong, “Facial expression recognition via\\na boosted deep belief network,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition, 2014, pp. 1805–1812.\\n[18] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, “Joint ﬁne-tuning in deep\\nneural networks for facial expression recognition,” in Proceedings of\\nthe IEEE international conference on computer vision, 2015, pp. 2983–\\n2991.\\n[19] C. Liu and H. Wechsler, “Enhanced ﬁsher linear discriminant models for\\nface recognition,” in Proceedings. Fourteenth International Conference\\non Pattern Recognition (Cat. No. 98EX170), vol. 2. IEEE, 1998, pp.\\n1368–1372.\\n[20] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local\\nbinary patterns: Application to face recognition,” IEEE transactions on\\npattern analysis and machine intelligence, vol. 28, no. 12, pp. 2037–\\n2041, 2006.\\n[21] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”\\nInternational journal of computer vision , vol. 60, no. 2, pp. 91–110,\\n2004.\\n[22] Z. Li, “A discriminative learning convolutional neural network for facial\\nexpression recognition,” in 2017 3rd IEEE international conference on\\ncomputer and communications (ICCC). IEEE, 2017, pp. 1641–1646.\\n[23] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”\\narXiv preprint arXiv:1411.1784, 2014.\\n[24] H. Yang, Z. Zhang, and L. Yin, “Identity-adaptive facial expression\\nrecognition through expression regeneration using conditional generative\\nadversarial networks,” in 2018 13th IEEE International Conference on\\nAutomatic Face & Gesture Recognition (FG 2018). IEEE, 2018, pp.\\n294–301.\\n[25] A. Fathallah, L. Abdi, and A. Douik, “Facial expression recognition\\nvia deep learning,” in 2017 IEEE/ACS 14th International Conference\\non Computer Systems and Applications (AICCSA). IEEE, 2017, pp.\\n745–750.\\n[26] M. T. B. Iqbal, M. Abdullah-Al-Wadud, B. Ryu, F. Makhmudkhujaev,\\nand O. Chae, “Facial expression recognition with neighborhood-aware\\nedge directional pattern (nedp),” IEEE Transactions on Affective Com-\\nputing, 2018.\\n[27] A. Bhavsar and H. M. Patel, “Facial expression recognition using neural\\nclassiﬁer and fuzzy mapping,” in 2005 Annual IEEE India Conference-\\nIndicon. IEEE, 2005, pp. 383–387.\\n[28] T. Jabid, M. H. Kabir, and O. Chae, “Robust facial expression recog-\\nnition based on local directional pattern,” ETRI journal, vol. 32, no. 5,\\npp. 784–794, 2010.\\n[29] P. Zhao-Yi, Z. Yan-Hui, and Z. Yu, “Real-time facial expression recogni-\\ntion based on adaptive canny operator edge detection,” in 2010 Second\\nInternational Conference on MultiMedia and Information Technology,\\nvol. 2. IEEE, 2010, pp. 154–157.\\n[30] R. Samad and H. Sawada, “Edge-based facial feature extraction using\\ngabor wavelet and convolution ﬁlters.” in MVA, 2011, pp. 430–433.\\n[31] D. Bhattacharjee and H. Roy, “Pattern of local gravitational force (plgf):\\nA novel local image descriptor,” IEEE transactions on pattern analysis\\nand machine intelligence, 2019.\\n[32] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,\\nB. Hamner, W. Cukierski, Y . Tang, D. Thaler, D.-H. Lee et al.,\\n“Challenges in representation learning: A report on three machine\\nlearning contests,” in International Conference on Neural Information\\nProcessing. Springer, 2013, pp. 117–124.\\n[33] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and J. Budynek, “The\\njapanese female facial expression (jaffe) database,” in Proceedings of\\nthird international conference on automatic face and gesture recognition,\\n1998, pp. 14–16.\\n[34] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,\\n“The extended cohn-kanade dataset (ck+): A complete dataset for action\\nunit and emotion-speciﬁed expression,” in 2010 ieee computer soci-\\nety conference on computer vision and pattern recognition-workshops.\\nIEEE, 2010, pp. 94–101.\\n[35] D. Lundqvist, A. Flykt, and A. ¨Ohman, “The karolinska directed\\nemotional faces (kdef),” CD ROM from Department of Clinical Neu-\\nroscience, Psychology section, Karolinska Institutet, vol. 91, no. 630,\\npp. 2–2, 1998.\\n[36] S. Li and W. Deng, “Reliable crowdsourcing and deep locality-\\npreserving learning for unconstrained facial expression recognition,”\\nIEEE Transactions on Image Processing, vol. 28, no. 1, pp. 356–370,\\n2019.\\n[37] S. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-\\npreserving learning for expression recognition in the wild,” in 2017\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR).\\nIEEE, 2017, pp. 2584–2593.\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 12\\n[38] X. Chen and W. Cheng, “Facial expression recognition based on edge\\ndetection,” International Journal of Computer Science and Engineering\\nSurvey, vol. 6, no. 2, p. 1, 2015.\\n[39] M. Abdulrahman and A. Eleyan, “Facial expression recognition using\\nsupport vector machines,” in 2015 23nd Signal Processing and Commu-\\nnications Applications Conference (SIU). IEEE, 2015, pp. 276–279.\\n[40] C. Laurent, G. Pereyra, P. Brakel, Y . Zhang, and Y . Bengio, “Batch\\nnormalized recurrent neural networks,” in 2016 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP).\\nIEEE, 2016, pp. 2657–2661.\\n[41] D. Ghimire and J. Lee, “Extreme learning machine ensemble using\\nbagging for facial expression recognition.” JIPS, vol. 10, no. 3, pp. 443–\\n458, 2014.\\n[42] J. Chen, Z. Chen, Z. Chi, and H. Fu, “Facial expression recognition\\nin video with multiple feature fusion,” IEEE Transactions on Affective\\nComputing, vol. 9, no. 1, pp. 38–50, 2016.\\n[43] Z. Yu and C. Zhang, “Image based static facial expression recognition\\nwith multiple deep network learning,” in Proceedings of the 2015 ACM\\non international conference on multimodal interaction , 2015, pp. 435–\\n442.\\n[44] X. Zhao, X. Liang, L. Liu, T. Li, Y . Han, N. Vasconcelos, and\\nS. Yan, “Peak-piloted deep network for facial expression recognition,” in\\nEuropean conference on computer vision. Springer, 2016, pp. 425–442.\\n[45] S. Alizadeh and A. Fazel, “Convolutional neural networks for facial\\nexpression recognition, 1704.06756,” 2017.\\n[46] Y . Tang, “Deep learning using support vector machines,” arXiv preprint\\narXiv:1306.0239, 2013.\\n[47] D. Orozco, C. Lee, Y . Arabadzhi, and D. Gupta, “Transfer learning for\\nfacial expression recognition.”\\n[48] Y . Sun, Y . Chen, X. Wang, and X. Tang, “Deep learning face rep-\\nresentation by joint identiﬁcation-veriﬁcation,” in Advances in neural\\ninformation processing systems, 2014, pp. 1988–1996.\\n[49] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, “Training deep\\nnetworks for facial expression recognition with crowd-sourced label\\ndistribution,” in Proceedings of the 18th ACM International Conference\\non Multimodal Interaction, 2016, pp. 279–283.\\n[50] B. Yang, J. Cao, R. Ni, and Y . Zhang, “Facial expression recognition\\nusing weighted mixture deep neural network based on double-channel\\nfacial images,” IEEE Access, vol. 6, pp. 4630–4640, 2017.\\n[51] A. Ravi, “Pre-trained convolutional neural network features for facial\\nexpression recognition,” arXiv preprint arXiv:1812.06387, 2018.\\n[52] W. Hua, F. Dai, L. Huang, J. Xiong, and G. Gui, “Hero: Human emotions\\nrecognition for realizing intelligent internet of things,” IEEE Access,\\nvol. 7, pp. 24 321–24 332, 2019.\\n[53] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database\\nfor facial expression, valence, and arousal computing in the wild,” IEEE\\nTransactions on Affective Computing, vol. 10, no. 1, pp. 18–31, 2017.\\n[54] J.-H. Kim, B.-G. Kim, P. P. Roy, and D.-M. Jeong, “Efﬁcient facial\\nexpression recognition algorithm based on hierarchical deep neural\\nnetwork structure,” IEEE Access, vol. 7, pp. 41 273–41 285, 2019.\\n[55] P. Dhankhar, “Resnet-50 and vgg-16 for recognizing facial emotions,”\\nInternational Journal of Innovations in Engineering and Technology\\n(IJIET), vol. 13, no. 4, pp. 126–130, 2019.\\n[56] B. Hasani and M. H. Mahoor, “Facial expression recognition using\\nenhanced deep 3d convolutional neural networks,” in Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition\\nWorkshops, 2017, pp. 30–40.\\n[57] A. Majumder, L. Behera, and V . K. Subramanian, “Automatic facial\\nexpression recognition system using deep network-based data fusion,”\\nIEEE transactions on cybernetics, vol. 48, no. 1, pp. 103–114, 2016.\\n[58] D. H. Kim, W. J. Baddar, J. Jang, and Y . M. Ro, “Multi-objective based\\nspatio-temporal feature representation learning robust to expression\\nintensity variations for facial expression recognition,” IEEE Transactions\\non Affective Computing, vol. 10, no. 2, pp. 223–236, 2017.\\n[59] G. Pons and D. Masip, “Supervised committee of convolutional neural\\nnetworks in automated facial expression analysis,” IEEE Transactions\\non Affective Computing, vol. 9, no. 3, pp. 343–350, 2017.\\n[60] M. G. Villanueva and S. R. Zavala, “Deep neural network architecture:\\napplication for facial expression recognition,” IEEE Latin America\\nTransactions, vol. 18, no. 07, pp. 1311–1319, 2020.\\n[61] Z. Meng, P. Liu, J. Cai, S. Han, and Y . Tong, “Identity-aware con-\\nvolutional neural network for facial expression recognition,” in 2017\\n12th IEEE International Conference on Automatic Face & Gesture\\nRecognition (FG 2017). IEEE, 2017, pp. 558–565.\\n[62] X. Liu, B. Vijaya Kumar, J. You, and P. Jia, “Adaptive deep metric\\nlearning for identity-aware facial expression recognition,” in Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition\\nWorkshops, 2017, pp. 20–29.\\n[63] M. Alam, L. S. Vidyaratne, and K. M. Iftekharuddin, “Sparse simulta-\\nneous recurrent deep learning for robust facial expression recognition,”\\nIEEE transactions on neural networks and learning systems, vol. 29,\\nno. 10, pp. 4905–4916, 2018.\\n[64] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez, “Discriminant\\nfunctional learning of color features for the recognition of facial action\\nunits and their intensities,” IEEE transactions on pattern analysis and\\nmachine intelligence, vol. 41, no. 12, pp. 2835–2845, 2018.\\n[65] S. Xie and H. Hu, “Facial expression recognition using hierarchical fea-\\ntures with deep comprehensive multipatches aggregation convolutional\\nneural networks,” IEEE Transactions on Multimedia, vol. 21, no. 1, pp.\\n211–220, 2018.\\n[66] T. Zhang, Z. Liu, X.-H. Wang, X.-F. Xing, C. P. Chen, and E. Chen,\\n“Facial expression recognition via broad learning system,” in 2018 IEEE\\nInternational Conference on Systems, Man, and Cybernetics (SMC).\\nIEEE, 2018, pp. 1898–1902.\\n[67] D. H. Nguyen, S. Kim, G.-S. Lee, H.-J. Yang, I.-S. Na, and S. H.\\nKim, “Facial expression recognition using a temporal ensemble of multi-\\nlevel convolutional neural networks,” IEEE Transactions on Affective\\nComputing, 2019.\\n[68] S. Minaee and A. Abdolrashidi, “Deep-emotion: Facial expression\\nrecognition using attentional convolutional network,” arXiv preprint\\narXiv:1902.01019, 2019.\\n[69] Y . Liu, X. Zhang, Y . Lin, and H. Wang, “Facial expression recognition\\nvia deep action units graph network based on psychological mechanism,”\\nIEEE Transactions on Cognitive and Developmental Systems, 2019.\\n[70] H. Roy and D. Bhattacharjee, “Local-gravity-face (lg-face) for\\nillumination-invariant and heterogeneous face recognition,” IEEE Trans-\\nactions on Information Forensics and Security, vol. 11, no. 7, pp. 1412–\\n1424, 2016.\\n[71] X. Glorot, A. Bordes, and Y . Bengio, “Deep sparse rectiﬁer neural\\nnetworks,” in Proceedings of the fourteenth international conference on\\nartiﬁcial intelligence and statistics, 2011, pp. 315–323.\\n[72] Y . Bengio, “Practical recommendations for gradient-based training of\\ndeep architectures,” in Neural networks: Tricks of the trade. Springer,\\n2012, pp. 437–478.\\n[73] A. Ross and A. Jain, “Information fusion in biometrics,” Pattern\\nrecognition letters, vol. 24, no. 13, pp. 2115–2125, 2003.\\nKarnati Mohan is pursuing PhD in computer\\nscience department, PDPM Indian Institute of\\nInformation Technology, Design and Manufactur-\\ning Jabalpur, Madhya Pradesh, India and he has\\nalso received the M. tech degree from the same\\ninstitute in 2020. He received his B. Tech degree in\\ncomputer science and engineering from Jawahar-\\nlal Technological University, Hyderabad, India\\nin 2016. His research interests include machine\\nlearning, affect recognition, deep learning, and\\nImage processing. He has served as a reviewer\\nfor IEEE Access journal. He has also served as a program committee\\nmember for ISAC conference.\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n0018-9456 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIM.2020.3031835, IEEE\\nTransactions on Instrumentation and Measurement\\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 13\\nAyan Seal [M’14, SM’19] received the PhD degree\\nin Engineering from Jadavpur University, West\\nBengal, India, in 2014. He is currently an As-\\nsistant Professor with the Computer Science and\\nEngineering Department, PDPM Indian Institute\\nof Information Technology, Design and Manu-\\nfacturing Jabalpur, Jabalpur, Madhya Pradesh,\\nIndia. He has visited the Universidad Politecnica\\nde Madrid, Spain as a visiting research scholar.\\nHe is the recipient of several awards. He has re-\\nceived Sir Visvesvaraya Young Faculty Research\\nFellowship from Media Lab Asia, Ministry of Electronics and Information\\nTechnology, Government of India. He is a senior member of IEEE.\\nHe has authored or co-authored of several journals, conferences and\\nbook chapters in the area of biometric and medical image processing.\\nHis current research interests include image processing and pattern\\nrecognition.\\nOndrej Krejcar is a full professor in systems\\nengineering and informatics at the University\\nof Hradec Kralove, Czech Republic. In 2008 he\\nreceived his Ph.D. title in technical cybernetics at\\nTechnical University of Ostrava, Czech Republic.\\nHe is currently a vice-rector for science and\\ncreative activities of the University of Hradec\\nKralove from June 2020. At present, he is also\\na director of the Center for Basic and Applied\\nResearch at the University of Hradec Kralove. In\\nyears 2016-2020 he was vice-dean for science and\\nresearch at Faculty of Informatics and Management, UHK. His h-index\\nis 18, with more than 1150 citations received in the Web of Science.\\nIn 2018, he was the 14th top peer reviewer in Multidisciplinary in the\\nWorld according to Publons and a Top Reviewer in the Global Peer\\nReview Awards 2019 by Publons. Currently, he is on the editorial board\\nof the MDPI Sensors IF journal (Q1/Q2 at JCR), and several other\\nESCI indexed journals. He is a Vice-leader and Management Committee\\nmember at WG4 at project COST CA17136, since 2018. He has also been\\na Management Committee member substitute at project COST CA16226\\nsince 2017. Since 2019, he has been Chairman of the Program Committee\\nof the KAPPA Program, Technological Agency of the Czech Republic as\\na regulator of the EEA/Norwegian Financial Mechanism in the Czech\\nRepublic (2019-2024). Since 2020, he has been Chairman of the Panel\\n1 (Computer, Physical and Chemical Sciences) of the ZETA Program,\\nTechnological Agency of the Czech Republic. Since 2014 until 2019, he has\\nbeen Deputy Chairman of the Panel 7 (Processing Industry, Robotics, and\\nElectrical Engineering) of the Epsilon Program, Technological Agency\\nof the Czech Republic. At the University of Hradec Kralove, he is a\\nguarantee of the doctoral study program in Applied Informatics, where\\nhe is focusing on lecturing on Smart Approaches to the Development\\nof Information Systems and Applications in Ubiquitous Computing\\nEnvironments. His research interests include Control Systems, Smart\\nSensors, Ubiquitous Computing, Manufacturing, Wireless Technology,\\nPortable Devices, biomedicine, image segmentation and recognition,\\nbiometrics, technical cybernetics, and ubiquitous computing. His second\\narea of interest is in Biomedicine (image analysis), as well as Biotelemetric\\nSystem Architecture (portable device architecture, wireless biosensors),\\ndevelopment of applications for mobile devices with use of remote or\\nembedded biomedical sensors.\\nAnis Yazidi received the M.Sc. and Ph.D. degrees\\nfrom the University of Agder, Grimstad, Nor-\\nway, in 2008 and 2012, respectively. He was a\\nResearcher with Teknova AS, Grimstad, Norway.\\nFrom 2014 till 2019 he was an associate professor\\nwith the Department of Computer Science, Oslo\\nMetropolitan University, Oslo, Norway. He is cur-\\nrently a Full Professor with the same department\\nwhere he is leading the research group in Applied\\nArtiﬁcial Intelligence. He is also Professor II\\nwith the Norwegian University of Science and\\nTechnology (NTNU), Trondheim, Norway. His current research interests\\ninclude machine learning, learning automata, stochastic optimization, and\\nautonomous computing.\\nAuthorized licensed use limited to: Auckland University of Technology. Downloaded on October 29,2020 at 18:16:40 UTC from IEEE Xplore.  Restrictions apply. \\n',\n",
       " 'Facial Expression Recognition\\nvia Deep Learning\\nAbir Fathallah\\nENISO, Sousse university\\nSousse, Tunisia\\nEmail: abir.fathallah1803@gmail.comLotﬁ Abdi\\nENISO, Sousse university\\nSousse, Tunisia\\nEmail: lotﬁabdi@hotmail.comAli Douik\\nENISO, Sousse university\\nSousse, Tunisia\\nEmail: ali.douik@enim.rnu.tn\\nAbstract —Automated Facial Expression Recognition has re-\\nmained a challenging and interesting problem in computer vision.\\nThe recognition of facial expressions is difﬁcult problem for\\nmachine learning techniques, since people can vary signiﬁcantly\\nin the way they show their expressions. Deep learning is a new\\narea of research within machine learning method which can\\nclassify images of human faces into emotion categories using Deep\\nNeural Networks (DNN). Convolutional neural networks (CNN)\\nhave been widely used to overcome the difﬁculties in facial expres-\\nsion classiﬁcation. In this paper, we present a new architecture\\nnetwork based on CNN for facial expressions recognition. We\\nﬁne tuned our architecture with Visual Geometry Group model\\n(VGG) to improve results. To evaluate our architecture we tested\\nit with many largely public databases (CK+, MUG, and RAFD).\\nObtained results show that the CNN approach is very effective\\nin image expression recognition on many public databases which\\nachieve an improvements in facial expression analysis.\\nIndex T erms —Facial Expression; Recognition; Deep Learning;\\nCNN; Architecture; Classiﬁcation.\\nI. I NTRODUCTION\\nInteraction Human-Machine (IHM) has long conﬁned re-\\nsearches to develop techniques based on the use of triplet\\nscreen-keyboard-mouse. Today, it is moving towards new\\nparadigms: the user must be able to evolve unimpeded in its\\nnatural environment; ﬁngers,hand, face or familiar objects are\\nseen as many devices input/output; the boundary between the\\nphysical and electronic worlds is blurring. These new forms\\nof interaction require usually the capture of the observable\\nbehavior of the user and his environment. That is why they\\nrely on artiﬁcial perception techniques, including computer\\nvision. IHM is a rapidly evolving discipline. Future genera-\\ntions of human-machine environment will become multimodal\\nintegrating new information, from the consideration of the\\ndynamic behavior, speech and/or facial expressions, so as to\\nmake the use of machines the most intuitive and natural as\\npossible.\\nThe face is the most expressive and communicative part of\\na human being [1], it represents a major focus in current\\nresearch concerning the improvement of IHM for establishing\\na dialogue between the two entities.\\nFacial expression is a visible manifestation of a face from\\nthe state of mind (emotion, reﬂection), cognitive activity,\\nphysiological (fatigue, pain), personality and psychopathology\\nof a person. The essential of facial expression informationis contained in the deformation of main permanent facial\\nfeatures, characterized by a change visually perceptible.\\nToday, analysis computer assisted of face and its facial ex-\\npressions is an emerging ﬁeld. Emotion recognition consists\\nin associating an emotion to face image. So the goal is\\nto determine from a face, the internal emotional state of a\\nperson. An automatic facial expression recognition system\\nis an important component in human machine interaction. It\\nconsists to evaluate the possibility of emotions recognition.\\nHowever, this is not an easy task.\\nFacial expression recognition usually employs a three-stage\\ntraining consisting of face Acquisition [2], facial feature\\nextraction [3] and classiﬁer construction [4, 5].\\nRecently, Many works [6, 7] demonstrated that expression\\nrecognition can beneﬁt from collecting two stages together\\nfacial feature extraction and classiﬁer construction.\\nDeep learning methods have been successfully applied to\\nextract features and classiﬁcation, in particular Convolutional\\nNeural Networks (CNN) architectures which are biologically-\\ninspired multi-stage one that learned automatically hierarchies\\nof invariant features [8]. The ConvNets consist of a multi-\\nstage processing of an input image to extract hierarchical and\\nhigh-level feature representations.\\nMotivated by this, we present in this paper an effective\\napproach system based on ConvNets for facial expression\\nrecognition. We proposed a new architecture which the input\\nof the system is an image; then, we use CNN to predict the\\nfacial expression label which should be one these labels[9]:\\nanger,happiness, sadness, disgust, surprise and neutral.\\nII. R ELATED WORK\\nMany facial expression recognition methods represent faces\\nby high-dimensional over-complete face descriptors, followed\\nby shallow models. In this context, MLIKI et al. [10] proposed\\na method which is able to monitor the intensity variation of\\nfacial expression and to reduce the classiﬁcation confusion\\nbetween facial expressions classes. The approach uses Vector\\nField Convolution (VFC) method to segment facial feature\\ncontours. COTRET et al. [11] also proposed a wavelet-based\\nface recognition method robust against face position and light\\nvariations for real-time applications. Wang et al. [12] proposed\\nexpression recognition method based on evidence theory and\\nlocal texture where the facial image is divided into regions\\n2017 IEEE/ACS 14th International Conference on Computer Systems and Applications\\n2161-5330/17 $31.00 © 2017 IEEE\\nDOI 10.1109/AICCSA.2017.124745\\n\\nwith important recognition features, and the Local Binary\\nPatterns (LBP) textural features of the regions are extracted.Recent works showed how deep learning models could beapplied on facial expression recognition, in [13] the authorspresent a novel Boosted Deep Belief Network (BDBN) forperforming the three training stages iteratively in a uniﬁedloopy framework. Burkert et al. [14] proposed a CNN archi-tecture for facial expression recognition. Mollahosseini [15]also proposed a deep neural network architecture to addressthe facial expression recognition problem across multiple well-known standard face datasets. In [16] Zhang et al. proposed anovel deep neural network: DNN-driven facial feature learn-ing model which employs several layers to characterize thecorresponding relationship between the SIFT feature vectorsand their corresponding high-level semantic information. Theauthors in [17] extract from facial landmarks ﬁxed number ofSIFT features used as an input matrix to CNN architecture.The matrix size is X× Y where X is the number of SIFT fea-\\ntures, and Y is the size of each feature. Moreover, in [18] Sunet al. proposed a mixture of SIFT and deep convolution, theauthors used Partial least squares regression and linear SVMto train these features. Then, the output from all classiﬁersare combined with fusion network. In [19] a deep learningtechnique is adopted. The idea is to combine two models,in the ﬁrst deep network temporal appearance features fromimage sequences are extracted. Meanwhile temporal geometryfeatures from temporal facial landmark points are extracted bythe second deep network. The authors in [20] proposed a noveltechnique of taking Inter Vector Angles (IV A) as geometricfeatures, which proved to be scale invariant and person inde-pendent. A feature redundancy-reduced convolutional neuralnetwork (FRR-CNN) is presented in [21].\\nIII. P\\nROPOSED APPROACH\\nOur purpose is to ameliorate the accuracy of facial expres-\\nsion classiﬁcation by using a new CNN architecture. As deepnetworks need a big database for the trainig, we combine manydatabases to get a ﬁnal one.As a ﬁrst step, After preparing the database we ﬁxed the batchsize input of CNN architecture to 165 ×165 then we trained\\nthe architecture with ﬁne tuning by Visual Geometry Group(VGG) model to generate the ﬁrst model.In second step to improve the classiﬁcation we repeat thetraining of our CNN architecture but the ﬁne tuning here isachieved with the obtained ﬁrst model, and ﬁnally we get ourﬁnal model as shown in Figure 1.\\nA. Network architecture\\nDeep Neural Networks (DNN) are models inspired of the\\nhuman brain, and particularly its ability to extract structures\\n(patterns) from raw data. From raw data input, deep learningmodels operate a large number of successive transformationsto discover representations increasingly abstract of such data.The operated transformations are combinations of linear andnonlinear operations. These transformations are used to repre-sent the data at different levels abstraction. The most popular\\nFig. 1. Proposed approach.\\nimage processing structure of DNN is CNN [8] which isconstructed by three main processing layers: ConvolutionalLayer, Pooling Layer and Fully Connected Layer.The adopted architecture of the convolutional neural networkis given in Figure 2.The proposed deep ConvNet contain four convolutional layers(with three max-pooling layers) to extract features hierarchi-cally, followed by the fully-connected layer and the softmaxoutput layer indicating 6 expression classes. The input of thenetwork is 165× 165×k for all patches, where k = 3 for color\\npatches and k = 1 for gray patches. The output is one of the6 expression classes.\\nFig. 2. Architecture of the convolutional neural network.\\nCNN units are described below:Convolutional layer: A convolution layer C\\ni(i network\\nlayer) is parameterized by its number N of convolution cardsM\\ni\\nj(j∈{1,...,N}),the size of the convolution kernels\\nKx×Ky(often square), and the connection diagram in the\\nprevious layer Li−1. Each convolution card Mi\\njis the result\\nof a convolution sum of cards previous layer Mi−1\\nj by its\\nrespective convolution kernel. In the case of a fully connected\\n746\\nTABLE I\\nNETWORK CONFIGURATION .\\nLayer type Size/Stride Output\\nDropout Probability\\nData 165×165 -\\nConvolution 1 4×4/1 20\\nMax Pooling 1 2×2/2 -\\nConvolution 2 3×3/2 40\\nMax Pooling 2 2×2/2 -\\nConvolution 3 3×3/2 60\\nMax Pooling 3 2×2/2 -\\nConvolution 4 2×2/2 80\\nFully Connected - 160\\nDropout=0.5\\nFully Connected - 7\\ncard to the cards of the previous layer, the result is calculated\\nby the equation 1.\\nN/summationdisplay\\nn=1Mi\\nj=φ/parenleftBigg\\nbi\\nj+N/summationdisplay\\nn=1Mi−1\\nn∗ki\\nn/parenrightBigg\\n(1)\\nwhere * is the convolution operator.\\nPooling layer: In the classical architectures of convolutional\\nneural networks, convolution layers are followed by sub-\\nsampling layers. A sub-sampling layer reduces the size of\\ncards, and introduces invariance to (low) rotations and transla-\\ntions can appear as input. The output of max-pooling layer is\\ngiven by the maximum activation value, in the input layer for\\ndifferent regions of size Kx×Kynon-overlapping. Similarly\\nto a convolution layer, a bias is added and the result is passed\\nto the transfer function φdeﬁned above.\\nFully connected layer: After several max pooling and convo-\\nlutional layers, the high-level reasoning in the neural network\\nis done via fully connected layers. Neurons in a fully con-\\nnected layer have full connections to all activations in the\\nprevious layer, as seen in regular Neural Networks. Their\\nactivations can hence be computed with a matrix multipli-\\ncation followed by a bias offset. Table I present network\\nconﬁguration, the patch size od data input is 165 ×165. Then,\\nthe convolutional and Max Pooling layers are chosen with\\ndifferent kernel sizes ( 4 ×4, 3×3, 2×2) and different strides\\n(1, 2).\\nB. Fine tuning\\nIn the step of Fine tuning, we chose to use VGG model[22].\\nFor ﬁne-tuning with VGG, we used Caffe [23] framework to\\nget the model weights. VGG model was trained on the large\\nCASIA WebFace dataset [24] and the Static Facial Expressions\\nin the Wild (SFEW) dataset, which is a smaller database of\\nlabeled facial emotions and it has been developed by selecting\\nframes from Acted Facial Expressions In The Wild (AFEW: is\\na dynamic temporal facial expressions data corpus consisting\\nof close to real world environment extracted from movies)[25].\\nIV . E XPERIMENTAL RESULTS\\nSeveral databases were used to train and evaluate the\\nproposed architecture.TABLE II\\nDATABASES DESCRIPTION .\\nBase Images Number Pose\\nLearning baseCk+ 8000 1\\nKDEF 4900 5\\nMUG 21000 1\\nRAFD 1400 1\\nTest baseCk+ 150 1\\nRaFD 120 1\\nMUG 3006 1\\nA. Databases\\nTo train our CNN architecture, we used many databases\\nand we standardize the size of all images to 224 ×242\\npixels.\\nThe CK+ database [26] The CK+ database presented by\\n327 expression sequences. From each image sequence, we\\nselected only the last four frames to save the most clear\\nexpression. Meanwhile the ﬁrst frames from each sequence\\nare collected for neutral expression.\\nWe use also KDEF database is a set of totally 4900 pictures of\\nhuman facial expressions of emotion. This database proposed\\n7 facial expressions in different positions. We choose to use\\nthe images of this database in the training step but only with\\n3 positions.\\nThe Radboud Faces Database (RaFD) [27] is a set of\\npictures of 67 models (including Caucasian males and\\nfemales, Caucasian children, both boys and girls, and\\nMoroccan Dutch males) displaying 8 emotional expressions.\\nMUG database [28] consists of image sequences of 86\\nsubjects performing facial expressions. In the database\\nparticipated 35 women and 51 men all of Caucasian origin\\nbetween 20 and 35 years of age. Each image was saved with\\na jpg format, 896896 pixels.\\nBy this way, we built an experimental dataset with a total of\\n37000 images. 330000 images are used in training step, and\\nthe rest of images are used as test images. As shown in Table\\nII, for learnig base we uses Ck+, KDEF, RaFD and MUG\\ndatasets while for test base we tested only with Ck+, RaFD\\nand MUG datasets.\\n1) Perfermances of the proposed method: To verify the ef-\\nfectiveness of our proposed approach, we opted for a validation\\non standard databases MUG, Rafd and CK+. Figure 3 illustrate\\nthe confusion matrix on CK+ database. We can observe that\\nour proposed method achieves the best perfermance on only 5\\ndatabase classes (Disgust, Happy,Neutral, Sad and Surprise)\\nwith recognition rate 100 %. However recognition rate of\\nAngry emotion is still difﬁcult (96 %) because of the database\\ncharacteristics. Our model excelled with classifying the CK+\\ndataset with recognition rate of 99.33 %. Figure 4 shows the\\nconfusion matrix on MUG database we achieved only 87.65 %\\nas recognition rate, the differences between different emotions\\namong the subjects is very subtle. Also, MUG images are in\\n747\\nFig. 3. Confusion matrix of proposed method on CK+ database.\\ngrey-scale format, whereas our model work is optimized for\\nRGB images. The lack of RGB format can exacerbate the\\nability of the network to make distinction between important\\nfeatures and background elements. Our model is also evalu-\\nFig. 4. Confusion matrix of proposed method on MUG database.\\nated with RaFD and we achieved a accuracy rate of 93.33 %as\\nshown in Figure5. In this database we observe some incorrect\\nclassiﬁcation for Sad and Neutral emotions and specially the\\nSad expression due to the unclarity features in this class.\\nB. Method comparison\\nTo get accuracy close to 99 %in training step, it took about\\n4 days to ﬁnish the training (300000 iterations). To accelerate\\nthe learning process with parallelization, we used CAFFE on\\nubuntu 14.04 LTS and a GeForce GT 525M GPU, which\\nhas 2GB of memory. we tried an architecture with 3, 5 and\\n6 hidden layers but our chosen architecture present the best\\nFig. 5. Confusion matrix of proposed method on RaFD database.\\naccuracy of features extraction and emotions classiﬁcation.\\nTo demonstrate the perfermance of our method we compared\\nit with other methods tested on Ck+ database. As shown in\\nFig. 6. Performance comparison on the CK+ database, A1[20] , A2 [19], A3\\n[21].\\nFigure 6, our method outperformed all the methods in compar-\\nison. This demonstrated that the features learned and selected\\nthrough our method contain more discriminative information\\nfor facial expression recognition.\\nOur Deep learning method achieves the best performance on\\nCK+ database. The deep learning work developed by [19]\\nachieved the second best performance of 96.93 %accuracy\\non Ck+. The third best performance of 95.34 %accuracy is\\npresented by [20]. The work proposed at the same time of this\\npaper achieved the last best performance of 71.04 %accuracy\\non Ck+.\\nV. C ONCLUSION AND DISCUSSION\\nA part of automatic analysis of facial expression ﬁeld is\\npresented in this paper. We proposed a new deep neural\\nnetwork architecture for facial expression recognition. The\\nproposed network consists of four convolutional layers, th ﬁrst\\nthree layers are followed by max pooling and the last one\\n748\\nis followed by fully connected layer. It takes facial images\\nas the input and classiﬁes them into either of the six facial\\nexpressions: angry, disgust, happy, neutral, sad and surprise.\\nThe proposed architecture obtained is evaluated with MUG,\\nRAFD and Ck+ databases. Results and recognition rates prove\\nthat our method outperforms the state-of-the-art methods.\\nFor this project, we trained the model with images which the\\nface was in one position. In the futurework, we would like to\\nextend our model to different face positions. This will allow\\nus to investigate the efﬁcacy of pre-trained models such as\\nVGGNet for facial emotion recognition.\\nREFERENCES\\n[1] R. G. Harper, A. N. Wiens, and J. D. Matarazzo, Non-\\nverbal communication: The state of the art. John Wiley\\n& Sons, 1978.\\n[2] P. Viola and M. J. Jones, “Robust real-time face detec-\\ntion,” International journal of computer vision , vol. 57,\\nno. 2, pp. 137–154, 2004.\\n[3] S. M. Lajevardi and M. Lech, “Facial expression recog-\\nnition from image sequences using optimized feature\\nselection,” in Image and Vision Computing New Zealand,\\n2008. IVCNZ 2008. 23rd International Conference .\\nIEEE, 2008, pp. 1–6.\\n[4] A. Ben-Hur and J. Weston, “A users guide to support\\nvector machines,” Data mining techniques for the life\\nsciences , pp. 223–239, 2010.\\n[5] R. Samad and H. Sawada, “Extraction of the minimum\\nnumber of gabor wavelet parameters for the recogni-\\ntion of natural facial expressions,” Artiﬁcial Life and\\nRobotics , vol. 16, no. 1, pp. 21–31, 2011.\\n[6] J. Susskind, V . Mnih, G. Hinton et al. , “On deep\\ngenerative models with applications to recognition,” in\\nComputer Vision and Pattern Recognition (CVPR), 2011\\nIEEE Conference on . IEEE, 2011, pp. 2857–2864.\\n[7] S. Rifai, Y . Bengio, A. Courville, P. Vincent, and\\nM. Mirza, “Disentangling factors of variation for facial\\nexpression recognition,” Computer Vision–ECCV 2012 ,\\npp. 808–822, 2012.\\n[8] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner,\\n“Gradient-based learning applied to document recogni-\\ntion,” Proceedings of the IEEE , vol. 86, no. 11, pp. 2278–\\n2324, 1998.\\n[9] P. Ekman and D. Keltner, “Universal facial expressions\\nof emotion,” California Mental Health Research Digest ,\\nvol. 8, no. 4, pp. 151–158, 1970.\\n[10] H. Mliki, N. Fourati, M. Hammami, and H. Ben-\\nAbdallah, “Data mining-based facial expressions recog-\\nnition system.” in SCAI , 2013, pp. 185–194.\\n[11] P. Cotret, S. Chevobbe, and M. Darouich, “Embedded\\nwavelet-based face recognition under variable position,”\\ninSPIE/IS&T Electronic Imaging . International Society\\nfor Optics and Photonics, 2015, pp. 94 000A–94 000A.\\n[12] W. Wang, F. Chang, Y . Liu, and X. Wu, “Expression\\nrecognition method based on evidence theory and localtexture,” Multimedia Tools and Applications , pp. 1–15,\\n2016.\\n[13] P. Liu, S. Han, Z. Meng, and Y . Tong, “Facial expres-\\nsion recognition via a boosted deep belief network,” in\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , 2014, pp. 1805–1812.\\n[14] P. Burkert, F. Trier, M. Z. Afzal, A. Dengel, and\\nM. Liwicki, “Dexpression: Deep convolutional neu-\\nral network for expression recognition,” arXiv preprint\\narXiv:1509.05371 , 2015.\\n[15] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going\\ndeeper in facial expression recognition using deep neural\\nnetworks,” in Applications of Computer Vision (WACV),\\n2016 IEEE Winter Conference on . IEEE, 2016, pp. 1–\\n10.\\n[16] T. Zhang, W. Zheng, Z. Cui, Y . Zong, J. Yan, and\\nK. Yan, “A deep neural network-driven feature learning\\nmethod for multi-view facial expression recognition,”\\nIEEE Transactions on Multimedia , vol. 18, no. 12, pp.\\n2528–2536, 2016.\\n[17] ——, “A deep neural network-driven feature learning\\nmethod for multi-view facial expression recognition,”\\nIEEE Transactions on Multimedia , vol. 18, no. 12, pp.\\n2528–2536, 2016.\\n[18] B. Sun, L. Li, G. Zhou, and J. He, “Facial expression\\nrecognition in the wild based on multimodal texture\\nfeatures,” Journal of Electronic Imaging , vol. 25, no. 6,\\npp. 061 407–061 407, 2016.\\n[19] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, “Joint\\nﬁne-tuning in deep neural networks for facial expression\\nrecognition,” in Proceedings of the IEEE International\\nConference on Computer Vision , 2015, pp. 2983–2991.\\n[20] R. Islam, K. Ahuja, S. Karmakar, and F. Barbhuiya,\\n“Sention: A framework for sensing facial expressions,”\\narXiv preprint arXiv:1608.04489 , 2016.\\n[21] S. Xie and H. Hu, “Facial expression recognition with\\nfrr-cnn,” Electr onics Letters , vol. 53, no. 4, pp. 235–237,\\n2017.\\n[22] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisser-\\nman, “Return of the devil in the details: Delving deep\\ninto convolutional nets,” arXiv preprint arXiv:1405.3531 ,\\n2014.\\n[23] (2016, september) Caffe: Deep learning framework by\\nthe bvlc. http://caffe.berkeleyvision.org/.\\n[24] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face repre-\\nsentation from scratch,” arXiv preprint arXiv:1411.7923 ,\\n2014.\\n[25] A. Dhall et al. , “Collecting large, richly annotated facial-\\nexpression databases from movies,” 2012.\\n[26] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar,\\nand I. Matthews, “The extended cohn-kanade dataset\\n(ck+): A complete dataset for action unit and emotion-\\nspeciﬁed expression,” in 2010 IEEE Computer Society\\nConference on Computer Vision and Pattern Recognition-\\nWorkshops . IEEE, 2010, pp. 94–101.\\n[27] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus,\\n749\\nS. T. Hawk, and A. van Knippenberg, “Presentation and\\nvalidation of the radboud faces database,” Cognition and\\nemotion , vol. 24, no. 8, pp. 1377–1388, 2010.\\n[28] N. Aifanti, C. Papachristou, and A. Delopoulos, “The\\nmug facial expression database,” in Image Analysis for\\nMultimedia Interactive Services (WIAMIS), 2010 11th\\nInternational Workshop on . IEEE, 2010, pp. 1–4.\\n750\\n',\n",
       " '1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 1\\nFacial Expression Recognition with Identity and\\nEmotion Joint Learning\\nMing Li,Member,IEEE , Hao Xu, Xingchang Huang, Zhanmei Song, Xiaolin Liu, and Xin Li Fellow, IEEE\\nAbstract—Different subjects may express a speciﬁc expression in different ways due to inter-subject variabilities. In this work, besides\\ntraining deep-learned facial expression feature (emotional feature), we also consider the inﬂuence of latent face identity feature such as\\nthe shape or appearance of face. We propose an identity and emotion joint learning approach with deep convolutional neural networks\\n(CNNs) to enhance the performance of facial expression recognition (FER) tasks. First, we learn the emotion and identity features\\nseparately using two different CNNs with their corresponding training data. Second, we concatenate these two features together as a\\ndeep-learned Tandem Facial Expression (TFE) Feature and feed it to the subsequent fully connected layers to form a new model.\\nFinally, we perform joint learning on the newly merged network using only the facial expression training data. Experimental results\\nshow that our proposed approach achieves 99.31% and 84.29% accuracy on the CK+ and the FER+ database, respectively, which\\noutperforms the residual network baseline as well as many other state-of-the-art methods.\\nIndex Terms—Facial expression recognition, Emotion recognition, Face recognition, Joint learning, Transfer learning\\nF\\n1 I NTRODUCTION\\nFacial Expression Recognition (FER) is a well deﬁned\\ntask, aiming to recognize facial expressions with discrete\\ncategories (e.g., neutral, sad, contempt, happy, surprise,\\nangry, fear, disgust, etc.) or continuous levels (e.g., valance,\\narousal) from still images or videos. Although many recent\\nworks focus on video or image sequence based FER tasks\\n[1], [2], still image based FER still remains as a challenging\\nproblem. First, the differences between some facial expres-\\nsions might be subtle and thus difﬁcult to classify them ac-\\ncurately in some cases [3]. Second, different subjects express\\nthe same speciﬁc facial expression in different ways due to\\nthe inter-subject variability and their facial biometric shapes\\n[4], [5], [6], etc.\\nThese two challenging problems can be visualized in the\\nfollowing examples in Fig. 1. The left part of Fig. 1 contains\\ntwo representative faces and both of them are labeled with\\nthe ”sad” facial expression. However, their eight-category\\nclassiﬁcation scores have great differences in our initial\\nexperiment, which is shown in Table 1. The prediction for\\nthe left subject is reasonable as the ”sad” emotion ranks\\nthe top. However, for the person on the right, the score of\\n”angry” is a little higher than ”sad”, which did not correctly\\npredict her emotion.\\nGenerally, the inter-speaker variability of the faces could\\npotentially lead to errors in emotion classiﬁcation because\\nthe neutral face of one subject could already be very similar\\nto the typical faces of other emotion categories (e.g. the right\\nsubject in Fig. 1 and the ”angry” emotion). Furthermore,\\nneural science studies also show that the facial expression\\nand identity representations in human cortex are closely\\n\\x0fMing Li is with the Data Science Research Center at Duke Kunshan\\nUniversity and the ECE department of Duke University. Hao Xu and\\nXingchang Huang are with the School of Data and Computer Science,\\nSun Yat-sen University. Zhanmei Song, Xiaolin Liu, and Xin Li are with\\nthe School of Preschool Education, Shandong Yingcai University.\\nCorresponding author: Zhanmei Song, E-mail: songzhanmei@126.com\\nManuscript received; revised.connected to each other [7]. Therefore, we believe that if we\\nadd a compact description of the subject’s facial identity or\\nbiometric information as an auxiliary input to our model,\\nthe FER system can become more subject adapted and\\nrobust against the inter-speaker variability just as the role\\nof speaker adaptation technique in speech recognition tasks\\n[8], [9].\\nPrevious works show that deep neural network based\\nmethods have achieved excellent performance in face relat-\\ned recognition tasks [1] [10] [11] [12] [13] [14] [15]. In face\\nrecognition, Deep Convolutional Neural Networks (CNNs)\\noutperform traditional methods with hand-crafted features\\n[16] [17] [18] [19], and even perform better than human\\nbeings [12] [10] [11] [20]. However, in FER tasks, the system\\nperformance still needs to be further enhanced. Lack of\\nlarge scale labeled training data, inconsistent and unreliable\\nemotion labels and inter-subject variabilities all limit the\\nperformance of CNN on the FER task. Therefore, in this\\nwork, we aim to utilize additional face recognition training\\ndata to perform identity and emotion joint learning for FER.\\nRelated to our work, Xu et al. [21] proposed a transfer\\nlearning method from face recognition to FER using CNN\\ndirectly. Also, Jung et al. [1] proposed a joint ﬁne-tuning\\nmethod that jointly learns the parameters from image se-\\nquences. However, unlike these two methods, we do not\\ntransfer the network structures and parameters from face\\nrecognition to FER directly. Instead, we extract the high-\\nlevel identity feature from the face recognition network and\\nconsider it as an auxiliary input feature for our FER model.\\nAs shown in Fig. 2, we concatenate both the high-level\\nemotion and identity features as Tandem Facial Expression\\n(TFE) features and feed it to the subsequent fully connected\\nlayers to form a new network.\\nIn this paper, we adopt the CNN architecture to discover\\nlatent identity and emotion features. First, we pre-train\\nlatent emotion and identity features separatively using two\\ndifferent CNNs (ResNet [22] for emotion and DeepID [10]\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 2\\nFig. 1. Two example face images with ”sad” facial expressions. The left part is the original faces of these two subjects and the right part is the faces\\nwith detected landmarks.\\nTABLE 1\\nThe predicted scores on 8 facial expression categories for the two example images in Fig. 1.\\nFigure Neutral Angry Contempt Disgust Fear Happy Sad Surprise\\nLeft 0.2536 0.0042 0.0038 0.0004 0.0025 0.0003 0.7332 0.0020\\nRight 0.0002 0.5110 0.0 0.0009 0.0001 0.0001 0.4876 0.0001\\nFig. 2. Our model consists of two convolutional neural networks. The left one represents the DeepID network learning the identity features. The\\nright deep residual network is trained with facial expression databases. After training separatively, the identity feature and the deep-learned emotion\\nfeature are concatenated as the TFE features and feed to the subsequent fully connected layers. Finally, we perform joint learning on the new\\nmerged network using only the facial expression database.\\nfor identity) with their own training data. Furthermore, we\\nmerge these two networks together by concatenating the\\ndeep-learned features and feed to a new fully connected\\nlayer. Finally, we use FER training data to jointly learn the\\nparameters of the merged new network. To the best of our\\nknowledge, there is no previous work using auxiliary deep\\nidentity feature with deep emotion feature together for joint\\nfacial expression learning.\\n2 R ELATED WORK\\nIn this section, we will introduce two main types of features\\nused in the FER task, namely hand-crafted features and\\ndeep-learned features.2.1 Hand-Crafted Feature Based Method\\nBefore deep learning based approaches dominate face recog-\\nnition and FER tasks, many works have been conducted\\nbased on the hand-crafted features [3]. These approaches\\nusually perform frontend feature extraction and backend\\nclassiﬁcation separately [15]. During the stage of feature\\nextraction, traditional features, such as Local Binary Pat-\\nterns (LBP) [16] [18], Gabor wavelet coefﬁcients [17], Scale-\\nInvariant Feature Transform (SIFT) [19], and Gaussian Face\\n[20] are designed with prior domain knowledge. Moreover,\\nsupervised classiﬁers, such as SVM [23], feedforward Neu-\\nral Network (NN) [24] and Extreme Learning Machine [25],\\n[26] are adopted for the subsequent modeling.\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 3\\n2.2 Deep-Learned Feature Based Method\\nGenerally, deep learning based methods outperform hand-\\ncrafted feature based approaches and achieve state-of-the-\\nart performance on both face recognition [12] [10] [11] and\\nFER [27] [1] [15] [2] tasks. For example, Sun et al. [12] pro-\\nposed CNN model and DeepID features for face recognition.\\nIn order to boost the performance, Sun et al. [10] proposed\\na Siamese network to train in face pairs. Furthermore, CNN\\nmodels have been widely used in the FER task. Tang et al.\\n[13] replace the softmax layer with SVM in the CNN frame-\\nwork and achieved the best accuracy on FER2013 dataset\\n[28] in the ICML 2013 Representation Learning Challenge.\\nEmad et al. [29] then proposed a FER+ dataset with more\\naccurate labels and produced a benchmark on this dataset\\nwith VGG13 network. Jung et al. [1] and Zhao et al. [2] both\\nconsider temporal structures on top of CNNs to model the\\nimage sequences.\\nRecent high performance models normally accompany\\nwith deep architectures and a large number of convolutional\\nkernels. Alex et al. [30] has proposed AlexNet for the Ima-\\ngeNet challenge and achieve superior performance at that\\ntime. Subsequently, Karen et al. [31] presented their deeper\\nnetworks with 16 and 19 layers respectively and found\\nthat deeper structures can achieve better performance. Fur-\\nthermore, He et al. [22] proposed deep residual network\\n(ResNet) and trained a CNN with 152 layers. ResNet can\\nconverge faster and perform more accurately due to its\\nresidual learning mechanism, shortcut connection [22] and\\nbatch normalization [32]. Also, Christian et al. [33] proposed\\nGoogleNet and its inception v4 architecture. They further\\ncombined the residual network architecture with Inception-\\nv4 as Inception-ResNet [34] and achieve better performance\\non the ImageNet dataset [35]. In this work, our approach\\nadopts deep ResNet and CNN for their high performance\\nand less chance of overﬁtting on image related tasks.\\n2.3 Transfer Learning with CNN\\nA recent work proposed by Xu et al. [21] used transfer\\nlearning with CNN for FER. The difference between our\\nwork and their work is that we learn both the emotion and i-\\ndentity features using two separate deep convolutional neu-\\nral networks and construct a deep-learned Tandem Facial\\nExpression (TFE) feature in the merged model instead of just\\ntransferring the weights of the pre-trained face recognition\\nnetworks and ﬁne-tuning. Compared with the work by Jung\\net al. [1], we use feature-level concatenation of deep-learned\\nidentity and emotion features to form a new network with\\njoint learning rather than ﬁne-tuning two softmax layers\\nwith the landmark-based features.\\n3 O URAPPROACH\\nIn this section, we will introduce our proposed method in\\ndetails.\\n3.1 Overview of our Network Architecture\\nAs shown in Fig. 2, our model consists of two CNNs. The left\\none is the DeepID network proposed in [10], [12], containing\\nfour convolutional layers. Actually, the architecture we use\\nis the same as the ConvNet structure proposed in [10], which\\nFig. 3. Our ResNet12 and ResNet18 architectures. ”project”, ”identity”\\nand ”gap” are projection block, identity block and global average pooling,\\nrespectively.\\nlearns fully-connected layer (DeepID2 feature) from both the\\nthird and fourth convolutional layers, generating a compact\\nfeature representation with 160 dimensions. The right net-\\nwork is constructed according to the deep ResNet [22] and\\nwe choose the ResNet18 structure and also build a shallower\\nnetwork called ResNet12 for different tasks based on the\\nsize of the input images and the size of datasets. In ResNet,\\nwe use shortcut connection for deep residual learning and\\nbatch normalization layers [32] for faster convergence and\\nbetter accuracy. We do not use fully-connected layer (FC)\\nto ﬂatten the feature maps. Instead, we use global average\\npooling (Gap) to generate a compact representation with\\nreduced number of parameters [22]. But for convenience,\\nwe still use fully-connected layer to model the concatenated\\nTFE features.\\nActually, the left and the right networks are considered\\nas a merged joint network in Fig. 2. During training, the\\nfeatures of DeepID network and ResNet are concatenated\\nas the input for fully-connected layers and jointly learn the\\nentire network for FER tasks. In order to guide this merged\\nnetwork to extract identity and emotion features, we do not\\ntrain from scratch but pre-train the weights of both two sub-\\nnetworks separately with the corresponding datasets except\\nthe fully connected layer.\\nDeep residual network (ResNet) has achieved great suc-\\ncess in multiple challenges [22]. ResNet is based on the deep\\nresidual learning framework and it is easier to optimize\\nthe residual rather than the original mapping. Therefore,\\nin our work we adopt ResNet for training emotion features.\\nSpeciﬁcally, the architecture of residual networks is made\\nup of the following two blocks shown in Fig. 4. The left one,\\ncalled ”identity block”, contains a shortcut link connecting\\nthe input x to the convolution output. The right one, called\\n”projection block”, contains a convolution operation in the\\nshortcut connection, aiming to ensure the same output size\\nof feature maps using a 1\\x021convolution with a stride of 2\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 4\\nFig. 4. The building blocks of our residual network. ndenotes the\\nnumber of ﬁlters in convolution.\\nwhile doing the element-wise sum at the output.\\nIn our work, the right block is used to increase the\\nnumber of convolutional kernels and decrease the output\\nsize of feature maps by half. The stride for each convolution\\noperation is 2 and there is no max-pooling layer. After the\\nlast convolution operation, we use Global Average Pooling\\n(Gap) layer to generate a 64-dimensional emotion feature as\\nshown in Fig. 2. Considering the size of datasets, we use two\\nResNet structures. The ﬁrst one is ResNet18 structure for\\nprocessing the FER+ dataset [29] with over 35k 48\\x0248im-\\nages. For the smaller dataset, CK+, we reduce the number of\\nparameters by removing four identity building blocks when\\nn= 16; 16;32;64and adding one project building block\\nwhen n= 16 . We call this shallower network as ResNet12.\\nWe adopt batch normalization after each convolution and\\nbefore the rectiﬁed linear unit (ReLU) activations [36] [22].\\nThe architectures of these two ResNets are shown in Fig. 3.\\n3.2 Identity and Emotion Feature Concatenation\\nSuppose that the identity and emotion features of an arbi-\\ntrary input image are represented as ZiandZe, respectively.\\nThen, we can reconstruct the new TFE representation Ztfe\\nby concatenating ZiandZetogether.\\nHowever, sometimes the deep-learned ZiandZefea-\\ntures are not in the same scale because both network struc-\\nture and training data are different. In our work, we ﬁrstly\\nnormalize ZiandZewith batch normalization [32]. Then\\nwe concatenate these two features together to form the TFE\\nfeature Ztfe.\\n4 E XPERIMENTAL RESULTS\\n4.1 Datasets\\nIn this work, we evaluate the proposed method on t-\\nwo popular FER datasets, namely Extended Cohn-Kanade\\n(CK+) database [37] and FER+ database [29]. These two\\nFER datasets are used for deep-learned emotion feature\\nextraction and joint learning, while the identity features are\\nlearned from the CASIA-WebFace database [38].\\n\\x0fCASIA-WebFace [38]: In this dataset, the face images\\nare collected from the Internet, containing 10,575\\nsubjects and 494,414 images. It is usually considered\\nas a standard large scale training dataset for face\\nrecognition challenges [39].\\x0fLFW: LFW (Labeled Faces in the Wild) dataset con-\\ntains 13,233 face images from 5,749 identities collect-\\ned on the Internet. As a benchmark for comparison,\\nLFW suggests reporting performance with 10-fold\\ncross validation using splits they have randomly\\ngenerated (6,000 pairs) [39]. In this study, the LFW\\ndatabase is adopted purely as a stand alone testing\\ndata to evaluate the quality of our identity features\\ntrained by the CASIA-WebFace dataset.\\n\\x0fCK+: The CK+ database includes 327 image se-\\nquences with labeled facial expressions. For each\\nimage sequence, only the last frame is provided with\\nan expression label. In order to collect more images\\nfor training, we usually selected the last three frames\\nof each sequence for training or validation purpose.\\nAdditionally, the ﬁrst frame from each of the 327\\nlabeled sequences would be chosen as the ”neutral”\\nexpression. As a result, this dataset can provide total-\\nly 1308 images with 8 labeled facial expressions. For\\ntesting, we follow the 10-fold cross validation testing\\nprotocol on the CK+ database.\\n\\x0fFER+: This dataset comes from the face expression\\nrecognition challenge [28] in the ICML 2013 Rep-\\nresentation Learning Workshop. It consists 28,709\\n48\\x0248face images for training. The test set has\\n3,589 images and there are totally 7 discrete facial\\nexpressions (anger, disgust, fear, happiness, sadness,\\nand surprise) for classiﬁcation. However, due to it-\\ns noisy labels, this dataset is labeled again using\\ncrowd-sourced services [29]. In this study, we simply\\nuse majority voting to derive the new set of labels for\\nour experiments.\\n4.2 Parameter Settings\\nFor the DeepID network, we follow the same parameter\\nsetting in [12], with a 160-dimensional representation in the\\nfully-connected layers. A dropout layer is used after the\\nDeepID layer, with a probability of 0.4 to reduce over-ﬁtting\\n[40].\\nFor the deep ResNet, we have two different settings for\\nthe number of layers. We use the ResNet18 architecture for\\nthe FER+ dataset but use a shallower network ResNet12 for\\nthe CK+ dataset, which has much less data for training and\\ntesting.\\nAs for Stochastic Gradient Descent (SGD) method dur-\\ning back propagation [41], we apply different parameters\\nin CK+ dataset and FER+ dataset. For CK+, we initialize\\nthe learning rate as 0.16 and 0.01 respectively for ResNet\\nand DeepID network with a mini-batch size of 128 and\\na momentum of 0.9 [42]. The networks in our model are\\ntrained for up to 200 epochs and ﬁne-tuned for up to 100\\nepochs as well. For FER+, the learning rate is initialized as\\n0.1 for ResNet training and 0.001 for ﬁnal joint learning.\\n4.3 Pre-Processing\\nFor CASIA-WebFace dataset, we need to use pre-processing\\npipeline, including face detection, face landmarks detection\\nface alignments and face cropping. We use the tools from\\nmmlab, CUHK [43] to detect face and landmarks. After\\nthese processing steps, those missed faces are removed\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 5\\nTABLE 2\\nAverage accuracy on CK+ of our models using 10-fold cross validation\\nOur Methods Average Accuracy on CK+\\nResNet12 97.56%\\nTFE-JL 99.31%\\nand there are totally 435,863 faces remaining. Then we use\\na template (the ﬁrst image) in the LFW [39] dataset to\\nalign the faces in the CASIA-WebFace. Finally, images from\\nthe datasets we use (CASIA-WebFace, CK+, LFW) will be\\ncropped in the same way retaining the eye brow and jaw. As\\nLFW provides a deep-funneled version and CK+ is collected\\ncontaining the frontal whole faces, there is no need to do\\nface alignment. FER+ has been pre-processed and cropped\\nas well.\\nDuring training the DeepID network using CASIA-\\nWebFace, we randomly select one image from each person\\nfor validation and therefore generate a validation set of\\n10,575 images, while the remaining images are used for\\ntraining. It is worth noting that the training set of CASIA-\\nWebFace is augmented with horizontal ﬂipping while the\\ntraining set in FER+ is augmented with horizontal ﬂipping,\\nshifting and rotation. In addition, all the images are pre-\\nprocessed with per-pixel mean subtraction and standard\\ndeviation normalization [13].\\n4.4 Evaluation of identity features on the LFW database\\nAfter the pre-processing step, we train our DeepID net-\\nwork for extracting the auxiliary identity feature on CASIA-\\nWebFace. After training 200 epochs for DeepID network, the\\naccuracy on LFW for face veriﬁcation can achieve 91% using\\ncosine similarity with a 0.15 threshold.\\nSince our goal is to extract a reasonable good quality\\nidentity feature, we may not need to fully optimize the face\\nveriﬁcation performance on LFW. We use a single DeepID\\nnetwork and single patch for each face image without any\\nensemble method.\\n4.5 Evaluation of FER on the CK+ database\\nDuring the 10-fold cross validation testing, we train our\\nResNet from scratch for each rotation with 200 epochs. After\\nthe ﬁrst step training, we combine these two networks,\\nwhich extract identity features and emotion features respec-\\ntively, to form a 224-dimensional TFE representation. Final-\\nly, we jointly learn the parameters of the merged network\\nwith the CK+ training data.\\nOne example of the training and joint learning process\\non the CK+ database is provided in Fig. 5. The training accu-\\nracy (blue) increase gradually while the validation accuracy\\n(orange) increase with ﬂuctuation but ﬁnally converge to\\n97.56%. In the joint learning stage using the TFE feature, the\\naccuracy on the validation set converges faster and better\\nthan the ﬁrst training stage and can achieve up to 99.24%. As\\nshown in Table 2, the performance of our proposed method\\noutperforms the ResNet baseline by 1.68% absolutely.\\nBesides the comparisons with our own implemented\\nbaselines, we also compare our method with other state-\\nof-the-art approaches. Our method can achieve around 2%\\nFig. 5. Training and validation performance on the CK+ database during\\nthe training and the joint learning stage.\\nabsolute improvement compared with PPDN model [2] as\\nshown in Table 3. Actually, the sequence-based PPDN also\\ncan achieve 99.3% accuracy on the test set but it was pre-\\ntrained on CASIA-WebFace and used the whole sequence of\\nimages in CK+ for training, which does not match with our\\nexperimental setting (only the ﬁrst one and the last three\\nframes of each CK+ sequence are used).\\nTABLE 3\\nComparion with state-of-the-art methods\\nMethods Average Accuracy on CK+\\nDTAGN(Weighted Sum) [1] 96.9\\nDTAGN(Joint) [1] 97.3\\nIntraFace [44] 96.4\\nBDBN [15] 96.7\\nCNN [45] 95.8\\nPPDN [2] 97.3%\\nTFE-JL 99.3%\\n4.6 Evaluation FER on the FER+ database\\nSimilarly, we show the performance of our proposed meth-\\nods on the FER+ dataset in Table 4. We train these two\\nmodels on the training set and evaluate them on the private\\ntest set. We mainly focus on the private test set for direct\\ncomparison with VGG13(MV) proposed by [29]. Speciﬁcally,\\nwe train our ResNet18 on the training set with 200 epochs\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 6\\nFig.\\n6. Training and validation performance on the FER+ dataset during\\nthe training and the joint learning stage.\\nTABLE 4\\nAccuracy on FER+ of our proposed methods\\nMethods Accuracy\\non FER+\\nResNet18 83.1%\\nResNet18\\n+ FC 83.4%\\nTFE-joint\\nlearning 84.3%\\nand\\nthe pre-trained accuracy is 83.1% on the private test set.\\nThen we jointly learn the network using TFE features gener-\\nated from DeepID and ResNet, getting an improvement up\\nto 1.2% from 83.1% to 84.3%.\\nIn Table 5, we compare our methods with the state-\\nof-the-art approaches on FER2013 and FER+. The work\\nDLSVM-L2 has been presented in [13] and ranks the top\\nin the ICML2013 Representation Learning Challenge on\\nthe FER2013 dataset. The VGG13(MV) system outperforms\\nthe DLSVM-L2 baseline as it used new labels on FER+.\\nTherefore, we just compare our proposed TFE-JL method\\nwith VGG13(MV). As shown in Table 5, the proposed TFE-JL\\nmethod also achieves 0.5% accuracy gain compared with the\\naverage performance of VGG13(MV) model, which again\\nshows the advantage and effectiveness of the proposed\\nidentity and emotion joint learning framework.\\nWe also implement the single network transfer learning\\nmethod in [21] by directly using the deep-id identity feature\\nlearned from CASIA-WebFace as inputs for the subsequent\\nSVM modeling on FER+ database. Results in Table 5 showTABLE 5\\nAccuracy on FER2013 with old and new labels compared with the\\nstate-of-the-art methods\\nLabels Methods Accuracy\\non FER2013\\nOld DLSVM-L2\\n[13] 71.2%\\nFER2013 Zhou\\net al. [28] 69.3%\\nMaxim\\nMilakov [28] 68.8%\\nRadu+Marius+Cristi\\n[28] 67.5%\\nOur\\nimplementation of [21] 71.1%\\nNew VGG13(MV)\\n[29] 83.8%\\nFER+ TFE-JL 84.3%\\nthat\\nour proposed joint learning method also outperforms\\nthe single network transfer learning approach.\\nFurthermore, considering the problem that the face im-\\nages in FER2013 or FER+ dataset are not aligned as well as in\\nCASIA-WebFace used for pre-training the DeepID network,\\nthe gain of our joint learning approach on FER+ may not be\\nas large as in the CK+ database.\\nIn order to demonstrate how our proposed identity and\\nemotion joint learning method improves the FER perfor-\\nmance, we list the output scores of 8 facial expression\\ncategories on ten representative face images from the test\\nset in Table 6. These face images are misclassiﬁed using our\\nbaseline (ResNet) model but are corrected by the proposed\\nidentity and emotion joint learning method. As shown in\\nFig. 7, these images were ﬁrstly misclassiﬁed, the reason\\nmight be their appearances. For example, image (5) looks\\n”sad” and image (7-8) look ”angry”. By adding their identity\\ninformation as an auxiliary input to our FER model, the\\nTFE joint learning approach could reduce the inter-subject\\nvariability.\\n5 C ONCLUSION AND FUTURE WORK\\nIn this work, we learn both the emotion and identity features\\nusing two separate deep convolutional neural networks\\nand construct a deep-learned Tandem Facial Expression\\n(TFE) feature by feature level concatenation. We perform\\nﬁne-tuning on the newly merged model instead of just\\ntransferring the weights of the pre-trained face recognition\\nnetworks. Experimental results show that the proposed\\napproach outperforms the residual network baseline as well\\nas many other state-of-the-art methods on two popular FER\\ndatabases, namely CK+ and FER+. Future works include\\ninvestigating the effect of face image format differences or\\nalignment mismatch between face recognition data and FER\\ndata as well as exploring other transfer learning and multi-\\ntask learning methods for the FER task.\\nACKNOWLEDGMENTS\\nThis research was funded in part by the National Nat-\\nural Science Foundation of China (61773413), Natural\\nScience Foundation of Guangzhou City (201707010363),\\nNational Key Research and Development Program\\n(2016YFC0103905).\\nREFERENCES\\n[1] H. Jung, S. Lee, J. Yim, and S. Park, “Joint ﬁne-tuning in deep\\nneural networks for facial expression recognition,” in IEEE Inter-\\nnational Conference on Computer Vision, 2015, pp. 2983–2991.\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 7\\nFig. 7. Ten representative face images whose prediction is corrected by our joint learning method. These face images are indexed with number 1 to\\n10 from left to right, top to bottom.\\nTABLE 6\\nThe predicted scores of those representative images in Fig. 7 using ResNet and our TFE joint learning method. For each face image, the ﬁrst line\\nis the output score using the ResNet baseline and the second line is the output score of our TFE joint learning method.\\nImage Neutral Happy Surprise Sad Angry Disgust Fear Contempt\\n1 0.0 0.0093 0.9905 0.0767 0.0 0.0001 0.0001 0.0\\n0.0 0.6180 0.3726 0.0 0.0005 0.0012 0.00072 0.0004\\n2 0.0023 0.3341 0.6633 0.0 0.0001 0.0 0.0 0.0001\\n0.0005 0.6191 0.3493 0.0 0.0200 0.0076 0.0032 0.0002\\n3 0.4771 0.0 0.0 0.5229 0.0 0.0 0.0 0.0\\n0.7262 0.0 0.0 0.2735 0.0 0.0002 0.0 0.0\\n4 0.3601 0.6396 0.0 0.0 0.0 0.0 0.0 0.0003\\n0.6632 0.3219 0.0002 0.0001 0.0058 0.0005 0.0003 0.0081\\n5 0.4883 0.0 0.0 0.5097 0.0020 0.0001 0.0 0.0\\n0.7153 0.0 0.0 0.2837 0.0003 0.0005 0.0001 0.0002\\n6 0.4172 0.0 0.0 0.5798 0.0002 0.0006 0.0 0.0022\\n0.7978 0.0 0.0 0.1997 0.0001 0.0013 0.0001 0.0009\\n7 0.4940 0.0 0.0 0.0019 0.5037 0.0004 0.0 0.0\\n0.5742 0.0 0.0006 0.0047 0.4027 0.0097 0.0017 0.0063\\n8 0.0059 0.0 0.0 0.1972 0.7967 0.0 0.0 0.0001\\n0.0095 0.0 0.0 0.7818 0.2000 0.0017 0.0049 0.0020\\n9 0.3260 0.0007 0.2076 0.0275 0.0016 0.0020 0.4337 0.0010\\n0.7291 0.0081 0.0612 0.0347 0.0397 0.0683 0.0420 0.0168\\n10 0.0983 0.0 0.3654 0.0006 0.0096 0.0 0.5260 0.0002\\n0.0560 0.0 0.5034 0.0046 0.0176 0.0106 0.4046 0.0031\\n[2] X. Zhao, X. Liang, L. Liu, T. Li, Y. Han, N. Vasconcelos, and S. Yan,\\n“Peak-piloted deep network for facial expression recognition,” in\\nEuropean Conference on Computer Vision. Springer, 2016, pp. 425–\\n442.\\n[3] C. A. Corneanu, M. O. Sim ´on, J. F. Cohn, and S. E. Guerrero,\\n“Survey on rgb, 3d, thermal, and multimodal approaches for\\nfacial expression recognition: History, trends, and affect-related\\napplications,” IEEE transactions on pattern analysis and machine\\nintelligence, vol. 38, no. 8, pp. 1548–1568, 2016.\\n[4] A. Mohammadian, H. Aghaeinia, and F. Towhidkhah, “Incorpo-\\nrating prior knowledge from the new person into recognition of\\nfacial expression,” Signal, Image and Video Processing, vol. 10, no. 2,\\npp. 235–242, 2016.\\n[5] S. Rifai, Y. Bengio, A. Courville, P . Vincent, and M. Mirza, “Disen-\\ntangling factors of variation for facial expression recognition,” in\\nEuropean Conference on Computer Vision, 2012, pp. 808–822.\\n[6] P . Werner, A. Al-Hamadi, K. Limbrecht-Ecklundt, S. Walter,\\nS. Gruss, and H. C. Traue, “Automatic pain assessment with facial\\nactivity descriptors,” IEEE Transactions on Affective Computing,\\nvol. 8, no. 3, pp. 286–299, 2017.\\n[7] K. Dobs, J. Schultz, I. B ¨ulthoff, and J. L. Gardner, “Task-dependent\\nenhancement of facial expression and identity representations in\\nhuman cortex,” NeuroImage, vol. 172, pp. 689–702, 2018.\\n[8] G. Saon, H. Soltau, D. Nahamoo, and M. Picheny, “Speaker\\nadaptation of neural network acoustic models using i-vectors,”\\ninAutomatic Speech Recognition and Understanding, 2014, pp. 55–59.\\n[9] V . Gupta, P . Kenny, P . Ouellet, and T. Stafylakis, “I-vector-based\\nspeaker adaptation of deep neural networks for french broadcast\\naudio transcription,” in IEEE International Conference on Acoustics,\\nSpeech and Signal Processing, 2014, pp. 6334–6338.\\n[10] Y. Sun, X. Wang, and X. Tang, “Deep learning face representationby joint identiﬁcation-veriﬁcation,” Advances in Neural Information\\nProcessing Systems, vol. 27, pp. 1988–1996, 2014.\\n[11] Y. Sun, D. Liang, X. Wang, and X. Tang, “Deepid3: Face recognition\\nwith very deep neural networks,” Computer Science, 2015.\\n[12] Y. Sun, X. Wang, and X. Tang, “Deep learning face representation\\nfrom predicting 10,000 classes,” in IEEE Conference on Computer\\nVision and Pattern Recognition, 2014, pp. 1891–1898.\\n[13] Y. Tang, “Deep learning using support vector machines,” ICML\\nWorkshop on Representational Learning, 2013.\\n[14] Y. Sun, X. Wang, and X. Tang, “Hybrid deep learning for face\\nveriﬁcation,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 38, no. 10, pp. 1997–2009, 2016.\\n[15] P . Liu, S. Han, Z. Meng, and Y. Tong, “Facial expression recognition\\nvia a boosted deep belief network,” in IEEE Conference on Computer\\nVision and Pattern Recognition, 2014, pp. 1805–1812.\\n[16] X. Feng, M. Pietikinen, and A. Hadid, “Facial expression recog-\\nnition based on local binary patterns,” Computer Engineering and\\nApplications, vol. 17, no. 4, pp. 592–598, 2007.\\n[17] C. Liu and H. Wechsler, “Gabor feature based classiﬁcation using\\nthe enhanced ﬁsher linear discriminant model for face recogni-\\ntion.” IEEE Transactions on Image Processing , vol. 11, no. 4, p. 467,\\n2002.\\n[18] T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with\\nlocal binary patterns: Application to face recognition,” IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence , vol. 28, no. 12,\\npp. 2037–2041, 2006.\\n[19] D. G. Lowe, “Distinctive image features from scale-invariant key-\\npoints,” International Journal of Computer Vision, vol. 60, no. 2, pp.\\n91–110, 2004.\\n[20] C. Lu and X. Tang, “Surpassing human-level face veriﬁcation\\nperformance on lfw with gaussianface,” Computer Science, 2014.\\n1949-3045 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2018.2880201, IEEE\\nTransactions on Affective Computing\\nIEEE TRANSACTION ON AFFECTIVE COMPUTING, VOL. 14, NO. 8, AUGUST 2015 8\\n[21] M. Xu, W. Cheng, Q. Zhao, L. Ma, and F. Xu, “Facial expression\\nrecognition based on transfer learning from deep convolutional\\nnetworks,” in International Conference on Natural Computation, 2016,\\npp. 702–708.\\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\\nfor image recognition,” in IEEE Conference on Computer Vision and\\nPattern Recognition, June 2016, pp. 770–778.\\n[23] B. E. Boser, I. M. Guyon, and V . N. Vapnik, “A training algorithm\\nfor optimal margin classiﬁers,” in The Workshop on Computational\\nLearning Theory, 1992, pp. 144–152.\\n[24] L. Ma and K. Khorasani, “Facial expression recognition using\\nconstructive feedforward neural networks,” IEEE Transactions on\\nSystems Man Cybernetics Part B Cybernetics , vol. 34, no. 3, p. 1588,\\n2004.\\n[25] S. J. Wang, H. L. Chen, W. J. Yan, Y. H. Chen, and X. Fu, “Face\\nrecognition and micro-expression recognition based on discrim-\\ninant tensor subspace analysis plus extreme learning machine,”\\nNeural Processing Letters, vol. 39, no. 1, pp. 25–43, 2014.\\n[26] D. Ghimire and J. Lee, “Extreme learning machine ensemble using\\nbagging for facial expression recognition,” Journal of Information\\nProcessing Systems, vol. 10, no. 3, p. 443 458, 2014.\\n[27] Z. Yu and C. Zhang, “Image based static facial expression recog-\\nnition with multiple deep network learning,” in ACM on Interna-\\ntional Conference on Multimodal Interaction, 2015, pp. 435–442.\\n[28] I. J. Goodfellow, D. Erhan, P . L. Carrier, A. Courville, M. Mirza,\\nB. Hamner, W. Cukierski, Y. Tang, D. Thaler, and D. H. Lee,\\n“Challenges in representation learning: A report on three machine\\nlearning contests,” Neural Networks, vol. 64, p. 59, 2015.\\n[29] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, “Training deep\\nnetworks for facial expression recognition with crowd-sourced\\nlabel distribution,” in ACM International Conference on Multimodal\\nInteraction, 2016, pp. 279–283.\\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-\\ncation with deep convolutional neural networks,” Communications\\nof the ACM, vol. 60, no. 2, p. 2012, 2012.\\n[31] K. Simonyan and A. Zisserman, “Very deep convolutional net-\\nworks for large-scale image recognition,” Computer Science, 2014.\\n[32] S. Ioffe and C. Szegedy, “Batch normalization: accelerating deep\\nnetwork training by reducing internal covariate shift,” in Interna-\\ntional Conference on Machine Learning, 2015, pp. 448–456.\\n[33] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\\nconvolutions,” in IEEE Conference on Computer Vision and Pattern\\nRecognition, 2015, pp. 1–9.\\n[34] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. A. Alemi, “Inception-\\nv4, inception-resnet and the impact of residual connections on\\nlearning,” in AAAI Conference on Artiﬁcial Intelligence , 2017, pp.\\n4278–4284.\\n[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, and M. Bernstein, “Imagenet\\nlarge scale visual recognition challenge,” International Journal of\\nComputer Vision, vol. 115, no. 3, pp. 211–252, 2015.\\n[36] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nboltzmann machines,” in International Conference on Machine Learn-\\ning, 2010, pp. 807–814.\\n[37] T. Kanade, J. F. Cohn, and Y. Tian, “Comprehensive database\\nfor facial expression analysis,” in IEEE International Conference\\non Automatic Face and Gesture Recognition, 2000. Proceedings , 2002,\\np. 46.\\n[38] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation\\nfrom scratch,” in IEEE Conference on Computer Vision and Pattern\\nRecognition, 2014.\\n[39] G. B. Huang, M. A. Mattar, H. Lee, and E. Learned-Miller, “Learn-\\ning to align from scratch,” Advances in Neural Information Processing\\nSystems, pp. 764–772, 2012.\\n[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\\nR. Salakhutdinov, “Dropout: a simple way to prevent neural\\nnetworks from overﬁtting,” Journal of Machine Learning Research,\\nvol. 15, no. 1, pp. 1929–1958, 2014.\\n[41] Y. L. Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D.\\nJackel, and D. Henderson, “Handwritten digit recognition with\\na back-propagation network,” in Advances in Neural Information\\nProcessing Systems, 1990, pp. 396–404.\\n[42] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the im-\\nportance of initialization and momentum in deep learning,” in\\nInternational Conference on Machine Learning, 2013, pp. 1139–1147.[43] Y. Sun, X. Wang, and X. Tang, “Deep convolutional network\\ncascade for facial point detection,” in IEEE Conference on Computer\\nVision and Pattern Recognition, 2013, pp. 3476–3483.\\n[44] F. D. L. Torre, W. S. Chu, X. Xiong, and F. Vicente, “Intraface,” in\\nIEEE International Conference and Workshops on Automatic Face and\\nGesture Recognition, 2015, pp. 1–8.\\n[45] A. T. Lopes, E. D. Aguiar, A. F. D. Souza, and T. Oliveira-\\nSantos, “Facial expression recognition with convolutional neural\\nnetworks: Coping with few data and the training sample order,”\\nPattern Recognition, vol. 61, pp. 610–628, 2017.\\nMing Li received his Ph.D. in Electrical Engi-\\nneering from University of Southern California in\\nMay 2013. He is currently an associate professor\\nof Electrical and Computer Engineering at Duke\\nKunshan University and a research scholar at\\nthe ECE department of Duke University. His re-\\nsearch interests are in the areas of speech pro-\\ncessing and multimodal behavior signal analysis\\nwith applications to human centered behavioral\\ninformatics notably in health, education and se-\\ncurity.\\nHao Xu is an undergraduate student at School\\nof Data and Computer Science, Sun Y at-sen\\nUniversity. He is a research assistant in Speech\\nand Multimodal Intelligent Information Process-\\ning Laboratory. His research interests include\\nfacial expression recognition and gaze recogni-\\ntion.\\nXingchang Huang is an undergraduate student\\nat Sun Y at-sen University, major in computer\\nscience. He is a research assistant in Speech\\nand Multimodal Intelligent Information Process-\\ning Laboratory. His research interests include\\nmachine learning, computer vision and their ap-\\nplication on face recognition and facial expres-\\nsion recognition.\\nZhanmei Song received the Ph.D. degree in\\nEarly Childhood Education(ECE) from East Chi-\\nna Normal University in 2012. She is currently\\na professor at Department of Early Childhood\\nEducation of Shandong Yingcai University. Her\\nresearch interests include Compensation Edu-\\ncation for Children Left-behind and Kindergarten\\nCurriculum. She is the General Secretary of the\\nNational Association of ECE,Chinese Society of\\nEducation.\\nXiaolin Liu received a master degree in Early\\nChildhood Education from Northeast China Nor-\\nmal University in 2009,and now studying Doctor\\nDegree of Education Management at Dhurakij\\nPundit University, Thailand. She is currently an\\nAssociate Professor in the School of Preschool\\nEducation at Shandong Yingcai University. Her\\nresearch interests include multimodel technolo-\\ngy in early childhood education.\\nXin Li received the Ph.D. degree in Electrical\\nand Computer Engineering from Carnegie Mel-\\nlon University in 2005. He is currently a Pro-\\nfessor in the School of Preschool Education at\\nShandong Yingcai University. His research inter-\\nests include signal processing and data analyt-\\nics. He is a Fellow of IEEE.\\n',\n",
       " '5th Conference on Knowledge-Based Engineering and Innovation , Iran University of Science and Technology, Tehran, Iran  \\n \\n978-1-7281-0872-8/19/$31.00 ©2019 IEEE  Fast Facial emotion recognition Using Convolutional \\nNeural Networks and Gabor Filters \\n \\nMilad Mohammad Taghi Zadeh  \\nDepartment of Electrical Engineering  \\nKhatam University \\nTehran, Iran \\nm.mohammadtaghizadeh@khatam.ac.ir  \\n Maryam Imani \\nDepartment of Electrical Engineering  \\nTarbiat Modarres University \\nTehran, Iran \\nmaryam.imani@modares.ac.ir \\n   Babak Majidi \\nDepartment of Computer Engineering  \\nKhatam University \\nTehran, Iran \\nb.majidi@khatam.ac.ir   \\n \\nAbstract - The emotions evolved in human face have a great \\ninfluence on decisions and arguments about various subjects. In \\npsychological theory, emotional states of a person can be classified \\ninto six main categories: surprise, fear, disgust, anger, happiness \\nand sadness. Automatic extraction of these emotions from the face \\nimages can help in human computer interaction as well as many \\nother applications. Machine learning algorithms and especially \\ndeep neural network can learn complex features and classify the \\nextracted patterns. In this paper, a deep learning based \\nframework is proposed for human emotion recognition. The \\nproposed framework uses the Gabor filters for feature extraction \\nand then a Convolutional Neural Network (CNN) for \\nclassification. The experimental results show that the proposed \\nmethodology increases both of the speed training process of CNN \\nand the recognition accuracy.  \\nKeywords—Facial emotion r ecognition, Gabor filter, \\nConvolution neural network \\nI. INTRODUCTION  \\nEmotions have an important role in our everyday lives, and \\ndirectly affect decisions, reason ing, attention, prosperity, and \\nquality of life of human. Establishing communication between \\npeople is through emotions and facial expressions. Nowadays, \\nwith the influence of computers on human lives and the \\nmechanization of lives of individuals, establishment of human \\nand computer interaction (HCI) has played a crucial and very \\nimportant role [1]. There is a strong interest in improving the \\ninteraction between humans and computers. Many people \\nbelieve in this theory and there is a positive and useful emotional \\nresponse for establishing a good and useful cognitive link \\nbetween computers with users. This interaction between the \\ncomputer and the human beings can be created through speech \\n[2, 3]. Psychological theory states that human emotions can be \\nclassified into six different forms:  surprise, fear, disgust, anger, \\nhappiness, and sadness. By making changes in the facial \\nmuscles, human can represent this group of emotions. \\nRecently, the deep neural networks achieved significantly \\ngood results in modelling complex patterns [4, 5, 6]. In this \\npaper, a deep learning based framework for human emotion \\ndetection is presented. The proposed framework uses the Gabor \\nfilters for feature extraction and then the deep convolutional \\nneural network. The experimental results show that the proposed \\nfeatures increase the speed and accuracy of training the neural network. The rest of this paper is organized as follows. After \\ndescribing the related literature in Section II, the proposed fast \\nfacial emotion recognition framework is detailed in Section III. \\nThe experimental design and the simulation scenarios are \\ndiscussed in Section IV. Finally, Section V concludes the paper. \\nII. RELATED WORKS  \\nKwolek et al. [7] performed facial recognition using the \\nGabor filter to extract features. It was shown in that paper that \\nusing the Gabor filter improves the accuracy of convolutional \\nneural network from 79% to 87.5%. Wu et al. [8] suggested that \\nGabor motion energy filters (GME) could be used to detect \\nfacial expressions. In this method, GME filter was compared \\nwith Gabor energy (GE) filter and the results showed that the \\nproposed method was 7% better. Li et al. [9] presented a deep \\nfusion convolutional neural network (DF-CNN) method using \\n2D+3D images. In this method, three-dimensional scan images \\nof the faces are used. These images make up a total of 32 \\ndimensions. This paper explains that prediction is done using \\ntwo methods. 1- Classification by using the SVM from the 32-\\ndimensional features. 2- The normal prediction of the Softmax \\nfunction using the six-state probability vector. Experimental \\nresults show DF-CNN achieved good results. Li et al. [10] \\nargued that the existing face data bases are not reliable for the \\nreal world applications. This article presents a new database \\ncalled RAF-DB that contains 30,000 facial expressions from \\nthousands of people. In the gathering of this database, it has been \\nseen that real faces often have mixed feelings, in other words, a \\nmixture of several feelings. By reviewing the RAF-DB database, \\nit is the first natural database that  is much more diverse than the \\nseven most commonly used datasets. As a result, a DLP-CNN \\nsuggested a way of expressing a difference between feelings. \\nThese experiments were performed based on 7 basic conditions \\nand 11 combinations. By examining this method on the SFEW \\nand CK + databases, it was shown that the proposed DLP-CNN \\nmethod is the best method for de tecting the state of the face  \\nemotion. Tzirakis et al. [11] suggested that both voice \\nprocessing and image processing can be used to detect human \\nsituations. In this method, the sound is separately conveyed to a \\nneural network. In another direction, the image also provides a \\n50-layer convolutional neural network. This method was \\nperformed on the RECOLA database and it is argued that the \\nresults of the experiments show  better performance than other \\n577\\nmethods. Lee et al. [12] suggested that the eye can be used to \\nextract emotions.  \\nIn this method, which was performed using STFT and CNN, \\nthe STFT extracts the necessary features in two cases of eye size \\nand motion and serves as an in put to the CNN network with two \\nlayers. This review shows that the CNN network is also effective \\nin the eye-based diagnosis, in addition to being effective in \\nrecognizing emotions in different ways. Pons et al. [13] \\nproposed to train a CNN network for the final face emotion \\nclassification, which can be used to evaluate the combination of \\nCNN networks. In this method, 72 CNN networks were trained \\nwith four layers and with different parameters, and 64 CNN \\nnetworks were trained with VGG-16. In the final analysis, the \\noutput of different networks wa s classified using the proposed \\nclassification and the result was presented. The results of this \\nstudy show that this method can increase accuracy by 5% in \\ndifferent classification methods. Tang et al. [14] presented three \\ndifferent methods called DGFN, DFSN, and DFSN-I. DGFN is \\nactually based on the critical points of psychology and \\nphysiology rules and uses the traditional machine learning \\nnetwork. DFSN is designed on the basis of CNN. In the end, \\nDFSN-I combines both DGFN and DFSN, which has both \\nadvantages for better performance. Experimental results show \\nthat the performance in the CK + database and Oulu CASIA \\nimproved. \\nIII. METHODOLOGY  \\nA. Gabor filter \\nGabor filters [15] are generally used in texture analysis, edge \\ndetection, feature extraction . In (1), the Gabor filter is described. \\nWhen a Gabor filter is applied to an image, it gives the highest \\nresponse at edges and at points where texture changes. The \\nfollowing images show a test image and its transformation after \\nthe filter is applied. \\n22 2\\n2\\'\\' \\'( , ; , , , , ) exp( )exp( (2 ))2xy xgxy iγλθψσγ π ψσλ+=− + (1) \\nin which the real part is: \\n22 2\\n2\\'\\' \\'(, ; , , , , ) e x p ( ) c o s ( 2 )2xy xgxyγλθψ σ γ π ψσλ+=− +  (2) \\nand the imaginary part is: \\n22 2\\n2\\'\\' \\'(, ; , , , , ) e x p ( ) s i n ( 2 )2xy xgxyγλθψσ γ π ψσλ+=− +  (3) \\nwhere: \\n\\'c o s s i nxxyθθ =+  (4) \\nand \\n\\'s i n c o syx y θθ =− +  (5) \\nThe proposed filter is shown in Figure 1. As shown in Figure \\n1, the original image of the first Gabor filter is displayed and \\nthen we pass the filtered image again from the second Gabor \\nfilter. In Figure 2, four samples of the original images are shown.  \\n \\n  \\n \\n \\n \\nFigure 1– The proposed filter \\n \\n \\nFigure 2- Original Images  \\n \\nIn Figure 3, four sample images are shown after applying the \\nGabor filter on Orginal images with the following parameters: \\n() (, ) 1 8 , 1 8 1 . 5 , / 4 , 5 , 1 . 5\\n  0, xy σθ π λ γ\\nψ== = = =\\n=\\n \\nTable 1 - Definition the parameters of Gabor filte r\\nDefinition  Parameters  \\nThe size of the Gabor kernel  K=(x, y)  \\nThe wavelength of the sinusoidal factor in \\nthe above equation. λ  \\nThe orientation of the normal to the \\nparallel stripes of the Gabor function. θ  \\nPhase offset. ψ \\nStandard deviation of the Gaussian \\nfunction used in the Gabor filter.  σ \\nSpatial aspect ratio. γ \\n578\\n \\nFigure 3 -Sample image after applying first Gabor Filter  \\n \\nIn Figure 4, four sample images are shown after applying the \\nGabor filter on first Gabor filtered images with the following \\nparameters: \\n() ( , ) 18 ,18 1. 5, 3 /4, 5, 1. 5\\n  0, xy σθ π λ γ\\nψ== = = =\\n= \\n \\n     \\nFigure 4– Sample image after applying second Gabor Filter \\nB. CNN Architecture \\nThe structure of the convolutional neural network is shown \\nin Figure 4 . As shown in Table 2,  first, the image is received \\nand after passing through different layers and the learning \\nprocess returns a vector with seven modes as output. In fact, \\nthese seven modes are: Angry, Disgust, Fear, Happy, Neutral, \\nSad and Surprise. \\n  \\nFigure 5– The CNN architecture  \\n \\nAs shown in Table 1, in the first stage, the deep neural \\nnetwork applies a convolution of a 6 × 6 filter on the image. In \\nthe next step, using MaxPooling, the dimensions are reduced to \\n128 × 128 × 6. Next, another convolutional network will be \\napplied to the 16 × 16 filter size, and in the next step, using the \\nMaxPooling function, we will reduce the size to 64 × 64 × 16. \\nThe next convolution is applied to the data with a 120 × 120 \\nfilter size. In the next step, using the Flatten function, all data is \\nconverted to a vector of the size 432000. Then, the vector is \\nconverted to a vector of length 84, and at the end, it is reduced \\nto seven, which is the 7 categories of emotional states. \\n \\nTable 2- Proposed CNN architecture method details \\nOutput Shape  Details  Layer type  \\n256, 256, 6  Conv (6x6)  Conv  \\n256, 256, 6  Relu  Activation  \\n128, 128, 6  Pool size (2,2)  MaxPooling  \\n128, 128, 16  Conv (16x16)  Conv  \\n128, 128, 16  Relu  Activation  \\n64, 64, 16  Pool size (2,2)  MaxPooling  \\n60, 60, 120  Conv (120x120)  Conv  \\n60, 60, 120  Relu  Activation  \\n60, 60, 120  --------------------  Dropout            \\n432000  Flatten to a vector  Flatten           \\n84  Input \\uf0e084   Dense                \\n84  Relu  Activation   \\n84  --------------------  Dropout            \\n7  Input \\uf0e0  \\nClassese Num =7  Dense                \\n7  softmax  activation  \\n \\nRectified Linear Unit (Relu) function: \\n() m a x ( 0 ,)fxx x+==  (6)\\n \\n \\n579\\nSoftmax function: \\n1[ 0 , 1 ]ii\\niXx==\\uf0e5  (7) \\nC. Proposed facial emotion recognition algorithm: \\n The proposed method is to first apply a Gabor filter to the \\nimages and then convey the output results as inputs to the neural \\nnetwork. The output of the Gabor filter is given to the \\nconvolutional neural network. \\n \\n \\nFigure 6- The proposed method  \\nIV. EXPERIMENTAL RESULTS  \\nFor the experimental results, the JAFFE database [16] is used. \\nThe database contains 213 Japanese female model images that \\ninclude seven emotional states of the face, in which six modes, \\nnatural states of the face and normal face. Table 3 shows the \\nspecification of the simulation hardware. According to Figure \\n8 and Figure 9 and Table 4, the results show that after 10 \\nepochs, the proposed method reaches 86% accuracy, but \\nconventional CNN method reach 51% accuracy. After 15 \\nepochs, the proposed method reaches 92% accuracy but \\nconventional CNN reaches 73% accuracy. After 25 epochs, the \\nproposed method reaches 97% accuracy while its competitor \\nachieves 90% accuracy. At the end, the proposed approach \\nobtains 97% accuracy and the other reaches 91% accuracy. \\nAccording to Table 5, the accuracy of 70-73% is achieved after \\n8 epochs which take 308 seconds using the proposed approach \\nwhile it takes 521 seconds using the convolutional method. In \\naddition, the accuracy of 91-92% takes 541 seconds to be \\nachieved using the proposed approach and it takes 1189 \\nseconds using the convolutional method. Therefore, the \\nproposed approach needs less time than the convolutional \\napproach to achieve the same  accuracy. Incr ease of running \\ntime in the proposed method is expected because the sub-\\nfeatures extracted by two cons ecutive Gabor filters simplify the \\nlearning process (feature extraction and classification) of the \\nCNN. The first used Gabor filter extracts texture information in \\ndifferent scales and directions while the second one extracts \\nmore sub-features from the previous Gabor feature map. \\n \\n  \\nTable 3 – The system specifications \\nModel Dell XPS L502 \\nProcesso r Intel Core i7 CPU – 2.00GHz\\nRAM 8 GB  \\nSystem type 64- bit Operating Syste m\\n \\n             \\nFigure 7– sample of correct emotional recognition \\n \\n      Table 4 – Comparison accuracy of proposed method and simple CNN  \\nEpoch CNN Method \\nAccuracy 2Gabor + CNN \\nMethod \\n1 0.1050 0.1326 \\n10 0.5138 0.8619 \\n15 0.7348 0.9227 \\n20 0.8343 0.9558 \\n25 0.9006 0.9779 \\n30 0.9116 0.9716 \\n \\n \\nTable 5 – Comparison speed of proposed method and simple CNN  \\nAccuracy CNN Method  2Gabor + CNN  \\nEpoch Time(s) Epoch Time(s) \\n10-15% 1 42 1 41 \\n50-55% 9 330 6 232 \\n70-73% 14 521 8 308 \\n86-87% 18 683 10 390 \\n91-92% 30 1189 14 541 \\n \\nInput \\nImages\\nResize\\nFirst Gabor \\nFilter\\nSecond \\nGabor Filter\\nCNN\\n Emotions\\nSub feature Extraction \\n580\\n \\nFigure 8- Accuracy of simple CNN method \\n \\n \\nFigure 9– Accuracy of 2Gabor filter+CNN method \\n \\nV. CONCLUSION  \\n After the Gabor filter applied, the system learning became \\nfaster and the accuracy has improved. As seen in Figures 7 and \\n8. The learning speed of the convolutional neural network has \\nincreased profoundly. This is be cause the Gabor filter actually \\nextracts the image subfeature and gives the neural network. By \\ndoing this, the convolutional neural network receives a number \\nof subfeature and takes one step further in extracting the \\nemotions from the faces. \\nREFERENCES  \\n \\n[1] S. L. Happy and A. Routray, \"Automatic facial expression recognition \\nusing features of salient facial patches,\" IEEE Transactions on Affective \\nComputing, vol. 6, pp. 1-12, 2015. [2] A. Nicolai and A. Choi, \"Facial Emotion Recognition Using Fuzzy \\nSystems,\" in 2015 IEEE International Conference on Systems, Man, and \\nCybernetics , 2015, pp. 2216-2221. \\n[3] M. Imani, G. A. Montazer, GLCM Features and Fuzzy Nearest Neighbor \\nClassifier for Emotion Recognition from Face,  7th International \\nConference on Computer and Knowledge Engineering (ICCKE 2017), \\nMashhad, Iran, pp. 8-13, 26-27 October 2017. \\n[4] M. H. Abbasi, B. Majidi, and M. T. Manzuri, \"Deep cross altitude visual \\ninterpretation for service robotic agents in smart city,\" in 2018 6th Iranian \\nJoint Congress on Fuzzy and Intelligent Systems (CFIS), 2018, pp. 79-82. \\n[5] M. H. Abbasi, B. Majidi, and M. T. Manzuri, \"Glimpse-gaze deep vision \\nfor Modular Rapidly Deployable Decision Support Agent in smart \\njungle,\" in 2018 6th Iranian Joint Congress on Fuzzy and Intelligent \\nSystems (CFIS), 2018, pp. 75-78. \\n[6] A. Fadaeddini, M. Eshghi, and B. Majidi, \"A deep residual neural network \\nfor low altitude remote sensing image classification,\" in 2018 6th Iranian \\nJoint Congress on Fuzzy and Intelligent Systems (CFIS), 2018, pp. 43-46. \\n[7] B. Kwolek, \"Face Detection Using Convolutional Neural Networks and \\nGabor Filters,\" in Artificial Neural Networks: Biological Inspirations – \\nICANN 2005 , Berlin, Heidelberg, 2005, pp. 551-556. \\n[8] T. Wu, M. S. Bartlett, and J. R. Movellan, \"Facial expression recognition \\nusing Gabor motion energy filters,\" in 2010 IEEE Computer Society \\nConference on Computer Vision and Pattern Recognition - Workshops , \\n2010, pp. 42-47. \\n[9] H. Li, J. Sun, Z. Xu, and L. Chen, \"Multimodal 2D+3D Facial Expression \\nRecognition With Deep Fusion Convolutional Neural Network,\" IEEE \\nTransactions on Multimedia, vol. 19, pp. 2816-2831, 2017. \\n[10] S. Li, W. Deng, and J. Du, \"Reliable Crowdsourcing and Deep Locality-\\nPreserving Learning for Expression Recognition in the Wild,\" in 2017 \\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , \\n2017, pp. 2584-2593. \\n[11] P. Tzirakis, G. Trigeorgis, M. Nicolaou, B. Schuller, and S. Zafeiriou, \\nEnd-to-End Multimodal Emotion Recognition Using Deep Neural \\nNetworks  vol. PP, 2017. \\n[12] H. Lee and S. Lee, \"Arousal-valence recognition using CNN with STFT \\nfeature-combined image,\" Electronics Letters, vol. 54, pp. 134-136, 2018. \\n[13] G. Pons and D. Masip, \"Supervised Committee of Convolutional Neural \\nNetworks in Automated Facial Expression Analysis,\" IEEE Transactions \\non Affective Computing, vol. 9, pp. 343-350, 2018. \\n[14] Y. Tang, X. M. Zhang, and H. Wang, \"Geometric-Convolutional Feature \\nFusion Based on Learning Propagation for Facial Expression \\nRecognition,\" IEEE Access, vol. 6, pp. 42532-42540, 2018. \\n[15] Maryam Imani , 3D Gabor Based Hyperspectral Anomaly \\nDetection, Amirkabir International Journal of Modeling, Identification, \\nSimulation & Control (AUT Journal of Modeling and \\nSimulation), vol.  50, no. 2, pp. 101-110, 2018. \\n[16] The Japanese Female Facial Expression (JAFFE) Database . Available: \\nhttp://www.kasrl.org/jaffe.html \\n \\n581\\n',\n",
       " \"Abstract\\nFace alignment can fail in  real-world conditions, \\nnegatively impacting the performance of automatic facial \\nexpression recognition (FER) s ystems. In this study, we \\nassume a realistic situation including non-alignable faces \\ndue to failures in facial la ndmark detection. Our proposed \\napproach fuses information about non-aligned and aligned \\nfacial states, in order to boost FER accuracy and efficiency. \\nSix experimental sc enarios using discriminative deep \\nconvolutional neural networks (DCNs) are compared, and \\ncauses for performance differ ences are identified. To \\nhandle non-alignable faces b etter, we further introduce \\nDCNs that learn a mapping from non-aligned facial states \\nto aligned ones, alignment-ma pping networks (AMNs). We \\nshow that AMNs represent geometric transformations of \\nface alignment, providing features benefic ial for FER. Our \\nautomatic system based on ens embles of the discriminative \\nDCNs and the AMNs achieves impressive results on a \\nchallenging database for FER in the wild. \\n1. Introduction \\nFacial expression is a primary means for understanding \\nhuman emotions and has been actively studied over two \\ndecades [see 1-3 for survey]. Rapid progress has been made \\non technologies for automatic facial expression recognition \\n(FER), particularly in controlled laboratory settings. \\nNevertheless, FER still remains a challenge in uncontrolled \\nreal-life situations in which a FER system must handle \\nunpredictable variability in head poses, lighting conditions, \\nocclusions, and subjects. To r esolve this issue, researchers\\nhave collected large volumes of data “in the wild” [4 -7], \\nhave held many grand challenges [8-11], and have \\npresented excellent approaches notably with deep learning \\ntechniques [12-18]. \\nFace alignment commonly performed in preprocessing \\nmodules is essential for achieving good FER performance [6, 16, 18]. However, many  studies have applied some \\n“manual” steps when this pr eprocessing fails. For real-life \\napplications, automatic FER systems in which human \\nintervention is not necessar y are preferred, and this \\ndemands automatic face alignment. Recently, such \\nalignment techniques have been extensively studied by\\nusing holistic deformable models (DMs) [19, 20] and \\nparts-based DMs [21, 22], by taking advantage of both of \\nthe DMs [23, 24], and by applying regression-based \\nmethods [25, 26]. However, when alignment errors or \\nfailures occur “in the wild” and propagate to later stages of \\nFER systems, final performance declines. \\nIn this paper, we propose a framework based on an \\nensemble of deep convolutional neural networks (DCNs) \\ntoward automatic FER in the wild. We begin with the \\nfollowing realistic assumptio n (validated in Section 3):  \\n/g132Face alignment under real-world conditions is \\nassumed to be not always successful. \\n/g132Consequently, face imag es are either “alignable”, \\ni.e., cap able of being aligned or “non -alignable”, i.e.,\\nnot capable of being aligned. Fusing Aligned and Non-Aligned Face Information \\nfor Automatic Affect Recognition in the Wild: A Deep Learning Approach\\nBo-Kyeong Kim, Suh-Yeon Dong, Jihyeon Roh, Geonmin Kim, Soo-Young Lee\\nComputational NeuroSystems Laboratory (CNSL)\\nKorea Advanced Institute of Science and Technology (KAIST)\\n \\n{bokyeong1015, suhyeon.dong}@gmail.com, {rohleejh, gmkim90, sylee}@kaist.ac.kr\\nFigure 1: Overview of our approach. Our automatic FER \\nsystem contains several DCNs to fuse information of \\nnon-aligned and aligned facial states.\\n2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n978-1-5090-1437-8/16 $31.00 © 2016 IEEE\\nDOI 10.1109/CVPRW.2016.1871499\\n\\nOur FER system is designed to separately deal with \\nalignable faces and non-alignable faces, as shown in Figure \\n1. For alignable faces, we combine information from both of \\nthe “n on-aligned state” and the “aligned state”. For \\nnon-alignable faces, we initially obtain class predictions \\nusing the “non -aligned state”. In order to determine the \\nmethod to train discrimin ative DCNs, we assess several \\nexperimental scenarios for a better information fusion \\n(Section 4). Then, in order to estimate aligned states of\\nnon-alignable faces, we introduce DCNs which learn a \\nmapping  from the non-aligned facial state to the aligned one \\n(Section 5), called alignment-mapping networks (AMNs). \\nExperimental results demonstrate that these AMNs \\nrepresent similarity transformations performed in face \\nalignment and yield hidden fe atures beneficial for FER. To \\nsummarize, our empirical findings in developing the \\nautomatic FER system show that  \\n/g132In the learning phase of individual discriminative \\nDCNs, it is beneficial to use the merged training dataset \\nof alignable and non-alignable faces for efficiency in \\nevaluation time as well as for effectiveness in FER \\nperformance. \\n/g132In the testing phase for alignable faces, combining \\ninformation of both non-aligned and aligned states from \\ndiscriminative DCNs improves FER accuracy. \\n/g132In the testing phase for non-alignable and alignable \\nfaces, it is better to add decision-level information from \\nclassifying the hidden features of AMNs. \\nWe evaluate the proposed framework on the facial \\nexpression recognition 2013 (FER-2013) database [7], a \\nchallenging benchmark collected from the Web. Our final \\nensemble-based FER system achieves great performance on \\nthis in-the-wild database. \\n2. Related Work\\nDiversity and ensembles  of neural networks.  Combining \\ndecisions of multiple artificial neural networks is a method \\nwith a long history [27-29] in the field of classifier ensemble. \\nHere, employing “diversity” of individual networks has \\nbeen shown to improve ensemble performance, by \\nproviding uncorrelated, different, and thus informative \\ndecisions [30, 31]. For high diversity, individual networks \\nare built using several diversification strategies [28, 29], \\ne.g., by altering network architectu res and random weight \\ninitializations as well as by using variously normalized and \\ndifferently divided datasets. Su ch strategies are now applied \\nto ensembles of deep  neural networks, resulting in \\nremarkable successes in image classification [32-34]. In this \\nwork, our DCNs are also built towards high diversity, \\nspecifically by changing in put normalization and weight initialization. \\nDeep learning for facial expression recognition.  In recent \\nyears, similar to other computer vision problems, applying \\ndeep learning techniques to FER has attracted considerable \\nattention. For disentangling laten t factors in face images, \\nRifai et al.  [12] gradually separated discriminative \\nexpression information from non-discriminative pose and \\nmorphology factors based on convolutional networks and \\nauto-encoders, while Reed et al.  [13] modeled higher-order \\ninteractions of expression an d pose manifolds based on a \\nrestricted Boltzmann machine. In addition, Reed et al.  [14] \\nused a bootstrapping-based ap proach regarding prediction \\nconsistency, resolving a problem  of noisy- and subjective \\nlabels in FER. Liu et al. [15] used convolutional kernels for \\ncapturing facial action units r elevant to facial expressions \\nand extracted higher-level fe atures based on deep belief \\nnetworks. \\nTo improve affect recognition in the wild, there are \\nseveral ensemble-based deep learning approaches, which \\ninspire our work. Kahou et al. [16] proposed a multimodal \\nframework including a DCN and a deep belief network for \\nextracting visual and audio features, respectively, and \\napplied a random-search-based weighted decision fusion. \\nMoreover, in order to combine multiple DCNs using late \\nfusion schemes, Yu et al. [17] applied a hinge-loss-based \\nweighted fusion in a single-level committee, while Kim et al.\\n[18] used an exponentially weighted fusion in a hierarchical \\ncommittee. These three studies used the FER-2013 database \\ndescribed in Sect. 3 to pre-tr ain their DCNs for transfer \\nlearning or feature extraction. However, they did not cope \\nwith the failure in face alignment nor consider its influence \\non FER performance. \\nConverting pose st ates of face images.  Many deep models \\nfor face recognition aim to learn a mapping from \\nnon-frontal faces to frontal ones, which share the similar \\ngoal with our AMNs. Kan et al.  [35] used a stacked \\nauto-encoder network to model gradual change in poses, \\nwhile Zhu et al. [36] designed a deep convolutional network \\nincluding locally-connected layers to extract pose- and \\nillumination-robust features. In order to model the \\ncontinuous pose space, Zhu et al.  [37] used stochastic \\nneurons in their multi-view perceptron which disentangled \\npose and identity factors. In addition, Yim et al.  [38] \\nemployed a multi-task learning scheme to convert head \\nposes while concurrently maintain ing identity information.  \\nOne limitation of the aforementioned deep models is that \\nthey focused on changes only in yaw rotation. In addition, \\nthey dealt with data only ca ptured under controlled lab \\nsettings, and 2-D aligned faces were used as inputs and \\noutputs of their models. In contrast, our work attempts to \\nhandle all rotations of yaw, pitch, and roll as well as \\ncropping and resizing operations. We deal with faces in the \\nwild, and the faces before and after 2-D alignment are used \\n1500\\nas inputs and target outputs of our AMNs, respectively. \\nVery recently, interesting approaches have been \\nproposed to yield frontal facial poses in unconstrained \\nsettings. After localizing facial landmarks, Hassner et al. \\n[39] used a 3-D reference model common to all faces and \\nefficiently estimated visible facial parts, improving \\nperformance. On the other hand, Sagonas et al.  [40] \\nemployed an effective statistical model to localize \\nlandmarks and convert facial poses at the same time, \\nnotably using only frontal faces. Although these studies did \\nnot use deep learning techniques, excellent results on \\nin-the-wild datasets were ob tained particularly due to \\nrigorous mathematical derivation. \\n3. Data and Face Alignment \\nThis section describes the FER-2013 database used in our \\nwork. To investigate the validity of assumption in Sect. 1, \\nwe present face alignment results on this database. Finally, \\nexperimental datasets which we construct are introduced.  \\n3.1. The FER-2013 database \\nThe FER-2013 database was collected from the Web, and \\nmost images were captured under real-world imaging \\nconditions. This database has been reported to include some \\nnoisy or confusing annotation and show a low human FER \\naccuracy of approximately 65% [11, 41, 42]. This is\\npossibly due to the method of its construction [11]. \\nSpecifically, more than 100 fine-grained emotion keywords \\nsearching out images were clustered into seven target \\nclasses: anger, disgust, fear, ha ppiness, sadness, surprise, \\nand neutral expression. Here, clustering errors could result \\nin noisy and subjective labels. \\nThe original FER-2013 was introduced for the \\nsub-challenge of ICMLW’13 [11], and consists of 35,887 \\ndetected faces in grayscale at a size of 48-by-48 pixels: 28,709 faces for training, 3,589 for public test, and 3,589 \\nfor private test. In the present study, after removing 11 \\nnon-number-filled training images, we randomly divide the \\ntraining data into two parts: 25,110 faces (about seven \\neighths) for training our models and 3,588 faces for \\nvalidation. For fair comparison with previous studies on this \\ndatabase, we only use the pr ivate test data for evaluation, \\nand do not use any public test data for validation or \\nevaluation. \\n3.2. Face alignment \\nFor face registration, we conduct a conventional 2-D\\nalignment using IntraFace [25, 43], a publicly-available \\nlandmark detector. After localizin g the 49 predefined facial \\nlandmarks, face images ar e automatically rotated and \\ncropped based on the eye coordinates, and finally resized \\ninto 48-by-48 pixels. Notice that manual preprocessing \\nsteps are not employed, i.e., erroneously-aligned and \\nnon-alignable faces are not removed, and so are used in later\\nprocessing stages. \\n Alignment performance on the FER-2013 is reported in \\nTable 1. Here, 85 p ercent of faces are “alignable”, meaning \\nthat IntraFace can provide the set of landmarks with the \\ncorresponding confidence sc ore and pose angle of pitch, \\nyaw and roll. Figure 2 shows histograms of the number of \\nalignable faces as a function of the pose angle, some aligned \\nexamples, and their localized  landmarks. Due to failed \\nFigure 2: Statistics of head pose and examples of 2-D aligned result on the FER-2013 DB. The graph (a) shows the number of\\nalignable f aces as a function of the pose angle of yaw, pitch, and roll on a log scale, and the (b) shows the percent ratio of aliganble \\nfaces for each pose angle on a linear scale. The ranges of (-31.7, 32.7) degree for yaw rotation, (-17.5, 29.1) for pitch, and (-2 0.5, 20.3) \\nfor roll contain 95% of the alignable faces , indicating in-the-wild conditions of the FER-2013 DB.\\nFER-2013 DBNumber of Faces (Ratio)\\nNon-Alignable Alignable Total\\nDataTrain 3764 (15.0 %) 21346 (85.0 %) 25110\\nValid 537 (15.0 %) 3051 (85.0 %) 3588\\nTest 543 (15.1 %) 3046 (84.9 %) 3589\\nStateNon-Aligned XNA XA X\\nAligned - ZA -\\nTable 1: Alignment performance and summary of \\nexperimental datasets according to aligned facial states.\\n1501\\nlandmark detection, 15 percent of faces are “n on-alignable” .\\nMost of such failures are und er extreme pose, occlusion, \\nand/or bad illumination conditions. Examples of \\nnon-alignable faces are depicted in Figure 7. Clearly, face \\nalignment in real-world situations is not always successful, \\nposing a specific challen ge for FER research. \\n3.3. Formation of ex perimental datasets \\nWe form experimental datasets  based on the alignment \\nresults in Sect 3.2. More spec ifically, after the data division \\ndescribed in Sect 3.1, face alignment with IntraFace is \\nperformed on each of training, validation, and testing data. \\nHere, depending on whether IntraFace can localize the \\nfacial landmarks or not, face images are able to be aligned \\n(alignable) or not to be aligned (non-alignable). \\nNon-aligned and aligned facial states thus constitute \\ndistinct classes, and we form  our experimental datasets \\naccordingly. Let X= {x} be a set of whole faces before the \\n2-D alignment, which are in the non-aligned  state. We \\nfurther divide this set X into two disjoint subsets: XNA= {xNA\\nis a non-alignable x}, a set of non-alignable  faces in the \\nnon-aligned state, and XA = {xA is an alignable x}, a set of \\nalignable  faces in the non-aligned state. Furthermore, let ZA\\n= {zA is obtained from xA by face alignment} be a set of \\nalignable  faces in the aligned state. Hence, a one-to-one \\ncorrespondence exists between XA and ZA. See Table 1 for \\nsummary of the notations. \\n4. Information Fusion of Non-Aligned and \\nAligned Facial States \\nThis section discusses four  scenarios with information \\nfusion of non-aligned and alig ned states and for comparison \\ntwo scenarios without information fusion. Next, we present \\nindividual deep models and their ensembles used in the \\nconsidered scenarios. We then  move onto the experimental \\nresult and analysis. \\n4.1. Scenarios of information fusion \\nAs shown in Sect. 3.2, face alignment during \\npreprocessing is not always successful in uncontrolled \\nenvironments. Therefore, both alignable faces (AF) and \\nnon-alignable faces (N-AF) exist in real-world situations, \\nand FER researchers are faced with the question of using \\neither aligned or non-aligned state information, or using \\nboth kinds of information during system development. To \\nanswer this question, we consid er six scenarios depicted in \\nFigure 3. Firstly, the scenarios are categorized according to \\nthree types of  training dataset  for deep models, i.e.,X(in \\nnon-aligned facial states), ZA (in aligned facial states), and \\nX+ZA. We mark them with red, blue, and yellow color, respectively, in Figure 3. The dataset of X+ZA is constructed \\nby merging X  and ZA into one set. The expectation is that \\nmodels trained using X+ZA could extract some features \\nrepresenting the fused knowledge  about non-aligned and \\naligned states, since these models simultaneously handle \\nfaces in both states during learning.  \\nMoreover, they are categorized  according to whether the \\nAF are evaluated with a decision-level  fusion. The scenarios \\nwith the fusion compute an  average of posterior class \\nprobabilities estimated from deep models, P( y|xA) and \\nP(y|zA), for combining information of non-aligned and \\naligned facial states. On the other hand, the scenarios \\nwithout the fusion use either P( y|xA) or P( y|zA). Now, we \\nintroduce the details of each scenario. \\nScenarios without information fusion.  The first two \\nscenarios, S1 and S2 in Figure 3, do not use any information \\nfusion and are designed for comparison. The models in S1 \\nare trained and evaluated using X without considering \\nwhether data are AF or N-AF. In other words, FER systems \\nin S1 do not apply face alignment in preprocessing steps, \\nand they have been commonly  used for the FER-2013\\ndatabase. In contrast, the models in S2 are trained only \\nusing ZA. Then, the AF are tested in the aligned state zA to \\nmaintain a consistency with training, while the N-AF are \\nevaluated inevitably in the non-aligned state xNA. \\nScenarios with information fusion 1.  The next two \\nscenarios, S3 and S4, fuse information of non-aligned and \\nFigure 3: Scenarios to combine information of aligned and \\nnon-aligned facial states. See Sect. 3.3 and Sect. 4.2 for the \\nnotations of datasets and deep models, respectively.\\nFigure 4: Architecture of the discriminative DCN and\\nevaluation procedure with data augmentation.\\n1502\\naligned states in evaluating the AF. S3 is similar to S2, \\nexcept that it combines P( y|xA) and P( y|zA) for the AF. In S4, \\nwe consider two types of m odels, which are trained using X\\nand ZA, respectively. Then, the N-AF are tested with the \\nmodels trained using Xin the same way as S1. The AF are \\ntested with information fusion of xA and zA. Here, P( y|xA)\\nand P( y|zA) in S4 are obtained from the two types of models \\nrespectively, whereas these two class probabilities in S3 are \\nobtained from only the one type of models. \\nScenarios with information fusion 2.  In the last two \\nscenarios, S5 and S6, the models are trained using the \\nmerged set of X+ZA, and we expect them to learn fused \\nknowledge of non-aligned and aligned facial states. The \\nevaluation procedure in S5 is identical to that in S2, without \\ninformation fusion for the AF. In contrast, S6 applies \\ninformation fusion of combining both facial states to the \\ntesting phase for the AF as well as to the training phase of \\nthe models.   \\n4.2. Deep models and their ensemble \\nTo achieve “diverse” FER deci sions, which are necessary \\nfor a good ensemble, we design nine discriminative DCNs. \\nThese deep models are trained us ing three different methods \\nfor input normalization  as well as using three different \\nweight initialization  by changing the random seed numbers. \\nFor input normalization, the pixel values are rescaled by \\napplying the min-max normalization (denoted as Raw), the \\nillumination variation among faces is reduced by applying \\nthe illumination normalization ( iNor) based on the isotropic \\ndiffusion [44], or the global contrast of faces is increased by \\napplying the contrast enhancement ( cEnh ) based on \\nhistogram equalization. Examples of input normalization \\nare shown in Figure 7. \\nAs illustrated in Figure 4, each DCN consists of three \\nstages of convolutional and max-pooling layers, followed \\nby two fully-connected layers. The convolutional layers use \\n32, 32, and 64 filters of size 5x5, 4x4, and 5x5, respectively. \\nIn the max-pooling layers, ov erlapping-pooling is applied \\nwith the kernels of size 3x3 an d stride 2, and the size of resulting maps becomes halved. The fully-connected hidden \\nand output layers contain 1024 and 7 neurons, respectively, \\nwhere each output neuron co rresponds to ea ch expression \\nclass of FER. For nonlinearity, Rectified linear unit (ReLU) \\nactivation is used in all convolutional and penultimate \\nlayers, while softmax activation is  used in the output layer. \\nFollowing the notation rule in [32], our model can be \\ndenoted as 1x42x42 - 32C5 - MP3 - 32C4 - MP3 - 64C5 - \\nMP3 - 1024N - 7N and we shall use this rule in the rest of \\nthis paper for brevity. The DC N model is learned using the \\naugmented training data obtained from label-preserving \\ntranslation and reflection. At  the evaluation phase, to \\nmaintain a consistency with the training phase, ten patches \\nextracted from each face image are fed to the model, and the \\ncorresponding ten predictions  are averaged. For other \\ntraining details, refer to Appendix A. \\nDepending on the training dataset as described in Sect. \\n3.3, we organize our nine DC Ns into a unit of ensemble: \\n            (1) \\nwhere kଲK= {X,ZA,X+ZA} denotes the type of training \\ndataset, nଲN= {Raw,iNor,cEnh } denotes the input \\nnormalization method, and rଲR= {1,2,3} denotes the \\nrandom seed number for weight initialization. To combine \\nthe nine decisions in the ensemble, we apply two widely \\nused fusion rules, the majority vote rule and the average rule.\\nIn order to select a final class, the majority vote directly uses \\nthe predicted labels to compute the largest number of votes, \\nwhile the average rule uses th e posterior class probabilities \\nto compute the highest mean class score.\\n4.3. Experimental result \\nOverall performance.  The 7-class FER test accuracies of \\nDCNs and their ensembles und er the six experimental \\nscenarios are reported in Tab le 2. We first observe that both \\nof the best single model and th e best ensemble are achieved \\nunder the S6 scenario. Second , the scenarios S4 and S6 \\nshow better ensemble performance than other scenarios. \\nFurthermore, the ensemble ac curacies of S2 and S3 are Scenario Individual DCN models Ensemble\\nNo.Train\\nSetTest Set\\n(AF, N-AF)n = Raw n = iNor n = cEnhMeanMaj.\\nVoteAve.\\nRule r = 1 r = 2 r = 3 r = 1 r = 2 r = 3 r = 1 r = 2 r = 3\\nS1 X (XA,XNA) 70.38 69.24 69.55 69.63 70.08 69.30 69.18 69.49 69.16 69.56 72.39 72.47\\nS2 ZA (ZA,XNA) 65.48 64.84 63.95 64.17 64.86 65.09 65.84 64.78 65.00 64.89 66.76 67.46\\nS3 ZA (ZA+XA,XNA) 63.92 63.97 63.39 63.14 63.81 63.97 64.45 64.20 63.69 63.84 65.98 66.93\\nS4 X,ZA (ZA+XA,XNA) 70.88 70.74 70.99 70.80 70.88 70.38 71.08 71.22 71.25 70.91 72.97 72.81\\nS5 X+ZA(ZA,XNA) 70.66 70.41 69.80 69.07 69.16 69.49 69.30 70.66 69.99 69.84 72.33 72.42\\nS6 X+ZA(ZA+XA,XNA)71.86* 70.88 70.83 70.02 70.33 70.69 70.63 71.75 71.22 70.91 73.31 73.31\\nTable 2: Classification accuracy (%) of individual deep m odels and their ensemble for each experimental scenario on the \\nFER-2013 DB. For a given model or an ensemble (each column), th e highest accuracy indicating the best scenario is written in bold.\\nThe asterisk* denotes the best single DCN. Note that test sets of all scenarios contain ex actly the same face images, indicating a fair \\ncomparison. The evaluati on strategies for test sets differ in how to deal with alignable faces (AF) and non -alignable faces (N-AF).\\n1503\\nlower than those of other scenarios. The aforementioned \\nobservations will be examined deeply in the followed \\nanalysis with Figure 5. Notice that there is no much \\ndifference or clear trend in the ensemble accuracies \\ndepending on the majority vote and the average rule. It \\nindicates that what to combine for ensemble has more \\ninfluence on the final performance rather than how to \\ncombine decisions. \\nIt is worth noting that S6 not only yields higher ensemble \\naccuracy than S4 but also uses the less number of models. \\nSpecifically, the ensemble in S6 includes 9 DCNs (trained \\nusing X + ZA), whereas that in S4 does 18 DCNs (9 trained \\nusing X and 9 using  ZA). It shows the strengths of the \\nscenario S6, i.e., the efficiency  in the evaluation time with \\nless models as well as the superiority in FER performance. \\nFor a later usage in Sect. 5.3, the best ensemble using the \\naverage rule in S6 is denoted as ES 6.\\nAccuracies for alignable an d non-alignable faces.  The \\nsix different scenarios perfor m differently given AF and \\nN-AF. Here, we identify the causes for performance \\ndifferences. Figure 5 shows the ensemble accuracies using \\nthe average rule in each scen ario, separately computed for \\nAF and N-AF. We also plot overall test accuracies for total \\nfaces (TF) as reported in the last column in Table 2. \\nAccuracy for AF is computed  as the proportion of correctly \\nestimated AF over the whole AF, and the same for N-AF.  \\nFor both AF and N-AF, the en semble performances of S4 \\nand S6 are improved over th e single best DCN. Also, the \\nensemble of S6 achieves the highest accuracies for both. \\nThese results sugg est the following. \\n/g132Using the merged training dataset of X+ZA is \\nbeneficial in that the fused knowledge  about non-aligned \\nand aligned facial states can be learned. It is verified by \\nthe improved accuracies for both AF and N-AF in S6. \\n/g132For AF, applying the late information fusion to \\ncombine P( y|xA) estimated from non-aligned states and \\nP(y|zA) from aligned states is beneficial. It is supported \\nby the improved accuracies for AF in S4 and S6. \\nIn addition, as shown in the right graph of Figure 5, much lower accuracies for N-AF in S2 and S3 are reported, \\nresulting in poor ensemble perf ormances. It is because a \\nnon-alignable face in non-aligned states ( xNA) for testing is \\nfed to the DCNs trained using AF in aligned states ( ZA), \\nresulting in an inconsistency of data character istics between \\ntraining and testing. \\nNotably, S1 in which face alignment is not applied is a \\ncommon setting for the FER- 2013 database. Compared to \\nS1, S4 and S6 show better en semble accuracies for AF. This \\nsuggests that even if some faces are non-alignable, it is \\nbetter to apply face alignment to other alignable faces and \\nthen to conduct the improved information fusion. \\n5. Mapping from Non-Aligned State to \\nAligned State \\nThis section describes our mapping function from \\nnon-aligned states to aligned states. When compared to \\nauto-encoding functions, we show the effectiveness of our \\nmapping. Then, we present ex perimental results including \\nqualitative analysis and FER accuracy using the individual \\nmappings as well as the final ensemble performance. \\n5.1. Deep models to learn our desired mapping \\nThe basic idea driving this section is that a mapping from \\nnon-aligned facial states to aligned ones can represent \\nsimilarity transformations of face alignment. First, we \\nexamine whether this mapping can be learned using deep \\nneural networks (known as universal approximators). \\nSpecifically, given an alignable face in its non-aligned state \\nxA, a deep model is trained to  generate the corresponding \\naligned state zA by minimizing the L2 Euclidean objective \\nfunction. Therefore, XA (as inputs) and ZA (as target outputs) \\ncontaining only AF are used for training the models, as \\ndepicted in Figure 6.  \\nPreliminary experiments comp ared deep architectures in \\nlearning mapping, e.g., multi-layer perceptrons (MLPs) and \\nDCNs (see Appendix B for more details). The final DCN \\nconsists of two stages of convolutional and max-pooling \\nlayers followed by two fully-connected layers: 1x42x42 - \\n64C5 - MP3 - 64C5 - MP3 - 1000N - 1764N. This \\nalignment-mapping network (AMN) is denoted as AMN n\\nwhere nଲN= {Raw,iNor,cEnh } is the input \\nnormalization. For training details, see Appendix C.\\nNotice that there are no ground-truth outputs for XNA,i.e.,\\nthe aligned states of N-AF. We expect that the mapping \\noutput for XNA (usually with extreme pose angles and \\nocclusions) can be in the closely-aligned state if AMN n\\nsatisfies the following two requirements. First, AMN n can \\nrotate, crop, and resize faces in the non-aligned states by \\nlearning important features for face alignment. Second, \\nFigure 5: Classification accuracy (%) of the e nsemble in \\neach scenario for alignable faces, non -alignable faces, and \\ntotal faces. The horizontal lines show performance of the \\nsingle best DCN model in S6.\\n1504\\nAMN n can be generalized successfully, thus working well \\nnot only on AF but also on unobserved N-AF. Sect. 5.4 \\nprovides qualitative analysis to confirm these properties \\nusing the mapping output for XA and for XNA. \\n5.2. Comparison with au to-encoding functions \\nA popular method for feature extraction is using neural \\nnetworks that learn auto-encoding functions. These \\nnetworks are conventionally trained to produce outputs \\nwhich are identical to inputs, an d the activations of hidden \\nneurons are used as features. Here, whether AMN n provides \\nbetter features than auto-encoding networks (AEs) in terms \\nof classification is examined. As shown in Figure 6, we \\ncompare our AMN n, which learns the mapping from XA to \\nZA, with two AEs. One network learns an auto-encoding \\nfunction from XA to XA using raw face images, denoted as \\nAE X. The other network similarly learns an auto-encoding \\nfunction from ZA to ZA, denoted as AE Z. For a fair \\ncomparison, we use the sa me architectures and training \\nschemes for AMN n,AE X, and AE Z. \\nFor qualitative analysis, outputs of the examined \\nnetworks are observed. Then, in terms of FER performance, \\nwe investigate the effectiveness of AMN n as follows. The \\n1000-D hidden activations of penultimate layer in AMN n\\nare extracted. Then, we use th ese features to train a 3-layer \\nMLP classifier of 1000N - 1000N - 1000N - 7N. For \\ntraining details, see Appendix C. This procedure is repeated for each of AMN n,AE X, and AE Z, for comparison. \\n5.3. Improving the ensemble for FER \\nTo improve the best ensemble from the scenario S6 in \\nSect. 4.3, i.e.,ES6, we additionally use decision-level \\ninformation obtained using AMN n. Specifically, the \\nfeatures extracted from AMN Raw,AMN iNor, and AMN cEnh\\nare fed to the 3-layer MLP class ifiers in Sect. 5.2, yielding \\nestimated posterior class probabilities. After that, we \\ncompute the average of th ese class probabilities and the \\noutput probability from ES6. This improved ensemble is \\ndenoted as ES6+AMN Raw+AMN iNor+AMN cEnh, used for our \\nfinal FER system. \\nThe same procedure is conducted with AE X and AE Z,\\nresulting in ES6+AE X and ES6+AE Z, respectively. To \\nensure that the performance improvement is not just coming \\nfrom adding other information, ES6+AMN Raw is also \\ncompared with the aforementioned ensembles. \\n5.4. Experimental result \\nFigure 7 shows examples of the inputs, the corresponding \\ntarget outputs, and the estimated outputs of examined \\nnetworks. Outputs of AE X and AE Z are identical to their \\ninput images. Notice that outputs of AE Z for unseen N-AF \\nFigure 6: Training procedures for AMNs and AEs to learn \\nthe examined mapping s.\\nFigure 7: Examples of inputs, target outputs, and \\nestimated outputs yielded from AMNs and AEs .Model L2 Euclidean Loss Accuracy (%)\\nAE X 6.77 55.45\\nAE Z 4.57 59.52\\nAMN Raw 18.09 62.94\\nAMN iNor 16.74 59.99\\nAMN cEnh 30.08 63.36\\nEnsemble Type Accuracy (%)\\nES6 73.31\\nES6+AE X 73.29\\nES6+AE Z 73.31\\nES6+AMN Raw 73.47\\nES6+AMN Raw+AMN iNor+AMN cEnh 73.73\\nExisting Method Acc. (%)\\nZhang et al. [45]A DCN using multi-task loss\\nwith using external databases75.10\\nKim et al. [18]An ensemble of 36 DCNs \\nin a hierarchical committee72.72\\nA DCN using cross-entropy loss 70.58\\nDevries et al. [46] A DCN using multi task loss 67.21\\nTang [ 42]A DCN using L2-SVM loss 71.16\\nA DCN using cross-entropy loss 70.1\\nIonescu et al. [47]Multiple kernel learning \\nbased on SIFT descriptors67.48\\nTable 3: Performance comparison of AMNs and AEs \\n(upper), ensemble accuracies obtained by adding AMNs\\nor AEs to discriminative DCNs in ES 6(middle ), and \\nexisting results on the FER- 2013 database (bottom).\\n1505\\nare in non-aligned states. In contrast, AMN Raw,AMN iNor,\\nand AMN cEnh provide closely-aligned  outputs for both the \\nAF and the N-AF. These results support that our mapping \\nrepresents geometric transformations of face alignment. \\nInterestingly, outputs of AMN cEnh are less blurred and \\nclearly show facial expressions while preserving identity \\ninformation. It could be linked to higher accuracy using \\nAMN cEnh in Table 3, by giving better features for FER. \\nIn the upper part of Table 3, we report the L2 Euclidean \\nloss and the classification accura cy of extracted hidden \\nfeatures for each network. Lower L2 loss values are \\nachieved by AE X and AE Z, indicating that they are trained \\nwell for their purpose of auto-encoding functions. However, \\nthe classification accuracies using AMN n are higher than \\nthose of the AEs. It demonstrates that our AMNs provide \\ninformative features for FER and correct unsuitable \\nknowledge particularly by transforming N-AF to be \\nclosely-aligned.  \\nIn the middle part of Table 3, ensemble performances \\nusing the examined networks are shown. Combining ES6\\nand AE decreases ( ES6+AE X) or does not improve \\n(ES6+AE Z) the accuracy of ES6. In contrast, ES6+AMN Raw \\nand ES6+AMN Raw+AMN iNor+AMN cEnh achieve higher \\naccuracies than only using ES6. It indicates that decision \\ninformation from our AMNs can be complementary to that \\nof discriminative DCNs in ES6, thus improving FER. \\nIn the bottom part of Table 3,  the existing results on the \\nFER-2013 database are shown for performance comparison. \\nWithout using external databases, our final system of \\nES6+AMN Raw+AMN iNor+AMN cEnh yields the best FER \\naccuracy. Note that the deep model in [45] has used much \\nbigger architecture than ours  as well as huge external data \\nfor its multi-task loss. Compar ed to the ensemble in [18], \\nour ensemble includes fewer models having the similar \\narchitecture, but we achieve b etter results. Compared to \\nother single DCNs trained only using the FER-2013, our \\nsingle best DCN in S6 yields a higher accuracy of 71.86 %.  \\n6. Conclusion \\nTowards automatic facial ex pression recognition (FER), \\nwe present a framework based on an ensemble of deep \\nconvolutional neural networks. We aim at overcoming a \\nspecific challenge to FER researchers: there are \\nnon-alignable faces in real-w orld conditions. To start, \\nalignment results on a challenging FER database are \\nanalyzed. Then, we evalu ate possible scenarios of \\ninformation fusion with discriminative deep models. Here, \\nan efficient and effective means to combine information of \\naligned and non-aligned facial states is proposed. In order to \\nbetter deal with non-alignable faces, we introduce \\nalignment-mapping networks which learn the operations in face alignment. Using these networks as well as the \\ndiscriminative deep models, the final ensemble achieves \\nexcellent performance on the examined database collected \\nin the wild. We believe that th e proposed approach can be \\napplied not only to FER but also to other face analysis \\nresearch using face alignment under unconstrained \\nconditions. \\nAcknowledgement\\nWe would like to thank Jeffrey White, Kyeongho Lee, and Jisu \\nChoi for their discussions and suggestions. This work was \\nsupported by the Industrial Strategic Technology Development \\nProgram (10044009, Development of a self-improving \\nbidirectional sustainable HRI technol ogy for 95% of  successful \\nresponses with understanding user’s complex emotion and \\ntransactional intent through cont inuous interactions) funded by \\nthe Ministry of Knowledge Economy (MKE, Korea). \\nAppendix \\nA. Training details for discriminative DCNs in Sect. 4.2.  We \\nuse the MatConvNet toolbox [ 48] on NVIDIA GeForce GTX 690 \\nGPUs. To train the DCNs, stochastic gradient descent is used to \\nminimize the cross-entropy objective function with a mini-batch \\nsize of 200 samples. The dropout probability of the penultimate \\nlayer is set to 0.5, and weight decay of  0.0001 and momentum of \\n0.9 are also applied. The initial learning rate is set to 0.01 and \\nreduced by a factor of 2 at every 25 epoch where the number of \\ntotal epochs is 100. Moreover, th e training data were augmented \\nby 10 times, through using 5 crops of size 42x42 (1 from resizing \\nan original 48x48 face and 4 from  extracting its 4 corners) and \\ntheir horizontal flopping.  \\nB. Preliminary experiments for architecture selection of \\nAMN n in Sect. 5.1. To learn the mapping from the non-aligned \\nstate to the aligned state, we have explored several candidates: \\nMLPs having 2 or 3 fully-connected layers (FC), DCNs having 1 \\nor 2 convolutional layer s (CONV) “without” max -pooling layers \\n(MP) followed by 2 or 3 FC, and DCNs having 1 or 2 CONV \\n“with” MP followed by 2 or 3 FC. After comparing the L2 \\nEuclidean validation loss, we have finally selected the best DCN \\ndescribed in the main text. We also empir ically find that “using \\nCONV and MP” and “increasing the number of hidden neurons in \\nFC (500→1000)” are beneficial for learning the desired mapping, \\nwhile increasing the number of FC (2→3) is not.\\nC. Training details for AMN n in Sect. 5.1 and for MLP \\nclassifiers in Sect. 5.2.  We apply the similar training schemes \\nintroduced in Appendix A. For AMNs, a mini-batch size of 500 \\nsamples and a constant learnin g rate of 0.0001 during a total 1000 \\nepochs are used. For MLP classifiers, a mini-batch size of 200 is \\nused, while the initial learning rate is set to 0.01 and halved at \\nevery 100 epoch where the number of total epochs is 400.\\nReferences \\n[1] B. Fasel and J. Luettin. Automatic facial expression analysis: \\na survey. Pattern Recognit ., 36(1):259 –275, 2003. \\n1506\\n[2] R. A. Calvo and S. D'Mello. Affect detection: An \\ninterdisciplinary review of models, methods, and their \\napplications.  IEEE Trans. Affect Comput ., 1(1):18 –37, 2010. \\n[3] E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic \\nanalysis of facial affect: A survey of registration, \\nrepresentation, and recognition. IEEE Trans. Pattern Anal. \\nMach. Intell ., 37(6):1113 –1133, 2015. \\n[4] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. \\nMovellan. Toward practical smile detection. IEEE Trans. \\nPattern Anal. Mach. Intell ., 31(11):2106 –2111, 2009. \\n[5] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Collecting \\nlarge, richly annotated facial-expression databases from \\nmovies. IEEE MultiMedia Mag ., 19(3):34 –41, 2012. \\n[6] A. Dhall, R. Goecke, S. Lu cey, and T. Gedeon. Static facial \\nexpression analysis in tough conditions: Data, evaluation \\nprotocol and benchmark. In ICCV Workshops , 2011. \\n[7] P.-L. Carrier, A. Courville, I. J. Goodfellow, M. Mirza, and \\nY. Bengio. FER-2013 Face Database. Technical report ,\\n1365, Université de Montréal, 2013. \\nhttp://www.kaggle.com/c/challenges-in-representation-learn\\ning-facialexpression-recognition-challenge. \\n[8] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S.\\nBilakhia, and M. Pantic. AVEC 2013: the continuous \\naudio/visual emotion and depression recognition challenge. \\nIn The 3rd ACM International Workshop on Audio/Visual \\nEmotion Challenge , 2013. \\n[9] A. Dhall, OVR. Murthy, R. Go ecke, J. Joshi, and T. Gedeon. \\nVideo and image based emotion recognition challenges in \\nthe wild: Emotiw 2015. In ICMI , 2015. \\n[10] M. F. Valstar, B. Jiang, M. Me hu, M. Pantic, and K. Scherer. \\nThe first facial expression recognition and analysis challenge. \\nIn FG, 2011. \\n[11] I. J. Goodfellow, D. Erhan, P. L. Carrier, and A. Courville. \\nChallenges in representation learning: A report on three \\nmachine learning contests.  Neural Networks , 64:59 –63, \\n2015.\\n[12] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. \\nDisentangling factors of variation for facial expression \\nrecognition. In ECCV , 2012. \\n[13] S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to \\ndisentangle factors of variation with manifold interaction. In \\nICML , 2014 \\n[14] S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. \\nRabinovich. Training deep neural networks on noisy labels \\nwith bootstrapping. In ICLR Workshops , 2015. \\n[15] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep \\nnetworks for facial expression feature learning. \\nNeurocomputing , 159:126 –136, 2015. \\n[16] S. E. Kahou, et al . Emonets: Multimodal deep learning \\napproaches for emotion recognition in video. Journal on \\nMultimodal User Interfaces , 2015. (online) \\n[17] Z. Yu and C. Zhang. Imag e based static facial expression \\nrecognition with multiple deep network learning. In ICMI ,\\n2015.\\n[18] B.-K. Kim, J. Roh, S.-Y. Dong, and S.-Y. Lee. Hierarchical \\ncommittee of deep convolutional neural networks for robust \\nfacial expression recognition. Journal on Multimodal User \\nInterfaces , 2016. (online)\\n[19] J. Alabort-i-Medina and S. Zafeiriou. Bayesian active \\nappearance models. In CVPR , 2014.[20] E. Antonakos, J. Alabort-i-Medina, G. Tzimiropoulos, and S. \\nP. Zafeiriou. Feature-Based Lucas –Kanade and Active \\nAppearance Models. IEEE Trans. Image Process. ,\\n24(9):2617 –2632, 2015. \\n[21] X. Zhu and D. Ramanan. Face detection, pose estimation, \\nand landmark localization in the wild. In CVPR , 2012. \\n[22] A. Asthana, S. Zafeiriou, S.  Cheng, and M. Pantic. Robust \\ndiscriminative response map fitting with constrained local \\nmodels. In CVPR , 2013. \\n[23] J. Alabort-i-Medina and S. Zafeiriou. Unifying holistic and \\nparts-based deformable model fitting. In CVPR , 2015. \\n[24] E. Antonakos, J. Alabort-i-Medina, and S. Zafeiriou. Active \\npictorial structures. In CVPR , 2015. \\n[25] X. Xiong and F. de la Torre. Supervised descent method and \\nits applications to face alignment. In CVPR , 2013. \\n[26] S. Zhu, C. Li, C. Change Loy, and X. Tang. Face alignment \\nby coarse-to-fine shape searching. In CVPR , 2015. \\n[27] L. K. Hansen and P. Salamon. Neural network ensembles. \\nIEEE Trans. Pattern Anal. Mach. Intell ., 12(10):993 –1001,\\n1990.\\n[28] A. J. C. Sharkey. On combining artificial neural nets. \\nConnection Science , 8(3-4):299 –314, 1996.\\n[29] G. Giacinto and F. Roli. Design of effective neural network \\nensembles for image classification purposes. Image and \\nVision Computing , 19(9):699 –707, 2001.\\n[30] C. A. Shipp and L. I. Kuncheva. Relationships between \\ncombination methods and measures of diversity in \\ncombining classifiers. Information Fusion , 3(2):135 –148,\\n2002. \\n[31] L. I. Kuncheva. Diversity in multiple classifier systems.\\nInformation Fusion , 6(1):3 –4 (Editorial), 2005. \\n[32] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column \\ndeep neural networks for image classification. In CVPR ,\\n2012.\\n[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet \\nclassification with deep convolutional neural networks. In \\nNIPS , 2012. \\n[34] K. Simonyan and A. Zisserman. Very deep convolutional \\nnetworks for large-scale image recognition. In ICLR , 2015. \\n[35] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked \\nprogressive auto-encoders (spae) for face recognition across \\nposes. In CVPR , 2014. \\n[36] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning \\nidentity-preserving face space. In ICCV , 2013. \\n[37] Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view \\nperceptron: a deep model for learning face identity and view \\nrepresentations. In NIPS , 2014. \\n[38] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. \\nRotating your face using multi-task deep neural network. In \\nCVPR , 2015. \\n[39] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face \\nfrontalization in unconstrained images. In CVPR , 2015. \\n[40] C. Sagonas, Y. Panagakis, S.  Zafeiriou, and M. Pantic. \\nRobust Statistical Face Frontalization. In ICCV , 2015. \\n[41] A. Ruiz, J. Van de Weijer, and X. Binefa. From Emotions to \\nAction Units with Hidden and Semi-Hidden-Task Learning. \\nIn ICCV , 2015. \\n[42] Y. Tang. Deep Learning with Linear Support Vector \\nMachines, In ICML Workshops , 2013. \\n[43] F. de la Torre, et al. Intraface. In FG, 2015. \\n1507\\n[44] R. Gross and V. Brajovic. An image preprocessing algorithm \\nfor illumination invariant face recognition. In The 4th \\nInternational Conference on Audio- and Video-Based \\nBiometric Person Authentication , 2003. \\n[45] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Learning Social \\nRelation Traits from Face Images. In ICCV , 2015. \\n[46] T. Devries, K. Biswaranjan, and G. W. Taylor. Multi-task \\nlearning of facial landmarks and expression. In 2014\\nCanadian Conference on Computer and Robot Vision , 2014. \\n[47] R. T. Ionescu, M. Popescu, and C. Grozea. Local learning to \\nimprove bag of visual words model for facial expression \\nrecognition. In ICML Workshops , 2013. \\n[48] A. Vedaldi and K. Lenc. Matconvnet – convolutional neural \\nnetworks for matlab. In The 23rd ACM Conference on \\nMultimedia , 2015. \\n1508\\n\",\n",
       " 'Human-computer interaction using emotion recognition from facial expression\\nF. ABDAT, C. MAAOUI and A. PRUSKI\\nLaboratoire d’Automatique humaine et de Sciences Comportementales\\nUniversite de metz\\nMetz, France\\nEmail: abdat, maaoui, pruski@univ-metz.fr\\nAbstract —This paper describes emotion recognition system\\nbased on facial expression. A fully automatic facial expression\\nrecognition system is based on three steps: face detection, facial\\ncharacteristic extraction and facial expression classiﬁcation. We\\nhave developed an anthropometric model to detect facial fea-\\nture points combined to Shi&Thomasi method. The variations\\nof 21 distances which describe the facial features deformations\\nfrom the neutral face, were used to coding the facial expression.\\nClassiﬁcation step is based on SVM method (Support Vector\\nMachine). Experimental results demonstrate that the proposed\\napproach is an effective method to recognize emotions through\\nfacial expression with an emotion recognition rate more than\\n90% in real time. This approach is used to control music player\\nbased on the variation of the emotional state of the computer\\nuser.\\nKeywords -Emotion recognition, facial expression, anthropo-\\nmetric model, SVM, music controler.\\nI. I NTRODUCTION\\nToday, one of the interesting challenges in the community of\\nhuman-computer interaction is how to make computers be more\\nhuman-like for intelligent user interfaces. In several experiments,\\nReeves and Nass [15] show that humans impose their interpersonal\\nbehavioral patterns onto their computers. Thus, the design of\\nrecent human-computer interfaces should reﬂect this observation in\\norder to facilitate more natural and more human-like interaction.\\nEmotion, one of the user affect, has been recognized as one of\\nthe most important ways of people to communicate with each\\nother. Given the importance and potential of the emotions, affective\\ninterfaces using the emotion of the human user are gradually\\nmore desirable in intelligent user interfaces such as human-robot\\ninteractions [12]. For such, an affective user interface and in order\\nto make use of user emotions, the emotional state of the human\\nuser should be recognized or sensed in many ways from diverse\\nmodality such as facial expression, speech, and gesture. Facial\\nexpression constitutes 55 % of the effect of communicated message\\n[13] and is hence a major modality in human communication.\\nIn this paper, we propose an automatic method for real time\\nemotion recognition using facial expression using a new anthropo-\\nmetric model for facial feature points detection. The remainder of\\nthis paper is organized as follows: ﬁrst, we present the facial feature\\npoints localization. Our facial expression recognition system is\\npresented in section 3, where we specify the feature extraction\\nmethod and the recognition approach. In section 4, we present theobtained experimental results. Finally, conclusion and future works\\nare presented in section 5.\\nII. F ACIAL FEATURE POINTS LOCALIZATION\\nFacial feature points are generally referred to as facial salient\\npoints such as the corners of the eyes, corners of the eyebrows,\\ncorners and outer mid points of the lips, corners of the nostrils, tip\\nof the nose, and the edge of the face. Detection of facial feature\\npoints is often the ﬁrst step in computer vision applications such\\nas face identiﬁcation, facial expression recognition, face tracking\\nand lip reading. Currently, however, this step is usually carried out\\nby manually labeling the required set of points.\\nSeveral methods for facial feature point detection could be clas-\\nsiﬁed in two categories: texture-based and shape-based methods.\\nTexture-based methods model local texture around a given feature\\npoint [19], for example the pixel values in a small region around a\\nmouth corner. Shape-based methods regard all facial feature points\\nas a shape [20], which is learned from a set of labeled faces, and\\ntry to ﬁnd the proper shape for any unknown face.\\nWe propose in this paper a simple and fast method to detect\\nautomatically facial feature points, based on an anthropometric\\nmodel and on Shi&Thomasi method for more accuracy.\\nA. Face detection\\nFace detection is the ﬁrst step in our facial expression recog-\\nnition system, to localize the face in the image. This step allows\\nan automatical labeling for facial feature points in a face image.\\nWe use a real-time face detector proposed in [17] available in\\nOpenCV library [5] which represents an adapted version of the\\noriginal Viola-Jones face detector(ﬁgure 1-a).\\nThe next step in the automatic facial points detection is to\\ndetermine the coarse position for each point. To achieve this, we\\ndevelop a fully automatic method using an anthropometric model.\\nB. Anthropometric model for facial feature points localiza-\\ntion\\nAnthropometry is a biological science that deals with the mea-\\nsurement of the human body and its different parts. Data obtained\\nfrom anthropometric measurement informs a range of enterprises\\nthat depend on knowledge of the distribution of measurements\\nacross human populations. After carefully performing anthropo-\\nmetric measurement on FEEDTUM database [18], we have been\\nable to build an anthropometric model of human face that can\\nbe used in localizing facial feature points from face images. The\\nlandmarks points that have been used in our face anthropometric\\nmodel for facial feature localization are represented in ﬁgure 1-e.\\n2011 UKSim 5th  European Symposium on Computer Modeling and Simulation\\n978-0-7695-4619-3/11 $26.00 © 2011 IEEE\\nDOI 10.1109/EMS.2011.20196\\n\\nFigure 1. Facial feature points detection outline\\nIt has been observed from the statistics of proportion evolved\\nduring our initial observation that, location of these points (P1 to\\nP38) can be obtained from:\\n•The distance Dmeasured between eyes axis EA and mouth\\naxisMA (ﬁgure 5 -c);\\n•Face symmetry axis SAas reference for xposition of points.\\nOur proposed anthropometric face model of facial feature points\\nlocalization has been developed from these proportional constants\\n(Table I) using distance between eyes axis, mouth axis, symmetry\\naxis and the face box center Y C as the principle parameter of\\nmeasurement.\\nTo realize the anthropometric model, we have used the following\\nstages:\\n1)Main axis localization:\\na)Eyes axis localization using gradient projection;\\nb)Mouth axis localization using color information;\\nc)Symmetry axis localization using gray image projec-\\ntion;\\n2)Facial feature points localization using the proportional\\nposition (table I);\\n3)Facial feature points detection using the Shi&Thomasi\\nmethod for more accuracy;\\nEach step will be detailed in the next part.Point X position Y position\\nP1 SA-0.91*D YC-0.91*D\\nP2 SA-0.58*D YC-1.17*D\\nP3 SA-0.26*D YC-1.3*D\\nP4 SA+0.26*D YC-1.3*D\\nP5 SA+0.58*D YC-1.17*D\\nP6 SA+0.91*D YC-0.91*D\\nP7 SA+1.04*D YC\\nP8 SA+0.71*D YC+0.91*D\\nP9 SA+0.52*D YC+1.17*D\\nP10 SA+0.13*D YC+1.30*D\\nP11 SA-0.13*D YC+1.3*D\\nP12 SA-0.52*D YC+1.17*D\\nP13 SA-0.71*D YC+0.91*D\\nP14 SA-1.04*D YC\\nP15 SA-0.06*D YC-0.26*D\\nP16 SA+0.06*D YC-0.26*D\\nP17 SA-0.78*D YC-0.52*D\\nP18 SA-0.58*D YC-0.58*D\\nP19 SA-0.26*D YC-0.52*DPoint X position Y position\\nP20 SA+0.26*D YC-0.52*D\\nP21 SA+0.58*D YC-0.58*D\\nP22 SA+0.78*D YC-0.52*D\\nP23 SA-0.26*D YC+0.32*D\\nP24 SA+0.26*D YC+0.32*D\\nP25 SA-0.65*D MA\\nP26 SA-0.32*D MA-0.1*D\\nP27 SA MA-0.15*D\\nP28 SA+0.32*D MA-0.1*D\\nP29 SA+0.45*D MA\\nP30 SA+0.32*D MA+0.1*D\\nP31 SA+0.19*D MA+0.13*D\\nP32 SA MA+0.15*D\\nP33 SA-0.19*D MA+0.13*D\\nP34 SA-0.32*D MA+0.1*D\\nP35 SA-0.74*D YC-0.26*D\\nP36 SA-0.58*D YC-0.39*D\\nP37 SA+0.58*D YC-0.39*D\\nP38 SA+0.74*D YC-0.26*D\\nTAB. I\\nPROPORTION OF FACIAL FEATURE POINTS POSITIONS MEASURED FROM\\nSUBJECTS OF DIFFERENT GEOGRAPHICAL TERRITORIES\\nC. Main axis localization\\nThe key step of our anthropometric model realization is facial\\nfeatures axis localization. In order to correctly localize facial axis,\\na constraint must be considered that the face must be frontal to\\nthe camera. Figure 5-c shows the three main axis which passe by\\nfacial features: two horizontal axis for mouth (MA) and eyes (EA)\\nand vertical axis passed by nose which give the symmetry of the\\nface (SA). Y C is the face box center.\\n1) Eyes axis localization: It is determined by the maximum\\nof the projection curve which has a high gradient. First, we\\ncalculate the gradient of the image I (corresponds to the face\\nrectangle extracted by the Viola-Jones detector):\\n∇Ix=δI\\nδx/vectori (1)\\n∇Ixcorresponds to the differences in the x(column) direction.\\nThe spacing between points in each direction is assumed to be\\none. Computing the absolute gradient value in each line is given\\nby:\\nHIx(x) =n/summationdisplay\\ny=1∇Ix(x, y) (2)\\nThen, we ﬁnd the maximum value which corresponds to the\\nline contains eyes (ﬁgure 1-b). This line corresponds to many\\ntransitions: skin to sclera, sclera to iris, iris to pupil and the same\\nthing for the other side (high gradient) [1].\\n2) Mouth axis localization: To locate the mouth axis, we\\nﬁrst deﬁne a Region Of Interest (ROI) of the mouth to be the\\nhorizontal strip whose top is at 0.67XR from the face bounding\\nbox top and has a width equal to 0.25XR. This strip is located\\naround the median of the bounding box of the face with a width\\nof0.1XR (ﬁgure 1-c), where R is the side face box.\\n197\\nFigure 2. Facial feature points localization\\nThen, we use the color information to locate the accurate position\\nof mouth axis. Given that the natural color of the lips is red, the\\nROI of the mouth is transformed into the HSV color space (Hue,\\nSaturation, Value), in order to separate color from intensity, and\\nthen into red domain in order to segment mouth area from the\\nrest of the strip image [16]. Image in red domain is obtained from\\nthe hue and saturation components of the input frame by applying\\nsome thresholds:\\n•120< H < 200\\n•S >70\\nThe center of this area is calculated as:\\nCx=M10/M00, C y=M01/M00 (3)\\nwhere,\\nM00=/summationdisplay\\nx,y∈ROIROI(x, y) (4)\\nM10=/summationdisplay\\nx,y∈ROIxROI (x, y) (5)\\nM01=/summationdisplay\\nx,y∈ROIyROI (x, y) (6)\\nM00is the ﬁrst moment. M10andM01are the second moments\\nof the region.\\nThe localization of Mouth axis corresponds to CY.\\n3) Symmetry axis localization: It is a vertical line which\\ndevides the frontal face in two equal sides. In other words, it is the\\nline passed by the nose. To locate the symmetry axis, we ﬁrst deﬁne\\na ROI of the nose. Since knowing the location of eyes axis (EA)\\nand mouth axis (MA), we deﬁne the nose region to be the vertical\\nstrip whose top is the eyes axis and has a height equal to D. This\\nstrip is located around the median of the bounding box of the face\\nwith a width of 10% of the face windows width. Analysis of the\\ngray level vertical projection of the ROI of the nose, shows that\\nthe maximum of the projection curves corresponds to symmetry\\naxis (ﬁgure 1-d).\\nExperimental results show that the extraction of the facial\\nfeature points using anthropometric model gives a good location\\nindependently to skin color and illumination changes (ﬁgure 2).\\nD. Detection of facial feature points using the Shi&Thomasi\\nmethod\\nOnce the anthropometric model is built, we look to ensure the\\nlocalization of points using the Shi&Thomasi method which gives\\nthe points with strong variation in a neighborhood N. This step is\\nnecessary, to robustly track the facial feature points [1].\\nFigure 3 -a shows the rectangles around the points detected using\\nthe anthropometric model. We will apply Shi&Thomasi method\\nonly inside of the rectangles, this method is available in OpenCv\\nlibrary [5].\\nFigures 3-b,c,d show results for feature points detection with the\\ncombination of anthropometric based method and Shi&Thomasi\\nRed small box: results from the anthropometric model\\nWhite big box: results from the combination of the anthropometric\\nmodel and Shi&Thomasi method\\nFigure 3. The inﬂuence of the window size on the quality of detection.\\nmethod. For the ﬁrst image, we have used a block of 5X5; for the\\nsecond image, we have used a block of 15X15; for the last one,\\nwe have used a block of 8X8.\\nThere is a great inﬂuence of the window’s size on the quality of\\ndetection. If the size is too small (like: 5X5), there is no change.\\nIf the size is big (like: 15X15),the result of the detection change,\\nwhere the position of other points are distorted.\\nFor the last size, the detection of eyebrows points are improved,\\nfor this reason, we must must change the window’s size depending\\nto the position of the facial feature point. We have chosen different\\nwindow’s sizes for each facial features.\\nThe facial feature points detected in the ﬁrst image of the\\nsequence will be tracked using pyramidal Lucas-Kanade algorithm\\n[4] which suppose that the brightness of every point of a moving\\nor static object does not change in time.\\nFigure 4 shows an example of facial feature points detection\\nand tracking. Where, we have a good localization of facial feature\\npoints in the ﬁrst image using our developed method. Also, we\\nhave a good localization of facial feature points in the remaining\\nof the sequence with the tracking step.\\nIII. F ACIAL EXPRESSION RECOGNITION\\nOur approach uses a feature based representation of facial data\\nfor SVM classiﬁer. It classiﬁes single images taken from an image\\nsequence with respect to six basic emotions of Ekman [7] happy,\\nfear, disgust, anger, sadness, surprise and neutral state.\\nOur work is based on the facial features deformation compared\\nto the neutral state.\\nA. Coding\\nFor Facial Action Coding System (FACS) [8], there are twenty\\ntwo facial muscles which are closely relevant to human expressions.\\nIn fact, the human facial expressions originate from the movements\\nof facial muscles beneath the skin. Thus, we represent each facial\\nmuscle by a pair of key points [9], namely dynamic point and ﬁxed\\npoint. As shown in ﬁgure 5-a, the dynamic points can be moved\\nduring an expression, while ﬁgure 5-b shows the ﬁxed points which\\ncan not be moved during a facial expression (face edge, nose root\\nand outer corners of the eyes).\\nFurther, each facial muscle is represented by a distance (ﬁgure\\n5-d), as:\\na* Eyebrows motions are described by the distances from\\nD1 to D7,\\nb* Eyes motions are described by the distances D8 and D9,\\nc* Nose motions are described by the distances D10 and\\nD11,\\nd* Mouth motions are described by the distances from D12\\nto D21.\\n198\\nimage 1 image 24 image 51 image 52 image 54 image 56 image 61\\nFigure 4. Facial features points detection and tracking in video sequence.\\n(a) (b) (c) (d)\\nFigure 5. a:Dynamic points, b:Static Points,c:Principal axis, d:facial\\ndistances\\nX-Position: frame number Y-Position: distance (pixel)\\nFigure 6. Distances variation from video sequence for different emotions\\nFigure 6 shows a sample of the characteristics distances of\\nFEEDTUM database[18] for three emotions: happy, fear and dis-\\ngust. This ﬁgure shows the distances evolution during a video\\nsequence that begins with a neutral state.\\nFor example, the distances variation of D20characterizes the\\nmouth motions. This distance presents a signiﬁcant variation in the\\ncase of happy where the lip’s corners withdrew back. This variation\\nis less important in the case of disgust and fear.\\nThe analysis of these curves, shows that a facial expression can\\nbe regarded as a combination of different distances variations from\\nneutral state.The used method to encode a facial expression takes into\\nconsideration all distances Divariations of each muscle during the\\nsequence. If DT= (d1, d2, ..., d i, ..., d 21)is the vector parameters\\nextracted from a video sequence at the moment T.∆Dis the\\ndistance variation from the ﬁrst image (a neutral expression),\\nwhether:\\n∆D= (D1\\nD01, ...,Di\\nD0i, ...,D21\\nD021) (7)\\nWhere D0iis theithdistance of the neutral state, with i∈[1,21].\\nThere are several works in the ﬁeld of facial expression recog-\\nnition using feature points. The added value of our work is the\\nmodelization of muscle contraction using the variation of muscle\\ndistances relative to the neutral state. The previous studies on facial\\nfeature modelization are based on:\\n1)points displacements [14],\\n2)facial points coordinates [3],\\n3)or distances between points but it is based on the deformation\\nof the facial contour or dimension [2],\\n4)deformation of the shape from the neutral state but not on\\nthe contraction of facial muscles [10].\\nB. Classiﬁcation\\nAfter the extraction of the necessary information from the facial\\nexpression, we have trained a statistical classiﬁer Support vector\\nmachine SVM. This method has the following advantages :\\n•A low parameter number to set,\\n•Very fast training,\\n•A low samples number is sufﬁcient for supports vectors\\ndetermination allowing discrimination between classes,\\n•Treatment of linear or nonlinear problems according to the\\nkernel function.\\nIV. R ESULTS AND DISCUSSION\\nA. Recognition from video sequence\\nFor the evaluation of our work, we have used two databases\\nCohn-Kanade [11] and FEEDTUM [18].\\nFigure 7 describes our facial expression recognition system as:\\n1)Figure 7(A) shows facial features extraction step which has\\nas input a video sequence and gives as output the distances\\nvector,\\n2)Figure 7(B) shows training step which has as input a series\\nof distances vector and gives as output the SVM model,\\n3)Figure 7(C) shows classiﬁcation step which has as input a\\ndistances vector and gives as output the recognized expres-\\nsion using the model built during the learning stage.\\nFor our system, we use libSVM, an open source SVM package\\n[6]. We have used the radial base function (RBF) as kernel which\\nis given by: K(x, xi) =exp(−/bardblx−xi/bardbl\\nσ).We perform k-fold cross-\\nvalidation, in order to compute the accuracy of each SVM. Every\\nfacial expression is represented by a set of distances (ﬁgure 5-d).\\nTo evaluate the recognition rate, we have tested several distances\\ncombinations. These distances are grouped in such a way that each\\n199\\nFigure 7. Facial expression recognition system\\nCombination Cohn-Kanade (%) FEEDTUM (%)\\nEyebrows: 7 distances 68.05 78.33\\nEyes: 2 distances 26.01 52.66\\nNose: 2 distances 46.66 40.33\\nMouth: 10 distances 83.05 59.75\\n21 distances 89.44 95.32\\nVariation of 21 distances 95.83 97.5\\nTable II\\nEMOTION RECOGNITION RATE\\nfacial feature is represented separately (eyebrows, eyes, nose and\\nmouth).\\nThe ﬁrst four rows of the table II represent emotions recognition\\nrate for different face parts. These results show the importance\\nof each part of the face. However, the facial expression is well\\ndescribed by the mouth in Cohn-Kanade database and the eyebrows\\nin a the FEEDTUM database, so we can not conclude that this part\\nis more descriptive than another, which leads to use all parts of the\\nface to avoid any ambiguity.\\nWe use all different parts of the face in order to have more\\ninformation on facial expression. The ﬁfth row of table II shows\\ncombinations results for each used database. We note that the best\\nrecognition rates are achieved by merging more information (21\\ndistances: Eyebrows+eyes+nose+mouth).\\nThe last row of table II shows emotion recognition rate using\\nvariations distance (equation 7). We note that using the report\\ndistance, we get a better recognition rate because the parameters\\nwill be more homogeneous. The variation of the distance using the\\nreport is robust with respect to the change of scale, as in the case\\nof camera zoom.\\nTables III and IV represent confusion matrix of different emo-\\ntions with Kohn-Kanade and FEEDTUM database respectively.\\nThis matrix shows the effectiveness of classiﬁcation method with\\nthe SVM. For our classiﬁer, we achieve the rate of 90% for all\\nemotions.\\nKernel choice is among the most important customizations that\\ncan be made when adjusting an SVM classiﬁer to a particular\\napplication domain. We experimented with a Gaussian radial basis\\nfunction (RBF) and linear kernels to compared recognition perfor-\\nmance. Table V shows that the classiﬁcation based on the RBF\\nkernel is best than the classiﬁcation based on the linear kernel.1 2 3 4 5 6 7\\n1 98.33% 0% 0% 0% 0% 0% 1.66%\\n2 0% 100% 0% 0% 0% 0% 0%\\n3 0% 0% 98.33% 0% 0% 0% 1.66%\\n4 0% 0% 0% 100% 0% 0% 0%\\n5 0% 0% 0% 0% 91.66% 0% 8.33%\\n6 0% 0% 0% 0% 0% 100% 0%\\n7 0% 0% 0% 0% 5% 0% 95%\\nTable III\\nEMOTION CONFUSION MATRIX FOR KOHN -KANADE DATABASE .\\n1:Happy 2:Fear 3:Disgust 4:Anger 5:Sadness 6:Surprise 7:Neutral\\n1 2 3 4 5 6 7\\n1 98.5% 0% 0% 0.5% 0% 0% 1%\\n2 0% 100% 0% 0% 0% 0% 0%\\n3 0% 0% 99% 0% 1% 0% 0%\\n4 0% 0 % 0 % 100% 0% 0% 0%\\n5 0 % 0 % 0 % 0% 97% 0 % 3%\\n6 0% 0% 0% 0% 0 % 100% 0%\\n7 0% 0% 0% 0% 2.5% 0% 97.5%\\nTable IV\\nEMOTION CONFUSION MATRIX FOR FEEDTUM DATABASE .\\nB. Facial expression recognition to control music player\\nEmotion recognition from facial expressions is a very powerful\\nmean for human-machine-interaction application due to the fact\\nthat the use of the camera allows non-intrusive application. In\\nthis paper, we present our controller of music player based on the\\nemotion of computer user. The person is in front of the embedded\\ncamera of the computer, recognition of emotions in real time is\\nperformed with our approach based on the variation of distance\\nand SVM classiﬁer. The aim of this application is to change the\\nmusic depending on the change of user’s emotion. The control of\\nmusic depend to emotion state, for example if the person is sad,\\nthe music is generated in such a way to switch the person to a\\npositive emotional state. This kind of application can be used by\\ndisabilities person to order their need.\\nThe method has achieved average recognition rates of 95%. The\\nresulting confusion matrix is shown in table VI.\\nDuring the real time test, the system achieved an average frame\\nrate of 25 frame per second for 320*240 images on a PC with\\n3.4 GHz Intel Pentium. Testing results have shown that the system\\nhas a practical range of working distances ( from user to camera),\\nand is robust against variation in lighting and backgrounds. The\\ntable VII presents the elapsed time for each step in our system.\\nThe elapsed time for points detection with the combined method\\ndepends on the windows size. For windows size of 8X8, the elapsed\\ntime is ≃0.3s.\\nlinear RBF\\nFEEDTUM database 94.25% 97.5%\\nKohn-Kanade database 94.66% 95.83%\\nTable V\\nCOMPARISON OF RECOGNITION RATE BETWEEN TWO KERNELS\\n200\\nhappy disgust neutral\\nhappy 90.47% 9.53% 0%\\ndisgust 0% 100% 0%\\nneutral 0% 0% 100%\\nTable VI\\nEMOTION CONFUSION MATRIX IN REAL TIME (INDEPENDENT CASE ).\\nOperation Time (S)\\nFace localization ≃0.28\\nPoint detection using anthropometric model ≃0.11\\nPoint detection using the combination method ≃0.30\\nTracking and classiﬁcation ≃0.031\\nTable VII\\nELAPSED TIME FOR EACH STEP .\\nV. C ONCLUSION\\nIn this paper we have presented an automatic approach to\\nemotion recognition based on facial expression analysis.To detect\\nthe face in the image, we have used the face detector of Viola-\\nJones which is fast and robust to illumination condition. For\\nfeature points extraction, we have developed an anthropometric\\nmodel which suppose that the position of facial feature points\\nare proportional to the vertical distance between eyes and mouth.\\nIn order to ensure detection robustness, we used the algorithm of\\nShi&Thomasi to extract feature points in an narrow window around\\nthe points found with the anthropometric model. The combination\\nof both method gives good results, when tested on images from\\nthe Cohn-Kanade database and FEEDTUM datbase, under various\\nillumination. The tracking of the detected points have been realized\\nwith Lucas-Kanade algorithm. Variations distance vector was used\\nas a descriptor of the facial expression. This vector is the input\\nof SVM classiﬁer. We have used our facial expression recognition\\nsystem to control a music player. Emotion recognition rates about\\n95% were achieved in real time.\\nIn future, we intend to develop a multimodal system using\\nfacial expression and physiological signals, in order to improve\\nthe emotions recognition results.\\nRÉFÉRENCES\\n[1]F. Abdat, C. Maaoui, and A. Pruski. Real facial feature points\\ntracking with pyramidal lucas-kanade algorithm. IEEE RO-\\nMAN08, The 17th International Symposium on Robot and\\nHuman Interactive Communication, Germany , 2008.\\n[2]Koen van de Sande Roberto Valenti Aitor Azcarate, Felix Ha-\\ngeloh. Automatic facial emotion recognition, 2005.\\n[3]J. Bailonson, E. Pontikakisb, I. Maussc, J. Grossd, M. Ja-\\nbone, C. Hutchersond, C. Nassa, and O. Johnf. Real-time\\nclassiﬁcation of evoked emotions using facial feature tracking\\nand physiological responses. International Human Computer\\nStudies , 66 :303–317, 2008.\\n[4]J.Y . Bouguet. Pyramidal implementation of the lucas kanade\\nfeature tracker. Intel Corporation , Microprocessor Research\\nLabs, 2000.[5]G. Bradski, T. Darrell, I. Essa, J. Malik, P. Perona, S. Sclaroff,\\nand C. Tomasi. http ://sourceforge.net/projects/opencvlibrary/,\\n2006.\\n[6]C.C. Chang and C.J. Lin. Libsvm : library for support vector\\nmachines. 2001.\\n[7]P. Ekman. Emotion in the human face. Cambridge University\\nPress , 1982.\\n[8]P. Ekman and W. Friesen. Facial action coding system : A\\ntechnique for the measurement of facial movement palo alto\\ncalif. Consulting psychologists press , 1978.\\n[9]S. S. Ge, C. Wang, and C. C. Hang. Facial expression\\nimitation in human robot interaction. IEEE RO-MAN08,\\nThe 17th International Symposium on Robot and Human\\nInteractive Communication, Germany , 2008.\\n[10] Z. Hammal. Segmentation des traits du visages, analyse et\\nreconnaissance des expressions faciales par le modèle de\\ncroyance transférable . PhD thesis, Université Joseph Fourier\\nde Grenoble, 2006.\\n[11] T. Kanade, J. F. Cohn, and Y . Tian. Comprehensive database\\nfor facial expression analysis. Fourth IEEE International\\nConference on Automatic Face and Gesture Recognition\\nGrenoble France , FG’00 :46–53, 2000.\\n[12] H.R. Kim, K.W. Lee, and D.S. Kwon. Emotional interac-\\ntion model for a service robot. Proceedings of the IEEE\\nInternational Workshop on Robots and Human Interactive\\nCommunication , pages 672–678, 2005.\\n[13] A. Mehrabian. Communication without words. Psychology\\nToday , 2.4 :53–56, 1968.\\n[14] I. Cohen Y . Sun T. Gevers N. Sebe, M.S. Lew and T.S.\\nHuang. Authentic facial expression analysis. Proc. IEEE\\nInt’l Conf.Automatic Face and Gesture Recognition (AFGR) ,\\n2004.\\n[15] B. Reeves and C. Nass. The media equation. Cambridge\\nUniversity Press , 1996.\\n[16] S. Shamik, Q. Gang, and Sakti P. Segmentation and histogram\\ngeneration using the hsv color space for image retrieval. icip,\\npages 7803–7622, 2002.\\n[17] P. Viola and M. Jones. Robust real-time object detection. 2nd\\ninternational workshop on statistical and computational theo-\\nries of vision - modeling, learning, computing, and sampling\\nvancouver, canada , 2001.\\n[18] F. Wallhoff. Feedtum : Facial expressions and emotion\\ndatabase, 2005.\\n[19] Z. Xin, X. Yanjun, and D. Limin. Locating facial features\\nwith color information. IEEE International Conference on\\nSignal Processing , 2 :889–892, 1998.\\n[20] P.W. Yuille, A.L.and Hallinan and D.S. Cohen. Feature ex-\\ntraction from faces using deformable templates. International\\nJournal of Computer Vision , 8 :99–111, 1992.\\n201\\n',\n",
       " 'Identity-Adaptive Facial Expression Recognition Through Expression Regeneration\\nUsing Conditional Generative Adversarial Networks\\nHuiyuan Y ang, Zheng Zhang and Lijun Yin\\nDepartment of Computer Science\\nState University of New York at Binghamton, USA\\n{hyang51, zzhang27 }@binghamton.edu; lijun@cs.binghamton.edu\\nAbstract —Subject variation is a challenging issue for fa-\\ncial expression recognition, especially when handling unseen\\nsubjects with small-scale lableled facial expression databases.\\nAlthough transfer learning has been widely used to tackle the\\nproblem, the performance degrades on new data. In this paper,\\nwe present a novel approach (so-called IA-gen ) to alleviate the\\nissue of subject variations by regenerating expressions from any\\ninput facial images. First of all, we train conditional generative\\nmodels to generate six prototypic facial expressions from any\\ngiven query face image while keeping the identity related\\ninformation unchanged. Generative Adversarial Networks are\\nemployed to train the conditional generative models, and each\\nof them is designed to generate one of the prototypic facial\\nexpression images. Second, a regular CNN (FER-Net) is ﬁne-\\ntuned for expression classiﬁcation. After the corresponding\\nprototypic facial expressions are regenerated from each facial\\nimage, we output the last FC layer of FER-Net as features for\\nboth the input image and the generated images. Based on the\\nminimum distance between the input image and the generated\\nexpression images in the feature space, the input image is\\nclassiﬁed as one of the prototypic expressions consequently.\\nOur proposed method can not only alleviate the inﬂuence of\\ninter-subject variations, but will also be ﬂexible enough to\\nintegrate with any other FER CNNs for person-independent\\nfacial expression recognition. Our method has been evaluated\\non CK+, Oulu-CASIA, BU-3DFE and BU-4DFE databases,\\nand the results demonstrate the effectiveness of our proposed\\nmethod.\\nKeywords -FER; GAN; Identity-adaptive; CNN;\\nI. I NTRODUCTION\\nIn the past decades, research on automatic facial expres-\\nsion recognition has been conducted through classifying\\nclassic prototypic facial expressions ( i.e., anger, disgust, fear,\\nhappiness, sadness, surprise) from static images or dynamic\\nimage sequences. Although signiﬁcant progresses have been\\nmade in recent years, the challenge still remains in real-\\nworld scenarios with respect to various poses, illumination,\\nocclusion, and in particular, the identity related expression\\nvariations. The inter-subject variations come from a variety\\nof identity components, including age, gender, race and\\nperson-speciﬁc characteristics [1]. Compared with the VGG\\nface dataset [2] and Imagenet [3], the current public expres-\\nsion datasets are limited in size, making it more difﬁcult to\\ndeal with identity-related variations.\\nFigure 1. The upper part is the illustration for features of different subjects\\nwith a variety of expressions in the feature space. The lower part shows\\nthe generated identity-adaptive sub-space for each subject.\\nRecently, several approaches have been proposed by fo-\\ncusing on improving performance on person-independent\\nfacial expression recognition [4] [1] [5]. Transfer learning is\\none of the mostly used methods, which ﬁne-tunes a network\\nthat has been pre-trained on a large dataset i.e. VGG [2],\\nFaceNet [6] and FER-2013 [7] , to a relatively small facial\\nexpression dataset. Other methods include: using a deeper\\nnetwork with more training data [8]; learning a person-\\nspeciﬁc model [9] and adding extra constraints for identity-\\nrelated variations in FER task. Although those efforts have\\nalleviated the problem to a certain degree, the challenge still\\nremains unsolved.\\nFor facial expression recognition task, the same expres-\\nsions are expected to have a smaller distance between each\\nother in the feature space than others of different expres-\\n2942018 13th IEEE International Conference on Automatic Face & Gesture Recognition\\n978-1-5386-2335-0/18/$31.00 ©2018 IEEE\\nDOI 10.1109/FG.2018.00050\\n\\nFigure 2. Framework of our proposed method ( IA-gen )\\nsions. As illustrated in Fig. 1, I(happiness,A )andI(sad,A )\\nare the same subject A showing different expressions,\\nwhileI(happiness,A )andI(happiness,B )are different subjects\\nshowing the same expressions. However, due to high inter-\\nsubject variations, the distance between I(happiness,A )and\\nI(happiness,B )is larger than I(happiness,A )andI(sad,A ).\\nV ariations are greater related to identity compared to\\nvariations related to expression. This partially explains why\\nmost current methods could degrade on unseen subjects. As\\nin the lower part of Fig. 1, we generate two sub-spaces\\nfor subject A and subject B respectively, each of them\\ncontains the generated six basic expression images of the\\nquery image. Only the same identity information exists in\\neach sub-space, thus allowing for expression classiﬁcation\\nto be more reliable. This motivated us to design a new\\nscheme by generating a kind of sub-space for facial expres-\\nsion classiﬁcation while aiming each subject individually.\\nThis identity-adaptive FER approach is robust to identity\\nvariations, achieving the goal of person-independent facial\\nexpression recognition.\\nGenerative Adversarial Networks (GAN) [10], an effective\\ntraining method for training generative models, was ﬁrst de-\\nveloped by Goodfellow et al in 2014. The GAN framework\\nplays an adversarial game with two players, a generator and\\ndiscriminator . The discriminator is designed to distinguish\\nbetween samples from the generator and samples from the\\ntraining data. On the other hand, the generator learns to out-\\nput samples that can maximally confuse the discriminator.\\nThe conditional Generative Adversarial Networks (cGAN)\\n[11], an extension of the basic GAN model, enables the\\ngenerative model to learn different contextual informationby adding extra conditional variations to the input. Another\\nextension is the combination of GAN and CNN [12], which\\nhas facilitated a number of interesting applications, e.g,\\nobject attributes manipulation by vector arithmetic [12], face\\ngeneration [13], and object reconstruction from edge maps\\n[14].\\nThe potential application of cGAN in face generation, as\\nwell as the current limitation of facial expression recogni-\\ntion, motivated us to develop an identity-adaptive structure\\nto address the issue of high inter-subject variations through\\nregenerating six basic facial expression images of a same\\nsubject. Unlike the previous method [1] that tried to split\\nthe facial image feature into two parts: expression-related\\nand identity-related. However, our method is based on the\\nassumption that individual facial expression is dependent on\\nthe individual’s identity, i.e. gender, age, and r ace with var-\\nious expression styles , whereby expression-related features\\nand identity-related features are partially overlapped, and not\\nseparable. By regenerating six facial expression images of a\\nsubject given a face image of the individual, our approach\\nlimits the feature comparison in a single identity sub-space,\\nallowing us to further compare the features merely caused\\nby expression variations of the subject. The feature space\\nis adaptive to a single identity without involving other\\nindividuals, thus identity variations will not be an issue\\nfor facial expression classiﬁcation. This enables the facial\\nexpression recognition to work in a person-independent\\nmanner. The main contribution of this paper is two-fold:\\n•To our knowledge, this is the ﬁrst work to address\\nthe facial expression recognition by expression re-\\ngeneration through the exploration of GANs and the\\n295\\napplication of cGANs.\\n•The proposed method can effectively deal with the issue\\nof subject variations, because the feature sub-space is\\ngenerated for a single subject with no afﬁliation with\\nthe other subjects. This ”adaptive” property allows FER\\nto work person-independently. In addition, the proposed\\nmethod can be easily integrated into regular networks\\nfor performance improvement.\\nTo validate the effectiveness of our proposed method, we\\nconducted four experiments on four public databases: CK+\\n[15], Oulu-CASIA [16], BU3DFE [17] and BU-4DFE [18].\\nThe results show that our proposed method achieved superior\\nperformance compared to the state-of-the-art methods.\\nII. R ELA TED WORKS\\nFacial expressions are varied from person to person with\\nrespect to age, race, gender, and cultural background in terms\\nof their appearances and styles of facial actions. Such a sub-\\nject variation issue could cause performance degradation in\\nfacial expression recognition, especially on unseen subjects.\\nThere are existing approaches developed for focusing on\\nimproving person-independent facial expression recognition.\\nChen et al. [9] attempted to learn a person-speciﬁc model\\nthrough transfer learning. Ding et al. [5] proposed a so-called\\nFaceNet2ExpNet structure for facial expression recognition\\non relatively small datasets, which used the pre-trained\\nmodel as supervision rather than as a source of some initial\\nparameter values. Their method consists of two stages:\\nﬁrst, only the convolutional layers were trained to generate\\nsimilar intermediate features to those of the pre-trained\\nmodel; second, the whole deep model was trained with\\nthe label information. The experiments showed that their\\nmethod worked well on relatively small expression datasets.\\nZhao et al. [4] achieved invariance to expression intensity\\nby considering both peak expressions and weak expressions\\nto train a network. The invariance was achieved by a peak\\ngradient suppression learning algorithm, which drove the\\nintermediate features of weak expressions towards those of\\npeak expressions. Meng et al. [1] proposed an identity-\\naware deep model for facial expression recognition, which\\nwas capable of learning the features that were invariant to\\nboth expression and identity-related variations, by utilizing\\nan expression-sensitive contrastive loss function.\\nGenerative Adversarial Networks (GANs) have been\\nvigorously studied in recent years. It was ﬁrst proposed by\\nGoodfellow et al. [10] known as the generative adversarial\\nnet (GAN), which applied the minimax game to two players,\\ni.e.,generator (G) and discriminator (D) for recovering the\\ndistribution of the training data by G while keeping D to1\\n2.\\nRadford et al. [12] had successfully scaled up GANs using\\nCNNs to model images, and showed good result with the\\ntrained discriminators for image classiﬁcation tasks, as well\\nas the interesting vector arithmetic properties of generated\\nsamples of generator. Gauthier [13] extended the generativeadversarial net framework by adding a conditional informa-\\ntion, which could deterministically control the output of the\\ngenerator, and showed how to generate faces with speciﬁc\\nattributes from nothing but random noise and conditional\\ninformation. Isola et al. [14] utilized conditional adversarial\\nnetworks for image-to-image translation, and showed that\\nthe conditional adversarial networks were applicable for a\\nwide variety of applications, i.e. synthesizing images from\\nlabels, reconstructing objects from edge maps and colorizing\\nimages.\\nInspired by the above-mentioned prior works, especially\\nby the solution of image-to-image translation with condi-\\ntional generative adversarial networks [14], we propose to\\nexplicitly train a set of expression-speciﬁc models ( e.g., six\\nprototypic expression models) to generate the corresponding\\nexpressions of each subject for a given image of the subject.\\nBy doing so, we can utilize expression features oriented by\\neach individual adaptively in the corresponding feature sub-\\nspace even with the identity-related variations occurred from\\nperson to person, thus leading to the FER improvement with\\nunseen subjects.\\nIII. P ROPOSED METHOD\\nOur proposed method is based on the regeneration of six\\nbasic facial expressions, which are adaptive to each identity\\nof individual, we call it the Identity-Adaptive Generation\\nmethod (a.k.a. IA-gen ). As shown in Fig.2, the IA-gen\\nframework consists of two parts. The upper part is aimed\\nto generate six basic facial expression images of the same\\nsubject for any query image using six cGANs, and each of\\nthem is designed to generate one expression respectively.\\nThe lower part is the facial expression recognition module.\\nA pre-trained CNN is ﬁrst ﬁne-tuned on the database, and\\nthen the last fully connected layer is used as features for\\nboth the query image and regenerated images. The query\\nimage is labeled as one of the six basic expressions based\\non a minimum distance in feature space.\\nA. GANs for Facial Expression Regeneration\\nGANs have been successfully used to generate images\\n[14] [13] [12]. Note that CNN based methods could also\\nbe used for image generation. However, they use a mean\\nsquared error (MSE) based solution, resulting in overly\\nsmooth of the generated image. However, GANs force\\nthe regeneration towards the natural image manifold and\\nproduce more perceptually natural results [19]. Therefore,\\nwe apply the GANs for facial expression image regeneration.\\nThe generator G is trained to produce outputs that cannot\\nbe distinguished from ”real” image pairs by discriminator\\nD, which is adversarially trained to detect ”fake” image\\npairs. The training procedure is illustrated in Fig. 3, where G\\ngenerates an output for any input image, and the image pairs\\n<I input,Ioutput>are constructed as negative examples\\n296\\nFigure 3. Training a GAN to generate facial expression image. The\\ndiscriminator (D) learns to distinguish between the [input, target] and\\n[input, output] pairs, while the generator (G) learns not only to fool the\\ndiscriminator, but also to be close to the ground truth (target image).\\nfor the D, while image pairs <I input,Itarget>from the\\ntraining dataset are also constructed as positive examples.\\nThe objective for discriminator can be expressed as:\\nLcGAN (D)=1\\nNN/summationdisplay\\ni=1/braceleftbigg\\nlogD (Ii\\ninput,Ii\\ntarget )+\\nlog/bracketleftbig\\n1−D/parenleftbig\\nIi\\ninput,G(Ii\\ninput )/parenrightbig/bracketrightbig/bracerightbigg\\n(1)\\nwhereNis the total number of training image pairs <\\nIinput,Itarget>. To against an adversarial D, G is used\\nto not only fool the discriminator, but also to generate an\\noutput as similar to the target image as possible. The loss\\nfunction for the generator is deﬁned as follow:\\nLcGAN (G)=1\\nNN/summationdisplay\\ni=1/braceleftbigg\\nLadv+α·Lcont/bracerightbigg\\n(2)\\nLadv=−log/bracketleftbig\\nD/parenleftbig\\nIinput,G(Iinput )/parenrightbig/bracketrightbig\\n(3)\\nHere,Ladv is the adversarial loss, as deﬁned in Eq.3,\\nD/parenleftbig\\nIinput,G(Iinput )/parenrightbig\\nis the probability that the regenerated\\nimage pairs <I input,Ioutput>are recognized as training\\nexample. Lcont is the content loss between the regenerated\\nIoutput and training example Itarget . The most widely used\\nloss function is the pixel-wise MSE loss, which is calculated\\nas:\\nLMSE\\ncont =1\\nc2WHcW/summationdisplay\\nx=1cH/summationdisplay\\ny=1/braceleftbigg\\nItarget (x,y)−Ioutput (x,y)/bracerightbigg2\\n(4)\\nHere, c, W, H are the channel, width and height of the\\nimage, respectively. However, MSE based solution often\\noverly smooths textures, which results in the lack of highfrequency content, and generates perceptually unsatisfying\\nresults [19]. In our experiment, we use L1loss rather than\\nL2loss, because the L1loss can reduce the effect of over-\\nsmoothing on the generated image [14]. Another choice for\\ncontent loss is perceptual similarity that measure high-level\\nperceptual differences between images [20]. The perceptual\\nloss is deﬁned on a loss network φwhich is pre-trained\\nfor image classiﬁcation, e.g., VGG network [2]. Rather\\nthan penalizing the pixel differences between Ioutput and\\nItarget , the perceptual loss allows to have similar feature\\nrepresentations to be computed in φ. Letφj(I)be the j-th\\nconvolutional layer with a shape of Cj×Hj×Wj; then\\nthe perceptual loss is the euclidean distance between feature\\nrepresentations:\\nLpep\\ncont=1\\nC2\\njHjWjCjWj/summationdisplay\\nx=1CjHj/summationdisplay\\ny=1/braceleftbigg\\nφx,y(Itarget )−φx,y(Ioutput )/bracerightbigg2\\n(5)\\nThe ﬁnal loss function for the generator G can be written\\nas:\\nLcGAN (G)=1\\nNN/summationdisplay\\ni=1/braceleftbigg\\nLadv+α·LMSE\\ncont +β·Lpep\\ncont/bracerightbigg\\n(6)\\nand the ﬁnal optimization target is to solve the adversarial\\nmin-max problem:\\nG∗=argmin\\nGmax\\nD/braceleftbig\\nLcGAN (D)+λLcGAN (G)/bracerightbig\\n(7)\\nThe complete architecture of GAN follows the structure\\nused in [14]. Speciﬁcally, the generator G is a deconvo-\\nlutional neural network [21], which contains six Encoders\\nwith output channels {64, 128, 256, 512, 512, 512 }and six\\nDecoders with output channels {512, 512, 256, 128, 64, 1 }.\\nEach layer is followed by a non-linear activation function\\n(ReLU) and batch normalization (BN) layer. The stride size\\nis set as 2 to avoid max-pooling operation, and the ﬁlter size\\nis set as 4×4for all layers. Skip connections ( U-net [22]\\n) are also used here, which help pass low-level information\\nshared between the input and output image directly across\\nthe net. The discriminator D is a regular convolutional neural\\nnetwork with {64, 128, 256, 512, 1 }output channels.\\nB. A CNN for Facial Expression Recognition\\nDue to large variations across subjects, facial expression\\nrecognition performance degrades on unseen subjects in\\nreal world scenarios. To cope with this problem, IA-gen\\nis ﬁrst used to construct a sub-space with six expressions\\nof the same subject, and then a regular convolutional neural\\nnetworks (FER-net) is used for facial expression recognition.\\nThe last fully connected layer of FER-net is used as features\\nfor both the input image and the generated images, and\\nthe input image is labeled as one of the six basic facial\\nexpressions based on a distance function in the feature space:\\nPredict =argmin\\ni||Feat (Iinput )−Feat (Ii)|| (8)\\n297\\nFigure 4. The generated six facial images for the input image in four\\ndatabases.\\nHere,Feat (·)is a feature extraction function. Any method,\\nexcept the FER-net used here, that can be used to extract\\nthe facial expression related features works as well.\\nIV . E XPERIMENTS\\nWe apply the proposed IA-gen approach to the task of\\nfacial expression recognition on four publicly available fa-\\ncial expression databases: extended CK+ [15], Oulu-CASIA\\n[16], BU-3DFE [17] and BU-4DFE [18].\\nA. Implementation\\nWe apply the tree-structured deformable models (TSM)\\n[23] for face detection and landmarks localization. The\\nfaces are then cropped and resized to 70×70. The la-\\nbeled facial expression database is quite small, thus we\\nutilize conventional data augmentation method to gener-\\nate more training data, where each image is rotated by\\ndegree [−15o,−12o,−9o,−6o,−3o,0o,3o,6o,9o,12o,15o]\\nrespectively. Five 64×64 patches are cropped out from\\nﬁve locations of each image ( center and four corners,\\nrespectively ), then each patch is ﬂipped horizontally, thus\\nresulting in an augmented dataset which is 110 times larger\\nthan the original one. During testing, neither the rotation nor\\nthe ﬂipping operation is used on the input image.\\nThe cGAN models are pre-trained on BU-4DFE database,\\nand then ﬁne-tuned on other databases. In order to have\\na similar baseline as [5], the VGG [2] model is used as\\nbaseline in Oulu-CAISA database. All the others use the\\nFER-Net, which is ﬁne-tuned from a pre-trained CNN model\\non the Facial Expression Recognition (FER-2013) database\\n[7]. The batch size is 100, the momentum is ﬁxed to be\\n0.9, and the dropout is set to 0.5 during training, and 1.0\\nduring testing. The initial learning rate is set to 0.001, and\\nis decreased by 0.8 for 20 epochs. α,βandλare set to\\n100, 10 and 1 respectively. Both cGANs and FER-Net are\\nimplemented using tensorﬂow [24].\\nB. Facial Expression Generation\\nFig.4 illustrates the generated facial expression images by\\ncGAN. The ﬁrst column (from top to bottom) represents the\\ninput images from CK+ [15], Oulu-CASIA [16], BU3DFETable I\\nAVERAGE ACCURACY ON THE CK+ DA TABASE FOR SEVEN CLASSES .\\nMethod Setting Accuracy\\nLBP-TOP [25] sequence-based 88.99\\nHOG 3D [26] sequence-based 91.44\\n3DCNN [27] sequence-based 85.9\\nSTM-Explet [28] sequence-based 94.19\\nIACNN [1] image-based 95.37\\nDTAGN [29] sequence-based 97.25\\nCNN (baseline) image-based 89.50\\nOurs image-based 96.57\\nFigure 5. Confusion matrix on the CK+ database.\\n[17] and BU-4DFE [18] respectively, and the rest of the\\ncolumns show the generated facial images with different\\nexpressions. As illustrated in Fig.4, the facial expression\\nregeneration part is capable of generating different facial ex-\\npression images while keep the identity-related information\\nvisually unchanged.\\nC. Evaluation on CK+\\nThe Extended Cohn-Kanade database (CK+) [15] is\\nwidely used for evaluating facial expression recognition. It\\ncontains 593 video sequences recorded from 123 subjects,\\neach of which displayed one of seven expressions, from\\nneutral to peak expression. The label information is only\\nprovided for the last frame of each sequence. Following\\nthe general procedure, we use the last three frames of\\neach sequence with the provided label, which results in\\n981 images. The images are further split into 10 folds by\\nID in ascending order, so the subjects in any two subsets\\nare mutually exclusive. Both the proposed method and\\nCNN based baseline are trained and tested on static images.\\nThe reported results are the average of the 8 runs. As\\nshown in Table I, the performance of our proposed method\\noutperforms the CNN based baseline in terms of the average\\naccuracy of seven expressions. The proposed method is also\\ncompared to the state-of-the-art methods evaluated on the\\nCK+ database. Among them. IACNN [1], CNN baseline and\\nour proposed method are image-based methods, while others\\nare sequence-based, and use the temporal information. In\\ncontrast, our method IA-gen, which is more suitable for ap-\\n298\\nTable II\\nAVERAGE ACCURACY ON THE OULU -CASIA DATABASE .\\nMethod Setting Accuracy\\nLBP-TOP [25] sequence-based 68.13\\nHOG 3D [26] sequence-based 70.63\\nSTM-Explet [28] sequence-based 74.59\\nAtlases [30] sequence-based 75.52\\nDTAGN-Joint [29] sequence-based 81.46\\nFN2EN [5] image-based 87.71\\nPPDN [4](peak expression) image-based 84.59\\nVGG (baseline) image-based 82.33\\nOurs image-based 88.92\\nFigure 6. Confusion matrix on the Oulu-CASIA database.\\nplications where videos or image sequence is not available,\\nachieves comparable result, with 96.57% for seven classes.\\nAimed at solving the identity-related issues, IACNN [1] tried\\nto split the whole face image feature into two parts: identity-\\nrelated and expression-related features, but this method may\\nnot generalize well for all six basic facial expressions. For\\nexample, people may show something in common when\\nsmiling, but often individuals have unique ways to express\\nother emotions. This difference can be related to age, gender,\\nrace, and even the background of education. Instead of trying\\nto split the identity-related information (which, sometimes, is\\nnot separable), our proposed IA-gen adaptively regenerates\\na sub-space that contains six basic facial expressions of the\\nsame subject, so the facial expression recognition is limited\\nin a single-identity subspace, and that’s why our proposed\\nmethod shows higher performance than IACNN [1].\\nFig. 5 is the confusion matrix, from which we can see that,\\nour proposed method performs well in recognizing contempt\\nand surprise , while showing lowest recognition accuracy in\\nfear , which is confused mainly with contempt and anger .\\nD. Evaluation on Oulu-CASIA\\nThe Oulu-CASIA database [16] contains two subsets: VIS\\nand NIR. Here, we only use the VIS subset, in which all\\nvideos were captured by VIS camera. The Oulu-CASIA VIS\\nhas 480 video sequences taken from 80 subjects and six ex-\\npressions under dark, strong, weak illumination conditions.\\nIn this experiment, only videos that were captured underTable III\\nAVERAGE ACCURACY ON THE BU-3DFE DATABASE .\\nMethod Setting Accuracy\\nWang et al. [31] 3D 61.79\\nBerretti et al. [32] 3D 77.54\\nY ang et al. [33] 3D 84.80\\nLopes [34] image-based 72.89\\nCNN (baseline) image-based 73.2\\nOurs image-based 76.83\\nFigure 7. Confusion matrix on the BU-3DFE database.\\nstrong condition are used. As a general procedure, the last\\nthree frames of each sequence with the provided label are\\nused, which result in 1440 images. Similar to experiment\\nsettings in [4], a 10-fold subject independent cross validation\\nis performed.\\nThe82.33% baseline is reported by ﬁne-tuning the VGG\\n[2] network on the dataset. With the introduction of IA-\\ngen approach, the classiﬁcation performance is increased to\\n88.92%.We also compare with the state-of-the-art methods,\\nas summarized in Table II. Among them, FN2EN [5],\\nPPDN [4] and our proposed method are image-based, while\\nthe others are sequence-based methods, which utilize the\\ntemporal-spatio information for facial expression recogni-\\ntion.\\nIn the confusion matrix in Fig.6, the happiness and\\nsurprise expressions have the highest recognition rate, while\\ndisgust shows the lowest recognition accuracy, and is mainly\\nconfused with anger .\\nE. Evaluation on BU-3DFE\\nThe BU-3DFE database [17] contains 100 subjects, rang-\\ning in age from 18 to 70 years old with a variety of race.\\nEach subject displays six basic expressions with four levels\\nof intensity and a neutral expression, which results in a total\\nof 2,500 3D facial expression models and texture images.\\nIn this experiment, we only use the texture images, and\\nhigh intensity expressions ( the last two frames). A 10-\\nfold cross-validation is performed, and the split is subject\\nindependent. Table III shows the average result of 10\\nruns on the BU-3DFE database. Unlike the CK+ and Oulu-\\nCASIA databases, the BU-3DFE is more challenging as it\\n299\\nTable IV\\nAVERAGE ACCURACY ON THE BU-4DFE DATABASE .\\nMethod Setting Accuracy\\nDapogny et al. [35] sequence-based 75.8\\nLSH-CORF. [36] sequence-based 77.1\\nPCRF [37] image-based 76.1\\nL´aszl ´o et al. [38] 3D 78.2\\nPan et al. [39] image-based 80.89\\nCNN (baseline) image-based 76.45\\nOurs image-based 89.55\\nFigure 8. Confusion matrix on the BU-4DFE database.\\nhas a wider variety of ethnicities and a larger range of ages\\nwhile the data size is relatively small. Note that [33] has\\nhigher performance due to the use of geometric feature of the\\n3D shape model. However, our proposed method improves\\nthe baseline by 3.6%, and also outperforms the image-based\\nmethod [34].\\nFig. 7 shows the confusion matrix, where the expressions\\n(surprise, happiness, and disgust) are classiﬁed better than\\nthe other expressions.\\nF . Evaluation on BU-4DFE\\nThe BU-4DFE [18] contains 606 facial expression se-\\nquences captured from 101 subjects. Each sequence shows\\none of the six basic facial expressions starting from neutral\\nto peak expression, and ending with neutral again. Seven\\nframes around the peak expression are collected with the\\nprovided sequence label, which results in 4272( 101×6×7)\\nimages. Similar to the CK+ database setting, a 10-fold\\nsubject-independent cross validation is performed. Table IV\\nreports the average accuracy of 10 runs on the BU-4DFE\\ndatabase for recognizing six expressions. As shown, our\\nproposed method has a 13.1% improvement over the CNN\\nbaseline, and achieves the highest accuracy compared to the\\nstat-of-the-art methods, including sequence-based methods\\n[35] [36], image-based methods [37] [39] and 3D model\\nbased method [38]. The confusion matrix in Fig.8 shows\\nthat happiness and surprise are the easiest expressions to\\nrecognize, while sadness shows relatively low recognition\\nrate, which is mainly confused with anger .\\nFigure 9. Per-class precision and F1 score on CK+, Oulu-CASIA, BU-\\n3DFE and BU-4DFE databases\\nPer-expression precision and F1 score for CK+, Oulu-\\nCASIA, BU-3DFE and BU-4DFE databases are shown in\\nFig.9. We observe that both precision and F1 score are the\\nlowest in BU-3DFE database. This is due to the limited data\\nsize and high variety of ethnicities with larger range of ages,\\nmaking it more challenging to recognize facial expressions.\\nV. C ONCLUSIONS AND FUTURE WORKS\\nIn this paper, we proposed an identity-adaptive training\\nalgorithm for facial expression recognition. First, six basic\\nfacial expression images are regenerated for any query im-\\nage. Then, the facial expression is recognized by comparing\\nthe query image and the generated six facial images in\\nthe face space. Rather than trying to split image features\\ninto expression-related and identity-related parts, we keep\\nthe identity-related information adaptive to each query im-\\nage, and make the facial expression recognition identity-\\nindependent.\\nFor the future work, we plan to apply our method for\\nfacial expression recognition in the wild by training a more\\ngeneral facial expression regeneration model using more\\ndatabases.\\nVI. A CKNOWLEDGMENTS\\nThe material is based upon the work supported in part by\\nthe National Science Foundation under grants CNS-1629898\\nand CNS-1205664.\\nREFERENCES\\n[1] Z. Meng, P . Liu, J. Cai, S. Han, and Y . Tong, “Identity-\\naware convolutional neural network for facial expression\\nrecognition,” in FG. IEEE, 2017, pp. 558–565.\\n[2] K. Simonyan and A. Zisserman, “V ery deep convolutional\\nnetworks for large-scale image recognition,” arXiv preprint\\narXiv:1409.1556 , 2014.\\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei, “Imagenet: A large-scale hierarchical image database,”\\ninCVPR . IEEE, 2009, pp. 248–255.\\n300\\n[4] X. Zhao and X. e. a. Liang, “Peak-piloted deep network for\\nfacial expression recognition,” in ECCV . Springer, 2016, pp.\\n425–442.\\n[5] H. Ding, S. K. Zhou, and R. Chellappa, “Facenet2expnet:\\nRegularizing a deep face recognition net for expression recog-\\nnition,” in Automatic Face & Gesture Recognition (FG 2017) .\\nIEEE, 2017, pp. 118–126.\\n[6] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A\\nuniﬁed embedding for face recognition and clustering,” in\\nCVPR , 2015, pp. 815–823.\\n[7] I. J. Goodfellow, D. Erhan, and P . L. e. a. Carrier, “Challenges\\nin representation learning: A report on three machine learning\\ncontests,” in International Conference on Neural Information\\nProcessing . Springer, 2013, pp. 117–124.\\n[8] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper\\nin facial expression recognition using deep neural networks,”\\ninApplications of Computer Vision (WACV) . IEEE, 2016.\\n[9] J. Chen, X. Liu, P . Tu, and A. Aragones, “Learning person-\\nspeciﬁc models for facial expression and action unit recogni-\\ntion,” Pattern Recognition Letters , vol. 34, no. 15, 2013.\\n[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\\nFarley, S. Ozair, A. Courville, and Y . Bengio, “Generative ad-\\nversarial nets,” in Advances in neural information processing\\nsystems , 2014, pp. 2672–2680.\\n[11] M. Mirza and S. Osindero, “Conditional generative adversar-\\nial nets,” arXiv preprint arXiv:1411.1784 , 2014.\\n[12] A. Radford, L. Metz, and S. Chintala, “Unsupervised repre-\\nsentation learning with deep convolutional generative adver-\\nsarial networks,” arXiv preprint arXiv:1511.06434 , 2015.\\n[13] J. Gauthier, “Conditional generative adversarial nets for\\nconvolutional face generation,” Class Project for Stanford\\nCS231N: Convolutional Neural Networks for Visual Recog-\\nnition, Winter semester , vol. 2014, no. 5, p. 2, 2014.\\n[14] P . Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-\\nto-image translation with conditional adversarial networks,”\\narXiv preprint arXiv:1611.07004 , 2016.\\n[15] P . Lucey and J. F. e. a. Cohn, “The extended cohn-kanade\\ndataset (ck+): A complete dataset for action unit and emotion-\\nspeciﬁed expression,” in CVPR Workshop . IEEE, 2010.\\n[16] G. Zhao, X. Huang, and M. e. a. Taini, “Facial expression\\nrecognition from near-infrared videos,” Image and Vision\\nComputing , vol. 29, no. 9, pp. 607–619, 2011.\\n[17] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato, “A 3d\\nfacial expression database for facial behavior research,” in\\nFG. IEEE, 2006, pp. 211–216.\\n[18] L. Yin, X. Chen, Y . Sun, T. Worm, and M. Reale, “A high-\\nresolution 3d dynamic facial expression database,” in FG.\\nIEEE, 2008, pp. 1–6.\\n[19] C. Ledig and L. e. a. Theis, “Photo-realistic single im-\\nage super-resolution using a generative adversarial network,”\\narXiv preprint , 2016.\\n[20] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses\\nfor real-time style transfer and super-resolution,” in ECCV .\\nSpringer, 2016, pp. 694–711.\\n[21] M. D. Zeiler and R. Fergus, “Visualizing and understanding\\nconvolutional networks,” in ECCV . Springer, 2014, pp. 818–\\n833.\\n[22] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convo-\\nlutional networks for biomedical image segmentation,” in\\nInternational Conference on Medical image computing and\\ncomputer-assisted intervention . Springer, 2015.\\n[23] X. Zhu and D. Ramanan, “Face detection, pose estimation,\\nand landmark localization in the wild,” in CVPR . IEEE,\\n2012, pp. 2879–2886.[24] M. Abadi, A. Agarwal, and P . e. a. Barham, “Tensorﬂow:\\nLarge-scale machine learning on heterogeneous distributed\\nsystems,” arXiv preprint arXiv:1603.04467 , 2016.\\n[25] G. Zhao and M. Pietikainen, “Dynamic texture recognition\\nusing local binary patterns with an application to facial\\nexpressions,” IEEE transactions on pattern analysis and\\nmachine intelligence , vol. 29, no. 6, pp. 915–928, 2007.\\n[26] A. Klaser, M. Marszałek, and C. Schmid, “A spatio-temporal\\ndescriptor based on 3d-gradients,” in BMVC 2008-19th British\\nMachine Vision Conference . British Machine Vision Asso-\\nciation, 2008, pp. 275–1.\\n[27] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, “Deeply\\nlearning deformable facial action parts model for dynamic\\nexpression analysis,” in Asian Conference on Computer Vi-\\nsion . Springer, 2014, pp. 143–157.\\n[28] M. Liu, S. Shan, R. Wang, and X. Chen, “Learning ex-\\npressionlets on spatio-temporal manifold for dynamic facial\\nexpression recognition,” in CVPR , 2014, pp. 1749–1756.\\n[29] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, “Joint ﬁne-tuning\\nin deep neural networks for facial expression recognition,” in\\nICCV , 2015, pp. 2983–2991.\\n[30] Y . Guo, G. Zhao, and M. Pietik ¨ainen, “Dynamic facial expres-\\nsion recognition using longitudinal facial expression atlases,”\\ninECCV . Springer, 2012, pp. 631–644.\\n[31] J. Wang, L. Yin, X. Wei, and Y . Sun, “3d facial expression\\nrecognition based on primitive surface feature distribution,”\\ninComputer Vision and Pattern Recognition, 2006 IEEE\\nComputer Society Conference on , vol. 2. IEEE, 2006, pp.\\n1399–1406.\\n[32] S. Berretti, A. Del Bimbo, P . Pala, B. B. Amor, and\\nM. Daoudi, “A set of selected sift features for 3d facial ex-\\npression recognition,” in Pattern Recognition (ICPR) . IEEE,\\n2010, pp. 4125–4128.\\n[33] X. Y ang, D. Huang, Y . Wang, and L. Chen, “Automatic\\n3d facial expression recognition using geometric scattering\\nrepresentation,” in FG, vol. 1. IEEE, 2015, pp. 1–6.\\n[34] A. T. Lopes and E. e. a. de Aguiar, “Facial expression\\nrecognition with convolutional neural networks: Coping with\\nfew data and the training sample order,” Pattern Recognition ,\\nvol. 61, pp. 610–628, 2017.\\n[35] A. Dapogny, K. Bailly, and S. Dubuisson, “Dynamic facial\\nexpression recognition by joint static and multi-time gap\\ntransition classiﬁcation,” in FG, vol. 1. IEEE, 2015.\\n[36] O. Rudovic, V . Pavlovic, and M. Pantic, “Multi-output lapla-\\ncian dynamic ordinal regression for facial expression recog-\\nnition and intensity estimation,” in CVPR . IEEE, 2012, pp.\\n2634–2641.\\n[37] A. Dapogny, K. Bailly, and S. Dubuisson, “Pairwise condi-\\ntional random forests for facial expression recognition,” in\\nICCV , 2015, pp. 3783–3791.\\n[38] L. A. Jeni, D. Takacs, and A. Lorincz, “High quality facial\\nexpression recognition in video streams using shape related\\ninformation only,” in ICCV Workshops . IEEE, 2011, pp.\\n2168–2174.\\n[39] Z. Pan, M. Polceanu, and C. Lisetti, “On constrained local\\nmodel feature normalization for facial expression recog-\\nnition,” in International Conference on Intelligent Virtual\\nAgents , 2016, pp. 369–372.\\n301\\n',\n",
       " 'Identity-Aware Convolutional Neural Network for Facial Expression\\nRecognition\\nZibo Meng*1Ping Liu*2Jie Cai1Shizhong Han1Yan Tong1\\n1Department of Computer Science and Engineering, South Carolina University, USA\\n2Sony Electronics, USA\\nAbstract — Facial expression recognition suffers under real-\\nworld conditions, especially on unseen subjects due to high\\ninter-subject variations. To alleviate variations introduced by\\npersonal attributes and achieve better facial expression recog-\\nnition performance, a novel identity-aware convolutional neural\\nnetwork (IACNN) is proposed. In particular, a CNN with a new\\narchitecture is employed as individual streams of a bi-stream\\nidentity-aware network. An expression-sensitive contrastive loss\\nis developed to measure the expression similarity to ensure the\\nfeatures learned by the network are invariant to expression\\nvariations. More importantly, an identity-sensitive contrastive\\nloss is proposed to learn identity-related information from iden-\\ntity labels to achieve identity-invariant expression recognition.\\nExtensive experiments on three public databases including a\\nspontaneous facial expression database have shown that the\\nproposed IACNN achieves promising results in real world.\\nI. I NTRODUCTION\\nFacial activity is the most powerful and natural means for\\nunderstanding emotional expression for humans. Extensive\\nefforts have been devoted to facial expression recognition in\\nthe past decades [31], [51], [36]. An automatic facial expres-\\nsion recognition system is desired in emerging applications\\nin human-computer interaction (HCI), such as online/remote\\neducation, interactive games, and intelligent transportation.\\nAlthough great progress has been made from posed or\\ndeliberate facial displays, facial expression recognition in\\nreal-world suffers from various factors including uncon-\\nstrained face pose, illumination change, and high inter-\\nsubject variations. Moreover, recognition performance usu-\\nally degrades on unseen subjects, primarily due to high inter-\\nsubject variations introduced by age, gender, and especially\\nperson-speciﬁc characteristics associated with identity as\\ndiscussed in [43]. Since these factors are nonlinearly coupled\\nwith facial expressions in a multiplicative way , features\\nextracted through existing methods are not purely related\\nto expressions. As illustrated in Fig. 1, I1andI2are the\\nsame subject displaying different expressions, whereas I1\\nandI3are different subjects displaying the same expression.\\nFor facial expression recognition, it is desired to have the\\ndistanceD1between images I1andI2larger than D2\\nbetweenI1andI3in the feature space, as in Fig. 1b.\\nHowever, due to high inter-subject variations, D1is usually\\nsmaller than D2in the feature spaces of existing approaches,\\nas in Fig. 1a. This motivates us to attack the challenging\\nproblem of learning and extracting personal-independent and\\n* indicates equal contribution.\\nSurprise\\nAnger\\n(a) (b)\\nFig. 1: Illustration for features learned by (a) existing methods,\\nand (b) the proposed IACNN. Best viewed in color.\\nexpression-discriminative features. To solve this problem,\\nwe develop an identity-aware convolutional neural network\\n(IACNN) to learn expression-related representations and, at\\nthe same time, to learn identity-related features to facilitate\\nidentity-invariant facial expression recognition.\\nIn addition to minimizing the classiﬁcation errors, a simi-\\nlarity metric for expression is developed and employed in the\\nIACNN to pull the samples with the same expression together\\nwhile pushing those with different expressions apart in the\\nfeature space. As a result, the intra-expression variations are\\nreduced, while the inter-expression differences are increased.\\nHowever, the learned expression representations may contain\\nirrelevant identity information such that the performance\\nof facial expression recognition is affected by high inter-\\nsubject variations as illustrated in Fig. 1a. To alleviate the\\neffect of the inter-subject variations, a similarity metric for\\nidentity is proposed in the IACNN to learn identity-related\\nfeatures, which will be combined with the expression-related\\nrepresentations for facial expression recognition.\\nThe architecture of the proposed IACNN is illustrated in\\nFig 2. During the training process, expression and identity\\nrelated features are jointly estimated through a deep CNN\\nframework, which is composed of two identical CNN streams\\nand trained by simultaneously minimizing the classiﬁca-\\ntion errors while maximizing the expression and identity\\nsimilarities. Speciﬁcally, given a pair of images, each of\\nwhich is fed into one CNN stream, the similarity losses are\\ncomputed using the expression and identity related features,\\nrespectively. In addition, the classiﬁcation errors in terms of\\nexpression recognition are also calculated for both images\\nand used to ﬁne-tune the model parameters to ensure the\\nlearned features are meaningful for expression recognition.\\nDuring testing, an input image is fed into one CNN stream,\\nand predictions are generated based on both the expression-\\n2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition\\n978-1-5090-4023-0/17 $31.00 © 2017 IEEE\\nDOI 10.1109/FG.2017.140558\\n2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition\\n978-1-5090-4023-0/17 $31.00 © 2017 IEEE\\nDOI 10.1109/FG.2017.140558\\n2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition\\n978-1-5090-4023-0/17 $31.00 © 2017 IEEE\\nDOI 10.1109/FG.2017.140558\\n2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition\\n978-1-5090-4023-0/17 $31.00 © 2017 IEEE\\nDOI 10.1109/FG.2017.140558\\n2017 IEEE 12th International Conference on Automatic Face & Gesture Recognition\\n978-1-5090-4023-0/17 $31.00 © 2017 IEEE\\nDOI 10.1109/FG.2017.140558\\n\\nFig. 2: The architecture of the IACNN used for training, where a pair of images are input into two identical CNNs with sharing weights,\\nrespectively. In each CNN, there are two FC layers, i.e. FCexp, andFCID, on top of the ﬁrst FC layer for learning the expression-related\\nand identity-related features, respectively. LExp\\nContrasitve /LID\\nContrasitve is a contrastive loss used to minimize the differences between the\\nsamples with the same expression/identity. With explicit expression labels, LExp\\nSoftmax andLSoftmax are employed to ensure the learned\\nfeatures are meaningful for expression recognition. Loss layers are highlighted by gray blocks. Best viewed in color.\\nrelated and the identity-related features. In summary, our\\nmain contributions in this paper are:\\n1)Developing an IACNN, which is capable of utilizing\\nboth expression-related and identity-related informa-\\ntion for facial expression recognition;\\n2)Introducing a new auxiliary layer with an identity-\\nsensitive contrastive loss to learn identity-related rep-\\nresentations to alleviate high inter-subject variations;\\n3)Proposing a joint loss function, which considers clas-\\nsiﬁcation errors of expression recognition as well as\\nexpression and identity similarities, to ﬁne tune the\\nexpression-related and identity-related features simul-\\ntaneously.\\nExtensive experiments on two well-known posed facial\\nexpression databases, i.e. Extended Cohn-Kanade database\\n(CK+) [14], [25] and MMI database [32], have demon-\\nstrated the effectiveness of the proposed method for facial\\nexpression recognition. Furthermore, the proposed method\\nwas evaluated on a spontaneous facial expression dataset,\\ni.e. Static Facial Expressions in the Wild (SFEW) [6], which\\ncontains face images with large head pose variations and\\ndifferent illuminations and has been widely used for bench-\\nmarking facial expression recognition. Experimental results\\non the SFEW dataset have shown that the proposed approach\\nachieves promising results in real world.\\nII. R ELATED WORK\\nAs elaborated in the surveys [31], [51], [36], facial expres-\\nsion recognition has been extensively studied over the past\\ndecades. Both 2D and 3D features have been extracted fromstatic images or image sequences to capture the appearance\\nand geometry facial changes caused by target expressions.\\nThe features employed can be human-designed including\\nHistograms of Oriented Gradients (HOG) [2], Scale Invariant\\nFeature Transform (SIFT) features [50], [5], histograms of\\nLocal Binary Patterns (LBP) [43], [3], histograms of Local\\nPhase Quantization (LPQ) [12], and their spatiotemporal ex-\\ntensions [18], [38], [52], [12], [47]. Other spatiotemporal ap-\\nproaches, such as temporal modeling of shapes (TMS) [10],\\ninterval temporal Bayesian network (ITBN) [46], expres-\\nsionlets on spatiotemporal manifold (STM-ExpLet) [22],\\nand spatiotemporal covariance descriptors (Cov3D) [35],\\nhave been developed to utilize both spatial and temporal\\ninformation in an image sequence.\\nHuman-crafted features can achieve high performance on\\nposed expression data. However, performance degenerates\\non spontaneous data with large variations in head pose and\\nillumination. In contrast, features can be learned in a data-\\ndriven manner by sparse coding [24], [54], [33], [28] or\\ndeep learning [34], [42], [23], [29], [20], [16], [49], [30],\\n[13], [21], [53]. Most recently, CNN-based deep learning\\napproaches [16], [49], [30] have been demonstrated to be\\nmore robust to real world conditions in EmotiW2015 [6].\\nMost of the aforementioned approaches focus on improv-\\ning person-independent recognition, while a few of them\\nlearn models from person-speciﬁc data. For example, Chen\\net al. [3] learn a person-speciﬁc model from a few person-\\nspeciﬁc samples based on transfer learning.\\nExisting approaches learn a similarity metric using either\\nhuman-designed features [17], [37] or deep metric learn-\\n559\\n559\\n559\\n559\\n559\\nFig. 3: The proposed Exp-Net for facial expression recognition includes a pair of identical component CNNs, whose weights are shared.\\nAs depicted in the dashed rectangle, the component CNN includes three convolutional layers, each of which is followed by a PReLU\\nlayer and a BN layer. A max pooling layer is employed after each of the ﬁrst two BN layers. Following the third convolutional layer, two\\nFC layers are used to generate the representation for each input sample. Finally, a softmax loss layer ( LExp\\nSoftmax ) is employed to produce\\nthe distribution over the target expressions and to calculate the classiﬁcation errors for ﬁne-tuning the parameters. A contrastive loss, i.e.\\nLExp\\nContrastive , is employed to reduce the intra-class variations, while to decrease the inter-class similarity. Loss layers are highlighted by\\ngray blocks. Best viewed in color.\\ning such as CNNs with pairwise-constraints [41], [4], [53]\\nor triplet-constraints [45]. Compared with those based on\\nhand-crafted features, deep metric learning achieves more\\npowerful representations with low intra-class yet high inter-\\nclass distances in a data driven manner and thus, has shown\\npromising results in many applications. For example, Zhao\\net al. [53] considered similarity between peak and non-\\npeak frames from the same subject to achieve invariance to\\nexpression intensity. In this work, we explicitly learn the\\nexpression-related and identity-related features to achieve\\nidentity-invariant expression recognition, where pairwise-\\nconstraints based contrastive loss is chosen as the loss func-\\ntion for learning the similarity metric for either expression\\nor identity. Unlike previous approaches, we utilize both con-\\ntrastive loss and softmax loss to learn both features jointly.\\nMore importantly, a new auxiliary layer plus a contrastive\\nloss is employed, which is responsible for learning identity-\\nrelated representations to alleviate the inter-subject variations\\nintroduced by personal attributes. The proposed framework\\nis an end-to-end system, which can be optimized via standard\\nstochastic gradient descent (SGD).\\nIII. P ROPOSED METHOD\\nA. A Component CNN\\nInstead of utilizing an off-the-shelf model, e.g.\\nAlexNet [19], a new CNN architecture is designed for the\\ncomponent network of IACNN, due to limited expression-\\nlabeled data. As shown in Fig. 3, the CNN enclosed in the\\ndashed rectangle includes three convolutional layers, each\\nof which is followed by a parametric rectiﬁed unit (PReLU)\\nlayer [8] as the activation function. As an extension of a\\nrectiﬁed linear unit (ReLU) activation function, PReLU\\nhas better ﬁtting capability than the sigmoid function or\\nhyperbolic tangent function [19] and further boosts the\\nclassiﬁcation performance as compared to the traditional\\nReLU. After each PReLU layer, a batch normalization (BN)layer [9] is utilized to normalize each scalar feature to zero\\nmean and unit variance. The BN layer has been shown\\nto improve classiﬁcation performance and accelerate the\\ntraining process [9]. After each of the ﬁrst two BN layers,\\nthere is a max pooling layer. Following the third PReLU\\nlayer, two fully connected (FC) layers consisting of 1,024\\nneurons are employed. Finally, a softmax loss layer is used\\nto generate the probabilistic distribution over the Ktarget\\nexpressions and to calculate the classiﬁcation loss given the\\nexpression labels.\\nB. An Exp-Net for Facial Expression Recognition\\nIn real-world scenarios, facial expression recognition suf-\\nfers from intra-expression variations. As a result, CNNs can\\ngenerate quite different representations for image samples\\ncontaining the same expression. To cope with the problem,\\nan Exp-Net composed of two identical component CNNs is\\nconstructed, as illustrated in Fig. 3. Two softmax losses and\\none contrastive loss are employed to learn representations\\nthat are meaningful for facial expression recognition. Given\\nthe expression labels, a softmax loss is employed on top of\\neach component network to calculate the classiﬁcation errors\\nand is used to ﬁne tune the parameters in the lower layers.\\nIn addition, a contrastive loss is utilized to learn a similarity\\nmetric for image pairs to make sure that the samples with\\nthe same expression have similar representations, and at the\\nsame time, those with different expressions are far away in\\nthe feature space.\\n1) Expression-Sensitive Contrastive Loss: As illustrated\\nin Fig. 4, an expression-sensitive contrastive loss is designed\\nto pull the samples with the same expression towards each\\nother, and to push the samples with different expressions\\naway from each other at the same time. Speciﬁcally, the\\nsimilarity between two images I1andI2is deﬁned as the\\nsquared Euclidean distance in the feature space:\\nD\\x10\\nfE(I1);fE(I2)\\x11\\n=kfE(I1)\\x00fE(I2)k2\\n2 (1)\\n560\\n560\\n560\\n560\\n560\\nSurprise (I1)CNN\\nCNNShared\\nWeightsFeature Space\\nf(I1)\\nf(I2)Lp\\nSurprise (I2)(a)\\nFeature Space\\nSurprise (I1)\\nSadness (I2)CNN\\nCNNShared\\nWeightsf(I1)\\nf(I2)LnD(f(I1),f(I2)<α (b)\\nFig. 4: During the training process, the contrastive loss (a) pulls the samples with the same expression towards each other, and (b) pushes\\nthe samples with different expressions apart. LpandLnrepresent the contrastive losses for the image pairs with the same expression and\\ndifferent expressions, respectively.\\nwherefE(\\x01)is the expression-related image representation,\\ni.e., the output of the FCexplayer in Fig. 3. A smaller\\ndistanceD\\x10\\nf(I1);f(I2)\\x11\\nindicates that the two images are\\nmore similar in the feature space.\\nThen, the expression-sensitive contrastive loss is deﬁned\\nas follows:\\nLexp\\nContrastive=MX\\ni=1LE\\x10\\nzE\\ni;fE(Ii;1);fE(Ii;2)\\x11\\n(2)\\nLE\\x10\\nzE\\ni;fE(Ii;1);fE(Ii;2)\\x11\\n=zE\\ni\\n2\\x03D\\x10\\nfE(Ii;1);fE(Ii;2)\\x11\\n+1\\x00zE\\ni\\n2\\x03max\\x10\\n0;\\x0bE\\x00D\\x00\\nfE(Ii;1);fE(Ii;2)\\x01\\x11\\n(3)\\nwhereM is the number of image pairs; and \\x10\\nzE\\ni;fE(Ii;1);fE(Ii;2)\\x11\\nrepresents the label and the\\nexpression-related image features for the ithpair of training\\nsamples, respectively. When the pair of images has the same\\nexpression label, zE\\ni= 1 and the loss is\\nLE\\x10\\n1;fE(Ii;1);fE(Ii;2)\\x11\\n=1\\n2D\\x10\\nfE(Ii;1);fE(Ii;2)\\x11\\n: (4)\\nOtherwise,zE\\ni= 0 and the loss becomes\\nLE\\x10\\n0;fE(Ii;1);fE(Ii;2)\\x11\\n=1\\n2max\\x10\\n0;\\x0bE\\x00D\\x00\\nfE(Ii;1);fE(Ii;2)\\x01\\x11\\n(5)\\nwhere\\x0bE>0is a parameter to determine how much\\ndissimilar pairs contribute to the loss function. When the\\ndistance between two dissimilar samples in a pair is less\\nthan\\x0bE, then the loss will be calculated. In our experiment,\\n\\x0bEis set to 10 for the expression-sensitive contrastive loss,\\nempirically.\\nC. The IACNN for Facial Expression Recognition\\nThe performance of facial expression recognition often\\ndrops signiﬁcantly for unseen subjects, mainly due to high\\ninter-subject variations introduced by age, gender, and espe-\\ncially person-speciﬁc characteristics associated with identity.\\nTo deal with this problem, an IACNN is proposed by\\nintroducing an auxiliary FC layer into the Exp-Net, which\\ntakes the input from the lower layers of the Exp-Net, but has\\nits own set of learnable weights.\\nAs shown in Fig. 2, the FCIDlayer is on top of the\\nﬁrst FC layer of the Exp-Net after the convolutional layers.\\nGiven the identity labels, the FCIDis responsible forlearning identity-related features using an identity-sensitive\\ncontrastive lossLID\\nContrastive , deﬁned as follows\\nLID\\nContrastive =MX\\ni=1LID\\x12\\nzID\\ni;fID(Ii;1);fID(Ii;2)\\x13\\n(6)\\nwherefID(\\x01)is the identity-related feature, i.e., the output\\nof theFCIDlayer; andLID\\x10\\nzID\\ni;fID(Ii;1);fID(Ii;2)\\x11\\nis\\ndeﬁned similar to the expression-sensitive contrastive loss as\\nbelow:\\nLID\\x10\\nzID\\ni;fID(Ii;1);fID(Ii;2)\\x11\\n=zID\\ni\\n2\\x03D\\x10\\nfID(Ii;1);fID(Ii;2)\\x11\\n+1\\x00zID\\ni\\n2\\x03max\\x10\\n0;\\x0bID\\x00D\\x00\\nfID(Ii;1);fID(Ii;2)\\x01\\x11\\n(7)\\nwherezID\\ni= 1, if the pair of images comes from the same\\nsubject; otherwise, zID\\ni= 0.\\x0bIDis set to 10 for the identity-\\nsensitive contrastive loss, empirically.\\nThe identity-related features are then concatenated with\\nthe expression-related features encoded by the FCexplayer\\nto form the ﬁnal feature vector FCfeat for facial expres-\\nsion recognition. Therefore, the overall loss function of the\\nproposed IACNN is deﬁned as\\nL=\\x151LExp\\nContrastive+\\x152LID\\nContrastive +\\x153L1\\nSoftmax\\n+\\x154L2\\nSoftmax +\\x155LExp 1\\nSoftmax+\\x156LExp 2\\nSoftmax(8)\\nwhere\\x151-\\x156are the weights of each loss, respectively.\\nLExp\\nContrastive andLID\\nContrastive are the expression-sensitive\\ncontrastive loss and the identity-sensitive contrastive loss,\\nas deﬁned in Eq. 2 and Eq. 6, respectively. LExp1\\nSoftmaxand\\nLExp2\\nSoftmaxrepresent the classiﬁcation errors using only the\\nexpression-related features from the component CNN; while\\nL1\\nSoftmax andL2\\nSoftmax represent the classiﬁcation errors\\nusing the concatenated feature vector FCfeat.\\nThe overall loss is back-propagated to both FCexpand\\nFCID. As a result, the expression-related and identity-\\nrelated features are ﬁne-tuned jointly in the IACNN.\\nDuring the testing, only one stream, i.e., the component\\nCNN, is employed for making the decision. Given a testing\\nsampleIi, the expression and identity related representations,\\ni.e.fE(Ii)andfID(Ii)are calculated and concatenated to\\nconstruct the ﬁnal feature vector, i.e. FCfeat, for facial ex-\\npression recognition. In this work, classiﬁcation is performed\\nusing the softmax classiﬁer of the CNN.\\n561\\n561\\n561\\n561\\n561\\nIV. E XPERIMENTS\\nTo demonstrate the effectiveness of the proposed method\\nin terms of facial expression recognition, extensive ex-\\nperiments have been conducted on three public databases\\nincluding two posed facial expression databases, i.e. the CK+\\ndatabase [14], [25] and the MMI database [32], and more\\nimportantly, a spontaneous facial expression database, i.e.,\\nthe SFEW datasets [6].\\nA. Preprocessing\\nFace alignment is conducted to reduce variation in face\\nscale and in-plane rotation across different facial images.\\nSpeciﬁcally, 66landmarks are detected using a state-of-\\nthe-art face alignment method, i.e., Discriminative Response\\nMap Fitting (DRMF) [1]. The face regions are aligned based\\non three ﬁducial points: the centers of the eyes and the\\nmouth, and then are cropped and scaled to a size of 60\\x0260.\\nIt may be not sufﬁcient to learn a deep model with\\nthe limited number of sequences or images in the facial\\nexpression databases. To alleviate the chance of over-ﬁtting,\\nan augmentation procedure is employed to train the CNN\\nmodels, where a 48\\x0248patch is randomly cropped from an\\nimage and randomly ﬂipped horizontally as the input of the\\nCNN, resulting 288 times larger than the original training\\ndata. During testing, only the 48\\x0248patch centered at the\\nface image is used as the input to one stream of the IACNN.\\nB. Implementation Details\\nThe proposed component CNN is ﬁne-tuned from a\\nCNN model pretrained on the Facial Expression Recognition\\n(FER-2013) dataset [7] using stochastic gradient decent with\\na batch size of 128, momentum of 0:9, and a weight decay\\nparameter of 0:005. Dropout is applied to each FC layer\\nwith a probability of 0:6, i.e. zeroing out the output of a\\nneuron with probability of 0:6. In Eq. 8,\\x152is set to 5 for\\nCK+/SFEW and 2 for MMI, while other parameters are set\\nto 1 for all datasets empirically. The CNNs are implemented\\nusing the Caffe library [11].\\nIdentity information is required to train the IACNN model.\\nLabeled subject IDs are provided in the CK+ and MMI\\ndatabases and we manually labeled subject IDs for the SFEW\\ndatabase. In practice, the proposed system only requires\\nweakly supervised identity information, i.e., whether the two\\nimages are from the same subject, which can be automati-\\ncally obtained by an off-the-shelf face veriﬁcation method.\\nC. Experimental Results\\nTo better demonstrate the effectiveness of the proposed\\nmodel, two baseline methods are employed, i.e. the one-\\nstream component CNN described in Section III-A, denoted\\nbyCNN , and the Exp-Net introduced in Section III-B,\\ndenoted byExp\\x00Net.\\n1) Results on CK+ dataset: CK+ database [14], [25]\\nis widely used for evaluating facial expression recognition\\nsystem. It contains 327 image sequences collected from 118\\nsubjects, each of which is labeled as one of 7 expressions,\\ni.e. anger, contempt, disgust, fear, happiness, sadness, and\\nsurprise. For each sequence, the label is only provided forTABLE I: Confusion matrix of the proposed IACNN method\\nevaluated on the CK+ database [14], [25]. The ground truth and\\nthe predicted labels are given by the ﬁrst column and the ﬁrst row,\\nrespectively.\\nAn Co Di Fe Ha Sa Su\\nAn 91.1% 0% 0% 1.1% 0% 7.8% 0%\\nCo 5.6% 86.1% 0% 2.7% 0% 5.6% 0%\\nDi 0% 0% 100% 0% 0% 0% 0%\\nFe 0% 4% 0% 98% 2% 0% 8%\\nHa 0% 0% 0% 0% 100% 0% 0%\\nSa 3.6% 0% 0% 1.8% 0% 94.6% 0%\\nSu 0% 1.2% 0% 0% 0% 0% 98.8%\\nTABLE II: Performance comparison on the CK+ database [14],\\n[25] in terms of the average accuracy of 7 expressions.\\nMethod Accuracy\\n3DCNN [21] 85.9\\nMSR [33] 91.4\\nHOG 3D [18] 91.44\\nTMS [10] 91.89\\nCov3D [35] 92.3\\n3DCNN-DAP [21] 92.4\\nSTM-ExpLet [22] 94.19\\nDTAGN [13] 97.25\\nCNN (baseline) 89.30\\nExp-Net (baseline) 92.81\\nIACNN 95.37\\nthe last frame (the peak frame). To collect more data, the\\nlast three frames of each sequence are selected as peak\\nframes associated with the provided expression label. Thus,\\nan experimental database consisting of 981 images is built.\\nThe database is further divided into 8 subsets, where the\\nsubjects in any two subsets are mutually exclusive. Then\\nan 8-fold cross-validation strategy is employed, where, for\\neach run, data from 6 subsets are used for training and that\\nfrom the remaining two subsets for validation and testing,\\nrespectively.\\nThe proposed IACNN and the two baseline methods are\\ntrained and tested on static images. The ﬁnal sequence-\\nlevel predictions are obtained by choosing the class with\\nthe highest average score of the three images. The results\\nare reported as the average of the 8 runs. The confusion\\nmatrix of the proposed IACNN model is reported in Table I,\\nwhere diagonal entries represent the recognition accuracy for\\neach expression. As shown in Table II, the performance of\\nthe proposed IACNN outperforms the two baseline methods,\\nespecially the one-stream component CNN, in terms of the\\naverage accuracy of the 7 expressions. The IACNN model\\nis also compared with the state-of-the-art methods evaluated\\non the CK+ database including methods using human crafted\\nfeatures (HOG 3D [18], TMS [10], Cov3D [35], and STM-\\nExpLet [22]), methods using sparse coding (MSR [33]), and\\nCNN-based methods (3DCNN and 3DCNN-DAP [21] and\\nDTAGN [13]). As shown in Table II, the IACNN outperforms\\nthe methods based on human crafted features or sparse\\ncoding and also performs better or is at least comparable\\nto the CNN-based methods. Note that all these methods\\nexcept the MSR employed temporal information extracted\\nfrom image sequences. In contrast, the proposed IACNN\\nlearns and extracts features from static images, which is more\\n562\\n562\\n562\\n562\\n562\\nAnger\\nContempt\\nDisgust\\nFear\\nHappy\\nSad\\nSurprise\\n(a) Original images (b) Features learned by CNN (c) Features learned by Exp-Net (d) Features learned by IACNNSubject504\\nSubject504Subject504\\nSubject504Subject504\\nCentroidCentroid\\nCentroid\\nCentroidCentroid\\nCentroidFig. 5: A visualization study of (a) the original raw images and the features learned by (b) the component CNN, (c) the Exp-Net, and (d)\\nthe IACNN model on the CK+ database. The number of samples is 981including 247\\x023training data from 6 subsets, 38\\x023validation\\ndata, and 42\\x023testing data. The dots, stars, and diamonds represent training, validation, and testing data, respectively. The features\\nlearned by the IACNN are better separated according to expressions for both the validation and testing data. Best viewed in color.\\nTABLE III: Confusion matrix of the proposed IACNN method\\nevaluated on the MMI database [32]. The ground truth and the\\npredicted labels are given by the ﬁrst column and the ﬁrst row,\\nrespectively.\\nAn Di Fe Ha Sa Su\\nAn 81.8% 3% 3% 1.5% 10.6% 0%\\nDi 10.9% 71.9% 3.1% 4.7% 9.4% 6%\\nFe 5.4% 8.9% 41.1% 7.1% 7.1% 30.4%\\nHa 1.1% 3.6% 0% 92.9% 2.4% 0%\\nSa 17.2% 7.8% 0% 1.6% 73.4% 0%\\nSu 7.3% 0% 14.6% 1.2% 0% 76.9%\\nsuitable for applications, where videos or image sequences\\nare not available.\\nVisualization Study: To further demonstrate the effective-\\nness in terms of learning good representations for expres-\\nsion recognition, we visualize the features learned by the\\ncomponent CNN, the Exp-Net, and the IACNN, respectively,\\nusing t-SNE [44], which is widely employed to visualize high\\ndimensional data. As shown in Fig. 5a, the training samples\\nare denoted by dots, validation samples denoted by stars,\\nand testing samples denoted by diamonds. As illustrated in\\nFig. 5a, the original raw images are randomly distributed. In\\ncontrast, the learned features (Fig. 5b, c, and d) are clustered\\nbased on their expression labels. A close-up enclosed by\\nthe rectangle is given for Fig. 5b, c, and d. Comparing\\nFig. 5b (the component CNN) with Fig. 5c (Exp-Net) and d\\n(IACNN), the samples of the same subject with different\\nexpressions are closer to each other rather than to their\\ncorresponding cluster centers marked by magenta circles. As\\ncompared to the features learned by the component CNN\\nand the Exp-Net, the proposed IACNN model yields a better\\nseparation of the features, as depicted in Fig. 5d.\\n2) Results on MMI dataset: The MMI dataset [32] con-\\nsists of 213 image sequences, among which 208 sequences\\ncontaining frontal-view faces of 31 subjects will be used in\\nour experiment. Each sequence is labeled as one of six basic\\nexpressions, i.e. anger, disgust, fear, happiness, sadness, and\\nsurprise. Starting with a neutral expression, each sequence\\ndevelops the facial expression as time goes on, reaches peak\\nnear the middle of the sequence, and ends with a neutral\\nexpression. Since the actual location of the peak frame is not\\nprovided, three frames in the middle of each image sequence\\nare collected as peak frames and associated with the providedTABLE IV: Performance comparison on the MMI database [32]\\nin terms of the average accuracy of 6 expressions.\\nMethod Accuracy\\n3DCNN [21] 53.2\\nITBN [46] 59.7\\nHOG 3D [18] 60.89\\n3DCNN-DAP [21] 63.4\\n3D SIFT [38] 64.39\\nDTAGN [13] 70.24\\nSTM-ExpLet [22] 75.12\\nCNN (baseline) 57.00\\nExp-Net (baseline) 64.31\\nIACNN 69.48\\nCNNCK(baseline trained from MMI+CK+) 65.17\\nExp-NetCK(baseline trained from MMI+CK+) 70.55\\nIACNNCK(trained from MMI+CK+) 71.55\\nexpression labels. Hence, there are a total of 208\\x023images\\nused in our experiments.\\nSimilar to that on the CK+ database, the proposed IACNN\\nand the two baseline methods are trained and tested on static\\nimages and the ﬁnal sequence-level predictions are made by\\nselecting the class with the highest average score of the three\\nimages. The dataset is divided into 10 subsets for person-\\nindependent 10-fold cross validation, where, for each run,\\ndata from 8 subsets are used for training and those from\\nthe remaining 2 subsets are used for validation and testing,\\nrespectively. The results are reported as the average of 10\\nruns. The confusion matrix of the proposed IACNN model\\nevaluated on the MMI dataset is reported in Table III.\\nAs shown in Table IV, the proposed IACNN outperforms\\nthe two baseline methods signiﬁcantly. Furthermore, the\\nIACNN also outperforms most of the state-of-the-art meth-\\nods. Note that the image sequences in the MMI database\\ncontain a full temporal pattern of expressions, i.e., from\\nneutral to apex, and then released, and are especially favored\\nby these methods exploiting temporal information, e.g., all\\nthe state-of-art methods in comparison.\\nSince the MMI dataset contains a small number of sam-\\nples, i.e. only 624 face images, it is not large enough to train\\na deep model. To demonstrate that the IACNN can achieve\\nbetter performance given more training data, we employed\\nadditional data of the six basic expressions from the CK+\\ndataset. Speciﬁcally, for each run, the 8 subsets of the MMI\\ndataset plus the data from the CK+ dataset will be used as\\n563\\n563\\n563\\n563\\n563\\nTABLE V: Confusion matrix of the proposed IACNN method\\nevaluated on the SFEW [6] validation set. The ground truth and\\nthe predicted labels are given by the ﬁrst column and the ﬁrst row,\\nrespectively.\\nAn Di Fe Ha Ne Sa Su\\nAn 70.7% 0% 2.7% 5.2% 6.7% 8% 6.7%\\nDi 19.1% 0% 0% 4.8% 33.3% 38% 4.8%\\nFe 37.8% 0% 8.9% 11.1% 15.6% 13.3% 13.3%\\nHa 9.9% 0% 0% 70.4% 9.9% 8.4% 1.4%\\nNe 5.1% 0% 2.6% 1.3% 60.3% 24.3% 6.4%\\nSa 7.4% 0% 2.9% 5.9% 20.6% 58.8% 4.4%\\nSu 23.1% 0% 3.8% 3.8% 28.9% 11.5% 28.9%\\nTABLE VI: Confusion matrix of the proposed IACNN method\\nevaluated on the SFEW [6] testing set. The ground truth and the\\npredicted labels are given by the ﬁrst column and the ﬁrst row,\\nrespectively.\\nAn Di Fe Ha Ne Sa Su\\nAn 79.8% 0% 1.4% 0% 8.7% 1.4% 8.7%\\nDi 35.3% 0% 0% 29.4% 11.8% 23.5% 0%\\nFe 34.1% 0% 9.8% 12.2% 9.8% 7.3% 26.8%\\nHa 8.4% 0% 0% 75.8% 4.2% 7.4% 4.2%\\nNe 17.2% 0% 1.7% 3.5% 60.4% 10.3% 6.9%\\nSa 23.6% 0% 7.3% 20% 5.5% 34.6% 9%\\nSu 24.3% 0% 2.7% 8.1% 10.8% 8.1% 46%\\ntraining set and the remaining two subsets of the MMI dataset\\nare used as the validation and testing sets, respectively.\\nThe results are reported at the bottom of Table IV denoted\\nas CNNCK, CNNCK, and IACNNCKfor the one-stream\\nCNN, the Exp-Net, and the proposed IACNN, respectively.\\nWith additional training data, the recognition accuracy of the\\nIACNN further improves from 69:48to71:55.\\n3) Results on SFEW dataset: The SFEW [6] is the most\\nwidely used benchmark for facial expression recognition,\\nwhich “targets the efforts required towards affect analysis\\nin the wild” [6]. The SFEW database is composed of 1,766\\nimages, i.e. 958 for training, 436 for validation, and 372 for\\ntesting. Each of the images has been assigned to one of seven\\nexpression categories, i.e., anger, disgust, fear, neutral, happy,\\nsad, and surprise. The expression labels of the training and\\nvalidation sets are provided, while those of the testing set is\\nheld back by the challenge organizer.\\nAs illustrated in Table VII, Kim et al. [16], Yu et al. [49],\\nNg et al. [30], and Yao et al. [48] are ranked at the 1st\\nto4thamong the 18 teams in the EmotiW2015 challenge,\\nrespectively. Note that the ﬁrst two methods, i.e. Kim et\\nal. [16] and Yu et al. [49], used an ensemble of CNNs.\\nFor example, Kim et al. [16] employed an ensemble of\\nCNNs with different architectures, which can boost the ﬁnal\\nperformance. The proposed IACNN model outperforms the\\nbaseline of SFEW ( 35:93on the validation set and 39:13on\\nthe testing set) by a large margin. The proposed IACNN is\\nranked at the 5thplace on both the validation set and the\\ntesting set among all the methods compared and achieves\\ncomparable performance with Ng et al. [30] and Yao et\\nal. [48], which demonstrates the effectiveness of the IACNN\\nmodel in the real world.\\n4) Cross-database validation: To further demonstrate that\\nthe proposed IACNN is less affected by identity, a cross-TABLE VII: Performance comparison on the SFEW database [6]\\nin terms of the average accuracy of 7 expressions.\\nMethod Validation Set Test Set\\nKim et al. [16] 53.9 61.6\\nYu et al. [49] 55.96 61.29\\nNg et al. [30] 48.5 55.6\\nYao et al. [48] 43.58 55.38\\nSun et al. [40] 51.02 51.08\\nZong et al. [55] N/A 50\\nKaya et al. [15] 53.06 49.46\\nMollahosseini et al. [29] 47.7 N/A\\nDhall et al. [6] (baseline of SFEW) 35.93 39.13\\nCNN (baseline) 47.80 50.54\\nExp-Net 49.51 52.96\\nIACNN 50.98 54.30\\nTABLE VIII: Cross-database facial expression recognition perfor-\\nmance in terms of average accuracy.\\nTest Set [26] [39] [27] CNN IACNN\\nCK+ 47.1 - 56 69.26 71.29\\nMMI 51.4 50.8 36.8 54.87 55.41\\ndatabase experiment was conducted. The classiﬁers trained\\non CK+ and MMI in the previous experiments are directly\\napplied to MMI and CK+, respectively. The results are\\nreported as the average of 10 runs for CK+ and 8 runs\\nfor MMI. As shown in Table VIII, the proposed IACNN\\noutperforms all the state-of-the-art methods as well as the\\nbaseline CNN method.\\nV. C ONCLUSION\\nIn this work, we proposed a novel identity-aware CNN\\nto capture both expression-related and identity-related infor-\\nmation to alleviate the effect of personal attributes on facial\\nexpression recognition. Speciﬁcally, a softmax loss combined\\nwith an expression-sensitive contrastive loss is utilized to\\nlearn the expression-related representations. To alleviate the\\nvariations introduced by different identities, a new auxiliary\\nlayer with an identity-sensitive contrastive loss is employed\\nto learn identity-related representations. Both the expression-\\nrelated and identity-related features are concatenated and\\nemployed to achieve an identity-invariant facial expression\\nrecognition.\\nExperimental results on two posed facial expression\\ndatasets have demonstrated that the proposed IACNN model\\noutperformed the baseline CNN methods as well as most of\\nthe state-of-the-art methods that exploit dynamic information\\nextracted from image sequences. More importantly, IACNN\\nhas shown promise on a spontaneous facial expression\\ndataset, which demonstrates its effectiveness in real world.\\nIn the future, we plan to recognize face expressions from\\nvideos by incorporating temporal information in the proposed\\nIACNN model.\\nVI. A CKNOWLEDGEMENT\\nThis work is supported by National Science Foundation\\nunder CAREER Award IIS-1149787.\\nREFERENCES\\n[1]A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Robust discrimi-\\nnative response map ﬁtting with constrained local models. In CVPR ,\\npages 3444–3451, 2013.\\n564\\n564\\n564\\n564\\n564\\n[2]T. Baltrusaitis, M. Mahmoud, and P. Robinson. Cross-dataset learning\\nand person-speciﬁc normalisation for automatic action unit detection.\\nInFG, volume 6, pages 1–6, 2015.\\n[3]J. Chen, X. Liu, P. Tu, and A. Aragones. Learning person-speciﬁc\\nmodels for facial expression and action unit recognition. Pattern\\nRecognition Letters , 34(15):1964–1970, 2013.\\n[4]S. Chopra, R. Hadsell, and Y . LeCun. Learning a similarity metric\\ndiscriminatively, with application to face veriﬁcation. In CVPR ,\\nvolume 1, pages 539–546, 2005.\\n[5]W. Chu, F. De la Torre, and J. Cohn. Selective transfer machine for\\npersonalized facial expression analysis. IEEE T-PAMI , 2016.\\n[6]A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and T. Gedeon.\\nVideo and image based emotion recognition challenges in the wild:\\nEmotiw 2015. In ICMI , pages 423–426. ACM, 2015.\\n[7]I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza,\\nB. Hamner, W. Cukierski, Y . Tang, D. Thaler, D. Lee, et al. Challenges\\nin representation learning: A report on three machine learning contests.\\nInICML , pages 117–124. Springer, 2013.\\n[8]K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation. In\\nICCV , pages 1026–1034, 2015.\\n[9]S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. In ICML , pages\\n448–456, 2015.\\n[10]S. Jain, C. Hu, and J. K. Aggarwal. Facial expression recognition with\\ntemporal modeling of shapes. In ICCV Workshops , pages 1642–1649,\\n2011.\\n[11]Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\\nfast feature embedding. In ACM MM , pages 675–678. ACM, 2014.\\n[12]B. Jiang, M. Valstar, and M. Pantic. Action unit detection using sparse\\nappearance descriptors in space-time video volumes. In FG, 2011.\\n[13]H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint ﬁne-tuning in deep\\nneural networks for facial expression recognition. In ICCV , pages\\n2983–2991, 2015.\\n[14]T. Kanade, J. F. Cohn, and Y . Tian. Comprehensive database for facial\\nexpression analysis. In FG, pages 46–53, 2000.\\n[15]H. Kaya, F. G ¨urpinar, S. Afshar, and A. A. Salah. Contrasting and\\ncombining least squares based learners for emotion recognition in the\\nwild. In ICMI , pages 459–466, 2015.\\n[16]B.-K. Kim, H. Lee, J. Roh, and S.-Y . Lee. Hierarchical committee of\\ndeep cnns with exponentially-weighted decision fusion for static facial\\nexpression recognition. In ICMI , pages 427–434, 2015.\\n[17]J. J. Kivinen and C. K. Williams. Transformation equivariant boltz-\\nmann machines. In IACNN , pages 1–9. Springer, 2011.\\n[18]A. Klaser, M. Marszałek, and C. Schmid. A spatio-temporal descriptor\\nbased on 3d-gradients. In BMVC , pages 275–1, 2008.\\n[19]A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. In NIPS , pages 1097–1105,\\n2012.\\n[20]M. Liu, S. Li, S. Shan, and X. Chen. AU-aware deep networks for\\nfacial expression recognition. In FG, pages 1–6, 2013.\\n[21]M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning\\ndeformable facial action parts model for dynamic expression analysis.\\nInACCV , 2014.\\n[22]M. Liu, S. Shan, R. Wang, and X. Chen. Learning expressionlets on\\nspatio-temporal manifold for dynamic facial expression recognition.\\nInCVPR , pages 1749–1756, 2014.\\n[23]P. Liu, S. Han, Z. Meng, and Y . Tong. Facial expression recognition\\nvia a boosted deep belief network. In CVPR , pages 1805–1812, 2014.\\n[24]P. Liu, S. Han, and Y . Tong. Improving facial expression analysis using\\nhistograms of log-transformed nonnegative sparse representation with\\na spatial pyramid structure. In FG, pages 1–7, 2013.\\n[25]P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews. The extended cohn-kanade dataset (ck+): A complete\\nexpression dataset for action unit and emotion-speciﬁed expression.\\nInCVPR Workshops , pages 94–101, 2010.\\n[26]C. Mayer, M. Eggers, and B. Radig. Cross-database evaluation for\\nfacial expression recognition. Pattern recognition and image analysis ,\\n24(1):124–132, 2014.\\n[27]Y . Miao, R. Araujo, and M. S. Kamel. Cross-domain facial expression\\nrecognition using supervised kernel mean matching. In ICMLA ,\\nvolume 2, pages 326–332, 2012.\\n[28]M. R. Mohammadi, E. Fatemizadeh, and M. H. Mahoor. Simultaneous\\nrecognition of facial expression and identity via sparse representation.\\nInWACV , pages 1066–1073, 2014.\\n[29]A. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper in facial\\nexpression recognition using deep neural networks. In WACV , 2015.[30]H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler. Deep learning\\nfor emotion recognition on small datasets using transfer learning. In\\nICMI , pages 443–449, 2015.\\n[31]M. Pantic, A. Pentland, A. Nijholt, and T. S. Huang. Human computing\\nand machine understanding of human behavior: A survey. In T. S.\\nHuang, A. Nijholt, M. Pantic, and A. Pentland, editors, Artiﬁcial\\nIntelligence for Human Computing , LNAI. Springer Verlag, London,\\n2007.\\n[32]M. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web-based\\ndatabase for facial expression analysis. In ICME , pages 5–pp. IEEE,\\n2005.\\n[33]R. Ptucha, G. Tsagkatakis, and A. Savakis. Manifold based sparse\\nrepresentation for robust expression recognition without neutral sub-\\ntraction. In ICCV Workshops , pages 2136–2143, 2011.\\n[34]S. Rifai, Y . Bengio, A. Courville, P. Vincent, and M. Mirza. Disentan-\\ngling factors of variation for facial expression recognition. In ECCV ,\\npages 808–822, 2012.\\n[35]A. Sanin, C. Sanderson, M. T. Harandi, and B. C. Lovell. Spatio-\\ntemporal covariance descriptors for action and gesture recognition. In\\nWACV , pages 103–110, 2013.\\n[36]E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic analysis of\\nfacial affect: A survey of registration, representation and recognition.\\nIEEE T-PAMI , 37(6):1113–1133, 2015.\\n[37]U. Schmidt and S. Roth. Learning rotation-aware features: From\\ninvariant priors to equivariant descriptors. In CVPR , pages 2050–2057,\\n2012.\\n[38]P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descriptor\\nand its application to action recognition. In ACM MM , pages 357–360,\\n2007.\\n[39]C. Shan, S. Gong, and P. McOwan. Facial expression recognition based\\non Local Binary Patterns: A comprehensive study. J. IVC , 27(6):803–\\n816, 2009.\\n[40]B. Sun, L. Li, G. Zhou, X. Wu, J. He, L. Yu, D. Li, and Q. Wei.\\nCombining multimodal features within a fusion network for emotion\\nrecognition in the wild. In ICMI , pages 497–502, 2015.\\n[41]Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing\\nthe gap to human-level performance in face veriﬁcation. In CVPR ,\\npages 1701–1708, 2014.\\n[42]Y . Tang. Deep learning using linear support vector machines. In ICML ,\\n2013.\\n[43]M. F. Valstar, M. Mehu, B. Jiang, M. Pantic, and K. Scherer. Meta-\\nanalysis of the ﬁrst facial expression recognition challenge. IEEE\\nT-SMC-B , 42(4):966–979, 2012.\\n[44]L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR ,\\n9(2579-2605):85, 2008.\\n[45]J. Wang, Y . Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\\nB. Chen, and Y . Wu. Learning ﬁne-grained image similarity with\\ndeep ranking. In CVPR , pages 1386–1393, 2014.\\n[46]Z. Wang, S. Wang, and Q. Ji. Capturing complex spatio-temporal\\nrelations among facial muscles for facial expression recognition. In\\nCVPR , pages 3422–3429, 2013.\\n[47]P. Yang, Q. Liu, and D. N. Metaxas. Boosting encoded dynamic\\nfeatures for facial expression recognition. Pattern Recognition Letters ,\\n30(2):132–139, Jan. 2009.\\n[48]A. Yao, J. Shao, N. Ma, and Y . Chen. Capturing AU-aware facial\\nfeatures and their latent relations for emotion recognition in the wild.\\nInICMI , pages 451–458, 2015.\\n[49]Z. Yu and C. Zhang. Image based static facial expression recognition\\nwith multiple deep network learning. In ICMI , pages 435–442, 2015.\\n[50]A. Yuce, H. Gao, and J. Thiran. Discriminant multi-label manifold\\nembedding for facial action unit detection. In FG, 2015.\\n[51]Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A survey of affect\\nrecognition methods: Audio, visual, and spontaneous expressions.\\nIEEE T-PAMI , 31(1):39–58, Jan. 2009.\\n[52]G. Zhao and M. Pieti ¨ainen. Dynamic texture recognition using local\\nbinary patterns with an application to facial expressions. IEEE T-PAMI ,\\n29(6):915–928, June 2007.\\n[53]X. Zhao, X. Liang, L. Liu, T. Li, Y . Han, N. Vasconcelos, and S. Yan.\\nPeak-piloted deep network for facial expression recognition. In ECCV ,\\npages 425–442. Springer, 2016.\\n[54]L. Zhong, Q. Liu, P. Yang, J. Huang, and D. Metaxas. Learning\\nmultiscale active facial patches for expression analysis. IEEE Trans.\\non Cybernetics , 45(8):1499–1510, August 2015.\\n[55]Y . Zong, W. Zheng, X. Huang, J. Yan, and T. Zhang. Transductive\\ntransfer lda with riesz-based volume lbp for emotion recognition in\\nthe wild. In ICMI , pages 491–496, 2015.\\n565\\n565\\n565\\n565\\n565\\n',\n",
       " 'Image based Facial Micro-Expression Recognition \\nusing Deep Learning on Small Datasets \\nMadhumita A. Takalkar1, Min Xu2 \\n1,2School of Electrical and Data Engineering \\nUniversity of Technology Sydney \\n15 Broadway, Ultimo, NSW 2007, Australia \\n1madhumita.a.takalkar@student.uts.edu.au, 2min.xu@uts.edu.au\\n \\n \\nAbstract —Facial micro-expression refers to split-second \\nmuscle changes in the face, indicating that a person is either \\nconsciously or unconsciously suppressing their true emotions and \\neven mental health. Therefore, micro-expression recognition \\nattracts increasing research efforts in both fields of psychology \\nand computer vision. Existing research on micro-expression \\nrecognition has mainly used hand-crafted features, for example, \\nLocal Binary Pattern-Three Orthogonal Planes (LBP-TOP), \\nGabor filter and optical flow. Recently, Deep Convolutional \\nneural systems have demonstrated a high degree effectiveness for \\ndifficult face recognition tasks. This paper explores the possible \\nuse of deep learning for micro-expression recognition. To develop \\na reliable deep neural network extensive training sets are \\nrequired with a huge number of labeled image samples. However, \\nmicro-expression recognition is a challenging task due to the \\nrepressed facial appearance and short duration, which results in \\nthe lack of training data. In this paper, we propose to generate \\nextensive training datasets of synthetic images using data \\naugmentation on CASME and CASME II databases. Then, these \\ndatasets are combined to tune a satisfactory CNN-based micro-\\nexpression recognizer. Experimental results demonstrate the \\neffectiveness of the proposed CNN approach in image based \\nmicro-expression recognition and present comparable results \\nwith the best-related works. \\nKeywords— Micro-expression recognition; deep learning; \\nConvolutional Neural Network (CNN); small training data; data \\naugmentation \\nI.  INTRODUCTION  \\nBreaking down facial micro-expression is a relatively new \\nresearch theme with much-developing interests as of late. The \\nprinciple explanation behind naturally distinguishing the \\nmicro-expression is that micro- expression is a critical \\nemotional sign for different real-life applications. Micro-\\nexpression is a form of non-verbal communication that \\nunconsciously reveals the true sentiments of a person. As \\ncompared to macro-expression, micro-expression has three \\nbasic qualities: short duration, subtle movement, and trouble \\nabout concealing [32]. Micro-expressions are exhibited subtly \\nand typically occur very briefly, at a length of about 1/5 to 1/25 \\nof a second [7], and they usually occur in several parts of the \\nface where most people do not realise [18]. Similar to macro-\\nexpressions, micro-expressions can be grouped into six basic \\nexpressions: happy, surprise, anger, sad, disgust, and fear. It is not straightforward to recognise the genuine emotion \\nshown on one’s face. Thus recognising micro-expressions is \\nbeneficial in our daily life as we can read if someone is \\nattempting to cover his/her feeling or trying to deceive you. To \\nsolve this challenging problem, Ekman developed the Micro \\nExpression Training Tool (METT) which can help people to \\ndetect micro-expression [3]. Some research found that it was \\nstill hard for a trained human to detect facial micro-expression. \\nIn a psychological experiment [4] conducted using METT \\nmicro-expression training dataset, the average micro-\\nexpression recognition rate was 50%. To solve these problems, \\nthe advanced computer vision technology can achieve higher \\ndetection and classification performance and solve this issue. \\nDeep learning is a group of machine learning methods \\nbiologically inspired from the structure of the brain. \\nConvolutional Neural Networks (CNN) learns hierarchical \\nfeatures from multiple labelled images. It has been used for \\nvarious applications including recognition of objects, actions, \\nand places, face recognition [9, 24] and face expression \\nrecognition [20].  \\nThe deep convolutional neural network has recently yielded \\nexcellent performance in a wide variety of image classification \\ntasks. The careful design for mapping of local to global feature \\nlearning with convolution, pooling, and layered architecture \\nrenders excellent visual representation ability, making it an \\neffective tool for facial micro-expression recognition. In this \\npaper, we concentrate on the task of the image based static \\nfacial micro-expression recognition on CASME, CASME II \\nand both combined (CASME+2) with deep Convolutional \\nNeural Networks (CNN). The input for recognition is a raw \\nimage; which is then pre-processed and given as input to CNN \\nto predict the facial micro-expression label which should be \\none of the labels: Disgust, Fear, Happiness, Neutral, Sadness, \\nand Surprise. \\nDeep learning requires a lot of data but for micro-\\nexpression recognition, there is not enough data: there are three \\nspontaneous micro-expression datasets which altogether \\ncontain 748 videos. It is also not possible to collect millions of \\nlabelled training images from the internet for micro-\\nexpressions. Considering these limitations, we decided to \\nconvert the labelled micro-expression videos into frames and \\nuse these labelled frames for training the CNN model for image \\nbased micro-expression recognition. The frames are extracted \\nevery 0.2 seconds (or 200 milliseconds) from the video. The \\n978-1-5386-2839-3/17/$31.00 ©2017 IEEE\\nnumber of frames per video depends on the length of the video. \\nFor example, for a video of length 2 seconds, 157 frames could \\nbe extracted. All the images extracted from a video will have \\nthe same label as the video. Even though we have extracted the \\nframes from the videos, the number of image samples is not \\nenough to train the deep neural network model. This leads us to \\nthe idea of combining the widely used micro-expression \\ndatasets to form a large dataset of training samples.   \\nOur significant research contributions can be summarised \\nas follows: 1. We propose a CNN architecture that achieves \\nsatisfactory recognition accuracy on micro-expression images; \\n2. We also aim to present a novel thought to combine the \\nwidely used micro-expression databases CASME and CASME \\nII to increase the number of samples for training CNN model. \\nThe structure of the paper is as follows: Section II reviews \\nrelated work; Section III discusses the existing publicly \\navailable and widely used datasets for micro-expressions; \\nSection IV introduces our proposed micro-expression \\nrecognition model; Section V presents the experimental setup \\nand results, and the conclusions are drawn in Section VI. \\nII. RELATED WORK \\nStudies in psychology demonstrate that facial features of \\nexpression are located around mouth, nose and eyes and their \\nlocations are essential for categorising facial micro-\\nexpressions. In this part, we review some existing micro-\\nexpression analysing approaches as well as the deep learning \\ntechniques. Most work on micro-expressions in the field of \\nComputer Vision has mainly focused on micro-expression \\nrecognition [10, 19, 22, 23, 28, 29]. Micro-expression \\nrecognition task is defined as recognising the emotional label \\nof well-segmented video containing micro-expression from \\nstart to end. Micro-expression analysing methods can be \\nseparated into two major groups, the local methods and the \\nholistic methods. Initial research work on micro-expression \\nrecognition has been conducted on posed micro-expression. \\nAccording to the Action Units [5], the local methods partition \\nthe face area into some subregions. The reported results were \\ncarried out within each face subregions.  Wu et al. [28] \\nextracted features using Gabor filters and used Support Vector \\nMachine (SVM) to recognise them. Polikovsky et al. [23] \\nproposed to utilise Active Shape Model (ASM) model to detect \\nfacial landmarks which used to segment face area into twelve \\nsubregions. In each subregion, 3D-gradient orientation \\ndescriptor was extracted as a descriptor of facial muscle \\nmovement. Finally, the micro-expression video was divided \\ninto onset, apex and offset stages. The holistic methods \\nhandled the whole face for analysing the micro-expression \\ncategory. Pfister et al. [22] proposed a spatiotemporal holistic \\nfeature for recognising micro-expression. More specifically, \\nthe algorithm detected 68 landmarks in the first sequence by \\nusing ASM model. Then, these landmarks were utilised to \\nalign face for alleviating the issue of head movements. \\nTemporal Interpolation Model (TIM) was used to each \\nsequence so that the lengths of all the video were normalised \\n[22]. Furthermore, LBP-TOP features were used to extract the \\nfeature of micro-expression and SVM, Multiple Kernel \\nLearning (MKL), Random Forest (RF) were used to perform \\nclassification. Huang et al. [10] proposed SpatioTemporal Completed Local Quantization Patterns (STCLQ) feature for \\nrecognition. Liu et al. [19] proposed Main Directional Mean \\nOptical Flow feature and used SVM for classification. Feng et \\nal. [29] calculated principal optical flow direction resulting \\nFacial Dynamics Map with SVM as a classifier.  From the \\nliterature review, it was found that the main method for facial \\nlandmark localisation is ASM. \\nRecently, Deep Learning [13] has become very effective \\nimage analysis approach, such as image classification, \\nsemantic segmentation, object detection and image super-\\nresolution. Compare to traditional hand-engineered features, \\nsuch as Local Binary Pattern (LBP) [31] and Histogram of \\nGradients (HoG) [1, 16]; a deep convolutional neural network \\nconsists of multiple layers which can automatically learn \\nhierarchies visual features directly from the raw image pixels. \\nIn the research [12], Kim et al. introduced deep learning \\nfeatures for micro-expression recognition. They proposed a \\nnew method consisting of two sections. First, the spatial \\nfeatures of micro-expressions at different expression states \\n(onset, onset to apex transition, apex, apex to offset transition, \\nand offset) are converted using convolutional neural networks \\n(CNN). Next, the learned spatial features with expression-state \\nconstraints are transferred to learn temporal features of micro-\\nexpression. The temporal feature learning converts the \\ntemporal characteristics of the different states of the micro-\\nexpression using long short-term memory (LSTM) recurrent \\nneural networks (RNNs) [12]. The time scale dependent \\ninformation that resides along the video sequences is \\nconsequently learned by using LSTM. \\nRecent advances in micro-expression recognition focus on \\nrecognising more spontaneous facial micro-expressions. The \\nChinese Academy of Sciences Micro-Expression (CASME) \\n[32] and CASME II [30] were collected to mimic more \\nspontaneous scenarios and contain seven basic micro-\\nexpression categories. Both these datasets contain video clips \\nwhich were recorded in a controlled environment. The idea is \\nthat videos, although not truly spontaneous, at least provide \\nfacial micro-expressions in a much more natural and versatile \\nway than posed datasets. With the introduction of deep learning \\nmethods, a wide range of image classification work gives the \\nfinest performance. The existing works focus on spatio-\\ntemporal features for micro-expression recognition whereas we \\nconcentrate on the task of image-based static facial micro-\\nexpression recognition on CASME and CASME II with deep \\nCNNs.  \\nIII. DATABASES  \\nThere are three publicly available spontaneous micro-\\nexpression databases for recognition task: spontaneous micro-\\nexpression dataset (SMIC) [15], the Chinese Academy of \\nSciences Micro-Expression (CASME) [32]and CASME II [30]. \\nThese datasets have recorded micro-expression faces in frontal \\nview. Table I lists the key features of existing micro-expression \\ndatabases. \\nIn order to evaluate the proposed architecture, we use two \\nof these known databases, CASME and CASME II, as well as \\npresent an additional database which is an aggregation of \\nCASME and CASME II, we will refer it as CASME+2 in our  \\nTABLE  I \\nDETAILS OF EXISTING SPONTANEOUS MICRO -EXPRESSION DATABASES  \\nDataset Frame \\nrate \\n(Fps) Subjects Samples Emotion class \\nSMIC HS 100 20 164 \\n3 (Positive, \\nNegative, Surprise) VIR 25 10 71 \\nNIS 25 10 71 \\nCASME 60 35 195 8 (Contempt, \\nDisgust, Fear, \\nHappiness, \\nRegression, Sadness, \\nSurprise, Tense) \\nCASME II 200 35 247 7 (Disgust, Fear, \\nHappiness, Others, \\nRegression, Sadness, \\nSurprise, Tense) \\n \\narticle. \\nIV. PROPOSED METHOD  \\nThe conventional pipeline consists of four stages: 1) face \\ndetection; 2) pre-processing; 3) feature extraction; and 4) \\nclassification. Fig. 1 shows the basic block diagram of the \\nmicro-expression recognition. In this section, we describe the \\nwhole structure of our micro-expression recognition. We \\nexplain the method to increase the number of samples in the \\ndataset using data augmentation in Section A. Section B \\ndiscusses the preparation of the training, testing and validation \\ndatasets for training and validating the developed CNN model. \\nSection C explains about the method applied for face detection \\nand pre-processing, i.e. steps 1 and 2 of the block diagram. \\nSection D represents steps 3, and 4 of the block diagram where \\nthe deep network does the feature extraction and classification. \\nWe developed CNN with variable depths to evaluate the \\nperformance of our model for facial micro-expression \\nrecognition. \\nA. Data Augmentation \\nThe absence of large training datasets is a crucial \\nbottleneck that keeps the utilisation of profound (deep) learning \\ntechniques in such cases, as the models will overfit drastically \\nwhen utilising small training datasets. To address this issue, a \\nlarge number of strategies have been proposed: fine-tuning \\nmodels trained from other large public datasets (e.g. ImageNet \\n[2]), using the big synthetic training datasets explored by some \\nauthors [8, 12, 14]. \\nThere are two critical points of interest of utilising synthetic \\ndata (i) one can produce the same number of training samples \\nas required, and (ii) it permits explicit control over the \\nunwanted factors. Data augmentation is a technique that is \\ncommonly used to reduce the scarcity problem. It is a set of \\nlabel-preserving transforms that introduce some new instances \\nwithout collecting the new data. In this, the existing training \\nimages are transformed without affecting the semantic class \\nlabel. Examples of such transformations are horizontal/vertical \\nmirroring [17], cropping, small rotations, etc. Flipping and  \\n \\nFig. 1. General block diagram of micro-expression recognition \\n \\nTABLE  II \\nMICRO -EXPRESSION DATABASE AFTER DATA AUGMENTATION  \\n \\nDatabase Original After augmentation \\nCASME 26,423 52,846 \\nCASME II 46,416 92,832 \\nCASME+2 69,520 1,39,040 \\n \\nmirroring images vertically or horizontally producing two \\nsamples of each is a commonly used data augmentation \\ntechnique for face recognition. In our evaluations, both original \\nand vertically mirrored images are used for training. Table II \\ndemonstrates the data augmentation process has doubled the \\nnumber of samples. \\nB. Data preparation \\n  Training and testing of the model have carried on images \\nfrom CASME, CASME II and CASME+2 databases which \\ncomprise of human faces; each labelled with one of 5 emotion \\ncategories: disgust, fear, happiness, sadness, and surprise. We \\nhave also taken into consideration the 6th category as ‘neutral’. \\nThe given images are divided into two different sets which are \\ntraining and testing sets. For data augmentation, mirrored \\nimages were generated by flipping images vertically. The \\ntraining set comprises of 80% of the total images in the \\nsynthetic database and remaining 2% of the images are further \\ndivided as the testing set (1%) and validation set (1%). \\nC. Face detection and Pre-processing \\nWe note that all the images are pre-processed so that they \\nform a bounding box around the face region. Using the raw \\nimages from the CASME and CASME II database, we \\nimplemented the DLib face detector in OpenCV to detect and \\ncrop the face region from the raw image. The cropped face is \\nthen processed for head pose correction by computing the \\nangle between the eye centroids and later applying the affine \\ntransformation. The transformed image is again passed to DLib \\nface detector to crop and save the more accurate face region.  \\nFig. 2. shows the face detection and pre-processing steps. \\nThe face region is detected from the raw image frame given as \\nthe input. The Fig. 2. shows the red bounding box around the \\ndetected face region and the cropped face image. Since the \\nframes are extracted from the videos, there is slight head \\nmovement observed in some of the videos. In the pre-\\nprocessing step, the head pose correction method is applied to \\nthe cropped face. The head pose is aligned with the use of \\naffine transformation, and the face region is cropped again to \\nensure that only the face region is given as input to the CNN \\nmodel. \\n \\nFig. 3.The CNN architecture of our deep convolutional neural network\\n \\nFig. 2. Face detection and Pre-processing raw face images  \\nD. Convolutional Neural Network (CNN) framework \\nFig. 3. describes a basic framework for facial micro-\\nexpression recognition using the deep convolutional neural \\nnetwork. The pre-processed and cropped face image is passed \\nas input to CNN model where the image has to pass through \\ndifferent layers of CNN: 1) Convolutional; 2) Rectified Linear \\nUnit (ReLU); 3) Pooling or Sub sampling, and 4) Classification \\n(Fully Connected Layer). \\n The primary purpose of the Convolutional step is to extract \\nfeatures from the input image. It maintains the spatial \\nrelationship between pixels by learning image features using \\nsmall squares of input data and creates a feature map. ReLU is \\na non-linear operation. ReLU is a component wise operation \\n(applied per pixel) and replaces all negative pixel values in the \\nfeature map by zero. Convolution is a linearity process which \\nis element wise matrix multiplication and addition, so we \\nrepresent non-linearity by presenting a non-linear function like \\nReLU. Spatial Pooling (also called subsampling or \\ndownsampling) reduces the dimensionality of each feature map \\nbut retains the most important information. In case of Max \\nPooling, the largest element from the rectified feature map \\nwithin that window is taken. \\nUnitedly these layers select the useful features from the \\nimages, embed non-linearity in the network and reduce feature \\ndimension while intending to make the features to some degree \\nequivariant to scale and translation. The output of the third \\npooling layer acts as an input to the Fully Connected Layer. \\nThe Fully Connected Layer is a conventional Multi Layer \\nPerceptron that uses a Softmax initiation function in the output \\nlayer (different classifiers like SVM can likewise be utilised,  \\n \\nFig. 4. Deep Network configuration  \\nhowever we are utilising Softmax). The output of the \\nconvolutional and pooling layers constitute high-level features \\nof the input image. The purpose of the Fully Connected layer is \\nto utilise these features for classifying the input image into \\nseveral classes based on the training dataset. \\nPutting it all together, the Convolution and Pooling layers \\nact as Feature Extractors from the input image while the Fully \\nConnected later serves as a Classifier. \\nThe network contains five convolutional layers where the \\nConv1 layer has 11x11 filters and rest with 3x3 filters, three \\nmax pooling layers of kernel size 3x3, stride 2 and three fully \\nconnected layers (Fig. 4). The fully connected layers contain \\ndropout, a mechanism for randomization which reduces the \\nrisk of the network over fitting. The Rectified Linear Unit \\n(ReLU) was used for activation function. The system operates \\nin two main phases: Training and Testing. During training, the \\nsystem receives a training data comprising of images of faces \\nwith their respective micro-expression label. To ensure that the \\ntraining performance is not affected by order of presentation of \\nthe examples, a few images are separated as a validation set. \\nDuring testing, the images in the validation set are fed to the \\nnetwork which outputs the predicted micro-expression label \\nusing the final network weights learned during training. The \\nbase learning rate (base_lr) for the model is set to 0.001, \\nstepsize parameter is set to 10,000, and maximum iterations \\n(max_iter) are 100,000. The batch size of the images is 50 \\nimages per batch for training. We also changed the learning \\npolicy parameter (lr_policy) value to “step” where the learning \\nrate will drop at every step size. The rest of the parameter \\nsettings were used the default. We used a baseline Softmax \\nclassifier.  \\nThe VGGFace is a network trained on a very large-scale \\nface images dataset (2.6M images, 2.6k people) for the task of \\nface recognition available from Visual Geometry Group at the \\nUniversity of Oxford [21]. Since the dataset was trained for a \\nsimilar application but on a much larger dataset than ours, we \\ntried fine-tuning the model for micro-expressions. \\nV. EXPERIMENT AND RESULTS  \\nThe proposed micro-expression recognition verifies the \\neffectiveness by conducting experiments on the CASME, \\nCASME II and CASME+2 datasets. All the images in the \\ndatabases are pre-processed and flipped vertically to increase \\nthe number of samples. The new synthetic database is then \\ndivided into two groups as Training and Testing. Each image \\nhas been categorized as: 0 = Disgust, 1 = Fear, 2 = Happiness, \\n3 = Neutral, 4 = Sadness, and 5 = Surprise. \\nWe implemented the deep convolutional neural networks \\nbased on the Caffe [11] (a fast open framework for deep \\nlearning and computer vision) and took 10-12 hours to train \\nthis network. The models were trained for 100,000 iterations \\non CASME and 41,000 iterations on CASME II and \\nCASME+2. The learning rate is changed from 0.001 to 0.0001 \\nwhen the training iteration reaches 10,000. At each round of \\niterations in model training, the layer parameters of the \\nnetwork are updated based on the loss. We set a maximum \\nnumber of iterations, and when the training time reaches the \\nnumber, we obtain a trained model, which is essentially the \\nparameter of all the filters. We then save the model so we can \\nuse the model to predict a micro-expression of images. \\nThe input is given from the Validation sets which are the \\nraw face images collected from the original databases. For each \\nexperiment, a corresponding Validation set is used depending \\non the training database. The confusion matrixes for each \\nexperiment are as shown in the Figures 5, 6 and 7. \\nThe recognition accuracy results for the three databases \\nused are summarised in Table III. From the table, we can \\nobserve that the recognition accuracy improves as the number \\n \\nFig. 5.   Confusion matrix on CASME database \\n \\nTABLE  III \\nMICRO -EXPRESSION RECOGNITION ACCURACY WITH DIFFERENT DATABASES  \\n \\nDatabase CASME CASME II CASME+2\\nAccuracy 74.25% 75.57% 78.02% \\n \\nFig. 6.   Confusion matrix on CASME II database \\n \\n \\nFig. 7.   Confusion matrix on CASME + CASME II database \\n \\n \\nof training samples increases. It can be interpreted from the \\nresults in Table II that the larger the training dataset is, the \\nbetter the recognition accuracy is achieved. \\n Tables IV and V lists the recognition accuracy of using our \\nmethod and of the state-of-the-art methods in CASME dataset \\nand CASME II dataset respectively. Most of the existing \\nmethods are video based, which more or less take advantage of \\nvideos temporal information. Our method is image based, \\nwhich applies CNN on image frames extracted from videos. \\nThe tables testify that proposed CNN method exhibits \\nsatisfying micro-expression recognition accuracy. These results \\nalso demonstrate that image based micro-expression \\nrecognition delivers identical results as video based \\napproaches. \\nDue to larger number samples in CASME II dataset as \\ncompared to CASME dataset, the researchers in [12] opted to \\ndemonstrate deep learning results on video clips from CASME \\nII dataset. In our research, we have applied data augmentation \\ntechnique to increase the number of samples. Therefore, our \\nexperiments use both CASME and CASME II datasets to \\nshowcase the effectiveness of proposed CNN method. \\n \\nTABLE  IV \\nMICRO -EXPRESSION RECOGNITION ON CASME  DATABASE  \\n \\nMethod Accuracy \\n LBP-TOP+ELM [6] 73.82% \\nMDMO+SVM [19] 68.86% \\nLBP-TOP+SVM [25] 61.85% \\nProposed CNN method 74.25% \\n \\n \\nTABLE  V \\nMICRO -EXPRESSION RECOGNITION ON CASME  II DATABASE  \\n \\nMethod Accuracy \\nLBP-TOP+SVM [26] 75.30% \\nMDMO+SVM [19] 67.37% \\nCNN+LSTM [12] 60.98% \\nProposed CNN method 75.57% \\n \\nIn [6, 12, 19, 25, 26], the researchers have considered the \\ntemporal factor from the video for recognition of micro-\\nexpressions which have contributed an additional feature in \\nthe calculations. In case of our method, we tried to eliminate \\nthe temporal factor and simply exhibit the image based micro-\\nexpression recognition approach.  \\nA. Difficulties with certain expressions \\nWe observe that some classes appear to be “harder” to train \\nin the sense that, (1) none of our models were able to score \\nhighly on them, and (2) our model predictions for those classes \\ntend to fluctuate depending on our training scheme and \\narchitecture. This appears to be the case for classes such as \\n“fear”, “happiness”, and “sadness”. There might be two \\nreasons discussed as follows. First, these classes tend to have \\nmuch fewer training samples compared to classes such as \\n“disgust”, “neutral”, and “surprise”, making it harder to train \\nthe CNN to recognise them. Second, these micro-expressions \\ncan be very nuanced, making it difficult even for humans to \\nagree on their correct labelling [27]. \\nWe suspect that the inherent difficulty in assigning labels to \\nsome of the samples may have caused them to be \\n“mislabelled”, thereby affecting the models that were trained \\non them. Lastly, we note that all except one model (CASME II) \\nwere unable to predict a sufficient number of samples for label \\n“fear” correctly. The reason for this could be an imbalance in \\nthe training datasets. The imbalance in the number of training \\nsamples for each class of micro-expressions most likely caused \\nour models to overfit the micro-expressions with more samples \\n(e.g. “disgust”) at the expense of this class. Furthermore, the \\nexpression of fear is very subtle, which means that it will be \\nhard for our CNN models to discover features to robustly \\ndistinguish this micro-expression from other similarly nuanced \\nexpressions such as sad, happiness, and surprise. This can be \\nverified by examining the confusion matrices in Figure 3, 4 and \\n5, which indicates that the “disgust” class is often the highest \\nscoring class. The classes “fear”, “neutral”, “happiness”, “sadness”, and “surprise” are often mistaken for each other by \\nour models. \\nThe combination of these two factors makes it even harder \\nto train models to predict this micro-expression accurately. The \\nabove observations highlight the difficulty in training CNNs \\nusing a small unbalanced dataset with classes that are not \\nvisually distinctive. \\nVI. CONCLUSION  \\nWe have shown that it is possible to obtain a significant \\nimprovement in accuracy over the baseline results for micro-\\nexpression classification using CNNs pre-trained model \\nutilised for the task for face recognition and fine-tuning it on \\nface micro-expression databases. The experiments also \\nconclude that the small sizes of the datasets do not favour them \\nfor being used for training CNNs. However, CNNs trained on \\nsufficiently large face micro-expression datasets also can be \\nused to obtain better results than the baseline without using the \\ndata augmentation technique to increase the size of the dataset \\nartificially. This suggests that if we were to exploit deep neural \\nnetworks such as CNN for face micro-expression recognition \\nto achieve the significant gains seen in other domains, then \\nhaving bigger datasets is crucial. This is where we implanted \\nthe idea of combining the two databases CASME and CASME \\nII to form a larger database. Image based face expression \\nrecognition is a popular research topic, but we demonstrated \\nthrough our experiments that image based micro-expression \\nrecognition could also yield acceptable accuracy. \\nLastly, we also noted the inherent difficulty in assigning \\ncorrect labels to faces depicting some of the more nuanced \\nmicro-expressions and how that can affect the performance of \\nour models. \\nREFERENCES  \\n \\n[1] G. K. Chavali, S. K. N. Bhavaraju, T. Adusumilli, and V. \\nPuripanda, \"Micro-Expression Extraction For Lie Detection Using \\nEulerian Video (Motion and Color) Magnication,\" 2014. \\n[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \\n\"Imagenet: A large-scale hierarchical image database,\" in \\nComputer Vision and Pattern Recognition, 2009. CVPR 2009. \\nIEEE Conference on , 2009, pp. 248-255. \\n[3] P. Ekman, Micro Expressions Training Tool : Emotionsrevealed. \\ncom, 2003. \\n[4] J. Endres and A. Laidlaw, \"Micro-expression recognition training \\nin medical students: a pilot study,\" BMC Medical Education, vol. \\n9, p. 47, 2009. \\n[5] E. Friesen and P. Ekman, \"Facial action coding system: a \\ntechnique for the measurement of facial movement,\" Palo Alto, \\n1978. \\n[6] Y. Guo, C. Xue, Y. Wang, and M. Yu, \"Micro-expression \\nrecognition based on CBP-TOP feature with ELM,\" Optik-\\nInternational Journal for Light and Electron Optics, vol. 126, pp. \\n4446-4451, 2015. \\n[7] C. House and R. Meyer, \"Preprocessing and Descriptor Features \\nfor Facial Micro-Expression Recognition,\" 2015. \\n[8] G. Hu, X. Peng, Y. Yang, T. Hospedales, and J. Verbeek, \\n\"Frankenstein: Learning deep face representations using small \\ndata,\" arXiv preprint arXiv:1603.06470, 2016. \\n[9] G. Hu, Y. Yang, D. Yi, J. Kittler, W. Christmas, S. Z. Li , et al. , \\n\"When face recognition meets with deep learning: an evaluation of \\nconvolutional neural networks for face recognition,\" in \\nProceedings of the IEEE International Conference on Computer \\nVision Workshops , 2015, pp. 142-150. \\n[10] X. Huang, G. Zhao, X. Hong, W. Zheng, and M. Pietikäinen, \\n\"Spontaneous facial micro-expression analysis using \\nSpatiotemporal Completed Local Quantized Patterns,\" \\nNeurocomputing, vol. 175, pp. 564-578, 2016. \\n[ 1 1 ]  Y .  J i a ,  E .  S h e l h a m e r ,  J .  D o n a h u e ,  S .  K a r a y e v ,  J .  L o n g ,  R .  \\nGirshick , et al. , \"Caffe: Convolutional architecture for fast feature \\nembedding,\" in Proceedings of the 22nd ACM international \\nconference on Multimedia , 2014, pp. 675-678. \\n[12] D. H. Kim, W. J. Baddar, and Y. M. Ro, \"Micro-Expression \\nRecognition with Expression-State Constrained Spatio-Temporal \\nFeature Representations,\" in Proceedings of the 2016 ACM on \\nMultimedia Conference , 2016, pp. 382-386. \\n[13] Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature, \\nvol. 521, pp. 436-444, 2015. \\n[14] W. Li, M. Li, Z. Su, and Z. Zhu, \"A deep-learning approach to \\nfacial expression recognition with candid images,\" in Machine \\nVision Applications (MVA), 2015 14th IAPR International \\nConference on , 2015, pp. 279-282. \\n[15] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikäinen, \"A \\nspontaneous micro-expression database: Inducement, collection \\nand baseline,\" in Automatic Face and Gesture Recognition (FG), \\n2013 10th IEEE International Conference and Workshops on , \\n2013, pp. 1-6. \\n[16] X. Li, H. Xiaopeng, A. Moilanen, X. Huang, T. Pfister, G. Zhao , et \\nal., \"Towards Reading Hidden Emotions: A Comparative Study of \\nSpontaneous Micro-expression Spotting and Recognition \\nMethods,\" IEEE Transactions on Affective Computing, 2017. \\n[17] X. Li, J. Yu, and S. Zhan, \"Spontaneous facial micro-expression \\ndetection based on deep learning,\" in Signal Processing (ICSP), \\n2016 IEEE 13th International Conference on , 2016, pp. 1130-\\n1134. \\n[18] S.-T. Liong, J. See, K. Wong, A. C. Le Ngo, Y.-H. Oh, and R. \\nPhan, \"Automatic apex frame spotting in micro-expression \\ndatabase,\" in Pattern Recognition (ACPR), 2015 3rd IAPR Asian \\nConference on , 2015, pp. 665-669. \\n[19] Y. J. Liu, J. K. Zhang, W. J. Yan, S. J. Wang, G. Zhao, and X. Fu, \\n\"A Main Directional Mean Optical Flow Feature for Spontaneous \\nMicro-Expression Recognition,\" IEEE Transactions on Affective \\nComputing, vol. PP, pp. 1-1, 2015. \\n[20] Y. Lv, Z. Feng, and C. Xu, \"Facial expression recognition via deep \\nlearning,\" in Smart Computing (SMARTCOMP), 2014 \\nInternational Conference on , 2014, pp. 303-308. \\n[21] O. M. Parkhi, A. Vedaldi, and A. Zisserman, \"Deep Face \\nRecognition,\" in BMVC , 2015, p. 6. \\n[22] T. Pfister, X. Li, G. Zhao, and M. Pietikäinen, \"Recognising \\nspontaneous facial micro-expressions,\" in 2011 International \\nConference on Computer Vision , 2011, pp. 1449-1456. \\n[23] S. Polikovsky, Y. Kameda, and Y. Ohta, \"Facial micro-expressions \\nrecognition using high speed camera and 3D-gradient descriptor,\" \\nin Crime Detection and Prevention (ICDP 2009), 3rd International \\nConference on , 2009, pp. 1-6. \\n[24] Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf, \"Deepface: \\nClosing the gap to human-level performance in face verification,\" \\nin Proceedings of the IEEE Conference on Computer Vision and \\nPattern Recognition , 2014, pp. 1701-1708. \\n[25] S. Wang, W.-J. Yan, X. Li, G. Zhao, and X. Fu, \"Micro-expression \\nRecognition Using Dynamic Textures on Tensor Independent \\nColor Space,\" in ICPR , 2014, pp. 4678-4683. \\n[26] Y. Wang, J. See, Y.-H. Oh, R. C.-W. Phan, Y. Rahulamathavan, \\nH.-C. Ling , et al. , \"Effective recognition of facial micro-\\nexpressions with video motion magnification,\" Multimedia Tools \\nand Applications, pp. 1-26, 2016. \\n[27] S. C. Widen, J. A. Russell, and A. Brooks, \"Anger and disgust: \\nDiscrete or overlapping categories,\" in 2004 APS Annual \\nConvention, Boston College, Chicago, IL , 2004. \\n[28] Q. Wu, X. Shen, and X. Fu, \"The machine knows what you are \\nhiding: an automatic micro-expression recognition system,\" in \\nInternational Conference on Affective Computing and Intelligent \\nInteraction , 2011, pp. 152-162. [29] F. Xu, J. Zhang, and J. Wang, \"Microexpression Identification and \\nCategorization using a Facial Dynamics Map,\" IEEE Transactions \\non Affective Computing, vol. PP, pp. 1-1, 2016. \\n[30] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen , et \\nal., \"CASME II: An improved spontaneous micro-expression \\ndatabase and the baseline evaluation,\" PloS one, vol. 9, p. e86041, \\n2014. \\n[31] W.-J. Yan, S.-J. Wang, Y.-H. Chen, G. Zhao, and X. Fu, \\n\"Quantifying micro-expressions with constraint local model and \\nlocal binary pattern,\" in Workshop at the European Conference on \\nComputer Vision , 2014, pp. 296-305. \\n[32] W.-J. Yan, Q. Wu, Y.-J. Liu, S.-J. Wang, and X. Fu, \"CASME \\ndatabase: A dataset of spontaneous micro-expressions collected \\nfrom neutralized faces,\" in Automatic Face and Gesture \\nRecognition (FG), 2013 10th IEEE International Conference and \\nWorkshops on , 2013, pp. 1-7. \\n \\n',\n",
       " 'Error',\n",
       " 'Intra-Class Variation Reduction Using Training\\nExpression Images for Sparse Representation\\nBased Facial Expression Recognition\\nSeung Ho Lee, Student Member, IEEE , Konstantinos N. (Kostas) Plataniotis, Fellow, IEEE , and\\nYong Man Ro, Senior Member, IEEE\\nAbstract— Automatic facial expression recognition (FER) is becoming increasingly important in the area of affective computing\\nsystems because of its various emerging applications such as human-machine interface and human emotion analysis. Recently,\\nsparse representation based FER has become popular and has shown an impressive performance. However, sparse representation\\ncould often produce less meaningful sparse solution for FER due to intra-class variation such as variation in identity or illumination. This\\npaper proposes a new sparse representation based FER method, aiming to reduce the intra-class variation while emphasizing the\\nfacial expression in a query face image. To that end, we present a new method for generating an intra-class variation image of each\\nexpression by using training expression images. The appearance of each intra-class variation image could be close to the appearance\\nof the query face image in identity and illumination. Therefore, the differences between the query face image and its intra-class\\nvariation images are used as the expression features for sparse representation. Experimental results show that the proposed FER\\nmethod has high discriminating capability in terms of improving FER performance. Further, the intra-class variation images of\\nnon-neutral expressions are complementary with that of neutral expression, for improving FER performance.\\nIndex Terms— Facial expression recognition (FER), sparse representation based classiﬁer (SRC), intra-class variation, facial expression\\nfeatures\\nÇ\\n1I NTRODUCTION\\nAUTOMATIC facial expression recognition (FER) has been\\nan interesting and challenging research topic in the\\narea of affective computing systems because of its growing\\napplications such as human-machine interface and human\\nemotion analysis [1], [32]. Recently, the advantages of\\nexploiting sparsity in pattern classiﬁcation have been exten-\\nsively demonstrated in [2], [3], [4], [5], [6], [7], [28]. In [2],\\nWright et al. suggested a framework for face recognition\\n(FR) using sparse representation. The experimental results\\nof [2] showed that the sparse representation based classiﬁer\\n(SRC) was superior to other widely used classiﬁers under\\nchallenging FR conditions. Inspired by the successful use in\\nFR, SRC was studied for the purpose of FER in [4], [5], [6],\\n[7], [28]. Most of the FER methods based on SRC made use\\nof features which capture intensity changes of face appear-\\nance itself (i.e., appearance based feature). Huang et al.\\nattempted to combine the SRC with local binary patterns\\n(LBPs) [4] features, a very powerful method to describe tex-\\nture and shapes of image [8]. Experimental result in [4]showed that SRC using LBP features could achieve better\\nFER performance than SRC using raw pixels of face images.\\nIn [5], it was shown that SRC outperformed nearest neigh-\\nbor (NN) [10] and support vector machine (SVM) classiﬁers\\n[11], independent of features used (e.g., raw pixel or LBP or\\nGabor wavelet [9]). In [6] and [28], the effectiveness of SRC\\nwith appearance based features such as two-dimensional\\nprincipal component analysis (2DPCA) [12] and local phase\\nquantization (LPQ) [24] was experimentally veriﬁed. In the\\nprevious works in [4], [5], [6], [28], the authors showed that\\nSRC could achieve some improvements in FER performance\\nby using appearance based features. However, the experi-\\nments were performed only on data sets created under rela-\\ntively well controlled environment (e.g., face images with\\nhigh intensity expression, and without illumination varia-\\ntion). In addition, under SRC framework, the use of such\\nappearance based features may not be a straightforward\\nmethod. This is mainly because facial identity of person is\\noften confused with facial expressions [7].\\nThe authors of [7] tried to extract a meaningful expres-\\nsion feature of SRC by introducing difference image\\nbetween query face image and neutral face image for the\\npurpose of reducing the effect of identity information con-\\ntained in face image. However, this method did not\\nconsider the effect of varying image acquisition conditions\\n(e.g., illumination change) between face images. The\\nauthors of [7] also introduced a geometric feature for SRC,\\ncalled facial grids, which used the difference of node coor-\\ndinates between neutral face image and expressive face\\nimage. However, tracking (by Kanade-Lucas-Tomasi (KLT)\\ntracker in [13]) of facial feature points could be highly/C15S.H. Lee and Y.M. Ro are with the Department of Electrical Engineering,\\nKorea Advanced Institute of Science and Technology (KAIST), Republic of\\nKorea. E-mail: leesh09@kaist.ac.kr, ymro@ee.kaist.ac.kr.\\n/C15K.N. Plataniotis is with the Department of Electrical and Computer Engi-\\nneering, University of Toronto, Toronto, ON, Canada.\\nE-mail: kostas@comm.utoronto.ca.\\nManuscript received 28 Aug. 2013; revised 21 July 2014; accepted 1 Aug.\\n2014. Date of publication 7 Aug. 2014; date of current version 30 Oct. 2014.\\nRecommended for acceptance by Q. Ji.\\nFor information on obtaining reprints of this article, please send e-mail to:\\nreprints@ieee.org, and reference the Digital Object Identiﬁer below.\\nDigital Object Identiﬁer no. 10.1109/TAFFC.2014.2346515340 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\n1949-3045 /C2232014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nsensitive against illumination change [14], leading to deg-\\nradation in FER performance. Another limitation of the\\nboth features in [7] was that the identity of query face\\nimage needed to be known and its neutral face image\\nneeded to be obtained in advance. In practice, however, it\\nis generally difﬁcult to satisfy these requirements in real-\\nworld applications.\\nThe above-mentioned methods of [4], [5], [6], [7], [28] do\\nnot consider dealing with appearance variation either in\\nsubject or in image acquisition condition. This appearance\\nvariation is regarded as intra-class variation. Because of\\nintra-class variation, sparse representation is likely to con-\\ntain coefﬁcients corresponding to both similar expression\\nand similar appearance such as identity and illumination,\\nwhich is not desirable for FER.\\nThe aim of this paper is to propose a new sparse repre-\\nsentation based FER method which is robust to the afore-\\nmentioned intra-class variation. The main contribution of\\nour paper is twofold:\\n1) For a query face image, we propose the so-called\\nintra-class variation image which is generated by using\\nthe training face expression images. The intra-class\\nvariation image is similar to the query face image in\\nterms of identity and illumination. To extract expres-\\nsion features, we subtract the intra-class variation\\nimage from the query face image. The expression dif-\\nference is emphasized while intra-class variation due\\nto identity and illumination is reduced. We demon-\\nstrate that the use of the proposed intra-class varia-\\ntion images can improve facial expression sparsity,\\nwhich leads to high FER performance.\\n2) A facial expression is generally deﬁned by begin and\\nend with a neutral state [15]. For this reason, in some\\nFER methods, the difference between neutral face\\nand query face was used as expression feature [7],\\n[16], [17]. However, face image of neutral state is not\\nalways available in reality [15]. To cope with this\\nissue, we investigate that expressive (or non-neutral)\\nstate can be used for extracting facial expression. As\\ndescribed in Section 2.1 and Fig. 3, expressive state\\ncould also be useful for reducing identity information\\nand emphasizing the expression of a query face\\nimage. Experimental results show that difference\\nbetween query face image and intra-class variation\\nimage of a non-neutral expression can yield similar\\ndiscriminating power to difference using intra-class\\nvariation image of neutral expression. Further, differ-\\nences made by all expression states in training images\\nare able to signiﬁcantly improve FER performance.\\nFrom the extensive and comparative experiments on the\\nJAFFE [18], Cohn-Kanade (CK þ) [37], MMI [38], BU-3DFE\\n[20], and CMU Multi-PIE [19] databases (DBs), the proposed\\nmethod is found to be effective in both person-dependent\\nand—independent recognition conditions (detailed in\\nSection 3.1). In particular, substantial FER improvement (of\\nup to 21 percent enhancement) can be achieved by the pro-\\nposed method in person-independent recognition, com-\\npared to the case of employing SRC without using the\\nsubtraction of the intra-class variation image. In addition, it\\nhas been demonstrated that the proposed method can befeasible for sparse representation based FER even in the\\npresence of variation in illumination or expression intensity.\\nThe remainder of the paper is organized as follows:\\nSection 2 presents the proposed method for extracting facial\\nexpression features. In Section 3, experimental results are\\npresented followed by conclusions in Section 4.\\n2P ROPOSED FACIAL EXPRESSION RECOGNITION\\nMETHOD\\nAs shown in Fig. 1a, the feature extractions of a query face\\nimage largely consist of two sequential steps: 1) generation\\nof intra-class variation image for each expression and 2)\\nsubtraction of the intra-class variation image from the query\\nface image. As is seen in Fig. 1a, training face images of\\neach expression are used for the generation of correspond-\\ning intra-class variation image for the query face image. The\\nintra-class variation image would present the same facial\\nexpression as the corresponding training face images.\\nAfter obtaining the intra-class variation image for every\\nexpression, we subtract each intra-class variation image\\nfrom the query face image. Since the intra-class variation\\nimage is similar to the query face image in appearance other\\nthan expression, each subtraction image is likely to high-\\nlight the expression difference between the query face\\nimage and the intra-class variation image. As demonstrated\\nin our experiment, each of the subtraction images can be\\ndiscriminative for FER.\\nIn classiﬁcation phase, we separately apply sparse repre-\\nsentation to the individual expression feature, i.e., subtrac-\\ntion image. Dictionary [2] constructions of training\\nexpression features are depicted in Fig. 1b. Sparsity based\\nclassiﬁcation using the dictionaries is performed as shown\\nin Fig 1a.\\nDetailed descriptions on the feature extractions and the\\nclassiﬁcation are given in the following sections.\\n2.1 Generations of Intra-Class Variation Images and\\nExpression Feature Extractions\\nIn this section, we describe the extractions of expression\\nfeatures from a given query face image, q2<N. LetFF¼\\n½FF1;FF2;...;FFC/C1382<N/C2Mdenotes the training set that con-\\ntains (vectorized) training face images with Cdifferent\\ntypes of expression classes. Nis the number of dimensions\\nof each face image and Mis the total number of training\\nface images. In addition, FFi¼½ti;1;ti;2;...;ti;Mi/C1382<N/C2Mi\\ndenotes the training set of the ith expression class. Miis the\\nnumber of training face images in FFiso that M¼PC\\ni¼1Mi:\\nti;j2<Ncorresponds to the jth training face image of the\\nith expression class.\\nFor the query face image q;we generate intra-class varia-\\ntion image hq\\ni2<Nby using FFi:Speciﬁcally, the intra-class\\nvariation image hq\\niis generated based on an approximation\\nof query face image that is a linear combination of the train-\\ning face images of FFi:For the approximation, we need a\\nweight vector wq\\ni¼½w1;w2;...;wMi/C138T2<Mi:The optimal\\nweight vector ^wq\\nican be obtained by employing the follow-\\ning regularized least square method:\\n^wq\\ni¼arg min/C8/C13/C13q/C0FFiwq\\ni/C13/C132\\n2þ/C21/C13/C13wq\\ni/C13/C132\\n2g; (1)LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 341\\nwhere k/C1k2is‘2-norm of a vector. Note that in (1), the\\nterm jq/C0FFiwq\\nij2\\n2corresponds to the reconstruction error,\\nwhile the jwq\\nij2\\n2is regularized term which aims at making\\nthe solution (i.e., weight vector) ^wq\\niin a stable way [21].\\nIn this paper, the ^wq\\niin (1) is derived by computing^wq\\ni¼ðFFT\\niFFiþ/C21/C1IÞ/C01FFT\\niqi nt h es a m ew a ya sd e s c r i b e d\\nin [21], where the /C21is a regularization parameter (set to\\n0.0001 in this paper) and I2<Mi/C2Miis an identity\\nmatrix. Note that the matrix ðFFT\\niFFiþ/C21/C1IÞ/C01FFT\\niis inde-\\npendent of q:Hence, this matrix is pre-calculated as a\\nFig. 1. The frameworks for (a) extracting the expression features of a query face image and classifying the query face image using sparsity of the\\nexpression features, (b) constructing dictionaries of the expression features of training face images for the sparsity based classiﬁcation.342 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\nprojection matrix, which yields efﬁciency in computation\\ntime. Using the weight vector ^wq\\ni;we can express the\\nintra-class variation image hq\\nifor the query face image q\\nas the following linear representation:\\nhq\\ni¼FFi^wq\\ni: (2)\\nIn (2), each element of ^wq\\niis the weight value describing the\\ncontribution of the associated training face image in FFi:hq\\ni\\npresents a single expression state ( ith expression) that is com-\\ning from the training face images of the corresponding ith\\nexpression. Figs. 2a and 2b show intra-class variation images\\nof neutral and surprise expressions, which are generated\\nusing (1) and (2) for the corresponding query face images.\\nTheir appearances (e.g., illumination or accessories such as\\nglasses) are similar to their query faces except for expression\\nappearances. Hence, by using (1), it can be expected that each\\nintra-class variation image has larger contribution from the\\ntraining face images which have similar appearance of iden-\\ntity and illumination to query face image. Taking difference\\nbetween the query face image and its intra-class variation\\nimage could reduce the effect of unnecessary appearance (i.e.,\\nintra-class variation) contained in face image.\\nAfter obtaining the Cintra-class variation images hq\\ni\\nði¼1;...;CÞ–each corresponding for one of the Cexpres-\\nsions in the training set, the expression features yq\\ni2<N\\nði¼1;...;CÞof the query face image qcan be computed as\\nyq\\ni¼q/C0hq\\ni: (3)\\nNote that the intra-class variation image hq\\niis obtained\\nby combining the training face images of the ith expres-\\nsion. As a result, each expre ssion feature is well-suited\\nfor highlighting expression di fference (to a large extent)\\nbetween the intra-class variation image hq\\niand the query\\nface image q:\\nThe motivation for using the difference between two\\nexpressive faces (i.e., the query face image qand the\\nintra-class variation image hq\\ni) as feature is explained as\\nfollows. It has been demonstrated in [7], [16] that the useof the difference between an expressive query face and a\\nneutral face of the same subject leads to the normalization\\nthat is capable of reducing the identity information in the\\nquery face. Note that human f acial expressi on associated\\nwith a basic emotion (e.g., Happy) can be represented as\\na combination of action units (AUs) [1], [15], [37] and cer-\\ntain AU types should appear in the combination [37] (For\\nexample, a face with ‘Happy’ emotion should include\\nAU12 (Lip Corner Puller) [37]). For this reason, typical\\nfaces with an emotional expression, normalized by their\\nneutral faces could look sim i l a rt os o m ee x t e n ti no t h e r\\nsubjects. This makes FER discriminative. The normaliza-\\ntion effect and discriminating capability can be extended\\nto the difference between two expressive faces . Let us consider\\nan example of a query face with Disgust and its intra-\\nclass variation image with Surprise. In this example, sup-\\npose that the difference between the two expressive faces\\nis decomposed into the two following differences: 1) an\\nimaginary neutral face subtracted from the query face (of\\nthe same subject) with Disgust, and 2) the intra-class vari-\\nation image with Surprise, subtracted from the neutral\\nf a c e .B a s e do nt h er e s u l tr e p o r t e di n[ 7 ] ,w ec a ns e et h a t\\neach of the both difference information can reduce iden-\\ntity information and emphasize the expression as well.\\nTherefore, the combination of the two differences (that\\ncorresponds to the difference between the two expressive\\nfaces with Disgust and Surprise) could also reduce the\\nidentity information and emphasize the expression. This\\nresults in the reduced intra-class variation and the\\nincreased inter-class variation without using neutral face.\\nFig. 3 illustrates examples of intra-class variation images\\nand the corresponding subtraction images for expressive\\n(e.g., disgust) face images belonging to four distinct sub-\\njects. As shown in Fig. 3, the appearances of the subtraction\\nimages have similar pattern along the subjects. This indi-\\ncates that the difference between query face image and\\nintra-class variation image, i.e., proposed feature, could be\\ndiscriminating for FER irrespective of subjects at hand\\n(please see Tables 3 and 4 in Section 3). It is noted that the\\nsubjects in training set are used to collaboratively represent\\nFig. 2. Examples of intra-class variation images of neutral (a) and surprise (b) expressions for the ﬁve query face images, respectively. In these\\nexamples, 50 training face images of neutral and surprise expressions are used for generating the corresponding intra-class variation images,\\nrespectively. For schematic description, three top training face images are present in order based on their contributions to the generation of intra -\\nclass variation image.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 343\\nthe query face image [21]. Using the collaborative rep-\\nresentation [21], the intra-class variation image could resem-\\nble the query face image in identity, even when the subject\\nin the query face image does not exist in the training set\\n(e.g., the subjects in the third and fourth rows in Fig. 3).\\nAlso, as observed in Fig. 3, the subtraction image derived\\nby the intra-class variation image of a particular expression\\ntype (e.g., Neutral) looks different from those derived by\\nthe intra-class variation images of the other expression\\ntypes (e.g., Smile, Surprise, Squint, and Scream). This obser-\\nvation indicates that difference information derived by\\ndifferent expression states could be complementary for clas-\\nsifying facial expression of query face image (please refer to\\nthe results in Fig. 6 in Section 3.2 to see the effectiveness of\\nthe proposed method).\\n2.2 Sparse Representation Classiﬁcation Using\\nProposed Expression Features\\nIn this section, we present the classiﬁcation approach based\\non the sparsity of the proposed expression features. Let\\nys\\ni2<Nði¼1;...;CÞdenote the expression features from\\n8s2fti;1;ti;2;...;ti;Mii¼1;...;C j g;(i.e., M¼PC\\ni¼1Mi\\ntraining face images contained in the training set FFÞ:As is\\nseen in Fig. 1b, ys\\niis obtained by computing the differencebetween sand its intra-class variation image hs\\niof the ith\\nexpression. Note that hs\\niis generated by using FFiin a similar\\nway to the calculation of hq\\ni(refer to (1) and (2)). Using the\\ntraining set FF;we construct the dictionary Ai2<N/C2Mði¼\\n1;...;CÞwhere Aiconsists of the expression features ys\\niof\\nthe face images in the training set FF(see Fig. 1b).\\nFor classifying a given query face image q;we perform\\nthe sparse representation for the query expression feature\\nyq\\niði¼1;...;CÞ:The query expression feature yq\\nican\\napproximately lie in the linear space spanned by the train-\\ning expression features within dictionary Ai:\\nyq\\ni¼Aix0\\niþni: (4)\\nIn (4), x0\\ni2<Mrepresents a sparse solution vector\\nwhose entries are zeros except for those associated with\\nt h et r u ee x p r e s s i o nc l a s s .T h e ni2<Nis a noise term\\nbounded by a small amount of energy \"(i.e., nikk2/C20\").\\nTo efﬁciently obtain sparse solution vector x0\\nidealing\\nwith NP-hard problem [2], the ‘1-norm minimization can\\nbe computed as follows [2]:\\nx0\\ni/C25^xi¼arg min xikk1;s : t : yq\\ni/C0Aixi/C13/C13/C13/C13\\n2/C20\": (5)\\nFig. 3. Examples of the intra-class variation images (i.e., hq\\ni) and the corresponding subtraction images (i.e., yq\\niÞfor the query face images (Disgust)\\nof four distinct subjects for (a) i¼Neutral , (b) i¼Smile , (c) i¼Surprise , (d) i¼Squint , (e) i¼Scream . Note that the subjects of the query faces\\nimages in the third and fourth rows do not exist in training images.344 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\nIn this paper, we adopt regularized orthogonal matching\\npursuit (ROMP) technique [30] to solve the ‘1-norm minimi-\\nzation problem deﬁned in (5).\\nTo take advantage of difference information obtained\\nusing intra-class variation images of distinct expression\\nstates, we combine the individual difference information\\nalong with different expressions. For this, we combine\\nmultiple difference information at the decision level. The\\ncombined sparse solution vector ^xcomis then computed\\nas the normalized sum of the individual sparse solution\\nvectors ^xi[22]:\\n^xcom¼XC\\ni¼1ð^xi=^xikk2Þ: (6)\\nEach sparse solution vector ^xiin (6) makes equal contribu-\\ntion to the combined sparse solution vector ^xcomwith the\\nnormalization term ( ^xikk2).\\nFinally, we perform the classiﬁcation for qin question,\\nbased on density of sparse coefﬁcients of the combined\\nsparse solution vector ^xcom:Speciﬁcally, the expression class\\nlabel i/C3can be determined by ﬁnding the expression class\\nfor which the maximum of the average sparse coefﬁcient is\\nachieved [22]:\\ni/C3¼arg maxC\\ni¼11\\nMiXMi\\nj¼1xcom\\ni;j; (7)\\nwhere xcom\\ni;jis the element (i.e., sparse coefﬁcient) of the\\ncombined sparse solution vector ^xcom;that corresponds\\nto the features for the jth training face image of the ith\\nexpression class.\\n3E XPERIMENTS\\nTo evaluate our proposed method, we used ﬁve public DBs,\\ni.e., JAFFE [18], Extended Cohn-Kanade (CK þ) [37], MMI\\n[38], BU-3D FE [20], and CMU Multi-PIE [19]. Some exam-\\nple face images are shown in Fig. 4. Following the recom-\\nmendation in [33], each face image used in our experiments\\nwas cropped based on two eye locations. In this paper, two\\neye locations were manually determined. The cropped face\\nimage was rescaled to the size of 64 /C264 pixels.\\n3.1 Comparisons with State-of-the-Art Sparse\\nRepresentation Based FER Methods\\nIn this experiment, we compared the proposed FER method\\nwith other state-of-the-art FER method based on SRC. In\\ngeneral, there are two possible FER scenarios in practical\\nFER-based applications: 1) person-dependent FER [34] and\\n2) person-independent FER [34]. The comparative experi-\\nments have been conducted under the two different FER\\nscenarios.\\nThe experiments for testing person-dependent recogni-\\ntion were performed using JAFFE DB, MMI DB, and BU-\\n3DFE DB. The dataset constructions were done as follows:\\n1) JAFFE DB: a total of 213 face images (i.e., 30, 30, 29,\\n32, 31, 31, 30 face images for Neutral, Anger, Disgust,\\nFear, Happiness, Sadness, Surprise, respectively)\\nfrom 10 subjects were used. The face images from\\nthe neutral expression were used as the neutral states\\nof faces for extracting the feature in [7]. In order toextract the feature in [7], the neutral face image with\\nthe ﬁle name including ‘NE1’ [31] in JAFFE DB was\\nused for each subject. For the comparison, the same\\nsix-expression classiﬁcation (without the neutral\\nexpression) was performed for the comparison meth-\\nods [4], [5], [7], [28], [36] and the proposed method.\\nFor each of the six expressions, we used a random\\npartition so that 25 face images were used for the\\ntraining set and the remaining face images (i.e., ﬁve,\\nfour, seven, six, six, ﬁve face images for Anger, Dis-\\ngust, Fear, Happiness, Sadness, Surprise, respec-\\ntively) were used for the test set.\\n2) MMI DB: a total of 150 image sequences (25 sequen-\\nces/C26 expressions) were collected from 21 subjects.\\nFollowing the method in [39], the ﬁrst frame with\\nneutral face and three peak frames of each sequence\\n(hence 450 expressive face images and 150 neutral\\nface images in total) were used for the six-class\\nexpression recognition. One hundred ﬁfty neutral\\nface images were used for determining the neutral\\nstates of faces for extracting the features in [7]. In\\neach of the seven expressions, there were 75 ( ¼25\\nsequences /C23 frames) expressive face images. We\\nFig. 4. Example face images used in our experiments. (a) JAFFE DB. (b)\\nCKþDB. (c) MMI DB. (d) BU-3DFE DB. (e) CMU Multi-PIE DB.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 345\\nused a random partition so that 40 face images were\\nused for the training set and the remaining 35 face\\nimages were used for the test set.\\n3) BU-3DFE DB: this DB has been known to be a\\nchallenging and difﬁcult mainly due to a variety\\nof ethnic/racial ancestries and expression intensity\\n[20]. A total of 2,400 expressive face images (4\\nintensities x 6 expressions x 100 subjects) and 100\\nneutral face images (each of which is for one sub-\\nj e c t )[ 2 0 ]w e r eu s e d .H u n d r e dn e u t r a lf a c ei m a g e s\\nof 100 subjects were used for determining the neu-\\ntral states of faces for extracting the features of [7].\\nBy using a random partition, following the\\nmethod in [27], 1,200 face images were used for\\nthe training set and the remaining 1,200 face\\nimages were used for the test set. In the experi-\\nment with the proposed method, the training face\\nimages with sufﬁciently high intensities were\\nselected and used for generating intra-class varia-\\ntion images. For the sake of fair comparison, the\\nface images of all the four intensities [20] were\\nincluded in the test after the random partition.\\nThe evaluation for person-independent recognition was\\nperformed using CK þDB and CMU Multi-PIE DB, where\\nthe subjects in training set were completely different from\\nthe subjects within test set (i.e., the subjects used for training\\nprocedure cannot be used for testing phase). The dataset\\nconstructions were done as follows:\\n1) CK þDB: this DB includes 593 image sequences from\\n123 subjects. From the 593 sequences, we selected 325\\nsequences of 118 subjects, which meet the criteria for\\none of the seven emotions [37]. The selected 325\\nsequences consist of 45, 18, 58, 25, 69, 28, and 82\\nsequences of Angry, Contempt, Disgust, Fear,\\nHappy, Sadness, and Surprise, respectively [37]. Sim-\\nilar to the method in [42], 325 expressive face images\\n(1 peak frame /C2325 sequences) were collected for\\ntraining set and test set. For the method in [7], 325\\nneutral face images (each of which is obtained from\\nthe ﬁrst frame of a sequence) were employed for the\\npurpose of extracting the features of 325 expressive\\nface images. Prior to dataset partitioning into training\\nset and test set, a neutral face image was used as the\\nneutral state of the expressive face image in the\\nsequence during the extraction of the feature in [7].\\nSimilar to the methods in [44], [45], [46], [47], Leave-\\none-subject-out (LOSO) cross validation was adopted\\nin the evaluation. LOSO selected 117 out of the 118\\nsubjects for training and used the remaining subject\\nfor test. This procedure was repeated for all the 118\\nsubjects. The number of training data varies from 319\\nto 324 during the cross validation.\\n2) CMU Multi-PIE: By using a random partition, for\\neach of the six expressions (Neutral, Smile, Surprise,\\nSquint, Disgust, and Scream), 70 subjects were used\\nfor the training set while the rest of 18 subjects were\\nused for the test set. Because a subject had one or\\ntwo face images in each expression class, the num-\\nbers of the training face images and the testing face\\nimages in the expression class varied with therandom partitions. Note that CMU Multi-PIE con-\\ntains face images captured under 19 different illumi-\\nnation conditions [19]. Hence, the face dataset from\\nthe CMU Multi-PIE contained variation in illumina-\\ntion (see Fig. 4b), which made FER more challenging.\\nFor comparison purposes, pixel-based appearance fea-\\nture and three widely used texture-based features (i.e., LBP,\\nGabor wavelet, and LPQ) were implemented. For extracting\\nLBP features used in [4] and [36], we adopted a uniform\\nLBP operations [8] with parameters of P¼8,R¼1[4]. For\\nextracting LPQ, we used 5/C25neighborhoods [28]. As sug-\\ngested in [4], [28], and [36], histograms were extracted from\\n15 local regions and they were concatenated to form a face\\nfeature. For obtaining the Gabor wavelet representation,\\nﬁve scales and eight orientations were used to construct a\\nset of Gabor ﬁlter banks [5].\\nBefore constructing the dictionaries, we reduced the fea-\\nture dimensions by using Eigenfaces [23]. The dimension\\nreduction was performed in the following way. The feature\\n(e.g., the proposed method which is the subtraction of the\\nintra-class variation image from the query face image, or\\nLBP feature vectors in [4]) extracted from every training face\\nimage of a database (e.g., 150 face images for JAFFE DB) was\\nused to generate a projection matrix (or a set of Eigenvec-\\ntors). To reduce feature dimension, we selected 150 signiﬁ-\\ncant Eigenvectors corresponding to the largest Eigenvalues.\\nFor raw pixel [4], LBP [4], Gabor [5], LPQ [28], and the feature\\nin [7], the dimension of the single feature vector of a face\\nimage was reduced by using the 150 Eigenvectors. For the\\nproposed method, before obtaining the sparse solution ^xi;\\nthe dimension of each query expression feature yq\\ni\\nði¼1;...;CÞwas independently reduced by using the asso-\\nciated 150 Eigenvectors. Using 150 Eigenvectors was able to\\nproduce the 150-dimensional feature vector. However, in the\\ncase of JAFFE DB, we discarded the 150th dimension because\\nonly 149 (i.e., the number ( ¼150) of training samples minus\\none) Eigenvectors have nonzero values [23]. Therefore, the\\nfeature dimension for JAFFE DB was 149 and the feature\\ndimension for the remaining DBs (i.e., CK þ, MMI, CMU\\nMulti-PIE, and BU-3DFE) was 150. For raw pixel [4], LBP [4],\\nGabor [5], LPQ [28], the feature in [7], as well as the proposed\\nmethod, classiﬁcation was performed using the density of\\nsparse coefﬁcients. In addition, for the fusion method [36] of\\n‘SRC with raw pixel’ and ‘SRC with LBP’, we strictly fol-\\nlowed the method in [36]. Speciﬁcally, if the classiﬁcation\\nresults of ‘SRC with raw pixel’ and ‘SRC with LBP’ are same\\nfor a given test face image, then the ﬁnal class label is\\nunchanged [36]. If the class labels are different, then we\\nselected the classiﬁcation result with the larger ratio of the\\nsmallest residual to the second smallest residual [36].\\nIn the case of JAFFE, MMI, CMU Multi-PIE, and BU-3DFE\\nDBs, performance measurements presented in the experi-\\nments were averaged over 20 times. In addition, In the case\\nof CK þDB which uses LOSO method, and the resulting rec-\\nognition rates computed for the 118 folds were averaged.\\nThe comparative results are given in Tables 1 and 2 for\\nperson-dependent and person-independent recognition sce-\\nnarios, respectively. As observed in both tables, SRC with\\npixel-based appearance feature shows poor performances\\n(i.e., less than 60 percent in person-independent recognition\\nunder illumination change with CMU Multi-PIE DB). On346 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\nthe other hand, the proposed method shows signiﬁcantly\\nbetter performances (i.e., about 79 percent in CMU Multi-\\nPIE DB). In addition, in both person-dependent and person-\\nindependent recognition scenarios, the proposed method\\noutperforms pixel-based SRC and SRC with three widelyused texture-based appearance features (i.e., LBP, Gabor,\\nand LPQ). Note that the feature of subtraction of a neutral\\nface image [7] could not be evaluated on person-indepen-\\ndent recognition using CMU Multi-PIE DB because the sub-\\nject of a query face image did not exist in training faceTABLE 1\\nRecognition Rates Evaluated under Person-Dependent Recognition Scenario\\n(a)\\nFER method Recognition rate (%)\\nAnger Disgust Fear Happiness Sadness Surprise Overall\\nProposed 97.00 98.75 98.57 95.83 86.67 92.00 94.70\\nSRC þRaw pixel [4] 86.00 88.75 88.57 88.33 75.83 84.00 85.15\\nSRC þLBP [4] 89.00 93.75 94.27 93.33 80.83 91.00 90.30\\nSRC þRawLBP [36] 96.00 85.00 94.27 93.33 76.67 92.00 89.70\\nSRC þGabor [5] 92.00 93.75 95.71 94.17 81.67 90.00 91.21\\nSRC þLPQ [28] 93.00 95.00 96.43 94.17 79.17 93.00 91.67\\nSRC þSubtraction of a neutral face image [7] 94.00 93.75 91.43 85.83 78.33 94.00 89.09\\n(b)\\nProposed 93.14 92.57 93.14 96.57 89.71 97.71 93.81\\nSRC þRaw pixel [4] 77.14 89.71 80.57 92.57 86.29 96.00 87.05\\nSRC þLBP [4] 89.14 97.14 82.29 97.14 88.00 90.86 90.76\\nSRC þRawLBP [36] 94.29 86.29 92.57 96.00 84.57 93.71 91.24\\nSRC þGabor [5] 84.57 89.71 88.00 97.71 89.14 95.43 90.76\\nSRC þLPQ [28] 88.57 92.57 80.57 99.43 92.00 97.71 91.81\\nSRC þSubtraction of a\\nneutral face image [7] 91.43 91.43 82.29 92.57 92.57 98.86 91.52\\n(c)\\nProposed 82.63 89.13 83.30 89.28 87.40 95.35 87.85\\nSRC þRaw pixel [4] 57.65 51.53 51.38 74.33 57.43 82.38 62.45\\nSRC þLBP [4] 63.50 70.88 61.90 82.28 75.13 86.80 73.42\\nSRC þRawLBP [36] 74.48 82.53 80.55 81.10 82.85 87.63 81.52\\nSRC þGabor [5] 73.98 73.88 68.10 82.45 72.03 87.55 76.33\\nSRC þLPQ [28] 72.03 84.25 76.30 90.58 83.63 86.33 82.19\\nSRC þSubtraction of a neutral face image [7] 72.45 73.43 71.15 83.43 70.58 87.50 76.42\\n(a) JAFFE DB. (b) MMI DB. (c) BU-3DFE DB.\\nTABLE 2\\nRecognition Rates Evaluated under Person-Independent Recognition Scenario\\n(a)\\nFER method Recognition rate (%)\\nAngry Contempt Disgust Fear Happy Sadness Surprise Overall\\nProposed 84.44 77.78 91.38 80.00 98.55 67.86 100.00 90.47\\nSRC þRaw pixel [4] 68.89 66.67 87.93 52.00 95.65 57.14 95.12 80.71\\nSRC þLBP [4] 75.56 55.56 87.93 72.00 98.55 64.29 100.00 85.07\\nSRC þRawLBP [36] 73.33 88.89 84.48 68.00 98.55 67.86 95.12 86.65\\nSRC þGabor [5] 84.44 66.67 98.28 68.00 97.10 64.29 98.78 88.67\\nSRC þLPQ [28] 82.22 55.56 100.00 68.00 100.00 50.00 100.00 87.13\\nSRC þSubtraction of a neutral face image [7] 62.22 72.22 84.48 68.00 98.55 42.86 96.34 81.48\\n(b)\\nFER method Recognition rate (%)\\nNeutral Smile Surprise Squint Disgust Scream Overall\\nProposed 70.67 85.98 89.98 61.66 66.36 97.49 78.72\\nSRC þRaw pixel [4] 48.15 58.06 63.47 39.14 48.15 85.09 57.49\\nSRC þLBP [4] 54.66 75.38 79.88 51.25 64.57 89.79 69.15\\nSRC þRawLBP [36] 54.59 77.30 83.24 58.92 67.03 94.59 72.61\\nSRC þGabor [5] 59.93 79.75 84.25 50.02 63.53 94.16 71.74\\nSRC þLPQ [28] 58.32 79.04 81.10 49.41 67.44 93.58 71.48\\n(a) CK þDB. (b) CMU Multi-PIE DB.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 347\\nimages, thus the neutral face image of the corresponding\\nsubject was not available.\\nTo further verify the usefulness of our proposed features\\nin SRC, we compared the sparsity of the proposed method\\nwith the comparison methods using the sparsity concentra-\\ntion of the true class (SCTC) [35], which can be written as\\nSCTC ðxÞ¼dtrueðxÞ kk1\\nxkk1; (8)\\nwhere xdenotes a sparse solution vector. dtrueð/C1Þdenotes a\\ncharacteristic function that selects coefﬁcients associated\\nwith true class. The value of SCTC ðxÞbecomes 1 if non-zero\\nsparse coefﬁcients are only concentrated on true class, while\\nSCTC ðxÞis equal to 0 if non-zero sparse coefﬁcients do not\\nexist in true class. The measurement in (8) quantiﬁes how\\nmuch the coefﬁcients are concentrated on true class for a\\nquery expression feature. It reﬂects true class in facial\\nexpression recognition.\\nFrom Tables 3 and 4, we can observe that the correspond-\\ning SCTC values of the proposed method are higher than\\nthose of the comparison methods for the both recognition\\nscenarios. These results show that the proposed method\\nworks well for realizing sparsity based classiﬁcation\\nscheme, resulting in high recognition rates as demonstrated\\nin Tables 1 and 2.\\n3.2 Analysis of the Effectiveness of Proposed FER\\nMethod\\nIn order to verify the usefulness of the proposed method,\\nwe investigated the effect of number of subjects used in gen-\\nerating intra-class variation image. Recognition rates for the\\nproposed method were measured with seven different\\nnumbers of subjects. The face dataset from CMU Multi-PIE\\nDB was used in this experiment. In 70 subjects in the train-\\ning set, training face images of a particular number of sub-jects were randomly selected for generating intra-class vari-\\nation images. In constructing dictionaries in SRC, the fea-\\ntures extracted from the training face images of all the 70\\nsubjects were used.\\nFig. 5 shows experimental results of recognition rates for\\nthe proposed method with respect to the number of subjects\\nused in generating intra-class variation images. For a larger\\nnumber of subjects (e.g., 70), intra-class variation image\\ncould be more accurately approximated because of increas-\\ning possibility of the presence of the similar faces to a query\\nface in identity and illumination. This yields increasing rec-\\nognition rate as shown in Fig. 5. Even when small number\\nof subjects (e.g., 10) are used in generation of intra-class var-\\niation images, the proposed method still yields good and\\nstable recognition rate which is higher than those for SRC\\nbased FER methods with LBP, Gabor, and LPQ features\\n(e.g., recognition rates are 69.15, 71.74, and 71.48, respec-\\ntively) as seen in Table 1b. The experimental results demon-\\nstrated that the proposed method could be feasible for the\\nperson-independent recognition even with the limited num-\\nber of subjects available in training set.\\nAnother experiment was performed to investigate the\\neffectiveness of non-neutral expression used for generating\\nintra-class variation image. We separately measured the\\nrecognition rate for each expression of intra-class variation\\nimage with CMU Multi-PIE DB. In experiments, individual\\nsparse solution vector ^xiwas obtained and classiﬁcation\\nusing the single sparse solution vector was performed\\n(instead of the combined sparse solution vector ^xcom).\\nExperimental results are shown in Fig. 6. As is seen, the\\nuse of non-neutral expression for intra-class variation\\nimage is able to yield similar recognition performance to\\nthe case of using neutral expression. Moreover, the recog-\\nnition rate using the combined sparse solution vector ^xcom\\nis higher than that using a single sparse solution vector ^xi\\n(see Fig. 6 for ‘All’). The results show that the intra-class\\nvariation image of an expression (e.g., Neutral) are comple-\\nmentary with those of the other expressions (e.g., Smile,\\nSurprise, Squint, Disgust, and Scream), leading to improve-\\nment of FER.\\nWe measured the computation time for the proposed fea-\\nture extraction and FER. They were implemented using 64\\nbit Matlab programming on a PC with 3.50 GHz Pentium\\nCPU and 32 GB memory. The feature extraction (including\\ndimension reduction) took 0.11 sec for each query face\\nFig. 5. Recognition rates (averaged over 20 times) for the proposed fea-\\ntures with respect to the number of subjects that are used for generating\\nintra-class variation images.\\nTABLE 4\\nComparisons in the Sparse Coefﬁcient of the\\nTrue Class in Person-Independent Recognition\\nScenario (CMU Multi-PIE DB was used)\\nFER method Sparsity SCTC\\nProposed 0.3688\\nSRC þRaw pixel [4] 0.3019\\nSRC þLBP [4] 0.3254\\nSRC þGabor [5] 0.3431\\nSRC þLPQ [28] 0.3368TABLE 3\\nComparisons in the Sparse Coefﬁcient of the True Class\\nin Person-Dependent Recognition Scenario\\n(JAFFE DB was used)\\nFER method Sparsity SCTC\\nProposed 0.3654\\nSRC þRaw pixel [4] 0.2937\\nSRC þLBP [4] 0.3275\\nSRC þGabor [5] 0.3332\\nSRC þLPQ [28] 0.3292\\nSRC þSubtraction of a neutral face image [7] 0.3228348 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\nimage. About 0.15 sec was taken for entire process (includ-\\ning feature extraction and classiﬁcation) of each query face\\nimage, which was feasible.\\n3.3 Cross-Database Evaluation\\nTo investigate the generalization performance of the pro-\\nposed method, we performed an inter-database experi-\\nment. For the experiment, the datasets (used in Section 3.1)\\nfrom MMI DB and CK þDB were used to construct the\\ntraining and test sets. The training and test sets contained\\nsix expression classes, i.e., Anger, Disgust, Fear, Happi-\\nness, Sadness, and Surprise, which were common in both\\nCKþand MMI databases. Note that the expression class\\nContempt of CK þwas not used in the experiment,\\nwhich was not available in MMI database. From MMI DB,\\n450 (3 images /C2150 sequences) expressive face images\\nwere used for constructing training set. From CK þDB, 307\\nexpressive face images of the six expression classes were\\nused for test set while 18 expressive face images of Con-\\ntempt were excluded in the experiment. In addition, 150\\nneutral face images from MMI DB and 307 neutral face\\nimages from CK þDB were used for the neutral states of\\nfaces for extracting the features in [7].\\nExperimental result in Table 5 shows that the proposed\\nmethod achieves the recogni tion rate of 65.47 percent,which is higher than those of the comparison methods.\\nO n ep o s s i b l er e a s o nf o rt h eh i g h e rr e c o g n i t i o nr a t ei st h a t\\nthe proposed FER method has been designed to deal with\\nperson-independent recognition where the subjects of\\nt h eq u e r yf a c ei m a g e s( e . g . ,t h ef a c ei m a g e sf r o mC K þDB)\\nare not present in the training face images (e.g., the face\\nimages from MMI DB). However, the recognition rate is\\nl o w e rt h a nt h er e c o g n i t i o nr a t ei nt h ec a s ew h e r ef a c e\\nimages from a single DB (CK þor MMI) are used for both\\ntraining set and test set. This performance degradation is\\nmainly attributed to the fact that the face images are col-\\nlected under two different controlled conditions. In order\\nto generalize across image acquisition conditions, it is\\nrequired to collect large trai ning datasets with various\\nimage acquisition conditions [40].\\n3.4 Comparisons with Recent Advances\\nThis section presents the comparisons between the result of\\nthe proposed method and the reported results of 8 recently\\nproposed FER methods in [42], [43], [44], [45], [46], [47] on\\nCKþDB. These FER methods were chosen because they\\nused similar testing strategies (e.g., person-independent rec-\\nognition) to that used in the proposed method.\\nTable 6 shows the comparisons of experimental setup and\\nrecognition result between the different FER methods. As is\\nseen in Table 6, the proposed method can achieve higher rec-\\nognition rate than the single frame-based methods in [42],\\nunder 10-fold person-independent cross validation protocolTABLE 5\\nGeneralization Performance on the Two Different DBs\\nFER method Recognition rate (%)\\nProposed 65.47\\nSRC þRaw pixel [4] 52.44\\nSRC þLBP [4] 55.70\\nSRC þRawLBP [36] 59.28\\nSRC þGabor [5] 60.59\\nSRC þLPQ [28] 62.54\\nSRC þSubtraction of a\\nneutral face image [7]58.31\\nCKþDB and MMI DB were used for testing and training, respectively.\\nTABLE 6\\nComparisons with Recent Advances in Expression Recognition on CK þDB\\nMethod Video-based or\\nFrame-basedCross validation No. of subjects No. of expressions Recognition rate (%)\\nProposed frame-based LOSO 118 7/C390.5\\nProposed frame-based 10-fold 118 7/C389.6\\nLDN_K [42] frame-based 10-fold 118 7/C382.3\\nLDN_G [42] frame-based 10-fold 118 7/C389.3\\nAR-LBP [43] frame-based 10-fold not stated 7y83.1\\nSTLMBP-C [44] video-based LOSO 118 7/C392.3\\nSTLMBP-J [44] video-based LOSO 118 7/C392.6\\nFeature tracking [45] video-based LOSO 90 6z87.4\\nAvatar image [46] video-based LOSO 123 7/C382.6\\nMCF [47] video-based LOSO 123 7/C389.4\\n“LOSO” denotes leave-one-subject-out cross validation.\\n“10-fold” denotes 10-fold person-independent cross validation.\\n/C3six basic expressions þcontempt.\\nysix basic expressions þneutral (contempt is excluded).\\nzsix basic expressions (contempt is excluded).\\nFig. 6. Comparisons of recognition rate (averaged over 20 times) with sin-\\ngle expression used for generating intra-class variation images. ‘All’ means\\nthe recognition rate when using the combined sparse solution vector ^xcom.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 349\\n(for details on the 10-fold person-independent cross-\\nvalidation protocol, please refer to [42]). The recognition\\nrates for ﬁve video-based FER methods are also presented in\\nTable 6. The proposed method does not outperform some\\nvideo-based FER methods (e.g., STLMBP-J [44]). In this\\npaper, the proposed method is limited to using a single\\nframe of a video sequence. However, it should be noted that\\nthe proposed method could be further improved by incorpo-\\nrating the temporal information present in a video sequence.\\nFor example, to exploit temporal information, the multiple\\nquery face images within a temporal window can be fused\\nby using a simple and effective technique (e.g., local average\\n[48]), which is followed by the generations of intra-class var-\\niation images and the extractions of the expression features\\nusing subtraction. In this way, we would expect a higher rec-\\nognition rate for the proposed method, which could be com-\\nparable to the best reported results in Table 6.\\nAlthough some results in Table 6 cannot be directly com-\\npared due to different experimental setups, different\\nexpression classes, different preprocessing methods (e.g.,\\nface alignment) and so on, it is demonstrated that the pro-\\nposed method can yield a feasible and promising recogni-\\ntion rate (around 90 percent) even with static facial images\\nunder person-independent recognition scenario.\\n4C ONCLUSION\\nIn this paper, we proposed a new sparse representation\\nbased facial expression recognition method which aims to\\ndeal with intra-class variations in FER. For a given query\\nface image, we generated an intra-class variation image\\nusing training face images of each expression. The pro-\\nposed intra-class variation image had similar appearance\\nwith the query face image in terms of identity and illumi-\\nnation, while representing the same expression as the asso-\\nciated training face images. As the features of sparse\\nrepresentation, we used the differences between the query\\nface image and its intra-class variation images, which\\nare effective for FER. The reasons for the effectiveness of\\nthe proposed method can be summarized as follows: 1)\\nthe difference information obtained by using the intra-\\nclass variation image of a non-neutral expression could be\\nused for reducing identity information (which is similar\\nwith the normalization effect obtained by using the neutral\\nexpression state); 2) the diff erence information could also\\nprovide robustness to illumination variation; 3) the multi-\\nple differences derived by the different expression states\\navailable in training data could provide complementary\\ninformation to maximize discr iminating capability of the\\nproposed method. Experimental results showed that the\\nproposed method was able to y ield acceptable FER perfor-\\nmance even when the subjects in query face images were\\nnot available in training images, or even in the absence of\\ntheir neutral face images.\\nIn this paper, the face images used in the experiment\\nwere cropped manually. Thus, we assumed that well-\\naligned face images were given for the proposed method.\\nFor future work, we will consider incorporating a robust\\nalignment method (such as face alignment by using a\\ndeformable sparse recovery [41]) to realize reliable auto-\\nmatic FER system.ACKNOWLEDGMENTS\\nThis research was funded by the MSIP (Ministry of Science,\\nICT and Future Planning), Korea in ICT R&D Program\\n2014. Yong Man Ro is the corresponding author.\\nREFERENCES\\n[1] I. Kotsia and I. Pitas, “Facial expression recognition in image\\nsequences using geometric deformation features and support vec-\\ntor machines,’’ IEEE Trans. Image Process. , vol. 16, no. 1, pp. 172–\\n187, Jan. 2007.\\n[2] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma, “Robust face\\nrecognition via sparse representation,” IEEE Trans. Pattern Anal.\\nMach. Intell. , vol. 30, no. 2, pp. 210–227, Feb. 2009.\\n[3] X. Yuan, “Visual classiﬁcation with multi-task joint sparse repre-\\nsentation,” in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog. ,\\n2010, pp. 3493–3500.\\n[4] M. Huang, Z. Wang, and Z. Ying, “A new method for facial\\nexpression recognition based on sparse representation plus LBP,”\\ninProc. IEEE Int. Cong. Image Sig. Process. , 2010, pp. 1750–1754.\\n[5] S. Zhang, X. Zhao, and B. Lei, “Robust facial expression recogni-\\ntion via compressive sensing,” Sensors , vol. 12, no. 3, pp. 3747–\\n3761, 2012.\\n[6] Z. Wang, M. Huang, and Z. Ying, “The performance study of facial\\nexpression recognition via sparse representation,” presented at the\\n9th Int. Conf. Mach. Learn. Cybern., Qingdao, China, 2010.\\n[7] S. Zafeiriou and M. Petrou, “Sparse representation for facial\\nexpression recognition via l1 optimization,” in Proc. IEEE Int.\\nConf. Comput. Vis. Pattern Recog. , 2010, pp. 32–39.\\n[8] T. Ahonen, A. Hadid, and M. Pietik €aznen, “Face description with\\nlocal binary pattern: Application to face recognition,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 28, no. 12, pp. 2037–2041, Dec. 2006.\\n[9] L. Shen, L. Bai, and M. C. Fairhurst, “Gabor wavelets and general\\ndiscriminant analysis for face identiﬁcation and veriﬁcation,”\\nImage Vis. Comput. , vol. 25, no. 5, pp. 553–563, 2007.\\n[10] N. Sebe, M. S. Lew, Y. Sun, I. Cohen, T. Gevers, and T. S. Huang,\\n“Authentic facial expression analysis,” Image Vis. Comput. , vol. 25,\\nno. 12, pp. 1856–1863, Dec. 2007.\\n[11] M. S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J.\\nMovellan, “Recognizing facial expression: Machine learning and\\napplication to spontaneous behavior,” in Proc. IEEE Comput. Soc.\\nInt. Conf. Comput. Vis. Pattern Recog. , 2005, pp. 568–573.\\n[12] J. Yang, D. Zhang, A. F. Frangi, and J. Y. Yang, “Two-dimensional\\nPCA: A new approach to appearance-based face representation\\nand recognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 26,\\nno. 1, pp. 131–137, Jan. 2004.\\n[13] J. Bouguet, “Pyramidal implementation of the Lucas-Kanade fea-\\nture tracker: Description of the algorithm,” OpenCV Documenta-\\ntion, Santa Clara, CA: Intel Corp., Microprocess. Res. Labs, 1999.\\n[14] Y. C. Lim, M. Lee, C. H. Lee, S. Kwon, and J. Lee, “Integrated posi-\\ntion and motion tracking method for online multi-vehicle tracking-\\nby-detection,” Opt. Eng. , vol. 50, no. 7, pp. 077203-1–077203-10, 2011.\\n[15] Y. L. Tian, T. Kanade, and J. F. Cohn, “Facial expression analysis,”\\ninHandbook of Face Recognition , S.Z. Li, and A.K. Jain, eds. New\\nYork, NY, USA: Springer, 2005, pp. 247–276.\\n[16] G. Donato, M. Stewart, J. C. Hager, P. Ekman, and T. J. Sejnowski,\\n“Classifying facial actions,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 21, no. 10, pp. 974–989, Oct. 1999.\\n[17] J. J. Bazzo and M. V. Lamar, “Recognizing facial actions using\\nGabor wavelets with neutral face average difference,” in Proc.\\nIEEE Int. Conf. Autom. Face Gesture Recog. , 2004, pp. 505–510.\\n[18] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial\\nexpressions with gabor wavelets,” in Proc. IEEE Int. Conf. Autom.\\nFace Gesture Recog. , 1998, pp. 200–205.\\n[19] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-\\nPIE,” Image Vis. Comput. , vol. 28, no. 5, pp. 807–813, 2010.\\n[20] Y. Lijun, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, “A 3D facial\\nexpression database for facial behavior research,” in Proc. IEEE\\nInt. Conf. Autom. Face Gesture Recog. , 2006, pp. 211–216.\\n[21] L. Zhang, M. Yang, X. C. Feng, Y. Ma, and D. Zhang,\\n“Collaborative representation based classiﬁcation for face recog-\\nnition,” arXiv:1204.2358, 2012.\\n[22] R. Min and J. Dugelay, “Improved combination of LBP and sparse\\nrepresentation based classiﬁcation (SRC) for face recognition,” in\\nProc. IEEE Int. Conf. Multimedia Expo. , 2011, pp. 1–6.350 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 5, NO. 3, JULY-SEPTEMBER 2014\\n[23] M. A. Turk and A. P. Pentland, “Eigenfaces for recognition,”\\nJ. Gogn. Neurosci. , vol. 3, no. 1, pp. 71–86, 1991.\\n[24] C. H. Chan, J. Kittler, N. Poh, T. Ahonen, and M. Pie-\\ntik€aznen, “(Multiscale) local phase quantization histogram\\ndiscriminant analysis with score normalisation for robust face\\nrecognition,” in P r o c .I E E EI n t .C o n f .C o m p u t .V i s .W o r k s h o p ,\\n2009, pp. 633–640.\\n[25] S. M. Lajevardi and Z. M. Hussain, “Automatic facial expression\\nrecognition: Feature extraction and selection,” Sig., Image Video\\nProcess. , vol. 6, no. 1, pp. 159–169, 2012.\\n[26] S. M. Lajevardi and Z. M. Hussain, “Emotion recognition from\\ncolor facial images based on multilinear image analysis and log-\\nGabor ﬁlters,” in Proc. 25th Int. Conf. Imag. Vis. Comput. , 2010,\\npp. 1–6.\\n[27] S. M. Lajevardi and H. R. Wu, “Facial expression recognition in\\nperceptual color space,” IEEE Trans. Image Process. , vol. 21, no. 8,\\npp. 3721–3733, Aug. 2012.\\n[28] W. Zhen and Y. Zilu, “Facial expression recognition based on local\\nphase quantization and sparse representation,” in Proc. IEEE Int.\\nConf. Nat. Comput. , 2012, pp. 222–225.\\n[29] S. H. Lee, H. Kim, K. N. Plataniotis, and Y. M. Ro, “Using color\\ntexture sparsity for facial expression recognition,” in Proc. IEEE\\nInt. Conf. Autom. Face Gesture Recog. , 2013, pp. 1–6.\\n[30] D. Needell and R. Vershynin, “Uniform uncertainty principle and\\nsignal recovery via regularized orthogonal matching pursuit,”\\nFound. Comput. Math. , vol. 9, no. 3, pp. 317–334, 2009.\\n[31] [Online]. Available: http://www.kasrl.org/jaffe_info.html, 1997.\\n[32] K. Anderson and P. W. McOwan, “A real-time automated system\\nfor the recognition of human facial expressions,” IEEE Trans.\\nSyst., Man, Cybern. Part B , vol. 36, no. 1, pp. 96–105, Feb. 2006.\\n[33] Y. Tian, “Evaluation of face resolution for expression analysis,” in\\nProc. IEEE Int. Conf. Comput. Vis. Pattern Recog. Workshop , 2004, p. 82.\\n[34] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, “A survey of\\naffect recognition methods: Audio, visual, and spontaneous\\nexpressions,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 31, no. 1,\\npp. 39–58, Jan. 2009.\\n[35] W. Eom, W. D. Neve, and Y. M. Ro, “Sparse feature analysis for\\ndetection of clustered microcalciﬁcations in mammogram\\nimages,” in Proc. Int. Forum Med. Imaging Asia , 2012, p. 4.\\n[36] Z. L. Ying, Z. W. Wang, and M. W. Huang “Facial expression rec-\\nognition based on fusion of sparse representation,” in Proc. Adv.\\nIntell. Comput. Theories Appl., 6th Int. Conf. Intel-l. Comput. , Chang-\\nsha, China, 2010, pp. 457–464.\\n[37] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, and Z. Ambadar, “The\\nextended Cohn-Kanade dataset (CK þ): A complete dataset for\\naction unit and emotion-speciﬁed expression,” in Proc. IEEE Int.\\nConf. Comput. Vis. Pattern Recog. , 2010, pp. 94–101.\\n[38] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, “Web-based\\ndatabase for facial expression analysis,” in Proc. IEEE Int. Conf.\\nMultimedia Expo. , 2005, pp. 317–321.\\n[39] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recogni-\\ntion based on Local Binary Patterns: A comparative study,” Image\\nVis. Comput. , vol. 27, no. 6, pp. 803–816, 2009.\\n[40] G. Littlewort, M. S. Bartlett, I. Fasel, J. Susskind, and J. Movellan,\\n“Dynamics of facial expression extracted automatically from vid-\\neo,” Image Vis. Comput. , vol. 24, no. 6, pp. 615–625, 2006.\\n[41] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma,\\n“Toward a practical face recognition system: Robust alignment\\nand illumination by sparse representation,” IEEE Trans. Pattern\\nAnal. Mach. Intell. , vol. 34, no. 2, pp. 372–386, Feb. 2012.\\n[42] A. R. Rivera, J. R. Castillo, and O. Chae, “Local directional number\\npattern for face analysis,” IEEE Trans. Image Process. , vol. 22, no. 5,\\npp. 1740–1752, May 2013.\\n[43] C. L. S. Naika, S. S. Jha, P. K. Das, and S. B. Nair, “Automatic facial\\nexpression recognition using extended ar-lbp,” Wireless Netw.\\nComput. Intell. , vol. 292, no. 3, pp. 244–252, 2012.\\n[44] X. Huang, G. Zhao, W. Zheng, and M. Pietik €ainen,\\n“Spatiotemporal local monogenic binary patterns for facial\\nexpression recognition,” IEEE Signal Process. Lett. , vol. 19, no. 5,\\npp. 243–246, May 2012.\\n[45] Y. Li, S. Wang, Y. Zhao, and Q. Ji, “Simultaneous facial feature\\ntracking and facial expression recognition,” IEEE Trans. Image Pro-\\ncess., vol. 22, no. 7, pp. 2559–2573, Jul. 2013.\\n[46] S. Yang and B. Bhanu, “Understanding discrete facial expressions\\nin video using an emotion avatar image,” IEEE Trans. Syst., Man,\\nCybern. B , vol. 42, no. 4, pp. 980–992, Aug. 2012.[47] S. W. Chew, S. Lucey, P. Lucey, S. Sridharan, and J. F. Cohn,\\n“Improved facial expression recognition via uni-hyperplane clas-\\nsiﬁcation,” in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog. ,\\n2012, pp. 2554–2561.\\n[48] S. W. Chew, R. Rana, P. Lucey, S. Lucey, and S. Sridharan,\\n“Sparse temporal representations for facial expression recog-\\nnition,” in Proc. 5th Paciﬁc Rim Conf. Adv. Image Video Technol. ,\\n2012, pp. 311–322.\\nSeung Ho Lee received the BS degree from\\nDongguk University, Seoul, South Korea, in 2009\\nand the MS degree from the Korea Advanced\\nInstitute of Science and Technology (KAIST),\\nDaejeon, South Korea, in 2011, and is currently\\nworking toward the PhD degree at KAIST. In\\n2011, he was a visiting researcher at the\\nUniversity of Toronto, Toronto, ON, Canada. His\\nresearch interests include face detection/tracking,\\nfacial expression recognition, face recognition,\\nfacial age classiﬁcation, image/video indexing,\\npattern recognition, machine learning, and computer vision. He is a\\nstudent member of the IEEE.\\nKonstantinos N. (Kostas) Plataniotis (S’90-\\nM’92-SM’03-F’12) received the BEng degree in\\ncomputer engineering from the University of\\nPatras, Patras, Greece, in 1988 and the MS and\\nPhD degrees in electrical engineering from Flor-\\nida Institute of Technology, Melbourne, in 1992\\nand 1994, respectively. He is a professor with\\nThe Edward S. Rogers Sr. Department of Electri-\\ncal and Computer Engineering, University of Tor-\\nonto in Toronto, ON, Canada, and an adjunct\\nprofessor with the School of Computer Science,\\nRyerson University, Canada. He is the director of the Knowledge Media\\nDesign Institute with the University of Toronto, where he is also the\\ndirector of Research for the Identity, Privacy and Security Institute. His\\nresearch interests include biometrics, communications systems, multi-\\nmedia systems, and signal and image processing. He is the editor-in-\\nchief (2009-2011) for the IEEE Signal Processing Letters , a registered\\nprofessional engineer in the province of Ontario, and a member of the\\nTechnical Chamber of Greece. He is the 2005 recipient of the IEEE Can-\\nada’s Outstanding Engineering Educator Award “for contributions to\\nengineering education and inspirational guidance of graduate students”\\nand the corecipient of the 2006 IEEE Transactions on Neural Networks\\nOutstanding Paper Award for the published paper in 2003 entitled “Face\\nRecognition Using Kernel Direct Discriminant Analysis Algorithms. He is\\na fellow of the IEEE.\\nYong Man Ro (S’85-M’92-SM’98) received the\\nBS degree from Yonsei University, Seoul and the\\nMS and PhD degrees from the department of\\nelectrical engineering at KAIST. He was a\\nresearcher at Columbia University, a visiting\\nresearcher at the University of California, Irvine,\\nand a research fellow at the University of Califor-\\nnia, Berkeley. He was a visiting professor in the\\ndepartment of electrical and computer engineer-\\ning at the University of Toronto. He is currently a\\nprofessor and the chair of signals and systems\\ngroup of the department of electrical engineering in KAIST. He estab-\\nlished Image and video systems (IVY) Lab at KAIST in 1997. Among the\\nyears he has been conducting research in a wide spectrum of image\\nand video systems research topics. Among those topics; image process-\\ning, computer vision, visual recognition, medical image processing and\\nvideo representation/compression. He received the young investigator\\nﬁnalist award of ISMRM in 1992 and the year’s scientist award (Korea)\\nin 2003. He served as an associate editor for IEEE signal processing let-\\nters. He serves as an associate editor in transitions on data hiding and\\nmultimedia security (Springer-Verlag). He served as a TPC in many\\ninternational conferences including the program chair and organized\\nmany special sessions including “Digital Photo Album Technology” in\\nAIRS 2005, “Social Media” in DSP 2009 and “Human 3D Perception and\\n3D Video Assessments” in DSP 2011.\\n\"For more information on this or any other computing topic,\\nplease visit our Digital Library at www.computer.org/publications/dlib.LEE ET AL.: INTRA-CLASS VARIATION REDUCTION USING TRAINING EXPRESSION IMAGES FOR SPARSE REPRESENTATION BASED... 351\\n',\n",
       " 'Joint Fine-Tuning in Deep Neural Networks\\nfor Facial Expression Recognition\\nHeechul Jung Sihaeng Lee Junho Yim Sunjeong Park Junmo Kim\\nSchool of Electrical Engineering\\nKorea Advanced Institute of Science and Technology\\n{heechul, haeng, junho.yim, sunny0414, junmo.kim} @kaist.ac.kr\\nAbstract\\nTemporal information has useful features for recogniz-\\ning facial expressions. However , to manually design useful\\nfeatures requires a lot of effort. In this paper , to reduce thiseffort, a deep learning technique, which is regarded as a\\ntool to automatically extract useful features from raw data,\\nis adopted. Our deep network is based on two differentmodels. The ﬁrst deep network extracts temporal appear-ance features from image sequences, while the other deepnetwork extracts temporal geometry features from tempo-ral facial landmark points. These two models are combinedusing a new integration method in order to boost the perfor-mance of the facial expression recognition. Through severalexperiments, we show that the two models cooperate witheach other . As a result, we achieve superior performance toother state-of-the-art methods in the CK+ and Oulu-CASIAdatabases. Furthermore, we show that our new integrationmethod gives more accurate results than traditional meth-ods, such as a weighted summation and a feature concate-nation method.\\n1. Introduction\\nRecognizing an emotion from a facial image is a clas-\\nsic problem in the ﬁeld of computer vision, and many stud-ies have been conducted. It can be classiﬁed into two cat-egories: image sequence-based and still image-based ap-proaches. Image sequence-based approach has been used toincrease the recognition performance by extracting usefultemporal features from the image sequences, and the per-formance is usually better than a still image-based approach[15,20,12,17,8]. Both appearance and geometric features\\ncan be used for the spatio-temporal feature [1, 22].\\nWell-known deep learning algorithms, such as the deep\\nneural networks (DNNs) and the convolutional neural net-works (CNNs), have an ability to automatically extract use-ful representations from raw data (e.g., image data). How-ever, there is a limit when applying them directly to facial\\nFigure 1. Overall structure of our approach. The blue and red\\nboxes with dotted lines correspond to the two architectures of the\\ndeep networks. Our two deep networks receive an image sequence\\nand facial landmark points as input, respectively. Conv and FCrefer to the convolutional and fully connected layers. Finally, theoutputs of these networks are integrated using a proposed jointﬁne-tuning method, which is represented in the purple box.\\nexpression recognition databases, such as CK+ [13], MMI\\n[18], and Oulu-CASIA [23]. The major reason is that theamount of data is too small, so a deep network that has manyparameters can easily fall into overﬁtting when training. (Ingeneral, data collection is expensive.) Furthermore, if thetraining data is high dimensional, the overﬁtting problem\\nbecomes more crucial.\\nIn this paper, we are interested in recognizing facial ex-\\npressions using a limited amount of (typically a few hun-\\ndreds of) image sequence data with a deep network. In orderto overcome the problem of having a small amount of data,\\nwe construct two small deep networks that complementeach other. One of the deep networks is trained using image\\nsequences, while the other deep network learns the tempo-ral trajectories of facial landmark points. In other words,the ﬁrst network focuses more on appearance changes of\\nfacial expressions over time, while the second network is di-\\nrectly related to the motion of facial parts. Furthermore, wepresent a new integration method called joint ﬁne-tuning,which performs better than simple weighted summation\\n2015 IEEE International Conference on Computer Vision\\n1550-5499/15 $31.00 © 2015 IEEE\\nDOI 10.1109/ICCV.2015.3412983\\n\\nmethod. Therefore, our main contributions in this paper can\\nbe summarized as follows:\\n•Two deep network models are presented in order to\\nextract useful temporal representations from twokinds of sequential data: image sequences and thetrajectories of landmark points.\\n•We observed that the two networks automatically de-\\ntect moving facial parts and action points, respec-tively.\\n•We presented a joint ﬁne-tuning method integrating\\nthese two networks with different characteristics, and\\nperformance improvement was achieved in terms of\\nthe recognition rates.\\n2. Related Work\\n2.1. Deep Learning-Based Method\\nTypically, a CNN uses a single image, but CNN can\\nalso be used for temporal recognition problems, such as ac-\\ntion recognition [15]. In this 3D CNN method, the ﬁltersare shared along the time axis. Additionally, this methodhas been applied to facial expression recognition with de-formable action part constraints, which is called 3D CNN-DAP [ 11]. The 3D CNN-DAP method is based on 3D CNN\\nand uses the strong spatial structural constraints of the dy-\\nnamic action parts. It could receive a performance boostfrom using the hybrid method, but it falls short of the per-formance of other state-of-the-art methods.\\n2.2. Hand-Crafted Feature-Based Method\\nMany studies in this ﬁeld have been conducted. Tradi-\\ntional local features, such as HOG, SIFT, LBP, and BoWhave been extended in order to be applicable to video, and\\nthese are called 3D HOG [9], 3D SIFT [16], LBP-TOP [24],and BoW [17], respectively. Additionally, there was an at-tempt to improve accuracy through temporal modeling ofeach facial shape (TMS) [6]. They used conditional random\\nﬁelds and shape-appearance features created manually.\\nRecently, spatio-temporal covariance descriptors with\\nthe Riemannian locality preserving projection approach\\nwere developed (Cov3D) [15], and an interval temporal\\nBayesian network (ITBN) for capturing complex spatio-temporal relations among muscles was proposed [20]. Re-cently, expressionlet-based spatio-temporal manifold repre-sentation was developed (STM-ExpLet) [12].\\nIn addition to the ones mentioned above, perceptual\\ncolor space was considered for facial expression [10]. Ap-\\npearance and geometric feature-based approaches were alsoconsidered to be a solution for facial expression recogni-\\ntion, so several approches were developed [1, 22]. Recently,\\nthere is an approach that uses 3D shape model, and they im-proved facial expression recognition rate [8]. They used aZFace algorithm [7] to estimate 3D landmark points, so thealgorithm requires 3D information of face for training.\\n3. Our Approach\\nWe utilize deep learning techniques in order to recognize\\nfacial expressions. Basically, two deep networks are com-bined: the deep temporal appearance network (DTAN) andthe deep temporal geometry network (DTGN). The DTAN,which is based on a CNN, is used to extract the temporalappearance feature necessary for facial expression recog-nition. The DTGN, which is based on a fully connectedDNN, catches geometrical information about the motionof the facial landmark points. Finally, these two modelsare integrated in order to increase the expression recogni-tion performance. This network is called the deep temporalappearance-geometry network (DTAGN). The architectureof our deep network is shown in Figure 1.\\n3.1. Preprocessing\\nIn general, the length of image sequences is variable,\\nbut the input dimension is usually ﬁxed in a deep network.\\nConsequently, the normalization along the time axis is re-quired as input for the networks. We adopt the method in[26], which makes an image sequence into a ﬁxed length.\\nThen, the faces in the input image sequences are detected,cropped, and rescaled to 64× 64. From these detected faces,\\nfacial landmark points are extracted using the algorithm\\ncalled IntraFace [21]. This algorithm provides accurate fa-\\ncial landmark points consisting of 49 landmark points, in-cluding two eyes, a nose, a mouth, and two eyebrows.\\n3.2. Deep Temporal Appearance Network\\nIn this paper, a CNN is used for capturing temporal\\nchanges of appearance. Conventional CNN uses still im-\\nages as input, and 3D CNN was presented recently for deal-\\ning with image sequences. As mentioned in Section 2,\\nthe 3D CNN method shares the 3D ﬁlters along the timeaxis [15]. However, we use the n-image sequences without\\nweight sharing along the time axis. This means that eachﬁlter plays a different role depending on the time. The acti-vation value of the ﬁrst layer is deﬁned as follows:\\nf\\nx,y,i=σ(Ta/summationdisplay\\nt=1R/summationdisplay\\nr=0S/summationdisplay\\ns=0I(t)\\nx+r,y +s·w(t)\\nr,s,i+bi),(1)\\nwherefx,y,i is the activation value of position (x,y)of the\\ni-th feature map. RandSare the number of rows and\\ncolumns of the ﬁlter, respectively. Tais the total frame num-\\nber of the input grayscale image sequences. I(t)\\nx+r,y +smeans\\nthat the value at the position (x+r,y+s)of the input frame\\nat timet.w(t)\\nr,s,iis thei-th ﬁlter coefﬁcient at (r,s)for the\\nt-th frame, and biis the bias for the i-th ﬁlter. σ(·)is an\\n2984\\nactivation function, which is usually a non-linear function.\\nAdditionally, we utilize a ReLU, σ(x)=max(0,x)as an\\nactivation function, where xis an input value [3].\\nThe other layers are not different from the conventional\\nCNN as follows: the output of the convolutional layer isrescaled to half-size in a pooling layer for efﬁcient calcula-tion. Using these activation values, a convolution operationand pooling are performed one more time. Finally, theseoutput values are passed through the two fully connectedlayers and then classiﬁed using softmax. For training ournetwork, the stochastic gradient descent method is used foroptimization, and dropout [5] and weight decay methods are\\nutilized for regularization.\\nWe designed our network with a moderate depth and a\\nmoderate number of parameters to avoid overﬁtting, sincethe size of the facial expression recognition database is toosmall— there are only 205 sequences in the MMI database.\\nAdditionally, the ﬁrst layer turns out to detect the temporal\\ndifference of the appearance in input image sequences asdiscussed in Section 4.\\n3.3. Deep Temporal Geometry Network\\nDTGN receives the trajectories of facial landmark points\\nas input. These trajectories can be considered as one-dimensional signals and deﬁned as follows:\\nX\\n(t)=/bracketleftBig\\nx(t)\\n1y(t)\\n1x(t)\\n2y(t)\\n2···x(t)\\nny(t)\\nn/bracketrightBig/latticetop\\n,\\n(2)\\nwherenis the total number of landmark points at frame t,\\nandX(t)is a2ndimensional vector at t.x(t)\\nkandy(t)\\nkare\\ncoordinates of the k-th facial landmark points at frame t.\\nThesexy-coordinates are inappropriate for direct use as\\nan input to the deep network, because they are not normal-ized. For the normalization of the xy-coordinates, we ﬁrst\\nsubtract the xy-coordinates of the nose position (the posi-\\ntion of the red point among the facial landmark points inthe red box with the dotted line in Figure 1) from the xy-\\ncoordinates of each point. Then, each coordinate is dividedby each standard deviation of xy-coordinates in each frame\\nas follows:\\n¯x\\ni(t)=x(t)\\ni−x(t)\\no\\nσ(t)\\nx, (3)\\nwherex(t)\\niisx-coordinate of the i-th facial landmark point\\nat framet,x(t)\\noisx-coordinate of the nose landmark coordi-\\nnate at frame t.σ(t)\\nxis standard deviation of x-coordinates\\nat frame t. This process is also applied to the y(t)\\ni. Fi-\\nnally, these normalized points are concatenated along the\\ntime, and these points are used for the input to the DTGN.\\n¯X=/bracketleftBig\\n¯x(1)\\n1¯y(1)\\n1···¯x(Tg)\\nn¯y(Tg)\\nn/bracketrightBig/latticetop\\n, (4)\\nFigure 2. Joint ﬁne-tuning method. The green box denotes linear\\nfully connected network which has logit values. The logit val-\\nues are used as the input to the softmax activation. To integratetwo networks, we freeze the weight values in gray boxes of twotrained networks, and retrain the top layer in green boxes. In the\\ntraining step, we use three softmax functions for calculating three\\nloss functions, and we only use Softmax\\n3for prediction.\\nwhere¯Xis a2nT gdimensional input vector, and ¯x(Tg)\\nkand\\n¯y(Tg)\\nkare coordinates of k-th normalized landmark points at\\nframeTg.\\nThe ﬁgure in the red box with a dotted line in Figure 1il-\\nlustrates the architecture of our DTGN model. Our network\\nreceives the concatenated landmark points ¯Xas input. Ba-\\nsically, we utilize two hidden layers, and the top layer is a\\nsoftmax layer. Similar to the DTAN, this network is alsotrained by using the stochastic gradient descent method.The activation function for each hidden layer is ReLU. Fur-\\nthermore, for regularization of the network, dropout [5] and\\nweight decay are used.\\n3.4. Data Augmentation\\nIn order to better classify unseen data, a number of train-\\ning data covering various situations are required. However,facial expression databases, such as CK+, Oulu-CASIA,and MMI, provide only hundreds of sequences. This makesa deep network easily overﬁt, because a typical deep net-work has many parameters. To overcome this problem, var-ious data augmentation techniques are required.\\nFirst, whole image sequences are horizontally\\nﬂipped. Then, each image is rotated by each angle in{−15\\n◦,−10◦,−5◦,5◦,10◦,15◦}. This makes the model\\nrobust against the slight rotational changes of the inputimages. Finally, we obtain 14 times more data: originalimages (1), ﬂipped images (1), rotated images with sixangles, and their ﬂipped versions (12).\\nSimilar to the augmentation of image sequences, the nor-\\nmalized facial landmark points are also horizontally ﬂipped.Then, Gaussian noise is added to the raw landmark points.\\n˜x\\n(t)\\ni=¯x(t)\\ni+z(t)\\ni, (5)\\nwherez(t)\\ni∼N(0,σ2\\ni)is additive noise with noise level σi\\nfor thex-coordinate of the i-th landmark points at frame t.\\n2985\\nWe set the value of σito 0.01. Additionally, we contam-\\ninatedy-coordinate with noise in the same way. The net-\\nwork learns to be robust against slight pose changes using\\nthis method. To prepare for rotational changes, we constructrotated data as follows:\\n/bracketleftBig\\n˜x\\n(t)\\ni˜y(t)\\ni/bracketrightBig/latticetop\\n=R(t)/bracketleftBig\\n¯x(t)\\ni¯y(t)\\ni/bracketrightBig/latticetop\\n, (6)\\nfori=1,...,n where˜x(t)\\niand˜y(t)\\niarei-th rotated xy-\\ncoordinates at time t, andR(t)is a 2×2 rotation matrix for\\nthexy-coordinates at time t, which has an angle θ(t). The\\nvalue ofθ(t)is drawn from a uniform distribution where\\nθ(t)∼Unif[β,γ]. We set the values of βandγto−π/10\\nandπ/10, respectively.\\nWe performed the ﬁrst data augmentation methods in\\nequation 5three times, and the second data augmentation in\\nequation 6was also conducted three times. Consequently,\\nwe obtained six times more facial landmark points. As aresult, we augmented the training data fourteen times: orig-inal coordinates (1), ﬂipped coordinates (1), and six aug-mented coordinates, and their ﬂipped versions (12).\\n3.5. Model Integration\\n3.5.1 Weighted Summation\\nThe outputs from the top layers of the two networks were\\nintegrated using equation 7.\\noi=αpi+(1−α)qi,0≤α≤1, (7)\\nfori=1,...,c wherecis the total number of emotion\\nclass,pi,qiare outputs of DTAN and DTGN, and oiis the\\nﬁnal score. Finally, the index with the maximum value is\\nthe ﬁnal prediction. The parameter αusually depends on\\nthe performance of each network. For all the experiments\\nin this paper, we set the value of αto 0.5 that is the optimal\\nvalue as shown in Figure 11.\\n3.5.2 Joint Fine-Tuning Method\\nThe above method is simple to use, but it may not use the\\nmost of the ability of the two models. Consequently, we\\npropose an alternative integration method for the two net-works using a joint ﬁne-tuning method, which achieves bet-ter results than the above method.First, the two trained networks are reused, as shown in Fig-\\nure2. Next, we retrain the linear fully connected network,\\nwhich is located below softmax activation function, with the\\nloss function L\\nDTAGN of DTAGN deﬁned as follows:\\nLDTAGN=λ1L1+λ2L2+λ3L3, (8)\\nwhereL1,L2, andL3are loss functions computed by\\nDTAN, DTGN, and both, respectively. The λ1,λ2, andλ3\\nare tuning parameters. Usually, the parameters λ1andλ2\\nFigure 3. Filters learned by a single frame-based CNN. The in-\\nput image size was 64 ×64, and the ﬁlter size was 5 ×5. 18\\nﬁlters were selected for visualization from 64 learned ﬁlters in the\\nﬁrst convolutional layer. The black and white colors represent thenegative and positive values, respectively. There were several di-rectional edge and blob detection ﬁlters.\\nFigure 4. Feature maps corresponding to Figure 3.The left im-\\nage represents the input image, and the right image shows the fea-ture maps extracted by each ﬁlter in Figure 3. The emotion label\\nfor the input image was surprise. The blue and red values repre-\\nsent the low and high response values, respectively. The edges of\\nthe input image are detected in most of the ﬁlter.\\nare the same, and λ3has a smaller value than the value of\\ntwo tuning parameters. For all the experiments perfomed in\\nthis paper, we set λ1,λ2, andλ3to 1, 1, and 0.1, respec-\\ntively. The parameters were intuitively chosen. Each loss\\nfunction is a cross entropy loss function, which is deﬁned\\nas follows:\\nLi=−c/summationdisplay\\nj=1yjlog(˜yi,j), (9)\\nwhereyjis thej-th value of the ground truth label, and ˜yi,j\\nis thej-th output value of softmax of network i. (For conve-\\nnience, we call DTAN, DTGN, and the integrated networkby network 1,2, and3, respectively.) The ˜y\\n3,jis deﬁned\\nusing logit values of network 1 and 2 as follows:\\n˜y3,j=σs(l1,j+l2,j), (10)\\nwherel1,jandl2,jarej-th logit values of network 1 and 2,\\nrespectively. σs(·)is a softmax activation function.\\nFinally, the ﬁnal decision ˜ois obtained using the output\\nof softmax of network 3 as follows:\\n˜o=a r gm a x\\nj˜y3,j, (11)\\nAs a result, we utilized three loss functions in the training\\nstep, and use only integrated result for prediction. Whenusing our joint ﬁne-tuning method, we use the same trainingdataset used in training of each network. Also, the dropoutmethod is used for reducing over-ﬁtting.\\n2986\\nFigure 5. Filters learned by DTAN. The number of input frames\\nwas three in this ﬁgure, so there are three ﬁlters corresponding to\\neach frame. The three ﬁlters in each bold black box generate onefeature map. As with Figure 3, 18 ﬁlters were selected from 64\\nlearned ﬁlters. In this ﬁgure, we can see that our network detectsdifferences between frames.\\nFigure 6. Feature maps corresponding to Figure 5.The gray\\nimages on the left side form the image sequence used as input, andthe images on the right side are the feature maps corresponding toeach ﬁlter in Figure 5. Blue and red represent the low and high\\nresponse values. The emotion label for the input image sequencewas surprise. We observed that our network responded to movingparts for expressing emotion.\\n4. What Will Deep Networks Learn?\\n4.1. Visualization of DTAN\\nTo ﬁnd out what our DTAN has learned, the learned\\nﬁlters were visualized. Figure 3demonstrates the ﬁlters\\nlearned by a single frame-based CNN in the ﬁrst convolu-\\ntional layers using the CK+ database. The ﬁlters were sim-ilar to the edge or blob detectors. Corresponding responses\\nto each ﬁlter are provided in Figure 4. The edge components\\nwith several directions were detected by these ﬁlters.\\nIn our DTAN, which is a multiple frame-based CNN, the\\nlearned ﬁlters are shown in Figure 5. Unlike the ﬁlters of a\\nsingle frame-based CNN, the ﬁlters were not edge or blobdetectors. To exaggerate a little, these were just combina-tions of black, gray, and white ﬁlters. Figure 6shows the\\nmeaning of these ﬁlters. High response values were usuallyshown in parts with big differences between input frames.In other words, we can see that the ﬁrst convolutional layerof our DTAN detects facial movements arising from the ex-pression of emotion.\\n4.2. Visualization of DTGN\\nThe left side of Figure 7(a) shows the signiﬁcant facial\\nlandmark points for facial expression recognition. Thesepositions were automatically identiﬁed by DTGN. The ex-tracted positions were very similar to those of emotionalfacial action coding system (EFACS) [2] in Figure 7(b). To\\nexplain it further, the two extracted points on the nose be-come wider when people make a happy expression becauseboth cheeks are pulled up.\\nIn order to ﬁgure out the characteristics of the featuresAn Co Di Fe Ha Sa Su All\\nCK+ 45 18 59 25 69 28 83 327\\nOulu 8 0 - 8 08 08 08 08 0 480\\nMMI 3 2 - 3 12 84 23 24 0 205\\nTable 1. The number of image sequences for each emotion:\\nanger (An), contempt (Co), disgust (Di), fear (Fe), happiness (Ha),\\nsadness (Sa), and surprise (Su).\\nextracted from the top layer, we also visualized the feature\\nvectors using t-SNE, which is a useful tool for visualizationof high dimensional data [19]. The input data were spreadrandomly in Figure 7(c), but the features extracted from the\\nsecond hidden layer were well separated according to theirlabel, as shown in Figure 7(d).\\n5. Experiments\\nIn this section, we compare our approach with other\\nstate-of-the-art algorithms in facial expression recognition,such as manifold-based sparse representation (MSR) [ 14],\\nAdaLBP [23], Atlases [4], and common and speciﬁc active\\npatches (CSPL) [25]. We excluded person dependent algo-\\nrithms or algorithms that utilize 3D geometry informationin the experiments. For assessing the performance of ourmethod, we used three databases: the CK+, Oulu-CASIA,and MMI databases. The number of image sequences ineach database is listed according to each emotion in Table 1.\\n5.1. Network Architecture\\nThe architecture of DTGN for the CK+ is D1176-\\nFC100-FC600-S7. D1176 is a 1176 dimensional inputvector, and FC100 refers to a fully connected layer with100 nodes. Also, S7 is the softmax layer with seven out-puts. Our DTAN model for the CK+ is I64-C(5,64)-L5-P2-C(5,64)-L3-P2-FC500-FC500-S7, where I64 means 64× 64\\ninput image sequences, and C(5,64) is a convolutional layerwith64ﬁlters of5×5. L5 is a local contrast normalization\\nlayer with a window size of 5×5. P2 means a 2×2max\\npooling layer. The stride of each layer was 1 with the ex-ception of the pooling layer. The value of the stride for eachpooling layer was set to 2. The DTGN and DTAN models\\nMethod Accuracy\\nHOG 3D [9] 91.44\\nMSR [14] 91.4\\nTMS [6] 91.89\\nCov3D [15] 92.3\\nSTM-ExpLet [12] 94.19\\n3DCNN [11] 85.9\\n3DCNN-DAP [11] 92.4\\nDTAN 91.44\\nDTGN 92.35\\nDTAGN(Weighted Sum) 96.94\\nDTAGN(Joint) 97.25\\nTable 2. Overall accuracy in the CK+ database. The red and\\nblue colors represent the ﬁrst and second most accurate, respec-\\ntively.\\n2987\\n(a) (b) (c) (d)\\nFigure 7. Visualization of representation extracted by DTGN. (a) (b) show extracted feature points and emotional action parts [2],\\nrespectively. The important top-10 positions are detected by our network (red points in the left ﬁgure). In order to visualize these ten\\npoints, we calculated the average of the absolute values of weights in the ﬁrst layer connected to each landmark point. Then, these valueswere sorted in descending order, and the top 10 points with highest value were selected. The action parts deﬁned by EFACS [2] are shownin the right ﬁgure (green colored area). (c) Visualization of original input data in CK+ database, using t-SNE [19]. The number of data was4149: (327-33)×14 augmented training data and 33 test data. The small dots and large squares represent training and test data, respectively.The numbers in the legend correspond to each label of the CK+ database: 0-anger, 1-contempt, 2-disgust, 3-fear, 4-happiness, 5-sadness,and 6-surprise. (d) Visualization of the outputs in the second hidden layer. The data points were automatically grouped by DTGN.\\nfor Oulu-CASIA were the same as the models for the CK+\\nexcept the number of nodes in the top layer, because thereare six labels in the Oulu-CASIA.\\nFor the MMI, we used the DTGN model of D1176-\\nFC100-FC200-FC6. Our DTAN model was designed asI64-C(5,32)-P3-C(3,32)-FC30-S6. Unlike the other twodatabases, the number of subjects and image sequences arevery small. Consequently, we decreased the total number ofparameters signiﬁcantly.\\n5.2. CK+\\nDescription of the database. CK+ is a representative\\ndatabase for facial expression recognition. This database iscomposed of 327 image sequences with seven emotion la-bels: anger, contempt, disgust, fear, happiness, sadness, andsurprise. There are 118 subjects, and these subjects are di-vided into ten groups by ID in ascending order. Nine subsetswere used for training our networks, and the remaining sub-set was used for validation. This process is the same as the10-fold cross validation protocol in [12]. In this database,each sequence starts with a neutral emotion and ends with apeak of the emotion.\\nResults. The total accuracy of 10-fold cross validation is\\nshown in Table 2. The performances of DTAN and DTGN\\nare lower than other algorithms, but the performance of the\\nintegrated network is better than other state-of-the-art algo-\\nAn Co Di Fe Ha Sa Su\\nAn 100 0 000 0 0\\nCo 0 94.44 0 0 0 5.56 0\\nDi 00 100 00 0 0\\nFe 00 0 84 80 8\\nHa 00 0 0 100 00\\nSa 10.71 0 0 0 0 89.29 0\\nSu 0 1.2 0 0 0 0 98.8\\nTable 3. Confusion matrix of the joint ﬁne-tuning method for\\nthe CK+ database. The labels in the leftmost column and on the\\ntop represent the ground truth and prediction results, respectively.\\nFigure 8. Comparison of accuracy in the CK+ according to each\\nemotion among three networks.\\nrithms. The two networks were complementary, and this isshown in Figure 8. The DTAN had a good performance\\nwith respect to contempt, whereas it had lower accuracywith fear. On the other hand, the geometry-based modelwas strong with fear. Table 3shows the confusion matrix for\\nCK+. Our algorithm performed well in recognizing anger,disgust, happiness, and surprise. For the other emotions,our method also performed reasonably well.\\n5.3. Oulu-CASIA\\nDescription of the database. For further experiments, we\\nused Oulu-CASIA, which includes 480 image sequencestaken under normal illumination conditions. Each imagesequence has one of six emotion labels: anger, disgust, fear,happiness, sadness, or surprise. There are 80 subjects, and10-fold cross validation was performed in the same way asin the case of CK+. Similar to the CK+ database, each se-quence begins with a neutral facial expression and ends withthe facial expression of each emotion.\\nResults. The accuracy of our algorithm was superior to\\nthe other state-of-the-art algorithms, as shown in Table 4.\\nThe best performance from among the existing methods was\\n75.52%, which was achieved by Atlases, and this record hadnot been broken for three years. However, we have signif-\\n2988\\nFigure 9. Comparison of accuracy in the Oulu-CASIA accord-\\ning to each emotion among three networks.\\nicantly improved the accuracy by about 6% using our inte-\\ngrated two deep networks. In Figure 9, the performance of\\ntwo networks and the combined model is compared. Sim-ilar to the case of CK+, we can see that the two networksare complementary to each other. In particular, the perfor-mance of the DTGN in the case of disgust was lower thanthe DTAN, but the combined model produced good results.Table 5shows the confusion matrix for our algorithm. The\\nperformance in the cases of happiness, sadness, and surprisewas good, but the performance for anger, disgust, and fearwas relatively poor. In particular, anger and disgust wereconfused in our algorithm.\\n5.4. MMI\\nDescription of the database. MMI consists of 205 im-\\nage sequences with frontal faces and includes only 30 sub-jects. Similar to the Oulu-CASIA database, there are sixkinds of emotion labels. This database was also divided\\nMethod Accuracy\\n3D SIFT [16] 55.83\\nLBP-TOP [24] 68.13\\nHOG 3D [9] 70.63\\nAdaLBP [23] 73.54\\nAtlases [4] 75.52\\nSTM-ExpLet [12] 74.59\\nDTAN 74.38\\nDTGN 74.17\\nDTAGN(Weighted Sum) 80.62\\nDTAGN(Joint) 81.46\\nTable 4. Overall accuracy in the Oulu-CASIA database. The\\nred and blue colors represent the ﬁrst and second most accurate,\\nrespectively.\\nAn Di Fe Ha Sa Su\\nAn 72.5 16.25 1.25 1.25 8.75 0\\nDi 21.25 75 3.75 0 0 0\\nFe 2.5 1.25 77.5 6.25 2.5 10\\nHa 0 0 7.5 90 2.5 0\\nSa 13.75 0 2.5 0 83.75 0\\nSu 00 1 0 0 0 90\\nTable 5. Confusion matrix of the joint ﬁne-tuning method for\\nthe Oulu-CASIA database. The labels in the leftmost column\\nand on the top represent the ground truth and prediction results,respectively.\\nFigure 10. Comparison of accuracy in the MMI according to\\neach emotion among three networks.\\ninto 10 groups for person independent 10-fold cross valida-\\ntiaon. This database is different from the other databases;each sequence begins with a neutral facial expression, andhas the facial expression of each emotion in the middle ofthe sequence. This ends with the neutral facial expression.The location of the peak frame is not provided as a priorinformation.\\nResults. This dataset is especially difﬁcult for a deep learn-\\ning algorithm to learn from, because there are too small\\nnumber of data and subjects. The previous top recordachieved by a deep learning technique was only 63.4% us-ing 3D CNN-DAP. However, we improved the recognitionrate to 70.24% as shown in Table 6. Finally, our algorithm is\\nmuch better than 3D SIFT, which was the second best algo-rithm. In particular, our joint ﬁne-tuning method achieveda signiﬁcantly improved recognition rate compared with a\\nMethod Accuracy\\nHOG 3D [9] 60.89\\n3D SIFT [16] 64.39\\nITBN [20] 59.7\\nCSPL [25] (73.53)\\nSTM-ExpLet [12] 75.12\\n3DCNN [11] 53.2\\n3DCNN-DAP [11] 63.4\\nDTAN 62.45\\nDTGN 59.02\\nDTAGN (Weighted Sum) 65.85\\nDTAGN (Joint) 70.24\\nTable 6. Overall accuracy in the MMI database. The red and\\nblue colors represent the ﬁrst and second most accurate, respec-\\ntively. The CSPL used additional ground truth information, so itwas excluded from the ranking.\\nAn Di Fe Ha Sa Su\\nAn 61.29 25.8 0 0 12.9 0\\nDi 15.62 71.88 0 9.37 0 3.13\\nFe 10.71 0 35.71 10.71 14.29 28.57\\nHe 0 0 4.76 95.24 00\\nSa 9.38 3.13 15.62 0 68.8 3.12\\nSu 2.5 0 20 2.5 0 75\\nTable 7. Confusion matrix of the joint ﬁne-tuning method for\\nthe MMI Database. The labels in the leftmost column and on the\\ntop represent the ground truth and prediction results, respectively.\\n2989\\nFigure 11. Performance of DTAGN using the weighted summa-\\ntion method with respect to α.We changed the value of αfrom\\n0 to 1 with interval of 0.01.\\nFigure 12. All failure cases with fear in the MMI database. Our\\ndeep network predicted fear to surprise (green box), anger (red\\nbox), sadness (orange box), and happiness (blue box).\\nweighted summation method.\\nWe compared two networks and the combined model in\\nFigure 10. The two networks were complementary to each\\nother for most of the emotions. However, with fear, our al-\\ngorithm was not successful enough. This is also shown inthe confusion matrix in Table 7. We observed that the accu-\\nracy for fear was much lower than other emotions. In partic-ular, most of the fear emotions were confused with surprise.To examine this phenomenon, we checked all the failurecases, as shown in Figure 12. The results indicated that a\\nvariety of facial expressions are labeled as fear, even thoughmany cases were similar to surprise or sadness. To success-fully recognize these various expressions, various kinds oftraining data are additionally required. However, we had\\nonly 27 subjects for training data. (Three subjects were\\nused for validation.) Unfortunately, performance of deeplearning techniques highly depends on the quality of train-ing data, so our accuracy with fear was not good enough.\\n6. Discussion on the Joint Fine-Tuning Method\\nIn this section, we discuss the joint ﬁne-tuning method.\\nFirst we evaluated the effectiveness of the three loss func-tions of our joint ﬁne-tuning method using each database.As a result, the accuracy using the three loss functions was\\nbetter than using L\\n3only, as shown in Table 8. Also, we\\ncompared our algorithm with a concatenation of high level\\nfeatures, which is one natural way for integrating two deepnetworks. To evaluate the concatenation method, we con-catenated the activation values of the top hidden layers inthe two gray areas in Figure 2. Then the concatenated ac-\\ntivation values are used for inputs to a fully connected net-work with a softmax activation, and a dropout was used forregularizing the network. Table 9shows the experimental\\nresults using each database. The performance of the con-# Of Loss Functions Baseline One (L 3only) Three\\nAccuracy (CK+) 96.94 96.64 97.25\\nAccuracy (Oulu) 80.62 81.04 81.46\\nAccuracy (MMI) 65.85 69.76 70.24\\nTable 8. Comparison between one and three loss function\\nmethods. Baseline denotes the weighted summation method pre-\\nsented in this paper.\\nConcatenation Joint Fine-tuning\\nAccuracy (CK+) 94.5 97.25\\nAccuracy (Oulu) 75.63 81.46\\nAccuracy (MMI) 67.8 70.24\\nTable 9. Comparison between the concatenation method and\\nproposed joint ﬁne-tuning method. Our joint ﬁne-tuning method\\nshowed about 3∼6% improvement in terms of the recognition\\nrates.\\ncatenation method was worse than either one of our two\\nnetworks.\\nAs mentioned in Section 3.5.2, we used the same dataset\\nfor ﬁne-tuning together with dropout, which reduces over-\\nﬁtting. Of course, without dropout, there will be little toﬁne-tune with the same dataset, as the error for the sametraining dataset would be already almost zero before ﬁne-\\ntuning starts. Interestingly, thanks to dropout, there is some-thing to ﬁne-tune with the same dataset as the dropout by\\nrandomly inducing errors achieves an effect of providingdifferent training data to the layers to ﬁne-tune. Further, asdropout creates an ensemble of many networks and eachnetwork in the ensemble experiences a different random\\nsubset of whole dataset, each member of ensemble with a\\nparticular dropout pattern experiences different subsets ofdata for the ﬁrst training and the following ﬁne-tuning.\\n7. Conclusion\\nWe presented two deep network models that collaborate\\nwith each other. The ﬁrst network was DTAN, which wasbased on appearances of multiple frames, while the second\\nnetwork was DTGN, which extracted useful temporalgeometric features from raw facial landmark points. Weshowed that the ﬁlters learned by the DTAN in the ﬁrst\\nlayer have the ability to obtain the difference between theinput frames. Furthermore, the important landmark pointsextracted by DTGN were also shown. We achieved best\\nrecognition rates using the integrated deep network onthe CK+ and Oulu-CASIA databases. Furthermore, we\\nshowed that our joint ﬁne-tuning method is superior toother integration methods, such as a weighted summationand a feature concatenation method.\\nAcknowledgements This work was partially supported by\\nthe National Research Foundation of Korea(NRF) grant\\nfunded by the Korea government (MSIP) NRF-2010-0028680 and NRF-2014R1A2A2A01003140.\\n2990\\nReferences\\n[1] D. Ghimire and J. Lee. Geometric feature-based facial expression\\nrecognition in image sequences using multi-class adaboost and sup-\\nport vector machines. Sensors, 13(6):7714–7734, 2013. 1,2\\n[2] J. Girard and J. Cohn. Ground truth facs action unit coding on the\\ngroup formation task. In Tech. rep., University of Pittsburgh, 2013.\\n5,6\\n[3] X. Glorot, A. Bordes, and Y . Bengio. Deep sparse rectiﬁer networks.\\nInProceedings of the 14th International Conference on Artiﬁcial In-\\ntelligence and Statistics. JMLR W&CP V olume , volume 15, pages\\n315–323, 2011. 3\\n[4] Y . Guo, G. Zhao, and M. Pietik ¨ainen. Dynamic facial expression\\nrecognition using longitudinal facial expression atlases. In ECCV ,\\n2012, pages 631–644. Springer, 2012. 5,7\\n[5] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by preventing co-\\nadaptation of feature detectors. arXiv preprint arXiv:1207.0580,\\n2012. 3\\n[6] S. Jain, C. Hu, and J. K. Aggarwal. Facial expression recognition\\nwith temporal modeling of shapes. In ICCV Workshops, 2011, pages\\n1642–1649. IEEE, 2011. 2,5\\n[7] L. A. Jeni, J. F. Cohn, and T. Kanade. Dense 3d face alignment from\\n2d videos in real-time. 2\\n[8] L. A. Jeni, A. L ˝orincz, Z. Szab ´o, J. F. Cohn, and T. Kanade. Spatio-\\ntemporal event classiﬁcation using time-series kernel based struc-tured sparsity. In Computer Vision–ECCV 2014, pages 135–150.\\nSpringer, 2014. 1,2\\n[9] A. Klaser and M. Marszalek. A spatio-temporal descriptor based on\\n3d-gradients. 2008. 2,5,7\\n[10] S. M. Lajevardi and H. R. Wu. Facial expression recognition in\\nperceptual color space. Image Processing, IEEE Transactions on,\\n21(8):3721–3733, 2012. 2\\n[11] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply learning de-\\nformable facial action parts model for dynamic expression analysis.InACCV , 2014, pages 1749–1756. IEEE, 2014. 2,5,7\\n[12] M. Liu, S. Shan, R. Wang, and X. Chen. Learning expressionlets on\\nspatio-temporal manifold for dynamic facial expression recognition.InCVPR, 2014 IEEE Conference on , pages 1749–1756. IEEE, 2014.\\n1,2,5,6,7\\n[13] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews. The extended cohn-kanade dataset (ck+): A complete\\ndataset for action unit and emotion-speciﬁed expression. In CVPRW,\\n2010 IEEE Computer Society Conference on, pages 94–101. IEEE,2010. 1\\n[14] R. Ptucha, G. Tsagkatakis, and A. Savakis. Manifold based sparse\\nrepresentation for robust expression recognition without neutral sub-traction. In ICCV Workshops, 2011, pages 2136–2143. IEEE, 2011.\\n5\\n[15] A. Sanin, C. Sanderson, M. T. Harandi, and B. C. Lovell. Spatio-\\ntemporal covariance descriptors for action and gesture recognition.\\nInWACV , 2013, pages 103–110. IEEE, 2013. 1,2,5\\n[16] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descrip-\\ntor and its application to action recognition. In Proceedings of the\\n15th international conference on Multimedia , pages 357–360. ACM,\\n2007. 2,7\\n[17] K. Sikka, T. Wu, J. Susskind, and M. Bartlett. Exploring bag of\\nwords architectures in the facial expression domain. In Computer\\nVision–ECCV 2012. Workshops and Demonstrations, pages 250–259. Springer, 2012. 1,2\\n[18] M. Valstar and M. Pantic. Induced disgust, happiness and surprise:\\nan addition to the mmi facial expression database. In Proc. Intl\\nConf. Language Resources and Evaluation, Workshop on EMOTION,\\npages 65–70, 2010. 1\\n[19] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Jour-\\nnal of Machine Learning Research, 9(2579-2605):85, 2008. 5,6[20] Z. Wang, S. Wang, and Q. Ji. Capturing complex spatio-temporal\\nrelations among facial muscles for facial expression recognition. In\\nCVPR, 2013 IEEE Conference on , pages 3422–3429. IEEE, 2013. 1,\\n2,7\\n[21] X. Xiong and F. De la Torre. Supervised descent method and its\\napplications to face alignment. In CVPR, 2013 IEEE Conference on,\\npages 532–539. IEEE, 2013. 2\\n[22] A. A. Youssif and W. A. Asker. Automatic facial expression recogni-\\ntion system based on geometric and appearance features. Computer\\nand Information Science, 4(2):p115, 2011. 1,2\\n[23] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietik ¨aInen. Facial\\nexpression recognition from near-infrared videos. Image and Vision\\nComputing, 29(9):607–619, 2011. 1,5,7\\n[24] G. Zhao and M. Pietikainen. Dynamic texture recognition using lo-\\ncal binary patterns with an application to facial expressions. PAMI,\\n29(6):915–928, 2007. 2,7\\n[25] L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, and D. N. Metaxas.\\nLearning active facial patches for expression analysis. In CVPR,\\n2012 IEEE Conference on, pages 2562–2569. IEEE, 2012. 5,7\\n[26] Z. Zhou, G. Zhao, and M. Pietikainen. Towards a practical lipreading\\nsystem. In CVPR, 2011 IEEE Conference on, pages 137–144. IEEE,\\n2011. 2\\n2991\\n',\n",
       " \"1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 1\\nLearning Affective Features with a Hybrid Deep\\nModel for Audio-Visual Emotion Recognition\\nShiqing Zhang, Shiliang Zhang, Member, IEEE, Tiejun Huang, Senior Member, IEEE, Wen Gao, Fellow, IEEE,\\nand Qi Tian, Fellow, IEEE\\nAbstract—Emotion recognition is challenging due to the emo-\\ntional gap between emotions and audio-visualfeatures. Motivated\\nby the powerful feature learning ability of deep neural networks,\\nthis paper proposes to bridge the emotional gap by using a\\nhybrid deep model, which ﬁrst produces audio-visual segment\\nfeatures with Convolutional Neural Networks (CNN) and 3D-\\nCNN, then fuses audio-visual segment features in a Deep Belief\\nNetworks (DBN). The proposed method is trained in two stages.\\nFirst, CNN and 3D-CNN models pre-trained on corresponding\\nlarge-scale image and video classiﬁcation tasks, are ﬁne-tuned\\non emotion recognition tasks to learn audio and visual segment\\nfeatures, respectively. Second, the outputs of CNN and 3D-\\nCNN models are combined into a fusion network built with a\\nDBN model. The fusion network is trained to jointly learn a\\ndiscriminative audio-visual segment feature representation. After\\naverage-poolingsegmentfeatureslearnedbyDBNtoformaﬁxed-\\nlength global video feature, a linear Support Vector Machine\\n(SVM) is used for video emotion classiﬁcation. Experimental\\nresults on three public audio-visual emotional databases, in-\\ncluding the acted RML database, the acted eNTERFACE05\\ndatabase, and the spontaneous BAUM-1s database, demonstrate\\nthe promising performance of the proposed method. To the best\\nof our knowledge, this is an early work fusing audio and visual\\ncues with CNN, 3D-CNN and DBN for audio-visual emotion\\nrecognition.\\nIndex Terms —Emotion recognition, deep learning, convolu-\\ntional neural networks, deep belief networks, multimodality\\nfusion.\\nI. INTRODUCTION\\nRECOGNIZING human emotions with computers is usu-\\nally performed with a multimodal approach due to\\nthe inherent multimodality characteristic of human emotion\\nexpression. Speech and facial expression are two natural\\nand effective ways of expressing emotions when human\\nbeings communicate with each other. During the last two\\ndecades, audio-visual emotion recognition integrating speech\\nand facial expression, has attracted extensive attention owing\\nto its promising potential applications in human-computer-\\ninteraction [ 1], [2]. However, recognizing human emotions\\nw\\nith computers is still a challenging task because it is difﬁcult\\nto extract the best audio and visual features characterizing\\nhuman emotions.\\nS.Q. Zhang, S.L. Zhang, T.J. Huang and W. Gao are with the Institute\\nof Digital Media, School of Electronic Engineering and Computer Science,\\nPeking University, China. S.Q. Zhang also works in the Institute of Intel-\\nligent Information Processing, Taizhou University, China. e-mail: {tzczsq,\\nslzhang.jdl, tjhuang, wgao }@pku.edu.cn.\\nQ. Tian is with the Department of Computer Science, University of Texas\\nat San Antonio, San Antonio, TX 78249 USA. e-mail: qitian@cs.utsa.edu.\\nManuscript received xxxx, 2016; revised xxxx, 2016.Feature extraction and multimodality fusion are two key\\nsteps for audio-visual emotion recognition. As far as feature\\nextraction is concerned,there has been a number of works [ 3],\\n[4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14] focusing\\no\\nn extracting low-level hand-crafted features for audio-visual\\nemotion recognition. Nevertheless, due to the emotional gap\\nbetween human emotions and low-level hand-crafted features,\\nthese hand-crafted features can not sufﬁciently discriminate\\nhuman emotions. Here, the ”emotional gap” is deﬁned as ”the\\nlack of coincidence between the measurable signal properties,\\ncommonly referred to as features, and the expected affective\\nstates in which the user is brought by perceiving the signal”\\n[15]. Therefore, the ”emotional gap” essentially represents th e\\ndifferences between emotions and the extracted affective fea-\\ntures. To bridge the ”emotional gap”, it is desirable to extract\\nhigh-level audio and visual features effectively distinguishing\\nemotions.\\nAfter feature extraction, multimodality fusion is employed\\nto integrate audio and visual modalities for emotion recogni-\\ntion. Previous works [ 3], [8], [12], [16], [17], [18] focus on\\nf\\nour typical fusion strategies: feature-level fusion, decision-\\nlevel fusion, score-level fusion, and model-level fusion, re-\\nspectively. Although most of existing fusion methods exhibit\\ngood performance on audio-visual emotion recognition tasks,\\nthey belong to shallow fusion models with a limited ability\\nin jointly modeling highly non-linear correlations of multiple\\ninputs with different statistical properties [ 19]. It is thus\\nn\\needed to design deeper fusion methods to produce a more\\noptimized joint discriminant feature representation for audio-\\nvisual emotion recognition.\\nTo alleviate above-mentioned two problems, the recently-\\nemerged deep leaning [ 20] techniques may present a cue.\\nD\\nue to the large-scale available training data and the effective\\ntraining schemes, deep learning techniques have exhibited\\npowerful feature learning ability in a wide variety of domains,\\nsuch as speech recognition, image processing and understand-\\ning, object detection and recognition, etc. Among them, two\\nrepresentative deep learning models are DBN [ 21] and CNN\\n[22], [23], as described below.\\nD\\nBNs are built by stacking multiple Restricted Boltzmann\\nMachines (RBMs) [ 24]. By using multiple RBMs and the\\ng\\nreedy layer-wise training algorithm [ 21], DBNs can effec-\\nt\\nively learn a multi-layer generative model of input data.\\nBased on this generative model, the distribution properties\\nof input data can be discovered, and the hierarchical feature\\nrepresentationscharacterizinginput data can be also extracted.\\nDue to such good property, DBNs and its variant called\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 2\\n9LGHR\\x03VHJPHQW0\\nHO\\x10VSHFWURJUDP\\x03\\nVHJPHQW$XGLR\\x031HWZRUN\\x03\\x0b&11\\x0c\\n\\x1b\\n\\x14\\x1c\\x15\\n&RQY\\x14 & RQY\\x15 &RQY\\x17 &RQY\\x18 IF\\x19\\x17\\x13\\x1c\\x19\\n9LVXDO\\x031HWZRUN\\x03\\x0b\\x16'\\x10&11\\x0c\\n&\\nRQY\\x14D &RQY\\x15D &RQY\\x18D &RQY\\x18E\\x17\\x13\\x1c\\x19)XVLRQ\\x031HWZRUN\\x03\\x0b'%1\\x0c\\n\\u0378\\nͶ ൈ \\u0378Ͷ ൈ ͵\\nͳͷͲ ൈ ͳͳͲ ൈ ͵ ൈ ͳ\\u0378IF\\x1a\\nI\\nF\\x19IF\\x1a\\nUHVL]H\\nU\\nHVL]H9LVLEOH\\x03\\x03\\nO\\nD\\\\HU+LGGHQ\\x03\\nOD\\\\HU\\x032XWSXW\\x03\\nOD\\\\HU\\x03\\n3RRO\\x183RRO\\x14\\n3RRO\\x183RRO\\x14\\n9LGHR\\x03VDPSOH6HJPHQWN\\n6\\nHJPHQW16HJPHQW26\\nHJPHQWi\\n9LGHR\\x03VHJPHQW0\\nHO\\x10VSHFWURJUDP\\x03\\nVHJPHQW$XGLR\\x031HWZRUN\\x03\\x0b&11\\x0c\\n\\x1b\\n\\x14\\x1c\\x15\\n&RQY\\x14 & RQY\\x15 &RQY\\x17 &RQY\\x18 IF\\x19\\x17\\x13\\x1c\\x19\\n9LVXDO\\x031HWZRUN\\x03\\x0b\\x16'\\x10&11\\x0c\\n&\\nRQY\\x14D &RQY\\x15D &RQY\\x18D &RQY\\x18E\\x17\\x13\\x1c\\x19)XVLRQ\\x031HWZRUN\\x03\\x0b'%1\\x0c\\n\\u0378\\nͶ ൈ \\u0378Ͷ ൈ ͵\\nͳͷͲ ൈ ͳͳͲ ൈ ͵ ൈ ͳ\\u0378IF\\x1a\\nI\\nF\\x19IF\\x1a\\nUHVL]H\\nU\\nHVL]H9LVLEOH\\x03\\x03\\nO\\nD\\\\HU+LGGHQ\\x03\\nOD\\\\HU\\x032XWSXW\\x03\\nOD\\\\HU\\x03\\n3RRO\\x183RRO\\x14\\n3RRO\\x183RRO\\x14*OREDO\\x03YLGHR\\x03\\nI\\nHDWXUHV9LGHR\\x03HPRWLRQV6906HJPHQW\\x03\\nH\\nPRWLRQV\\n6HJPHQW\\x03\\nH\\nPRWLRQV$YHUDJH\\x10SRROLQJ\\nFig. 1. The structure of our proposed hybrid deep model for aud io-visual emotion recognition.\\nDeep BolzmannMachines(DBM) [ 25] have beensuccessfully\\nu\\ntilized to learn high-level feature representations from low-\\nlevelhand-craftedfeaturesformultimodalemotionrecognition\\n[26], [27], [28].\\nC\\nNNs employ raw image data as inputs instead of hand-\\ncrafted features. CNNs are mainly composed of convolutional\\nlayers and fully connected layers, where convolutional layers\\nlearn a discriminative multi-level feature representation from\\nraw inputs and fully connected layers can be regarded as a\\nnon-linear classiﬁer. Due to the large-scale available training\\ndata and the effective training strategies introduced in recent\\nworks, CNNs have exhibited signiﬁcant success in various\\nvision tasks like object detection and recognition [ 29], [30],\\n[31], [32]. However, such CNN models are mostly applied\\no\\nn 2D images and fail to capture motion cues in videos,\\ndue to the usage of 2D spatial convolution. To address this\\nproblem,a recent work [ 33] has extendedCNNs with deep 3D\\nc\\nonvolution to produce a 3D-CNN model. 3D-CNNs compute\\nfeature maps from both spatial and temporal dimensions,\\nand have exhibitedpromisingspatial-temporalfeaturelearning\\nability on video classiﬁcation tasks [ 33].\\nI\\nnspired by the powerful feature learning ability of deep\\nmodels, this work proposes a hybrid deep learning framework\\ncomposedby CNN, 3D-CNN, and DBN to learn a joint audio-\\nvisual feature representation for emotion classiﬁcation. Fig.\\n1presents the structure of our framework. It is comprised\\no\\nf three steps: (1) we convert the raw audio signals into\\na representation similar to the RGB image as the CNN\\ninput. Consequently, a deep CNN model pre-trained on large-\\nscale ImageNet dataset can be ﬁne-tuned on audio emotion\\nrecognition tasks to learn high-level audio segment features.\\n(2) For multiple contiguousframes in a video segment, a deep\\n3D-CNN model pre-trained on the large-scale video dataset is\\nﬁne-tunedtolearnvisualsegmentfeaturesforfacialexpression\\nrecognition.(3) The audio and visual segment features learned\\nby CNN and 3D-CNN are integrated in a fusion network built\\nwith a deep DBN, which is trained to predict correct emotionlabels of video segments. Finally, we adopt the outputs of the\\nlast hidden layer of DBN as the audio-visual segment feature.\\nAverage-poolingis employedto aggregateall segmentfeatures\\nto form a ﬁxed-length global video feature. Then, a linear\\nSVM is used for video emotion classiﬁcation. Note that, DBN\\nis used to fuse audio-visualfeatures, rather than to classify the\\nemotion of the whole video sample.\\nLearning audio-visual features for emotion recognition is\\none of the critical steps in bridging the emotional gap. Pre-\\nvious works focus on using low-level hand-crafted features,\\nwhich have been veriﬁed not discriminative enough to human\\nemotions. In contrast, this work aims at automatically learning\\na joint audio-visual feature representation from raw audio and\\nvisual signals using a hybrid deep learning framework. The\\nhybriddeeplearningmodelconvertsthe raw 1-Daudiosignals\\ninto a 2D representation and integrates CNN, 3D-CNN, and\\nDBN for audio-visual feature learning and fusion. It thus also\\npresents a new method of transforming 1-D audio signals into\\nthe suitable input of CNN that conventionally processes 2-D\\nor 3-D images. Experimental results indicate that our learned\\nfeatures present promising performance. The success of this\\nwork also guarantees further investigation in this direction.\\nThe remainder of this paper is organized as follows. The\\nrelated works are reviewed in Section II. Section IIIpresent\\no\\nur hybrid deep learning model for affective feature learning\\nin detail. Section IVdescribes the experimental results. The\\nc\\nonclusions and future work are given in Section V.\\nI\\nI. RELATEDWORK\\nAn audio-visual emotion recognition system generally con-\\nsists of two important steps: feature extraction and multi-\\nmodality fusion. In the following parts, we review related\\nworks focusing on these two steps, respectively.\\nA. Feature Extraction\\nThe widely-used audio affective features can be categorized\\ninto prosody features, voice quality features, and spectral fea-\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 3\\ntures [34], respectively. Pitch, intensity, energy, and duration\\nt\\nime are popular prosody features, as they are able to reﬂect\\nthe rhythm of spoken language. The representative voice\\nquality features include formants, spectral energy distribution,\\nharmonics-to-noise-ratio, and so on. Mel-frequency Cepstral\\nCoefﬁcient (MFCC) is the most well-known spectral features\\nsince it is used to model the human auditory perception sys-\\ntem. Zeng et al.[4] extract 20 audio features including pitch,\\ni\\nntensity, and the ﬁrst four formants and their bandwidths\\nfor audio emotion recognition. Wang et al.[3], [16] employ\\np\\nitch, intensity, and the ﬁrst 13 MFCC features on audio\\nfeature extraction tasks. Similar works, which extract prosody\\nfeatures, voice quality features and spectral features for audio\\nemotion recognition, can be also found in [ 12], [35], [36],\\n[37], [38], respectively.\\nV\\nisual feature extraction methods can be summarized into\\ntwo categories according to the format of inputs that are\\nstatic or dynamic [ 39], [40]. For static images, the well-\\nk\\nnown ones are appearance-based feature extraction methods.\\nThese methods adopt the whole-face or speciﬁc regions in\\na face image to describe the subtle changes of the face\\nsuch as wrinkles and furrows. Among them, Gabor wavelet\\nrepresentation [ 3], [16], [37], [41], Local Binary Patterns\\n(\\nLBP) [42] and its variants such as Local Phase Quantization\\n(\\nLPQ) [12], [43] are two representative appearance-based\\nf\\neature extraction methods. Wang et al.[3], [16] adopt a\\nG\\nabor ﬁlter bank of 5 scales and 8 orientations to extract\\nhigh-dimensional Gabor coefﬁcients from each facial image.\\nIn recent years, CNNs are used to extract facial features\\nfrom static facial images as visual features [ 44]. In [44],\\nt\\nhe authors use a Long Short-Term Memory (LSTM) [ 45]\\nf\\nor audio emotion recognition, and a CNN for video feature\\nextraction, and ﬁnally fuse audio and visual modality at score-\\nlevel.Fordynamicimagesequencesrepresentingdeformations\\nand facial muscle movements, the popular visual features are\\nfacial animation parameters or motion parameters. In [ 5], 34\\nm\\notion parameters of head, eyebrows, eyes, and mouth are\\ncollected as visual features from each facial image in dynamic\\nimage sequences. In [ 46], 18 facial animation parameters are\\ne\\nxtracted with a facial feature tracking technique performed\\non dynamic image sequences in video.\\nB. Multimodality Fusion\\nMultimodalityfusionis to integrateaudioand visual modal-\\nities with different statistical properties. Existing fusion strate-\\ngies [12], [16], [17], [18] can be summarized into four cate-\\ng\\nories,i.e., feature-level fusion, decision-level fusion, score-\\nlevel fusion, and model-level fusion, respectively.\\nFeature-level fusion is the most common and straightfor-\\nward way, in which all extracted features are directly con-\\ncatenated into a single high-dimensional feature vector. Then,\\na single classiﬁer can be trained with this high-dimensional\\nfeature vector for emotion recognition. Feature-level fusion\\nis thus also called Early Fusion (EF). A substantial number\\nof previous works [ 3], [5], [16], [47], [48] have testiﬁed the\\np\\nerformance of feature-level fusion on audio-visual emotion\\nrecognitiontasks.However,becauseitmergesaudioandvisualfeatures in a straightforward way, feature-level fusion can\\nnot model the complicated relationships, e.g., the difference\\non time scales and metric levels, between audio and visual\\nmodalities.\\nDecision-level fusion aims to combine several unimodal\\nemotion recognition results through an algebraic combination\\nrule. Speciﬁcally, each input modality is modeled indepen-\\ndently with an emotion classiﬁer, then these unimodal recog-\\nnition results are combined with certain algebraic rules such\\nas ”Max”, ”Min”, ”Sum”, etc. Thereby, decision-level fusion\\nis also called as Late Fusion (LF). In previousworks [ 5], [12],\\n[16], [47], [48], the authorshave adopteddecision-levelfusion\\ni\\nn audio-visual emotion recognition. Nevertheless, decision-\\nlevel fusion can not capture the mutual correlation among\\ndifferent modalities, because these modalities are assumed\\nto be independent. Therefore, decision-level fusion does not\\nconform to the fact that human beings show audio and visual\\nexpressionsin a complementaryredundantmanner,ratherthan\\na mutually independent manner.\\nScore-level fusion, as a variant of decision-level fusion, has\\nbeen recently employed for audio-visual emotion recognition\\n[16], [18]. In [16], an equally weighted summation is adopted\\nt\\no the obtained class score values. The emotion category\\ncorrespondingto themaximumvaluein thisfusedscore vector\\nis taken as the ﬁnal predicted category. Note that, score-level\\nfusion is implemented by combining the individual classi-\\nﬁcation scores, which indicate the likelihood that a sample\\nbelongs to different classes. By contrast, decision-level fusion\\nis performed by combining multiple predicted class labels.\\nModel-level fusion, as a compromise between feature-level\\nfusion and decision-level fusion, has also been used for audio-\\nvisual emotion recognition. This method aims to obtain a\\njoint feature representation of audio and visual modalities.\\nIts implementation mainly depends on the used fusion model.\\nFor instance, Zeng et al.[4] employ a Multi-stream Fused\\nH\\nidden Markov Models (MFHMM) to implement model-level\\nfusion. This MFHMM combines bimodal information from\\naudio and visual streams in terms of the maximum entropy\\nprinciple and the maximum mutual information criterion.\\nLinet al.[8] employ an error weighted semi-coupled Hidden\\nM\\narkov Models (HMM) to fuse audio and visual streams for\\nemotionrecognition.In[ 46],aTripledHiddenMarkovModels\\n(\\nTHMM) model is adopted to perform audio-visual emotion\\nrecognition. As for neural networks, model-level fusion is\\nperformed by ﬁrst concatenating feature representations of\\ndifferent hidden layers of neural networks corresponding to\\nmultiple input modalities. Then, an additional hidden layer\\nis added to learn a joint feature representation from the\\nconcatenated feature representations [ 26], [27], [28]. Limited\\nb\\ny the shallow structure like the maximum entropy principle\\n[4] and single hidden layer [ 26], [27], [28], existing model-\\nl\\nevel fusion methods are still not effective in modeling highly\\nnon-linear correlations between audio and visual modalities.\\nC. Summary\\nFrom the above-mentionedworks, we can make the follow-\\ning two summarizations.\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 4\\nFirst, low-level hand-crafted features are widely-used for\\naudio and visual emotion recognition. However, these hand-\\ncraftedfeaturescannotsufﬁcientlyandefﬁcientlydiscriminate\\nemotions. It is thus desirable to develop automatic feature\\nlearning algorithms to obtain high-level affective features.\\nCNNs [23] automatically learn features from raw pixels. With\\nr\\naw Mel-spectrogram as inputs, CNNs may present a cue\\nfor high-level audio feature extraction. CNNs have exhibited\\npromising performance on feature learning from static images\\n[44]. However it can not directly capture motion cues in\\nv\\nideos.To addressthisissue, a 3D-CNN[ 33], whichcomputes\\nf\\neature maps from both spatial and temporal dimensions, may\\nbe a possible solution.\\nSecond, Most of existing fusion methods belong to the\\nshallow fusion method, which can not effectively model\\nthe complicated non-linear joint distribution and correlations\\nof multiple modalities [ 19],e .g., the feature concatenation.\\nTherefore, it is necessary to develop deep fusion methods\\nthat leverage deep models for feature fusion. To alleviate\\nthis problem, it is hence needed to design a deep fusion\\nmodel in which multiple meaningful fusion operations can be\\nperformed to learn the complicated joint audio-visual feature\\nrepresentation. Because each RBM in a DBN can be used to\\nlearn the joint audio-visual feature representation, it may be\\nfeasible to employ a DBN consisting of multiple layers of\\nRBMs as a deep fusion model.\\nIII. PROPOSED METHOD\\nAs described in Fig. 1, our hybrid deep learning model\\nc\\nontains two individual input streams, i.e., the audio network\\nprocessing audio signals with a CNN model, and the visual\\nnetwork processing visual data with a 3D-CNN model. The\\noutputs of fully connected layers of these two networks are\\nfused in a fusion network built with a DBN model.\\nDue to the limited amount of labeled data, we ﬁrst employ\\nthe existing CNN and 3D-CNN models pre-trained on large-\\nscaleimageandvideoclassiﬁcationtaskstoinitializeourCNN\\nand 3D-CNN, respectively. Then, ﬁne-tuning is conducted for\\nthese two CNN models with the labeled emotion data. To this\\nend, we adopt the AlexNet [ 23] for CNN network initializa-\\nt\\nion,andtheC3D-Sports-1Mmodel[ 33]for3D-CNNnetwork\\ni\\nnitialization,respectively.TheAlexNet[ 23]has5convolution\\nl\\nayers (Conv1-Conv2-Conv3-Conv4-Conv5), 3 max-pooling\\nlayers(Pool1-Pool2-Pool5),and3 fully connected(FC) layers.\\nThe ﬁrst two FC layers (fc6, fc7) consist of 4096 units and\\nthe last FC layer (fc8) has 1000 dimensions corresponding\\nto 1000 image categories. The C3D-Sports-1M model [ 33]\\nc\\nontains 8 convolution layers (Conv1a-Conv2a- ···-Conv5a-\\nConv5b), 5 max-pooling layers (Pool1-Pool2-Pool3-Pool4-\\nPool5), followed by 3 FC layers. In this 3D-CNN, its fc6,\\nfc7 also have 4096 units, and its fc8 corresponds to 487 video\\ncategories.ToinitializetheaudioandvisualnetworksinFig. 1,\\nw\\ne copytheinitial networkparametersfromthecorresponding\\npre-trainedCNN and 3D-CNN models mentionedabove. Note\\nthat the fc8 parametersin these two pre-trainedmodelsare not\\nused.In the followings, we describe how to generate the inputs of\\nboth CNN and 3D-CNN, and how this hybrid deep learning\\nmodel is trained.\\nA. Generation of Network Inputs\\nSince emotional video samples may have different duration,\\nwe split each of them into a certain number of overlapping\\nsegments and then learn audio-visual features from each\\nsegment. This also enlarges the amount of training data for\\nour deep models. In detail, we ﬁrst extract the whole log\\nMel-spectrogram from audio signals. The extracted log Mel-\\nspectrogram is computed with the output of Mel-frequency\\nﬁlter banks, and shows more discriminant power than MFCC\\nfor audio emotion recognition [ 49]. Then, we use a ﬁxed\\nc\\nontext window to split the spectrogram into overlapping\\nsegments which are converted into the suitable input of CNN.\\nThe corresponding video segment in this context window is\\nused as the input of 3D-CNN after preprocessing. In this\\nway, for each video segment, we produce its Mel-spectrogram\\nsegment and video frames in the framework as illustrated in\\nFig.1. In the followings, we present how these audio and\\nv\\nisual cues are processed in detail.\\n1)Audio input generation: It is known that the 1-D\\nspectrogram, represented by the squared magnitude of the\\ntime-varying spectral characteristics of audio signals, con-\\ntains tremendous low-level acoustic information related to the\\nspeaker’s emotion expression, such as energy, pitch, formants,\\nandso on[ 50]. However,CNNs arecommonlyusedto process\\n2\\n-D or 3-D images in vision tasks [ 23]. To leverage the\\na\\nvailable CNN models and make our deep model initialization\\neasier, it is hence intuitive to transform the 1-D spectrogram\\ninto a 2-D array as the input of CNN.\\nRecently, Abdel-Hamid et al., [51] have employed a CNN\\nw\\nith a shallow 1-layer structure for speech recognition. Spe-\\ncially, the authors extract the log Mel-spectrogram from raw\\naudio signals and reorganize it into a 2-D array as the input\\nof CNN. Then 1-D convolution can be applied along the\\nfrequency axis. Nevertheless, audio emotion recognition is\\ndifferent from speech recognition [ 51]. First, 1-D convolution\\no\\nperation along the frequency axis can not capture the useful\\ntemporal information along the time axis for emotion recog-\\nnition. Second, a speech segment length of 15 frames (about\\n165 ms) widely-used for speech recognition does not carry\\nsufﬁcient temporal cues for distinguishing emotion. Some\\nprevious studies also show that 250 ms is the suggested\\nminimum segment length required for identifying emotion\\n[52], [53].\\nA\\ns shown in Fig. 1, to convert the 1-D audio signals\\ni\\nnto the suitable input of CNN, we extract three channels\\nof log Mel-spectrogram segment ( i.e., thestatic,deltaand\\ndelta−delta) with size 64×64×3. Speciﬁcally, for a\\ngiven utterance we adopt 64 Mel-ﬁlter banks from 20 to\\n8000 Hz to obtain the whole log Mel-spectrogram by using\\na 25ms Hamming window and a 10ms overlapping. Then,\\na context window of 64 frames is used to divide the whole\\nlog Mel-spectrogram into audio segments with size 64×64.\\nA shift size of 30 frames is used during segmentation, i.e.,\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 5\\ntwo adjacent segments are overlapped with 30 frames. Each\\ndivided segment hence has a length of 64 frames and its time\\nduration is 10ms×(64−1)+25ms = 655ms . In this case, the\\ndivided segment length is 2.5 times longer than the suggested\\nminimum segment length (250 ms) for identifying emotion\\n[52], [53]. Consequently, each divided segment conveys sufﬁ-\\nc\\nienttemporalcuesforidentifyingemotion.Theproduced2-D\\nMel-spectrogramsegmentwith size 64×64is takenas the ﬁrst\\nchannel (static) among three channels of Mel-spectrogram.\\nAfter extracting the static Mel-spectrogram segment with\\nsize64×64, we compute its ﬁrst-order ( delta) and second-\\norder (delta−delta) frame-to-frame time derivatives. This\\nis used to better capture the temporal information of Mel-\\nspectrogram, i.e.,the feature trajectories over time, as usually\\ndone in speech recognition tasks [ 54].\\nT\\no calculate the delta coefﬁcients of the static 2-D Mel-\\nspectrogram segment, the following regression formula is\\nused:\\ndt=N/summationtext\\nn=1n(ct+n−ct−n)\\n2N/summationtext\\nn=\\n1n2, (1)\\nwheredtis a delta coefﬁcients of frame tcomputed using\\nthe static Mel-spectrogram segment coefﬁcients ct+ntoct−n.\\nThe value of Nrepresents the regression window with a\\ntypical value of 2. Then, in the same way we can calculate the\\ndelta−deltacoefﬁcients from the obtained deltacoefﬁcients.\\nAs a result, we can obtain three channels of Mel-spectrogram\\nsegment with size: 64×64×3, as illustrated in Fig. 1.\\nT\\nhis extracted Mel-spectrogram can be regarded as the\\nRGB image feature representation of audio data. It has two\\ndesired properties. First, we can use it to implement the 2-\\nD convolution operation along the frequency and time axis,\\nratherthanthe 1-D convolutionoperation.Second,as theRGB\\nimage feature representation, it is convenient to resize it into\\nthe suitable size as the input of the pre-trained CNN models.\\nSpeciﬁcally, we initialize the audio network with the AlexNet\\n[23], which has the input size 2 27×227×3. Therefore, we\\nresize the original spectrogram with size 64×64×3into\\nnew size: 227×227×3with bilinear interpolation. In the\\nfollowings, we denote the audio input as a.\\n2)Visual input generation: After splitting the video sample\\ninto segments, we use the video segments as the 3D-CNN\\ninput. For each frame in the video segment, we run face\\ndetection, estimate the eye distance, and ﬁnally crop a RGB\\nface image of size 150×110×3, as done in [ 55], [56]. In\\nd\\netail, we employ the robust real-time face detector presented\\nby Viola and Jones [ 57] to perform automatic face detection\\no\\nn each frame. From the results of automatic face detection,\\nthe centers of two eyes can be located in a typical up-right\\nface. Then, we calculate the eye distance of facial images and\\nnormalized it to a ﬁxed distance of 55 pixels. For a facial\\nimage, it is usually observed that its height is roughly three\\ntimeslongerthantheeyedistance,whereasitswidthisroughly\\ntwice. Consequently, based on the normalized eye distance, a\\nresized RGB image of 150×110×3is ﬁnally cropped from\\neach frame. To conduct a ﬁne-tuning task, the cropped facialimage for each frame is resized to 227×227×3as the input\\nof the pre-trained 3D-CNN model. Similar resize operation is\\nalso used in a previous work [ 58].\\nT\\no make sure each video segment has 16 frames, i.e., the\\ninput size in C3D-Sports-1M model [ 33], we delete the ﬁrst\\na\\nnd lastL−16\\n2overlapping frames if a video segment has L≥\\n16frames. On the contrary, for L <16we repeat the ﬁrst and\\nlast16−L\\n2overlapping frames. It should be noted that, since\\nwe employ 64 audio frames in a context window to divide\\nthe extracted log Mel-spectrogram into audio segments, the\\ndurance of each segment is 655ms corresponding to about 20\\nvideoframesin eachvideosegment, i.e.,0.655s×30frame/s.\\nIn this case, our implementationdoesnot needto deal with the\\ncase with L <16frames. By contrast, when using 15 audio\\nframes of Mel-spectrogram segments corresponding to about\\n5 video frames ( L= 5) for experiments, we need to repeat\\nthe ﬁrst 5 and last 6 overlapping frames. We denote the visual\\ninput asv.\\nB. Network Training\\nGiven audio-visual data X={(ai,vi,yi)}i=1,2,···,K, where\\niis the index of the divided audio-visual segments, aiand\\nvidenote the audio data and visual data, respectively, and yi\\nrepresents the class label of a segment. Note that, we use the\\nclass label of the global video sample as the class label of a\\nsegmentyi. LetΥA(ai;θA)denotes the 4096-D output of fc7\\nin audio network (denoted as A) with network parameters θA.\\nSimilarly, ΥV(vi;θV)denotes the 4096-D visual feature (fc7)\\nof the visual network (denoted as V) with network parameters\\nθV.Duringnetworktraining,weﬁrst traintheaudioandvisual\\nnetworks respectively in the ﬁrst stage, then jointly train the\\nfusion network in the second stage.\\n1)Training audio and visual networks: The audio and\\nvisual networks are ﬁrst trained individually with a ﬁne-\\ntuning scheme. For the CNN and 3D-CNN, we replace their\\nﬁnal fully connected layers, i.e., fc8 layer, with two new\\nFC layers, which correspond to the emotion categories on\\ntarget audio-visual emotion recognition dataset. For instance,\\nfor 6 emotions, fc8 should produces 6 outputs. Accordingly,\\nwe predict the emotional labels with the audio and visual\\nnetworks, respectively,then calculate the prediction errors and\\nﬁnallyupdatethenetworkparameterstominimizethenegative\\nlog likelihood Lover the training data.\\nOn audiotrainingdata,we solvethe followingminimization\\nproblemto updatetheaudionetwork Awithbackpropagation:\\nmin\\nWA,θAK/summationdisplay\\ni=1L(softmax( WA·ΥA(ai;θA)),yi),(2)\\nwhereWAis the weight values of the softmax layer, and the\\nsoftmax log-loss is calculated by\\nL(A,y) =−l/summationdisplay\\nj=1yjlog(yA\\nj), (3)\\nwhereyjis thej-th value of the ground truth label, yA\\nj\\nrepresents the j-th output value of the softmax layer for the\\nnetworkA, andlrepresents the total number of class labels.\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 6\\nOn visual training data, we solve a minimization problem\\nsimilar to the audio network, since a 3D-CNN has the same\\nminimization problem as CNNs. In this way, we can minimize\\nthe prediction error of Vto update the visual network V.\\nDuring the ﬁrst stage of training, we can separately update\\nthe parameters in the audio and visual networks, producing\\nmore discriminativeaudio and visual features, i.e.,ΥA(ai;θA)\\nandΥV(vi;θV). To fuse the audio and visual features, we\\nproceed to describe the training of our fusion network.\\n2)Training fusion network: After training the audio and\\nvisual networks, we discard their fc8 layers and merge their\\nfc7 layers into the fusion network illustrated in Fig. 1. In this\\nw\\nay, two 4096-D features i.e.,ΥA(ai;θA)andΥV(vi;θV)\\nare concatenated to constitute a 8192-D feature as the in-\\nput of the fusion network f([ΥA\\ni,ΥV\\ni];θF)(denoted as F)\\nwith network parameters θF. Here,ΥA\\ni= ΥA(ai;θA)and\\nΥV\\ni= ΥV(vi;θV).\\nOur fusion network is built with a deep DBN model,\\nwhich aims to capture highly non-linear relationships across\\nmodalities,andformajointdiscriminantfeaturerepresentation\\nfor emotion classiﬁcation. It contains one visible layer, two\\nhidden layers and one output layer ( i.e., softmax layer), as\\ndepicted in Fig. 1. This DBN model is constructedby stacking\\nt\\nwo RBMs, each of which is a bipartite graph and its hidden\\nnodes are able to obtain higher-order correlation of input data\\nof visible nodes.\\nFollowing [ 59], we train the fusion network through two\\nt\\nraining steps. First, an unsupervised pre-training is imple-\\nmented in the bottom-up manner by using a greedy layer-wise\\ntraining algorithm [ 59]. This unsupervised pre-training aims\\nt\\no minimize the following reconstruction error, i.e.,\\nmin\\nWF,θFK/summationdisplay\\ni=1C(zi,z′\\ni), (4)\\nwhereKis the number of training samples, C(zi,z′\\ni)denotes\\nthe cross-entropy loss function between the input data ziand\\nthe reconstructed data z′\\ni. Here,C(zi,z′\\ni)is deﬁned as\\nC(zi,z′\\ni) =D/summationdisplay\\nd=1(−zi,jlogz′\\ni,d+(1−zi,d)log(1−z′\\ni,d)),(5)\\nSecond,afterpre-training,eachlayerofRBMs isinitialized.\\nThen, a supervised ﬁne-tuning is performed to optimize the\\nnetwork parameters. In detail, we take the last hidden layer\\noutput as the input of a classiﬁer, and compute the classiﬁ-\\ncation error. Then, back propagation is used to readjust the\\nnetwork parameters.\\nSince the input features of DBNs are continuous values, we\\nuse a Gaussian-Bernoulli RBM with 4096 hidden nodes for\\nits ﬁrst layer, a Bernoulli-Bernoulli RBM with 2048 hidden\\nnodes for its second layer, outputting 2048-D features for\\nemotion classiﬁcation. In this way, we get a 8192-4096-2048-\\nCstructure of DBNs, which is used to identify Cemotions\\non target audio-visual emotional datasets. Note that, we ﬁx\\nthe parameters in AandVduring the second stage training,\\nand update the parameters of the fusion network Fto produce\\nmoreaccurateemotionalpredictions,resultinginbetter feature\\nfusion results.\\nDQJHU M R\\\\ VDGQHVV VXUSULVH IHDU GLVJXVW\\nFig. 2. Some samples of the cropped facial images from the RML d ataset.\\nDQJHU M R\\\\ VDGQHVV VXUSULVH IHDU GLVJXVW\\nFig. 3. Some samples of the cropped facial images from the eNTE RFACE05\\ndataset.\\nC. Emotion Classiﬁcation\\nAfter ﬁnishing training the fusion network, a 2048-D joint\\nfeature representation can be computed on each audio-visual\\nsegment. Since each audio-visual video sample has a different\\nnumberofsegments,average-poolingisappliedonall segment\\nfeatures from each video sample to form the ﬁxed-length\\nglobalvideofeaturerepresentation.Ourexperimentscompared\\naverage-pooling and max-pooling, and found average-pooling\\nperforms better. Therefore, we employ average poling to\\nprocessfeaturesextractedfromsegments.Based on thisglobal\\nvideo feature representation, the linear SVM classiﬁer can be\\neasily employed for emotion identiﬁcation.\\nIV. EXPERIMENTS\\nTo testify the effectiveness of our proposed hybrid deep\\nlearning networks for audio-visual emotion recognition, we\\nconduct emotion recognition experiments on three public\\naudio-visual emotional datasets, including the acted RML\\ndataset [3], the acted eNTERFACE05 dataset [ 60], and the\\ns\\npontaneous BAUM-1s dataset [ 12]. To evaluate the perfor-\\nm\\nance of our proposed method, we present the unimodal\\naudio and video emotion recognition results, and then give\\nthe multimodal emotion recognition results integrating audio\\nand video cues.\\nA. Datasets\\nRML: The RML audio-visual dataset [ 3] is composed of\\n7\\n20 video samples from 8 subjects, speaking the six different\\nlanguages (English, Mandarin, Urdu, Punjabi, Persian, and\\nItalian). It contains the six emotions: anger, disgust, fear,\\njoy, sadness, and surprise. The audio samples are recorded\\nwith a sampling rate of 22,050 Hz with 16-bit resolution and\\nmono channel. At least two participants who do not know the\\ncorresponding language are employed in human perception\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 7\\nDQJHU MR\\\\ VDGQHVV V XUSULVH IHDU GLVJXVW\\n\\x11ĂƵŵͲϭƐ\\nFig.4. Somesamples ofthecropped facial imagesfromtheBAUM -1sdataset.\\ntest to evaluate whether the correct emotion is expressed. A\\nvideo sample is added to this dataset, when all testing subjects\\nare able to perceive the intended emotion categories. The\\naverage duration of each video sample is around 5 seconds.\\nThe size of original video frame is 720×480×3. In our\\nexperiments, for each frame in a video segment, we crop a\\nfacial image with size 150×110×3, as described in Section\\nIII-A. Fig.2shows some samples of the croppedfacial images\\no\\nn the RML dataset.\\neNTERFACE05 : The eNTERFACE05 [ 60] audio-visual\\na\\ncted dataset includes the six emotions, i.e., anger, disgust,\\nfear,joy,sadness,andsurprise,from43subjectswith14differ-\\nent nationalities. It contains 1290 video samples. Each audio\\nsample is recorded with a sampling rate of 48,000 Hz with\\n16-bit resolution and mono channel. Each subject is asked to\\nlisten to the six successive short stories, each of which is used\\nto induce a particular emotion. Two experts are employed to\\nevaluate whether the reaction expresses the intended emotion\\ninan unambiguousway.Thespeechutterancesarepulledfrom\\nvideo ﬁles of the subjects speaking in English. The video ﬁles\\nare in average 3-4 seconds long. The size of original video\\nframes is 720×576×3. Fig.3gives some samples of the\\nc\\nropped facial images on the eNTERFACE05 dataset.\\nBAUM-1s : The BAUM-1s [ 12] audio-visual spontaneous\\nd\\nataset contains1222 video samples from31 Turkish subjects.\\nThe dataset has the six basic emotions (joy, anger, sadness,\\ndisgust, fear, surprise) as well as boredom and contempt. It\\nalso contains four mental states, namely unsure, thinking,\\nconcentrating and bothered. To obtain spontaneous audio-\\nvisual expressions, emotion elicitation by watching ﬁlms is\\nemployed. The size of original video frames is 720×576×3.\\nSimilar to [ 3], [12], [60], this work focus on recognizing the\\ns\\nix basic emotions, which appear in total 521 video clips. Fig.\\n4gives some samples of the cropped facial images on the\\nB\\nAUM-1s dataset.\\nWe divide a video sample into a certain number of audio\\nandvideosegmentsasthe inputofCNN and3D-CNN, respec-\\ntively. Because multiple segments can be generated from each\\nvideosample,the amountoftrainingdatawill beenlarged.For\\nexample, we generate 11,316 audio-video segments from 720\\nvideo samples on the RML dataset, generate 16,186 segments\\nfrom 1290 video samples on the eNTERFACE05 dataset,\\nand generate 6386 segments from 521 video samples on the\\nBAUM-1s dataset, respectively.TABLE I\\nSUBJECT-INDEPENDENT UNIMODALITY RECOGNITION ACCURACY (%)ON\\nTHREE DATASETS .AlexAudioANDC3DV isual ARE AUDIO AND VISUAL\\nFEATURES EXTRACTED BY THE ORIGINAL PRE -TRAINED ALEXNET AND\\nC3D-S PORTS-1MMODELS,RESPECTIVELY .AnetANDVnetARE THE\\nLEARNED FEATURES OF THE FINE -TUNED AUDIO NETWORK (CNN) AND\\nVISUAL NETWORK (3D-CNN), RESPECTIVELY .\\nUnimodality Features RMLeNTERFACE05 BAUM-1s\\nAudioAlexA udio59.46 51.33 36.10\\nAnet 66.17 78.08 42.26\\nVisualC3DV isual53.03 48.97 41.69\\nVnet 68.09 54.35 50.11\\nB. Experimental Setup\\nF\\nor training our deep models, we use a mini-batch size of\\n30, and a stochastic gradient descent with stochastic momen-\\ntum of 0.9. The learning rate is 0.001 for ﬁne-tuning. The\\nnumber of epochs is set to 300 for CNNs, 400 for 3D-CNNs,\\nand 100 for DBNs, respectively. For the ”FC” fusion method,\\nthe dropout parameter is set to 0.3. We implement CNNs with\\ntheMatConvNettoolbox,13D-CNNswiththeCaffetoolbox,2\\nas well as DBNs with the DeeBNet toolbox.3One NVIDIA\\nG\\nTXTITANXGPUwith12GBmemory,isusedtotrainthese\\ndeep models. For emotion classiﬁcation, we utilize the LIB-\\nSVM package,4to performtheSVM algorithmwith the linear\\nk\\nernel function and one-versus-one strategy. As suggested in\\n[61], we adopt a subject-independent Leave-One-Subject-Out\\n(\\nLOSO)and Leave-One-Speakers-Group-Out(LOSGO) cross-\\nvalidation strategies for experiments, which are commonly\\nused in real-world applications. In detail, on the RML dataset\\nwith less than 10 persons, we employ the LOSO scheme.\\nOn the eNTERFACE05 and BAUM-1s datasets with more\\nthan 10 persons, we adopt the LOSGO scheme with ﬁve\\nspeaker groups. The average accuracy in the test-runs are\\nﬁnally reported to evaluate the performance of all compared\\nmethods.\\nC. Experimental Results and Analysis\\nInthissection,we presentexperimentalresultsofunimodal-\\nity and multimodality features on the RML, eNTERFACE05,\\nand BAUM-1s datasets, respectively.\\n1) UnimodalityPerformance: Totestifytheeffectivenessof\\nfeature learning with deep models, we present the recognition\\nperformance of two types of features, i.e., features extracted\\nwith the original AlexNet and C3D-Sports-1M models, and\\nthe learned features extracted with the ﬁne-tuned AlexNet and\\nC3D-Sports-1M models. For the features extracted with the\\noriginalAlexNetandC3D-Sports-1Mmodels,we directlytake\\nour generated audio and visual data as the inputs of AlexNet\\nand C3D-Sports-1M models, producing 4096-D features from\\nthe outputs of their fc7 layers, respectively.\\nTableIshows the recognition performanceof these features\\no\\nn the RML, eNTERFACE05, and BAUM-1s datasets. From\\n1available at http://www.vlfeat.org/matconvnet/\\n2available at http://caffe.berkeleyvision.org/\\n3available at http://ceit.aut.ac.ir/ keyvanrad/\\n4available at https://www.csie.ntu.edu.tw/ cjlin/libsvm/\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 8\\nTABLE II\\nSUBJECT-INDEPENDENT AUDIO EMOTION RECOGNITION PERFORMANCE\\n(%)COMPARISONS WITH PREVIOUS WORKS USING HAND -CRAFTED\\nFEATURES ON THREE DATASETS .AnetIS THE LEARNED FEATURES OF THE\\nFINE-TUNED AUDIO NETWORK (CNN).\\nDatasets Refs. Audio features Accuracy\\nRMLGaoe t al. ,[36] Prosody 51.04\\nElmadany e t al. , [38] Prosody 56.25\\nElmadany e t al. , [37] PNCC 58.33\\nZhange t al. , [62] LLD 61.86\\nOurs Anet 66.17\\neNTERFACE05Zhalehpour e t al.,[12]MFCC,RASTA-PLP 72.95\\nSchullere t al. , [35]Prosody, MFCC 72.40\\nMansoorizadeh e t al., [5]Prosody 43.00\\nBejanie t al. , [14]Prosody, MFCC 54.99\\nOurs Anet 78.08\\nBAUM-1sZhalehpour e t al.,[12]MFCC,RASTA-PLP 29.41\\nOurs Anet 42.26\\nTABLE III\\nSU\\nBJECT-INDEPENDENT VISUAL EMOTION RECOGNITION PERFORMANCE\\n(%)COMPARISONS WITH PREVIOUS WORKS USING HAND -CRAFTED\\nFEATURES ON THREE DATASETS .VnetIS THE LEARNED FEATURES OF THE\\nFINE-TUNED VISUAL NETWORK (3D-CNN).\\nDatasets Refs. Visual features Accuracy\\nRMLElmadany e t al. , [37]Gabor wavelet 64.58\\nZhange t al. , [62] LBP 56.90\\nOurs Vnet 68.09\\neNTERFACE05Zhalehpour e t al.,[12]LPQ 42.16\\nMansoorizadeh e t al., [5]Facial points 37.00\\nBejanie t al. , [14] QIM 39.27\\nOurs Vnet 54.35\\nBAUM-1sZhalehpour e t al.,[12]LPQ 45.04\\nOurs Vnet 50.11\\nthe results in Table I, we can see that the learned features\\nw\\nith ﬁne-tuned deep models (AlexNet and C3D-Sports-1M)\\nsigniﬁcantly outperform the features extracted with the origi-\\nnal pre-trained deep models. In detail, our ﬁne-tuning strategy\\nimproves the accuracies on the RML dataset from 59.46 %\\nto 66.17%for audio features, and 53.03 %to 68.09%for\\nvisual features, respectively. Similarly, on the eNTERFACE05\\ndataset, our method also makes an improvement from 51.33 %\\nto 78.08%foraudiofeatures,and48.97 %to 54.35%forvisual\\nfeatures,respectively.OntheBAUM-1sdataset, improvements\\nof 6.16%for audio features, and 8.42 %for visual features are\\nachieved, respectively. The experimental results demonstrate\\nthe effectiveness of our feature learning strategy, i.e., using\\ndeep model to learn emotional features. Our learned features\\nhave potentialto leveragethe powerfullearningability ofdeep\\nmodels to extract more discriminative cues than the manually\\ndesigned features. The experimental results also show the\\nvalidity of our ﬁne-tuning strategy. Fine-tuning allows deep\\nmodels pre-trained on other domains to learn meaningful\\nfeature representations for emotion recognition.\\nTo present the advantages of the learned features, we\\ndirectly compare our performance with the reported results of\\nprevious works using hand-crafted features on these datasets.\\nBecause these compared works use the same experimental\\nsettings with ours, i.e., subject-independent test-runs, we take\\ntheir reported results for comparison. It is not suitable tocompare our work with previous works adopting subject-\\ndependent test-runs. Table IIand Table IIIseparately give\\np\\nerformance comparisons of audio and visual emotion recog-\\nnition between our learned features and the corresponding\\nhand-crafted features.\\nFrom Table II, we can see that our learned audio features\\nw\\nithCNNsoutperformthehand-craftedaudiofeatureswidely-\\nused for audio emotion recognition [ 5], [12], [35], [36],\\n[37], [38], [62], such as prosody features, MFCC, Relative\\nS\\npectral Transform - Perceptual Linear Prediction (RASTA-\\nPLP), Power Normalized Cepstral Coefﬁcients (PNCC), and\\nother acoustic Low-level Descriptors (LLD). This shows that\\nour audio features learned by the ﬁne-tuned AlexNet model\\nis more discriminative than the hand-crafted audio features\\nfor audio emotion classiﬁcation. In addition, the promising\\nperformanceofourlearnedaudiofeaturesclearlyindicatesthat\\nit is reasonable to employ three channels of Mel-spectrogram\\nwith size 64×64×3as the input of AlexNet. It is also\\ninteresting to know that AlexNet trained on image domain\\ncan be applied to audio feature extraction. This might be\\nbecause of the powerful feature learning ability of AlexNet,\\ne.g., higher-level convolutions progressively infer semantics\\nfrom larger receptive ﬁelds. The extracted Mel-spectrogram is\\nsimilar to the RGB image representation. This representation\\nmakes it possible to ﬁrst extract meaningful low-level time-\\nfrequency features by low-level 2-D convolutions, then infer\\nmore discriminative features by higher levels of convolutions.\\nIt is also possible that, three channels of Mel-spectrogram\\npresent emotions as certain shapes and structures, which thus\\ncan be effectively perceived by AlexNet trained on the image\\ndomain. This thus presents a new method of transforming 1-D\\naudio signals into the suitable input of CNN that convention-\\nally processes 2-D or 3-D images.\\nFrom Table III, it can be observed that our learned visual\\nf\\neatures with 3D-CNNs yield better performance than the\\ncomparedhand-craftedfeatures[ 5], [12], [14], [37], [62], such\\na\\ns Gabor wavelet, LBP, LPQ, facial points and Quantized\\nImage Matrix (QIM). This demonstrates the advantages of\\nour learned visual features produced by the ﬁne-tuned C3D-\\nSports-1M model, which presents more discriminative power\\nthan the hand-craftedvisual features for visual emotion recog-\\nnition. The above experimentsclearly show that deep model is\\npowerful in feature learning and producesmore discriminative\\nfeatures than manually designed feature extraction models.\\nHowever,deep models require a large amount of training data.\\nThis thus motivates us to transfer pre-trained models on other\\ndomain for emotional feature learning.\\nTableIIandIIIshow that our learned features are more\\nd\\niscriminative to the hand-crafted features on emotion recog-\\nnition tasks. However, our feature learning needs a large\\ntraining set, and is easier to suffer from overﬁtting than\\nthe hand-crafted features. Moreover, extracting features with\\ndeep models requires more expensive computation due to the\\nmassive network parameters.\\nTo explain why we extract Mel-spectrogram segments with\\na length of 64 frames rather than 15 frames widely used\\nin speech recognition [ 51], we compare the performance of\\nt\\nwo types of extracted Mel-spectrogramswith different length,\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 9\\nTABLE IV\\nAUDIO EMOTION RECOGNITION PERFORMANCE (%)COMPARISONS\\nBETWEEN MEL-SPECTROGRAM SEGMENTS WITH SIZES 64×64×3AND\\n64×15×3.\\nMel-spectrogram RMLeNTERFACE05 BAUM-1s\\n64×1 5×350.06 52.33 33.64\\n64×6 4×366.17 78.08 42.26\\n&11\\n\\u0378\\nͶ ൈ \\u0378Ͷ ൈ ͵\\n $XGLR\\x03QHWZRUN\\n\\x16'\\x10&11\\n9LVXDO\\x03QHWZRUN\\x17\\x13\\x1c\\x19\\x10'IF\\x1a\\n\\x17\\x13\\x1c\\x19\\x10'I\\nF\\x1a\\x1b\\x14\\x1c\\x15\\x10'$YHUDJH\\n\\x10\\nSRROLQJ6909LGHR\\x03\\nH\\nPRWLRQV\\n)HDWXUH\\x10OHYHO\\x03IXVLRQ\\n&11\\n\\u0378\\nͶ ൈ \\u0378Ͷ ൈ ͵\\nͳͷͲ ൈ ͳͳͲ ൈ ͵ ൈ \\x14 \\x19$XGLR\\x03QHWZRUN\\n\\x16'\\x10&11\\n9LVXDO\\x03QHWZRUN\\x17\\x13\\x1c\\x19\\x10'IF\\x1a\\n\\x17\\x13\\x1c\\x19\\x10'I\\nF\\x1a'HFLVLRQ\\x10OHYHO\\x03\\nI\\nXVLRQ$YHUDJH\\n\\x10\\nSRROLQJ690\\n$YHUDJH\\n\\x10\\nSRROLQJ690\\x0bD\\x0c\\x03\\n\\x0b\\nE\\x0c\\x03\\nͳͷͲ ൈ ͳͳͲ ൈ ͵ ൈ \\x14 \\x19\\n9LGHR\\x03\\nH\\nPRWLRQV\\nFig. 5. The structure of feature-level fusion (a) and decisio n-level fusion (b)\\nwith two CNN models.\\ni.e.,64×64×3and64×15×3. The experimental results\\nare summarized in Table IV. From Table IV, we can see\\nt\\nhat the extracted Mel-spectrogram with size 64×64×3\\nclearly outperforms the other one. This indicate that the\\nsegment length of 15 frames is not suitable for audio emotion\\nrecognition. This might be because 15 frames is too short to\\nconveysufﬁcient informationfordistinguishingemotions[ 52],\\n[53].\\n2\\n) Multimodality Performance: To verify the effectiveness\\nof our fusion method, we compare our method with four mul-\\ntimodality fusion schemes, i.e.,feature-level fusion, decision-\\nlevel fusion, score-level fusion, as well as our recently-\\npresented method in [ 62]. Note that, this work employs a deep\\nD\\nBNmodeltobuildthefusionnetwork,whereas[ 62]usestwo\\nF\\nC layers.\\nOur goal is to recognize the emotion of the global video\\nsamples. Therefore, feature fusion methods are required to\\naggregate features extracted on audio-visual segments into a\\nTABLE V\\nMULTIMODALITY EMOTION RECOGNITION PERFORMANCE (%)\\nCOMPARISON OF SIX ENSEMBLE RULES AT DECISION -LEVEL FUSION WITH\\nTHE LEARNED FEATURES OF AnetANDVnet.\\nDecision-level RMLeNTERFACE05 BAUM-1s\\nMajority vote 63.58 69.14 45.35\\nMax 72.05 80.31 48.89\\nSum 72.79 80.85 50.01\\nMin 72.91 78.76 50.08\\nAverage 72.79 80.85 50.01\\nProduct 74.60 81.62 51.73TABLE VI\\nSU\\nBJECT-INDEPENDENT MULTIMODALITY EMOTION RECOGNITION\\nACCURACY (%)WITH THE LEARNED FEATURES OF AnetANDVnet. FC\\nDENOTES THE FUSION METHOD BUILT WITH TWO FCLAYERS IN [62],\\nD\\nBNDENOTE THE FUSION METHOD BUILT WITH A DBNMODEL.\\nFusion method RMLeNTERFACE05 BAUM-1s\\nFeature-level 74.04 81.02 51.72\\nProduct 74.60 81.62 51.73\\nScore-level 73.92 80.85 51.58\\nFC 78.84 83.55 52.35\\nDBN 80.36 85.97 54.57\\nglobal video feature representation. Then, a linear SVM coul d\\nbe used to perform emotion classiﬁcation on the generated\\nglobal video features. To this end, average-pooling is used, as\\ndoneinSection III-C. Fig.5givesthe structureoffeature-level\\nf\\nusion and decision-level fusion with our two CNN models.\\nNote that, the structure of score-level fusion is completely\\nsimilar to decision-level fusion.\\nFor decision-level fusion, six typical ensemble rules [ 63],\\n[64], including “Majority vote”, “Max”, “Sum”, “Min”, “Av-\\ne\\nrage” and “Product” are testiﬁed. For more details about the\\nsix ensemblerules,referto [ 63], [64]. Ondecision-levelfusion\\nt\\nasks, we ﬁrst investigate the performance of each ensemble\\nrule, and then ﬁnd the best one, which is used to generate\\nthe reported performance. Table Vpresents the performance\\nc\\nomparison of six ensemble rules on our learned features. As\\nshown in Table V, the “Product” rule yields best performance.\\nT\\nherefore, in the following experiments, we only report the\\nperformance of the “Product” rule for decision-level fusion.\\nWe implementscore-levelfusionreferringto the schemesin\\n[16]. Speciﬁcally, a equally weighted summation is adopted in\\nt\\nerms of the obtained class score values, as described below:\\nScorefusion= 0.5Scoreaudio+0.5Scorevisual.(6)\\nTableVIshows the recognitionperformanceof audio-visual\\nm\\nodalities by using different fusion strategies. From Table VI,\\nw\\ne can observe that the FC fusion method [ 62] outperforms\\nf\\neature-levelfusion,decision-levelfusion(Product),andscore-\\nlevel fusion. This demonstrates the advantages of the fusion\\nnetwork built with two FC layers. This also implies that the\\nFC fusion network is able to learn a joint audio-visual feature\\nrepresentation from the outputs of two ﬁne-tuned deep models\\nfor emotion identiﬁcation through the back-propagationlearn-\\ning algorithms.\\nIt also can be observed from Table VIthat, our DBN fusion\\nm\\nethod outperforms the other fusion methods. Compared\\nwith feature-levelfusion,decision-levelfusion,andscore-level\\nfusion methods, the DBN fusion can be regarded as a deep\\nfusion model. The comparison clearly shows the effectiveness\\nofthedeepfusionmethod,whichshowsbetterfeaturelearning\\nability in capturing the highly non-linear relationships across\\nmodalities. This is mainly because of the use of the multi-\\nlayer structure of DBNs, in which multiple RBMs are stacked\\nto form multiple hidden layers. A RBM is a generative model\\nrepresenting a probability distribution associated with input\\ndata. Each RBM in a DBN is able to learn the joint probability\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 10\\n88.70\\n0.90\\n0.00\\n1.94\\n3.79\\n8.622.61\\n95.50\\n4.20\\n0.97\\n3.03\\n0.000.00\\n0.00\\n71.33\\n15.53\\n1.52\\n0.001.74\\n0.90\\n20.98\\n68.93\\n8.33\\n4.310.00\\n1.80\\n3.50\\n2.91\\n83.33\\n0.006.96\\n0.90\\n0.00\\n9.71\\n0.00\\n87.07anger disgust fear joy sadness surprise\\nanger\\ndisgust\\nfear\\njoy\\nsadness\\nsurprise\\nFig. 6. Confusion matrix of multimodality emotion recogniti on results when\\nDBN performs best on the RML dataset.\\n90.61\\n1.36\\n3.00\\n0.00\\n1.34\\n4.610.94\\n89.55\\n2.00\\n3.24\\n0.45\\n1.844.23\\n2.73\\n79.50\\n0.46\\n9.82\\n8.292.35\\n1.82\\n0.50\\n92.13\\n0.45\\n2.300.47\\n3.18\\n5.50\\n0.46\\n83.93\\n3.231.41\\n1.36\\n9.50\\n3.70\\n4.02\\n79.72anger disgust fear joy sadness surprise\\nanger\\ndisgust\\nfear\\njoy\\nsadness\\nsurprise\\nFig. 7. Confusion matrix of multimodality emotion recogniti on results when\\nDBN performs best on the eNTERFACE05 dataset.\\ndistribution of input audio-video data. By using multiple\\nRBMs and the layer-wise training algorithm, DBNs can effec-\\ntively learn the non-lineardependenciesacross modalities, and\\nresults in better fusion of audio-visual features. This ﬁnding is\\nconsistent with the one in a previous work [ 19]. It is desirable\\nt\\no intuitively visualize the learned weights of our DBN model.\\nHowever, the input of our DBNs are audio-visual features,\\nratherthanthesemanticimages.Thismakethelearnedweights\\nof DBNs hard to interpret intuitively.\\nDBNs also outperform the FC fusion method in [ 62],e .g.,\\n78.84%vs. 80.36 %on the RML dataset, 83.55 %vs. 85.97 %\\non the eNTERFACE05 dataset, and 52.35 %vs. 54.57%on the\\nBAUM-1s dataset, respectively. This advantage might be due\\ntothe unsupervisedpre-traininginDBNs, whichpresentslocal\\noptimal weights for network initialization, whereas the initial\\nweights in the FC fusion method are randomly produced.\\nFig.6, Fig.7and Fig.8present the classiﬁcation confusion\\nm\\natrix on three datasets, respectively. Note that, these con-\\nfusion matrixes are obtained in terms of the average LOSO27.78\\n7.23\\n10.98\\n0.00\\n15.38\\n5.8811.11\\n65.11\\n1.83\\n0.00\\n20.00\\n0.0036.11\\n5.96\\n53.05\\n25.00\\n27.69\\n5.882.78\\n1.70\\n14.02\\n25.00\\n7.69\\n17.658.33\\n16.17\\n12.20\\n25.00\\n26.15\\n5.8813.89\\n3.83\\n7.93\\n25.00\\n3.08\\n64.71anger joy sadness fear disgust surprise\\nanger\\njoy\\nsadness\\nfear\\ndisgust\\nsurprise\\nFig. 8. Confusion matrix of multimodality emotion recogniti on results when\\nDBN performs best on the BAUM-1s dataset.\\nTABLE VII\\nMULTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION\\nWHENDBNGIVES AN AVERAGE ACCURACY OF 80.36%ON THERML\\nDATASET .\\nEmotion Precision RecallF-score\\nAnger 85.3388.7086.98\\nDisgust 89.7195.5092.51\\nFear 80.7171.3375.73\\nJoy65.5368.9367.19\\nSadness 91.0383.3387.01\\nSurprise 83.2187.0785.10\\nor LOSGO recognition results. It is interesting to ﬁnd that\\no\\nn the RML dataset, “joy” and “fear” are more difﬁcult to\\nbe identiﬁed than the other emotions. This might be because\\nthe audio-visual cues of ‘joy” and “fear” are not distinct\\nenough. On the eNTERFACE05 dataset, “sadness”, “surprise”\\nand “fear” are recognized with relatively lower accuracy, i.e.,\\nabout 80%, whereas others emotions are identiﬁed well with\\naccuracy of about 90 %. On the BAUM-1s dataset, the average\\nclassiﬁcation accuracy is much lower than the ones of the\\nother two datasets. This shows that the spontaneous emotions\\nare more difﬁcult to be recognized than the acted emotions.\\nIn addition to the classiﬁcation confusion matrix, we com-\\npute precision, recall and F-score to further measure the mul-\\ntimodality emotion recognition performance on three datasets.\\nTheexperimentalresultsarepresentedinTable VII,TableVIII\\nand Table IX, respectively. The results in these three tables\\ni\\nndicate that the three datasets show different difﬁculties in\\nrecognizing speciﬁc emotions. For example, it is easier to\\nidentify “disgust” on the RML dataset than the other two\\ndatasets. It is easier to identify “joy” on the eNTERFACE05\\nand BAUM-1s than the RML dataset.\\nD. Effect of Deep Structures in the Fusion network\\nThe structure of DBNs may heavily affect its performance\\nof fusingaudio-visualmodalities.To evaluatethe effectiveness\\nof different deep structures, we present the performance of\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 11\\nTABLE VIII\\nMULTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION\\nWHENDBNGIVES AN AVERAGE ACCURACY OF 85.97%ON THE\\nENTERFACE05 DATASET .\\nEmotion Precision RecallF-score\\nAnger 89.7890.6190.20\\nDisgust 91.3689.5590.45\\nFear 75.6979.5077.55\\nJoy92.5592.1392.34\\nSadness 86.7383.9385.31\\nSurprise 79.9579.7279.84\\nTABLE IX\\nMU\\nLTIMODALITY PERFORMANCE (%)MEASURE FOR EACH EMOTION\\nWHENDBNGIVES AN AVERAGE ACCURACY OF 54.57%ON THE\\nBAUM-1 S DATASET .\\nEmotion Precision RecallF-score\\nAnger 27.0627.7827.41\\nJoy82.0365.1172.60\\nSadness 52.4953.0552.77\\nFear 20.1425.0022.31\\nDisgust 25.6526.1525.90\\nSurprise 41.9364.7150.89\\nthree DBN fusion networks: DBN-1 (8192-4096-6), DBN-\\n2\\n(8192-4096-2048-6),and DBN-3 (8192-4096-2048-1024-6).\\nSimilarly, we also show the performance of three FC fusion\\nnetworks corresponding to DBNs: FC-1 (8192-4096-6), FC-\\n2 (8192-4096-2048-6), and FC-3 (8192-4096-2048-1024-6).\\nFor these fusion networks, as done in [ 62], a dropout layer is\\na\\ndded before the ﬁnal softmax layer correspondingto emotion\\ncategories. The dropout parameter is set to 0.3 to reduce over-\\nﬁtting.\\nTableXpresents the performance comparisons of different\\ns\\ntructuresinthefusionnetwork.FromTable X,wecanseethat\\nF\\nC-1 performs best among the three FC fusion networks. This\\nindicates that FC-1 is more effective than FC-2 and FC-3 to\\nfuse audio and visual cues. This might be because with more\\nlayers in the FC network, the massively increasing network\\nparameters make the FC network prone to over-ﬁtting. For\\nthe DBN fusion network, DBN-2 slightly outperforms DBN-\\n3, and yields substantially better performance than DBN-1.\\nDue to using multiple RBMs and the effective layer-wise\\ntraining algorithm [ 21], the deeper DBN models, i .e., DBN-\\n2 and DBN-3, exhibit better feature fusion ability than the\\n1-layer DBN-1. DBN-3 degrades the performance of DBN-2\\nmay be because DBN-3 is deeper than DBN-2, thus involves\\nmorenetworkparameters,which are more difﬁcultto optimize\\non a small-scale training dataset.\\nE. Comparisons with the state-of-the-art results\\nWe compare our method with some previous works on\\nthree datasets in Table XI. Note that these works also conduct\\ns\\nubject-independent experiments, which are consistent with\\nour experimental setting. The results in Table XIindicate that\\no\\nur method is very competitive to the state-of-the-art results.\\nSpecially, on the acted RML and eNTERFACE05 datasets,\\nour method outperforms previous works [ 5], [12], [13], [14],T\\nABLE X\\nSUBJECT-INDEPENDENT MULTIMODALITY EMOTION RECOGNITION\\nACCURACY (%)OF DIFFERENT DEEP STRUCTURES IN THE FUSION\\nNETWORK .\\nFusion method RMLeNTERFACE05 BAUM-1s\\nFC-1 78.84 83.55 52.35\\nFC-2 77.92 82.72 51.50\\nFC-3 75.67 81.25 50.43\\nDBN-1 78.50 84.16 51.61\\nDBN-2 80.36 85.97 54.57\\nDBN-3 80.10 85.02 53.38\\nTABLE XI\\nMU\\nLTIMODALITY EMOTION RECOGNITION PERFORMANCE (%)\\nCOMPARISONS WITH STATE -OF-THE-ART WORKS ON THREE DATASETS .\\nDatasets Refs. Accuracy\\nRMLSarvestani e t al. , [13]72.03\\nElmadany e t al. , [37]75.00\\nZhange t al. , [62]74.32\\nOurs 80.36\\neNTERFACE05Sarvestani e t al. , [13]70.11\\nMansoorizadeh e t al., [5]71.00\\nBejanie t al. , [14]77.78\\nZhalehpour e t al., [12]77.02\\nOurs 85.97\\nBAUM-1sZhalehpour e t al., [12]51.29\\nOurs 54.57\\n[37] by more than about 5 %. On the spontaneous BAUM-1s\\ndataset, we improve the performance of [ 12] from 51.29 %t o\\n54.57%. These compared works use hand-crafted features and\\nshallow fusion methods to integrate audio-visual modalities.\\nThis thus shows the advantages of our learned features and\\nfusion strategy.\\nIn addition, our method also improves our previous work\\n[62] from 74.32 %t o 80.36%on the RML dataset. This is\\nachievedbytwo improvements.First, comparedwith the CNN\\nmodels in [ 62], 3D-CNN models in this work can extract\\ns\\npatial-temporal cues from video. Second, as shown in our\\nexperiments,the DBN fusion methodshows better multimodal\\nfeature fusion ability than the FC fusion method in [ 62].\\nV\\n. CONCLUSIONS AND FUTUREWORK\\nThis paper presents a new method for audio-visual emotion\\nrecognitionwith a hybrid deep learning frameworkintegrating\\nCNN, 3D-CNN and DBN. The outputs of audio and visual\\nnetworks are connected with a deep DBN model to fuse\\naudio and visual cues. To learn a joint discriminant feature\\nrepresentation, this network is trained in two stages: 1) the\\naudio and visual networks are initialized with the pre-trained\\nAlexNet and C3D-Sports-1M for ﬁne-tuning, and 2) the DBN\\nfusion network is trained on the target emotion classiﬁcation\\ndatasets. Experimental results on the RML, eNTERFACE05,\\nand BAUM-1s datasets show that our hybrid deep learning\\nmodel jointly learns a discriminative audio-visual feature rep-\\nresentation, which performs better than previous hand-crafted\\nfeatures and fusion methods on emotion recognition tasks. Its\\nsuccess guarantees further research in this direction.\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 12\\nOur work transforms 1-D audio signals into three channels\\nof Mel-spectrogram with size 64×64×3as the suitable\\ninput of CNN. This extracted Mel-spectrogram is regarded\\nas a RGB image feature representation. Consequently, we\\ncan conveniently resize it into the suitable size as the input\\nof the existing CNN models pre-trained on image datasets.\\nIn this case, it is possible to ﬁne-tune the pre-trained CNN\\nmodels on target emotion datasets for audio feature extraction.\\nExperimental results demonstrate the validity of our cross-\\nmedia ﬁne-tuning scheme.\\nThis work employs 3D-CNNs to extract emotional features\\nfrom video segments. Another commonly used solution for\\nvideo featureextraction is combiningCNN and LSTM [ 65]. A\\nL\\nSTM could also be a better solution for audio-visual feature\\nfusion on video segments than average pooling. Therefore,\\nwe will investigate the performance of CNN+LSTM for facial\\nexpression recognition in our future work.\\nBesides, this work employs a two-stage learning strategy\\nto train the audio-visual networks and fusion network, respec-\\ntively. An end-to-endlearning strategy would be more concise\\nand has potential to further boost performance. Additionally,\\nan end-to-end recognition system could be constructed by\\nreferring to recent LSTM based visual recognition works\\n[65]. Therefore, end-to-endlearning and recognition strategi es\\nwould also be investigated in our future work.\\nIt shouldalso be notedthat, deep modelscommonlycontain\\na large number of network parameters, resulting in expensive\\ncomputational cost. It is thus meaningful to investigate how\\nto reduce the network parameters of deep models, e.g., deep\\ncompression [ 66], to achieve real-time emotion recognition\\nw\\nith deep model.\\nThis work employs face detector developed by Viola and\\nJones [57] for face detection in videos. It may fail in some\\nc\\nhallenging datasets like AFEW [ 67], where face images\\ns\\nuffer from substantial viewpoint and illumination changes.\\nMore robust face detectors and models will be studied in our\\nfuture work. Moreover,this work aims to employ deep models\\nto identify discrete emotions, such as anger, disgust, fear,\\njoy, sadness, and surprise. It is interesting to investigate the\\nperformance of our proposed method on dimensional emotion\\nrecognitiontasks, such as the recognitionof depression degree\\nontheAVDLCdataset[ 68].Textisanotherimportantmodality\\nc\\nharacterizing human emotion. Therefore, it is also an impor-\\ntant direction to employ deep models to learn audio, visual,\\nand textural features for multimodal emotion recognition [ 69].\\nRE\\nFERENCES\\n[1] M. S. Hossain, G. Muhammad, B. Song, M. M. Hassan, A. Alelaiwi,\\nand A. Alamri, “Audio–visual emotion-aware cloud gaming framework,”\\nIEEE Trans. Circuits Syst. Video Technol. , vol. 25, no. 12, pp. 2105–\\n2118, 2015. 1\\n[2] R. Gupta, N. Malandrakis, B. Xiao, T. Guha, M. Van Segbroec k,\\nM. Black, A. Potamianos, and S. Narayanan, “Multimodal prediction of\\naffective dimensions and depression in human-computer interactions,” in\\nProceedings of the 4th International Workshop on Audio/Visual Emotion\\nChallenge (AVEC) , Orlando, FL, USA, 2014, pp. 33–40. 1\\n[3] Y. Wang and L. Guan, “Recognizing human emotional state fr om\\naudiovisual signals*,” IEEE Trans. Multimedia , vol. 10, no. 5, pp. 936–\\n946, 2008. 1,3,6,7[4] Z. Zeng, J. Tu, B. M. Pianfetti, and T. S. Huang, “Audio-vis ual affective\\nexpression recognition through multistream fused hmm,” IEEE Trans.\\nMultimedia , vol. 10, no. 4, pp. 570–577, 2008. 1,3\\n[5] M. Mansoorizadeh and N. M. Charkari, “Multimodal informa tion fusion\\napplication to human emotion recognition from face and speech,”\\nMultimed. Tool. Appl. , vol. 49, no. 2, pp. 277–297, 2010. 1,3,8,11\\n[6] M. Glodek, S. Tschechne, G. Layher, M. Schels, T. Brosch, S . Scherer,\\nM. K¨ achele, M. Schmidt, H. Neumann, G. Palm et al., “Multiple\\nclassiﬁer systems for the classiﬁcation of audio-visual emotional states,”\\ninAffective Computing and Intelligent Interaction (ACII) . Springer,\\n2011, pp. 359–368. 1\\n[7] M. Soleymani, M. Pantic, and T. Pun, “Multimodal emotion r ecognition\\nin response to videos,” IEEE Trans. Affect. Comput. , vol. 3, no. 2, pp.\\n211–223, 2012. 1\\n[8] J.-C. Lin, C.-H. Wu, and W.-L. Wei, “Error weighted semi-c oupled\\nhidden markov model for audio-visual emotion recognition,” IEEE\\nTrans. Multimedia , vol. 14, no. 1, pp. 142–156, 2012. 1,3\\n[9] J. Wagner, E. Andre, F. Lingenfelser, and J. Kim, “Explori ng fusion\\nmethods for multimodal emotion recognition with missing data,” IEEE\\nTrans. Affect. Comput. , vol. 2, no. 4, pp. 206–218, 2011. 1\\n[10] A. Metallinou, M. W¨ ollmer, A. Katsamanis, F. Eyben, B. S chuller,\\nand S. Narayanan, “Context-sensitive learning for enhanced audiovisual\\nemotion classiﬁcation,” IEEE Trans. Affect. Comput. , vol. 3, no. 2, pp.\\n184–198, 2012. 1\\n[11] D. Gharavian, M. Bejani, and M. Sheikhan, “Audio-visual emotion\\nrecognition using fcbf feature selection method and particle swarm\\noptimization for fuzzy artmap neural networks,” Multimed. Tool. Appl. ,\\npp. 1–22, 2016. 1\\n[12] S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, “Baum -1: A\\nspontaneous audio-visual face database of affective and mental states,”\\nIEEE Trans. Affect. Comput. , 2016.1,3,6,7,8,11\\n[13] R. R. Sarvestani and R. Boostani, “Ff-skpcca: Kernel pro babilistic\\ncanonical correlation analysis,” Appl. Intell. , pp. 1–17, 2016. 1,11\\n[14] M. Bejani, D. Gharavian, and N. M. Charkari, “Audiovisua l emotion\\nrecognition using anova feature selection method and multi-classiﬁer\\nneural networks,” Neural Comput. Appl. , vol. 24, no. 2, pp. 399–412,\\n2014.1,8,11\\n[15] A. Hanjalic, “Extracting moods from pictures and sounds : Towards truly\\npersonalized tv,” IEEE Signal Process. Mag. , vol. 23, no. 2, pp. 90–100,\\n2006.1\\n[16] Y. Wang, L. Guan, and A. N. Venetsanopoulos, “Kernel cros s-modal\\nfactor analysis for information fusion with application to bimodal\\nemotion recognition,” IEEE Trans. Multimedia , vol. 14, no. 3, pp. 597–\\n607, 2012. 1,3,9\\n[17] Z. Zeng, M. Pantic, G. Roisman, T. S. Huang e t al., “A survey of affect\\nrecognition methods: Audio, visual, and spontaneous expressions,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 31, no. 1, pp. 39–58, 2009. 1,3\\n[18] Z. Xie and L. Guan, “Multimodal information fusion of aud io emotion\\nrecognition based on kernel entropy component analysis,” Int. J. Semant.\\nComput., vol. 7, no. 01, pp. 25–42, 2013. 1,3\\n[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\\n“\\nMultimodal deep learning,” in International Conference on Machine\\nLearning (ICML) , 2011, pp. 689–696. 1,4,10\\n[20] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimen sionality of\\ndata with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,\\n2006.1\\n[21] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural comput. , vol. 18, no. 7, pp. 1527–1554,\\n2006.1,11\\n[22] Y.LeCun,L.Bottou, Y.Bengio, andP.Haffner, “Gradient -based learning\\napplied to document recognition,” Proc.IEEE ,vol. 86, no. 11, pp. 2278–\\n2324, 1998. 1\\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” in Advances in neural infor-\\nmation processing systems (NIPS) , 2012, pp. 1097–1105. 1,4,5\\n[24] G. E. Hinton, “Training products of experts by minimizin g contrastive\\ndivergence,” Neural comput. , vol. 14, no. 8, pp. 1771–1800, 2002. 1\\n[25] R. Salakhutdinov and G. Hinton, “An efﬁcient learning pr ocedure for\\ndeep boltzmann machines,” Neural comput. , vol. 24, no. 8, pp. 1967–\\n2006, 2012. 2\\n[26] Y. Kim, H. Lee, and E. M. Provost, “Deep learning for robus t feature\\ngeneration in audiovisual emotion recognition,” in IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) ,\\nVancouver, BC, 2013, pp. 3687–3691. 2,3\\n[27] L.Pang,S.Zhu,and C.-W.Ngo,“Deep multimodal learning foraffective\\nanalysis and retrieval,” IEEE Trans. Multimedia , vol. 17, no. 11, pp.\\n2008–2020, 2015. 2,3\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 13\\n[28] L. Pang and C.-W. Ngo, “Mutlimodal learning with deep boltzmann\\nmachine for emotion prediction in user generated videos,” in Pro-\\nceedings of the 5th ACM on International Conference on Multimedia\\nRetrieval(ICMR) , Shanghai, China, 2015, pp. 619–622. 2,3\\n[29] C. Barat and C. Ducottet, “String representations and di stances in deep\\nconvolutional neural networks for image classiﬁcation,” Pattern Recogn. ,\\nvol. 54, pp. 104–115, 2016. 2\\n[30] Y. Cao, Y. Chen, and D. Khosla, “Spiking deep convolution al neural\\nnetworks for energy-efﬁcient object recognition,” Int. J. Comput. Vis. ,\\nvol. 113, no. 1, pp. 54–66, 2015. 2\\n[31] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, “Reading\\ntext in the wild with convolutional neural networks,” Int. J. Comput.\\nVis., vol. 116, no. 1, pp. 1–20, 2016. 2\\n[32] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S . Yang,\\nZ. Wang, C.-C. Loy et al., “Deepid-net: Deformable deep convolutional\\nneural networks for object detection,” in Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR) , 2015,\\npp. 2403–2412. 2\\n[33] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Palur i, “Learning\\nspatiotemporal features with 3d convolutional networks,” in 2015 IEEE\\nInternational Conference on Computer Vision (ICCV) , Santiago, Chile,\\n2015, pp. 4489–4497. 2,4,5\\n[34] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech e motion\\nrecognition: Features, classiﬁcation schemes, and databases,” Pattern\\nRecogn., vol. 44, no. 3, pp. 572–587, 2011. 3\\n[35] B. Schuller, B. Vlasenko, F. Eyben, G. Rigoll, and A. Wend emuth,\\n“Acoustic emotion recognition: A benchmark comparison of perfor-\\nmances,” in IEEE Workshop on Automatic Speech Recognition & Un-\\nderstanding (ASRU) , Merano, 2009, pp. 552–557. 3,8\\n[36] L.Gao, L.Qi, and L.Guan, “Information fusion based on ke rnel entropy\\ncomponent analysis in discriminative canonical correlation space with\\napplication to audio emotion recognition,” in 2016 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) ,\\nShanghai, China, 2016, pp. 2817–2821. 3,8\\n[37] N. E.D.Elmadany, Y. He, and L.Guan, “Multiview emotion r ecognition\\nvia multi-set locality preserving canonical correlation analysis,” in\\n2016 IEEE International Symposium on Circuits and Systems (ISCAS) ,\\nMontral, QC, Canada, 2016, pp. 590–593. 3,8,11\\n[38] ——, “Multiview learning via deep discriminative canoni cal correlation\\nanalysis,” in 2016 IEEE International Conference on Acoustics, Speech\\nand Signal Processing (ICASSP) , Shanghai, China, 2016, pp. 2409–\\n2413.3,8\\n[39] Y. Tian, T. Kanade, and J. F. Cohn, “Facial expression rec ognition,” in\\nHandbook of face recognition . Springer, 2011, pp. 487–519. 3\\n[40] X. Zhao and S. Zhang, “A review on facial expression recog nition:\\nFeature extraction and classiﬁcation,” IETE Tech. Review , vol. 33, no. 5,\\npp. 505–507, 2016. 3\\n[41] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu, “Compar ison\\nbetween geometry-based and gabor-wavelets-based facial expression\\nrecognition using multi-layer perceptron,” in Third IEEE International\\nConference on Automatic Face and Gesture Recognition , Nara, 1998,\\npp. 454–459. 3\\n[42] G. Zhao and M. Pietikainen, “Dynamic texture recognitio n using local\\nbinary patterns with an application to facial expressions,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915–928, 2007. 3\\n[43] A. Dhall, A. Asthana, R. Goecke, and T. Gedeon, “Emotion r ecognition\\nusing phog and lpq features,” in 2011 IEEE International Conference\\non Automatic Face & Gesture Recognition and Workshops (FG 2011) ,\\nSanta Barbara, USA, 2011, pp. 878–883. 3\\n[44] W. Ding, M. Xu, D. Huang, W. Lin, M. Dong, X. Yu, and H. Li,\\n“\\nAudio and face video emotion recognition in the wild using deep\\nneural networks and small datasets,” in Proceedings of the 18th ACM\\nInternational Conference on Multimodal Interaction (ICMI) , Tokyo,\\nJapan, 2016, pp. 506–513. 3,4\\n[45] S. Hochreiter and J. Schmidhuber, “Long short-term memo ry,”Neural\\ncomput., vol. 9, no. 8, pp. 1735–1780, 1997. 3\\n[46] M. Song, M. You, N. Li, and C. Chen, “A robust multimodal ap proach\\nfor emotion recognition,” Neurocomput. , vol. 71, no. 10, pp. 1913–1920,\\n2008.3\\n[47] B. Schuller, R. M¨ ueller, B. H¨ oernler, A. H¨ oethker, H. Konosu, and\\nG. Rigoll, “Audiovisual recognition of spontaneous interest within\\nconversations,” in 9th international conference on Multimodal interfaces\\n(ICMI), Aichi, Japan, 2007, pp. 30–37. 3\\n[48] C. Busso, Z. Deng, S. Yildirim, M. Bulut, C. M. Lee, A. Kaze mzadeh,\\nS. Lee, U. Neumann, and S. Narayanan, “Analysis of emotion recog-\\nnition using facial expressions, speech and multimodal information,” in6th international conference on Multimodal interfaces (ICMI) , PA,USA,\\n2004, pp. 205–211. 3\\n[49] C. Busso, S. Lee, and S. S. Narayanan, “Using neutral spee ch models\\nfor emotional speech analysis,” in Interspeech , Antwerp, Belgium, 2007,\\npp. 2225–2228. 4\\n[50] L. He, M. Lech, N. Maddage, and N. Allen, “Stress and emoti on\\nrecognition using log-gabor ﬁlter analysis of speech spectrograms,” in\\n3rd International Conference on Affective Computing and Intelligent\\nInteraction and Workshops (ACII) , Amsterdam, The Netherlands, 2009,\\npp. 1–6. 4\\n[51] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn ,\\nand D. Yu, “Convolutional neural networks for speech recognition,”\\nIEEE/ACM Trans. Audio, Speech, Lang. Process. , vol. 22, no. 10, pp.\\n1533–1545, 2014. 4,8\\n[52] E. M. Provost, “Identifying salient sub-utterance emot ion dynamics us-\\ning ﬂexible units and estimates of affective ﬂow,” in IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) ,\\nVancouver, BC, 2013, pp. 3682–3686. 4,5,9\\n[53] M. W¨ ollmer, M. Kaiser, F. Eyben, B. Schuller, and G. Rigo ll, “Lstm-\\nmodeling of continuous emotions in an audiovisual affect recognition\\nframework,” Image Vis. Comput. , vol. 31, no. 2, pp. 153–163, 2013. 4,\\n5,9\\n[54] X. Huang, A. Acero, H.-W. Hon, and R. Foreword By-Reddy, S poken\\nlanguage processing: A guide to theory, algorithm, and system develop-\\nment. Prentice Hall PTR, 2001. 5\\n[55] C. Shan, S. Gong, and P. W. McOwan, “Facial expression rec ognition\\nbased on local binary patterns: A comprehensive study,” Image Vis.\\nComput., vol. 27, no. 6, pp. 803–816, 2009. 5\\n[56] X. Zhao and S. Zhang, “Facial expression recognition usi ng local binary\\npatterns and discriminant kernel locally linear embedding,” EURASIP J.\\nAdv. signal process. , vol. 2012, no. 1, pp. 1–9, 2012. 5\\n[57] P. Viola and M. J. Jones, “Robust real-time face detectio n,”Int. J.\\nComput. Vis. , vol. 57, no. 2, pp. 137–154, 2004. 5,12\\n[58] L. Zheng, Z. Bie, Y. Sun, J. Wang, C. Su, S. Wang, and Q. Tian , “Mars:\\nAvideo benchmark for large-scale person re-identiﬁcation,” in European\\nConference on Computer Vision (ECCV) , Amsterdam, The Netherlands,\\n2016, pp. 868–884. 5\\n[59] G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural comput. , vol. 18, no. 7, pp. 1527–1554,\\n2006.6\\n[60] O. Martin, I. Kotsia, B. Macq, and I. Pitas, “The enterfac e’05 audio-\\nvisual emotion database,” in 22nd International Conference on Data\\nEngineering Workshops , Atlanta, GA, USA, 2006, pp. 8–8. 6,7\\n[61] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Dev illers, C. A.\\nM¨ uller, and S. S. Narayanan, “The interspeech 2010 paralinguistic\\nchallenge.” in INTERSPEECH ,Makuhari, Chiba, Japan, 2010,pp.2794–\\n2797.7\\n[62] S. Zhang, S. Zhang, T. Huang, and W. Gao, “Multimodal deep con-\\nvolutional neural network for audio-visual emotion recognition,” in\\nProceedings of the 6th ACM on International Conference on Multimedia\\nRetrieval(ICMR) , New York, USA, 2016, pp. 281–284. 8,9,10,11\\n[63] J.Kittler, M.Hatef, R.P.Duin, and J.Matas, “On combini ng classiﬁers,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 20, no. 3, pp. 226–239,\\n1998.9\\n[64] Z.Sun,Q.Song,X.Zhu,H.Sun,B.Xu,and Y.Zhou,“Anovele nsemble\\nmethod for classifying imbalanced data,” Pattern Recogn. , vol. 48, no. 5,\\npp. 1623–1637, 2015. 9\\n[65] Y. Yan, B. Ni, Z. Song, C. Ma, Y. Yan, and X. Yang, “Person re -\\nidentiﬁcation via recurrent feature aggregation,” in European Conference\\non Computer Vision (ECCV) , Amsterdam, The Netherlands, 2016, pp.\\n701–716. 12\\n[66] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Ch en,\\n“Compressing neural networks with the hashing trick,” in Proceedings\\nof The 32nd International Conference on Machine Learning (ICML) ,\\nLille, France, 2015, pp. 2285–2294. 12\\n[67] A.Dhall,O.RamanaMurthy,R.Goecke, J.Joshi,andT.Ged eon,“Video\\nand image based emotion recognition challenges in the wild: Emotiw\\n2015,” in Proceedings of the 2015 ACM on International Conference on\\nMultimodal Interaction (ICMI) , Seattle, WA, USA, 2015, pp. 423–426.\\n12\\n[68] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. B ilakhia,\\nS. Schnieder, R. Cowie, and M. Pantic, “The continuous audio/visual\\nemotion and depression recognition challenge,” in The 21st ACM Inter-\\nnational Conference on Multimedia(MM) , Barcelona, Spain, 2013. 12\\n[69] S. Poria, H. Peng, A. Hussain, N. Howard, and E. Cambria, “ Ensemble\\napplication of convolutional neural networks and multiple kernel learn-\\ning for multimodal sentiment analysis,” Neurocomput. , 2017.12\\n1051-8215 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2719043, IEEE\\nTransactions on Circuits and Systems for Video Technology\\nJOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, MARCH 2016 14\\nShiqing Zhang r eceived the Ph.D. degree at school\\nof Communication and Information Engineering,\\nUniversity of Electronic Science and Technology of\\nChina, in 2012. Currently, he is a postdoctor with\\nthe School of Electronic Engineering and Computer\\nScience, Peking University, Beijing, China, and also\\nworks as an associate professor of department of\\nphysics and electronics engineering, Taizhou Univer-\\nsity, China. His research interests include audio and\\nimage processing, affective computing and pattern\\nrecognition.\\nShiliang Zhang i s currently a tenure-track Assistant\\nProfessor in School of Electronic Engineering and\\nComputer Science, Peking University. He received\\nthe Ph.D. degree in computer science from Institute\\nof Computing Technology, Chinese Academy of\\nSciences in 2012. He was a Postdoctoral Scientist\\nin NEC Labs America and a Postdoctoral Research\\nFellow in University of Texas at San Antonio.\\nDr. Zhang’s research interests include large-scale\\nimage retrieval and computer vision for autonomous\\ndriving. He was awarded the National 1000 Youth\\nTalents Plan of China, Outstanding Doctoral Dissertation Awards from both\\nChinese Academy of Sciences and Chinese Computer Federation (CCF),\\nPresident Scholarship by Chinese Academy of Sciences, NEC Laboratories\\nAmerica Spot Recognition Award, and the Microsoft Research Fellowship.\\nHe has published over 30 papers in journals and conferences including IEEE\\nTrans. on Pattern Analysis and Machine Intelligence, IEEE Trans. on Image\\nProcessing, IEEE Trans. on Multimedia, ACM Multimedia, and ICCV. He is\\nthe recipient of Top 10% Paper Award in IEEE MMSP 2011. His research\\nis supported by the National 1000 Youth Talents Plan and Natural Science\\nFoundation of China (NSFC).\\nTiejun Huang T iejun Huang (M’01 −SM’12) is cur-\\nrently a Professor with the School of Electronic En-\\ngineering and Computer Science, Peking University,\\nBeijing, China, where he is also the Director of the\\nInstitute for Digital Media Technology. He received\\nthe Ph.D. degree in pattern recognition and intelli-\\ngent system from Huazhong (Central China) Uni-\\nversity of Science and Technology, Wuhan, China,\\nin 1998, and the masters and bachelors degree in\\ncomputer science from the Wuhan University of\\nTechnology, Wuhan, in 1995 and 1992, respectively.\\nHis research area includes video coding, image understanding, digital right\\nmanagement, and digital library. He has authored or co-authored over 100\\npeer-reviewed papers and three books. He is a member of the Board of Di-\\nrector for Digital Media Project, the Advisory Board of the IEEE Computing\\nSociety, and the Board of the Chinese Institute of Electronics.\\nWen Gao W en Gao (M’92 −SM’05−F’09) received\\nthe Ph.D. degree in electronics engineering from the\\nUniversity of Tokyo, Tokyo, Japan, in 1991.\\nHe is currently a Professor with the School of\\nElectronic Engineering and Computer Science with\\nPeking University, Beijing, China. Before joining\\nPeking University, he was a Professor of Computer\\nScience with the Harbin Institute of Technology,\\nHarbin, China, from 1991 to 1995, and a Profes-\\nsor with the Institute of Computing Technology,\\nChinese Academy of Sciences, Beijing, China. He\\nhas authored ﬁve books and more than 600 technical articles in refereed\\njournals and conference proceedings in image processing, video coding\\nand communication, pattern recognition, multimedia information retrieval,\\nmultimodal interface, and bioinformatics.\\nDr. Gao serves the editorial board for several journals, such as IEEE\\nTRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECH-\\nNOLOGY, IEEE TRANSACTIONS ON MULTIMEDIA, IEEE TRANS-\\nACTIONS ON AUTONOMOUS MENTAL DEVELOPMENT, EURASIP\\nJournal of Image Communications, and Journal of Visual Communication\\nand Image Representation. He chaired a number of prestigious international\\nconferences on multimedia and video signal processing, such as the IEEE\\nICME and ACM Multimedia, and also served on the advisory and technical\\ncommittees of numerous professional organizations.\\nQi TianQ i Tian (S’95 −M’96−SM’03) received the\\nB.E. degree in electronic engineering from Tsinghua\\nUniversity, Beijing, China, in 1992, the M.S. degree\\nin electrical and computer engineering from Drexel\\nUniversity, Philadelphia, PA, USA, in 1996, and the\\nPh.D. degree in electrical and computer engineering\\nfrom the University of Illinois at Urbana-Champaign\\n(UIUC), Champaign, IL, USA, in 2002.\\nHe is currently a Full Professor with the Depart-\\nment of Computer Science, the University of Texas\\nat San Antonio (UTSA), San Antonio, TX, USA.\\nHe was a tenure-track Assistant Professor from 2002 to 2008 and a tenured\\nAssociate Professor from 2008 to 2012 at the same institution. During 2008\\nand 2009, he took one-year faculty leave as Lead Researcher with the Media\\nComputing Group, Microsoft Research Asia, Beijing, China. He has authored\\nor coauthored over 310 refereed journal and conference papers. His research\\ninterests include multimedia information retrieval, computer vision, pattern\\nrecognition and bioinformatics.\\nDr. Tian is a Member of the ACM (2004). He is the Associate Editor of the\\nIEEE TRANSACTIONS ON MULTIMEDIA, the IEEE TRANSACTIONS\\nON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, and the\\nMultimedia System Journal, and is an Editorial Board Member for the Journal\\nof Multimedia and the Journal of Machine Vision and Applications. He is a\\nGuest Editor for a number of journals, including the IEEE TRANSACTIONS\\nON MULTIMEDIA and the Journal of Computer Vision and Image Under-\\nstanding. Hewas the recipient of the 2014 Research Achievement Award from\\nthe College of Science, UTSA, and the 2010 ACM Service Award. He was\\ncoauthor of the recipient of the Best Paper Award in ACM ICMR 2015, Best\\nPaper Award in PCM 2013, Best Paper Award in MMM 2013, Best Paper\\nAward in ACM ICIMCS 2012, Top 10% Paper Award in MMSP 2011, Best\\nStudent Paper in ICASSP 2006, a Best Student Paper Candidate in ICME\\n2015, and a Best Paper Candidate in PCM 2007.\\n\",\n",
       " 'Received January 16, 2019, accepted February 18, 2019, date of publication March 5, 2019, date of current version March 26, 2019.\\nDigital Object Identifier 10.1 109/ACCESS.2019.2901521\\nLearning Affective Video Features for\\nFacial Expression Recognition\\nvia Hybrid Deep Learning\\nSHIQING ZHANG\\n1, XIANZHANG PAN1, YUELI CUI1, XIAOMING ZHAO1, AND LIMEI LIU2\\n1Institute of Intelligent Information Processing, Taizhou University, Taizhou 318000, China\\n2Institute of Big Data and Internet Innovation, Hunan University of Commerce, Changsha 410205, China\\nCorresponding author: Shiqing Zhang (tzczsq@163.com)\\nThis work was supported in part by the Zhejiang Provincial National Science Foundation of China under Grant LY16F020011, in part by\\nthe Major Project for National Natural Science Foundation of China under Grant 71790615, in part by the Taizhou Science and Technology\\nProject under Grant 1802gy06, and in part by the Training Project of Taizhou University under Grant 2017PY026 and Grant 2018JQ003.\\nABSTRACT One key challenging issues of facial expression recognition (FER) in video sequences is\\nto extract discriminative spatiotemporal video features from facial expression images in video sequences.\\nIn this paper, we propose a new method of FER in video sequences via a hybrid deep learning model.\\nThe proposed method \\x1crst employs two individual deep convolutional neural networks (CNNs), including\\na spatial CNN processing static facial images and a temporal CN network processing optical \\x1dow images,\\nto separately learn high-level spatial and temporal features on the divided video segments. These two CNNs\\nare \\x1cne-tuned on target video facial expression datasets from a pre-trained CNN model. Then, the obtained\\nsegment-level spatial and temporal features are integrated into a deep fusion network built with a deep belief\\nnetwork (DBN) model. This deep fusion network is used to jointly learn discriminative spatiotemporal\\nfeatures. Finally, an average pooling is performed on the learned DBN segment-level features in a video\\nsequence, to produce a \\x1cxed-length global video feature representation. Based on the global video feature\\nrepresentations, a linear support vector machine (SVM) is employed for facial expression classi\\x1ccation tasks.\\nThe extensive experiments on three public video-based facial expression datasets, i.e., BAUM-1s, RML,\\nand MMI, show the effectiveness of our proposed method, outperforming the state-of-the-arts.\\nINDEX TERMS Facial expression recognition, spatio-temporal features, hybrid deep learning, deep\\nconvolutional neural networks, deep belief network.\\nI. INTRODUCTION\\nFacial expression is one of the most natural nonverbal ways\\nfor expressing human emotions and intentions. In recent\\nyears, automatic facial expression recognition (FER), which\\naims to analyze and understand human facial behavior, has\\nbecome an increasingly active research topic in the domains\\nof computer vision, arti\\x1ccial intelligence, pattern recognition,\\netc. This is because FER has many potential applications\\nsuch as human emotion perception, social robotics, human-\\ncomputer interaction and healthcare [1]\\x15[5].\\nFER methods can be divided into two categories: video\\nsequence-based methods (dynamic) and image-based meth-\\nods (static). Most previous FER studies focus on identifying\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Tariq Ahamed Ahanger.facial expressions from static facial images [1]\\x15[4]. Although\\nthese image-based methods can effectively derive spatial\\ninformation from still images, they cannot capture the tempo-\\nral variability in consecutive frames in video sequences. As a\\ndynamic event, classifying facial expression from consecu-\\ntive frames in a video is more natural, since video sequences\\nprovides much more information for FER than static facial\\nimages. One key issue for video sequence-based FER meth-\\nods is how to effectively encode input video sequences into\\nan appropriate feature representation. Currently, the main-\\nstream methods employ hand-designed feature representa-\\ntions, such as Gabor motion energy [6], Local Binary Patterns\\nfrom Three Orthogonal Planes (LBP-TOP) [7] or Local\\nPhase Quantization from TOP (LPQ-TOP) [8]. However,\\nthese hand-designed feature representations are low-level to\\ndiscriminate dynamic facial expressions. Recently, the deep\\nVOLUME 7, 20192169-3536 \\n2019 IEEE. Translations and content mining are permitted for academic research only.\\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.32297\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nneural network driven feature learning representations from\\ndata may achieve better performance without requiring\\ndomain expertise [9]\\x15[15].\\nInspired by the strong feature learning ability of deep neu-\\nral networks, this paper proposes a new deep neural network-\\nbased FER method in video sequences by using a hybrid deep\\nlearning model. Our hybrid deep learning model contains\\nthree deep models. The \\x1crst two deep models are deep Con-\\nvolutional Neural Networks (CNNs) [16], including a spatial\\nCNN network processing static facial images and a tem-\\nporal CNN network processing optical \\x1dow images. These\\ntwo CNNs are separately used to learn high-level spatial\\nfeatures and temporal features on the divided video segments.\\nThe third deep model is a deep fusion network built with a\\nDeep Belief Network (DBN) [17] model, which is trained to\\njointly learn a discriminative spatio-temporal segment-level\\nfeature representation. When \\x1cnishing the joint training of\\na DBN, an average-pooling is applied on all the divided video\\nsegments to produce a \\x1cxed-length global video feature rep-\\nresentation. Then, a linear Support Vector Machine (SVM)\\nis adopted to perform facial expression classi\\x1ccation tasks in\\nvideo sequences.\\nIt is noted that two-stream CNNs have been successfully\\nused for video action recognition [18]. Nevertheless, in [18],\\na score-level scheme, which belongs to a shallow fusion\\nmethod, is used to merge different features produced by two-\\nstream CNNs. This shallow fusion method is not able to\\neffectively model the complicated non-linear joint distribu-\\ntion of multiple input modalities [19]. To tackle this issue,\\nit is desired to design deep fusion methods which lever-\\nage a deep fusion model to implement multiple meaning-\\nful feature fusion operations. Since a DBN model consists\\nof multiple RBMs, each of which can be used to jointly\\nlearn feature representations of multiple input modalities,\\nit may be feasible to use a DBN model as a deep fusion\\nmethod to integrate different features produced by two-\\nstream CNNs. This motivates us to develop a hybrid deep\\nleaning method to learn video features for facial expression\\nrecognition in video sequences. Experiment results on three\\npublic video-based facial expression databases, including\\nthe BAUM-1s database [20], the RML database [21], and\\nthe MMI database [22], are presented to demonstrate the\\neffectiveness of the proposed method on FER tasks in video\\nsequences.\\nThe distinct features of this paper can be summarized\\nin two-fold: (1) We propose a hybrid deep learning model,\\ncomprising a spatial CNN network, a temporal CNN net-\\nwork and a deep fusion network built with a DBN model,\\nto apply for FER in video sequences. To the best of our knowl-\\nedge, it is the \\x1crst time to employ a hybrid deep learning\\nmodel to learn video features for FER in video sequences.\\n(2) To deeply fuse the spatial CNN features and temporal\\nCNN features, we employ a deep DBN model as a deep\\nfusion network to learn a joint discriminative spatio-temporal\\nsegment-level feature representation for FER. Extensive\\nexperiments are conducted on three public video-based facialexpression datasets, and experiment results demonstrate that\\nour method outperforms the-state-of-the-arts.\\nThe structure of this paper is organized as follows.\\nSection 2 reviews the related work in brief. Section 3\\ndescribes our proposed method in detail. Experiment results\\nand analysis are given in Section 4. Section 5 presents the\\nconclusions and future work.\\nII. RELATED WORK\\nIn this section, we review the recent works related to feature\\nextraction in FER in video sequences, which uses hand-\\ndesigned features and deep learning-based features.\\nA. HAND-DESIGNED FEATURE-BASED METHOD\\nFor facial feature representation in static images, a variety\\nof local image descriptors, including Local Binary Pattern\\n(LBP) [23], Histogram of Oriented Gradient (HOG) [24],\\nand Scale Invariant Feature Transform (SIFT) [25] have been\\nwidely used for FER. For dynamic expression recognition,\\nthese typical local features have been extended and applied\\nto video sequences, such as LBP-TOP [7], LPQ-TOP [8],\\n3D-HOG [26], 3D-SIFT [27], respectively. Hayat et al. [28]\\ncompare the performance of various dynamic descriptors\\nincluding HOG, 3D-HOG, 3D-SIFT and LBP-TOP by using\\nbag of features framework for video-based FER, and \\x1cnd that\\nLBP-TOP performs best among these dynamic descriptors.\\nAdditionally, spatio-temporal Gabor motion energy \\x1clters [6]\\nis presented for low-level integration of spatio-temporal\\ninformation on FER tasks.\\nRecently, some efforts have been conducted to develop\\nmore powerful spatio-temporal feature extraction methods\\nfor FER. For instance, Liu et al. [29] present an expressionlet-\\nbased spatio-temporal manifold descriptor which shows\\nthe superiority over traditional methods on FER tasks.\\nFan and Tjahjadi [30] provide a spatio-temporal feature based\\non local Zernike moment and motion history image for\\ndynamic FER. Yan [31] proposes a collaborative discrim-\\ninative multi-metric learning for FER in video sequences.\\nIn particular, for each video sequence they \\x1crstly calculate\\nmultiple feature descriptors such as 3D-HOG, and geometric\\nwarp features. Then, these extracted multiple features are\\nemployed to learn multiple distance metrics collaboratively\\nto obtain complementary and discriminative information for\\ndynamic FER.\\nB. DEEP LEARNING-BASED METHOD\\nIn recent years, deep CNNs [16], [32]\\x15[34], composed of\\nmultiple convolution layers and pooling layers, have domi-\\nnated various computer vision tasks such as image classi\\x1cca-\\ntion, object detection and face recognition. These deep CNNs\\nextends the traditional CNN model [35] into a deep multi-\\nlayered architecture which consists of \\x1cve convolution layers\\nfollowed by three max-pooling layers.\\nOne of the major drawbacks of conventional CNNs is that\\nthey are able to extract spatial relationships of input images,\\nbut cannot model the temporal relationships of them in\\n32298 VOLUME 7, 2019\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nFIGURE 1. The framework of our proposed hybrid deep learning network for facial expression recognition in video sequences.\\na video sequence. To solve this problem, the recently-\\ndeveloped 3D-CNNs [36] may present a possible solution.\\n3D-CNNs can extract spatio-temporal features in a video\\nsequence by means of sliding over the temporal dimen-\\nsion of input data as well as the spatial dimension simul-\\ntaneously. In recent years, 3D-CNNs have been used to\\nlearn spatio-temporal expression representations from suc-\\ncessive frames in video sequences [12], [15]. In addi-\\ntion, a variant of 3D-CNNs is 3DCNN-DAP [14] used for\\ndynamic FER. In 3DCNN-DAP, a constraint of Deformable\\nAction Parts (DAP) is incorporated into the basic 3D-CNN\\nframework. Similar to 3DCNN-DAP, Jung et al. [10] propose\\na small temporal CNN to extract temporal geometric features\\nfrom facial landmark points. Although these 3D-CNNs based\\nmethods have achieved good performance on FER tasks in\\nvideo sequences, but they still has a drawback. That is,\\nthese methods cannot take the deep fusion of spatio-temporal\\nfeatures into account simultaneously in the procedure of\\nextracting them.\\nTo tackle this problem, two-stream CNNs used for video\\naction recognition [18], may present a cue. However, the used\\nshallow fusion method in [18] based on a score-level scheme,\\ncannot able to effectively model the complicated non-linear\\njoint distribution of multiple input modalities. To make full\\nuse of the advantages of two-stream CNNs, we design a deep\\nfusion network built with a deep DBN model to jointly learn\\nthe outputs of two-stream CNNs. This is our proposed hybrid\\ndeep learning model. Then, we apply this hybrid deep learn-\\ning model for FER in video sequences. Experiment results\\non three video-based facial expression databases demonstrate\\nthe advantages of our proposed method.\\nIII. OUR METHOD\\nFigure 1 shows the framework of our proposed hybrid deep\\nlearning model. As depicted in Fig.1, our method is composedof two individual channels of input streams, i.e., a spatial\\nCNN network processing static frame-level cropped facial\\nimages and a temporal CNN network processing optical\\n\\x1dow images produced between consecutive frames. To inte-\\ngrate the learned spatio-temporal features represented by\\nthe outputs of fully connected layers of these two CNNs, a\\nfusion network built with a deep DBN model is designed.\\nIn detail, our method contains four key steps: (1) genera-\\ntion of CNN inputs (2) spatio-temporal feature learning with\\nCNNs (3) spatio-temporal fusion with DBNs (4) video-based\\nexpression classi\\x1ccation. In the followings, we present the\\ndetails about abovementioned four steps of our method.\\nA. GENERATION OF CNN INPUTS\\nSince CNNs require a \\x1cxed size of input data, we divide each\\nvideo sample with different durations into a certain number\\nof \\x1cxed-length segments as inputs of CNNs. This not only\\nproduces appropriate inputs of CNNs, but also augments the\\namount of training data to some extent.\\nFollowing in [18], the divided segment length Lis set\\nto be LD16 for its good performance when using the\\ntemporal CNN network. As a result, in the latter experiments,\\nwe divide each video sample into a \\x1cxed-length segment\\nwith LD16. To this end, when L>16 we eliminate the\\n\\x1crst and last ( L\\x0016)=2 frames. Oppositely, when L<16,\\nwe simply duplicate the \\x1crst and last (16 \\x00L)=2 frames. In this\\nway, we make sure that each divided segment has a length\\nofLD16.\\n1) INPUTS OF TEMPORAL CNNs\\nTo produce suitable inputs of temporal CNNs, we extract\\noptical \\x1dow images between consecutive frames in a video\\nsequence. Optical \\x1dow images represent the displacement\\nchanges of corresponding positions between consecutive\\nframes. Following in [37], we \\x1crstly transform the values of\\nVOLUME 7, 2019 32299\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nthe motion \\x1celd dx,dyinto the interval [0, 255] by\\nQdxjyDadxjyCb; (1)\\nwhere aD16,bD128.\\nThen, the transformed \\x1dow maps are conserved as an opti-\\ncal \\x1dow image containing three channels, which corresponds\\nto motionQdx,Qdyand the optical \\x1dow magnitude. In this\\nway, we \\x1cnally produce an optical \\x1dow image with size of\\n227\\x02227\\x023. It is noted that a video segment LD16 can\\ngenerate 15 optical \\x1dow images as inputs of temporal CNNs,\\nsince two consecutive frames yield one optical \\x1dow image.\\n2) INPUTS OF SPATIAL CNNs\\nFor inputs of spatial CNNs, we employ a cropped facial\\nimage of 150\\x02110\\x023 for each frame in a video segment,\\nas in [23]. In detail, a robust real-time face detector [38] is\\n\\x1crstly leveraged to perform face detection to crop a facial\\nimage from each frame in a video segment. Then, in terms of\\nthe normalized distance between two eyes, a cropped image\\nof 150\\x02110\\x023 containing facial key parts, such as head,\\nnose, mouth, etc., is obtained from a facial image. Finally,\\nwe resize the cropped facial image into 227 \\x02227\\x023 as\\ninputs of spatial CNNs. Note that we discard the \\x1crst frame in\\na video segment LD16, and employ the remaining 15 frames\\nas inputs of spatial CNNs. In this case, we can make sure that\\nthe input frames of spatial CNNs in a video segment equals\\nto that of temporal CNNs.\\nB. SPATIO-TEMPORAL FEATURE LEARNING WITH CNNs\\nAs described in Fig.1, the used spatial and temporal CNNs\\nhave the same structure as the original VGG16 [16], which\\nconsists of \\x1cve convolution layers (Conv1a-Conv1b, Conv2a-\\nConv2b, Conv3a-Conv3b-Conv3c-, \\x01\\x01\\x01, Conv5a, Conv5b-\\nConv5c), \\x1cve max-pooling layers (Pool1, Pool2, \\x01\\x01\\x01, Pool5),\\nand three fully connected (FC) layers (fc6, fc7, fc8). Note\\nthat fc6 and fc7 have 4096 units, while fc8 represents a class\\nlabel vector which equals to data categories. Note that fc8 in\\nVGG16 corresponds to 1000 image categories.\\nTo realize the task of spatio-temporal feature learning\\nwith CNNs, we \\x1cne-tune the pre-trained VGG16 [16] on tar-\\nget video-based facial expression data. In particular, we \\x1crstly\\ncopy the existing VGG16 parameters pre-trained on large-\\nscale ImageNet data to initialize the temporal CNN network\\nand the spatial CNN network, respectively. Then, we replace\\nthe fc8 layer in VGG16 with a new class label vector cor-\\nresponding to six facial expression categories used in our\\nexperiments. Ultimately, we individually retrain these two\\nCNN streams by using the standard back propagation strat-\\negy. Specially, we use the back propagation technique to solve\\nthe following minimizing problem so as to update the CNN\\nnetwork parameters:\\nmin\\nW;\\x12NX\\niD1H(softmax ( W\\x017(aiI#));yi); (2)where Wdenotes the weights of the softmax layer for the\\nnetwork parameters #belonging to spatial CNNs or tem-\\nporal CNNs. 7(viI#) is the 4096-D output of fc7 for input\\ndataai. And yiis the class label vector of the i-th segment, H\\nis the softmax log-loss function de\\x1cned as\\nH(#;y)D\\x00CX\\njD1yjlog(yj); (3)\\nwhere Cis the total number of facial expression categories.\\nOnce both spatial CNNs and temporal CNNs are trained,\\nthe 4096-D outputs of their fc7 layers represent the learned\\nhigh-level feature representations in video segments.\\nC. SPATIO-TEMPORAL FUSION WITH DBNs\\nWhen \\x1cnishing the training of spatial CNNs and tempo-\\nral CNNs, the 4096-D outputs of their fc7 layers were directly\\nconcatenated into a total 8192-D vector as inputs of the fusion\\nnetwork built with a deep DBN model [17], as illustrated\\nin Fig.1. This deep DBN model is used to capture highly\\nnon-linear relationships across spatial and temporal modali-\\nties, and produce a joint discriminative feature representation\\nfor FER.\\nA DBN model is a multi-layered neural network struc-\\nture formed by stacking a series of Restricted Boltzmann\\nMachines (RBMs) [39], each of which is a bipartite graph.\\nIn Fig.1, two RBMs constituted by one visible layer and two\\nhidden layers, are presented as an illustration of a DBN\\'s\\nstructure. Here, the output layer denotes the softmax layer\\nfor classi\\x1ccation. One key characteristic of a DBN is that it\\ncan employ multiple RBMs to learn a multi-layer generative\\nmodel of input data. As a result, DBNs can effectively dis-\\ncover the distribution properties of input data, and learn the\\nhierarchical feature representations of input data.\\nAs done in [40], we use a two-step strategy to train the\\nDBN fusion network, as described below.\\n(1) An unsupervised pre-training is conducted in the\\nbottom-up way by means of a greedy layer-wise training\\nalgorithm. According to the logarithm of the probability of\\nderivative, the weights of each RBM model is updated by\\n1wD\"(<vihj>data\\x00<vihj>model); (4)\\nwhere\"denotes the learning rate, <\\x01>represents the data\\nexpectation. viandhjare the status of visual nodes and hidden\\nnodes, respectively.\\n(2) A supervised \\x1cne-tuning is performed to update the\\nnetwork parameters with back propagation. Specially, super-\\nvised \\x1cne-tuning is realized by using the following loss func-\\ntion between input data and the reconstructed data.\\nL(x;x0)D\\r\\rx\\x00x0\\r\\r2\\n2; (5)\\nwhere xandx0separately denotes input data and the recon-\\nstruction data,kk2\\n2is the L2-norm reconstruction error.\\n32300 VOLUME 7, 2019\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nD. VIDEO-BASED EXPRESSION CLASSIFICATION\\nAfter implementing the training of the DBN fusion network,\\nthe output of its last hidden layer represents the jointly\\nlearned discriminative spatio-temporal feature representa-\\ntions in video segments. Based on this learned segment-\\nlevel features of DBNs, we then apply an average-pooling\\napproach on all divided segments in a video sample to\\nproduce a \\x1cxed-length global video feature representation\\nfor FER. Finally, a linear SVM classi\\x1cer is adopted to\\nperform the \\x1cnal FER tasks in video sequences.\\nIV. EXPERIMENTS\\nTo verify the performance of our proposed method on FER\\ntasks in video sequences, FER experiments are performed on\\nthree public video-based facial expression datasets, i.e., the\\nBAUM-1s database [20], the RML database [21] and the\\nMMI database [22].\\nFIGURE 2. Some examples of cropped facial expression images from the\\nBAUM-1s dataset.\\nA. DATASETS\\n1) BAUM-1s\\nThe original BAUM-1 is a newly-developed spontaneous\\naudio-visual face database of affective and mental states [20].\\nThe BAUM-1 database contains not only the six basic facial\\nexpressions (joy, anger, sadness, disgust, fear, surprise) as\\nwell as boredom and contempt, but also four mental states\\n(unsure, thinking, concentrating, bothered). It comprises\\nof 1222 video samples collected from 31 Turkish persons.\\nEach video frame is 720 \\x02576\\x023. Following in [20], we aim\\nto identify the six basic facial expressions, which forms a\\nsmall subset called the BAUM-1s dataset with 521 video\\nsamples in total. Fig.2 gives some examples of cropped facial\\nexpression images from the BAUM-1s dataset.\\n2) RML\\nThe RML database [21] consists of 720 video samples col-\\nlected from 8 persons. Each video frame is 720 \\x02480\\x023. This\\ndatabase has the six basic facial expressions (angry, disgust,\\nfear, joy, sadness and surprise). Fig.3 shows some samples of\\ncropped facial expression images from the RML database.\\n3) MMI\\nThe MMI database [22] consists of 2894 video samples,\\nout of which 213 sequences have been labeled with six\\nbasic expressions from 30 subjects aging from 19 to 62.\\nFIGURE 3. Some examples of cropped facial expression images from the\\nRML dataset.\\nFIGURE 4. Some examples of cropped facial expression images from the\\nMML dataset.\\nFig.4 provides some samples of cropped facial expression\\nimages from the MMI database.\\nB. EXPERIMENT SETTINGS\\nWhen training deep neural networks, we adopt a mini-\\nbatch size of 30. The maximum number of epochs is 300\\nfor CNNs, and 100 for DBNs, respectively. The learning rate\\nis set to 0.001. To accelerate the training of deep models,\\none NVIDIA GTX TITAN X GPU with 12GB memory is\\nemployed. For all experiments we adopt subject-independent\\ncross-validation strategy widely used in real applications.\\nIn particular, on the BAUM-1s and MMI database with more\\nthan 10 subjects, Leave-One-Subject-Group-Out (LOSGO)\\nwith \\x1cve subject groups is employed, whereas on the RML\\ndatabase with less than 10 subjects, Leave-One-Subject-\\nOut (LOSO) is used for experiments. Finally, we report the\\naverage recognition accuracy in all test-runs to testify the\\nperformance of all compared methods.\\nIt is noted that we train deep models on the divided\\nvideo segments so that the number of training data can be\\naugmented. In this work, on the BAUM-1s database about\\n7000 segments are produced from 521 video samples. Sim-\\nilarily, on the RML database about 12, 000 segments are\\nproduced from 720 video samples, whereas on the MMI\\ndatabase, about 4000 segments are given from 213 video\\nsamples.\\nC. RESULTS AND ANALYSIS\\nWe \\x1crstly evaluate the effects of deep structures of DBNs\\nin the fusion network, since the deep structures of DBNs\\nmay greatly affects the performance of fusing spatio-temporal\\nfeatures. To verify the different structures of DBNs, we pro-\\nvide the performance of three different DBNs, includ-\\ning DBN-1 (8192-4096-6), DBN-2 (8192-4096-2048-6),\\nVOLUME 7, 2019 32301\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nTABLE 1. Accuracy (%) of different structures of DBNS.\\nTABLE 2. Accuracy (%) of different learned deep features.\\nand DBN-3 (8192-4096-2048-1024-6). Table 1 presents the\\nrecognition accuracy of different structures of DBNs in the\\nfusion network. From Table 1, we can observe that DBN-3\\nperforms best among three different structures. In particular,\\nDBN-3 presents an accuracy of 55.85% on the BAUM-1s\\ndataset, 73.73% on the RML dataset, and 71.43% on the\\nMMI dataset, respectively. This demonstrates that the deeper\\nDBN exhibits stronger feature fusion ability based on the\\nused multiple RBMs. In the latter experiments, in the fusion\\nnetwork we thus adopt DBN-3 as the default structure of the\\nused DBN for its best performance.\\nTo verify the advantages of fusing spatio-temporal features\\nwith DBNs, Table 2 shows the performance of four methods:\\nthe single spatial CNN features, the single temporal CNN fea-\\ntures, the score-level fusion based on spatio-temporal CNN\\nfeatures, and the DBN fusion based on spatio-temporal CNN\\nfeatures. As shown in Table 2, we can see that the spatio-\\ntemporal CNNCDBN features, which fuse spatio-temporal\\nCNN features with DBNs, outperform the other two features.\\nThis indicates the effectiveness of fusing spatio-temporal\\nfeatures by using a deep DBN. This is because DBNs are\\nable to effectively discover the distribution properties of input\\nspatio-temporal data, and learn the hierarchical feature repre-\\nsentations of input spatio-temporal data.\\nTo further present the recognition performance for each\\nfacial expression, Fig.5-7 separately show the confusion\\nmatrix of recognition results achieved by the DBN fusion\\nnetwork on these three datasets. It can be seen from Fig.5 that\\non the BAUM-1s dataset only ``joy\\'\\' and ``sadness\\'\\' are\\nclassi\\x1ced well with an accuracy of 88.44% and 72.39%,\\nrespectively, whereas other four facial expressions are iden-\\nti\\x1ced badly with an accuracy of less than 35%. The results\\nin Fig.6 demonstrate that on the RML dataset ``disgust\\'\\',\\n``sadness\\'\\' and ``surprise\\'\\' are recognized well with an\\naccuracy of more than 84%, whereas the remaining three\\nfacial expressions are distinguished with an accuracy of less\\nthan 80%. In Fig.7, we can see that ``sadness\\'\\' and ``surprise\\'\\'\\nare distinguished with an accuracy of 100%, whereas the\\nothers are identi\\x1ced with an accuracy of less than 75%.\\nFIGURE 5. Confusion matrix of recognition results with DBNs on the\\nBAUM-1s dataset.\\nFIGURE 6. Confusion matrix of recognition results with DBNs on the RML\\ndataset.\\nFIGURE 7. Confusion matrix of recognition results with DBNs on the MMI\\ndataset.\\nNow we directly conduct a comparison with previous\\nworks on these three datasets. It is noted that these com-\\nparing works also employs subject-independent test-runs,\\n32302 VOLUME 7, 2019\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\nTABLE 3. Performance (%) comparisons of the-state-of-the-arts on the\\nused three datasets.\\nsimilar to ours. Table 3 provides the comparisons of the state-\\nof-the-arts. From Table 3, it can be seen that our proposed\\nmethod signi\\x1ccantly outperforms the state-of-the-arts on\\nthese three datasets. This exhibits the superiority of our pro-\\nposed method over other methods, including other deep mod-\\nels such as 3D-CNN [14], [15], and Inception-ResNet [41],\\nas well as hand-designed features such as LPQ [20], and\\nGabor wavelets [42]. Note that 3DCNN-DAP (Deformable\\nAction Parts) [14], 3D-CNN [15], Inception-ResNet [41] are\\npopular spatio-temporal deep feature learning methods by\\nusing the spatial and temporal convolutions simultaneously.\\nV. CONCLUSION\\nThis paper proposes a hybrid deep learning model, which\\nconsists of the spatial CNN network, the temporal CNN\\nnetwork, and the DBN fusion network, to apply for FER in\\nvideo sequences. We implement our proposed method in two\\nstages. (1) We employ the existing VGG16 model pre-tained\\non ImageNet data to individually \\x1cne-tune the spatial CNN\\nnetwork and the temporal CNN network on target video-\\nbased facial expression data. (2) To deeply fuse the learned\\nspatio-temporal CNN features, we train a deep DBN model to\\njointly learn discriminative spatio-temporal features. Exper-\\niment results on three public video-based facial expression\\ndatasets, i.e., BAUM-1s RML, and MMI, demonstrate the\\nadvantages of our proposed method.\\nIn future, we will extend our work to practical applications.\\nFor instance, it is challenging to develop a real-time FER\\nsystem based on our proposed method. In addition, it is also\\ninteresting to explore deep compression of deep models so as\\nto reduce the large network parameters of deep models.\\nREFERENCES\\n[1] B. Martinez, M. F. Valstar, B. Jiang, and M. Pantic, ``Automatic analysis of\\nfacial actions: A survey,\\'\\' IEEE Trans. Affective Comput. , to be published.\\ndoi: 10.1109/TAFFC.2017.2731763.\\n[2] X. Zhao and S. Zhang, ``A review on facial expression recognition: Feature\\nextraction and classi\\x1ccation,\\'\\' IETE Tech. Rev. , vol. 33, no. 5, pp. 505\\x15517,\\n2016.[3] C. A. Corneanu, M. O. Simón, J. F. Cohn, and S. E. Guerrero, ``Survey\\non RGB, 3D, thermal, and multimodal approaches for facial expression\\nrecognition: History, trends, and affect-related applications,\\'\\' IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 38, no. 8, pp. 1548\\x151568, Aug. 2016.\\n[4] E. Sariyanidi, H. Gunes, and A. Cavallaro, ``Automatic analysis of facial\\naffect: A survey of registration, representation, and recognition,\\'\\' IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 37, no. 6, pp. 1113\\x151133, Jun. 2015.\\n[5] G. Muhammad, M. Alsulaiman, S. U. Amin, A. Ghoneim, and\\nM. F. Alhamid, ``A facial-expression monitoring system for improved\\nhealthcare in smart cities,\\'\\' IEEE Access , vol. 5, pp. 10871\\x1510881, 2017.\\n[6] T. Wu, M. S. Bartlett, and J. R. Movellan, ``Facial expression recognition\\nusing Gabor motion energy \\x1clters,\\'\\' in Proc. IEEE Comput. Soc. Conf.\\nComput. Vis. Pattern Recognit. Workshops , San Francisco, CA, USA,\\nJun. 2010, pp. 42\\x1547.\\n[7] G. Zhao and M. Pietikäinen, ``Dynamic texture recognition using local\\nbinary patterns with an application to facial expressions,\\'\\' IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915\\x15928, Jun. 2007.\\n[8] B. Jiang, M. F. Valstar, B. Martinez, and M. Pantic, ``A dynamic appearance\\ndescriptor approach to facial actions temporal modeling,\\'\\' IEEE Trans.\\nCybern. , vol. 44, no. 2, pp. 161\\x15174, Feb. 2014.\\n[9] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, ``A deep neural\\nnetwork-driven feature learning method for multi-view facial expression\\nrecognition,\\'\\' IEEE Trans. Multimedia , vol. 18, no. 12, pp. 2528\\x152536,\\nDec. 2016.\\n[10] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim, ``Joint \\x1cne-tuning in deep\\nneural networks for facial expression recognition,\\'\\' in Proc. IEEE Int. Conf.\\nComput. Vis. (ICCV) , Dec. 2015, pp. 2983\\x152991.\\n[11] K. Zhang, Y. Huang, Y. Du, and L. Wang, ``Facial expression recognition\\nbased on deep evolutional spatial-temporal networks,\\'\\' IEEE Trans. Image\\nProcess. , vol. 26, no. 9, pp. 4193\\x154203, Sep. 2017.\\n[12] B. Hasani and M. H. Mahoor, ``Facial expression recognition using\\nenhanced deep 3D convolutional neural networks,\\'\\' in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. Workshops , Jul. 2017, pp. 2278\\x152288.\\n[13] A. T. Lopes, E. de Aguiar, A. F. De Souza, and T. Oliveira-Santos, ``Facial\\nexpression recognition with convolutional neural networks: Coping with\\nfew data and the training sample order,\\'\\' Pattern Recognit. , vol. 61,\\npp. 610\\x15628, Jan. 2017.\\n[14] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen, ``Deeply learning\\ndeformable facial action parts model for dynamic expression analysis,\\'\\'\\ninProc. Asian Conf. Comput. Vis. (ACCV) , Singapore, 2014, pp. 143\\x15157.\\n[15] S. Zhang, S. Zhang, T. Huang, W. Gao, and Q. Tian, ``Learning affective\\nfeatures with a hybrid deep model for audio\\x15visual emotion recognition,\\'\\'\\nIEEE Trans. Circuits Syst. Video Technol. , vol. 28, no. 10, pp. 3030\\x153043,\\nOct. 2018.\\n[16] K. Simonyan and A. Zisserman, ``Very deep convolutional networks for\\nlarge-scale image recognition,\\'\\' in Proc. ICLR , San Diego, CA, USA, 2015,\\npp. 1\\x1514.\\n[17] G. E. Hinton and R. R. Salakhutdinov, ``Reducing the dimensionality of\\ndata with neural networks,\\'\\' Science , vol. 313, no. 5786, pp. 504\\x15507,\\n2006.\\n[18] K. Simonyan and A. Zisserman, ``Two-stream convolutional networks for\\naction recognition in videos,\\'\\' in Proc. Int. Conf. Neural Inf. Process. Syst. ,\\nMontreal, QC, Canada, 2014, pp. 568\\x15576.\\n[19] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, ``Multimodal\\ndeep learning,\\'\\' in Proc. 28th Int. Conf. Mach. Learn. (ICML) , 2011,\\npp. 689\\x15696.\\n[20] S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, ``BAUM-1: A spon-\\ntaneous audio-visual face database of affective and mental states,\\'\\' IEEE\\nTrans. Affective Comput. , vol. 8, no. 3, pp. 300\\x15313, Jul./Sep. 2016.\\n[21] Y. Wang, L. Guan, and A. N. Venetsanopoulos, ``Kernel cross-\\nmodal factor analysis for information fusion with application to\\nbimodal emotion recognition,\\'\\' IEEE Trans. Multimedia , vol. 14, no. 3,\\npp. 597\\x15607, Jun. 2012.\\n[22] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, ``Web-based database\\nfor facial expression analysis,\\'\\' in Proc. IEEE Int. Conf. Multimedia\\nExpo (ICME) , Amsterdam, The Netherlands, Jul. 2005, pp. 317\\x15321.\\n[23] C. Shan, S. Gong, and P. W. McOwan, ``Facial expression recognition\\nbased on local binary patterns: A comprehensive study,\\'\\' Image Vis. Com-\\nput., vol. 27, no. 6, pp. 803\\x15816, 2009.\\n[24] U. Tariq, J. Yang, and T. S. Huang, ``Multi-view facial expression recog-\\nnition analysis with generic sparse coding feature,\\'\\' in Proc. Eur. Conf.\\nComput. Vis. (ECCV) , 2012, pp. 578\\x15588.\\nVOLUME 7, 2019 32303\\nS. Zhang et al. : Learning Affective Video Features for FER via Hybrid Deep Learning\\n[25] U. Tariq et al. , ``Emotion recognition from an ensemble of features,\\'\\'\\ninProc. IEEE Int. Conf. Autom. Face Gesture Recognit. Workshops (FG) ,\\nSanta Barbara, CA, USA, Mar. 2011, pp. 872\\x15877.\\n[26] A. Klaser, M. Marszaªek, and C. Schmid, ``A spatio-temporal descriptor\\nbased on 3D-gradients,\\'\\' in Proc. 19th Brit. Mach. Vis. Conf. (BMVC) ,\\nvol. 275, 2008, pp. 1\\x1510.\\n[27] P. Scovanner, S. Ali, and M. Shah, ``A 3-dimensional sift descriptor and its\\napplication to action recognition,\\'\\' in Proc. 15th ACM Int. Conf. Multime-\\ndia (MM) , Augsburg, Germany, 2007, pp. 357\\x15360.\\n[28] M. Hayat, M. Bennamoun, and A. El-Sallam, ``Evaluation of spatiotem-\\nporal detectors and descriptors for facial expression recognition,\\'\\' in Proc.\\n5th Int. Conf. Hum. Syst. Interact. (HSI) , Perth, WA, Australia, 2012,\\npp. 43\\x1547.\\n[29] M. Liu, S. Shan, R. Wang, and X. Chen, ``Learning expressionlets\\nvia universal manifold model for dynamic facial expression recogni-\\ntion,\\'\\' IEEE Trans. Image Process. , vol. 25, no. 12, pp. 5920\\x155932,\\nDec. 2016.\\n[30] X. Fan and T. Tjahjadi, ``A dynamic framework based on local Zernike\\nmoment and motion history image for facial expression recognition,\\'\\'\\nPattern Recognit. , vol. 64, pp. 399\\x15406, Apr. 2017.\\n[31] H. Yan, ``Collaborative discriminative multi-metric learning for facial\\nexpression recognition in video,\\'\\' Pattern Recognit. , vol. 75, pp. 33\\x1540,\\nMar. 2018.\\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ``ImageNet classi\\x1cca-\\ntion with deep convolutional neural networks,\\'\\' in Proc. Adv. Neural Inf.\\nProcess. Syst. , 2012, pp. 1106\\x151114.\\n[33] C. Szegedy et al., ``Going deeper with convolutions,\\'\\' in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. (CVPR) , Boston, MA, USA, Jun. 2015,\\npp. 1\\x159.\\n[34] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image\\nrecognition,\\'\\' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\\nLas Vegas, NV, USA, Jun. 2016, pp. 770\\x15778.\\n[35] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ``Gradient-based learn-\\ning applied to document recognition,\\'\\' Proc. IEEE , vol. 86, no. 11,\\npp. 2278\\x152324, Nov. 1998.\\n[36] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, ``Learn-\\ning spatiotemporal features with 3D convolutional networks,\\'\\' in Proc.\\nIEEE Int. Conf. Comput. Vis. (ICCV) , Santiago, Chile, Dec. 2015,\\npp. 4489\\x154497.\\n[37] G. Gkioxari and J. Malik, ``Finding action tubes,\\'\\' in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. (CVPR) , Boston, MA, USA, Jun. 2015,\\npp. 759\\x15768.\\n[38] P. Viola and M. J. Jones, ``Robust real-time face detection,\\'\\' Int. J. Comput.\\nVis., vol. 57, no. 2, pp. 137\\x15154, 2004.\\n[39] G. E. Hinton, ``Training products of experts by minimizing contrastive\\ndivergence,\\'\\' Neural Comput. , vol. 14, no. 8, pp. 1771\\x151800, 2002.\\n[40] G. E. Hinton, S. Osindero, and Y.-W. Teh, ``A fast learning algorithm for\\ndeep belief nets,\\'\\' Neural Comput. , vol. 18, no. 7, pp. 1527\\x151554, 2006.\\n[41] B. Hasani and M. H. Mahoor, ``Spatio-temporal facial expression recogni-\\ntion using convolutional neural networks and conditional random \\x1celds,\\'\\'\\ninProc. 12th IEEE Int. Conf. Autom. Face Gesture Recognit. (FG) ,\\nWashington, DC, USA, May/Jun. 2017, pp. 790\\x15795.\\n[42] N. El D. Elmadany, Y. He, and L. Guan, ``Multiview emotion recognition\\nvia multi-set locality preserving canonical correlation analysis,\\'\\' in Proc.\\nIEEE Int. Symp. Circuits Syst. (ISCAS) , Montreal, QC, Canada, May 2016,\\npp. 590\\x15593.\\nSHIQING ZHANG received the Ph.D. degree\\nfrom the School of Communication and Informa-\\ntion Engineering, University of Electronic Science\\nand Technology of China, in 2012. He held a\\nPostdoctoral position with the School of Electronic\\nEngineering and Computer Science, Peking Uni-\\nversity, Beijing, China, from 2015 to 2017. He is\\ncurrently a Professor with the Institute of Intelli-\\ngent Information Processing, Taizhou University,\\nChina. He has published over 30 papers in journals\\nsuch as the IEEE T RANSACTIONS ON MULTIMEDIA and the IEEE T RANSACTIONS ON\\nCIRCUITS AND SYSTEMS FOR VIDEOTECHNOLOGY . His research interests include\\naffective computing and pattern recognition.\\nXIANZHANG PAN received the B.S. and M.S.\\ndegrees in computer science from Lanzhou\\nJiaotong University. He is currently a Senior\\nEngineer with the Institute of Intelligent Infor-\\nmation Processing, Taizhou University, China.\\nHis research interests include image processing,\\naffective computing, and pattern recognition.\\nYUELI CUI received the B.S. degree in electron-\\nics and communication engineering from Zhejiang\\nUniversity City College, Hangzhou, in 2006, and\\nthe M.S. degree in electronics and communica-\\ntion engineering from Hebei University, Baoding,\\nin 2009. He is currently a Lecturer with the Depart-\\nment of Physics and Electronics Engineering,\\nTaizhou University, China. His research interests\\ninclude image processing and pattern recognition.\\nXIAOMING ZHAO received the B.S. degree\\nin mathematics from Zhejiang Normal Univer-\\nsity, in 1990, and the M.S. degree in software\\nengineering from Beihang University, in 2006.\\nHe is currently a Professor with the Institute\\nof Intelligent Information Processing, Taizhou\\nUniversity, China. His research interests include\\nimage processing, machine learning, and pattern\\nrecognition.\\nLIMEI LIU received the Ph.D. degree from the\\nSchool of Information Science and Engineering,\\nCentral South University, in 2011. She held a post-\\ndoctoral position with the Business School, Hunan\\nUniversity, Changsha, China, from 2013 to 2017.\\nShe is currently a Professor with the Institute of\\nBig Data and Internet Innovation, Hunan Univer-\\nsity of Commerce, China. Her research interests\\ninclude machine learning and pattern recognition.\\n32304 VOLUME 7, 2019\\n',\n",
       " 'Error',\n",
       " \"1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n1\\nMulti-Task Convolutional Neural Network for\\nPose-Invariant Face Recognition\\nXi Yin and Xiaoming Liu Member, IEEE\\nAbstract —This paper explores Multi-Task Learning (MTL) for\\nface recognition. First, we propose a multi-task Convolutional\\nNeural Network (CNN) for face recognition where identity\\nclassiﬁcation is the main task and Pose, Illumination, and\\nExpression (PIE) estimations are the side tasks. Second, we\\ndevelop a dynamic-weighting scheme to automatically assign the\\nloss weights to each side task, which solves the crucial problem\\nof balancing between different tasks in MTL. Third, we propose\\na pose-directed multi-task CNN by grouping different poses to\\nlearn pose-speciﬁc identity features, simultaneously across all\\nposes in a joint framework. Last but not least, we propose\\nan energy-based weight analysis method to explore how CNN-\\nbased MTL works. We observe that the side tasks serve as\\nregularizations to disentangle the PIE variations from the learnt\\nidentity features. Extensive experiments on the entire Multi-PIE\\ndataset demonstrate the effectiveness of the proposed approach.\\nTo the best of our knowledge, this is the ﬁrst work using all data\\nin Multi-PIE for face recognition. Our approach is also applicable\\nto in-the-wild datasets for pose-invariant face recognition and\\nachieves comparable or better performance than state of the art\\non LFW, CFP, and IJB-A datasets.\\nIndex Terms—multi-task learning, pose-invariant face recog-\\nnition, CNN, disentangled representation\\nI. I NTRODUCTION\\nFACE recognition is a challenging problem that has been\\nstudied for decades in computer vision. The large intra-\\nperson variations in Pose, Illumination, Expression (PIE),\\nand etc. will challenge any state-of-the-art face recognition\\nalgorithms. Recent CNN-based approaches mainly focus on\\nexploring the effects of 3D model-based face alignment [49],\\nlarger datasets [49], [41], or new metric learning algo-\\nrithms [45], [41], [32], [53] on face recognition performance.\\nMost existing methods consider face recognition as a single\\ntask problem. We believe that face recognition is not an iso-\\nlated problem — often tangled with other tasks. This motivates\\nus to explore multi-task learning for face recognition.\\nMulti-Task Learning (MTL) aims to learn several tasks\\nsimultaneously to boost the performance of the main task or\\nall tasks. It has been successfully applied to face detection [7],\\n[59], face alignment [62], pedestrian detection [50], attribute\\nestimation [1], and so on. Despite the success of MTL in\\nvarious vision problems, there is a lack of study on MTL for\\nface recognition. In this paper, we study face recognition as\\na multi-task problem where identity classiﬁcation is the main\\ntask with PIE estimations being the side tasks. The goal is\\nto leverage the side tasks to improve the performance of the\\nmain task, i.e., face recognition.\\nXi Yin and Xiaoming Liu are with the Department of Computer Science\\nand Engineering, Michigan State University, East Lansing, MI 48824. Corre-\\nsponding author: Xiaoming Liu, liuxm@cse.msu.edu\\nCNN \\nPIE-variant \\ntraining data entangled  \\nfeatures \\nweights in \\nFC layer Wd\\nWp\\ndisentangled \\nfeatures \\nidentity  \\nclassification \\npose \\nclassification PIE-invariant  \\nidentity features \\nFig. 1. We propose MTL for face recognition with identity classiﬁcation as\\nthe main task and PIE classiﬁcations as the side tasks (only pose is illustrated\\nin this ﬁgure for simplicity). A CNN framework learns entangled features from\\nthe data. The weight matrix in the fully connected layer of the main task is\\nlearnt to have close-to-zero values for PIE features in order to exclude PIE\\nvariations, which results in PIE-invariant identity features for face recognition.\\nWe answer the questions of how and why PIE estimations\\ncan help face recognition. Regarding how, we propose a multi-\\ntask CNN (m-CNN) framework for joint identity classiﬁcation\\nand PIE classiﬁcations, which learns a shared embedding.\\nRegarding why, we conduct an energy-based weight analysis\\nand observe that the side tasks serve as regularizations to\\ninject PIE variations into the shared embedding, which is\\nfurther disentangled into PIE-invariant identity features for\\nface recognition. As shown in Figure 1, a shared embedding\\nis learnt from PIE-variant training images through a CNN\\nframework. A fully connected layer is connected to the shared\\nfeatures to perform classiﬁcation of each task. The shared\\nfeatures entangles both identity and PIE variations, and the\\nweight matrix in the fully connected layer performs feature\\nselection for disentanglement.\\nOne crucial problem in MTL is how to determine the\\nimportance of each task. Prior work either treats each task\\nequally [57] or obtains the loss weights by greedy search [50].\\nIt may not be fair to assume that each task contributes equally.\\nHowever, it will be very time consuming or practically impos-\\nsible to ﬁnd the optimal weights for all side tasks via brute-\\nforce search. Instead, we propose a dynamic-weighting scheme\\nwhere we only need to determine the overall loss weight for\\nthe PIE estimations, and the CNN can learn to dynamically\\nassign a loss weight to each side task during training. This is\\neffective and efﬁcient as will shown in Section IV.\\nSince pose variation is the most challenging one among\\nother non-identity variations, and the proposed m-CNN al-\\nready classiﬁes all images into different pose groups, we\\npropose to apply divide-and-conquer to the CNN learning.\\nSpeciﬁcally, we develop a novel pose-directed multi-task CNN\\n(p-CNN) where the pose labels can categorize the training data\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n2\\nTABLE I\\nCOMPARISON OF THE EXPERIMENTAL SETTINGS THAT ARE COMMONLY USED IN PRIOR WORK ON MULTI -PIE. (* T HE20IMAGES\\nCONSIST OF 2DUPLICATES OF NON -FLASH IMAGES AND 18FLASH IMAGES . IN TOTAL THERE ARE 19DIFFERENT ILLUMINATIONS .)\\nsetting session pose illum exp train subjects / images gallery / probe images total references\\nI 4 7 1 1 200 /5;383 137 /2;600 8; 120 [4], [28]\\nII 1 7 20 1 100 /14;000 149 /20;711 34; 860 [65], [57]\\nIII 1 15 20 1 150 /45;000 99/29;601 74; 700 [54]\\nIV 4 9 20 1 200 /138;420 137 /70;243 208; 800 [66], [51]\\nV 4 13 20 1 200 /199;940 137 /101;523 301; 600 [58]\\nours 4 15 20* 6 200 /498;900 137 /255;163 754; 200\\ninto three different pose groups, direct them through different\\nroutes in the network to learn pose-speciﬁc identity features\\nin addition to the generic identity features. Similarly, the loss\\nweights for extracting these two types of features are learnt\\ndynamically in the CNN framework. During the testing stage,\\nwe propose a stochastic routing scheme to fuse the generic\\nidentity features and the pose-speciﬁc identity features for face\\nrecognition, which is more robust to pose estimation errors.\\nWe ﬁnd this technique to be very effective for pose-invariant\\nface recognition especially for in-the-wild faces, where pose\\nclassiﬁcation error is more likely to happen compared to\\ncontrolled datasets with discrete pose angles.\\nThis work utilizes alldata in the Multi-PIE dataset [16],\\ni.e., faces with the full range of PIE variations, as the main\\nexperimental dataset — ideal for studying MTL for PIE-\\ninvariant face recognition. To the best of our knowledge, there\\nis no prior face recognition work that studies the full range\\nof variations on Multi-PIE. We also apply our method to in-\\nthe-wild datasets for pose-invariant face recognition. Since the\\nground truth label of the side task is unavailable, we use the\\nestimated poses as labels for training.\\nIn summary, we make four contributions.\\n\\x0fWe formulate face recognition as an MTL problem and\\nexplore how it works via an energy-based weight analysis.\\n\\x0fWe propose a dynamic-weighting scheme to learn the loss\\nweights for each side task automatically in the CNN.\\n\\x0fWe develop a pose-directed multi-task CNN to learn\\npose-speciﬁc identity features and a stochastic routing\\nscheme for feature fusion during the testing stage.\\n\\x0fWe perform a comprehensive and the ﬁrst face recogni-\\ntion study on the entire Multi-PIE. We achieve compa-\\nrable or superior performance to state-of-the-art methods\\non Multi-PIE, LFW [20], CFP [42], and IJB-A [26].\\nII. R ELATED WORK\\nA. Face Recognition\\nRecent progress in face recognition has been mainly focused\\non developing metric learning algorithms including center\\nloss [53], A-softmax [33], N-pair [44], etc. In this work, we\\nstudy Pose-Invariant Face Recognition (PIFR) via CNN-based\\nmulti-task learning. Therefore, we focus our review on PIFR\\nand MTL-based approaches.\\nPose-Invariant Face Recognition According to [11], existing\\nPIFR methods can be classiﬁed into four categories including:\\nmulti-view subspace learning [27], [2], pose-invariant featureextraction [6], [41], [39], face synthesis [64], [19], [58],\\nand a hybrid approach of the above three [51], [57]. For\\nexample, FF-GAN [58] incorporates a 3D Morphable Model\\n(3DMM) [5] into a CNN framework for face frontalization\\nwith various loss functions. The frontalized faces can be\\nused to improve the face recognition performance especially\\nfor large-pose faces. By modeling the face rotation process,\\nDR-GAN [51] learns both a generative and discriminative\\nrepresentation from one or multiple face images of the same\\nsubject. This representation can be used for PIFR and face\\nimage synthesis.\\nOur work belongs to the second category, i.e., pose-invariant\\nfeature extraction. Previous work in this category treats each\\npose separately. For example, Masi et al. [34] propose a pose-\\naware face recognition method by learning a speciﬁc model\\nfor each type of face alignment and pose group. And the\\nresults are fused together during the testing stage. The idea\\nof divide-and-conquer is similar to our work. Differently, we\\nlearn the pose-invariant identity features for all poses jointly in\\none CNN framework and propose a stochastic routing scheme\\nduring the testing stage for feature fusion, which is more\\nefﬁcient and robust. Xiong et al. [54] propose a conditional\\nCNN for PIFR, which discovers the modality information\\nautomatically during training. In contrast, we utilize the pose\\nlabels as a side task to better disentangle pose variation from\\nthe learnt identity features. Peng et al. [39] use the pose\\nclassiﬁcation as a side task to learn a rich embedding, which is\\nfurther disentangled via pair-wise reconstruction. However, it\\nneed to be trained on datasets with non-frontal and frontal face\\npairs, which is not widely available especially for in-the-wild\\nscenario.\\nMTL for Face Recognition For MTL-based face recognition\\nmethod, Ding et al. [12] propose to transform the features\\nof different poses into a discriminative subspace, and the\\ntransformations are learnt jointly for all poses with one task\\nfor each pose. [57] develops a deep neural network to rotate\\na face image. The reconstruction of the face is considered\\nas a side task. This multi-task framework is more effective\\nthan the single task model without the reconstruction part.\\nSimilar work [66], [51] have developed along this direction\\nto extract robust identity features and synthesize face images\\nsimultaneously. In this work, we treat face recognition as a\\nmulti-task problem with PIE estimations as the side tasks. It\\nsounds intuitive to have PIE as side tasks for face recognition,\\nbut we are actually the ﬁrst to consider this and we have\\nexplored how it works.\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n3\\nB. Multi-Task Learning\\nMulti-task learning has been widely studied in machine\\nlearning [15], [3] and computer vision [50], [62]. We focus\\nour review on different regularizations in MTL, CNN-based\\nMTL, and the importance of each task in MTL.\\nRegularizations The underlying assumption for most MTL\\nalgorithms is that different tasks are related to each other.\\nThus, a key problem is how to determine the task relatedness.\\nOne common way is to learn shared features for different\\ntasks. The generalized linear model parameterizes each task\\nwith a weight vector. The weight vectors of all tasks form a\\nweight matrix, which is regularized by l2;1norm [3], [37] or\\ntrace norm [22] to encourage a low-rank matrix. For example,\\nObozinski et al. [37] propose to penalize the sum of l2\\nnorm of the blocks of weights associated with each feature\\nacross different tasks to encourage similar sparsity patterns.\\nLin et al. [30] propose to learn higher order feature interaction\\nwithout limiting to a linear model for MTL. Other work [14],\\n[61], [31] propose to learn the task relationship from a task\\ncovariance matrix computed from the data.\\nCNN-based MTL It is natural to fuse MTL with CNN to\\nlearn the shared features and the task-speciﬁc models. For\\nexample, [62] proposes a deep CNN for joint face detection,\\npose estimation, and landmark localization. Misra et. al. [35]\\npropose a cross-stitch network for MTL to learn the sharing\\nstrategy, which is difﬁcult to scale to multiple tasks. This\\nrequires training one model for each task and introduces\\nadditional parameters in combining them. In [59], a task-\\nconstrained deep network is developed for landmark detection\\nwith facial attribute classiﬁcations as the side tasks. However,\\nunlike the regularizations used in the MTL formulation in the\\nmachine learning community, there is no principled method\\nto analysis how MTL works in the CNN framework. In this\\npaper, we propose an energy-based weight analysis method to\\nexplore how MTL works. We discover that the side tasks of\\nPIE estimations serve as regularizations to learn more discrim-\\ninative identity features that are robust to PIE variations.\\nImportance of Each Task It is important to determine the\\nloss weight for each task in MTL. The work of [57] uses\\nequal loss weights for face recognition and face frontalization.\\nTian et al. [50] propose to obtain the loss weights of all side\\ntasks via greedy search within 0and1. Lettandkbe the\\nnumber of side tasks and searched values respectively. This\\napproach has two drawbacks. First, it is very inefﬁcient as\\nthe computation scales to the number of tasks (complexity\\ntk). Second, the optimal loss weight obtained for each task\\nmay not be jointly optimal. Further, the complexity would be\\nktif searching all combinations in a brute-force way. Zhang\\net al. [62] propose a task-wise early stopping to halt a task\\nduring training when the loss no longer reduces. However, a\\nstopped task will never resume so its effect may disappear.\\nIn contrast, we propose a dynamic-weighting scheme where\\nwe only determine the overall loss weight for all side tasks\\n(complexity k) and let CNN learn to automatically distribute\\nthe weights to each side task. In this case when one task is\\nsaturated, we have observed the dynamic weights will reduce\\nwithout the need of early stopping.III. T HEPROPOSED APPROACH\\nIn this section, we present the proposed approach by using\\nMulti-PIE dataset as an example and extend it to in-the-wild\\ndatasets in the experiments. First, we propose a multi-task\\nCNN (m-CNN) with dynamic weights for face recognition\\n(the main task) and PIE estimations (the side tasks). Second,\\nwe propose a pose-directed multi-task CNN (p-CNN) to tackle\\npose variation by separating all poses into different groups and\\njointly learning pose-speciﬁc identity features for each group.\\nA. Multi-Task CNN\\nWe combine MTL with CNN framework by sharing some\\nlayers between different tasks. In this work, we adapt CASIA-\\nNet [56] with three modiﬁcations. First, Batch Normalization\\n(BN) [21] is applied to accelerate the training process. Second,\\nthe contrastive loss is excluded for simplicity. Third, the\\noutput dimension of the fully connected layer is changed\\naccording to different tasks. Details of the layer parameters\\nare shown in Figure 2. The network consists of ﬁve blocks\\neach including two convolutional layers and a pooling layer.\\nBN and ReLU [36] are used after each convolutional layer.\\nSimilar to [56], no ReLU is used after conv52 layer to learn\\na compact feature representation, and a dropout layer with a\\nratio of 0:4is applied after pool5 layer.\\nGiven a training set TwithNimages and their labels:\\nT=fIi;yigN\\ni=1, where Iiis the image and yiis a vector\\nconsisting of the identity label yd\\ni(main task) and the side task\\nlabels. In our work, we consider three side tasks including pose\\n(yp\\ni), illumination (yl\\ni), and expression (ye\\ni). We eliminate the\\nsample index ifor clarity. As shown in Figure 2, the proposed\\nm-CNN extracts a high-level feature embedding x2RD\\x021:\\nx=f(I;k;b;\\r;\\x0c); (1)\\nwheref(\\x01)represents the non-linear mapping from the input\\nimage to the shared features. kandbare the sets of ﬁlters and\\nbias of all the convolutional layers. \\rand\\x0care the sets of\\nscales and shifts in the BN layers [21]. Let \\x02=fk;b;\\r;\\x0cg\\ndenote all parameters to be learnt to extract the features x.\\nThe extracted features x, which is pool5 in our model,\\nare shared among all tasks. Suppose Wd2RD\\x02Ddand\\nbd2RDd\\x021are the weight matrix and bias vector in the\\nfully connected layer for identity classiﬁcation, where Ddis\\nthe number of different identities in T. The generalized linear\\nmodel can be applied:\\nyd=Wd|x+bd: (2)\\nydis fed to a softmax layer to compute the probability of\\nxbelonging to each subject in the training set:\\nsoftmax(yd)n=p(^yd=njx) =exp(yd\\nn)P\\njexp(yd\\nj); (3)\\nwhere yd\\njis thejth element in yd. Thesoftmax(\\x01) function\\nconverts the output ydto a probability distribution over all\\nsubjects and the subscript selects the nth element. Finally, the\\nestimated identity ^ydis obtained via:\\n^yd= argmax\\nnsoftmax(yd)n: (4)\\n4\\nconv11:3x3/1/32 conv12: 3x3/1/64 pool1: max/2x2/2 conv21:3x3/1/64 conv22: 3x3/1/128 pool2: max/2x2/2 conv31:3x3/1/96 conv32: 3x3/1/192 pool3: max/2x2/2 conv41:3x3/1/128 conv42: 3x3/1/256 pool4: max/2x2/2 conv51:3x3/1/160 conv52: 3x3/1/320 pool5: avg/7x7/1 50x50x64 25x25x128 13x13x192 7x7x256 1x1x320 100x100x1 s-CNN:  \\nm-CNN:  \\np-CNN:  \\nµs\\nµm\\nfc6_id fc6_pos \\nfc6_exp fc6_illum fc: 200/13/6/19/3/2 \\nblock 1 \\nblock 2 \\nblock 3 \\nblock 4 \\nblock 5 \\nfc6_left fc6_frontal fc6_right fc: 200/200/200 \\npose-directed branch \\nbatch split \\nFig. 2. The proposed m-CNN and p-CNN for face recognition. Each block reduces the spatial dimensions and increases the feature channels. The parameter\\nformat for the convolutional layer is: ﬁlter size / stride / ﬁlter number. The parameter format for the pooling layer is: method / ﬁlter size / stride. The feature\\ndimensions after each block are shown on the bottom. The color indicates the component for each model. The dashed line represents the batch split operation\\nas shown in Figure 3. The layers with the stripe pattern are the identity features used in the testing stage for face recognition.\\nThe cross-entropy loss is employed:\\nL(I;yd) =\\x00log(p(^yd=ydjI;\\x02;Wd;bd)): (5)\\nSimilarly, we formulate the losses for the side tasks. Let\\nW=fWd;Wp;Wl;Wegrepresent the weight matrices for\\nidentity and PIE classiﬁcations. The bias terms are eliminated\\nfor simplicity. Given the training set T, our m-CNN aims to\\nminimize the combined loss of all tasks:\\nargmin\\n\\x02;W\\x0bdNX\\ni=1L(Ii;yd\\ni) +\\x0bpNX\\ni=1L(Ii;yp\\ni)+\\n\\x0blNX\\ni=1L(Ii;yl\\ni) +\\x0beNX\\ni=1L(Ii;ye\\ni);(6)\\nwhere\\x0bd,\\x0bp,\\x0bl,\\x0becontrol the importance of each task.\\nIt becomes a single-task model (s-CNN) when \\x0bp;l;e = 0.\\nThe loss drives the model to learn both the parameters \\x02\\nfor extracting the shared features and Wfor the classiﬁcation\\ntasks. In the testing stage, the features before the softmax layer\\n(yd) are used for face recognition by applying a face matching\\nprocedure based on cosine similarity.\\nB. Dynamic-Weighting Scheme\\nIn MTL, it is an open question on how to set the loss weight\\nfor each task. Prior work either treats all tasks equally [57] or\\nobtains the loss weights via brute-force search [50], which\\nis very time-consuming especially considering the training\\ntime for CNN models. To solve this problem, we propose a\\ndynamic-weighting scheme to automatically assign the loss\\nweights to each side task during training.\\nFirst, we set the weight for the main task to 1, i.e.\\x0bd= 1.\\nSecond, instead of ﬁnding the loss weight for each task, we\\nﬁnd the summed loss weight for all side tasks, i.e. 's=\\x0bp+\\n\\x0bl+\\x0be, via brute-force search on a validation set. Our m-\\nCNN learns to allocate 'sto three side tasks. As shown in\\nFigure 2, we add a fully connected layer and a softmax layer\\nto the shared features xto learn the dynamic weights. Let\\n!s2RD\\x023and\\x0fs2R3\\x021denote the weight matrix and\\nbias vector in this fully connected layer,\\n\\x16s=softmax (!s|x+\\x0fs); (7)where\\x16s= [\\x16p;\\x16l;\\x16e]|are the dynamic weight percentages\\nfor the side tasks with \\x16p+\\x16l+\\x16e= 1 and\\x16p;l;e\\x150. So\\nEquation 6 becomes:\\nargmin\\n\\x02;W;!sNX\\ni=1L(Ii;yd\\ni) +'sh\\n\\x16pNX\\ni=1L(Ii;yp\\ni)+\\n\\x16lNX\\ni=1L(Ii;yl\\ni) +\\x16eNX\\ni=1L(Ii;ye\\ni)i\\ns:t: \\x16 p+\\x16l+\\x16e= 1; \\x16 p;l;e\\x150(8)\\nThe multiplications of the overall loss weight 'swith the\\nlearnt dynamic percentage \\x16p;l;e are the dynamic loss weights\\nfor each side task, i.e., \\x0bp;l;e='s\\x01\\x16p;l;e.\\nWe use mini-batch Stochastic Gradient Descent (SGD) to\\nsolve the above optimization problem where the dynamic\\nweights are averaged over a batch of samples. Intuitively,\\nwe expect the dynamic-weighting scheme to behave in two\\ndifferent aspects in order to minimize the loss in Equation 8.\\nFirst, since our main task contribute mostly to the ﬁnal loss\\n('s<1), the side task with the largest contribution to the\\nmain task should have the highest weight in order to reduce\\nthe loss of the main task. Second, our m-CNN should assign\\na higher weight for an easier task with a lower loss so as to\\nreduce the overall loss. We have observed these effects as will\\nshown in the experiments.\\nC. Pose-Directed Multi-Task CNN\\nIt is very challenging to learn a non-linear mapping to\\nestimate the correct identity from a face image with arbitrary\\nPIE, given the diverse variations in the data. This challenge\\nhas been encountered in classic pattern recognition work. For\\nexample, in order to handle pose variation, [29] proposes\\nto construct several face detectors where each of them is in\\ncharge of one speciﬁc view. Such a divide-and-conquer scheme\\ncan be applied to CNN learning because the side tasks can\\n“divide” the data and allow the CNN to better “conquer” them\\nby learning tailored mapping functions.\\nTherefore, we propose a novel task-directed multi-task CNN\\nwhere the side task labels categorize the training data into\\nmultiple groups, and direct them to different routes in the\\nnetwork. Since pose is considered as the primary challenge\\nin face recognition [54], [60], [66], we propose pose-directed\\n5\\n1\\t 4\\t 7\\t 3\\t\\n2\\t 5\\t 8\\t 5\\t\\n..\\t ..\\t ..\\t ..\\t\\n3\\t 6\\t 9\\t 4\\t\\n1\\t 0\\t 0\\t 0\\t\\n2\\t 0\\t 0\\t 0\\t\\n..\\t ..\\t ..\\t ..\\t\\n3\\t 0\\t 0\\t 0\\t\\n0\\t 4\\t 0\\t 3\\t\\n0\\t 5\\t 0\\t 5\\t\\n..\\t ..\\t ..\\t ..\\t\\n0\\t 6\\t 0\\t 4\\t\\n0\\t 0\\t 7\\t 0\\t\\n0\\t 0\\t 8\\t 0\\t\\n..\\t ..\\t ..\\t ..\\t\\n0\\t 0\\t 9\\t 0\\t\\nFig. 3. Illustration of the batch split operation in p-CNN. The ﬁrst row shows\\nthe input images and the second row shows a matrix representing the features\\nxfor each sample. After batch split, one batch of samples is separated into\\nthree batches where each of them only consists of the samples belonging to\\nthe speciﬁc pose group.\\nWlWfWr\\nPose-Specific Identity Features Pose-Directed Multi-Task CNN \\nFig. 4. The proposed pose-directed multi-task CNN aims to learn pose-\\nspeciﬁc identity features jointly for all pose groups.\\nmulti-task CNN (p-CNN) to handle pose variation. However,\\nit is applicable to other variation.\\nAs shown in Figure 2, p-CNN is built on top of m-CNN\\nby adding the pose-directed branch (PDB). The PDB groups\\nface images with similar poses to learn pose-speciﬁc identity\\nfeatures via a batch split operation. We separate the training set\\ninto three groups according to the pose labels: left proﬁle ( Gl),\\nfrontal (Gf), and right proﬁle ( Gr). As shown in Figure 3, the\\ngoal of the batch split is to separate a batch of N0samples\\n(X=fxigN0\\ni=1) into three batches Xl,Xf, andXr, which are\\nof the same size as X. During training, the ground truth pose\\nis used to assign a face image into the correct group. Let us\\ntake the frontal group as an example to illustrate the batch\\nsplit operation:\\nXf\\ni=(\\nxi;ifyp\\ni2Gf\\n0;otherwise;(9)\\nwhere 0denotes a vector of all zeros with the same dimension\\nasxi. The assignment of 0is to avoid the case when no\\nsample is passed into one group, the next layer will still have\\nvalid input. As a result, Xis separated into three batches\\nwhere each batch consists of only the samples belonging to the\\ncorresponding pose group. Each group learns a pose-speciﬁc\\nmapping to a joint space, resulting in three different sets of\\nweights:fWl;Wf;Wrg, as illustrated in Figure 4. Finally\\nthe features from all groups are merged as the input to a\\nsoftmax layer to perform robust identity classiﬁcation jointly.\\nLeft frontal right \\n0.2 0.3 0.5 0.4 0.3 0.3 \\n1\\tFig. 5. Blue bars are the generic identity features and purple bars are the\\npose-speciﬁc features. The numbers are the probabilities of each input image\\nbelonging to each pose group. The proposed stochastic routing in the testing\\nstage taking account of all pair comparisons so that it is more robust to pose\\nestimation errors.\\nOur p-CNN aims to learn two types of identity features:\\nWdis the weight matrix to extract the generic identity features\\nthat is robust to all poses; Wl;f;rare the weight matrices to\\nextract the pose-speciﬁc identity features that are robust within\\na small pose range. Both tasks are considered as our main\\ntasks. Similar to the dynamic-weighting scheme in m-CNN,\\nwe use dynamic weights to combine our main tasks as well.\\nThe summed loss weight for these two tasks is 'm=\\x0bd+\\x0bg.\\nLet!m2RD\\x022and\\x0fm2R2\\x021denote the weight matrix\\nand bias vector for learning the dynamic weights,\\n\\x16m=softmax (!m|x+\\x0fm): (10)\\nWe have\\x16m= [\\x16d;\\x16g]|as the dynamic weights for generic\\nidentity classiﬁcation and pose-speciﬁc identity classiﬁcation.\\nFinally, the loss of p-CNN is formulated as:\\nargmin\\n\\x02;W;!'mh\\n\\x16dNX\\ni=1L(Ii;yd\\ni) +\\x16gGX\\ng=1NgX\\ni=1L(Ii;yd\\ni)i\\n+\\n'sh\\n\\x16pNX\\ni=1L(Ii;yp\\ni) +\\x16lNX\\ni=1L(Ii;yl\\ni) +\\x16eNX\\ni=1L(Ii;ye\\ni)i\\ns:t: \\x16 d+\\x16g= 1;\\x16p+\\x16l+\\x16e= 1;\\x16d;g\\x150;\\x16p;l;e\\x150(11)\\nwhereG= 3 is the number of pose groups and Ngis the\\nnumber of training images in the g-th group.!=f!m;!sg\\nis the set of parameters to learn the dynamic weights for both\\nthe main and side tasks. We set 'm= 1.\\nStochastic Routing Given a face image in the testing\\nstage, we can extract the generic identity features ( yd), the\\npose-speciﬁc identity features ( fygg3\\ng=1), as well as estimate\\nthe probabilities (fpgg3\\ng=1) of the input image belonging to\\neach pose group by aggregating the probabilities from the\\npose classiﬁcation side task. As shown in Figure 5, for face\\nmatching, we can compute the distance of the generic identity\\nfeatures and the distance of the pose-speciﬁc identity features\\nby selecting the pose group with the largest probability (red\\nunderline). However, the pose estimation error may cause in-\\nferior feature extraction results, which is inevitable especially\\nfor unconstrained faces.\\nTo solve this problem, we propose a stochastic routing\\nscheme by taking into account of all comparisons, which is\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n6\\nTABLE II\\nPERFORMANCE COMPARISON (%)OF SINGLE -TASK LEARNING (S-CNN), MULTI -TASK LEARNING (M-CNN) WITH ITS VARIANTS ,AND POSE -DIRECTED\\nMULTI -TASK LEARNING (P-CNN) ON THE ENTIRE MULTI -PIE DATASET .\\nmodel loss weights rank-1 (all / left / frontal /right) pose illum exp\\ns-CNN: id \\x0bd= 1 75:67 /71:51 /82:21/73:29 – – –\\ns-CNN: pos \\x0bp= 1 – 99:87 – –\\ns-CNN: exp \\x0bl= 1 – – 96:43 –\\ns-CNN: illum \\x0be= 1 – – – 92:44\\ns-CNN: id+L2 \\x0bd= 1 76:43 /73:31 /81:98/73:99 – – –\\nm-CNN: id+pos \\x0bd= 1;\\x0bp= 0:1 78:06 /75:06 /82:91/76:21 99:78 – –\\nm-CNN: id+illum \\x0bd= 1;\\x0bl= 0:1 77:30/74:87 /82:83/74:21 – 93:57 –\\nm-CNN: id+exp \\x0bd= 1;\\x0be= 0:1 77:76 /75:48 /82:32/75:48 – – 90:93\\nm-CNN: id+all \\x0bd= 1;\\x0bp;l;e= 0:033 77:59 /74:75 /82:99 /75:04 99:75 88:46 79:97\\nm-CNN: id+all (dynamic) \\x0bd= 1;'s= 0:1 79:35 /76:60 /84:65 /76:82 99:81 93:40 91:47\\np-CNN 'm= 1;'s= 0:1 79:55/76:14 /84:87/77:65 99:80 90:58 90:02\\nmore robust to pose estimation errors. Speciﬁcally, the distance\\ncbetween a pair of face images (I 1andI2) is computed as the\\naverage between the distance of the generic identity features\\n(yd\\n1,yd\\n2) and weighted distance of the pose-speciﬁc identity\\nfeatures (fyg\\n1g,fyg\\n2g):\\nc=1\\n2h(yd\\n1;yd\\n2) +1\\n23X\\ni=13X\\nj=1h(yi\\n1;yj\\n2)\\x01pi\\n1\\x01pj\\n2; (12)\\nwhereh(\\x01) is the cosine distance metric used to measure the\\ndistance between two feature vectors. The proposed stochastic\\nrouting accounts for all combinations of the pose-speciﬁc\\nidentity features weighted by the probabilities of each combi-\\nnation. We treat the generic features and pose-speciﬁc features\\nequally, and fuse them for face recognition.\\nIV. E XPERIMENTS\\nWe evaluate the proposed m-CNN and p-CNN under two\\nsettings: (1) face identiﬁcation on Multi-PIE with PIE estima-\\ntions being the side tasks; (2) face veriﬁcation/identiﬁcation on\\nin-the-wild datasets including LFW, CFP, and IJB-A, where\\npose estimation is the only side task. Further, we analyze\\nthe effect of MTL on Multi-PIE and discover that the side\\ntasks regularize the network to learn a disentangled identity\\nrepresentation for PIE-invariant face recognition.\\nA. Face Identiﬁcation on Multi-PIE\\nExperimental Settings The Multi-PIE dataset consists of\\n754;200images of 337subjects recorded in 4sessions. Each\\nsubject was recorded with 15different cameras where 13are at\\nthe head height spaced at 15\\x0einterval and 2are above the head.\\nFor each camera, a subject was imaged under 19different\\nilluminations. In each session, a subject was captured with 2\\nor3expressions, resulting in a total of 6different expressions\\nacross all sessions.\\nAll previous work [10], [18], [12], [17], [25], [28], [54],\\n[55], [60], [64], [66] studies face recognition on a subset\\nof PIE variations on Multi-PIE. In our work, we use the\\nentire dataset including all PIE variations. For the two cameras\\nabove the head, their poses are labeled as \\x0645\\x0e. The ﬁrst 200\\nsubjects are used for training. The remaining 137subjects areused for testing, where one image with frontal pose, neutral\\nillumination, and neutral expression for each subject is selected\\nas the gallery set and the remaining as the probe set.\\nWe use the landmark annotations [13] to align each face to\\na canonical view of size 100\\x02100. The images are normalized\\nby subtracting 127:5 and dividing by 128, similar to [53]. We\\nuse Caffe [23] with our modiﬁcations. The momentum is set to\\n0:9and the weight decay to 0:0005. All models are trained for\\n20epochs from scratch with a batch size of 4unless speciﬁed.\\nThe learning rate starts at 0:01 and reduces at 10th, 15th, and\\n19th epochs with a factor of 0:1. The rank-1 identiﬁcation rate\\nis reported as the face recognition performance. For the side\\ntasks, the mean accuracy over all classes is reported.\\nWe randomly select 20subjects from the training set to\\nform a validation set to ﬁnd the optimal overall loss weight\\nfor all side tasks. We obtain 's= 0:1 via brute-force search.\\nFor p-CNN model training, we split the training set into\\nthree groups based on the yaw angle of the image: right pro-\\nﬁle (\\x0090\\x0e;\\x0075\\x0e;\\x0060\\x0e,\\x0045\\x0e), frontal (\\x0030\\x0e,\\x0015\\x0e;0\\x0e;15\\x0e,\\n30\\x0e), and left proﬁle (45\\x0e,60\\x0e;75\\x0e;90\\x0e).\\nEffects of MTL Table II shows the performance comparison\\nof single-task learning (s-CNN), multi-task learning (m-CNN),\\nand pose-directed multi-task learning (p-CNN) on the entire\\nMulti-PIE. First, we train four single-task models for identity\\n(id), pose (pos), illumination (illum), and expression (exp)\\nclassiﬁcation respectively. As shown in Table II, the rank-1\\nidentiﬁcation rate of s-CNN is only 75:67%. The performance\\nof the frontal pose group is much higher than those of the\\nproﬁle pose groups, indicating that pose variation is indeed\\na big challenge for face recognition. Among all side tasks,\\npose estimation is the easiest task, followed by illumination,\\nand expression as the most difﬁcult one. This is caused by\\ntwo potential reasons: 1) discriminating expression is more\\nchallenging due to the non-rigid face deformation; 2) the\\ndata distribution over different expressions is unbalanced with\\ninsufﬁcient training data for some expressions.\\nSecond, we train multiple m-CNN models by adding only\\none side task at a time in order to evaluate the inﬂuence of\\neach side task. We use “id+pos”, “id+illum”, and “id+exp” to\\nrepresent these variants and compare them to the performance\\nof adding all side tasks denoted as “id+all”. To evaluate the\\neffects of the dynamic-weighting scheme, we train a model\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n7\\nepoch0 5 10 15 20dynamic weights\\n00.020.040.060.080.1,p\\n,l\\n,e\\nepoch0 5 10 15 20loss\\n00.511.522.533.5\\nidd\\npos\\nillum\\nexp\\nepoch0 5 10 15 20dynamic weights\\n00.20.40.60.81,d\\n,g\\n,p\\n,l\\n,e\\nepoch0 5 10 15 20loss\\n00.511.522.533.5\\nidd\\nidg\\npos\\nillum\\nexp\\n(a)\\nm-CNN: dynamic weights (b) m-CNN: losses (c) p-CNN: dynamic weights (d) p-CNN: losses\\nFig. 6. The learnt dynamic weights and the losses of each task for m-CNN and p-CNN models during the training process.\\nTABLE III\\nMULTI -PIE PERFORMANCE COMPARISON ON SETTING IIIOFTABLE I.\\n\\x0615\\x0e\\x0630\\x0e\\x0645\\x0e\\x0660\\x0e\\x0675\\x0e\\x0690\\x0ea\\nvg.\\nFisher\\nVector [43] 93:30 87 :21 80 :33 68 :71 45 :51 24 :53 66 :60\\nFIP20[65] 95:88\\n89:23 78 :89 61 :64 47 :32 34 :13 67 :87\\nFIP40[65] 96:30\\n92:98 85 :54 69 :75 49 :10 31 :37 70 :90\\nc-CNN [54] 95:64 92 :66 85 :09 70 :49 55 :64 41 :71 73 :54\\nc-CNN Forest [54] 96:97 94 :05 89 :02 74 :38 60 :66 47 :26 76 :89\\ns-CNN\\n(ours) 98:41 96 :89 85 :18 88 :71 82 :80 76 :72 88 :45\\nm-CNN (ours) 99:02 97 :40 89 :15 89 :75 84 :97 76 :72 90 :08\\np-CNN (ours) 99:19 98:01 90:34 92:07 87:83 76:96 91:27\\nT\\nABLE IV\\nMULTI -PIE PERFORMANCE COMPARISON ON SETTING VOFTABLE I.\\n0\\x0e\\x0615\\x0e\\x0630\\x0e\\x0645\\x0e\\x0660\\x0e\\x0675\\x0e\\x0690\\x0ea\\nvg.[\\x0060\\x0e;60\\x0e] avg.[\\x0090\\x0e;90\\x0e]\\nFIP\\n[65] 94:3 90:7 80 :7 64 :1 45 :9 – – 72:9 –\\nZhu et al. [66] 95:7 92:8 83 :7 72 :9 60 :1 – – 79:3 –\\nYim et al. [57] 99:5 95:0 88 :5 79 :9 61 :9 – – 83:3 –\\nDR-GAN [51] 97:0 94:0 90 :1 86 :2 83 :2 – – 89:2 –\\nFF-GAN [58] 95:7 94:6 92 :5 89 :7 85 :2 77 :2 61 :2 91:6 85:2\\ns-CNN\\n(ours) 95:9 95:1 92 :8 91 :6 88 :9 84 :9 78 :6 92:5 89:2\\nm-CNN (ours) 95:4 94:5 92 :6 91 :8 88 :4 85 :3 82 :2 92:2 89:6\\np-CNN (ours) 95:4 95:2 94 :3 93 :0 90 :3 87 :5 83 :9 93:5 91:1\\nwith\\nﬁxed loss weights for the side tasks as: \\x0bp=\\x0bl=\\x0be=\\n's=3 = 0:033. The summation of the loss weights for all side\\ntasks are equal to 'sfor all m-CNN variants in Table II for a\\nfair comparison.\\nComparing the rank-1 identiﬁcation rates of s-CNN and m-\\nCNNs, it is obvious that adding the side tasks is always helpful\\nfor the main task. The improvement of face recognition is\\nmostly on the proﬁle faces with MTL. The m-CNN “id+all”\\nwith dynamic weights shows superior performance to others\\nnot only in rank-1 identiﬁcation rate, but also in the side\\ntask estimations. Further, the lower rank-1 identiﬁcation rate\\nof “id+all” w.r.t “id+pos” indicates that more side tasks do\\nnot necessarily lead to better performance without properly\\nsetting the loss weights. In contrast, the proposed dynamic-\\nweighting scheme effectively improves the performance to\\n79:35% from the ﬁxed weighting of 77:59%. As will be\\nshown in Section IV-B, the side tasks in m-CNN help to\\ninject PIE variations into the shared representation, similar to\\na regularization term. For example, an L2 regularization will\\nencourage small weights. We add L2 regularization on the\\nshared representation to s-CNN (“id+L2”), which improves\\nover s-CNN without regularization. However it is still much\\nworse than the proposed m-CNN.Third, we train p-CNN by adding the PDB to m-CNN\\n“id+all” with dynamic weights. The loss weights are 'm= 1\\nfor the main tasks and 's= 0:1for the side tasks. The pro-\\nposed dynamic-weighting scheme allocates the loss weights\\nto both two main tasks and three side tasks. P-CNN further\\nimproves the rank-1 identiﬁcation rate to 79:55%.\\nDynamic-Weighting Scheme Figure 6 shows the dynamic\\nweights and losses during training for m-CNN and p-CNN.\\nFor m-CNN, the expression classiﬁcation task has the largest\\nweight in the ﬁrst epoch because it has the highest chance to be\\ncorrect with random guess with the least number of classes.\\nAs training goes on, pose classiﬁcation takes over because\\nit is the easiest task (highest accuracy in s-CNN) and also\\nthe most helpful for face recognition (compare “id+pos” to\\n“id+exp” and “id+illum”). \\x0bpstarts to decrease at the 11th\\nepoch when pose classiﬁcation is saturated. The increased\\n\\x0bland\\x0belead to a reduction in the losses of expression\\nand illumination classiﬁcations. As we expected, the dynamic-\\nweighting scheme assigns a higher loss weight for the easiest\\nand/or the most helpful side task.\\nFor p-CNN, the loss weights and losses for the side tasks\\nbehave similarly to those of m-CNN. For the two main tasks,\\nthe dynamic-weighting scheme assigns a higher loss weight to\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n8\\nn0 100 200 300energy\\n051015\\nsd\\nspsl\\nse\\n50 100 150 20050\\n100\\n150\\n200250300\\n20 40 6010\\n20\\n30\\n50 100 150 20050\\n100\\n150\\n200\\n250300\\n-0.4-0.200.20.40.6\\nn100 150 200 250 300accuracy\\n0.720.740.760.780.8\\nxn\\nyd\\n(a)\\n(b) (c)\\nFig. 7. Analysis on the effects of MTL: (a) the sorted energy vectors for all tasks; (b) visualization of the weight matrix Wallwhere the red box in the\\ntop-left is a zoom-in view of the bottom-right; (c) the face recognition performance with varying feature dimensions.\\nthe easier task at the moment. At the beginning, learning the\\npose-speciﬁc identity features is an easier task than learning\\nthe generic identity features. Therefore the loss weight \\x0bgis\\nhigher than \\x0bd. As training goes on, \\x0bdincreases as it has a\\nlower loss. Their losses reduce in a similar way, i.e., the error\\nreduction in one task will also contribute to the other.\\nCompare to Other Methods As shown in Table I, no\\nprior work uses the entire Multi-PIE for face recognition. To\\ncompare with state of the art, we choose to use setting III and\\nV to evaluate our method since these are the most challenging\\nsettings with more pose variation. The network structures and\\nparameter settings are kept the same as those of the full set\\nexcept that the outputs of the last fully connected layers are\\nchanged according to the number of classes for each task. Only\\npose and illumination are used as the side tasks.\\nThe performance on setting III is shown in Table III. Our\\ns-CNN already outperforms c-CNN forest [54], which is an\\nensemble of three c-CNN models. This is attributed to the deep\\nstructure of CASIA-Net [56]. Moreover, m-CNN and p-CNN\\nfurther outperform s-CNN with signiﬁcant margins, especially\\nfor non-frontal faces. We want to stress the improvement\\nmargin between our method 91:27% and the prior work of\\n76:89% — a relative error reduction of 62%.\\nThe performance on setting V is shown in Table IV.\\nFor fair comparison with FF-GAN [58], where the models\\nare ﬁnetuned from pre-trained in-the-wild models, we also\\nﬁnetune s-CNN, m-CNN, p-CNN models from the pre-trained\\nmodels on CASIA-Webface for 10epochs. Our performance is\\nmuch better than previous work with a relative error reduction\\nof60%, especially on large-pose faces. The performance gap\\nbetween Table III / IV and II indicates the challenge of face\\nrecognition under various expressions, which is less studied\\nthan pose and illumination variations on Multi-PIE.\\nB. How does m-CNN work?\\nIt is well known in both the computer vision and the\\nmachine learning communities that learning multiple tasks\\ntogether allows each task to leverage each other and improves\\nthe generalization ability of the model. For CNN-based MTL,\\nprevious work [63] has found that CNN learns shared features\\nfor facial landmark localization and attribute classiﬁcations,\\ne.g. smiling. This is understandable because the smiling at-tribute is related to landmark localization as it involves the\\nchange of the mouth region. However in our case, it is not\\nobvious how the PIE estimations can share features with the\\nmain task. On the contrary, it is more desirable if the learnt\\nidentity features are disentangled from the PIE variations.\\nIndeed, as we will show later, the PIE estimations regularize\\nthe CNN to learn PIE-invariant identity features.\\nWe investigate why PIE estimations are helpful for face\\nrecognition. The analysis is done on m-CNN model (“id+all”\\nwith dynamic weights) in Table II. Recall that m-CNN learns\\na shared embeding x2R320\\x021. Four fully connected lay-\\ners with weight matrices Wd\\n320\\x02200,Wp\\n320\\x0213,Wl\\n320\\x0219,\\nWe\\n320\\x026are connected to xto perform classiﬁcation of\\neach task (200 subjects, 13poses, 19illuminations, and 6\\nexpressions). We analyze the importance of each dimension\\ninxto each task. Taking the main task as an example, we\\ncalculate an energy vector sd2R320\\x021whose element is\\ncomputed as:\\nsd\\ni=200X\\nj=1jWd\\nijj: (13)\\nA higher value of sd\\niindicates that the ith feature in xis\\nmore important to the identity classiﬁcation task. The energy\\nvectors sp,sl,sefor all side tasks are computed similarly.\\nEach energy vector is sorted and shown in Figure 7 (a). For\\neach curve, we observe that the energy distributes unevenly\\namong all feature dimensions in x. Note that the indexes of\\nthe feature dimension do not correspond among them since\\neach energy vector is sorted independently.\\nTo compare how each feature in xcontributes to differ-\\nent tasks, we concatenate the weight matrix of all tasks as\\nWall\\n320\\x02238 = [Wd;Wp;Wl;We]and compute its energy\\nvector as sall. We sort the rows in Wallbased on the\\ndescending order in energy and visualize the sorted Wallin\\nFigure 7 (b). The ﬁrst 200columns represent the sorted Wd\\nwhere most energy is distributed in the ﬁrst \\x18280 feature\\ndimensions (rows), which are more crucial for face recognition\\nand less important for PIE classiﬁcations. We observe that x\\nare learnt to allocate a separate set of dimensions/features for\\neach task, as shown in the block-wise effect in the zoom-in\\nview. Each block shows the most essential features with high\\nenergy for PIE classiﬁcations respectively.\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n9\\n-90°-75°-60°-45°-30°-15°0°15°30°45°60°75°90°percentage\\n00.050.10.150.20.250.30.350.40.45\\n(a) (b) (c) (d)\\nFig. 10. (a) yaw angle distribution on CASIA-Webface; (b) average image of the right proﬁle group; (c) average image of the frontal group; (d) average\\nimage of the left proﬁle group.\\nTABLE V\\nPERFORMANCE COMPARISON ON LFW DATASET .\\nMethod #Net Training Set Metric Accuracy \\x06Std (% )\\nDeepID2 [45] 1 202;599 images of 10;177 subjects, private Joint-Bayes 95:43\\nDeepFace [49] 1 4:4M images of 4;030 subjects, private cosine 95:92\\x060:29\\nCASIANet [56] 1 494;414 images of 10;575 subjects, public cosine 96:13\\x060:30\\nWang et al. [52] 1 404;992 images of 10;553 subjects, public Joint-Bayes 96:2\\x060:9\\nLittwin and Wolf [32] 1 404;992 images of 10;553 subjects, public Joint-Bayes 98:14\\x060:19\\nMultiBatch [48] 1 2:6M images of 12K subjects, private Euclidean 98:20\\nVGG-DeepFace [38] 1 2:6M images of 2;622 subjects, public Euclidean 98:95\\nWen et al. [53] 1 0:7M images of 17;189 subjects, public cosine 99:28\\nFaceNet [41] 1 260 M images of 8M subjects, private L2 99:63 \\x060:09\\ns-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 97:87\\x060:70\\nm-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 98:07\\x060:57\\np-CNN (ours) 1 494;414 images of 10;575 subjects, public cosine 98:27\\x060:64\\nepoch0 5 10 15 20energy mean\\n0510152025id\\npos\\nillum\\nexp\\nepoch0 5 10 15 20energy std\\n012345id\\npos\\nillum\\nexp\\n(a) mean (b) std.\\nFig. 8. The mean and standard deviation of each energy vector during the\\ntraining process.\\nn0 100 200 300energy\\n05101520\\nsd\\nspsl\\nse\\nn0 100 200 300energy\\n05101520sd\\nspsl\\nse\\n(a)'s= 0:2 (b)'s= 0:3\\nFig. 9. Energy vectors of m-CNN models with different overall loss weights.\\nBased on the above observation, we conclude that the PIE\\nclassiﬁcation side tasks help to inject PIE variations into the\\nshared features x. The weight matrix in the fully connected\\nlayer learns to select identity features and ignore the PIE\\nfeatures for PIE-invariant face recognition. To validate this\\nobservation quantitatively, we compare two types of featuresfor face recognition: 1) xn:a subset of xwithnlargest\\nenergies in sd, which are more crucial in modeling identity\\nvariation; 2) yd\\n200\\x021=Wd\\nn\\x02200|xn\\x021+bd, which is the\\nmultiplication of the corresponding subset of Wdandxn.\\nWe varynfrom 100 to320 and compute the rank-1 face\\nidentiﬁcation rate on the entire Multi-PIE testing set. The\\nperformance is shown in Figure 7 (c). When xnis used, the\\nperformance improves with increasing dimensions and drops\\nwhen additional dimensions are included, which are learnt to\\nmodel the PIE variations. In contrary, the identity features\\nydcan eliminate the dimensions that are not helpful for\\nidentity classiﬁcation through the weight matrix Wd, resulting\\nin continuously improved performance w.r.t. n.\\nWe further analyze how the energy vectors evolve over time\\nduring training. Speciﬁcally, at each epoch, we compute the\\nenergy vectors for each task. Then we compute the mean and\\nstandard deviation of each energy vector, as shown in Figure 8.\\nDespite some local ﬂuctuations, the overall trend is that the\\nmean is decreasing and standard deviation is increasing as\\ntraining goes on. This is because in the early stage of training,\\nthe energy vectors are more evenly distributed among all\\nfeature dimensions, which leads to the higher mean values\\nand lower standard deviations. In the later stage of training,\\nthe energy vectors are shaped in a way to focus on some key\\ndimensions for each task, which leads to the lower mean values\\nand higher standard deviations.\\nThe CNN learns to allocate a separate set of dimensions\\nin the shared features to each task. The total number of\\ndimensions assigned to each task depends on the loss weights.\\nRecall that we obtain the overall loss weight for the side\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n10\\ntasks as's= 0:1 via brute-force search. Figure 9 shows\\nthe energy distributions with 's= 0:2and's= 0:3, which\\nare compared to Figure 7 (a) where 's= 0:1. We have two\\nobservations. First, a larger loss weight for the side tasks\\nleads to more dimensions being assigned to the side tasks.\\nSecond, the energies in sdincrease in order to compensate the\\nfact that the dimensions assigned to the main task decrease.\\nTherefore, we conclude that the loss weights control the energy\\ndistribution between different tasks.\\nC. Unconstrained Face Recognition\\nExperimental Settings We use CASIA-Webface [56] as our\\ntraining set and evaluate on LFW [20], CFP [42], and IJB-\\nA [26] datasets. CASIA-Webface consists of 494;414images\\nof10;575 subjects. LFW consists of 10folders each with\\n300 same-person pairs and 300 different-person pairs. Given\\nthe saturated performance of LFW mainly due to its mostly\\nfrontal view faces, CFP and IJB-A are introduced for large-\\npose face recognition. CFP is composed of 500subjects with\\n10frontal and 4proﬁle images for each subject. Similar to\\nLFW, CFP includes 10folders, each with 350 same-person\\npairs and 350 different-person pairs, for both frontal-frontal\\n(FF) and frontal-proﬁle (FP) veriﬁcation protocols. IJB-A\\ndataset includes 5;396 images and 20;412 video frames of\\n500subjects. It deﬁnes template-to-template matching for both\\nface veriﬁcation and identiﬁcation.\\nIn order to apply the proposed m-CNN and p-CNN, we need\\nto have the labels for the side tasks. However, it is not easy to\\nmanually label our training set. Instead, we only consider pose\\nestimation as the side task and use the estimated pose as the\\nlabel for training. We use PIFA [24] to estimate 34landmarks\\nand the yaw angle, which deﬁnes three groups: right proﬁle\\n[\\x0090\\x0e;\\x0030\\x0e), frontal [\\x0030\\x0e;30\\x0e], and left proﬁle (30\\x0e;90\\x0e].\\nFigure 10 shows the distribution of the yaw angle estimation\\nand the average image of each pose group. CASIA-Webface\\nis biased towards frontal faces with 88% faces belonging to\\nthe frontal pose group based on our pose estimation.\\nThe network structures are similar to those experiments on\\nMulti-PIE. All models are trained from scratch for 15epochs\\nwith a batch size of 8. The initial learning rate is set to 0:01 and\\nreduced at the 10th and 14th epoch with a factor of 0:1. The\\nother parameter settings and training process are the same as\\nthose on Multi-PIE. We use the same pre-processing as in [56]\\nto align a face image. Each image is horizontally ﬂipped for\\ndata augmentation in the training set. We also generate the\\nmirror image of an input face in the testing stage. We use the\\naverage cosine distance of all four comparisons between the\\nimage pair and its mirror images for face recognition.\\nPerformance on LFW Table V compares our face veri-\\nﬁcation performance with state-of-the-art methods on LFW\\ndataset. We follow the unrestricted with labeled outside data\\nprotocol. Although it is well-known that an ensemble of\\nmultiple networks can improve the performance [46], [47], we\\nonly compare CNN-based methods with one network for fair\\ncomparison. Our implementation of the CASIA-Net (s-CNN)\\nwith BN achieves much better results compared to the original\\nperformance [56]. Even with such a high baseline, m-CNNand p-CNN can still improve, achieving comparable results\\nwith state of the art, or better results if comparing to those\\nmethods trained with the same amount of data. Since LFW is\\nbiased towards frontal faces, we expect the improvement of\\nour proposed m-CNN and p-CNN to the baseline s-CNN to\\nbe larger if they are tested on cross-pose face veriﬁcation.\\nPerformance on CFP Table VI shows our face veriﬁcation\\nperformance comparison with state-of-the-art methods on CFP\\ndataset. For FF setting, m-CNN and p-CNN improve the\\nveriﬁcation rate of s-CNN slightly. This is expected, as there\\nis little pose variation. For FP setting, p-CNN substantially\\noutperforms s-CNN and prior work, reaching close-to-human\\nperformance (94:57%). Note our accuracy of 94:39% is9%\\nrelative error reduction of the previous state of the art [39] with\\n93:76%. Therefore, the proposed divide-and-conquer scheme\\nis very effective for in-the-wild face veriﬁcation with large\\npose variation. And the proposed stochastic routing scheme\\nimproves the robustness of the algorithm. Even with the\\nestimated pose serving as the ground truth pose label for MTL,\\nthe models can still disentangle the pose variation from the\\nlearnt identity features for pose-invariant face veriﬁcation.\\nPerformance on IJB-A We conduct close-set face iden-\\ntiﬁcation and face veriﬁcation on IJB-A dataset. First, we\\nretrain our models after removing 26overlapped subjects\\nbetween CASIA-Webface and IJB-A. Second, we ﬁne-tune\\nthe retrained models on the IJB-A training set of each fold\\nfor50epochs. Similar to [52], we separate all images into\\n“well-aligned” and “poorly-aligned” faces based on the face\\nalignment results and the provided annotations. In the testing\\nstage, we only select images from the “well-aligned” faces for\\nrecognition. If all images in a template are “poorly-aligned”\\nfaces, we select the best aligned face among them. Table VII\\nshows the performance comparison on IJB-A. Similarly, we\\nonly compare to the methods with a single model. The\\nproposed p-CNN achieves comparable performance in both\\nface veriﬁcation and identiﬁcation.\\nV. C ONCLUSIONS\\nThis paper explores multi-task learning for face recognition\\nwith PIE estimations as the side tasks. To solve the problem of\\nbalancing each task in MTL, we propose a dynamic-weighting\\nscheme to automatically assign the loss weights to each side\\ntask during the training process. This scheme is shown to as-\\nsign a larger loss weight to an easier side task and/or the most\\nhelpful side task. We also propose a pose-directed multi-task\\nCNN to learn pose-speciﬁc identity features during training\\nand a stochastic routing scheme for feature fusion in the testing\\nstage. A comprehensive study on the entire Multi-PIE dataset\\nhas shown the effectiveness of the proposed approach for PIE-\\ninvariant face recognition. An in-depth weight matrix analysis\\nhas shown why PIE estimations can help face recognition to\\nlearn a disentangled representation.\\nThe proposed method is applicable to in-the-wild datasets\\nwith estimated pose serving as the label for training. However,\\nwe do not see large improvement on LFW and IJB-A as that\\non Multi-PIE. This may due to several factors. First, both m-\\nCNN and p-CNN rely on the pose estimation, which is limited\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n11\\nTABLE VI\\nPERFORMANCE COMPARISON ON CFP DATASET . RESULTS REPORTED ARE THE AVERAGE \\x06STANDARD DEVIATION OVER THE 10FOLDS .\\nMethod# Frontal-Frontal Frontal-Proﬁle\\nMetric (%)! Accuracy EER AUC Accuracy EER AUC\\nSengupta et al. [42] 96:40\\x060:69 3:48\\x060:67 99:43\\x060:31 84:91\\x061:82 14:97\\x061:98 93:00\\x061:55\\nSankarana. et al. [40] 96:93\\x060:61 2:51\\x060:81 99:68\\x060:16 89:17\\x062:35 8:85\\x060:99 97:00\\x060:53\\nChen, et al. [9] 98:67\\x060:36 1:40\\x060:37 99:90\\x060:09 91:97\\x061:70 8:00\\x061:68 97:70\\x060:82\\nDR-GAN [51] 97:84\\x060:79 2:22\\x060:09 99:72\\x060:02 93:41\\x061:17 6:45\\x060:16 97:96\\x060:06\\nPeng, et al. [39] 98:67 – – 93:76 – –\\nHuman 96:24\\x060:67 5:34\\x061:79 98:19\\x061:13 94:57\\x061:10 5:02\\x061:07 98:92\\x060:46\\ns-CNN (ours) 97:34\\x060:99 2:49\\x060:09 99:69\\x060:02 90:96\\x061:31 8:79\\x060:17 96:90\\x060:08\\nm-CNN (ours) 97:77\\x060:39 2:31\\x060:06 99:69\\x060:02 91:39\\x061:28 8:80\\x060:17 97:04\\x060:08\\np-CNN (ours) 97:79\\x060:40 2:48\\x060:07 99:71\\x060:02 94:39\\x061:17 5:94\\x060:11 98:36\\x060:05\\nTABLE VII\\nPERFORMANCE COMPARISON ON IJB-A.\\nMethod# Veriﬁcation Identiﬁcation\\nMetric (% )! @FAR=0:01 @FAR=0:001 @Rank-1 @Rank-5\\nOpenBR [26] 23:6\\x060:9 10:4\\x061:4 24:6\\x061:1 37:5\\x060:8\\nGOTS [26] 40:6\\x061:4 19:8\\x060:8 44:3\\x062:1 59:5\\x062:0\\nWang et al. [52] 72:9\\x063:5 51:0\\x066:1 82:2\\x062:3 93:1\\x061:4\\nPAM [34] 73:3\\x061:8 55:2\\x063:2 77:1\\x061:6 88:7\\x060:9\\nDR-GAN [51] 77:4\\x062:7 53:9\\x064:3 85:5\\x061:5 94:7\\x061:1\\nDCNN [8] 78:7\\x064:3 – 85:2\\x061:8 93:7\\x061:0\\ns-CNN (ours) 75:6\\x063:5 52:0\\x067:0 84:3\\x061:3 93:0\\x060:9\\nm-CNN (ours) 75:6\\x062:8 51:6\\x064:5 84:7\\x061:0 93:4\\x060:7\\np-CNN (ours) 77:5\\x062:5 53:9\\x064:2 85:8\\x061:4 93:8\\x060:9\\nby the state-of-the-art pose estimation methods. Second, a\\nlarge training set might diminishes the beneﬁts of multi-\\ntask learning for unconstrained face recognition. Third, both\\nLFW and IJB-A have large variations other than pose such as\\nexpression, blurring, etc. that cannot be well handled by the\\nproposed method. Nevertheless, for dataset like CFP where\\npose variation is the major variation, we achieve state-of-the-\\nart performance on the frontal-to-proﬁle veriﬁcation protocol.\\nREFERENCES\\n[1] A. H. Abdulnabi, G. Wang, J. Lu, and K. Jia. Multi-task CNN model\\nfor attribute prediction. TMM, 2015.\\n[2] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canonical\\ncorrelation analysis. In ICML, 2013.\\n[3] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature\\nlearning. Machine Learning, 2008.\\n[4] A. Asthana, T. K. Marks, M. J. Jones, K. H. Tieu, and M. Rohith. Fully\\nautomatic pose-invariant face recognition via 3D pose normalization. In\\nICCV, 2011.\\n[5] V . Blanz and T. Vetter. A morphable model for the synthesis of 3D\\nfaces. SIGGRAPH, 1999.\\n[6] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-\\ndimensional feature and its efﬁcient compression for face veriﬁcation.\\nInCVPR, 2013.\\n[7] D. Chen, S. Ren, Y . Wei, X. Cao, and J. Sun. Joint cascade face detection\\nand alignment. In ECCV, 2014.\\n[8] J.-C. Chen, V . M. Patel, and R. Chellappa. Unconstrained face veriﬁca-\\ntion using deep cnn features. In WACV, 2016.\\n[9] J.-C. Chen, J. Zheng, V . M. Patel, and R. Chellappa. Fisher vector\\nencoded deep convolutional features for unconstrained face veriﬁcation.\\nInICIP, 2016.\\n[10] B. Chu, S. Romdhani, and L. Chen. 3D-aided face recognition robust\\nto expression and pose variations. In CVPR, 2014.\\n[11] C. Ding and D. Tao. A comprehensive survey on pose-invariant face\\nrecognition. ACM Transactions on intelligent systems and technology\\n(TIST), 2016.\\n[12] C. Ding, C. Xu, and D. Tao. Multi-task pose-invariant face recognition.\\nTIP, 2015.[13] L. El Shafey, C. McCool, R. Wallace, and S. Marcel. A scalable\\nformulation of probabilistic linear discriminant analysis: Applied to face\\nrecognition. TPAMI, 2013.\\n[14] H. Fei and J. Huan. Structured feature selection and task relationship\\ninference for multi-task learning. Knowledge and information systems,\\n2013.\\n[15] P. Gong, J. Zhou, W. Fan, and J. Ye. Efﬁcient multi-task feature learning\\nwith calibration. In ICKDDM, 2014.\\n[16] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-PIE.\\nIVC, 2010.\\n[17] H. Han, S. Shan, X. Chen, S. Lao, and W. Gao. Separability oriented\\npreprocessing for illumination-insensitive face recognition. In ECCV,\\n2012.\\n[18] H. Han, S. Shan, L. Qing, X. Chen, and W. Gao. Lighting aware\\npreprocessing for face recognition across varying illumination. In ECCV,\\n2010.\\n[19] T. Hassner, S. Harel, E. Paz, and R. Enbar. Effective face frontalization\\nin unconstrained images. In CVPR, 2015.\\n[20] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces\\nin the wild: A database for studying face recognition in unconstrained\\nenvironments. Technical report, Technical Report 07-49, University of\\nMassachusetts, Amherst, 2007.\\n[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network\\ntraining by reducing internal covariate shift. arXiv preprint:1502.03167,\\n2015.\\n[22] S. Ji and J. Ye. An accelerated gradient method for trace norm\\nminimization. In ICML, 2009.\\n[23] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\\nfast feature embedding. In ACMMM, 2014.\\n[24] A. Jourabloo and X. Liu. Large-pose face alignment via CNN-based\\ndense 3D model ﬁtting. In CVPR, 2016.\\n[25] M. Kan, S. Shan, H. Chang, and X. Chen. Stacked progressive auto-\\nencoders (SPAE) for face recognition across poses. In CVPR, 2014.\\n[26] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,\\nP. Grother, A. Mah, M. Burge, and A. K. Jain. Pushing the frontiers of\\nunconstrained face detection and recognition: IARPA Janus Benchmark\\nA. In CVPR, 2015.\\n[27] A. Li, S. Shan, X. Chen, and W. Gao. Maximizing intra-individual\\ncorrelations for face recognition across pose differences. In CVPR, 2009.\\n[28] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan. Morphable\\ndisplacement ﬁeld based image matching for face recognition across\\n1057-7149 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2765830, IEEE\\nTransactions on Image Processing\\n12\\npose. In ECCV, 2012.\\n[29] Y . Li, B. Zhang, S. Shan, X. Chen, and W. Gao. Bagging based efﬁcient\\nkernel ﬁsher discriminant analysis for face recognition. In ICPR, 2006.\\n[30] K. Lin, J. Xu, I. M. Baytas, S. Ji, and J. Zhou. Multi-task feature\\ninteraction learning. In ICKDDM, 2016.\\n[31] K. Lin and J. Zhou. Interactive multi-task relationship learning. In\\nICDM, 2016.\\n[32] E. Littwin and L. Wolf. The multiverse loss for robust transfer learning.\\nInCVPR, 2016.\\n[33] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep\\nhypersphere embedding for face recognition. In CVPR, 2017.\\n[34] I. Masi, S. Rawls, G. Medioni, and P. Natarajan. Pose-aware face\\nrecognition in the wild. In CVPR, 2016.\\n[35] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch networks\\nfor multi-task learning. In CVPR, 2016.\\n[36] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\\nboltzmann machines. In ICML, 2010.\\n[37] G. Obozinski, B. Taskar, and M. Jordan. Multi-task feature selection.\\nTechnical Report, Statistics Department, UC Berkeley, 2006.\\n[38] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In\\nBMVC, 2015.\\n[39] X. Peng, X. Yu, K. Sohn, D. N. Metaxas, and M. Chandraker.\\nReconstruction-based disentanglement for pose-invariant face recogni-\\ntion. In ICCV, 2017.\\n[40] S. Sankaranarayanan, A. Alavi, C. Castillo, and R. Chellappa. Triplet\\nprobabilistic embedding for face veriﬁcation and clustering. arXiv\\npreprint:1604.05417, 2016.\\n[41] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A uniﬁed\\nembedding for face recognition and clustering. In CVPR, 2015.\\n[42] S. Sengupta, J.-C. Chen, C. Castillo, V . M. Patel, R. Chellappa, and\\nD. W. Jacobs. Frontal to proﬁle face veriﬁcation in the wild. In WACV,\\n2016.\\n[43] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman. Fisher vector\\nfaces in the wild. In BMVC, 2013.\\n[44] K. Sohn. Improved deep metric learning with multi-class n-pair loss\\nobjective. In NIPS, 2016.\\n[45] Y . Sun, Y . Chen, X. Wang, and X. Tang. Deep learning face represen-\\ntation by joint identiﬁcation-veriﬁcation. In NIPS, 2014.\\n[46] Y . Sun, D. Liang, X. Wang, and X. Tang. DeepID3: Face recognition\\nwith very deep neural networks. arXiv preprint:1502.00873, 2015.\\n[47] Y . Sun, X. Wang, and X. Tang. Deeply learned face representations are\\nsparse, selective, and robust. In CVPR, 2015.\\n[48] O. Tadmor, Y . Wexler, T. Rosenwein, S. Shalev-Shwartz, and\\nA. Shashua. Learning a metric embedding for face recognition using\\nthe multibatch method. In NIPS, 2016.\\n[49] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace: Closing the\\ngap to human-level performance in face veriﬁcation. In CVPR, 2014.\\n[50] Y . Tian, P. Luo, X. Wang, and X. Tang. Pedestrian detection aided by\\ndeep learning semantic tasks. In CVPR, 2015.\\n[51] L. Tran, X. Yin, and X. Liu. Disentangled representation learning GAN\\nfor pose-invariant face recognition. In CVPR, 2017.\\n[52] D. Wang, C. Otto, and A. K. Jain. Face search at scale. TPAMI, 2016.\\n[53] Y . Wen, K. Zhang, Z. Li, and Y . Qiao. A discriminative feature learning\\napproach for deep face recognition. In ECCV, 2016.\\n[54] C. Xiong, X. Zhao, D. Tang, K. Jayashree, S. Yan, and T.-K. Kim.\\nConditional convolutional neural network for modality-aware face recog-\\nnition. In ICCV, 2015.\\n[55] M. Yang, L. Gool, and L. Zhang. Sparse variation dictionary learning\\nfor face recognition with a single training sample per person. In ICCV,\\n2013.\\n[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representation from\\nscratch. arXiv preprint:1411.7923, 2014.\\n[57] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim. Rotating your\\nface using multi-task deep neural network. In CVPR, 2015.\\n[58] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker. Towards large-pose\\nface frontalization in the wild. In ICCV, 2017.\\n[59] C. Zhang and Z. Zhang. Improving multiview face detection with multi-\\ntask deep convolutional neural networks. In WACV, 2014.\\n[60] H. Zhang, Y . Zhang, and T. S. Huang. Pose-robust face recognition via\\nsparse representation. Pattern Recognition, 2013.\\n[61] Y . Zhang and D.-Y . Yeung. A convex formulation for learning task\\nrelationships in multi-task learning. arXiv preprint:1203.3536, 2012.\\n[62] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial landmark detection\\nby deep multi-task learning. In ECCV, 2014.\\n[63] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Learning deep representation\\nfor face alignment with auxiliary attributes. TPAMI, 2016.\\n[64] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity pose and\\nexpression normalization for face recognition in the wild. In CVPR,\\n2015.\\n[65] Z. Zhu, P. Luo, X. Wang, and X. Tang. Deep learning identity-preserving\\nface space. In ICCV, 2013.[66] Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: A deep\\nmodel for learning face identity and view representations. In NIPS,\\n2014.\\nXi Yin received the B.S. degree in Electronic and\\nInformation Science from Wuhan University, China,\\nin 2013. Since August 2013, she has been work-\\ning toward her Ph.D. degree in the Department of\\nComputer Science and Engineering, Michigan State\\nUniversity, USA. Her research area are computer\\nvision, deep learning, and image processing. Her pa-\\nper on multi-leaf segmentation won the Best Student\\nPaper Award at Winter Conference on Application\\nof Computer Vision (WACV) 2014.\\nXiaoming Liu is an Assistant Professor at the\\nDepartment of Computer Science and Engineering\\nof Michigan State University. He received the Ph.D.\\ndegree in Electrical and Computer Engineering from\\nCarnegie Mellon University in 2004. Before joining\\nMSU in Fall 2012, he was a research scientist at\\nGeneral Electric (GE) Global Research. His research\\ninterests include computer vision, machine learning,\\nand biometrics. As a co-author, he is a recipient\\nof Best Industry Related Paper Award runner-up at\\nICPR 2014, Best Student Paper Award at WACV\\n2012 and 2014, and Best Poster Award at BMVC 2015. He has been the\\nArea Chair for numerous conferences, including FG, ICPR, WACV , ICIP,\\nand CVPR. He is the program chair of WACV 2018. He is an Associate\\nEditor of Neurocomputing journal. He has authored more than 100 scientiﬁc\\npublications, and has ﬁled 22 U.S. patents.\\n\",\n",
       " '980 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nMulti-task Pose-Invariant Face Recognition\\nChangxing Ding, Student Member, IEEE , Chang Xu, and Dacheng Tao, Fellow, IEEE\\nAbstract — Face images captured in unconstrained\\nenvironments usually contain signiﬁcant pose variation,which dramatically degrades the performance of algorithmsdesigned to recognize frontal faces. This paper proposes a novelface identiﬁcation framework capable of handling the full rangeof pose variations within ±90° of yaw. The proposed framework\\nﬁrst transforms the original pose-invariant face recognitionproblem into a partial frontal face recognition problem. A robustpatch-based face representation scheme is then developed torepresent the synthesized partial frontal faces. For each patch,a transformation dictionary is learnt under the proposed multi-task learning scheme. The transformation dictionary transformsthe features of different poses into a discriminative subspace.Finally, face matching is performed at patch level rather thanat the holistic level. Extensive and systematic experimentationon FERET, CMU-PIE, and Multi-PIE databases shows thatthe proposed method consistently outperforms single-task-basedbaselines as well as state-of-the-art methods for the poseproblem. We further extend the proposed algorithm for theunconstrained face veriﬁcation problem and achieve top-levelperformance on the challenging LFW data set.\\nIndex Terms — Pose-invariant face recognition, partial face\\nrecognition, multi-task learning.\\nI. I NTRODUCTION\\nFACE recognition has been one of the most active research\\ntopics in computer vision for more than three decades.\\nWith years of effort, promising results have been achievedfor automatic face recognition, in both controlled [1] and\\nuncontrolled environments [2], [3]. However, face recogni-\\ntion remains signiﬁcantly aff ected by the wide variations of\\npose, illumination, and expressi on often encountered in real-\\nworld images. The pose problem in particular is still largely\\nunsolved, as argued in a recent work [4]. In this paper,\\nwe mainly handle the identiﬁcation problem of matching an\\narbitrary pose probe face with frontal gallery faces, which is\\nManuscript received August 7, 2014; revised October 27, 2014; accepted\\nDecember 28, 2014. Date of publication January 12, 2015; date of current\\nversion January 30, 2015. This work was supported by the Australian\\nResearch Council under Grant DP-120103730, Grant DP-140102164,Grant FT-130101457, and Grant LP-140100569. The associate editor coor-dinating the review of this manuscript and approving it for publication was\\nProf. Shiguang Shan. Corresponding author: Dacheng Tao.\\nC. Ding and D. Tao are with the Centre for Quantum Computation and\\nIntelligent Systems, Faculty of Engineering and Information Technology,\\nUniversity of Technology at Sydney, Sydney, NSW 2007, Australia (e-mail:\\nchangxing.ding@student.uts.edu. au; dacheng.tao@uts.edu.au).\\nC. Xu was with the Center of Quantum Computation and Intelligent\\nSystems, Faculty of Engineering and Information Technology, University of\\nTechnology at Sydney, Sydney, NSW 2007, Australia. He is now with the\\nKey Laboratory of Machine Perception, School of Electronics Engineering and\\nComputer Science, Ministry of Education, Peking University, Beijing 100871,\\nChina (e-mail: xuchang@pku.edu.cn).\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TIP.2015.2390959\\nFig. 1. (a) The rigid rotation of the h ead results in self-occlusion as well\\nas nonlinear facial texture deformati on. (b) The pose problem is combined\\nwith other factors, e.g., variations in expression and illumination, to affectface recognition.\\nthe most common setting for both the research and application\\nof pose-invariant face recognition (PIFR) [4]–[8]. At the end\\nof the paper, we brieﬂy extend the proposed approach to solve\\nthe unconstrained face veriﬁcation problem [9].\\nPose variation induces drama tic appearance change in the\\nface image. Essentially, thi s is caused by the complex 3D\\ngeometrical structure of the human head. As shown in Fig. 1,\\nthe rigid rotation of the head results in self-occlusion, whichmeans that some facial texture will be invisible with variations\\nin pose. Even the shape and position in the image of visible\\nfacial texture vary nonlinearly from pose to pose. The pose\\nproblem is also usually combined with other factors, such\\nas variations in illumination and expression, to affect theappearance of face images. In consequence, the extent of\\nappearance change caused by pose variation is usually greater\\nthan that caused by differences in identity, and the performanceof frontal face recognition algorithms degrades dramatically\\nwhen the images to be matched feature different poses.\\nDirectly matching faces in different poses is difﬁ-\\ncult. One intuitive solution is to conduct face synthesis\\nso that the two facial imag es can be compared in the\\nsame pose. Most approaches fo llowing this idea are dedi-\\ncated to recovering complete frontal faces from non-frontal\\nfaces [4], [8], [10]–[12]. However, synthesizing the entirefrontal face from proﬁle faces is difﬁcult since most facial\\ntexture is invisible as a result of occlusion. Therefore, the\\naforementioned methods tend to constrain their recognition\\nability within ±45° of yaw variation.\\nInspired by the observation that human beings can easily\\nrecognize proﬁle faces without th e need to elaborately recover\\nthe whole frontal face, we present a novel face representation\\napproach that makes full use of just the facial texture thatis occlusion-free. This representation approach is general in\\n1057-7149 © 2015 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 981\\nFig. 2. Overview of the proposed PBPR-MtFTL framework for pose-invariant\\nface recognition, as applied to the recognition of arbitrary pose probe faces.\\nnature, which means that it applies continuously to the full\\nrange of pose variations between −90° and +90° of yaw.\\nUsing simple pose normalization and pre-processing opera-\\ntions, our approach converts the original PIFR problem into\\na partial face recognition task [13], [14], in which the faceis represented by the unoccluded facial parts in a patch-based\\nfashion. The proposed face representation scheme is therefore\\nnamed Patch-based Partial Representation (PBPR).\\nFeature transformation enhances recognition ability by\\ntransforming the features from the gallery and probe images\\nto a common discriminative subspace. Based on this intu-\\nition, we propose a learning method called Multi-Task\\nFeature Transformation Learning (MtFTL). By considering thecorrelation between the transformation matrices for different\\nposes, MtFTL consistently achieves better performance than\\nits single-task based counterparts. Its advantage is particularlyevident when the size of training data is limited. The trans-\\nformation matrices learnt by MtFTL are highly compact as a\\nresult of sharing most projection vectors across poses, which\\nadditionally reduces memory cost.\\nWe term the entire proposed framework for tackling the\\npose problem PBPR-MtFTL. Under this framework, matching\\nan arbitrary pose probe face and frontal gallery faces involves\\ntransformation of the extracted PBPR representation usingthe learnt MtFTL transformation dictionaries, followed by\\npatch-level cosine distance computation and score fusion,\\nas illustrated in Fig. 2. Extensive experiments on FERET,CMU-PIE, and Multi-PIE datasets indicate that superior per-\\nformance is consistently achieved with PBPR-MtFTL.\\nThe remainder of the paper is organized as follows:\\nSection II brieﬂy reviews related works for PIFR and multi-\\ntask learning. The proposed PBPR face representation schemeis illustrated in Section III. The m ulti-task feature transfor-\\nmation learning approach MtFT L is described in Section IV .\\nFace matching using PBPR-MtFTL is introduced in Section V .Experimental results are presented in Section VI, leading to\\nconclusions in Section VII.\\nII. R\\nELATED STUDIES\\nA. Pose-Invariant Face Recognition\\nMany promising approaches have been proposed to tackle\\nthe pose challenge in face recognition [15]. These methods can\\nbe broadly classiﬁed into two categories: face image synthesis-\\nbased methods and synthesis-free methods.\\nMost previous works rely on face image synthesis,\\nFace image synthesis can be accomplished with 2D or3D techniques. Using 2D techniques, Ashraf et al. [16] learnt\\npatch-wise warps between two i mages with the Lucas-Kanade\\noptimization algorithm. Chai et al. [11] proposed the\\nlocally linear regression method for frontal face synthesis.\\nLiet al. [17] proved that more accurate face synthesis can\\nbe achieved by imposing lasso or ridge regularization on\\nthe regression function. Ho et al. [8] proposed synthesizing\\nthe virtual frontal view using Markov Random Fields and avariant of belief propagation algorithm. Li et al. [18] proposed\\nthe Morphable Displacement Field method for frontal face\\nsynthesis and achieved pixel-level semantic correspondencebetween a face pair. Other 2D face synthesis methods can\\nbe found in related facial analysis topics, e.g., face hal-\\nlucination [19], [20]. Methods based on 3D face models\\nhave also been introduced because pose variation is essen-\\ntially caused by the 3D rigid transformation of the face.Blanz et al. [5] developed the 3D Morphable Model (3DMM)\\nwhich can be used to ﬁt a 2D face image in arbitrary\\nposes. Face recognition can be conducted by rendering aspeciﬁed view with the 3D model or directly matching the\\nregression coefﬁcents; however, the optimization of 3DMM is\\ncomputationally expensive. In [21], an efﬁcient 3D modeling\\nmethod called Generic Elastic Models (GEM) was introduced.\\nBy assuming that the depth information between individualsis not highly discriminative, GEM estimates the 3D shape\\nof a 2D frontal face by directly assigning generic depth\\ninformation. Arbitrary pos e face images can be rendered with\\nGEM for matching; however, GEM can only estimate the\\n3D shape for frontal faces. Asthana et al. [22] proposed\\nsynthesizing the frontal face image by aligning and mapping\\nthe texture of a 2D face image to a 3D generic shape model.\\nThis method can handle pose variations within ±45° of yaw.\\nThe synthesis-free approaches aim to extract pose-robust\\nfeatures or to transform features from different poses into a\\nshared subspace. For example, [12], [23] proposed learningpose-insensitive face representations with neural networks.\\nWright et al. [24] represented the face image as a histogram\\nof quantized patch descriptors which are expanded with theirspatial locations. Yi et al. [25] proposed the extraction of pose\\nadaptive features by transfor ming Gabor ﬁlers according to\\nthe pose of the face image. Of the feature transformation-\\nbased methods, Li et al. [26] proposed employing Canonical\\nCorrelation Analysis to project image patches under twodifferent views to a shared subspace, where the patch similarity\\ncan be measured by the correlation of their projections. Similar\\nmethods incorporate Partial Least Squares (PLS) [27] and\\n982 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nGeneralized Multiview Analysis [28]. Recently, Kan et al. [29]\\npropose an efﬁcient algorithm that simultaneously learns pose-\\nspeciﬁc transformations to a c ommon discriminative space.\\nThis approach enables conveni ent matching between two faces\\nwith arbitrary poses. Other methods to learn robust representa-\\ntions or feature transformations can be found in related areas,\\ne.g., scene classiﬁcation [30] and image denoising [31].\\nThe two categories of method described above are closely\\nrelated. For example, it is shown in [12] and [23] that pose-\\ninvariant features can be utilized for frontal-face reconstruc-\\ntion. It is also demonstrated that the coefﬁcients of regression\\nmodels for face synthesis [4], [5], [11], [17] can be regardedas pose-insensitive features for face matching. Our work is\\nrelated to both categories, and there is clear novelty: First, the\\nproposed PBPR method represents arbitrary pose face imagesfrom the perspective of partial face recognition. Second, the\\nMtFTL approach learns compact feature transformation for\\nvarious poses based on the principle of multi-task learning,\\nwhich is novel for PIFR. Third, the PBPR-MtFTL framework\\ncontinuously tackles the full range of pose variations from−90° to +90° of yaw and obtains strong performance. In com-\\nparison, the recognition ability of existing methods is typically\\nrestricted to a range of ±45° of yaw [4], [32].\\nB. Multi-Task Learning\\nMulti-task learning (MTL) is a machine learning technique\\nthat learns several tasks simultaneously for better performance\\nby capturing the intrinsic correla tion between different tasks.\\nMTL implicitly increases the sample size and improves the\\ngeneralization ability for each task; hence, it is especially\\nbeneﬁcial when the training data for the tasks is small.\\nWhile MTL has been widely applied to computer vision\\ntasks, e.g., visual tracking [33], action recognition [34]–[36],\\nand face recognition [37], [38], it is new for PIFR. Existing\\napproaches for PIFR ignore the correlation between the feature\\ntransformations of different poses [26], [28]. To the best ofour knowledge, MTL for PIFR is only brieﬂy mentioned\\nin [39] but no detailed information is provided, and multi-view\\nreconstruction is targeted rather than feature transformationlearning. Nevertheless, MTL provides a principled way for us\\nto model the correlation between poses if we view the learning\\nof feature transformation for each pose as a task. MtFTL is\\narguably the ﬁrst MTL approach that jointly learns feature\\ntransformations for different poses and is shown to proﬁt fromthe latent inter-pose correlations.\\nIII. F\\nACE REPRESENTATION FOR THE POSE PROBLEM\\nExisting face representation methods tend to extract ﬁxed-\\nlength features from face images, with the underlying assump-tion that all facial components are visible in the image [13].\\nHowever, as shown in Fig. 2, this hypothesis does not hold\\nfor a proﬁle face where there is severe self-occlusion. In this\\nsection, we propose the ﬂexible PBPR face representation\\nscheme, where the length of face representation is related tothe pose of the face; for example, a frontal face image will\\nhave larger face representatio n than a proﬁle face image. This\\nis reasonable, since the proﬁle face provides less information\\nFig. 3. Overview of the proposed PBPR face representation method. PBPR is\\napplied to arbitrary pose face images. The ﬁnal PBPR representation is a set\\nof patch-level DCP features af ter dimension reduction by PCA.\\nfor recognition. As shown in Fig. 3, PBPR is essentially\\ncomposed of three steps: face pose normalization, unoccluded\\nfacial texture detection, and p atch-wise feature extraction.\\nIn this section, we describe the three main components in\\ndetail.\\nA. Face Pose Normalization\\nA standard 3D method is adopted for face pose nor-\\nmalization [22]. The ﬁve most stable facial feature points,\\ni.e., the centers of both eyes, the tip of the nose, and the twomouth corners, are ﬁrst detected automatically or manually.\\nFor proﬁle faces (as shown in Fig. 2), the coordinates of\\nthe occluded facial feature poi nts are estimated. Using the\\northographic projection model [40] and the detected ﬁve facial\\nfeature points, a 3D generic shape model is aligned to the 2D\\nface image.\\n1The 2D face image is then back-projected to\\nthe 3D model, and a frontal face image is rendered with the\\ntextured 3D model.\\nP r e v i o u sw o r k sr e l yo nd e n se facial feature points,\\ne.g., 68 points in [22] and 79 points in [4], for accurate pose\\nnormalization. However, detecting dense facial feature pointsfor proﬁle faces is difﬁcult due to the severe self-occlusion of\\nthe face, which in turn restricts the range of poses that these\\nmethods can handle. In stead, only the ﬁve most stable facial\\nfeature points are utilized for pose normalization in this paper.\\nThis greatly facilitates the realization of a fully automaticface recognition system and extends the range of poses that\\ncan be processed. Although using sparse facial feature points\\nwill result in larger normalization error, we highlight the\\n1In this step, we roughly estimate the pose of the 2D face image by the\\nmethod described in [41].\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 983\\nFig. 4. Illustration of facial contour detection. (a) The 3D generic shape\\nmodel is projected to the 2D plane and its facial contour is detected;\\n(b) the region containing the facial contour of the 2D face image is estimated;\\n(c) candidate facial contour points; (d ) facial contour obtained by point set\\nregistration.\\npower of the proposed PBPR-MtFTL framework given its low\\nnormalization requirements.\\nB. Unoccluded Facial Texture Detection\\nPose normalization corrects the deformation of facial texture\\nresulting from pose variations, but it cannot recover the texture\\nlost by occlusion. Rather than trying to synthesize the occludedtexture to obtain a complete frontal face [4], we propose to\\nmake full use of the unoccluded texture only. This is inspired\\nby the observation that human beings can easily recognizeproﬁle faces without the need to recover the whole frontal\\nface. As shown in Fig. 3, the main boundary between the\\noccluded and unoccluded facial texture is the facial contour.\\nTherefore, facial contour detection is the key to identifying\\nthe occluded facial texture.\\nAlthough there are off-the-shelf face alignment tools for\\nfacial contour detection, they return only sparse facial contour\\npoints and may not be reliable enough to severe occlusion,expression, and pose variations. We propose a much simpler\\nbut effective method that makes use of the 3D generic shape\\nmodel. After aligning the 3D model and the 2D face image,\\nit can be projected to the 2D image plane roughly in the pose\\nof the 2D face. As shown in Fig. 4(a), the contour of the3D model can be easily detected. Based on the contour of the\\n3D model, the facial contour search of the 2D face can be\\nconstrained within a certain region, as illustrated in Fig. 4(b).The edge points are then detected in this region by the Canny\\noperator [42]. To reduce imposters, only the edge points with\\nhorizontal gradient directions are saved, with the prior that\\nfacial contour extends in a vertical direction. Lastly, the facial\\ncontour is obtained by a point sets registration algorithm calledCoherent Point Drift (CPD) [43]. Brieﬂy, CPD iteratively\\naligns the facial contour highlighted in Fig. 4(a) to the edge\\npoint set shown in Fig. 4(c) with afﬁne transformations. Theimposter contour points in Fig. 4(c) can gradually be detected\\nand ignored. The obtained facial contour is shown in Fig. 4(d).\\nMore detection examples on unconstrained face images in theLFW dataset [9] are shown in Fig. 5.\\nAlongside the projection of facial texture in the ﬁrst step,\\nthe detected facial contour poi nts are ﬁrst projected to the\\n3D model and then projected to the rendered frontal face\\nimage. Since the head is approximately an ellipsoid, thefacial contour points in the frontal view are ﬁtted with an\\narc. As shown in Fig. 3, the arc effectively separates the\\nunoccluded and occluded texture in the rendered frontal image.\\nFig. 5. Examples of facial contour det ection for unconstrained face images\\nin the LFW dataset.\\nIn the following subsection, face representation is built using\\nonly the detected unoccluded facial texture.\\nC. Patch-Based Face Representation\\nThe area of the unoccluded facial texture in the rendered\\nfrontal view varies with pose change, with demonstrable\\nﬂuctuation in the amount of effective information available forface recognition. In light of this observation, a variable-length\\nface representation method is proposed.\\nAs illustrated in Fig. 3, the normalized face image is\\nﬁrst divided into M×Noverlapped patches. The severity\\nof occlusion for each patch is then evaluated based on thedetected boundary between the occluded and unoccluded facial\\ntexture. If more than 80% of pixels in one patch fall into\\nthe unoccluded region, then it is designated as an unoccludedpatch; otherwise, the patch is ignored due to the large area of\\nocclusion. Next, each of the unoccluded patches is split into\\nJ×Jcells. A state-of-the-art local descriptor called Dual-\\nCross Patterns (DCP) [2] is employed for feature extraction.\\nThe concatenated DCP histogram feature from the J\\n2cells\\nforms the raw feature of the patch. Following [2], elements\\nin the DCP histogram are normalized by square root. Lastly,\\nPrincipal Component Analysis (PCA) is applied to each patchto project its feature into a subspace with dimension D,b y\\nwhich the noise is suppressed.\\nThe set of patch-level DCP features following PCA process-\\ning from all unoccluded patches forms the representation of\\nthe face image. Note that this representation method is generalin nature, meaning that it applie s to faces with arbitrary poses.\\nThis is a valuable property, because we do not need to apply\\ndifferent algorithms to frontal and non-frontal faces, unlikesome existing approaches [8].\\nIV . M\\nULTI -TASK FEATURE TRANSFORMATION LEARNING\\nThe previous section has introduced the PBPR face\\nrepresentation scheme, whereby face recognition can be\\n984 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nFig. 6. Pose normalization for non-frontal images. The boundary between\\nunoccluded and occluded facial texture is detected by the method illustratedin Fig. 3. (a) −90°≤yaw≤−45°; (b) −30°≤yaw≤+30°; (c) +45°≤\\nyaw≤+ 90°. The image quality is degraded with the increase in value of\\nthe yaw angles, and the amount of unoccluded facial texture for recognitiondecreases.\\naccomplished by directly matching corresponding patch fea-\\ntures of two face images. In this section, we further proposethe MtFTL approach for learning transformation dictionaries,\\nwhich enable the patch features of a frontal face and a non-\\nfrontal face to be transformed i nto a common discriminative\\nspace to enhance recognition ability. The learning process is\\npatch-wise, which means a separate transformation dictionary\\nis learnt by MtFTL for each patch. Consequently, we obtain\\nM×Ntransformations dictionaries. Details of the MtFTL\\napproach are illustrated below.\\nA. Feature Transformation Learning\\nThree aspects are considered in the design of feature trans-\\nformation learning. First, as shown in Fig. 6, the normalized\\nimages from different poses are of different image quality,therefore there will be differences in the transformations for\\ndifferent poses. Second, a strong correlation exists between\\nthe feature transformations for different poses, since they\\nessentially process the data of the same subjects. Third, the\\namount of training data might be limited in real scenarios,because collecting multi-pose face images tends to be difﬁcult.\\nIdeally, the shared knowledge from different poses should be\\nleveraged for robust transformation learning.\\nThese considerations call for a multi-task strategy for fea-\\nture transformation learning, in which the learning for each\\npose type is regarded as a task. Therefore, we propose theMtFTL approach which takes into consideration both the\\ncorrelation and difference between tasks. Instead of learning\\na separate transformation matrix for each task [44], MtFTL\\nlearns a common transformation dictionary for all the tasks.\\nDifferences between the tasks are reﬂected by the selection ofdifferent projection vectors in the transformation dictionary.\\nHence, MtFTL learns more compact feature transformations\\nthan previous approaches [44].Before presenting the formulation of the proposed model,\\nseveral necessary notations are introduced. Let Pbe the\\nnumber of tasks, i.e., the number of pose types that are\\navailable in the training set for the current patch. The set\\n{(X\\nt,Yt):1≤t≤P}stores the training data composed of\\nintra-personal and inter-personal patch pairs. Xt∈RD×Ntp\\nand Yt∈ RD×Ntn,w h e r e Ntpand Ntnare the number of\\nintra-personal and inter-personal patch pairs for the tth task,\\nrespectively. The nth column of Xtis denoted as xn\\ntand\\nxn\\nt=xtn−x0n,w h e r e {xtn,x0n}is one intra-personal patch\\npair between the pose type tand the frontal pose. Similarly,\\nyn\\ntis the nth column of Ytand yn\\nt=ytn−y0n,w h e r e\\n{ytn,y0n}is one inter-personal patch pair between the pose\\ntype tand the frontal pose. We learn the transformation\\ndictionary U∈ RD×Dshared by all tasks, and the set\\nof vectors αt∈{0,1}D,1≤t≤P.αtselects projec-\\ntion vectors for task tfrom the shared dictionary U.L e t\\nA∈RD×Pbe the matrix of stacked vectors αt. In addition,\\nAt∈RD×Dis a diagonal matrix expanded by αt, denoted\\nasAt=diag(αt).\\nThe loss function for task tis denoted as TU,t. It is based\\non the principle that the margi n between intra-personal patch\\npairs and inter-personal patch pairs should be as large aspossible.\\nT\\nU,t(αt)=1\\nNtp/bardblAtUTXt/bardbl2\\nF−λ\\nNtn/bardblAtUTYt/bardbl2\\nF, (1)\\nwhere λis a regularization paramet er that weights the intra-\\npersonal and inter-personal terms. We then formulate themulti-task learning algorithm as the optimization problem:\\nmin\\nU,A1\\nPP/summationdisplay\\nt=1TU,t(αt)\\ns.t.UTU=I. (2)\\nFor simplicity, the above problem is relaxed to\\nmin\\nU,A1\\nPP/summationdisplay\\nt=1TU,t(αt)+μ/bardblUTU−I/bardbl2\\nF, (3)\\nwhere μis another regularization parameter. We set the\\nnumber of non-zero elements in αtasd, and aim to optimize\\nαtto select dmost discriminative projection vectors from U\\nfor the tth task. The optimal value of μ,d,a n dλis estimated\\nthrough cross validation.\\nNote that the transformation dictionary Uis learnt jointly\\nfor all Ptasks, which enables knowledge sharing between\\nthe tasks. We show in the experiment section that knowledge\\nsharing is especially important when the amount of trainingdata for the tasks is limited.\\nB. Iterative Optimization Algorithm\\nThe optimization problem (3) of MtFTL is convex in U\\nfor ﬁxed Aand in Afor ﬁxed U. Therefore, we solve\\nthis problem by alternately optimizing Uand A.T h eﬁ n a l\\nlearning algorithm is summarized in Algorithm 1. The main\\noptimization procedure can be outlined in two steps.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 985\\nAlgorithm 1 Multi-Task Feature Transformation Learning\\nLearning A: With ﬁxed U, the optimization problems for\\neach task decouple. For the tth task, the optimal αtis obtained:\\nmin\\nαt∈RDTU,t(αt), (4)\\nTU,t(αt)=tr/parenleftbigg1\\nNtpAtUTXtXT\\ntUA t−λ\\nNtnAtUTYtYT\\ntUA t/parenrightbigg\\n=tr/braceleftbigg\\nAt/parenleftbigg1\\nNtpUTXtXT\\ntU−λ\\nNtnUTYtYT\\ntU/parenrightbigg\\nAt/bracerightbigg\\n=αT\\ntBtαt, (5)\\nwhere tr(·)represents the trace of a matrix; and Btis a\\ndiagonal matrix by directly copying the diagonal elements\\nfrom the matrix1\\nNtpUTXtXT\\ntU−λ\\nNtnUTYtYT\\ntU. Since the\\nrole of αtis to select dmost discriminative projection vectors\\nfor the tth pose type, the elements in αtthat correspond to d\\nsmallest diagonal elements in Btare set as 1 while the other\\nelements in αtare set as 0.\\nLearning U: The shared transformation dictionary Ucou-\\nples all the tasks. In this step, Uis updated efﬁciently via\\nthe limited-memory BFGS (LBFGS) algorithm. The choice of\\nLBFGS algorithm here is due to both its high efﬁciency and\\nlow memory requirement.\\nWhile Ais ﬁxed, the optimization problem (3) reduces to\\nmin\\nU∈RD×DTA(U), (6)\\nTA(U)=1\\nPP/summationdisplay\\nt=1/parenleftbigg1\\nNtp/bardblAtUTXt/bardbl2\\nF−λ\\nNtn/bardblAtUTYt/bardbl2\\nF/parenrightbigg\\n+μ/bardblUTU−I/bardbl2\\nF. (7)\\nThe derivative of TA(U)with respect to Uis\\n∂TA(U)\\n∂U=2\\nPP/summationdisplay\\nt=1/parenleftbigg1\\nNtpXtXT\\ntUA tAt−λ\\nNtnYtYT\\ntUA tAt/parenrightbigg\\n+4μ/parenleftbig\\nUUTU−U/parenrightbig\\n. (8)\\nWith the provided formula for calculating TA(U)and\\n∂TA(U)\\n∂U, the optimization problem can be readily solved with\\nthe LBFGS algorithm [45].\\nInitialization: The transformation matrix Uis simply ini-\\ntialized with a random matrix whose elements are drawn from\\nthe standard uniform distribution on the open interval (0,1).In the experiment section, we show that even the randomly\\ninitialized Uachieves promising performance.\\nStopping criterion: The iterative optimization process stops\\nwhen the Frobenius norms of both /Delta1Uand/Delta1Aare below\\n/epsilon1=10−3,w h e r e /Delta1Uand/Delta1Aare the difference matrices\\nbetween two successive iterations for Uand A, respectively.\\nC. Theoretical Analysis\\nIn this subsection, we study the robustness and general-\\nization error of the proposed MtFTL algorithm. The detailed\\nproof can be found in the Appendix. All through the theoretical\\nanalysis, we consider the loss function for face patch feature\\nxθat non-frontal pose θas\\n/lscript(A,U,x,θ)=/bardblAθUT(xθ−x0)/bardbl, (9)\\nwhose maximum value is assumed to be B.\\n1) Robustness Analysis: If two corresponding face patch\\nfeatures of two images are from the same subject, then their\\nassociated losses are close. This property is formalized as“robustness” in [46], and the precise deﬁnition is given below:\\nDeﬁnition 1: An algorithm Ais(K,/epsilon1(·))robust, for K ∈N\\nand/epsilon1(·):Z→ R,i ft h es a m p l e Zcan be partitioned into K\\ndisjoint sets, denoted as {C\\ni}K\\ni=1, so that the following holds\\nfor all s∈Z, given the loss function /lscript(As,z)of the algorithm\\nAstrained on s:\\n∀s∈s,∀z∈Z,∀i=1,···,K:\\nif s,z∈Ci,then|/lscript(As,s)−/lscript(As,z)|≤/epsilon1(s).\\nGiven two face patch features sand zof the same\\nsubject from different poses θsandθz,i f/bardbls−z/bardbl≤ γ\\nand|θs−θz|≤/Delta1θ, we suggest that these two face patch\\nfeatures are close. We assume that for one subject, the\\ndifference between any of its non-frontal face patch feature\\nxθand its frontal face patch feature x0can be bounded by\\n/bardblxθ−x0/bardbl≤γ0. Since matrix Aθat pose θis a sparse diagonal\\nmatrix that has dnon-zero elements, we have /bardblAθ/bardbl≤√\\nd.\\nAlso, we restrict /bardblAθ1−Aθ2/bardbl≤/Omega1/Delta1θfor any two matrices\\nAθ1andAθ2at different poses. A face recognition algorithm is\\nsaid to be robust if the corresponding face patch features from\\nimages of the same subject have close losses. This robustness\\ncan be measured by the following theorem.\\nTheorem 1: Example z is in space Z⊂RD,w h i c hc a nb e\\npartitioned into K disjoint sets and denoted as {Ci=1}K\\ni=1.\\nGiven the algorithm A{A,U:z→ Rd}, we have for\\nanys⊂Z,\\n|/lscript(As,z)−/lscript(As,s)|≤√\\ndγ+/Omega1/Delta1θγ0\\n∀i,j=1,···,K:s∈Ciand z ∈Cj.\\nHence Ais(K,√\\ndγ+/Omega1/Delta1θγ0)-robust.\\nRobustness is a fundamental property which ensures that\\na learning algorithm performs well. Since the sparse diag-\\nonal matrices Asand Az, which select projection vectors\\nfrom the transformation dictionary U, are learned from\\nface patches of different poses, they cannot be identical.According to Theorem 1, it is instructive to suggest that\\nthe robustness of the algorithm will be improved for the\\nface patches at close poses if their feature transformations\\n986 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nhave more shared elements, that is, encouraging /Omega1/Delta1θto be\\nsmall.\\n2) Generalization Analysis: Based on the robustness analy-\\nsis, we show a PAC generalization bound for the algorithm,\\ni.e., the difference between the expected error L(As)and\\nthe empirical error Lemp(As). We begin by presenting a\\nconcentration inequality [47] that helps to derive the bound.\\nProposition 1: Let (|N1|,···,|NK|)be an IID multinomial\\nrandom variable with parameters n and (β(C1),···,β(CK)).\\nBy the Breteganolle-Huber-Carol inequality we have\\nPr{/summationtextK\\ni=1|Ni\\nn−β(Ci)|≥ζ}≤ 2Kexp(−nζ2\\n2), hence with\\nprobability at least 1−δ,\\nK/summationdisplay\\ni=1|Ni\\nn−β(Ci)|≤/radicalbigg\\n2Kln 2+2l n(1/δ)\\nn.\\nThe generalization error bound is presented in the following\\ntheorem.\\nTheorem 2: If the algorithm Ais(K,/epsilon1(·))-robust and the\\ntraining sample sis composed of n examples {si}n\\ni=1,w h i c h\\nare generated from β, then for any δ>0, with the probability\\nat least 1−δwe have,\\n|L(As)−Lemp(As)|≤/epsilon1(s)+B/radicalbigg\\n2Kln 2+2l n(1/δ)\\nn.\\nBy combining the results of Theorem 1 and Theorem 2, we\\ncan easily illustrate the genera lization error of the proposed\\nalgorithm. Exploiting the shared information of face patches\\nfrom different poses can strengthen the robustness of thealgorithm and then improve the generalization error.\\nV. F\\nACE MATCHING WITHPBPR-MtFTL\\nIn this section, the face matching problem is addressed\\nbased on the proposed PBPR-MtFTL framework. It is assumed\\nthat/braceleftbig/parenleftbigUi,Ai/parenrightbig:1≤i≤MN/bracerightbig, i.e., the set of patch-wise\\ntransformation dictionaries an d selection matrices, has been\\nlearnt by MtFTL.\\nSuppose we are matching a probe face image xtof\\npose type tto a frontal gallery face image x0.I ti sa l s o\\nassumed that there are Kunoccluded patches for xt. Without\\nloss of generality, we denote the sets of features for the\\nKpatches as {xt1,xt2,···,xtK}and{x01,x02,···,x0K}for\\nxtandx0, respectively. First, the features of each patch pair\\n{(xtk,x0k):1≤k≤K}are projected into the discriminative\\nspace using the learnt Ukand Ak.\\nˆxtk=Ak\\nt(Uk)Txtk\\nˆx0k=Ak\\nt(Uk)Tx0k, (10)\\nwhere Ak\\ntis the diagonal matrix expanded by the tth column\\nofAk. Then, the cosine metric is utilized to calculate the\\nsimilarity of each patch pair and the similarity scores of all\\nKpatch pairs are fused by the sum rule.\\ns(xt,x0)=1\\nKK/summationdisplay\\nk=1ˆxT\\ntkˆx0k\\n/bardblˆxtk/bardbl/bardblˆx0k/bardbl, (11)\\nwhere sis the similarity score between the probe image xt\\nand the gallery image x0. Lastly, the nearest neighbor (NN)\\nclassiﬁer is adopted for face identiﬁcation.Although the adopted matching scheme is simple compared\\nto existing methods [17], [26], it is still expected that the\\nproposed PBPR-MtFTL framework will achieve stronger per-\\nformance, since the recognition ability of PBPR-MtFTL hasbeen enhanced by exploiting the correlation between poses.\\nVI. E\\nXPERIMENTAL EVA L UAT I O N\\nIn this section, extensive experiments are conduct to present\\nthe effectiveness of PBPR-MtFTL. We mainly conduct iden-\\ntiﬁcation experiments on the three most popular databases\\nfor the pose problem, i.e., CMU-PIE [48], FERET [49], andMulti-PIE [50]. These experiments are to recognize a subject\\nacross pose variations with a single enrolled frontal face\\nimage. At the end of this section, we slightly modify theproposed framework to deal with the unconstrained face veri-\\nﬁcation problem, and conduct experiments on the challenging\\nLFW dataset [9].\\nThe CMU-PIE [48] and FERET [49] datasets incorporate\\nmulti-pose images of 68 and 200 subjects, respectively. Forthe two databases, we adopt the same protocols as previous\\nworks [7], [18] that exclude both illumination and expression\\nvariations [51]. The Multi-PIE [50] database contains imagesof 337 subjects, each of which is captured in up to four\\nrecording sessions. Images in each session cover 15 view\\npoints and 20 illumination cond itions. As there is no uniﬁed\\nprotocol for the pose problem on Multi-PIE, we adopt the three\\nmost popular protocols in the literature [17], [22], [39].\\nEight sets of experiments are conducted. First, the per-\\nformance of PBPR-MtFTL is brieﬂy compared with previ-\\nous works for PIFR on CMU-PIE and FERET. Next, theMtFTL approach is compared with its single-task baselines\\non Multi-PIE to justify the signiﬁcance of MTL for the pose\\nproblem. Then, considering that the pose problem is often\\ncombined with other factors, we evaluate the performance\\nof PBPR-MtFTL in three different settings, i.e., combinedvariations of pose and illumination, combined variations of\\npose and recording session, and combined variations of pose,\\nillumination, and recording session. We also test the sensitivityof PBPR-MtFTL to the value of model parameters and face\\nalignment errors. Lastly, we slightly modify the proposed\\napproach to deal with the unconstrained face veriﬁcation prob-\\nlem and present experimental results on the LFW database.\\nAll images in this paper are normalized as follows. The\\nmean shape of the Basel Face Model (BFM) [52] is adopted\\nas the 3D generic shape model. The ﬁve facial feature points\\nare manually labeled in the ﬁrst six experiments and auto-matically detected in the last two experiments.\\n2After the\\npose normalization step described in Section III, the face\\nimages are cropped and resized to 156 ×130 pixels, as shown\\nin Fig. 6. The patch size M×Nis set at 26 ×24 pixels, with\\n50% overlap between nearby patches. The number of cells\\nJ×Jwithin each patch is set at 2 ×2. For the ﬁrst seven\\nexperiments, images are further photometrically normalized\\nusing a simple operator [1], with the two parameters σ1andσ2\\n2Coordinates of manually labeled facial feature points for Multi-PIE are\\nprovided by Zhu et al. [12]; we use an off-the-shelf tool [53] for automaticfacial feature point detection.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 987\\nTABLE I\\nMODEL PARAMETERS ESTIMATED ON THE VALIDA TION\\nSUBSETS FOR DIFFERENT DATABASES\\nset at 1.4 and 2.0. The two parameters RinandRexfor DCP are\\nset at[3,7]. For the last experiment, we omit the photometric\\nnormalization step since it slightly degrades the performance\\nof PBPR-MtFTL on the View 1 data of LFW. Three-scale DCP\\nfeatures are extracted and con catenated, with th e parameters\\nset at [2,4],[4,8],a n d[6,12], respectively.\\nFor each identiﬁcation experime nt, the subjects in the train-\\ning data are randomly divided into two subsets, one for model\\ntraining and the other for validation. The two subsets are of\\nequal size. The optimal values of model parameters μ,d,a n dλ\\nare estimated on the validation subset and applied to the\\ntest data. For simplicity, the value of μacross the models\\nof all patches keeps consistent and this applies to dandλ.\\nThe random division of the training data is repeated ﬁve\\ntimes, and the mean rank-1 identiﬁcation rates on the testdata are reported. The estimated model parameters for different\\ndatabases are tabulated in Table I, where the superscripts “ ∗\\n1”,\\n“∗2”, and “ ∗3” stand for one of the three protocols adopted\\nfor the Multi-PIE database, respectively.\\nA. Comparison on CMU-PIE and FERET\\nAll 68 CMU-PIE subjects with neutral expression and\\nnormal illumination at 11 different poses are employed. Note\\nthat pose type C31 and C25 are with hybrid yaw and pitchvariations. The 68 frontal images are utilized as gallery\\nimages and all the rest are used as probes. Following previous\\nworks [18], [57], we train the MtFTL model with randomlyselected 50 subjects in Multi-PIE database since there are\\nonly 68 subjects in CMU-PIE. For FERET, all 200 subjects\\nat 9 different poses are incorporated. Images of the ﬁrst\\n100 subjects consist the training data and the rest 100 subjects\\nare used for testing.\\nAs shown in Table II and III, the proposed PBPR-MtFTL\\napproach outperforms the other methods. But the advantage of\\nPBPR-MtFTL is not well exhib ited, because the performance\\nof existing methods has nearly reached the saturation point on\\nthe two databases. Therefore, we focus on the larger and more\\nchallenging Multi-PIE database in the following experiments.\\nB. Comparison With Single-Task Baselines\\nIn this experiment, we aim to justify the importance of\\nMTL for the pose problem. The proposed MtFTL algorithm is\\ncompared with three single-task baselines: (a) the Linear Dis-\\ncriminative Analysis (LDA) a pproach, which learns a singleLDA model for all poses; (b) the single-task feature trans-\\nformation learning (StFTL) approach, which learns a single\\nfeature transformation for all poses. StFTL is equal to the\\nDiscriminative Locality Alignment (DLA) model [58]; (c) themultiple independent feature transformation learning (MiFTL)\\napproach, which independen tly learns a DLA model for each\\npose. Unlike LDA and StFTL, MtFTL and MiFTL learn posespeciﬁc feature transformations. The main difference between\\nMtFTL and MiFTL is that MiFTL learns the transformation for\\neach pose independently, while MtFTL learns compact trans-\\nformations simultaneously and beneﬁts from the correlation of\\ndifferent poses.\\nThe protocol deﬁned in [17] is employed. This protocol\\ncovers 249 subjects in Session 1, in which images with neutral\\nexpression under 20 illumination conditions are involved. Theﬁrst 100 subjects (Subject ID 001 to 100) are used for training\\nand the remaining 149 subjects (Subject ID 101 to 250) are\\nused for testing. The gallery set is composed of 149 frontal\\nimages (Pose ID 051) with the illumination ID 07. The probe\\nsets cover 20 illumination conditions of the same subjects.In Fig. 7, we present the performance of the four algorithms\\nwith varied size of training data on the three most challenging\\nposes. The number of subjects (S) utilized for model learningis gradually increased from 20 to the maximum number 50.\\nIt is shown in Fig. 7 that MtFTL consistently outperforms\\nthe baselines under all settings. Speciﬁcally, MtFTL signiﬁ-\\ncantly outperforms StFTL and LDA, which means that learn-\\ning pose speciﬁc feature transformations is necessary. MtFTLoutperforms MiFTL while learni ng much more compact trans-\\nformations. This proves that MTL is helpful for enhancing\\nthe ability to recognize non-frontal faces. The advantage ofMtFTL is more evident when the amount of training data is\\nlimited, which indicates that knowledge sharing among related\\ntasks is important for better generalization ability.\\nC. Recognition Across Pose and Illumination\\nIn this subsection, the performance of the PBPR-MtFTL\\nframework is compared with existing algorithms under the set-\\nting of the combined variations of pose and illumination. The\\nadopted protocol is the same as the previous experiment [17].\\nThe experimental results are shown in Table IV and Fig. 8.In general, face recognition across combined variations of\\npose and illumination is a difﬁcult problem. However, it\\nis clear that the proposed method outperforms existingapproaches [12], [17] with a large margin, even though only\\nhalf training data is employed to train the MtFTL model.\\nIt is worth noting that the algorithm proposed in [17]\\nalso employs photometric normalization, and that all three\\napproaches employ manually labeled facial feature points.Notably, we employ exactly the same facial feature point\\ncoordinates as the method followed in [12].\\nD. Recognition Across Pose and Recording Session\\nThis experiment is to test the performance of algorithms\\nunder the combined variations of pose and recording session.\\nThe protocol described in [22] is followed. This protocol\\ncovers all 337 subjects across the four recording sessions.\\n988 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nTABLE II\\nPERFORMANCE COMPARISON WITHSTATE -OF-THE-ARTPIFR M ETHODS ON CMU-PIE\\nTABLE III\\nPERFORMANCE COMPARISON WITHSTATE -OF-THE-ARTPIFR M ETHODS ON FERET\\nFig. 7. Performance comparison of MtFTL and the three single-task baselines on the Multi-PIE database with varying numbers of training subjects.\\n(a) yaw =±90°; (b) yaw =±75°; (c) yaw =±60°.\\nTABLE IV\\nRANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE AND ILLUMINATION ON MULTI -PIE\\nOnly images with neutral expression and frontal illumina-\\ntion are employed. Images of the ﬁrst 200 subjects (SubjectID 001 to 200) are used for training, and images of the\\nremaining 137 subjects (Subject ID 201 to 346) are employed\\nfor testing. The frontal imag es from the earliest recording\\nsessions for the testing subjects are collected as the gallery\\nset (137 images in total). The non-frontal images of the\\ntesting subjects construct fourteen probe sets. The comparisons\\nbetween our approach and the state-of-the-art methods are\\npresented in Table V and Fig. 9. We observe that:\\n1) In general, the performance of all the algorithms is\\ngood when the pose value of the probe images is small.\\nWhile high performance is achieved by all methods onthe probe sets 130, 140, 050, and 041, our method\\nachieves perfect identiﬁcation rates on all four probesets.\\n2) There is a substantial drop in performance for existing\\nmethods on the probe sets 080 and 190, where the yawangles are ±45°. PBPR-MtFTL performs signiﬁcantly\\nbetter than the other methods on both probe sets,\\nindicating that it is more robust to large pose variations.\\n3) While most existing methods can only handle yaw\\nangle variations within [−45°,+45°], the proposed\\nmethod can tackle the full range of yaw angle variation.\\nFig. 9 shows that high performance is achieved even\\nwhen the yaw angle approaches ±75°.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 989\\nTABLE V\\nRANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE AND RECORDING SESSION ON MULTI -PIE\\nFig. 8. Performance comparison on combined variations of pose and illumi-\\nnation. The probe sets 081 and 191 are with hybrid yaw and pitch variations.\\nThe other probe sets contain only yaw variations from −90° to +90°.\\nFig. 9. Performance comparison of different methods on combined variations\\nof pose and recording session.\\nE. Recognition Across Pose, Illumination,\\nand Recording Session\\nTo examine the robustness of the proposed algorithm under\\nmore challenging conditions, a new protocol speciﬁed in [39]\\nis employed. This protocol extends the original protocolFig. 10. Performance comparison of different methods on combined\\nvariations of pose, illumination, and recording session.\\ndesigned in [22] by incorporating all 20 illumination types,\\nwhile the other settings remain the same. Therefore, the gallery\\nset is exactly the same as [22], while the number of probe\\nimages is 20 times more than th at in [22]. The performance\\nof the proposed method, compared with the state-of-the-art\\napproaches, is presented in Table VI and Fig. 10. All methodsin Table VI employ manually labeled facial feature points.\\nWe make the following observations:\\n1) PBPR-MtFTL signiﬁcantly outperforms the other three\\napproaches across all probe sets. This result isconsistent with those observed in the previous two\\nexperiments.\\n2) Among the four approaches, PBPR-MtFTL is the only\\none that can handle full range of pose variations, and\\nits performance degrades gracefully across wide pose\\nvariations including ±60°.\\nF . Parameter Evaluation for MtFTL\\nIn the above experiments, the optimal value of model\\nparameters μ,d,a n dλis estimated on the validation subsets.\\nIn this experiment, the impact of their value on the perfor-mance of MtFTL is investigated. The same protocol [17] as the\\nsecond experiment is followed. Also, the rank-1 identiﬁcation\\nrates on the three most challenging poses are reported.\\n990 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nTABLE VI\\nRANK -1 I DENTIFICATION RATES ON COMBINED VARIATIONS OF POSE,ILLUMINATION ,AND RECORDING SESSION ON MULTI -PIE\\nFig. 11. Inﬂuence of the parameters μ,d,a n dλto the performance of MtFTL. (a) evaluation against the value of μwhile dandλare set at 200 and 0.5,\\nrespectively; (b) evaluation against the value of dwhile μandλare set at 0.1 and 0.5, respectively; (c) evaluation against the value of λwhile μanddare\\nset at 0.1 and 200, respectively.\\nThe performance of the MtFTL approach for different value\\nofμ,d,a n d λis shown in Fig. 11. Their optimal value is\\naround 0.1, 200, and 0.5, respectively. The experimental results\\nalso indicate that the performance of MtFTL is robust to the\\nﬂuctuation of parameter value.\\nG. Performance in the Fully-Automatic Mode\\nThe performance of the presented PBPR-MtFTL framework\\nis related to the accuracy of the facial feature detection and\\npose estimation algorithms. The previous experiments aresemi-automatic (SA), i.e., the facial feature points are labeled\\nmanually and it is assumed that the probe image poses are\\nknown. In this experiment, the PBPR-MtFTL framework isrun in the fully-automatic (FA) mode. We leave the manu-\\nally labeled facial feature points for the gallery and training\\nimages intact. This is reasonabl e since the labeling work could\\nbe conducted ofﬂine. For all probe images, the ﬁve facial\\nfeature points are automatica lly detected. Since existing face\\nalignment tools cannot reliably detect facial feature points for\\nproﬁle or half-proﬁle faces, we limit the yaw range of the\\nprobe images to within ±45° in this experiment. For pose\\nestimation, we compare the unoccluded region of each probe\\nimage with those of a set of training images whose poses are\\nknown. The pose of the probe image is assigned to that of thetraining image whose unoccluded region is the most similar.\\nThe same protocol [17] as used in the second experiment\\nis adopted. The performance of PBPR-MtFTL in the SA and\\nFA modes is compared in Fig. 12. There is a minor drop in\\nperformance under the FA mode. In fact, the performance dropis mainly caused by the failure of face detection, whose failure\\nrates on the six probe sets are 3.66%, 1.95%, 1.21%, 1.51%,\\n1.98%, and 3.72%, respectively. Besides, when consideredFig. 12. Performance comparison of the proposed PBPR-MtFTL framework\\nin the SA and FA modes. In the FA mode, both facial feature point detection\\nand pose estimation are completely au tomatic. Note that the identiﬁcation\\nerror in the FA mode incorporates the failure in face detection.\\nalong with the results shown in Fig. 8, the performance of\\nPBPR-MtFTL in the FA mode is still considerably better thanthe state-of-the-art methods.\\nH. Extension to Unconstrained Face Veriﬁcation\\nIn this experiment, we slightly modify the proposed\\napproach to tackle the unconstrained face veriﬁcationproblem, and present experimental results on the LFW\\ndatabase [9].\\nIn Section IV , we assume that the tth task of MtFTL\\nis to learn the feature transformation between the tth non-\\nfrontal pose type and the frontal pose. As shown in Fig. 13,image pairs deﬁned in LFW m ay contain no frontal pose\\nimage. Therefore, we add tasks in the model that learn the\\nfeature transformation between every possible pair of poses.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 991\\nFig. 13. Many image pairs deﬁned in LFW contain no frontal faces. The ﬁrst\\nline shows the ﬁrst images in the image pairs, while the second line showsthe second images in the image pairs.\\nTABLE VII\\nP\\nERFORMANCE COMPARISON ON LFW W ITHSTATE -OF-THE-ART\\nMETHODS BASED ON SINGLE FACE REPRESENTATION\\nAnother characteristic of LFW is that it has very few proﬁle\\nface images. To make sure that each pose type incorporates\\nsufﬁcient training data, we quantize the pose space into three\\ntypes, i.e., left proﬁle (LP, yaw< −10°), frontal pose\\n(FP, −10° ≤ yaw≤+ 10°), and right proﬁle\\n(RP, yaw> +10°). Therefore, there are six tasks in total,\\ni.e., LP-LP, LP-FP, LP-RP, FP-FP, FP-RP, and RP-RP. Besides,\\nthe weight of each task in Eq. 3 is modiﬁed to be proportionalto the number of training sam ples in each task, since the\\ntraining samples are far from balanced among the tasks.\\nFor each pair of faces in LFW, both images are normalized\\nas described in Section III. Occlusion detection is conducted\\nfor non-frontal face images. Feat ures are extracted only from\\nthe patches that are un-occluded in both images. The features\\nare transformed with the learnt MtFTL model, as described\\nin Section V . Similarity scores between all un-occluded patchpairs are averaged as the similarity score of the face image\\npair. Since MtFTL explicitly employs the image labels, the\\nproposed method falls in the paradigm of “Unrestricted, Label-Free Outside Data” [9]. We conduct performance comparison\\nwith the state-of-the-arts in Table VII. To promote perfor-\\nmance, most existing methods designed for the LFW challenge\\nfuse multiple face representations, e.g., employing several\\ndescriptors [60] or mirroring the face image [61]. In compar-ison, this work is not targeted at the LFW challenge and we\\nemploy only a single face representation. For fair comparison,\\nwe report the best performance of all approaches in Table VIIachieved with a single face representation.\\n3\\n3The performance of [61] is obtained using the code and data released by\\nthe authors, while the performance of the other approaches is directly citedfrom the original papers.The ﬁrst ﬁve approaches in Table VII adopt metric learn-\\ning based classiﬁers, and PBPR-MtFTL achieves signiﬁcantly\\nbetter performance than the oth er approaches. Recently, gen-\\nerative model based classiﬁers have been introduced to theLFW challenge. We then replace the MtFTL model with\\nthe Probabilistic Linear Discriminative Model (PLDA) [66].\\nThe dimension of the PLDA subspace is set at 100.With generative model based classiﬁers, the high-dim LBP\\napproach [59] achieves a slightly higher accuracy than our\\napproach. However, this approach relies on dense facial feature\\ndetection. We emphasize here that only the 5 most stable facial\\nfeature points are required by our method. This makes ouralgorithm easier to use in practical applications.\\nVII. C\\nONCLUSION\\nFace recognition across pose is a challenging task because\\nof the signiﬁcant appearance ch ange caused by pose variations.\\nWe handle this problem from two aspects. First, we proposethe PBPR face representation scheme that makes use of\\nthe unoccluded face textures only. PBPR can be applied to\\nface images in arbitrary pos e, which is a great advantage\\nover existing methods. Second, we present the MtFTL model\\nfor learning compact feature transformations by utilizing thecorrelation between poses. Clear advantage is shown compared\\nto single-task based methods. To the best of our knowledge,\\nthis is the ﬁrst time that MTL has been formally applied tothe PIFR problem. As the proposed PBPR-MtFTL framework\\neffectively utilizes all the uno ccluded face texture and the\\ncorrelation between different poses, very encouraging results\\nfor face identiﬁcation in all th ree popular multi-pose databases\\nare achieved. We also slightly modify the proposed approach totackle the unconstrained face ve riﬁcation problem, and achieve\\ntop level performance on the challenging LFW database.\\nA\\nPPENDIX A\\nPROOF OF THEOREM 1\\nProof: We can partition Zinto Kdisjoint sets, so that if\\ntwo face patch features sandzare close, then\\n/bardbls−z/bardbl≤γand |θs−θz|≤/Delta1θ. (12)\\nBy arranging the loss functions so that the ﬁrst loss is always\\nlarger than the second one, we therefore have\\n|/lscript(As,U,s)−/lscript(Az,U,z)|\\n=/bardblAsUT(s−x0)/bardbl−/bardbl AzUT(z−x0)/bardbl\\n=/bardblAsUT(s+z−z−x0)/bardbl−/bardbl AzUT(z−x0)/bardbl\\n≤/bardblAsUT(s−z)/bardbl+/bardbl AsUT(z−x0)/bardbl−/bardbl AzUT(z−x0)/bardbl\\n≤/bardblAsUT(s−z)/bardbl+/bardbl(As−Az+Az)UT(z−x0)/bardbl\\n−/bardblAzUT(z−x0)/bardbl\\n≤/bardblAsUT(s−z)/bardbl+/bardbl(As−Az)UT(z−x0)/bardbl\\n≤/bardblAs/bardbl/bardbls−z/bardbl+/bardbl As−Az/bardbl/bardblz−x0/bardbl\\n≤√\\ndγ+/Omega1/Delta1θγ0,\\nwhich completes the proof.\\n\\n992 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 24, NO. 3, MARCH 2015\\nAPPENDIX B\\nPROOF OF THEOREM 2\\nProof: Let Nibe the set of index of points of sthat\\nfall into Ci.(|N1|,···,|NK|)is an IID random variable with\\nparameters nand(β(C1),···,β(CK)).W eh a v e\\n/vextendsingle/vextendsingleL(As)−Lemp(As)/vextendsingle/vextendsingle\\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay\\ni=1Ez∼β(/lscript(As,z)|z∈Ci)β(Ci)−1\\nnn/summationdisplay\\ni=1/lscript(As,si)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\\n≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay\\ni=1Ez∼β(/lscript(As,z)|z∈Ci)Nj\\nn−1\\nnn/summationdisplay\\ni=1/lscript(As,si)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\\n+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleK/summationdisplay\\ni=1Ez∼β(/lscript(As,z)|z∈Ci)β(Ci)\\n−K/summationdisplay\\ni=1Ez∼β(/lscript(As,z)|z∈Ci)Nj\\nn/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\\n≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1\\nnK/summationdisplay\\ni=1/summationdisplay\\nj∈Nimax\\nz∈Ci|/lscript(As,sj)−/lscript(As,z)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\\n+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglemax\\nz∈Z|/lscript(As,z)|K/summationdisplay\\ni=1||Ni|\\nn−β(Ci)|/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\\n≤/epsilon1(s)+BK/summationdisplay\\ni=1|Ni\\nn−β(Ci)|\\n≤/epsilon1(s)+B/radicalbigg\\n2Kln 2+2l n(1/δ)\\nn.\\nThe ﬁrst inequality is due to the triangle inequality, and\\nthe second inequality is because of/summationtextK\\ni=1β(Ci)=1a n d/summationtextK\\ni=1Nj\\nn=1. Finally, the last inequality is the application\\nof Proposition 1.\\nACKNOWLEDGMENT\\nThe authors would like to thank the associate editor\\nProf. Shiguang Shan and the three anonymous reviewers\\nfor their careful reading and va luable remarks, which have\\ncontributed to improving the quality of the paper. The authorswould also like to thank Prof. Thomas Vetter in the Department\\nof Computer Science, the University of Basel to provide the\\nBFM model.\\nR\\nEFERENCES\\n[1] X. Tan and B. Triggs, “Enhanced local texture feature sets for face\\nrecognition under difﬁcult lighting conditions,” IEEE Trans. Image\\nProcess. , vol. 19, no. 6, pp. 1635–1650, Jun. 2010.\\n[2] C. Ding, J. Choi, D. Tao, and L. S. Davis. (2014). “Multi-directional\\nmulti-level dual-cross patterns for robust face recognition.” [Online].\\nAvailable: http://arxiv.org/abs/1401.5311\\n[3] M. Gunther et al. , “The 2013 face recognition evaluation in mobile\\nenvironment,” in Proc. IEEE/IAPR Int. Conf. Biometrics , Jun. 2013,\\npp. 1–7.\\n[4] R. Abiantun, U. Prabhu, and M. Savvi des, “Sparse feature extraction for\\npose-tolerant face recognition,” IEEE Trans. Pattern Anal. Mach. Intell. ,\\nvol. 36, no. 10, pp. 2061–2073, Oct. 2014.\\n[5] V . Blanz and T. Vetter, “Face recognition based on ﬁtting a 3D\\nmorphable model,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,\\nno. 9, pp. 1063–1074, Sep. 2003.\\n[6] S. J. D. Prince, J. Warrell, J. H. Elder, and F. M. Felisberti, “Tied factor\\nanalysis for face recognition across large pose differences,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 30, no. 6, pp. 970–984, Jun. 2008.[7] S. R. Arashloo and J. Kittler, “Energy normalization for pose-invariant\\nface recognition based on MRF model image matching,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 33, no. 6, pp. 1274–1280, Jun. 2011.\\n[8] H. T. Ho and R. Chellappa, “Pose-invariant face recognition using\\nMarkov random ﬁelds,” IEEE Trans. Image Process. , vol. 22, no. 4,\\npp. 1573–1584, Apr. 2013.\\n[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller,\\n“Labeled faces in the wild: A database for studying face\\nrecognition in unconstrained environments,” Dept. Comput. Sci.,Univ. Massachusetts Amherst, Am herst, MA, USA, Tech. Rep. 07-49,\\nOct. 2007.\\n[10] V . Blanz, P. Grother, P. J. Phillips, and T. Vetter, “Face recognition based\\non frontal views generated from non-frontal images,” in Proc. IEEE\\nComput. Soc. Conf. Comput. Vis. Pattern Recognit. , vol. 2. Jun. 2005,\\npp. 454–461.\\n[11] X. Chai, S. Shan, X. Chen, and W. Gao, “Locally linear regression for\\npose-invariant face recognition,” IEEE Trans. Image Process. , vol. 16,\\nno. 7, pp. 1716–1725, Jul. 2007.\\n[12] Z. Zhu, P. Luo, X. Wang, and X. Tang, “Deep learning identity-\\npreserving face space,” in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2013,\\npp. 113–120.\\n[13] S. Liao, A. K. Jain, and S. Z. Li, “Partial face recognition: Alignment-\\nfree approach,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 35, no. 5,\\npp. 1193–1205, May 2013.\\n[14] R. Weng, J. Lu, J. Hu, G. Yang, and Y .-P. Tan, “Robust feature set\\nmatching for partial face recognition,” in Proc. IEEE Int. Conf. Comput.\\nVis., Dec. 2013, pp. 601–608.\\n[15] X. Zhang and Y . Gao, “Face recognition across pose: A review,” Pattern\\nRecognit. , vol. 42, no. 11, pp. 2876–2896, 2009.\\n[16] A. B. Ashraf, S. Lucey, and T. Ch en, “Learning patch correspondences\\nfor improved viewpoint invariant face recognition,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2008, pp. 1–8.\\n[17] A. Li, S. Shan, and W. Gao, “Coupled bias–variance tradeoff for cross-\\npose face recognition,” IEEE Trans. Image Process. , vol. 21, no. 1,\\npp. 305–315, Jan. 2012.\\n[18] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan, “Morphable\\ndisplacement ﬁeld based image mat ching for face recognition across\\npose,” in Proc. 12th Eur. Conf. Comput. Vis. , 2012, pp. 102–115.\\n[19] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “A comprehensive survey\\nto face hallucination,” Int. J. Comput. Vis. , vol. 106, no. 1, pp. 9–30,\\n2014.\\n[20] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, “Transductive face sketch-\\nphoto synthesis,” IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 9,\\npp. 1364–1376, Sep. 2013.\\n[21] U. Prabhu, J. Heo, and M. Savvides, “Unconstrained pose-invariant face\\nrecognition using 3D generic elastic models,” IEEE Trans. Pattern Anal.\\nMach. Intell. , vol. 33, no. 10, pp. 1952–1961, Oct. 2011.\\n[22] A. Asthana, T. K. Marks, M. J. Jones, K. H. Tieu, and M. Rohith, “Fully\\nautomatic pose-invariant face recognition via 3D pose normalization,”\\ninProc. IEEE Int. Conf. Comput. Vis. , Nov. 2011, pp. 937–944.\\n[23] Y . Zhang, M. Shao, E. K. Wong, and Y . Fu, “Random faces guided sparse\\nmany-to-one encoder for pose-invariant face recognition,” in Proc. IEEE\\nInt. Conf. Comput. Vis. , Dec. 2013, pp. 2416–2423.\\n[24] J. Wright and G. Hua, “Implicit elastic matching with random projec-\\ntions for pose-variant face recognition,” in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. , Jun. 2009, pp. 1502–1509.\\n[25] D. Yi, Z. Lei, and S. Z. Li, “Towards pose robust face recogni-\\ntion,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2013,\\npp. 3539–3545.\\n[26] A. Li, S. Shan, X. Chen, and W. Ga o, “Maximizing intra-individual\\ncorrelations for face recognition across pose differences,” in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 605–611.\\n[27] A. Sharma and D. W. Jacobs, “Bypassing synthesis: PLS for face\\nrecognition with pose, low-resolution and sketch,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2011, pp. 593–600.\\n[28] A. Sharma, A. Kumar, H. Daume, and D. W. Jacobs, “Generalized\\nmultiview analysis: A discriminative latent space,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2012, pp. 2160–2167.\\n[29] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen, “Multi-view\\ndiscriminant analysis,” in Proc. 12th Eur. Conf. Comput. Vis. , 2012,\\npp. 808–821.\\n[30] L. Zhang, X. Zhen, and L. Shao, “Learning object-to-class kernels\\nfor scene classiﬁcation,” IEEE Trans. Image Process. , vol. 23, no. 8,\\npp. 3241–3253, Aug. 2014.\\n[31] R. Yan, L. Shao, and Y . Liu, “Nonlo cal hierarchical dictionary learning\\nusing wavelets for image denoising,” IEEE Trans. Image Process. ,\\nvol. 22, no. 12, pp. 4689–4698, Dec. 2013.\\nDING et al. : MULTI-TASK POSE-INV ARIANT FACE RECOGNITION 993\\n[32] M. Kan, S. Shan, H. Chang, and X. Chen, “Stacked progressive auto-\\nencoders (SPAE) for face recognition across poses,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2014, pp. 1883–1890.\\n[33] Z. Hong, X. Mei, D. Prokhorov, and D. Tao, “Tracking via robust multi-\\ntask multi-view joint sparse representation,” in Proc. IEEE Int. Conf.\\nComput. Vis. , Dec. 2013, pp. 649–656.\\n[34] B. Mahasseni and S. Todorovic, “Latent multitask learning for view-\\ninvariant action recognition,” in Proc. IEEE Int. Conf. Comput. Vis. ,\\nDec. 2013, pp. 3128–3135.\\n[35] L. Liu, L. Shao, and P. Rockett, “Boosted key-frame selection and\\ncorrelated pyramidal motion-feature representation for human actionrecognition,” Pattern Recognit. , vol. 46, no. 7, pp. 1810–1818, 2013.\\n[36] F. Zhu and L. Shao, “Weakly-supervised cross-domain dictionary learn-\\ning for visual recognition,” Int. J. Comput. Vis. , vol. 109, nos. 1–2,\\npp. 42–59, 2014.\\n[37] D. Masip, Á. Lapedriza, and J. Vitrià, “Multitask learning: An applica-\\ntion to incremental face recognition,” in Proc. Int. Conf. Comput. Vis.\\nAppl. , 2008, pp. 585–590.\\n[38] D. Masip, Á. Lapedriza, and J. Vitrià, “Multitask learning applied to face\\nrecognition,” in Proc. 1st Spanish Workshop Biometrics , 2007, pp. 1–8.\\n[39] Z. Zhu, P. Luo, X. Wang, and X. Tang, “Multi-view perceptron: A deep\\nmodel for learning face identity and view representations,” in Advances\\nin Neural Information Processing Systems 27 . Red Hook, NY , USA:\\nCurran Associates, 2014, pp. 217–225.\\n[40] Z.-L. Sun, K.-M. Lam, and Q.-W. Gao, “Depth estimation of face images\\nusing the nonlinear least-squares model,” IEEE Trans. Image Process. ,\\nvol. 22, no. 1, pp. 17–30, Jan. 2013.\\n[41] A. M. Bruckstein, R. J. Holt, T. S. Huang, and A. N. Netravali,\\n“Optimum ﬁducials under weak perspective projection,” Int. J. Comput.\\nVis., vol. 35, no. 3, pp. 223–244, 1999.\\n[42] J. Canny, “A computationa l approach to edge detection,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. PAMI-8, no. 6, pp. 679–698, Nov. 1986.\\n[43] A. Myronenko and X. Song, “Point set r egistration: Coherent point drift,”\\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 12, pp. 2262–2275,\\nDec. 2010.\\n[44] L. Ma, X. Yang, and D. Tao, “Person re-identiﬁcation over camera\\nnetworks using multi-task distance metric learning,” IEEE Trans. Image\\nProcess. , vol. 23, no. 8, pp. 3656–3670, Aug. 2014.\\n[45] M. Schmidt. The minFunc Package . [Online]. Available: http://www.cs.\\nubc.ca/schmidtm/Software/minFunc.html, accessed Jul. 2014.\\n[46] H. Xu and S. Mannor, “Robustness and generalization,” Mach. Learn. ,\\nvol. 86, no. 3, pp. 391–423, 2012.\\n[47] A. W. van der Vaart and J. A. Wellner, Weak Convergence .N e wY o r k ,\\nNY , USA: Springer-Verlag, 1996.\\n[48] T. Sim, S. Baker, and M. Bsat, “The CMU pose, illumination, and\\nexpression database,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 25,\\nno. 12, pp. 1615–1618, Dec. 2003.\\n[49] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET\\nevaluation methodology for face-recognition algorithms,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 22, no. 10, pp. 1090–1104, Oct. 2000.\\n[50] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-PIE,”\\nImage Vis. Comput. , vol. 28, no. 5, pp. 807–813, 2010.\\n[51] L. Liu, L. Shao, and X. Li, “Evolutionary compact embedding for large-\\nscale image classiﬁcation,” Inf. Sci. , pp. 1–15, Jul. 2014.\\n[52] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter, “A 3D\\nface model for pose and illumination invariant face recognition,” in Proc.\\n6th IEEE Int. Conf. AVSS , Sep. 2009, pp. 296–301.\\n[53] Y . Sun, X. Wang, and X. Tang, “Deep convolutional network cascade\\nfor facial point detection,” in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. , Jun. 2013, pp. 3476–3483.\\n[54] S. R. Arashloo, J. Kittler, and W. J. Christmas, “Pose-invariant face\\nrecognition by matching on multi-resolution MRFs linked by super-\\ncoupling transform,” Comput. Vis. Image Understand. , vol. 115, no. 7,\\npp. 1073–1083, 2011.\\n[55] S. R. Arashloo and J. Kittler, “Fast pose invariant face recognition using\\nsuper coupled multiresolution Markov random ﬁelds on a GPU,” Pattern\\nRecognit. Lett. , vol. 48, pp. 49–59, Oct. 2014.\\n[56] S. R. Arashloo and J. Kittler, “Efﬁcient processing of MRFs for\\nunconstrained-pose face recognition,” in Proc. IEEE 6th Int. Conf.\\nBiometrics, Theory, Appl., Syst. , Sep./Oct. 2013, pp. 1–8.\\n[57] S. Li, X. Liu, X. Chai, H. Zhang, S. Lao, and S. Shan, “Maximal\\nlikelihood correspondence estimation for face recognition across pose,”IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4587–4600, Oct. 2014.\\n[58] T. Zhang, D. Tao, X. Li, and J. Yang, “Patch alignment for dimen-\\nsionality reduction,” IEEE Trans. Knowl. Data Eng. , vol. 21, no. 9,\\npp. 1299–1313, Sep. 2009.[59] D. Chen, X. Cao, F. Wen, and J. Sun, “Blessing of dimensionality:\\nHigh-dimensional feature and its ef ﬁcient compression for face veriﬁ-\\ncation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2013,\\npp. 3025–3032.\\n[60] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang, “Probabilistic elastic\\nmatching for pose variant face veriﬁcation,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2013, pp. 3499–3506.\\n[61] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Fisher\\nvector faces in the wild,” in Proc. Brit. Mach. Vis. Conf. , 2013, vol. 1,\\nno. 2, pp. 1–12.\\n[62] Y . Taigman, L. Wolf, and T. Hassner, “Multiple one-shots for utilizing\\nclass label information,” in Proc. Brit. Mach. Vis. Conf. , 2009, pp. 1–12.\\n[63] Q. Cao, Y . Ying, and P. Li, “Similar ity metric learning for face recogni-\\ntion,” in Proc. IEEE Int. Conf. Comput. Vis. , Dec. 2013, pp. 2408–2415.\\n[64] O. Barkan, J. Weill, L. Wolf, and H. Aronowitz, “Fast high dimensional\\nvector multiplication face recognition,” in Proc. IEEE Int. Conf. Comput.\\nVis., Dec. 2013, pp. 1960–1967.\\n[65] P. Li, Y . Fu, U. Mohammed, J. H. Elder, and S. J. D. Prince, “Proba-\\nbilistic models for inference about identity,” IEEE Trans. Pattern Anal.\\nMach. Intell. , vol. 34, no. 1, pp. 144–157, Jan. 2012.\\n[66] S. J. D. Prince and J. H. Elder, “Probabilistic linear discriminant analysis\\nfor inferences about identity,” in Proc. IEEE 11th Int. Conf. Comput. Vis. ,\\nOct. 2007, pp. 1–8.\\nChangxing Ding received the B.E. degree in\\nautomation from the Shandong University ofScience and Technology, Qingdao, China, and the\\nM.E. degree in control science and engineering from\\nthe Harbin Institute of Technology, Harbin, China,in 2009 and 2011, respectively. He is currently pur-\\nsuing the Ph.D. degree with the Centre for Quantum\\nComputation and Intelligent Systems and the Fac-ulty of Engineering and Information Technology,University of Technology at Sydney, Sydney, NSW,\\nAustralia. His research interests include computer\\nvision, machine learning, and in particular, on face recognition.\\nChang Xu received the B.E. degree from Tianjin\\nUniversity, Tianjin, China, in 2011. He is currentlypursuing the Ph.D. degree with the Key Labora-\\ntory of Machine Perception (Ministry of Educa-\\ntion), Peking University, Beijing, China. He was aResearch Intern with the Knowledge Mining Group,\\nMicrosoft Research Asia, Beijing, and a Research\\nAssistant with the Center of Quantum Computa-tion and Intelligent Systems and the Faculty ofEngineering and Information Technology, University\\nof Technology at Sydney, Sydney, NSW, Australia.\\nHe received the best student paper award in ACM ICIMCS 2013. His researchinterests lie primarily in machine learning, multimedia search, and computer\\nvision.\\nDacheng Tao (F’15) iis currently a Professor of\\nComputer Science with the Centre for QuantumComputation and Intelligent Systems and the Faculty\\nof Engineering and Information Technology, Univer-\\nsity of Technology at Sydney, Sydney, NSW, Aus-tralia. He mainly applies statistics and mathematicsto data analytics, and his research interests spread\\nacross computer vision, data science, image process-\\ning, machine learning, neural networks, and videosurveillance. His research results have expounded\\nin 1 monograph and over 100 publications at pres-\\ntigious journals and prominent conferences, such as the IEEE T\\nRANSAC -\\nTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , the IEEE\\nTRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE ,\\nthe IEEE T RANSACTIONS ON IMAGE PROCESSING ,t h e Journal of Machine\\nLearning Research ,t h e International Journal of Computer Vision ,t h eW o r k -\\nshop on Neural Information Processing Systems, the International Conference\\non Machine Learning, the Conference on Computer Vision and Pattern\\nRecognition, the International Confer ence on Computer Vision, the European\\nConference on Computer Vision, the I nternational Conference on Artiﬁcial\\nIntelligence and Statistics, and the International Conference on Data Mining,\\nand the ACM Conference on Knowledge Discovery and Data Mining, with\\nseveral best paper awards, such as the Best Theory/Algorithm Paper Runner-\\nUp Award in the IEEE ICDM’07, the best student paper award in the IEEE\\nICDM’13, and the 2014 ICDM 10-Year Highest Paper Award.\\n',\n",
       " '978-1-4244-2154-1/08/$25.00 ©2008 IE  \\nAbstract \\n \\nThe ability to handle multi-view  facial expressions is \\nimportant for computers to understand affective behavior under less constrained envir onment. However, most of \\nexisting methods for facial  expression recognition are \\nbased on the near-frontal view face data, which are likely to fail in the non-frontal facial expression analysis. In this paper, we conduct an investiga tion on analyzing multi-view \\nfacial expressions. Three local patch descriptors (HoG, LBP, and SIFT) are used to extract facial features, which are the inputs to a nearest-neighbor indexing method that identifies facial expressions. We also investigate the influence of feature dimension reductions (PCA, LDA, and LPP) and classifier fusion on the recognition performance. We test our approaches on multi- view data generated from \\nBU-3DFE 3D facial expression database that includes 100 subjects with 6 emotions and 4 intensity levels. Our extensive person-independent experiments suggest that the SIFT descriptor outperforms HoG and LBP, and LPP outperforms PCA and LDA in this application. But the classifier fusion does not show a significant advantage over SIFT-only classifier.  \\n1. Introduction \\nAnalysis of human facial expressions has attracted \\nincreasing attention from res earchers, because facial \\nexpression plays an important role in human social life [3], [15]. Automated human facial expression can benefit multiple research fields. On the one hand, it can provide researchers in social scie nce (psychology, psychiatry, \\neducation, etc.) a powerful inst rument to analyze data of \\nhuman affective behavior. On the other hand, it can contribute to the paradigm of human-centered human computer interfaces that shoul d be proactive and able to \\nunderstand human behavior.  \\nThe research of machine understanding of facial \\nexpressions has witnessed significant progress in the past years [15]. Automatic detection of the six basic emotions in posed and controlled displays can be done with reasonably high accuracy. However, detec ting these expressions or any expression of human affective behavior in less \\nconstrained settings is still a very challenging problem. Specifically, most of the exis ting efforts toward facial \\nexpression recognition focus on the images/sequences of near-frontal-view faces, which are not able to handle the arbitrary head movement of spontaneous affective behavior in a realistic interactive setting.  One main reason behind this fact is that the existing and frequently used databases (e.g., Cohn-Kanade database [6]) in facial expression research community typically  captured the frontal-view \\nfacial displays.  \\nA recent database named BU-3DFE database collected \\nby Yin et al. [14] represents the initial efforts toward building readily accessible da tabase of 3D facial \\nexpressions. Access to this database motivates us to explore some interesting and open issues about multi-view \\nfacial expressions, which have been precluded in the previous studies that focused on frontal-view expressions. \\nIn this paper, we focus on the investigation of multi-view \\nfacial expression recognition. We develop methods to recognize facial expressions under multiple view angles. \\nWe apply three local patch descriptor (Local Binary Pattern (LBP) [9], Histograms of Oriented Gradients (HoG) [4]  and Scale Invariant Feature Transform (SIFT) [8]) to \\ncharacterize facial expressions , and also investigate the \\ninfluence of feature dimension reductions (PCA, LDA, LPP [5]) and classifier fusion on the recognition performance. We test our approaches on the BU-3DFE data of 100 subjects with 6 emotions and 4 intensity levels, and 5 yaw rotation view angles. Our extensive person-independent experiment suggests that the SIFT descriptor performs better than HoG and LBP, LPP performs better than PCA and LDA in this application. The contributions of this paper include: 1) we explored the automatic multi-view facial expression recognition, to \\nwhich very few efforts are found to date; 2) we applied the advanced techniques of feature extraction (HoG, LBP, SIFT) for this application; 3) we investigated the classifier fusion in order to improve the performance. \\n2. Related work \\nResearch on machine understanding of facial \\nexpressions has attracted much attention from researchers in multiple disciplines, including computer science,  \\nMulti-View Facial Expression Recognition \\n \\n1Yuxiao Hu, 1Zhihong Zeng, 2Lijun Yin, 2Xiaozhou Wei, 1Xi Zhou and 1Thomas S. Huang \\n   1University of Illinois at Urbana-Champaign           2State University of New York at Binghamton \\n    Urbana, IL 61801, USA                                            Binghamton, NY 13902, USA \\n  {hu3,zhzeng,xizhou2,huang}@ifp.uiuc.edu       {lijun,xwei}@cs.binghamton.edu  \\n \\nlinguistics, psychology, psychiatry, education, \\nneuroscience, etc. Although most of the existing methods are based on deliberately displayed facial expressions [15], an increasing number of studies have been reported to recognize spontaneous facial expressions [1], [11], [12]. This trend of the field inspires us to explore some challenging and largely unexplored area, including the research of multi-view facial expression analysis.  \\nMost of existing efforts in this field, including studies \\nboth on deliberately displayed facial expressions and on spontaneous facial expressions, has been focused on recognition of facial expre ssions in near-frontal-view \\nrecordings. The human behavior in less constraint environment, e.g., arbitrary head movement which results in non-front-view facial expressions, challenges these existing methods.  \\nAn exemplar exception is the study of Pantic and Patras \\n[10], who explored automatic analysis of facial expressions from the profile view of the f ace. Recently, Yin et al. [14], \\nWang et al. [13] and Chang et al. [2] used 3D expression data for facial expression recognition. In particular, the study of [13] analyzed the influence of view change on recognition performance of facial expression, based on the classifier trained on frontal-view faces. As shown by the \\nexperimental results in [13], the classifier performed poorly to recognize facial expression undergoing a large view variation.  \\nCompared with the previous studies mentioned above, \\nthis paper has the following significant progresses: Firstly, we used the data of 100 subjects, which is much larger than the amount used in the previous studies; Secondly, we used images of all four levels of  intensity (from low to high) \\nrather than two levels in [13] for the experiment; Thirdly, we developed algorithms for the performance study with a wider range of viewing angles; Finally, we applied three advanced feature extraction methods (HoG, LBP, and SIFT) and investigated the combinati on of these classifiers based \\non different features. \\n3. BU-3DFE data and preprocessing \\nAlthough there have been some 2D face expression \\ndatabases accessible for facial  expression research, the \\nefforts toward building readily  accessible database for 3D \\nexpression analysis has just emerged [15]. To our best knowledge, the BU-3DFE database [13] is the only publicly available emotion database that contains 3D range data of six prototypical facial expressions. \\nIn BU_3DFE 3D facial expression database, there are \\n100 subjects who participated in face scans, including undergraduates, graduates and faculties from State University of New York at Binghamton. The resulting database consists of about  60% female and 40% male \\nsubjects with a variety of ethnic/racial ancestries.  \\nEach subject in the database performed seven expressions, captured by a 3D face scanner. With the \\nexception of the neutral expression, each of the six prototypic expressions ( happiness, disgust, fear, anger, \\nsurprise, and sadness ) includes four levels of intensity. \\nFigure 1 shows some samples in the BU-3DFE database. \\n \\n \\nFigure 1. Samples of expressi ons in the BU-3DFE database \\n \\nIn our study, we generate 64*64 multi-view images of \\nfacial expressions from the av ailable 3D data in BU-3DFE \\ndatabase. The data in our experiment includes 100 subjects, 6 emotions (excluding neutral) with four levels of intensities, and 5 yaw angles (0, 30, 45, 60, and 90 degrees), illustrated in Figure 2.  \\n \\n \\n \\nFigure 2: The examples of multi-view facial expressions \\n4. Features \\nMost of the existing facial expression recognizers \\nemploy various pattern recognition approaches, which are primarily based on 2D facial features [15]. The commonly used facial features are either geometric features such as the \\nshapes of the facial components (eyes, mouth, etc.) [10], [11] or appearance features re presenting the facial texture \\n(e.g., Gabor wavelets [1][13]). Wang et al. [13] proposed to use 3D geometric curvature features to recognize the 3D \\nAngry \\nDisgust \\nFear  Sad  \\nSurprise Happy \\n \\nfacial expressions. \\nRecently, many features ha ve been proposed in the \\ncomputer vision field. In particular, Local Binary Pattern (LBP) [9], Histograms of Oriented Gradients (HoG) [4]  and Scale Invariant Feature Transform (SIFT) [8] have  been successfully applied fo r texture classification, \\nsegmentation, image retrieval, object and face recognition, etc. These features are able to  characterize the local region, \\nand in the meantime they are robust to a certain degree to changes in image transformation and tolerant to occlusion and segmentation noise. In this paper, we propose to explore these features for our facial expression analysis.  \\nHoG [4] is to describe local object appearances and \\nshapes by distribution of local intensity gradients or edge directions, without the consideration of the corresponding gradient or edge position. During its implementation, an image is divided into a number of small blocks. In each block, a local 1-D histogram of gradient directions or edge orientations is calculated. As a result, a HoG descriptor is represented by the combin ed histogram entries.  \\nLBP [9] is to describe the pixels of an image by \\nthresholding the neighborhood of each pixel with the value \\nof the center point and using these binary numbers to construct a label. Then the histogram of the labels over a region can be used as the texture descriptor.  \\nSIFT [8] is also a local desc riptor of an image which is \\nimplemented by two steps: different-of-Gaussian (DoG) filters are first used to localize key points with the local scale-space maxima of DoG, and then each of these key \\npoints is used to generate a descriptor represented by a 3D histogram of gradient locations and orientations. This method transforms an image into a set of local descriptors, each of which is robust to image translation, scaling, and rotation and to some degree invariant to illumination changes and 3D projection due to the quantization of gradient locations and orientaitons. \\nIn recent years, computer vi sion research has witnessed a \\ngrowing interest in subspace analysis techniques. Before \\nwe utilize any classification technique, it is beneficial to first perform dimensionality reduction to project an image into a low dimensional f eature space, due to the \\nconsideration of learnability and computational efficiency.  \\nLocality Preserving Projection (LPP) [5] is a linear \\nmapping which is obtained by finding the optimal linear approximations to the eige n-functions of the Laplace \\nBeltrami operator on the manifold [5]. Different from the nonlinear manifold learning techniques, LPP could be simply applied to any new data point to locate it in the reduced representation manifold subspace, which is suitable for the classi fication application. \\nSome traditional subspace methods such as the PCA and \\nLDA aim to preserve the global structure. However, in many real world applicati ons, especially the facial \\nexpression recognition, the local structure could be more important. Different from PCA and LDA, LPP finds an \\nembedding that preserves local information, and obtains a subspace that best detects the e ssential manifold structure.  \\n5. Classification \\nThe variation of facial styl es (e.g., different facial \\ngeometry and textures associated with persons’ identity), facial expressions (e.g., defo rmation of facial surfaces), \\nand head poses (e.g., different viewing angles with respect to a camera) brings the big variation of the resulting facial images, thus challenging the current methods for analysis of facial expressions, which are mainly based on the nearly-frontal-view face image.  In our study, we focus on \\nanalyzing the factors of facial expressions and viewing angles while different facial styles (person identity) are considered as the w ithin-class variance.  \\nSuppose we have an image \\nx that captured a certain \\nhuman facial expression (emotion) ) ,...,2,1( m iCi=  at \\ncertain view angle (pose) ) ,...,2,1( n jDj= . The goal of \\nmulti-view facial expression recognition is to determine what emotion the test image represents, and at what pose the face is with respect to th e camera. This problem can be \\nformulated in the terms of probability theory as follows,  \\n)(),|(),()|,(xpDCxpDCPxDCPj i j i\\nj i=       (1) \\nIn our experimental data, the prior ),(j iDCp is an \\nevenly distribution probability. Thus, we use maximum likelihood criterion to predict the emotion and view angle of the face with resp ect to the camera.  \\nWe propose the following two schemes to recognize the \\nemotion expression and view angle (pose) from an input face image: view+emotion cascade classification and \\nview*emotion composite classification. And the classifier we use in this study is Nearest Neighbor (NN) which shows robust in many realistic applications. \\n5.1. Pose+Emotion cascade classification \\nThe view+emotion cascade cl assification divides the \\ntask of multi-view facial e xpression recognition into two \\nsequential steps, as illustrated in Figure 3. We first use an view classifier to recogni ze the view of this face (0\\n0, 300, \\n450, 600, or 900), then use the view-dependent emotion \\nclassifier to estimate the emo tion that the image represents \\n(happy, angry, disgust, fear, sad, and surprise ). This \\nclassification scheme can be described by Equation (2). \\n)|(),|( )|,( xDpxDCp xDCPj j i j i=              (2) \\nDuring the learning stage, we use all training data to \\nbuild a 5-class view classifier which output the view estimation (0\\n0, 300, 450, 600, or 900) of the face. In this step, \\nthe difference of the facial e xpression is regarded as the \\nwithin-class variance.  A set of view-dependent emotion \\n \\nclassifiers are learned by us ing training images that are \\ncorresponding to certain view angles. For example, the 300 \\nemotion classifier in Figur e 3 is a 6-class emotion \\nrecognizer, which is built on th e face images captured at \\n300 view angle. \\n \\n \\nFigure 3: The View+Emotion cascade classification, including a \\nview classifier and a set of view-dependent emotion classifiers.  \\n5.2. View*emotion composite classification \\nThe second solution for multi- view facial expression \\nrecognition is to build a cla ssifier that estimates the \\nemotion and facial views simultaneously.. In the other words, the each possible pair of emotions and poses is considered as a class. Considering 6 emotions (happy, angry, disgust, fear, sad, and surprise) and 5 viewing angles \\n(0\\n0, 300, 450, 600, or 900), the goal is to build a 30-class \\nclassifier. During the learning stage, we use all training \\ndata with emotion and view labels to build the emotion*view classifier. \\n5.3. Classifier fusion \\nClassifier fusion is an ac tive area in machine learning \\nand pattern recognition field. Many studies have demonstrated the advantage of  combination of classifiers \\nover individual classifiers due to uncorrelated errors from different classifiers [7]. \\nOur nearest neighbor classifiers output the similarity \\n(range 0~1) between a test sa mple and each class according \\nto its nearest distance to diffe rent class samples, so the \\nresults from different classifiers can be combined based on these similarities.  In this study, we are very interested in the \\ncombination of classifiers, each of which is built on different feature descriptors. Specifically, HoG, LBP and SIFT characterize the different  aspects of image patches, \\nproviding complementary information about facial expressions. They could produce different classification errors which can be reduced to certain degrees by the \\nclassifier fusion technique. T hus, the combined classifier \\ncould generate a better performance than the individual classifiers. 6. Experiments \\nIn our experiment, we apply 5-run two-fold \\ncross-validation person-indepe ndent scheme to evaluate \\nour approaches for multi-view facial expression recognition. In details, we randomly divided 100 subjects in BU-3DFE database into 2 groups without overlap. Each group includes 50 subjects.  For this test, all the data in one group are used as the test data, and the data of the remaining group are used as training samples. Then we switch the groups for training and testing. This experiment is repeated 5 times, each time dividing the subjects randomly. The statistics of our experimental data are 100 subjects, 6 emotions with 4 intensity levels, 5 view angles (0\\n0, 300, 450, 600, or 900), illustrated in Table 1. So the total \\nnumber of images is 12000. We use a histogram equalization to reduce the influence of the skin color variation due to the di fferent human races.   \\n \\nTable 1. Statistics of data \\nSubject Emotion Intensity Angle Total images\\n100 6 4 5 12000 \\n \\nWe first used the technique (SIFT, HoG, LBP) on \\naligned multi-view images described in Section 4 to extract facial features. The extracted features on key facial points \\nare concatenated to form a long vector as the input of our classifiers.  \\nWe also investigate the influence of various dimension \\nreduction methods on our emotion classification. We use PCA, LDA, and LPP to project the original features into \\nlow dimension spaces, where the low-dimensional features are used as input to a nearest-neighbor indexing method that identifies view and facial expressions. For each method, we use the optimal facial expression subspace which has the best recognition result with respect to the number of dimensions.   \\nTable 2, 3, 4 and 5 report the view-dependent \\nexperimental results of facial expression recognition, based on raw appearance intensity, HoG, LBP, and SIFT descriptors, respectively. Each  row in these tables shows \\nthe error rates of different di mension reductions (original, \\nLPP, PCA, LDA) with respect to a certain yaw rotation angle (0, 30, 45, 60 and 90 degrees). “Original” means the original space without any dime nsion reduction.  The last \\nrow is the average performance of these classifiers in the \\nmulti-view emotion recogniti on experiment.  The best \\naverage performance result with the lowest error rate in \\neach table is highlighted  by the bold font.  \\nTable 2 shows that based on the raw appearance intensity, \\nthe classification in the orig inal space achieves the best \\nperformance with 57.16% error rate. Table 3, 4 and 5 demonstrate that based on HoG, LBP and SIFT descriptors, the classification in the LPP space has the best performance with error rates of 32.618%, 35.764%, and 26.942%, 00 Emotion classifier \\n300 Emotion classifier \\n450 Emotion classifier \\n600 Emotion classifier \\n900 Emotion classifier x\\n  \\n   \\nView \\nclassifier 00 \\n300 \\n450 \\n600 \\n900 \\n \\nrespectively. All of them are much better than the \\nclassification based on the ra w appearance intensity. The \\nexperiments suggest that SIFT+LPP has the best performance with 26.942% average error rate, and achieves \\nthe lowest error rate ( 26.13 %) at 30\\n0 and highest error rate \\n(28.55% ) at the profile view (900). \\n \\nTable 2: Error rates of view-dep endent emotion recognition based \\non raw appearance intensity. \\nAngle Original  LPP PCA LDA \\n0 0.5458 0.5623 0.5467 0.5763\\n30 0.5498 0.6198 0.5488 0.6282\\n45 0.5637 0.6545 0.561 0.7128\\n60 0.5848 0.6962 0.585 0.695\\n90 0.6143 0.7707 0.6175 0.7835\\nAverage 0.5716 0.6607 0.5718 0.6791\\n \\nIn our experiment, the view recognition is pretty easy \\nbecause we use data with five view angles (00, 300, 450, 600, \\nor 900) from frontal view to profile view. The interval of \\nview angles is larger than or equal 150 so the image \\ndifferences between different vi ew angles are considerable. \\nOur experiments show less than  1% error rates with raw \\nappearance feature and HoG features. In our future work, we will add images with smaller angle interval. \\nTable 6 is the results of view-dependent emotion \\nrecognition, based on the combination of SIFT+LPP, HoG+LPP, and LBP+LPP classi fication. Specifically, we \\nuse the following combining rules in PRTools (www.prtools.org) to make th e final recognition decision:  \\nProdc: product combining classifier Meanc: averaging co mbining classifier \\nMedian: median combining classifier Maxc: maximum combining classifier Minc: minimum combining classifier Votec: voting comb ining classifier \\nTable 6 shows that the rank of combining classifiers with \\nrespect to average recognition performance from the best to \\nthe worst is \\nProdc > meanc > minc > median > maxc >votec However, even the best combination (prodc) with error \\nrate (25.54%) does not show significant advantage over SIFT-only classification whose error rate is 26.942% \\nTable 8 show the confusion matrix of average \\nrecognition results, based on the prodc combination of SIFT+LPP, HoG+LPP and LBP+L PP. It demonstrates that \\nthe rank of emotions with  respect to the average \\nrecognition performance from the best to the worst is  \\nSurprise > Happy > Angry > Sad > Disgust > Fear In our experiment of view*emotion composite \\nrecognition where a 30-class classifier is built to simultaneously recognize view and emotion, the results based on HoG features are shown in Table 7. These results are a little worse than the co rresponding results in Table 3 \\nbased on view+emotion cascade classification scheme. The view*emotion composite classification methods based on \\nLBP and SIFT features demand too much memory that are \\nbeyond our current computer  capability. So we gave up \\nusing LBP and SIFT features for this view*emotion composite recognition experiment.  \\nTable 3: Error rates of view-dep endent emotion recognition based \\non HoG features. \\nAngle Original  LPP PCA LDA \\n0 0.5302 0.3208 0.529 0.4147\\n30 0.5373 0.3155 0.5363 0.4004\\n45 0.5573 0.3234 0.5599 0.3867\\n60 0.5607 0.3198 0.5575 0.3716\\n90 0.5468 0.3514 0.5468 0.366\\nAverage 0.54646 0.32618 0.5459 0.38788\\n \\nTable 4: Error rates of view-dep endent emotion recognition based \\non LBP features. \\nAngle Original  LPP PCA LDA \\n0 0.5013 0.3567 0.5048 0.4636\\n30 0.5132 0.3488 0.515 0.4538\\n45 0.5277 0.3633 0.5274 0.5957\\n60 0.515 0.3482 0.5156 0.4908\\n90 0.5484 0.3712 0.5495 0.6059\\nAverage 0.52112 0.35764 0.52246 0.52196\\n \\nTable 5: Error rates of view-dep endent emotion recognition based \\non SIFT features. \\nAngle Original  LPP PCA LDA \\n0 0.4368 0.2724 0.4393 0.4084\\n30 0.4319 0.2613 0.4327 0.4041\\n45 0.4454 0.2665 0.4423 0.5518\\n60 0.4464 0.2614 0.4479 0.4229\\n90 0.4384 0.2855 0.4389 0.481\\nAverage 0.43978 0.26942 0.44022 0.45364\\n \\nTable 6: Error rates of view -dependent emotion, based on \\ncombination of SIFT+LPP, HoG+LPP and LBP+LPP classifiers \\nAngle prodc meanc median maxc minc votec \\n0 0.2641 0.2677 0.2757 0.278 0.2741 0.2849\\n30 0.2553 0.2555 0.2639 0.2683 0.265 0.2757\\n45 0.2626 0.2657 0.2772 0.2713 0.2717 0.2882\\n60 0.2657 0.2704 0.2718 0.2877 0.2746 0.2808\\n90 0.2796 0.2823 0.2954 0.2907 0.2907 0.3092\\nAve0.2654 0.2683 0.2768 0.2792 0.2752 0.2877\\n Table 7: Error rates of view*e motion composite classification, \\nbased on HoG features \\nOriginal  LPP PCA LDA \\n0.5788 0.3673 0.57097 0.48417 \\n7. Conclusion \\nDeveloping a facial expression recognition system to be \\ncapable of handling non-frontal-view facial expressions is important to advance th e research of machine \\nunderstanding of spontaneous facial expressions. However, little work has been done to investigate such an issue \\n \\nsystematically in the past.  Most of existing methods are \\nbased on data of near-frontal-v iew facial expression, which \\nare conceivable to fail in affec tive behavior analysis in less \\nconstraint interactive environment. \\nIn this paper, we investigated multi-view facial \\nexpression recognition, with a goal to improve performance by taking into account the influence of viewing angles of facial images. We applied three advanced local patch descriptors (HoG, LBP and SIFT) to characterize the facial expressi ons. We investigated feature \\ndimension reduction (LPP, PCA and LDA) and the classifier fusion for this application. Our extensive person-independent experime nt based on 100 subjects in \\nthe BU-3DFE database shows that the SIFT descriptor performs better than HoG and LBP, LPP performs better than PCA and LDA. Nevertheless, classifier fusion does not show significant improvement in our experiment. How to build an optimal combination is worthy further investigation.  \\nIn our experiment, the view recognition is pretty easy, \\nbecause the large interval of viewing angles (e.g., greater \\nthan or equal to 15\\n0 in our current study) causes obvious \\nimage differences between different view angles. In our future work, we will add images with smaller angle intervals in order to further explore the view recognition problem. \\nReferences \\n[1] Bartlett, M.S., Littlewort, G., Fra nk, M., Lainscsek, C., Fasel, \\nI., and Movellan, J.(2005), R ecognizing Facial Expression: \\nMachine Learning and Applicati on to Spontaneous Behavior, \\nIEEE International Conference on Computer Vision and Pattern Recognition, 568-573 \\n[2] Chang Y, Vieira M, Turk M,  and Velho L (2005). Automatic \\n3D facial expression analysis in videos. Analysis and Modelling of Faces and Gestures, 3723, 293-307. \\n[3] Cohn, J.F. (2006), Foundations of  Human Computing: Facial \\nExpression and Emotion, Int. Conf. on Multimodal Interfaces, 233-238 [4] Dalal, N. and Triggs, B. ( 2005). Histograms of oriented \\ngradients for human detection. IEEE International Conference on Computer Vision and Pattern Recognition. \\n[5] He, X., Yan, S., Hu, Y., and Zhang, H, Learning a Locality \\nPreserving Subspace for Visual Recognition, Int. Conf. on Computer Vision, 2003 \\n[6] Kanade, T., Cohn, J., and Tian, Y. (2000), Comprehensive \\nDatabase for Facial Expression Analysis, Int. Conf. on Face and Gesture Recognition, 46-53 \\n[7] Kuncheva, L.I. (2004), Combin ing Pattern Classifier: Meth\\nods and Algorithms, John Wiley and Sons, 2004 \\n[8] Lowe, D.G. (1999). Object Recognition from Local \\nScale-Invariant Features, Int’l Conf. on Computer Vision.   \\n[9] Ojala, T., Pietikainen, M.  and Maenpaa, I. (2002). \\nMulti-resolution gray-scale and rotation invariant texture classification with local binary patterns. IEEE Trans. on Pattern Analysis and Machine Intelligence, 24: 971-987, 20 \\n[10] Pantic, M., and Patras, I. (2006). Dynamics of facial \\nexpression: recognition of facial  actions and their temporal \\nsegments form face profile image sequences. IEEE Trans. Systems, Man and Cybernetic s–Part B, Vol. 36, No.2, \\n433-449 \\n[11] Sebe, N., Lew, M.S., Cohen, I., Sun, Y., Gevers, T., Huang, \\nT.S.(2004), Authentic Facial Expression Analysis, Int. Conf. on Automatic Face and Gesture Recognition. \\n[12] Valstar, M., Pantic, M., Ambada r, Z., and Cohn, J.F. (2006). \\nSpontaneous vs. Posed Facial Behavior: Automatic Analysis \\nof Brow Actions. Int. Conf. on Multimedia Interfaces. 162 ‐\\n170  \\n[13] Wang, J., Yin, L., Wei, X., a nd Sun, Y. (2006). 3D Facial \\nExpression Recognition Based on Primitive Surface Feature Distribution. IEEE Conference on Computer Vision and Pattern Recognition, 2:1399-1406 \\n[14] Yin, L., Wei, X., Sun, Y., Wang,  J., Rosato, M. J. (2006).  A \\n3D facial expression database for facial behavior research. \\nInt. Conf. on Automatic Face and Gesture Recognition,  211- 216 \\n[15] Zeng, Z., Pantic, M., Roisman, G.I. and Huang, T.S. (2007). \\nA Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. Int’l Conf. Multimodal Interfaces, 126-133. \\n \\nTable 8: confusion matrix of average recognition base on the pr odc combination of SIFT+LPP, LB P+LPP and HoG+LPP on 5 views  \\nRecognized   \\nAccuracy (%) Angry Disgust Fear Happy Sad Surprise\\nAngry 73.47 5.56 3.87 0.78 15.38 0.94\\nDisgust 10.15 71.00 7.74 3.67 4.05 3.39\\nFear 6.96 9.21 55.40 14.53 7.65 6.25\\nHappy 1.67 3.13 11.61 81.74 1.13 0.72\\nSad 19.65 2.08 5.69 0.92 71.40 0.26Ground \\ntruth \\nSurprise 1.30 2.31 5.49 1.48 1.69 87.73\\n \\n',\n",
       " \"1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n1\\nMultimodal 2D+3D Facial Expression Recognition\\nwith Deep Fusion Convolutional Neural Network\\nHuibin Li, Student Member, IEEE, Jian Sun, Member, IEEE, Zongben Xu,\\nMember, IEEE, and Liming Chen, Member, IEEE\\nAbstract —This paper presents a novel and efﬁcient Deep\\nFusion Convolutional Neural Network (DF-CNN) for multi-modal\\n2D+3D Facial Expression Recognition (FER). DF-CNN comprises\\na feature extraction subnet, a feature fusion subnet and a softmax\\nlayer. In particular, each textured 3D face scan is represented as\\nsix types of 2D facial attribute maps (i.e., geometry map, three\\nnormal maps, curvature map, and texture map), all of which are\\njointly fed into DF-CNN for feature learning and fusion learning,\\nresulting in a highly concentrated facial representation (32-\\ndimensional). Expression prediction is performed by two ways: 1)\\nlearning linear SVM classiﬁers using the 32-dimensional fused\\ndeep features; 2) directly performing softmax prediction using\\nthe 6-dimensional expression probability vectors. Different from\\nexisting 3D FER methods, DF-CNN combines feature learning\\nand fusion learning into a single end-to-end training framework.\\nTo demonstrate the effectiveness of DF-CNN, we conducted\\ncomprehensive experiments to compare the performance of DF-\\nCNN with handcrafted features, pre-trained deep features, ﬁne-\\ntuned deep features, and state-of-the-art methods on three 3D\\nface datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and\\nBosphorus Subset). In all cases, DF-CNN consistently achieved\\nthe best results. To the best of our knowledge, this is the ﬁrst\\nwork of introducing deep CNN to 3D FER and deep learning\\nbased feature-level fusion for multi-modal 2D+3D FER.\\nIndex Terms—Facial expression recognition, deep fusion con-\\nvolutional neural network, multimodal, textured 3D face scan.\\nI. I NTRODUCTION\\nFACIAL expressions, as a form of nonverbal communica-\\ntion, and a primary means of conveying social information\\namong humans, are ideal for human emotion measuremen-\\nt, computation, and interpretation. Therefore, machine-based\\nautomatic facial expression recognition (FER) has a wide\\nrange of applications in human-computer interaction, facial\\nanimation, entertainment, and psychology study [3], [27], [43],\\n[58], etc. It has been extensively investigated over the past\\ndecades in the ﬁelds of multimedia, affective computing, and\\ncomputer vision [6], [12], [37], [40].\\nExisting FER methods generally can be classiﬁed from three\\nperspectives, namely the data modality, expression granular-\\nity, and temporal dynamics [12], [37], [40]. From the ﬁrst\\nperspective, they are classiﬁed into: 2D FER (which uses 2D\\nface images), 3D FER (which uses 3D face shape models),\\nH. Li, J. Sun, and Z. Xu are with the Institute for Information and System\\nSciences, School of Mathematics and Statistics, Xi’an Jiaotong University,\\nXi’an, 710049, China. E-mail: fhuibinli, jiansun, zbxug@mail.xjtu.edu.cn,\\nL. Chen is with the LIRIS UMR 5205, Department of Mathematics\\nand Informatics, Ecole Centrale de Lyon, Lyon, 69134, France. E-mail:\\nliming.chen@ec-lyon.fr.\\nManuscript received xxx, xxx; revised xxx, xxx.and 2D+3D multi-modal FER (which uses both 2D and 3D\\nface data). From the second perspective, they are divided into:\\n1) recognition of prototypical facial expressios (i.e., anger,\\ndisgust, fear, happiness, sadness and surprise), 2) detection and\\nrecognition of facial Action Units (AU, e.g., brow raiser, lip\\ntightener, and mouth stretch). From the third perspective, they\\nare categorized into static (still images) or dynamic (image\\nsequences) FER [40], [68]. In this paper, we focus on the\\nproblem of recognizing the six prototypical facial expressions\\nusing multi-modal 2D and 3D static face data (i.e., textured\\n3D face scans).\\nIn the literature of FER, the majority of methods are\\nbased on 2D face images or videos (e.g., [5], [6], [8], [18],\\n[37], [49], [55], [56], [58], [61], [62]). Despite signiﬁcant\\nadvances have been achieved, 2D methods still fail to solve the\\nchallenging problems of illumination and pose variations [37].\\nDesigning FER systems using infrared facial images is a\\nbeneﬁcial attempt to solve the illumination issue [54], [55].\\nBut infrared images are usually fail to capture subtle facial\\ndeformations, e.g., skin wrinkles [18], and also sensitive to\\nthe effect of wearing glasses, which is often occur in uncon-\\ntrolled condition. With the fast development of 3D imaging\\nand scanning technologies, FER using 3D face scans has\\nattracted more and more attentions [12], [13], [16], [40]. This\\nis mainly due to that 3D face scans are naturally robust\\nto lighting and pose variations. Moreover, 3D facial shape\\ndeformations caused by facial muscle movements contain\\nimportant cues to distinguish different expressions. To meet\\nthe requirements of real applications, FER based on multi-\\nmodality data (e.g., visual and audio [49], visible and infrared\\nface images [54], [55]), especially using both 2D face images\\nand 3D face models [16], [24] [42], [50], is becoming a\\npromising research direction due to that there exist large\\ncomplementarity among different modalities.\\nThis paper is a new attempt along this promising direc-\\ntion, which dedicates to exploring multi-modal 2D+3D FER\\nmethod by combing the advantages of both 2D and 3D face\\ndata. The main challenges of such combination involve the\\nfollowing two issues: 1) how to ﬁnd a uniﬁed framework\\nto generate discriminative facial representations for both\\n2D and 3D face data? 2) how to optimally combine the\\nfacial representations of 2D and 3D face data for expression\\nprediction? As illustrated in Table I, handcrafted features\\nsuch as HOG [7], LBP [65], and Gabor [64] have been\\nwidely used for facial representations in 2D FER. Similarly,\\nthese handcrafted features have also been widely employed\\nin 3D FER, which are used to describe 3D facial shape\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n2\\nTABLE I\\nMOTIVATIONS :A FEW EXAMPLES OF CURRENT FER RESEARCH .\\nHandcrafted\\nfeatures Learned\\nFeatures\\n2D\\nFERHOG:\\nHu et al. [7] Deep\\nCNN: Yu and Zhang [60]\\nLBP:\\nZhao et al. [65] DBN:\\nKahou et al. [19]\\nGabor:\\nZhang et al. [64] Auto-Encoder:\\nRifai et al. [39]\\n3D\\nFERDepth-SIFT\\n: Berretti et al. [1]\\nNormal-LBP:\\nLi et al. [23] Learned\\nfeature for 3D FER?\\nCurv\\nature-HOG: Lemaire et al. [22]\\n2D+3D\\nFERHandcrafted\\nfeature-level fusion\\nHandcrafted\\nscore-level fusion Learning-based\\nfusion for 2D+3D FER?\\nSa\\nvran et al. [42], Li et al. [24]\\ninformation by coding different types of geometric maps like\\ndepth-SIFT [1], normal-LBP [23], and curvature-HOG [22].\\nRecently, with the signiﬁcant breakthrough of deep learning,\\nsuch kind of handcrafted features have been proven to be\\nsuboptimal. Thanks to the continuous updating and releasing\\nof large 2D expression datasets (e.g., Acted Facial Expressions\\nin the Wild (AFEW) [10] and Static Facial Expressions in\\nthe Wild (SFEW) [9]), leaning facial representations using\\ndeep learning is becoming the mainstream in 2D FER. For\\nexample, following the Emotion Recognition in the Wild\\n(EmotiW) Grand Challenge, a large number of deep learning\\nbased approaches, such as deep convolutional neural network\\n(CNN) [60], deep belief network (DBN) [19], and auto-\\nencoder [39] have been successfully used in 2D FER as shown\\nat the right side of Table I.\\nHowever, to the best of our knowledge, deep learning has\\nnever been used to learn 3D facial representations in 3D FER.\\nThis motivates us to ﬁll this gap although a very limited\\nnumber of 3D face scans with expression labels are available.\\nInspired by the fact that the off-the-shelf pre-trained deep CNN\\nmodels have surprising and consistent good generalization\\nability for various visual recognition tasks [11], [38], A\\npromising way is using transfer learning method that ﬁne tunes\\na pre-trained deep CNN model using as many as possible 3D\\nface data.\\nDeep CNN can provide a uniﬁed framework to learn facial\\nrepresentations for both 2D and 3D face data. Then, how to\\nﬁnd a strategy to optimally combine these learned 2D and 3D\\nfacial representations is becoming the key issue. As illustrated\\nin Table I, the suboptimal handcrafted feature-level fusion\\nand score-level fusion are widely used in current multi-modal\\n2D+3D FER methods. The importance weights of 2D and 3D\\nfacial features have not be well explored. This motivates us to\\ndesign a learning-base fusion strategy, i.e., a novel deep fusion\\nnetwork, which can automatically learn sophisticated fusion\\nweights of 2D and 3D facial representations for multi-modal\\n2D+3D FER. Overall, this paper presents a uniﬁed end-to-\\nend learning framework (i.e., Deep Fusion CNN or DF-CNN),\\nwhich can deals with both feature learning and fusion learning\\nfor multi-modal 2D+3D FER. Therefore, the main novelties\\nand contributions of this paper can be summarized as follows:\\n\\x0fThis is the ﬁrst work of introducing deep CNN to 3D\\nFER and using learned features to describe 3D facial\\nexpressions. To overcome the issue that training 3D faces\\nare far from enough, we propose to use multiple types offacial attribute maps to learn facial representations by ﬁne\\ntuning pre-trained deep CNN models trained from large-\\nscale image dataset for generic visual tasks.\\n\\x0fThis paper proposes to use a deep fusion net (i.e., a\\nlearning-based feature-level fusion) to learn the optimal\\ncombination weights of 2D and 3D facial representations\\nfor multi-modal 2D+3D FER. This is totally different\\nfrom the suboptimal handcrafted feature-level fusion and\\nscore-level fusion used in existing 2D+3D FER.\\n\\x0fThis paper presents a Deep Fusion CNN, which combines\\nfeature learning and fusion learning into a uniﬁed end-\\nto-end training framework, and consistently outperforms\\nthe handcrafted features, pre-trained deep features, ﬁne-\\ntuned deep features, and state-of-the-art 3D FER methods\\non three 3D face datasets.\\nThe remainder of this paper is organized as follows. Related\\nworks for 2D, 3D and 2D+3D FER are introduced in Sec-\\ntion II. Section III gives an overview of the proposed approach.\\nSection IV introduces the computational details of generating\\ndifferent facial attribute maps. Section V describes our DF-\\nCNN in detail, involving net architecture, training strategy, and\\nvisualization. Experimental results are shown in Section VI,\\nand Section VII concludes the paper.\\nII. R ELATED WORKS\\nA. Related works on 3D and 2D+3D FER\\nCurrent 3D FER approaches are mainly model-based or\\nfeature-based [12]. Model-based methods generally employ\\ndense rigid registration and non-rigid ﬁtting techniques to get\\nthe one-to-one point correspondence among face scans. This\\ngenerates a generic expression deformable model, which can\\nbe used to ﬁt unknown face scans, and the ﬁtting param-\\neters are ﬁnally used as expression features. For example,\\nMpiperis et al. [35] proposed to build a novel bilinear facial\\ndeformable model to characterize the behaviors of facial non-\\nrigid deformations. Given a new 3D face model, its expression\\nand identity parameters can be estimated using the well-\\ntrained bilinear model. These parameters are then used as\\nexpression features and fed into the Maximum Likelihood\\nclassiﬁer for expression prediction. Similarly, Gong et al. [15]\\nsuggested to learn a model to decompose the shape of an\\nexpressive face into a neutral-style basic facial shape com-\\nponent (BFSC) and an expression shape component (ESC).\\nThe ESC is then used to design expression features. Zhao et\\nal.[66] proposed to build a statistical facial feature model\\n(SFAM) for automatic facial landmarking, both 3D shape\\nand 2D texture features are extracted around these landmarks\\nfor expression recognition. Feature-based methods generally\\nextract local expression features around facial landmarks based\\non surface geometric attributes or differential quantities. For\\nexample, 3D landmark distances [44], [45], [46], [47], local\\nsurface patch distances [24] [32] [33], geometry and normal\\nmaps [36], conformal images [63], surface normal [26] and\\ncurvatures [26], [53] are some popular features use for 3D\\nFER. As a typical local feature-based method, Maalej et\\nal.[32] [33] proposed to extract local surface patches around\\n70 facial landmarks in the 3D mesh. These patches were\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n3\\nthen parameterized by a set of closed iso-level curves at the\\nlandmarks. The distance between two patches was computed\\nby the geodesic distance of deforming their corresponding iso-\\nlevel curves in the Riemannian shape analysis space. Finally,\\nmulti-boosting and support vector machines (SVM) classiﬁers\\nwere used to classify the six prototypical facial expressions.\\nBy combing the advantages of both feature-based and model-\\nbased methods, Zhen et al. [67], [68] proposed to study\\n3D FER problem from the perspective of facial muscular\\nmovement model. Their method ﬁrst automatically segments\\n3D face shapes into several facial regions according to the\\nmuscular movement model. Then, each region is described by\\na set of geometric features. The weights of different regions\\nare learned by genetic algorithm, and SVM classiﬁer with\\nscore-level fusion is used for expression prediction. Savran\\net al. [42] utilized multi-modal 2D+3D face data for facial\\nAU detection. They found that 3D data generally perform\\nbetter than 2D data, especially for lower AUs. Moreover,\\nthe fusion of two modalities can improve the detection rates\\nfrom 93.5% (2D) and 95.4% (3D) to 97.1% (2D+3D). Li\\net al. [24] proposed a fully automatic multi-modal 2D+3D\\nfeature based FER approach. Both 2D texture descriptors\\nand 3D geometry descriptors are used to describe the ap-\\npearances and geometric deformations of local facial patches\\naround automatically detected 2D and 3D facial landmarks.\\nThe complementarity between 2D descriptors, 3D descriptors,\\nand 2D+3D descriptors are demonstrated in their experiments\\nbased on both feature-level and score-level fusion strategies of\\nthe SVM classiﬁer.\\nThe main weakness of model-based methods lie in that they\\nrequire to establish dense correspondence among face scans,\\nwhich is still a challenging issue. Moreover, time consuming\\nprocedures like dense 3D face registration and model ﬁtting are\\nusually indispensable in practice. Feature-based methods gen-\\nerally perform better than model-based ones. However, their\\nperformances are largely dependent on the accuracy of 3D\\nfacial landmarking, which is also a challenging task [12]. FER\\nbased on 2D+3D multi-modal data is becoming a promising\\nresearch direction due to that there exist large complementarity\\namong different modalities. Giving a complete survey for\\n3D FER is out the scope of this paper, readers are strongly\\nsuggested to refer to the comprehensive survey [40] for the\\nissues of 3D and 4D face acquisition, dense correspondence,\\nalignment, tracking, available databases, as well as the details\\nof feature extraction, selection, classiﬁcation, and temporal\\nmodeling for static and dynamic 3D facial expression recog-\\nnition.\\nB. Related works on 2D FER\\nRifai et al. [39] designed a multi-scale contractive convolu-\\ntional network to learn hierarchical expression features which\\nare robust to the variations of factors like pose, identity, mor-\\nphology of the face. Tang [48] demonstrated the advantages\\nof replacing the softmax loss function of a deep CNN by\\na linear SVM loss for 2D FER. Liu et al. [30] proposed a\\nuniﬁed Boosted Deep Belief Network framework to iteratively\\noptimizing the expression training process of feature learning,feature selection, and classiﬁer construction. Burkert et al. [2]\\nproposed a convolutional neural network (CNN) architecture\\nfor 2D FER and claimed that it outperforms the earlier\\nproposed CNN based approaches. Liu et al. [29] designed a\\n3D CNN incorporating a deformable parts learning component\\nfor dynamic expression analysis. The authors also proposed\\nthe action unit inspired deep networks for 2D FER [28].\\nKhorrami et al. [20] showed both qualitatively and quanti-\\ntatively that CNNs can learn facial action units when doing\\nexpression recognition, and their method achieved state-of-the-\\nart performance on the extended Cohn-Kanade (CK+) and the\\nToronto Face Dataset (TFD). Kahou et al. [19] developed a\\ndeep learning approach for emotion recognition in video. Their\\nmethod respectively trained a CNN for video and a deep belief\\nnet for audio. “Bag of moutm” features are also extracted to\\nfurther improve the performance. To fusion different models,\\nthe ensemble weights are determined with random search. The\\nidea of ensemble multiple deep models has also been used in\\nKim et al. [21]. This work trained 216 deep CNNs by varying\\nnetwork architectures, input normalization, and weight initial-\\nization and by adopting several learning strategies. Then, the\\nvalid-accuracy-based exponentially-weighted decision fusion\\nmethod was proposed to ensemble different CNNs.\\nThe work by Yu and Zhang [60] is probably the most related\\nwork to ours. This method proposed to independently train\\nmultiple differently initialized CNNs and output their training\\nresponses. To combine multiple CNN models, they proposed\\nto learn the ensemble weighs of the network responses by\\nminimizing the log likelihood loss or hinge loss. Despite\\nwith the same spirit of fusing deep models, our proposed\\nlearning strategy differs from [60] signiﬁcantly. First, they\\ntrained multiple CNNs by varying the network initialization,\\nwhile we only need to train a single CNN for different facial\\nattribute maps. As shown in our experiments (Section VI-D),\\nthis kind of single network training can largely reduce both\\ncomputate time and memory consumption, while still preserve\\nthe accuracy. Second, their method learned different weights\\nfor different networks, thus corresponding to a learning-\\nbased score-level fusion strategy, while ours corresponds to\\na learning-based feature-level fusion strategy.\\nIII. O VERVIEW OF THE PROPOSED APPROACH\\nFigure 1 illustrates the pipeline of the proposed DF-CNN\\napproach for 2D+3D FER. Given a set of preprocessed tex-\\ntured 3D face scans with different expressions, each of which\\nis ﬁrst represented as six types of 2D facial attribute maps\\n(see Section IV), including geometry map (3D coordinates),\\nthree normal component maps (normal vectors), normalized\\ncurvature map (principle curvatures), and texture map. Then,\\nthese six facial attribute maps of each textured 3D face scan\\nare jointly fed into the feature extraction subnet (repetitions of\\nconvolution, ReLU, and pooling layers) with sharing param-\\neters, resulting in several hundreds of multi-channel feature\\nmaps. All these feature maps are fed into the following feature\\nfusion subnet (including a reshape and two feature fusion\\nlayers), leading to a highly concentrated facial representation\\n(32-dimensional fused deep feature). Finally, the softmax-loss\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n4\\nFig. 1. Pipeline of the proposed deep fusion CNN (DF-CNN) based multi-modal 2D+3D FER approach. Each textured 3D face scan is represented as six\\ntypes of 2D facial geometric and photometric attribute maps (i.e., 3D coordinates based geometry map, normal vectors based normal maps, principle curvatures\\nbased curvature map, and texture map). These attribute maps are jointly fed into the feature extraction subnet of DF-CNN with sharing parameters, generating\\nhundreds of multi-channel feature maps. All these feature maps are then fed into the feature fusion subnet (including a reshape and two fusion layers) of\\nDF-CNN, resulting in a highly concentrated facial representation (32-dimensional fused deep feature). Finally, the softmax-loss layer is followed for network\\ntraining (see section V-A for details). The ﬁnal expression label prediction is performed by two ways: learning linear SVM classiﬁers using the 32-dimensional\\nfused deep features or directly performing softmax prediction based on the 6-dimensional probability vectors.\\nor softmax layer is followed for network training or expression\\nprediction (see Section V-A for details).\\nFor DF-CNN training, considering that there are very limit-\\ned numbers of textured 3D face scans with expression labels,\\nthe feature extraction subnet is initialized using the off-the-\\nshelf convolutional layers of a pre-trained deep model (e.g.,\\nvgg-net-m). This kind of pre-trained deep models have been\\nproven to have a good generalization ability for generic visual\\nrecognition tasks [11], [38]. The feature fusion subnet is\\nrandomly initialized, and the whole net is trained by the back-\\nprorogation algorithm using the softmax-loss function and the\\nstochastic gradient descent (SGD) algorithm.\\nFor DF-CNN testing, six facial attribute maps of each\\ntextured 3D face scan are jointly fed into the feature extraction\\nand feature fusion subnets, generating a highly concentrat-\\ned facial representation (32-dimensional fused deep feature).\\nThis deep feature is further transformed into a 6-dimensional\\nexpression probability vector by the ﬁnal softmax layer. Ex-\\npression label prediction is preformed by training linear SVM\\nclassiﬁers using the 32-dimensional fused deep features (i.e.,\\nDF-CNN svm) or directly performing softmax prediction based\\non the 6-dimensional probability vectors (i.e., DF-CNN softmax ).\\nIV. A TTRIBUTE MAPS OF A TEXTURED 3D FACE\\nTo comprehensively describe the geometric and photometric\\nattributes of a textured 3D face scan, six types of 2D fa-\\ncial attribute maps, namely the geometry map, texture map,\\nthree normal maps, as well as normalized curvature map are\\nemployed. Given a raw textured 3D face scan, we ﬁrst run\\nthe preprocessing pipeline algorithm (see section VI-A) to\\ngenerate a 2D texture map Itand a geometry map Ig. The\\ncoordinates information of each geometry map are then used to\\nestimate the surface normals and curvatures, resulting in three\\nnormal component maps Ix\\nn,Iy\\nn, andIz\\nn, and one normalized\\ncurvature (i.e. shape index) map Ic. Finally, a textured 3D face\\nscanIcan be described by six types of 2D facial attributemaps:I=fIg;Ix\\nn;Iy\\nn;Iz\\nn;Ic;Itg, as shown in Fig. 2. The\\ndetails for generation of normal maps and curvature map are\\nintroduced as follows.\\nA. Normal maps\\nGiven a normalized facial geometry map Igrepresented by\\nam\\x02n\\x023matrix:\\nIg= [pij(x;y;z )]m\\x02n = [pijk]m\\x02n\\x02fx;y;zg; (1)\\nwherepij(x;y;z ) = (pijx;pijy;pijz)T;(1\\x14i\\x14m;1\\x14j\\x14\\nn;i;j2Z)represents the 3D coordinates of point pij:Let its\\nunit normal vector matrix (m \\x02n\\x023) be\\nIn= [n(pij(x;y;z ))]m\\x02n = [nijk]m\\x02n\\x02fx;y;zg; (2)\\nwheren(pij(x;y;z )) = (nijx;nijy;nijz)T;(1\\x14i\\x14m;1\\x14\\nj\\x14n;i;j2Z)denotes the unit normal vector of pij:In this\\npaper, we utilize the local plane ﬁtting method [17] to estimate\\nIn. That is to say, for each point pij2Ig, its normal vector\\nn(pij)can be estimated as the normal vector of the following\\nlocal ﬁtted plane:\\nSij:nijxqijx+nijyqijy+nijzqijz=d; (3)\\nwhere (qijx;qijy;qijz)Trepresents any point within the local\\nneighborhood of point pijandd=nijxpijx+nijypijy+\\nnijzpijz:In this work, a neighborhood of 5\\x025window is\\nused. To simplify, each normal component in equation (2) can\\nbe represented by an m\\x02nmatrix:\\nIn=8\\n><\\n>:Ix\\nn= [nx\\nij]m\\x02n;\\nIy\\nn= [ny\\nij]m\\x02n;\\nIz\\nn= [nz\\nij]m\\x02n:(4)\\nwherek(nx\\nij;ny\\nij;nz\\nij)Tk2= 1:\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n5\\nFig. 2. Illustration of the six types of 2D geometric and photometric facial\\nattribute maps of six textured 3D face scans (subject F0001 in the BU-\\n3DFE dataset) with six prototypical facial expressions (i.e., anger, disgust,\\nfear, happiness, sadness, and surprise). The left hand column shows: the\\ngeometry maps, texture maps, and curvature maps, and the three normal maps\\n(components x,y, andz) are shown at the right hand column.\\nB. Curvature map\\nSimilar to the local plane ﬁtting method used for normal\\nestimation, we explored the local cubic ﬁtting method [14] to\\nestimate the principle curvatures. This method assumes that the\\nlocal geometry of a surface is approximated by a cubic surface\\npatch. For robustly solving the local ﬁtting problem, both the\\n3D coordinates and the normal vectors of the neighboring\\npoints of the point pij2Igto be estimated are used. That\\nis, we are ﬁtting the following equations:\\n8\\n><\\n>:z(x;y) =a\\n2x2+bxy+c\\n2y2+dx3+ex2y+fxy2+gy3\\nzx=ax+by+ 3dx2+ 2exy +fy2\\nzy=bx+cy+ 3gy2+ 2fxy +ex2:\\n(5)\\nThese equations can be solved by the least squares regression,\\nand the shape operator Scan be computed as:\\nS=0\\n@a b\\nb c1\\nA:\\nThen, the eignvalues of Sgive the two principle curvatures\\n\\x141and\\x142at pointpij2Ig. The normalized curvatures (i.e.,\\nshape index value) at this point is deﬁned by:\\n1\\n2\\x001\\n\\x19arctan\\x12\\x141+\\x142\\n\\x141\\x00\\x142\\x13\\n: (6)\\nFigure 2 shows six types of 2D geometric and photometric\\nfacial attribute maps of six textured face scans with six\\nprototypical facial expressions of subject F0001 in the BU-\\n3DFE database.\\nV. D EEPFUSION CONVOLUTIONAL NEURAL NETWORK\\nThis section ﬁrst describes the architecture and training\\ndetails of DF-CNN. To intuitively highlight the discrimina-\\ntive ability of DF-CNN, both the highly concentrated 32-\\ndimensional fused deep features and the expression-speciﬁc\\nsaliency maps are visualized.\\nA. DF-CNN: Architecture and Training\\nThe architecture of DF-CNN is formed by a feature extrac-\\ntion subnet, a feature fusion subnet, and a softmax layer. The\\nfeature extraction subnet is used to generate hierarchical and\\nover-completed facial representations (i.e., feature maps) foreach type of attribute maps. And the feature fusion subnet is\\nused to combine hundreds of feature maps from different types\\nof attribute maps into a highly concentrated deep feature. The\\nmain building blocks of feature extraction subnet include the\\nconvolutional layers and ReLU nonlinearity, while the reshape\\nlayer and fusion layers are main components of feature fusion\\nsubnet. The details of these components are introduced as\\nfollows:\\nConvolutional Layer and ReLU Nonlinearity. A con-\\nvolutional layer transforms a 3D volume of activation maps\\n(i.e., feature maps) to another through a set of learnable 3D\\nﬁlters. In particular, input a volume of activation maps of\\nthe previous layer Yl\\x0012RWl\\x001\\x02Hl\\x001\\x02Dl\\x001, andKl3D\\nﬁltersfWl\\nkgKl\\nk=1, each with size Wl\\nf\\x02Hl\\nf\\x02Dl\\x001, it outputs\\na 3D volume of activation maps Yl2RWl\\x02Hl\\x02Dlat layer\\nl. Let the convolutional stride be S, and the amount of zero\\npadding beP, then we have Wl= (Wl\\x001\\x00Wl\\nf+2P )=S+1,\\nHl= (Hl\\x001\\x00Hl\\nf+ 2P )=S+ 1, andDl=Kl. Thek-th 2D\\nactivation mapYl\\nkis denoted by\\nYl\\nk='(Wl\\nk\\x03Yl\\x001+bl\\nk); (7)\\nwherebl\\nk2Rdenotes the bias term of k-th ﬁlterWl\\nk,\\x03is\\nthe convolution operator, and 'is the rectiﬁed linear units\\n(ReLU):'(x) = max(0 ;x).\\nReshape Layer. This layer is used to concatenate all the\\n3D volumes of activation maps produced from all types of 2D\\nfacial attribute maps. Suppose DF-CNN has Lconvolutional\\nlayers in total, and acts on Ndifferent types of facial attribute\\nmaps, then the reshape layer operation is deﬁned as:\\nYL\\nRe=Reshape(fYL(Ii)gN\\ni=1) =[YL(I1)j;\\x01\\x01\\x01;jYL(IN)]\\n2RWL\\x02HL\\x02(K L\\x02N);\\n(8)\\nwhereIiisi-th type of facial attribute maps, and the notation\\n[\\x01j;\\x01\\x01\\x01;j\\x01]denotes the concatenation of 3D matrices along\\nfeature channel dimension.\\nFeature Channel Fusion Layer. This is a fully connected\\nlayer, which is used to fuse all the activation volumes extracted\\nfrom all types of facial attribute maps in feature channel\\ndimension. Let this feature channel fusion layer be the (L+1)-\\nth layer, and its input be the output of the reshape layer YL\\nRe,\\nwhich is fully connected with KL+1 3D ﬁltersfWL+1\\nkgKL+1\\nk=1,\\neach with size 1\\x021\\x02(KL\\x02N), then the output of this\\nlayer isYL+12RWL+1\\x02HL+1\\x02DL+1. HereWL+1 =WL,\\nHL+1=HL, andDL+1=KL+1. Thek-th 2D activation\\nmapYL+1\\nkis denoted by\\nYL+1\\nk='(WL+1\\nk\\x03YL\\nRe+bL+1\\nk); (9)\\nwherebL+1\\nk2Rdenotes the bias term of k-th ﬁlterWL+1\\nk.\\nThat is to say, to achieve an activation volume YL+1with\\nmuch smaller number of feature channels, the number of ﬁlters\\nKL+1 should be much smaller than the number of feature\\nchannels in the previous activation volume YL\\nRe.\\nSpatial Dimension Fusion Layer. This is also a fully\\nconnected layer, which is used to fuse the activation volume\\nYL+1in the height-width spatial dimension. Let this fusion\\nlayer be the (L+ 2) -th layer, and its input be the output\\nvolume of feature channel fusion layer YL+1, which is fully\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n6\\nFig. 3. The architecture of the proposed deep fusion convolutional neural network (DF-CNN). Six types of facial attribute maps of a textured 3D face model\\nare jointly fed into ﬁve feature convolutional layers (convolution + ReLU + Pooling), a reshape layer, a feature channel fusion layer, a spatial dimension\\nfusion layer, and a ﬁnal softmax layer. The sizes and numbers of input data, feature maps and ﬁlters are listed for each layer.\\nconnected with KL+2 3D ﬁltersfWL+2\\nkgKL+2\\nk=1, each with size\\nofWL+2\\nk2RWL+1\\x02HL+1\\x02KL+1, then the output activation\\nfeature of this layer is YL+22R1\\x021\\x02K L+2. Thek-th 2D\\nvalueYL+2\\nkis denoted by\\nYL+2\\nk='(WL+2\\nk\\x03YL+1+bL+2\\nk); (10)\\nwherebL+2\\nk2Rdenotes the bias term of k-th ﬁlterWL+2\\nk.\\nSoftmax Layer. GivenKpossible expression classes, the\\nsoftmax layer has Knodes denoted by pi, wherei=\\n1;2;\\x01\\x01\\x01;K.pispeciﬁes a discrete probability distribution\\nof expressions, therefore,PK\\ni=1pi= 1. LetYL+2be the\\noutput of spatial dimension fusion layer, and fWL+3\\nk2\\nR1\\x021\\x02KL+2gK\\nk=1beKweights fully connecting spatial dime-\\nsion fusion layer to softmax layer. Then the total input into a\\nsoftmax layer, denoted by YL+3, is\\nYL+3\\nk=WL+3\\nkYL+2+bL+3\\nk2R; (11)\\nthen we have\\npi=exp(YL+3\\nk)P6\\njexp(YL+3\\nj): (12)\\nThe predicted expression class ^iwould be\\n^i= arg maxipi: (13)\\nIn practice, considering that there are very limited numbers\\nof 3D face scans with expression labels, we use the con-\\nvolutional architecture and parameters of a pre-trained deep\\nCNN model to build and initialize the convolutional layers.\\nIn particular, we choose vgg-net-m [4] as the pre-trained deep\\nmodel since it performs well and involves moderate amount of\\nparameters. In principle, other pre-trained deep CNN models\\nor newly designed deep CNN models are also possible to be\\nused if enough numbers of training samples are available. The\\nparameters of fusion layers and softmax layer are randomly\\ninitialized. The detailed architecture of DF-CNN, including\\nthe sizes and numbers of ﬁlters and activation maps for each\\nlayer, is illustrated in Fig. 3.\\nAs shown in Figure 3, DF-CNN comprises ﬁve convolu-\\ntional layers, a reshape layer, two fusion layers, and a softmax\\nlayer. Moreover, ReLU neuron is used after all convolutional\\nlayers and feature fusion layers. The max pooling layer is used\\nfollowing the ﬁrst, second, and the ﬁfth convolutional layers.\\nAnd Local Response Normalization (LRN) layer is used before\\nthe ﬁrst and second pooling layers.\\nEach 2D facial attribute map is converted to color scale\\nand resized to 224\\x02224\\x023, and then all six types of\\nattribute maps of each textured 3D face scan are jointly fedinto feature extraction subnet of DF-CNN, generating six\\nactivation volumes, each with size of 6\\x026\\x02512. These six\\nactivation volumes are concatenated and reshaped into size\\nof6\\x026\\x023;072 by reshape layer (Reshape6 in Fig. 3).\\nThe reshaped activation volumes are fused by the following\\nfeature channel fusion layer (Fusion7 in Fig. 3), resulting in\\nan activation volume with size of 6\\x026\\x0232. This activation\\nvolume is further fused by spatial dimension fusion layer\\n(Fusion8 in Fig. 3), generating a highly concentrated facial\\nrepresentation (i.e., 32-dimensional fused deep feature). This\\nfusion layer is followed by another fully connected layer,\\nwhich outputs a 6-dimensional expression probability vector.\\nFinally, a softmax loss layer is used to train all the parameters\\nof DF-CNN based on the back-propagation algorithm.\\nDuring training, the weight decay parameter is set to 5e-\\n4. The learning rate and momentum parameters are set to\\n1e-4 and 0.9, respectively. The open source implementation\\nMatConvNet1is used to build DF-CNN. During testing, ex-\\npression label of a textured 3D face scan is predicted by\\ntwo ways: 1) training linear SVM classiﬁers using the highly\\nconcentrated 32-dimensional deep features (i.e., DF-CNN svm).\\n2) performing softmax prediction based on the 6-dimensional\\nvectors of expression probabilities (i.e., DF-CNN softmax ).\\nB. DF-CNN: Deep Feature Visualization\\nTo have an intuitive impression and gain insight into the\\ndiscriminative ability of DF-CNN, we visualize both the\\n“low-level” and “high-level” deep features extracted from the\\nﬁrst convolution layer and the last fusion layer of DF-CNN,\\nrespectively. Figure 3 shows that there are totally 96 3D\\nﬁlters in the ﬁrst convolution layer, thus we can generate\\n96 “low-level” feature maps for each type of facial attribute\\nmaps. Figure 4 illustrates 11 typical feature maps for each\\ntype of facial attribute maps of a textured 3D face scan with\\nhappiness expression. From this ﬁgure, we can see that diverse\\nfeature maps can be extracted from DF-CNN using different\\nﬁlters and different attribute maps. Moreover, each feature\\nmap looks similar to conventional gradient-like facial maps\\nextracted from the shadow handcrafted features (e.g., LBP and\\nGabor face maps in [25]). Such a large number of feature\\nmaps can comprehensively capture various expression-related\\nfacial shape or texture deformations, of course with very high\\ndimensions. Therefore, how to combine such a large number\\nof over-completed and redundant deep representations into a\\nsingle compact facial representation becomes the key issue to\\n1http://www.vlfeat.org/matconvnet/\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n7\\nFig. 4. Visualization of 11 typical feature maps of geometric and photometric facial attribute maps extracted from the ﬁrst convolution layer of DF-CNN.\\nFrom top to bottom are the feature maps for the geometry map, texture map, curvature map, and normal maps with components x,y, andz.\\n  \\nHappiness\\nSurprise\\nFear\\nAnger\\nDisgust\\nSadness\\n  \\nFear\\nHappiness\\nAnger\\nSadness\\nDisgust\\nSurprise\\n  \\nSadness\\nSurprise\\nHappiness\\nFear\\nAnger\\nDisgust\\n(a)\\nHandcrafted feature (b) Pre-trained deep feature (c) Fused deep feature by DF-CNN\\nFig.\\n5. Comparison of the clustering structures of t-SNE based 2-dimensional embedding of the handcrafted feature (i.e., Gabor), pre-trained deep feature\\n(i.e., vgg-net-m-conv5), and 32-dimensional fused deep feature learned by DF-CNN associated with six prototypical facial expressions.\\nbe solved. Fortunately, DF-CNN is designed to handle this\\nproblem, and providing us a high-level, low-dimensional, and\\nhigh discriminative facial representation.\\nTo highlight the high discriminative property DF-CNN, Fig-\\nure 5 visualizes the clustering structures of t-SNE [51] based\\n2-dimensional embedding of handcrafted feature, pre-trained\\ndeep feature, and 32-dimensional fused deep feature associated\\nwith six prototypical facial expressions. In particular, the same\\nfeatures (i.e., Gabor, vgg-net-m-conv5, and 32-dimensional\\nfused deep feature) are used as those in Section VI-B. Notice\\nthat for Gabor and vgg-net-m-conv5, the features of different\\nattribute maps are concatenated together (i.e., feature-level\\nfusion) to generate a single high-dimensional representation of\\neach textured 3D face scan. The feature dimensions of Gabor\\nandvgg-net-m-conv5 are 40,320 (6 attribute maps, each one is\\ndescribed as a 6,720-dimensional Gabor feature) and 110,592\\n(6 attribute maps, each one is described as feature maps with\\nsize 6\\x026\\x02512 ), respectively. Figure 5 shows that the\\n32-dimensional fused deep feature has an obvious clustering\\nstructure for different expression categories, while other two\\ntypes of features demonstrate large category-wised overlap-\\nping. This clearly indicates that the 32-dimensional fused deep\\nfeatures learned by DF-CNN has more discriminative powerto distinguish different expressions than handcrafted feature\\nGabor and pre-trained deep feature vgg-net-m.\\nC. DF-CNN: Saliency Map Visualization\\nSince different facial expressions relate to different ways of\\nlocal facial shape deformations, the importance weights of d-\\nifferent facial parts are generally quite different for expression\\npredicting as shown in [31], [69]. In this section, we show\\nthat the importance weights can be revealed by pixel-level\\nexpression related saliency maps of DF-CNN.\\nTo this end, we visualize the importance of each pixel for\\nits ﬁnal discrimination ability of different facial expressions.\\nFor example, for “happiness”, we visualize the saliency map\\nfor a textured 3D face by the importance of each image\\npixel contributing to the ﬁnal discrimination of “happiness”.\\nTo compute the saliency map of a textured 3D face scan\\nI\\x03=fIg;Ix\\nn;Iy\\nn;Iz\\nn;Ic;Itgw.r.t. an expression indexed by\\ne, we construct a score function for assigning this face to\\nexpressioneby:\\nS(I\\x03je;\\x02) =weTf(I\\x03;\\x02); (14)\\nwhere \\x02is the set of learned parameters (i.e., ﬁlters and\\nbiases) of DF-CNN, f(I\\x03;\\x02)is the 32-dimensional fused\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n8\\nFig. 6. Visualization of the DF-CNN based facial expression saliency maps.\\nFrom top to bottom rows: saliency maps for anger, disgust, fear, happiness,\\nsadness and surprise. The less important pixels are shown in dark blue.\\ndeep feature of I\\x03, andweis the weight of a trained SVM\\nclassiﬁer for expression eusingf(I\\x03;\\x02). Obviously, the high-\\ner value ofweTf(I\\x03;\\x02)implies higher conﬁdence in labeling\\nthis textured 3D face as expression e. We next compute the\\ngradient of score function in Eqn. (14) w.r.t. the input pixels:\\nG(xjI \\x03;e;\\x02) =X\\nI2I\\x03weT@f(I\\x03;\\x02)\\n@I(x)(15)\\nwherexdenotes any pixel of an attribute map.@f(I\\x03;\\x02)\\n@I(x)is\\nthe gradient of fused deep feature w.r.t. the attribute map I\\nat pixelx, which can be computed by the back-propagation\\nalgorithm of DF-CNN from the spatial dimension fusion layer.\\nIts absolute value jG(xjI \\x03;e;\\x02)j measures the importance\\nof pixelxin labeling Ias expression e. We call this term\\ncomputed over all pixels of all facial attribute maps of a\\ntextured 3D face scan as saliency map.\\nFigure 6 visualizes some examples of saliency maps for\\ndifferent expressions. The saliency map is re-scaled to [0,\\n1]. We visualize it by fusing the face texture map with a\\ndark blue background using the saliency map as weights. The\\nless important pixels are shown in dark blue in these maps.\\nWe observe some interesting phenomena from these maps.\\nFirst, mouth is the most salient facial part for discriminating\\nall these expressions of interest, particularly for sadness and\\nsurprise. Second, the distributions of those salient maps for all\\nexpressions are approximately consistent with the patterns of\\nfacial shape deformations, which may spread over the whole\\nfaces with different importance. These observations indicate\\nthat the proposed DF-CNN can provide a discriminative facial\\nrepresentation and can distinguish facial expressions using the\\ndiscriminative facial parts.\\nFig. 7. Samples of 2D texture maps of BU-3DFE database with different\\ngenders, ethnicities, ages, expressions (from left to right, anger, disgust, fear,\\nhappiness, sadness and surprise) and levels of expression intensity (from top\\nto bottom: level 1 to level 4).\\nVI. E XPERIMENTAL EVALUATION\\nTo evaluate the effectiveness of DF-CNN for multi-modal\\n2D+3D FER, we will compare its performance with popular\\nhandcrafted features, pre-trained deep features, ﬁne-tuned deep\\nfeatures, and state-of-the-art methods over three expression\\nsubsets of two 3D face datasets (i.e., BU-3DFE and Bospho-\\nrus). Finally, we will discuss the issues of feature extraction\\nwith or without parameter sharing, effectiveness of learning-\\nbased fusion, and optimality of linear SVM based expression\\nprediction.\\nA. Databases and Preprocessing\\nBU-3DFE Database. The BU-3DFE (Binghamton Uni-\\nversity 3D Facial Expression) Database [59] has been the\\nbenchmarking for static 3D FER [12]. It includes 100 subjects\\n(56 females and 44 males), with age ranging from 18 to 70\\nyears old, and with a variety of racial ancestries (e.g., White,\\nBlack, East-Asian). Each subject has 25 samples of seven\\nexpressions: one sample for neutral, and other 24 samples for\\nsix prototypical expressions (anger, disgust, fear, happiness,\\nsadness, and surprise), each includes four levels of intensity\\n(see Fig. 7). As a result, this database consists of 2,500\\n2D texture images and 2,500 geometric shape models. To\\nfairly compare DF-CNN with state-of-the-art methods, and to\\nvalidate the effectiveness of DF-CNN for samples with lower\\nlevels of expression intensity, the following two subsets are\\nused.\\n\\x0fBU-3DFE Subset I. This subset is the standard dataset\\nused for 3D FER. It contains 1,200 2D and 3D face\\npairs (i.e., 7,200 2D facial attribute maps) of 100 subjects\\nwith 6 prototypical expressions and two higher levels of\\nexpression intensity.\\n\\x0fBU-3DFE Subset II. This subset includes all samples\\nof BU-3DFE except the 100 neutral samples. It contains\\n2,400 2D and 3D face pairs (i.e., 14,400 2D facial\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n9\\nattribute maps) of 100 subjects with 6 prototypical ex-\\npressions of four levels of intensity. To our knowledge,\\nthe samples with lower levels of expression intensity have\\nnot been used for 3D FER.\\nBosphorus 3D Face Database. The Bosphorus 3D Face\\nDatabase [41] has been widely used for 3D face recognition\\nunder adverse conditions, 3D facial action unit detection, 3D\\nfacial landmarking, etc. It contains 105 subjects and 4,666\\npairs of 3D face models and 2D face images with different\\naction units, facial expressions, poses and occlusions. In\\nthis dataset, there are totally 65 subjects performing the six\\nprototypical expressions with near frontal view. Each person\\nhas only one 2D (or 3D) sample for each expression, resulting\\nin 390 2D and 3D face pairs. To better partition, we use the\\nfollowing subset for experimental evaluations.\\n\\x0fBosphorus Subset. It contains 360 2D and 3D face pairs\\n(i.e., 2,160 facial attribute maps) of 60 subjects with 6\\nprototypical expressions.\\nPreprocessing. We performed similar preprocessing for both\\nBU-3DFE subsets and Bosphorus subset. First, we used the It-\\nerative Closest Point algorithm for 3D face registration. Then,\\nwe performed nose detection, face cropping, re-sampling,\\nand projection procedures using the 3D face normalization\\nmethod proposed in [34]. Finally, we achieved the normalized\\n2D range images (i.e., geometry maps) with x,y, andz\\ncoordinates. Once we have geometry maps, other geometric\\nfacial attribute maps can be estimated according to the method\\nintroduced in Section IV. The 2D texture maps of BU-3DFE\\ndataset are generated by projecting 3D texture images with\\nlinear interpolation. Samples of preprocessed facial attribute\\nmaps of BU-3DFE database are shown in Fig. 2. And Figure 8\\nillustrates some samples of 2D texture images of Bosphorus\\nsubset.\\nB. Evaluation and comparison on BU-3DFE Subset I\\nExperimental Protocol. This experimental protocol is ﬁrst-\\nly used in [15] and has been proven to be more stable than the\\none used in [53]. In this protocol, 60 subjects, each with 12\\nsamples (i.e., 6 prototypical expressions with two higher levels\\nof intensity) are randomly selected from the BU-3DFE subset\\nI. That is to say, 720 textured 3D face scans (i.e., 4,320 2D\\nfacial attribute maps) are used. To achieve stable results, 1,000\\ntimes random and independent 54-versus-6-subject-partition\\nexperiments (1,000 times train and test sessions in total) are\\nperformed. For each partition, 648 textured 3D face scans of\\n54 subjects are used for training and 72 textured 3D face\\nscans of 6 subjects are used for testing. Different partitions are\\nindependently trained and tested, and the average expression\\nrecognition accuracy of all the 1,000 test sessions across all 6\\nprototypical expressions are reported for the ﬁnal evaluation.\\nIn particular, we use the remaining 40 subjects (i.e., 2,880\\n2D facial attribute maps) of BU-3DFE Subset I to train our\\nDF-CNN. Once DF-CNN is trained, it is then used to extract\\nthe 32-dimensional fused deep features of the other 60 subject-\\ns. These fused deep features are then used to train linear SVM\\nclassiﬁers for expression prediction using above 1,000 times\\n54-versus-6-subject-partition experiments (i.e., DF-CNN svm).TABLE II\\nCOMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED\\nFEATURES ON BU-3DFE S UBSET I.\\nMethod Ig Ix\\nn Iy\\nn Iz\\nn Ic It All\\nMS-LBP 76.47 76.77 77.87 76.41 77.70 71.65 81.74\\ndense-SIFT 80.29 79.97 82.35 80.95 80.28 75.56 83.16\\nHOG 81.89 82.09 80.58 81.81 77.95 78.11 83.74\\nGabor 77.95 78.80 81.97 81.10 81.65 80.36 84.72\\nDF-CNN svm - - - - - - 86.86\\nDF-CNN softmax - - - - - - 86.20\\nAlternatively, expression labels of the other 60 subjects are\\nalso predicted directly by the softmax layer of the trained\\nDF-CNN (i.e., DF-CNN Softmax ). It’s important to note that\\nresult of this one-time prediction is very close to (86.20% vs.\\n86.25%) the one achieved by predicting expression label using\\nmaximum value of the 6-dimensional expression probabilities\\nwith the same 1,000 times 54-versus-6 experimental protocol.\\n1)Comparison with handcrafted features: This para-\\ngraph compares the performance of DF-CNN with the ones\\nachieved by using handcrafted features. Four classical hand-\\ncrafted image features: MS-LBP, dense-SIFT, HOG, and Ga-\\nbor, which have been proven to be quite efﬁcient for both 2D\\nand 3D facial expression analysis, are employed for compar-\\nisons. Please refer to [25], [52], [22], and [25], respectively for\\nthe implementations of these features. When used for multi-\\nmodal 2D+3D FER, these features are ﬁrst extracted from each\\ntype of facial attribute maps, then respectively fed into linear\\nSVM2classiﬁer with default parameter of C. To achieve ﬁnal\\nresults, score-level fusion of SVM scores with sum rule is\\nused.\\nTable II shows the average expression recognition accu-\\nracies across all six expressions of four handcrafted fea-\\ntures, and the proposed DF-CNN on BU-3DFE subset I.\\nFrom Table II, we can conclude that: 1) Gabor and HOG\\ngenerally perform better than dense-SIFT and MS-LBP. In\\nparticular, Gabor achieves the highest fusion accuracy of\\n84.72%, which outperforms HOG, dense-SIFT, and MS-LBP\\nby 0.98%, 1.56%, and 2.98%, respectively. 2) For different\\nfacial attribute maps, normal maps (Ix\\nn,Iy\\nn, andIz\\nn) generally\\nperform better than others, and the fusion of all six attribute\\nmaps (i.e., All) achieves the best performance. These results\\nindicate that different facial attribute maps indeed contain large\\ncomplementary information for multi-modal 2D+3D FER. 3)\\nDF-CNN svmand DF-CNN softmax achieves similar and much\\nbetter results (86.86% vs. 86.20%) than handcrafted features.\\n2)Comparison with pre-trained deep features: This\\nparagraph compares the performance of DF-CNN with the\\nones achieved by using deep features extracted from three deep\\nmodels (i.e., caffe-alex, vgg-net-m, and vgg-net-16 ) pre-trained\\non the ImageNet database [4]. Notice that the convolutional\\nlayers of vgg-net-m is used to initialize our DF-CNN. Similar\\nto the case of handcrafted features, each type of facial attribute\\nmaps are separately fed into these pre-trained models to\\n2http://www.csie.ntu.edu.tw/ cjlin/liblinear/\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n10\\nTABLE III\\nCOMPARISON OF THE AVERAGE ACCURACIES WITH PRE -TRAINED DEEP\\nFEATURES ON BU-3DFE S UBSET I.\\nMethod Ig Ix\\nn Iy\\nn Iz\\nn Ic ItAll\\ncaffe-alex-conv5 77.53 78.87 81.50 78.71 80.83 81.40 83.74\\nvgg-net-m-conv5 80.38 80.37 81.68 81.23 79.23 82.14 84.22\\nvgg-net-16-conv5-3 81.72 78.55 83.06 81.25 76.95 78.46 83.78\\ncaffe-alex-full7 68.64 73.43 76.64 75.72 74.52 74.45 82.56\\nvgg-net-m-full7 73.34 74.99 77.51 76.77 68.81 70.93 81.56\\nvgg-net-16-full7 76.71 72.22 73.87 74.61 64.35 67.03 82.45\\nDF-CNN svm - - - - - - 86.86\\nDF-CNN softmax - - - - - - 86.20\\nTABLE IV\\nCOMPARISON OF THE AVERAGE CONFUSION MATRICES WITH GABOR AND\\nPRE-TRAINED DEEP FEATURE FOR ALL FACIAL ATTRIBUTE MAPS ON\\nBU-3DFE S UBSET I.\\nGabor (average accuracy = 84.72)\\n% AN DI FE HA SA SU\\nAN 85.53 1.67 0.93 0 11.88 0\\nDI 1.63 84.48 6.19 4.35 0 3.36\\nFE 3.56 6.24 65.98 12.70 4.34 7.18\\nHA 0 0.83 3.03 96.14 0 0\\nSA 18.70 0 1.20 0.95 79.15 0\\nSU 0 1.26 1.67 0 0.04 97.03\\nvgg-net-m-conv5 (average accuracy = 84.22)\\n% AN DI FE HA SA SU\\nAN 86.96 1.68 0.83 0 10.53 0\\nDI 1.88 80.43 8.47 4.29 0 4.93\\nFE 3.23 9.53 66.41 12.83 2.10 5.91\\nHA 0 0.25 3.48 96.27 0 0\\nSA 19.82 0 2.83 0.39 76.96 0\\nSU 0 0.04 1.67 0 0.01 98.28\\nDF-CNN svm(average accuracy = 86.86)\\n% AN DI FE HA SA SU\\nAN 82.08 3.60 2.42 0 11.90 0\\nDI 3.27 84.94 5.70 2.50 0 3.59\\nFE 1.84 5.28 79.24 8.33 0.81 4.50\\nHA 0 0 3.74 96.26 0 0\\nSA 12.63 0.10 5.56 0.53 81.18 0\\nSU 0 0.07 1.67 0 0.83 97.43\\nextract deep features, and then linear SVM classiﬁers are\\ntrained for expression classiﬁcation. The ﬁnal fusion results\\nare achieved by performing score-level fusion of SVM scores\\nwith sum rule. For comparisons, deep features extracted from\\nthe 5th convolutional layer (net-conv5) and the penultimate\\nfully connected layer (net-full7 ) are used for each type of facial\\nattribute maps.\\nTable III shows the average expression recognition accura-\\ncies of pre-trained deep features and DF-CNN on BU-3DFETABLE V\\nCOMPARISON OF THE AVERAGE ACCURACIES WITH FINE -TUNED DEEP\\nFEATURES ON BU-3DFE S UBSET I.\\nMethod Ig Ix\\nn Iy\\nn Iz\\nn Ic ItAll\\ncaffe-alex-ft-full7 svm 79.44 79.84 80.51 79.50 79.46 80.83 84.05\\nvgg-net-m-ft-full7 svm 79.68 82.85 82.15 80.30 82.01 81.62 84.85\\nvgg-net-16-ft-full7 svm 80.21 82.30 82.04 80.43 80.87 84.10 86.01\\ncaffe-alex-ft softmax 78.19 80.96 81.94 78.75 78.89 80.83 83.61\\nvgg-net-m-ft softmax 78.33 83.06 82.78 81.11 81.11 80.42 85.00\\nvgg-net-16-ft softmax 78.33 82.08 80.69 79.19 79.31 84.17 85.14\\nDF-CNN svm - - - - - - 86.86\\nDF-CNN softmax - - - - - - 86.20\\nsubset I. From Table III, we can ﬁnd that: 1) Different pre-\\ntrained deep features have different superiorities associated\\nwith different facial attribute maps. For example, vgg-net-\\n16-conv5-3 achieves the best score for Iy\\nn, while vgg-net-m-\\nconv5 performs best for Ix\\nn. 2) For the fusion scores, vgg-\\nnet-m-conv5 andcaffe-alex-full7 achieve slightly better results\\nthan others among pre-trained deep features. 3) The deep\\nfeatures extracted from convolutional layers (i.e., conv5) of\\npre-trained deep models generally perform much better than\\nthe ones extracted from fully connected layers (i.e., full7). 4)\\nOur method achieves consistently better results than all pre-\\ntrained deep features. Notice that the dimension of vgg-net-m-\\nconv5 for one type of facial attribute maps is 18,432, which\\nis much higher than the 32-dimensional fused deep feature\\nproduced by DF-CNN.\\nTable IV compares the average confusion matrices achieved\\nby Gabor feature, vgg-net-m-conv5 and DF-CNN svm. It can be\\nseen that DF-CNN svmoutperforms Gabor for all expressions\\nexcept anger (with a difference of 3.45%). It is worth noting\\nthat DF-CNN svmhas more powerful discriminative ability\\nto distinguish fear expression, promoting the accuracy upto\\n13.26% and 12.83% for Gabor feature and vgg-net-m-conv5.\\n3)Comparison with ﬁne-tuned deep models: To further\\ndemonstrate the effectiveness of DF-CNN, we also compared\\nit with ﬁne-tuned deep models. The same pre-trained deep\\nmodels: caffe-alex, vgg-net-m, and vgg-net-16 are used for\\nﬁne-tuning. For each pre-trained deep model, we keep the\\nnet architecture of all layers and parameters unchanged except\\nthe ﬁnal fully connected layer. In particular, since we have six\\nexpression classes, the ﬁlter weight with size of 1\\x021\\x024096\\x02\\n1000 is changed to 1\\x021\\x024096\\x026and randomly initialized,\\nand the corresponding 1000-dimensional bias vector is also\\nreplaced by a 6-dimensional zero vector. Then, we separately\\nﬁne-tune the pre-trained deep models using different facial\\nattribute maps, resulting in six ﬁne-tuned deep models for each\\npre-trained deep model. Finally, testing data associated with\\neach kind of attribute maps are fed into the corresponding\\nﬁne-tuned deep model for feature extraction. Similar to DF-\\nCNN, expression prediction for each kind of attribute maps\\nis achieved by the following two ways: 1) learning linear\\nSVM classiﬁers using the 4,096-dimensional ﬁne-tuned deep\\nfeatures (e.g., vgg-net-m-ft-full7 svm); 2) performing softmax\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n11\\nTABLE VI\\nCOMPARISON OF THE AVERAGE CONFUSION MATRICES WITH FINE -TUNED\\nDEEP MODEL FOR ALL FACIAL ATTRIBUTE MAPS ON BU-3DFE S UBSET I.\\nvgg-net-m-ft-full7 svm(average accuracy = 84.85)\\n% AN DI FE HA SA SU\\nAN 81.48 1.92 0.83 0 15.77 0\\nDI 1.95 81.91 8.58 3.14 0 4.42\\nFE 3.42 6.96 73.51 11.57 1.99 2.56\\nHA 0 0.77 3.48 95.74 0 0\\nSA 15.88 0.19 4.18 0 79.75 0\\nSU 0 0.83 1.67 0 0.78 96.72\\nvgg-net-16-ft-full7 svm(average accuracy = 86.01)\\n% AN DI FE HA SA SU\\nAN 86.19 2.52 0.83 0 10.45 0\\nDI 1.97 82.00 9.59 2.27 0 4.17\\nFE 2.73 7.42 74.38 12.99 0.79 1.68\\nHA 0 0.83 3.47 95.69 0 0\\nSA 16.21 0 4.29 0 79.50 0\\nSU 0 0 1.67 0 0.06 98.27\\nDF-CNN svm(average accuracy = 86.86)\\n% AN DI FE HA SA SU\\nAN 82.08 3.60 2.42 0 11.90 0\\nDI 3.27 84.94 5.70 2.50 0 3.59\\nFE 1.84 5.28 79.24 8.33 0.81 4.50\\nHA 0 0 3.74 96.26 0 0\\nSA 12.63 0.10 5.56 0.53 81.18 0\\nSU 0 0.07 1.67 0 0.83 97.43\\nprediction using the 6-dimensional ﬁne-tuned deep features of\\nexpression probabilities (e.g., vgg-net-m-ft softmax ). To achieve\\nfusion results, score-level fusion with sum rule are used for\\nboth cases.\\nTable V shows the average expression recognition accura-\\ncies of ﬁne-tuned deep features and DF-CNN on BU-3DFE\\nsubset I. From Table V, we can ﬁnd that: 1) Fusion of\\nmultiple facial attribute maps can also signiﬁcantly improve\\nthe accuracies for all ﬁne-tuned deep features. 2) Fine-tuned\\ndeep feature vgg-net-16 achieves signiﬁcantly better results for\\ntexture maps, and also achieves the highest accuracies (86.01%\\nand 85.14%) for both two prediction ways. This conclusion of\\ndeeper net performs better is consistent with the one in [4]. 3)\\nOur DF-CNN initialized by vgg-net-m still achieves the best\\nresults. It’s necessary to compare the results of Table III and\\nTable V. It’s easy to ﬁnd that signiﬁcant improvements have\\nbeen achieved from pre-trained to ﬁne-tuned deep features,\\nparticularly for the case of 4096-dimensional deep features\\nextracted from the penultimate fully connected layer (i.e.,\\nfull7). For example, the improvements are upto 16.52% for\\ncurvature maps and 17.07% for texture maps when considering\\nthe pre-trained and ﬁne-tuned vgg-net-16-full7.\\nTable VI compares the average confusion matrices achieved\\nby two ﬁne-tuned deep features: vgg-net-m-ft-full7 svm,vgg-net-\\n16-ft-full7 svm, and our DF-CNN svm. It’s not difﬁcult to see thatDF-CNN svmachieves consistent better results than vgg-net-m-\\nft-full7 svmfor all six expressions. It even achieves better results\\nthan vgg-net-16-ft-full7 svm, which is ﬁne-tuned from a much\\ndeeper pre-trained deep model. In particular, the superiority\\nfor fear expression is upto 4.86% .\\n4)Comparison with other methods: To comprehensively\\nevaluate the effectiveness of DF-CNN, we compared it with\\n18 state-of-the-art methods on BU-3DFE subset I. To give a\\nthoroughly analysis, four aspects, including the data modal-\\nity, expression feature, expression classiﬁer, and recognition\\naccuracy are compared in Table VII.\\n1)For data modality, we can see that all previous methods\\nreported their results using only 3D data exception of [24]\\nand [66]. It is worth noting that Li et al. [24] proposed\\na local feature-based multimodal 2D+3D FER method, and\\nstudied the complementarity between 2D and 3D features.\\nHowever, their fusion results were produced by handcrafted\\nfeature-level and score-level fusion schemes. In contrast, our\\nmethod can automatically combine different 3D geometric and\\n2D photometric maps into a single 32-dimensional fused deep\\nfeature.\\n2)For expression feature, one way is directly building\\nhistograms of surface geometric quantities, such as coordinates\\n(e.g., [66], [67]), normals (e.g., [24], [26], [67]), and curvatures\\n(e.g., [24], [26], [53], [66], [67]). Another way is extracting\\npopular handcrafted features (e.g., HOG, SIFT, LBP, DWT)\\nfrom depth maps (e.g., [1], [15], [36], [57]), normal maps (e.g.,\\n[23], [36], [57], [22]), or curvature maps (e.g., [36], [57], [63]).\\nAs mentioned in our introduction section, all these state-of-\\nthe-art works for 3D-FER are based on handcrafted expression\\nfeatures. In contrast, our method can learn highly concentrated\\nand discriminative facial representation (only 32-dimensional)\\nfrom six types of facial attribute maps.\\n3)For expression classiﬁer, SVM (e.g., [1], [15], [26]) is the\\nmost popular classiﬁer compared with others such as Neural\\nNetworks (NN), Maximal Likelihood (ML), Bayesian Belief\\nNet (BBN), multi-boosting, and Sparse Representation-based\\nClassiﬁer (SRC). It is worth noting that a majority of methods\\nare based on SVM classiﬁer with non-linear RBF kernel (e.g.,\\n[26], [57], [63], [67]) or using multiple kernel learning [23] to\\ncombine multiple high-dimensional features (e.g., normal-LBP\\nin [23]), while our results are based on linear SVM classiﬁer\\nwith default parameter.\\n4)For recognition accuracy, beneﬁting from the end-to-\\nend training framework of DF-CNN, the fused deep features\\nproduced by DF-CNN have strong discriminative ability to\\ndistinguish different expressions. In particular, our method\\n(DF-CNN svm) achieves the highest accuracy of 86.86% com-\\npared with all state-of-the-art methods using the same (or\\nvery similar [1]) experimental protocol. Notice that both the\\nexperimental protocol used in our paper [15] and the similar\\none used in [1] have been proved stable since the scores are\\nachieved by averaging 100 times independent 10-fold cross-\\nvalidation tests. It should be pointed out that directly compar-\\nisons of the two accuracy columns in Table VII are far from\\nfair since the results listed in the second column were achieved\\nbased on an unstable experimental protocol (i.e., 10-fold or\\n20-fold cross-validation) ﬁrstly used in [53]. For example, the\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n12\\nTABLE VII\\nCOMPARISON OF EXPRESSION FEATURES ,CLASSIFIERS ,AND ACCURACIES\\nWITH THE STATE -OF-THE-ART ON BU-3DFE S UBSET I (N OTICE THAT\\nTHE ACCURACIES IN THE LEFT COLUMN ARE ACHIEVED BY AVERAGING\\n100 ROUND INDEPENDENT 10-FOLD CROSS -VALIDATION TESTS ,WHILE\\nTHE ONES IN THE RIGHT COLUMN ARE ACHIEVED BY AVERAGING ONLY\\nONE OR TWO ROUND 10-FOLD CROSS -VALIDATION TESTS ).\\nMethods Data Feature Classiﬁer Accuracy\\nWang et al. [53] 3D curvatures/hist. LDA 61.79 83.60\\nSoyel et al. [44] 3D points/distance NN 67.52 91.30\\nSoyel et al. [45] 3D points/distance NN - 93.72\\nTang et al. [46] 3D points/distance LDA 74.51 95.10\\nTang et al. [47] 3D slopes, distance SVM - 87.10\\nMpiperis [35] 3D deformable model ML - 90.50\\nGong et al. [15] 3D depth/PAC SVM 76.22 -\\nBerretti et al. [1] 3D depth/SIFT SVM 77.54 -\\nMaalej et al. [33] 3D facial curvesmuiti-\\nboosting-98.81\\n92.75\\nLi et al. [26] 3D normals, curv./hist. SVM 82.01 -\\nLi et al. [23] 3D normals/LBP MKL 80.14 -\\nLemaire [22] 3D curvature/HOG SVM 76.61 -\\nOcegueda [36] 3Dcoordinates, normals\\ncurvatures/DWTLogistic\\nReg.- 90.40\\nZeng et al. [63] 3D curvatures/LBP SRC 70.93 -\\nZhen et al. [67] 3Dcoordinates, normals,\\nshape indexSVM 84.50 -\\nYang et al. [57] 3Ddepth, normals,\\ncurv./scatteringSVM 84.80 -\\nZhao et al. [66] 2D+3Dintensity,coordinates,\\nshape index/LBPBBN - 82.30\\nLi et al. [24] 2D+3DmeshHOG/SIFT\\nmeshHOS/HSOGSVM 86.32 -\\nDF-CNN svm 2D+3D 32-D deep feature SVM 86.86 -\\nDF-CNN softmax 2D+3D 6-D deep feature Softmax 86.20 -\\naccuracy of [33] is reduced from 98.81% to 92.75% when\\nusing 20-fold instead of 10-fold cross-validation. As produced\\nby Gong et al. [15], the accuracies of [53], [44], [46] were\\ndropped signiﬁcantly (more than 20%) when using a more\\nstable experimental protocol. Overall, different from state-\\nof-the-art methods, the proposed DF-CNN combines feature\\nlearning and fusion learning into a single end-to-end training\\nframework, and achieves the best accuracy for multimodal\\n2D+3D FER under the more stable experimental protocol.\\nC. Evaluation and comparison on other datasets\\nThis section will show more experimental results evaluated\\non BU-3DFE subset II and Bosphorus subset.\\nExperimental Protocol. To get more training data and to\\nreduce the effect of data bias for DF-CNN training, we used\\nthe standard 10-fold cross-validation (10 train and test ses-\\nsions) experimental setting. That is, different DF-CNNs should\\nbe trained for different sessions, and the average recognition\\naccuracies of 10 different DF-CNNs across all six prototypical\\nexpressions are reported for evaluations and comparisons. InTABLE VIII\\nCOMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED\\nFEATURES ,PRE-TRAINED DEEP FEATURES ,AND FINE -TUNED DEEP\\nFEATURES ON BU-3DFE S UBSET II.\\nMethod Ig Ix\\nn Iy\\nn Iz\\nn Ic ItAll\\nMS-LBP 73.50 74.58 73.54 73.21 73.37 66.08 77.75\\ndense-SIFT 76.25 75.79 77.42 76.58 75.88 71.79 79.42\\nHOG 76.25 76.88 76.29 77.75 76.29 72.04 79.71\\nGabor 73.04 75.00 78.29 76.42 76.33 75.86 80.00\\nvgg-net-m-conv5 76.17 75.04 76.92 76.54 75.54 76.42 79.75\\nvgg-net-m-full7 70.21 69.71 72.67 70.67 67.00 66.83 77.38\\nvgg-net-m-ft-full7 svm 75.17 76.62 77.08 75.83 78.12 78.67 81.08\\nvgg-net-m-ft softmax 74.62 75.33 76.96 75.79 77.88 78.54 80.71\\nDF-CNN svm - - - - - - 81.04\\nDF-CNN softmax - - - - - - 81.33\\nparticular, for BU-3DFE subset II, 100 subjects are randomly\\ndivided into 10 subsets, and for each session, 12,960 attribute\\nmaps of 90 subjects are used for training and the remaining\\n1,440 attribute maps of 10 subjects are used for testing.\\nSimilarly, for Bosphorus subset, 60 subjects are randomly\\ndivided into 10 subsets, and for each session, 1,944 attribute\\nmaps of 54 subjects are used for training and the remaining\\n216 attribute maps of 6 subjects are used for testing.\\n1)Results on BU-3DFE subset II: Table VIII reports\\nthe performance comparisons of the proposed DF-CNN with\\nhandcrafted features, pre-trained deep features, and ﬁne-tuned\\ndeep features on BU-3DFE Subset II. From this table, we can\\nconclude that: 1) As before, Gabor feature still achieves the\\nbest results among handcrafted features. It even slightly out-\\nperforms the pre-trained deep feature vgg-net-m-conv5 (80%\\nvs. 79.75%). 2) Fine-tuned deep features achieve signiﬁcantly\\nbetter results than pre-trained deep features, e.g., 81.08% for\\nvgg-net-m-ft-full7 vs. 77.38% for vgg-net-m-full7. 3) Our DF-\\nCNN based methods achieve comparable (81.04% vs. 81.08%)\\nor slightly better (81.33% vs. 81.08%) results compared with\\nﬁne-tuned deep features. It is worth noting that for each train\\nand test session, DF-CNN only needs to train a single CNN\\nfor both feature learning and feature fusion, while ﬁne-tuned\\ndeep feature based method needs to respectively train different\\ndeep models for different types of facial attribute maps, and\\nrespectively extract ﬁne-tuned deep features from different\\ndeep models and combine all scores by hand. This leads to\\nmuch more consumptions of training time and parameter space\\ncompared with DF-CNN. Notice that the results of two BU-\\n3DFE subsets clearly indicate that the samples with lower\\nlevels of expression intensity are indeed much more difﬁcult\\nto be recognized than the higher level ones.\\n2)Results on Bosphorus subset: Table IX reports the\\nperformance comparisons of the proposed DF-CNN with\\nhandcrafted features, pre-trained deep features, and ﬁne-tuned\\ndeep features on Bosphorus subset. Similar to the conclusions\\nachieved on BU-3DFE subset I and subset II, we have: 1)\\nGabor feature achieves the highest accuracy of 77.50% among\\nhandcrafted features. 2) Fine-tuned deep feature (i.e., vgg-\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n13\\nFig. 8. Six pairs of 2D texture images with fear and surprise expressions of\\nBosphorus database. It’s not easy even for humans to distinguish these fear\\nand surprise pairs illustrated in this ﬁgure.\\nTABLE IX\\nCOMPARISON OF THE AVERAGE ACCURACIES WITH HANDCRAFTED\\nFEATURES PRE -TRAINED DEEP FEATURES ,AND FINE -TUNED DEEP\\nFEATURES ON BOSPHORUS SUBSET .\\nMethod Ig Ix\\nn Iy\\nn Iz\\nn Ic ItAll\\nMS-LBP 71.11 69.44 70.56 66.67 62.78 62.50 73.33\\ndense-SIFT 70.28 73.89 72.78 73.89 72.50 65.56 76.39\\nHOG 72.50 74.22 73.89 74.72 71.94 71.94 77.22\\nGabor 67.78 73.61 75.83 71.61 75.56 70.56 77.50\\nvgg-net-m-conv5 71.94 72.50 73.61 71.67 72.78 73.06 79.72\\nvgg-net-m-full7 61.11 63.33 63.89 65.83 60.56 61.94 75.56\\nvgg-net-m-ft-full7 svm 71.67 72.78 74.72 76.11 71.94 73.61 79.17\\nvgg-net-m-ft softmax 71.39 72.78 75.28 75.00 73.33 73.61 79.72\\nDF-CNN svm - - - - - - 80.28\\nDF-CNN softmax - - - - - - 80.00\\nnet-m-ft-full7 ) also signiﬁcantly outperforms the pre-trained\\none (i.e., vgg-net-m-full7 ). Note that although the pre-trained\\ndeep feature vgg-net-m-conv5 achieves the same accuracy of\\n79.72% as the ﬁne-tuned deep feature vgg-net-m-ft softmax , the\\nfeature dimension is much higher (18; 432\\x026vs. 6). 3) Our\\nDF-CNN based methods achieve slightly better results com-\\npared with the ﬁne-tuned deep features. Overall, Bosphorus\\nsubset is the most difﬁcult dataset among the three subsets\\nused in this paper.\\n3)Comparison with other methods: To compare the\\nperformance of the proposed DF-CNN with other methods\\non BU-3DFE subset II and Bosphorus subset, we reproduced\\nTABLE X\\nCOMPARISON WITH THE STATE -OF-THE-ART ON THE BU-3DFE S UBSET\\nIIAND BOSPHORUS SUBSET .\\nMethod BU-3DFE Subset II Bosphorus subset\\nLi et al. (2012) [23] 78.50 75.83\\nLi et al. (2015) [24] 80.42 79.72\\nYang et al. (2015) [57] 80.46 77.50\\nDF-CNN svm 81.04 80.28\\nDF-CNN softmax 81.33 80.00three state-of-the-art methods (i.e., [23], [24], and [57]) on\\nthese two datasets using the same experimental protocol (i.e.,\\n10-fold cross-validation with the same subjects for training\\nand testing in each train and test session) as DF-CNN. In\\nparticular, [23] and [24] are two of our previous methods.\\nResults of [57] were reproduced using the code shared by the\\nauthors. Notice that multiple kernel learning was used in [23],\\nnon-linear SVM was used in [24] and [57] for expression\\nprediction, respectively. For fair comparison, the non-linear\\nSVM was replaced by linear SVM classiﬁer, and the sum rule\\nbased score-level fusion was used for [24] and [57].\\nTable X reports the performance comparisons of DF-CNN\\nwith state-of-the-art methods [23], [24] and [57] on both BU-\\n3DFE subset II and Bosphorus subset. From this table, we\\ncan see that method [23] achieves the lowest accuracy on\\nboth subsets. Methods [24] and [57] achieve very similar\\nresults (80.42% vs. 80.46%) on BU-3DFE subset II, while\\nmethod [24] performs better by 2.22% on Bosphorus subset.\\nOur DF-CNN achieves the best results on both two sub-\\nsets. Similar to the case on BU-3DFE subset I, DF-CNN\\nhas signiﬁcant superiority to distinguish fear expression. For\\nexample, on the Bosphorus subset, DF-CNN svmachieves an\\naverage recognition rate of 65% for fear expression, which is\\nmuch higher than the results of 36.67%, 51.67%, and 43.33%\\nachieved by [23], [24], and [57], respectively. It is worth noting\\nthat distinguishing the samples of Bosphorus subset with fear\\nexpression and surprise expression is a very difﬁcult task even\\nfor humans as illustrated in Fig. 8. From this ﬁgure, we can\\nsee that there only exist very subtle differences between fear\\nand surprise pairs of the same person.\\nOverall, the proposed DF-CNN uniﬁes feature learning and\\nfusion learning into a single end-to-end training framework,\\nand performs better than handcrafted features, pre-trained deep\\nfeatures, ﬁne-tuned deep features, and state-of-the-art methods,\\nresulting in a good generalization ability on BU-3DFE subset\\nII and Bosphorus subset for multimodal 2D+3D FER.\\nD. Discussion\\nTo further validate the effectiveness of DF-CNN, three\\nissues: feature extraction with or without parameter sharing,\\neffectiveness of learning-based fusion, and optimality of lin-\\near SVM based expression prediction are discussed in this\\nparagraph. Noting that all the following discussions are based\\non BU-3DFE subset I and the corresponding experimental\\nprotocol introduced in section VI-B.\\n1)Feature extraction with or without parameter shar-\\ning: As shown in Fig. 1, the CNN parameters are shared\\nfor different types of facial attribute maps in the feature\\nextraction subnet of DF-CNN. Alternatively, different attribute\\nmaps can also been separately fed into different CNNs for\\nfeature fusion, then adding the following feature fusion and\\nexpression prediction layers. Clearly, the latter one (namely\\nDF-CNNa) needs to learn more parameters and thus perhaps\\nperforms better. In Table XI, we compared the parameter\\nquantity, compute time, and accuracy between DF-CNN and\\nDF-CNNa. We can see that, comparing with DF-CNN, DF-\\nCNNahas much more parameters (50M vs. 300M) and thus\\nruns more slowly (7.4 Hz vs. 4.1 Hz). However, DF-CNN still\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n14\\nachieves slightly better results than DF-CNNa. This might be\\ndue to that we used very limited number of training samples to\\ntrain DF-CNN and DF-CNNa. Therefore, we guess that if one\\nhas sufﬁcient training samples available, DF-CNNastill has a\\nlarge potential to outperform DF-CNN in general but needs to\\nlearn more parameters, and to take more training time.\\nTABLE XI\\nCOMPARISON OF DF-CNN AND DF-CNNa(WITHOUT PARAMETER\\nSHARING ,I.E.,DIFFERENT ATTRIBUTE MAPS CORRESPONDING TO\\nDIFFERENT FEATURE EXTRACTION SUBNETS )ONBU-3DFE SUBSET I.\\nMethod Parameter Time/epoch Accuracy\\nDF-CNN svm '50 MB 7.4 Hz 86.86\\nDF-CNN softmax '50 MB 7.4 Hz 86.20\\nDF-CNNa\\nsvm '300 MB 4.1 Hz 86.48\\nDF-CNNa\\nsoftmax'300 MB 4.1 Hz 85.97\\n2)Effectiveness of learning-based fusion: To show the\\neffectiveness of learning-based fusion, we compared DF-CNN\\nwith two popular classiﬁers: linear SVM with score-level\\nfusion and multiple kernel learning (MKL) with kernel-level\\nfusion. Deep CNN features DF-CNN-in-conv5 andDF-CNN-\\nft-conv5 are respectively extracted from the feature extraction\\nsubnet of DF-CNN with initialized and ﬁne-tuned CNN pa-\\nrameters. From Table XII, we can see that MKL with kernel-\\nlevel fusion achieves better results than liner SVM with score-\\nlevel fusion in both cases. Our ﬁne-tuned DF-CNN, which\\ncombines feature learning and fusion learning in a single end-\\nto-end training framework, achieves signiﬁcant better results\\nthan liner SVM and MKL.\\nMoreover, to see the effect of feature fusion subnet, we ﬁxed\\nall the initialized CNN parameters of the feature extraction\\nsubnet, and only learned the parameters of the following\\nfeature fusion subnet. This is equivalent to learn hierarchical\\nfusion weights to combine the high-dimensional pre-trained\\ndeep features. From Table XII, we can see that this kind\\nof pure fusion learning-based DF-CNN can achieve slightly\\nbetter results (84.79% vs. 84.22%) than linear SVM with\\nhandcrafted score-level fusion, but signiﬁcantly worse than\\nthe proposed DF-CNN. This indicates that the combination\\nof feature learning and fusion learning into a single end-to-\\nend training framework is very important for the proposed\\nDF-CNN.\\nTABLE XII\\nCOMPARISON OF THE LEARNING BASED FUSION STRATEGY (DF-CNN)\\nWITH OTHERS ON BU- SUBSET I.\\nFeature and\\nfusionlinear SVM\\n(score-level)MKL\\n(kernel)DF-CNN svm\\n(learning-based)DF-CNN softmax\\n(learning-based)\\nDF-CNN-in-\\nconv584.22 85.07 84.79 83.90\\nDF-CNN-ft-\\nconv584.17 85.73 86.86 86.20\\n3)Optimality of Linear SVM based prediction: To\\nvalidate the optimality of using linear SVM classiﬁer for\\nexpression prediction, we compared it with ﬁve popular clas-siﬁers: logistic regression, k-Nearest Neighbor, naive bayes,\\nrandom forests, and rbf-kernel SVM. All experiments were\\ncarried out on BU-3DFE subset I based on the 1,000 times 54-\\nvs-6 experimental setting, and using the 32-dimensional fused\\ndeep features produced by DF-CNN. The hyper-parameters of\\nthese classiﬁers (e.g., the value of kin k-Nearest Neighbor,\\nnumber of trees in random forests, and \\rin rbf-kernel SVM)\\nwere carefully selected by cross-validation on the training set\\nof each train session. In contrast, the parameter Cin linear\\nSVM was set to be the default value 1 for all 1,000 times\\ntrain sessions.\\nTable XIII reports the comparison results. We can see that:\\n1) All classiﬁers achieve comparable results except logistic re-\\ngression, which indicates again that the 32-dimensional fused\\ndeep feature is very discriminative. 2) Among all classiﬁers,\\nlinear SVM has obvious advantages in both accuracy and\\nspeed (without parameter tuning). Therefore, linear SVM is\\ngenerally the best candidate classiﬁer for expression prediction\\nusing fused deep features produced by DF-CNN.\\nTABLE XIII\\nCOMPARISON OF DIFFERENT CLASSIFIERS OVER THE 32-DIMENSIONAL\\nDEEP FEATURES EXTRACTED FROM DF-CNN ONBU-3DFE SUBSET I.\\nClassiﬁer Logistic\\nRegres.k-Nearest\\nNeighborNaive\\nBayesRandom\\nForestskernel\\nSVMlinear\\nSVM\\nAccuracy (%) 81.03 85.84 85.90 85.18 86.76 86.86\\nFinally, it is worth noting that we have also studied the\\nissue of optimal dimension for the fused deep feature produced\\nby DF-CNN. Our experimental results indicate that the 32-\\ndimensional fused deep feature can achieve slightly better\\nresults than both 16-dimensional and 64-dimensional fused\\ndeep features.\\nVII. C ONCLUSION AND FUTURE WORK\\nThis paper presents a novel deep fusion convolution neu-\\nral network (DF-CNN) for subject-independent multi-modal\\n2D+3D FER. DF-CNN comprises a feature extraction subnet,\\na feature fusion subnet, and a softmax-loss layer. Each textured\\n3D face scan is ﬁrstly represented as six types of facial\\nattribute maps, all of which are then jointly fed into DF-CNN\\nfor feature extraction and feature fusion, resulting in a highly\\nconcentrated facial representation. Expression prediction is\\nperformed by two ways: 1) learning linear SVM classiﬁers\\nusing the 32-dimensional fused deep features; 2) directly\\nperforming softmax prediction using the 6-dimensional ex-\\npression probabilities. Different from existing methods for 3D\\nFER, DF-CNN combines feature learning and fusion learning\\ninto a single end-to-end training framework. To demonstrate\\nthe effectiveness of DF-CNN, we conducted comprehensive\\nexperiments to compare the performance of DF-CNN with\\nhandcrafted features, pre-trained deep features, ﬁne-tuned deep\\nfeatures, and the state-of-the-art methods on three subsets of\\ntwo popular 3D face datasets (i.e., BU-3DFE and Bosphorus).\\nIn all cases, DF-CNN consistently achieves the best results.\\nBoth visualization and quantiﬁcation results indicate that the\\n32-dimensional fused deep feature of DF-CNN has strong dis-\\ncriminative ability to distinguish different facial expressions.\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n15\\nIn the future, some other issues of DF-CNN such as how to\\nchoose the optimal pre-trained deep CNN for initialization, and\\nthe optimal loss function for training will be studied. More-\\nover, we will also study to extend current DF-CNN frame-\\nwork to multi-modal 2D+3D video based facial expression\\nrecognition, or other multi-modal facial emotion prediction\\nproblems such as action unit detection and expression intensity\\nestimation.\\nACKNOWLEDGMENT\\nHuibin Li was supported in part by the NSFC under grant\\n11401464, Chinese Postdoctoral Science Foundation under\\ngrant 2014M560785, and International Exchange Founda-\\ntion of China NSFC and United Kingdom RS under grant\\n61711530242. Jian Sun was supported in part by the NSFC\\nunder grants 61472313 and 11622106. Liming Chen was sup-\\nported in part by the French Research Agency, l’Agence Na-\\ntionale de Recherche (ANR), through the Jemime project (N\\x0e\\ncontract ANR-13-CORD-0004-02) and the Biofence project\\n(N\\x0econtract ANR-13-INSE-0004-02) and the PUF 4D Vision\\nproject funded by the Partner University Foundation.\\nREFERENCES\\n[1] S. Berretti, A. Bimbo, P. Pala, B. Amor, and M. Daoudi. A set of\\nselected sift features for 3d facial expression recognition. In 20th Int.\\nConference on Pattern Recognition, pages 4125–4128, 2010.\\n[2] P. Burkert, F. Trier, M. Z. Afzal, A. Dengel, and M. Liwicki. Dex-\\npression: Deep convolutional neural network for expression recognition.\\nCoRR, abs/1509.05371, 2015.\\n[3] R. A. Calix, S. A. Mallepudi, B. Chen, and G. M. Knapp. Emotion\\nrecognition in text for 3-d facial expression rendering. IEEE Transac-\\ntions on Multimedia, 12(6):544–551, Oct 2010.\\n[4] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of\\nthe devil in the details: Delving deep into convolutional nets. In British\\nMachine Vision Conference, 2014.\\n[5] W. S. Chu, F. de la Torre, and J. Cohn. Selective transfer machine for\\npersonalized facial expression analysis. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, PP(99):1–1, 2016.\\n[6] C. A. Corneanu, M. Oliu, J. F. Cohn, and S. Escalera. Survey on rgb, 3d,\\nthermal, and multimodal approaches for facial expression recognition:\\nHistory, trends, and affect-related applications. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, PP(99):1–1, 2016.\\n[7] M. Dahmane and J. Meunier. Emotion recognition using dynamic\\ngrid-based hog features. In Automatic Face Gesture Recognition and\\nWorkshops (FG 2011), 2011 IEEE International Conference on, pages\\n884–888, March 2011.\\n[8] M. Dahmane and J. Meunier. Prototype-based modeling for facial\\nexpression analysis. IEEE Transactions on Multimedia, 16(6):1574–\\n1584, Oct 2014.\\n[9] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static facial expression\\nanalysis in tough conditions: Data, evaluation protocol and benchmark.\\nInComputer Vision Workshops (ICCV Workshops), 2011 IEEE Interna-\\ntional Conference on, pages 2106–2112, Nov 2011.\\n[10] A. Dhall, S. Member, S. Lucey, and T. Gedeon. Collecting large, richly\\nannotated facial-expression databases from movies, 2012.\\n[11] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and\\nT. Darrell. Decaf: A deep convolutional activation feature for generic\\nvisual recognition, 2014.\\n[12] T. Fang, X. Zhao, O. Ocegueda, S. Shah, and I. Kakadiaris. 3d facial\\nexpression recognition: A perspective on promises and challenges. In\\nIEEE International Conference on Automatic Face Gesture Recognition\\nand Workshops, pages 603–610, 2011.\\n[13] T. Fang, X. Zhao, O. Ocegueda, S. K. Shah, and I. A. Kakadiaris. 3d/4d\\nfacial expression analysis: An advanced annotated face model approach.\\nImage and Vision Computing, 30(10):738 – 749, 2012.\\n[14] J. Goldfeather and V . Interrante. A novel cubic-order algorithm for ap-\\nproximating principal direction vectors. ACM Transactions on Graphics\\n(TOG), 23(1):45–63, 2004.[15] B. Gong, Y . Wang, J. Liu, and X. Tang. Automatic facial expression\\nrecognition on a single 3d face by exploring shape deformation. In\\nProceedings of the 17th ACM international conference on Multimedia,\\npages 569–572, 2009.\\n[16] M. Hayat and M. Bennamoun. An automatic framework for textured\\n3d video-based facial expression recognition. IEEE Transactions on\\nAffective Computing, 5(3):301–313, July 2014.\\n[17] R. Hoffman and A. K. Jain. Segmentation and classiﬁcation of\\nrange images. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 9(5):608–620, 1987.\\n[18] Y . Huang, Y . Li, and N. Fan. Robust symbolic dual-view facial\\nexpression recognition with skin wrinkles: Local versus global approach.\\nIEEE Transactions on Multimedia, 12(6):536–543, Oct 2010.\\n[19] S. E. Kahou, X. Bouthillier, P. Lamblin, C. Gulcehre, V . Michal-\\nski, K. Konda, S. Jean, P. Froumenty, Y . Dauphin, N. Boulanger-\\nLewandowski, and et al. Emonets: Multimodal deep learning approaches\\nfor emotion recognition in video. Journal on Multimodal User Inter-\\nfaces, 10(2):99–111, 2016.\\n[20] P. Khorrami, T. L. Paine, and T. S. Huang. Do deep neural networks\\nlearn facial action units when doing expression recognition? CoRR,\\nabs/1510.02969, 2015.\\n[21] B.-K. Kim, H. Lee, J. Roh, and S.-Y . Lee. Hierarchical committee\\nof deep cnns with exponentially-weighted decision fusion for static\\nfacial expression recognition. In Proceedings of the 2015 ACM on\\nInternational Conference on Multimodal Interaction, ICMI ’15, pages\\n427–434, 2015.\\n[22] P. Lemaire, L. Chen, M. Ardabilian, and M. Daoudi. Fully automatic\\n3d facial expression recognition using differential mean curvature maps\\nand histograms of oriented gradients. In Workshop 3D Face Biometrics,\\nIEEE Automatic Facial and Gesture Recognition, FG’13, 2013.\\n[23] H. Li, L. Chen, D. Huang, Y . Wang, and J.-M. Morvan. 3d facial\\nexpression recognition via multiple kernel learning of multi-scale local\\nnormal patterns. In 21st International Conference on Pattern Recogni-\\ntion (ICPR), pages 2577–2580, 2012.\\n[24] H. Li, H. Ding, D. Huang, Y . Wang, X. Zhao, J.-M. Morvand, and\\nL. Chen. An efﬁcient multimodal 2d + 3d feature-based approach to\\nautomatic facial expression recognition. Computer Vision and Image\\nUnderstanding, 140:83 – 92, 2015.\\n[25] H. Li, D. Huang, L. Chen, and Y . Wang. A group of facial normal\\ndescriptors for recognizing 3d identical twins. In IEEE Fifth Inter-\\nnational Conference on Biometrics: Theory, Applications and Systems,\\npages 271–277, 2012.\\n[26] H. Li, J.-M. Morvan, and L. Chen. 3d facial expression recognition based\\non histograms of surface differential quantities. Advances Concepts for\\nIntelligent Vision Systems, pages 483–494, 2011.\\n[27] K. Li, Q. Dai, R. Wang, Y . Liu, F. Xu, and J. Wang. A data-driven\\napproach for facial expression retargeting in video. IEEE Transactions\\non Multimedia, 16(2):299–310, Feb 2014.\\n[28] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep networks for\\nfacial expression feature learning. Neurocomputing, 159:126 – 136,\\n2015.\\n[29] M. Liu, S. Li, S. Shan, R. Wang, and X. Chen. Deeply Learning De-\\nformable Facial Action Parts Model for Dynamic Expression Analysis,\\npages 143–157. Springer International Publishing, Cham, 2015.\\n[30] P. Liu, S. Han, Z. Meng, and Y . Tong. Facial expression recognition\\nvia a boosted deep belief network. In Computer Vision and Pattern\\nRecognition (CVPR), 2014 IEEE Conference on, pages 1805–1812,\\n2014.\\n[31] P. Liu, J. T. Zhou, I. W.-H. Tsang, Z. Meng, S. Han, and Y . Tong. ECCV\\n2014: 13th European Conference, Zurich, Switzerland, September 6-12,\\n2014, Proceedings, Part IV, chapter Feature Disentangling Machine -\\nA Novel Approach of Feature Selection and Disentangling in Facial\\nExpression Analysis, pages 151–166. Springer International Publishing,\\nCham, 2014.\\n[32] A. Maalej, B. B. Amor, M. Daoudi, A. Srivastava, and S. Berretti.\\nLocal 3d shape analysis for facial expression recognition. In 2010 20th\\nInternational Conference on Pattern Recognition, pages 4129–4132, Aug\\n2010.\\n[33] A. Maalej, B. B. Amor, M. Daoudi, A. Srivastava, and S. Berretti. Shape\\nanalysis of local facial patches for 3d facial expression recognition.\\nPattern Recognition, 44(8):1581 – 1589, 2011.\\n[34] A. Mian, M. Bennamoun, and R. Owens. Automatic 3d face detection,\\nnormalization and recognition. In In 3DPVT, pages 735–742, 2006.\\n[35] I. Mpiperis, S. Malassiotis, and M. Strintzis. Bilinear models for 3-d face\\nand facial expression recognition. IEEE Transactions on Information\\nForensics and Security, 3(3):498 –511, sept. 2008.\\n[36] O. Ocegueda, T. Fang, S. K. Shah, and I. A. Kakadiaris. Expressive maps\\nfor 3d facial expression recognition. In IEEE International Conference\\non Computer Vision Workshops(ICCV), pages 1270–1275, 2011.\\n1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2713408, IEEE\\nTransactions on Multimedia\\n16\\n[37] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of facial\\nexpressions: The state of the art. IEEE Trans. Pattern Anal. Mach.\\nIntell., 22(12):1424–1445, 2000.\\n[38] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. CNN\\nfeatures off-the-shelf: an astounding baseline for recognition. CoRR,\\nabs/1403.6382, 2014.\\n[39] S. Rifai, Y . Bengio, A. Courville, P. Vincent, and M. Mirza. Disentan-\\ngling factors of variation for facial expression recognition. In Computer\\nVision C ECCV 2012, volume 7577 of Lecture Notes in Computer\\nScience, pages 808–822. 2012.\\n[40] G. Sandbach, S. Zafeiriou, M. Pantic, and L. Yin. Static and dynamic\\n3d facial expression recognition: A comprehensive survey. Image and\\nVision Computing, 2012. in press.\\n[41] A. Savran, N. Aly ¨uz, H. Dibeklio ˘glu, O. C ¸ eliktutan, B. G ¨okberk,\\nB. Sankur, and L. Akarun. Bosphorus Database for 3D Face Analysis,\\npages 47–56. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008.\\n[42] A. Savran, B. Sankur, and M. T. Bilge. Facial action unit detection:\\n3d versus 2d modality. In IEEE Computer Society Conference on\\nComputer Vision and Pattern Recognition Workshops (CVPRW), pages\\n71–78, 2010.\\n[43] M. Song, Z. Dong, C. Theobalt, H. Wang, Z. Liu, and H. P. Seidel. A\\ngeneric framework for efﬁcient 2-d and 3-d facial expression analogy.\\nIEEE Transactions on Multimedia, 9(7):1384–1395, Nov 2007.\\n[44] H. Soyel and H. Demirel. Facial expression recognition using 3d facial\\nfeature distances. In Image Analysis and Recognition , volume 4633 of\\nLecture Notes in Computer Science, pages 831–838, 2007.\\n[45] H. Soyel and H. Demirel. 3d facial expression recognition with\\ngeometrically localized facial features. In 23rd International Symposium\\non Computer and Information Sciences (ISCIS), pages 1 –4, 2008.\\n[46] H. Tang and T. Huang. 3d facial expression recognition based on\\nautomatically selected features. In IEEE Computer Society Conference\\non Computer Vision and Pattern Recognition Workshops(CVPRW), pages\\n1–8, 2008.\\n[47] H. Tang and T. Huang. 3d facial expression recognition based on\\nproperties of line segments connecting facial feature points. In 8th IEEE\\nInternational Conference on Automatic Face Gesture Recognition(FG),\\npages 1–6, 2008.\\n[48] Y . Tang. Deep learning using support vector machines. CoRR,\\nabs/1306.0239, 2013.\\n[49] A. Tawari and M. M. Trivedi. Face expression recognition by cross\\nmodal data association. IEEE Transactions on Multimedia , 15(7):1543–\\n1552, Nov 2013.\\n[50] F. Tsalakanidou and S. Malassiotis. Real-time 2d+3d facial action and\\nexpression recognition. Pattern Recognition, 43(5):1763–1775, 2010.\\n[51] L. van der Maaten and G. Hinton. Visualizing data using t-sne, 2008.\\n[52] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of\\ncomputer vision algorithms, 2008.\\n[53] J. Wang, L. Yin, X. Wei, and Y . Sun. 3d facial expression recognition\\nbased on primitive surface feature distribution. In IEEE Conference on\\nComputer Vision and Pattern(CVPR), pages 1399–1406, 2006.\\n[54] S. Wang, Z. Liu, S. Lv, Y . Lv, G. Wu, P. Peng, F. Chen, and X. Wang.\\nA natural visible and infrared facial expression database for expression\\nrecognition and emotion inference. IEEE Transactions on Multimedia,\\n12(7), Nov 2010.\\n[55] S. Wang, Z. Liu, Z. Wang, G. Wu, P. Shen, S. He, and X. Wang.\\nAnalyses of a multimodal spontaneous facial expression database. IEEE\\nTransactions on Affective Computing, 4(1):34–46, Jan 2013.\\n[56] C. H. Wu, W. L. Wei, J. C. Lin, and W. Y . Lee. Speaking effect removal\\non emotion recognition from facial expressions based on eigenface\\nconversion. IEEE Transactions on Multimedia, 15(8):1732–1744, Dec\\n2013.\\n[57] X. Yang, D. Huang, Y . Wang, and L. Chen. Automatic 3d facial ex-\\npression recognition using geometric scattering representation. In IEEE\\nInternational Conference on Automatic Face and Gesture Recognition,\\n2015.\\n[58] M. Yeasin, B. Bullot, and R. Sharma. Recognition of facial expressions\\nand measurement of levels of interest from video. IEEE Transactions\\non Multimedia, 8(3):500–508, June 2006.\\n[59] L. Yin, X. Wei, Y . Sun, J. Wang, and M. J. Rosato. A 3d facial expression\\ndatabase for facial behavior research. IEEE International Conference on\\nAutomatic Face and Gesture Recognition, pages 211–216, 2006.\\n[60] Z. Yu and C. Zhang. Image based static facial expression recognition\\nwith multiple deep network learning. IEEE Institute of Electrical and\\nElectronics Engineers, November 2015.\\n[61] S. Zafeiriou and I. Pitas. Discriminant graph structures for facial\\nexpression recognition. IEEE Transactions on Multimedia, 10(8):1528–\\n1540, Dec 2008.\\n[62] G. Zen, L. Porzi, E. Sangineto, E. Ricci, and N. Sebe. Learning per-\\nsonalized models for facial expression analysis and gesture recognition.IEEE Transactions on Multimedia, 18(4):775–788, April 2016.\\n[63] W. Zeng, H. Li, L. Chen, J.-M. Morvan, and X. D. Gu. An automatic\\n3d expression recognition framework based on sparse representation\\nof conformal images. In 10th IEEE International Conference and\\nWorkshops on Automatic Face and Gesture Recognition (FG), pages\\n1–8, 2013.\\n[64] Z. Zhang, M. Lyons, M. Schuster, and S. Akamatsu. Comparison\\nbetween geometry-based and gabor-wavelets-based facial expression\\nrecognition using multi-layer perceptron. In FG, pages 454–459, 1998.\\n[65] G. Zhao and M. Pietikainen. Dynamic texture recognition using\\nlocal binary patterns with an application to facial expressions. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, 29(6):915–\\n928, June 2007.\\n[66] X. Zhao, D. Huang, E. Dellandrea, and L. Chen. Automatic 3d facial\\nexpression recognition based on a bayesian belief net and a statistical\\nfacial feature model. International Conference on Pattern Recognition\\n(ICPR), pages 3724–3727, 2010.\\n[67] Q. Zhen, D. Huang, Y . Wang, and L. Chen. Muscular movement\\nmodel based automatic 3d facial expression recognition. In MultiMedia\\nModeling,Lecture Notes in Computer Science , volume 8935, pages 522–\\n533, 2015.\\n[68] Q. Zhen, D. Huang, Y . Wang, and L. Chen. Muscular movement model\\nbased automatic 3d/4d facial expression recognition. IEEE Transactions\\non Multimedia, PP(99):1–1, 2016.\\n[69] L. Zhong, Q. Liu, P. Yang, J. Huang, and D. N. Metaxas. Learning mul-\\ntiscale active facial patches for expression analysis. IEEE Transactions\\non Cybernetics, 45(8):1499–1510, 2015.\\nHuibin Li received his Bachelor and Master de-\\ngrees both in mathematics from Shaanxi Normal\\nUniversity and Xian Jiaotong University, in 2006\\nand 2009, respectively. And he received Ph.D degree\\nin mathematics and computer science in 2013 from\\nUniversit ´e de Lyon, CNRS, Ecole Centrale de Lyon,\\nLIRIS, Lyon, France. He is currently an assistant\\nprofessor in the school of mathematics and statistics,\\nXi’an Jiaotong University. His research interests\\ninclude 3D shape analysis, 3D face recognition,\\n3D facial expression analysis, discrete differential\\ngeometry, geometric data analysis, modeling and learning.\\nJian Sun received Ph.D. in applied mathematics\\nfrom Xian Jiaotong University. He worked as a\\nvisiting student in Microsoft Research Asia (Nov.\\n2005 - March 2008), a post-doctoral researcher in\\nuniversity of central ﬂorida in USA (August 2009 -\\nApril 2010), and a post-doctoral researcher in willow\\nteam of cole Normale Sup ´erieure de Paris / INRIA\\n(Sept. 2012 - August 2014). He now serves as a\\nprofessor in the school of mathematics and statistics\\nof Xian Jiaotong University. His research interests\\nare in the mathematics &machine learning-based\\napproaches for image processing / recognition, and medical image analysis.\\nZongben Xu received his Ph.D. degree in mathemat-\\nics from Xian Jiaotong University, China, in 1987.\\nHe now serves as the Chief Scientist of National\\nBasic Research Program of China (973 Project), and\\nDirector of the Institute for Information and System\\nSciences of the university. He is owner of the Na-\\ntional Natural Science Award of China in 2007, and\\nwinner of CSIAM Su Buchin Applied Mathematics\\nPrize in 2008. He delivered a 45 minute talk on\\nthe International Congress of Mathematicians 2010.\\nHe was elected as member of Chinese Academy of\\nScience in 2011. His current research interests include intelligent information\\nprocessing and applied mathematics.\\nLiming Chen was awarded a joint BSc degree\\nin Mathematics and Computer Science from the\\nUniversity of Nantes in 1984. He obtained a Master’s\\ndegree in 1986 and a PhD in computer science from\\nthe University of Paris 6 in 1989. He ﬁrst served as\\nassociate professor at the Universit ´e de Technologie\\nde Compi `egne, then joined Ecole Centrale de Lyon\\nas Professor in 1998, where he leads an advanced\\nresearch team in multimedia computing and pattern\\nrecognition. He has been Head of the department of\\nMathematics and Computer science from 2007. His\\ncurrent research interests include multimedia processing, discrete differential\\ngeometry and statistical learning, with applications in particular to 2D/3D face\\nanalysis and recognition, image and video analysis and categorization.\\n\",\n",
       " 'Error',\n",
       " 'Patch-Gated CNN for Occlusion-aware Facial\\nExpression Recognition\\nY ong Li1,2, Jiabei Zeng1, Shiguang Shan1,2,3and Xilin Chen1,2\\n1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),\\nInstitute of Computing Technology, CAS, Beijing 100190, China\\n2University of Chinese Academy of Sciences, Beijing 100049, China\\n3CAS Center for Excellence in Brain Science and Intelligence Technology\\n{yong.li, jiabei.zeng }@vipl.ict.ac.cn, {sgshan, xlchen }@ict.ac.cn\\nAbstract —Facial expression recognition in the wild is challeng-\\ning due to various un-constrained conditions. Although existing\\nfacial expression classiﬁers have been almost perfect on analyzing\\nconstrained frontal faces, they fail to perform well on partiallyoccluded faces that are common in the wild. In this paper,\\nwe propose an end-to-end trainable Patch-Gated Convolution\\nNeutral Network (PG-CNN) that can automatically percept the\\noccluded region of the face and focus on the most discriminative\\nun-occluded regions. To determine the possible regions of intereston the face, PG-CNN decomposes an intermediate feature map\\ninto several patches according to the positions of related facial\\nlandmarks. Then, via a proposed Patch-Gated Unit, PG-CNNreweighs each patch by the unobstructed-ness or importance that\\nis computed from the patch itself. The proposed PG-CNN is eval-\\nuated on two largest in-the-wild facial expression datasets (RAF-DB and AffectNet) and their modiﬁcations with synthesized facial\\nocclusions. Experimental results show that PG-CNN improves the\\nrecognition accuracy on both the original faces and faces withsynthesized occlusions. Visualization results demonstrate that,\\ncompared with the CNN without Patch-Gated Unit, PG-CNN\\nis capable of shifting the attention from the occluded patch to\\nother related but unobstructed ones. Experiments also show that\\nPG-CNN outperforms other state-of-the-art methods on severalwidely used in-the-lab facial expression datasets under the cross-\\ndataset evaluation protocol.\\nI. I NTRODUCTION\\nFacial expression recognition (FER) has received signiﬁcant\\ninterest from computer scientists and psychologists over recentdecades, as it holds promise to an abundance of applications,such as human computer interaction, affect analysis, andmental health assessment. Although many facial expressionrecognition systems have been proposed and implemented,majority of them are builded on images captured in controlledenvironment, such as on CK+ [1], MMI [2], Oulu-CASIA[3], and other lab-collected datasets. The controlled faces arefrontal and without any occlusions. The FER systems that\\nperform perfectly on the lab-collected datasets, is highly pos-\\nsible to perform poorly when recognizing human’s expressionsunder natural and un-controlled conditions.\\nTo ﬁll the gap between the recognition accuracy on the\\ncontrolled faces and un-controlled faces, researchers makeefforts on collecting large-scale facial expression datasets inthe wild ( [4], [5]). Despite the usage of data from the wild,facial expression recognition is still challenging due to theexistence of partially occluded faces. It it non-trivial to address1. Input image 2. Convolutional Feature Extraction \\n3. Occlusion perc eption for each \\nregion of interest  4. Classification neutral \\nanger \\ndisgust \\nfear \\nhappy \\nsad \\nsurprise \\nAttention Net \\nlocal feature \\nPG-Unit \\nAttention Net \\nlocal feature \\nPG-Unit \\n… … … 0.80 \\n 0.82 0.85 0.83 0.70 0.75 Patches \\nunobstructed-ness \\n0.95 0.90 0.21 0.20 0.03 0.07 … \\n… \\nFig. 1. Illustration of the proposed PG-CNN for occlusion-aware facial\\nexpression recognition. During Part 3, PG-CNN extracts 24 regions of interest\\nfrom the intermediate feature maps. For each region, a speciﬁc Patch GatedUnit (PG-Unit) is learnt to reweigh the local representation according to the\\nregion’s “ unobstructed-ness ” (to what extent the patch is occluded). Then,\\nthe weighted representations are concatenated and passed to the classiﬁcation\\npart.\\nthe occlusion issue because occlusion varies in the occluders\\nand their positions. The occlusion may caused by hair, glasses,scarf, breathing mask, hands, arms, food, and other objects that\\ncould be placed in front of the face in daily life. These objects\\nmay block the eyes, mouth, part of the cheek, and any otherpart of the face. The variability of occlusion cannot be fullycovered by restricted amounts of data and will inevitably leadthe recognition accuracy to decrease.\\nTo address the issue of occlusion, we propose a Patch-\\nGated Convolution Neutral Network (PG-CNN), mimicing theway that human recognize the facial expression. Intuitively,human recognize the facial expression based on certain patchesof the face. When some regions of the face are blocked\\n(e.g., the lower left cheek), human may judge the expression\\naccording to the symmetric part of face (e.g., the lower rightcheek), or other highly related facial region (e.g., regionaround the eyes or mouth). Inspired by the intuition, PG-CNN automatically percepts the blocked facial patch and pays\\nattentions mainly to the unblocked and informative patches.\\nFig. 1 illustrates the main idea of PG-CNN. The patches ofinterest are cropped from the last convolution feature mapsaccording to the positions of the related facial landmarks. Foreach patch, a Patch-Gated Unit (PG-Unit) is learned to reweighthe patch’s local representation by its unobstructed-ness that iscomputed from the patch itself. As can be seen in Fig. 1, thelast four visualized patches are blocked and thus they have lowunobstructed-ness ( α\\np). Then, the weighted representations are2018 24th International Conference on Pattern Recognition (ICPR)\\nBeijing, China, August 20-24, 2018\\n978-1-5386-3788-3/18/$31.00 ©2018 IEEE 2209\\nconcatenated and used in the classiﬁcation part.\\nThe contributions of this work are summarized as follows:\\n1) We propose a PG-CNN to recognize facial expressions\\nwith partially occluded faces. PG-CNN can automati-cally percept the occluded region of the face and focuson the most informative and un-blocked regions. To thebest of our knowledge, PG-CNN is the ﬁrst end-to-endtrainable framework that addresses occlusions in facialexpression recognition.\\n2) Visualized results show that PG-Unit (the crucial part of\\nPG-CNN) is effective in perceiving the occluded facial\\npatch. PG-Unit is capable to learn a low weight for the\\nblocked patch and a high weight for an unblocked andinformative one.\\n3) Experimental results demonstrate the advantages of the\\nproposed PG-CNN over other state-of-the-art methodson two large in-the-wild facial expression datasets andseveral popular in-the-lab datasets, under settings witheither partially occluded or non-occluded faces.\\nII. R\\nELA TED WORK\\nWe review the previous work considering two aspects that\\nare related to ours, i.e., the similar tasks (facial analysis withoccluded faces) and related techniques (attention mechanism).\\nA. Methods towards facial occlusions\\nFor facial analysis tasks, occlusion is one of the inherent\\nchallenges in the real world FER and other facial analysistasks, e.g., facial recognition, age estimate, gender classiﬁca-tion, etc. Previous approaches that address facial occlusionscan be classiﬁed into two categories: holistic-based or part-based methods.\\nHolistic-based approaches treat the face as a whole and do\\nnot explicitly divide the face into sub-regions. They usuallyimprove the robustness of the features through designatedregularization, e.g., L\\n1-norm [6]. This idea is also suitable for\\nnon-facial occlusions, for example, Elad et al. [7] proposed to\\nmutually re-weight L1regularization in an end-to-end frame-\\nwork to deal with arbitrary occlusion in object recognition.Another holistic way is to reconstruct a complete face from theoccluded one( [8], [9]). These reconstruction based methodsrely on the training data with varied occlusion conditions.Specially, Irene et al. [10] analysed how partial occlusion\\naffects FER performance in detail.\\nPart-based methods explicitly divide the face into several\\noverlapped or non-overlapped segmentations. To determine thepatches on the face, existing works either divide the facialimage into several uniform parts( [11], [12]), or get the patchesaround the facial landmarks( [13], [14]), or get the patches by asampling strategy [15], or explicitly detect the occluders( [16],[17]). Then, the part-based methods detect and compensatethe missing part ( [18], [19]), or re-weight the occluded andnon-occluded patches differently( [13], [17]), or ignore theoccluded part( [15], [16]). We adopt the way of the part-based methods because they successfully incorporate the priorsinformation of the structure of human faces and have a betterinterpretation. The proposed PG-CNN is end-to-end trainable.\\nIt learns occlusion patterns from data and encodes them withmodel weights. Therefore, it is preferable to handle arbitrarykind of occluder at any position in front of the face.\\nB. CNN with attention\\nRecently, attention models have been successfully applied\\nin many computer vision tasks, including ﬁne-grained imagerecognition [20], image caption [21], visual question answer-ing [22], person re-identiﬁcation [23], etc. Usually attentioncan be modeled as a region sequence in an image. AnRNN/LSTM model is adopted to predict the next attentionregion based on current attention region’s location with visualfeatures.\\nMoreover, zheng et al. [20] adopted channel grouping sub-\\nnetwork to cluster convolutional feature maps into groups ac-cording to peak responses of maps, which do not need part an-notations but is not suitable for FER under occlusion. For false\\nresponses caused by occluders will inevitably disturb channels\\nclustering. Zhao et al. [23] estimated multiple 2-dimensionalattention maps, they have equal spatial size of convolutionalfeature maps to weight. This approach is straightforward butdo not take occlusion patterns into consideration.\\nAttention models allow for salient features to dynamically\\ncome to forefront as needed. This is especially beneﬁcial whenthere is some occlusion or clutter in an image. They also helpinterpret the results by visualizing where the model attends tofor certain tasks. Compare with existing attention models, Ourapproach adopts facial landmarks for region decomposition,which is straightforward and easily implemented. Meanwhile,PG-CNN adopts CNN based Patch-Gated Unit for occlusionperception, guiding the model to shift attention to informativeas well as unblocked facial patches.\\nIII. P\\nROPOSED METHOD\\nA. Method overview\\nWe propose a Patch-Gated CNN (PG-CNN) for facial\\nexpression recognition with partially occlusions. To address\\nthe occlusion issue, PG-CNN is end-to-end trainable with twokey schemes: region decomposition and occlusion perception.\\nFigure 2 illustrates the framework of the proposed PG-CNN.\\nAs can be seen in Fig . 2 , the network takes input as a facial\\nimage. The image is fed into VGG net and is represented assome feature maps. Then, PG-CNN decomposes the feature\\nmaps of the whole face to 24 sub-feature-maps for 24 localpatches. Each local patch is encoded as a weighted vectorof local feature by a Patch-Gated Unit (PG-Unit). PG-Unit\\ncomputes the weight of each patch by an Attention Net,considering its obstructed-ness (to what extent the patch is oc-cluded). Finally, the weighted local features are concatenated\\nand serve as a representation of the occluded face. Three fully\\nconnected layers are followed to assign the face to one of theemotional categories. PG-CNN is optimized by minimizingthe soft-max loss.\\nBelow, we present the details of the two key schemes, region\\ndecomposition and occlusion perception, in PG-CNN.2210\\nPatch 1 \\nPatch 24 512x6x6 1536 \\n1024 \\nAttention net 512x6x6 512x6x6 64 \\nAttention net \\npooling 512x3x3 128x3x3 64 1 512x6x6 VGG16 Net \\n3xHxW \\n512x28x28 PG-Unit Region decomposition \\nPG-Unit Soft-max \\nloss \\nsigmoid \\nFig. 2. Framework of the proposed PG-CNN. PG-CNN takes a facial image as input and encodes the image with VGG-16 Net. The feature maps from the\\nlast convolution layer ( conv 42in VGG [24]) are cropped into 24 local patches through a region decomposition scheme. Each patch is then processed by a\\nPatch-Gated Unit (PG-Unit). PG-Unit encodes a patch by a vector-shaped feature and estimates how informative the patch is through an Attention net. T he\\nsoft-max loss is attached at the end. Parameters in the overall network are learned by minimizing the soft-max loss.\\nFig. 3. Region decomposition of the face. The left ﬁgure shows the selected\\nlandmarks (green dots with numbers), around which the patches in right ﬁgure\\nare cropped. We select 24 points in total, covering the region on or around\\neach subject’s eyebrows, eyes, nose, mouth, and cheek.\\nB. Region decomposition\\nFacial expression is distinguished in speciﬁc facial regions,\\nbecause the expressions are facial activities invoked by setsof muscle motions. Localizing and encoding the expression-related parts is of beneﬁt to recognize facial expression [12].Additionally, dividing the face into multiple local patcheshelps to ﬁnd the position of occlusions [13].\\nTo ﬁnd the typical facial parts that related to expression, we\\nextract the patches according to the positions of each subject’sfacial landmarks. Fig. 3 shows the selection of facial patches.\\nWe ﬁrst detect 68 facial landmark points by the method in[25] and then, based on the detected 68 points, we select orre-compute 24 points that cover the informative region of theface, including the two eyes, nose, mouth, cheek, and dimple.The selected patches are deﬁned as the regions taking each ofthe 24 points as the center. It is noteworthy that face alignmentmethod in [25] is robust to occlusions, which is important forprecise region decomposition.\\nAs can be seen in the overall framework (Fig. 2), the patch\\ndecomposition operation is conducted on the feature map fromconvolution layers rather than from the original image. This isbecause sharing some convolutional operations can decreasethe model size and enlarge the receptive ﬁelds of subsequentneurons. Based on the 512×28×28feature maps as well asthe 24 local region centers, we get a total of 24 local regions,\\neach with a size of 512×6×6.\\nC. Occlusion perception with PG-Unit\\nWe embed the PG-Unit in the PG-CNN to automatically\\npercept the blocked facial patch and pay attentions mainly tothe unblocked and informative patches. The detailed structureof PG-Unit is illustrated in the blue dashed rectangle in Fig. 2.In each patch-speciﬁc PG-Unit, the cropped local feature mapsare fed to two convolution layers without decreasing the spatialresolution, so as to preserve more information when learningregion speciﬁc patterns. Then, the last 512×6×6feature\\nmaps are processed in two branches. The ﬁrst branch encodesthe input feature maps as the vector-shaped local feature. Thesecond branch consist an attention net that estimates a scaler\\nweight to denote the importance of the local patch. The local\\nfeature is then weighted by the computed weight.\\nMathematically speaking, let us suppose p\\nithe input 512×\\n6×6feature map of the i-th patch. Therefore, the i-th PG-Unit\\ntakes the feature map pias the input and outputs its weighted\\nfeatureφi. We formulate PG-Unit as:\\nφi=Ii(˜pi)⊙ψ(˜pi), (1)\\nwhere˜pi=˜φ(pi)is the last 512×6×6feature maps ahead\\nof the two branches. PG-Unit estimates patch i’s importance\\nor ‘unobstructed-ness’ as αi=Ii(˜pi)and then uses αi\\nto weight the local feature ψi=ψ(˜pi).ψ(˜pi)is a vector\\nthat represents the un-weighted feature. ⊙denotes production.\\nαi=Ii(˜pi)is a scaler that represent the patch i’s importance\\nor ‘unobstructed-ness’ (to what extent the patch is occluded).I(·)means the operations in the attention net, consisting\\na pooling operation, one convolution operations, two innerproductions, and a sigmoid activation. The sigmoid activationforces the output α\\niranges in [0,1], where1indicates the\\nmost salient unobstructed patch and 0indicates the completely\\nblocked patch.\\nIn PG-Unit, each patch is weighted differently according to\\nits occlusion conditions or importance. Through the end-to-end2211\\nFig. 4. Examples of the synthesized occluded facial images from RAF-DB\\ndataset. The occluders are various in color, shape, and positions.\\ntraining of the overall PG-CNN, PG-Units can automatically\\nlearn low weights for the occluded parts and high weights forthe unblocked and discriminative parts.\\nIV . E\\nXPERIMENT\\nIn this section, we present the experimental evaluations of\\nPG-CNN. Then, we compared our method with the state-of-the-art FER methods and methods with attention mechanism.Finally, we provide an ablation analysis of the proposed PG-CNN.\\nA. Experimental setup\\n1) Datasets: We evaluated the methods on both in-the-\\nwild datasets (RAF-DB [4] and AffectNet [5]) and in-the-lab datasets(CK+ [1], MMI [2], and Oulu-CASIA [3]). RAF-\\nDB contains 30,000 facial images annotated with basic or\\ncompound expressions by 40 trained human coders. In ourexperiment, only images with basic emotions were used,including 12,271 images as training data and 3,068 imagesas test data. AffectNet is the largest database with annotated\\nfacial emotions. It contains about 400,000 images manuallyannotated for the presence of seven discrete facial expressionsand the intensity of valence and arousal. We only used the oneswith neutral and 6 basic emotions, containing 280,000 trainingsamples and 3,500 test samples. The Extended Cohn-Kanade\\ndatabase (CK+) contains 593 video sequences recorded from\\n123 subjects. we selected the ﬁrst and ﬁnal frame of eachsequence as neutral and target expressions, which results in634 images. MMI database includes more than 30 subjects\\nof both genders (44% female), ranging in age from 19 to 62.There are 79 sequences of each subject. Each begin and endwith neutral facial expression. We extracted the neutral and\\npeak frames from each sequence, resulting in 7348 images.Oulu-CASIA dataset contains six prototypic expressions from\\n80 people between 23 to 58 years old. We selected peak andneutral frames from sequences captured in normal illumina-tion, which results in 9431 images.\\n2) Synthesis of occluded images: It seems unlikely that\\nany reasonable sized set of training images would serve todensely probe the space of possible occlusions. We tackle theproblem by manually collecting about 4k images as masksfor generating occluders. These mask images were collectedTABLE I\\nTEST ACCURACY (%) ONRAF-DB AND AFFECT NET.(clean :ORIGINAL\\nIMAGES .occ. :SYNTHETICALLY OCCLUDED IMAGES .)\\nMethods RAF-DB(clean/occ.) AffectNet(clean/occ.)\\nVGG-16 [24] 80.96/75.26 51.11/46.48\\nDLP-CNN [23] 80.89/76.29 54.47/51.07\\nP-CNN 81.64/76.09 53.9/50.32\\nPG-CNN (proposed) 83.27/78.05 55.33/52.47\\nfrom search engine using more than 50 keywords, such as\\nbeer, bread, wall, hand, hair, hat, book, cabinet, computer,\\ncup et al. All the items were selected due to their highfrequency of occurence as obstructions in facial images. SinceBenitez et al. [26] veriﬁed that small local occluders take noaffects on current FER algorithms, we heuristically restrainoccluder size Ssatisfying S∈[96,128] , which is smaller\\nor equal to half size of expression images. Fig. 4 shows\\nsome occluded examples derived from RAF-DB dataset. Theseartiﬁcial synthesised images are various in occlusion patternsand can better reﬂect occluder distribution in wild condition.\\n3) Implementation details: We implemented PG-CNN us-\\ning Caffe deep learning framework [27]. We adopted VGG-16 [24] as base for PG-CNN due to its simple structureand excellent performance in object classiﬁcation. We onlychoose the ﬁrst nine convolution layers as the feature mapfor region decomposition then attached 24 PG-Units. Thepre-trained model based on ImageNet dataset was used forinitializing the model. For each dataset, Both train and testcorpus are mixed with occluded images with a ratio of 1:1.We adopt a batch-based stochastic gradient descent method tooptimize the model. The base learning rate was set as 0.001and was reduced by polynomial policy with gamma of 0.1.The momentum was set as 0.9 and the weight decay was setas 0.0005. The training of models was completed on a Titan-XGPU with 12GB memory. During the training stage, we setthe actual batch size as 128 and the maximum iterations as50K. It took about 1.5 days to ﬁnish optimizing the model.\\n4) Evaluation metric: All the datasets are mixed with\\ntheir modiﬁcations with synthesized facial occlusions with 1:1ratio. We report FER performance on both non-occluded andoccluded images. For both occluded and non-occluded FER\\nscenarios we use the overall accuracy on seven facial expres-sion categories(i e. six prototypical plus neutral category) as a\\nperformance metric. Both cross-dataset evaluation and 10-foldevaluation within dataset are used in our experiments.\\nB. Comparison with state of arts\\n1) Comparison with other attention models: We compare\\nPG-CNN with DLP-CNN [23]. DLP-CNN estimates Kspatial\\nmaps for attention parts generation. The hyper-parameter Kis\\nﬁne-tuned to the best in out experiments. Table I reports theresults of PG-CNN and DLP-CNN on RAF-DB and AffectNetdatabases. PG-CNN outperforms DLP-CNN on non-occludedimages because the patch-based model can better reﬂect subtlemuscle motions than the model with global attention. PG-CNNexceeds DLP-CNN on occluded datasets with the help of PG-Unit, which encodes occlusion patterns in model weights andenable model attend to unblocked & distinctive patches. From2212\\nTABLE II\\n10- FOLD TEST ACCURACY (%) ONCK+ DA TASET WITH SYNTHETIC\\nOCCLUSIONS . (R8, R16, R24 DENOTE THE SIZE OF THE OCCLUSION AS\\n8×8,16×16,24×24.THE FULL -IMAGE SIZE IS 48×48.)\\nOcclusion PG-CNN∗PG-CNN WLS-RF [28] RGBT [15]\\nnon-occlusion 90.37 97.03 94.3 94.4\\nR8 89.74 96.58 92.2 92.0\\nR16 87.22 95.70 86.4 82.0\\nR24 83.91 92.86 74.8 62.5\\neyes occluded 85.02 96.50 87.9 88.0\\nmouth occluded 82.96 93.92 72.7 30.3\\n∗denotes cross-dataset test accuracy on CK+ by the PG-CNN trained on\\nAffectNet.\\nRAF-DB to AffectNet database, the performance gap becomes\\nnarrowed because signiﬁcant increase in training data.\\n2) Comparison with other methods handling FER with oc-\\nclusion: We compare PG-CNN with state-of-the-arts methods\\nWLS-RF [28] and RGBT [15]. WLS-RF adopted multiplyweighted random forests and RGBT converted a set of Gaborbased part-face templates into template match distance featuresfor FER with occlusion. We followed the same occlusionprotocol of WLS-RF and RGBT and evaluated performanceon model trained by AffectNet dataset.\\nTable II show the comparisons. The overall performance\\nof PG-CNN is signiﬁcantly better than that of WLS-RFand RGBT. Specially, PG-CNN suffers 4.30% performancedegradation under random occlusion with R24 pattern, whileeyes or mouth occlusion has little impact on PG-CNN. Theproceeds of PG-CNN are due to PG-Unit as well as largeamount of training data in AffectNet database.\\nPG-CNN\\n∗in Table II shows that without training on CK+\\ndataset, PG-CNN∗can achieve comparable performance com-\\npared with WLS-RF and RGBT.\\n3) Cross database evaluation: We evaluated the general-\\nization ability of PG-CNN under the cross-dataset evaluation\\nprotocol. In our experiments, PG-CNN was trained on RAF-\\nDB or AffectNet dataset and evaluated on CK+, MMI, Oulu-CASIA dataset with or without synthetic occlusions. Table IIIshows the results compared with other FER methods. Amongthe compared experiments, [29] adopted an inception basedCNN and provided the average cross-database recognition\\naccuracy. [30] and [31] reported the highest cross-database\\nresults, which were both trained on MMI and evaluated onCK+ or vice versa. PG-CNN(A) exceeds [29]–[31] by at least40.7% and 3.02% on CK+ and MMI dataset respectively.It suggests that PG-CNN(A) can generalize better than PG-CNN(R) due to a larger amount of training data.\\nC. Ablation analysis\\nWe conducted ablation analysis to ﬁgure out how PG-CNN\\nboosts performance on FER with occlusion task.\\n1) CNN VS P-CNN: We compared VGG-16 and P-CNN\\n(PG-CNN without PG-Unit) to verify beneﬁt of region de-composition. As listed in Table I, P-CNN exceeds VGG-16on both original and occluded images. The promotions of P-CNN suggest that globally encoded representation has fallenbehind in reﬂecting subtle muscle motions compared withlocally learned patterns.TABLE III\\nCROSS DA TASET EV ALUA TION (ACCURACY %) ON IN -THE -LAB DA TASETS\\n(clean :ORIGINAL IMAGES .occ. :SYNTHESIZED OCCLUDED IMAGES .)\\nmethod CK+(clean/occ.) MMI(clean/occ.) Oulu-CASIA(clean/occ.)\\n[29] 64.2 / − 55.6 / −− /−\\n[30] 60.8 / − 60.3 / −− /−\\n[31] 61.2 / − 66.9 / −− /−\\nP-CNN(R) 79.81 / 76.02 57.02 / 53.70 49.83 / 46.98\\nP-CNN(A) 89.27 / 85.33 66.94 / 61.26 54.77 / 51.05\\nPG-CNN(R) 80.28 / 79.49 55.61 / 53.44 50.04 / 47.15\\nPG-CNN(A) 90.38 /86.27 68.92 /63.94 57.93 /54.18\\nA denotes models trained on AffectNet dataset.\\nR denotes models trained on RAF-DB dataset.\\n(a) Expression images and  \\ncorresponding occluded \\nversion \\n(b) Attention maps of (a),  \\n       derived from P-CNN. \\n Left:  maps of original images.  Right: maps of occluded images (c) Attention maps of (a), \\n       derived from PG-CNN. \\nLeft:  maps of original images. Right: maps of occluded images neutral anger disgust fear happy sad surprise \\nFig. 5. Attention maps of several test images and their modiﬁcations with\\nartiﬁcial facial occlusions. Each image denotes one of seven basic expressions.\\nA deep white corresponds to high attention and a deep dark to no attention\\nat all. Better viewed in color and zoom in.\\n2) P-CNN VS PG-CNN: We compared P-CNN and PG-\\nCNN to verify beneﬁt of PG-Unit. As displayed in Table I,total improvements of PG-CNN on RAF-DB and AffectNetdatasets are 1.99%, 2.58% and 3.65%, 4.27% respectively.This is because PG-Unit enables the model to attend to mostrelated local patches, and shift attention to other related localparts when original ones are occluded. Similar performanceimprovements can be found in Table III, where PG-CNN\\noutperforms P-CNN on nearly all datasets except for MMI.\\nWe visualized the attention map of PG-CNN and P-CNN\\nusing the method in [32]. Simonyan et al. [32] derives aattention map by computing the gradient of the class scorewith respect to the input image. As can be seen in Fig. 5, P-CNN relies on almost the whole face region, while PG-CNN2213\\n(a) Image (b) Attention \\nmaps of (a),  \\nderived from P-\\nCNN \\nneutral disgust fear \\nsad surprise \\n(c) Attention \\nmaps of (a), \\nderived from \\nPG-CNN (d) Image (e) Attention \\nmaps of (d), \\nderived from P-\\nCNN (f) Attention \\nmaps of (d), \\nderived from \\nPG-CNN happy \\nFig. 6. Attention maps of several test images results of test images with real\\nocclusion in RAF-DB. Better viewed in color and zoom in.\\nattends to local discriminative patches. This can decrease the\\nprobability that an occluder take effect in PG-CNN. Moreover,PG-CNN responses weakly to an occluder and shifts attentionfrom the occluded patch(e.g., right eye in the subﬁgures fordisgust ) to other related but unobstructed one(e.g., left eye).\\nFig. 6 displays images with real occluders picked from testset in RAF-DB and AffectNet corpus. PG-CNN performs asconsistently as on artiﬁcial occluders. Take angry category for\\ninstance, we observe only PG-CNN attends to subjects’ nose,which is a strongly discriminative patch for anger.\\nV. C\\nONCLUSION\\nThis work presents a Patch-Gated CNN for facial expres-\\nsion recognition under occlusion. PG-CNN consists of regiondecomposition, Patch Gated Unit for robust facial expres-sion recognition. Experiments under intra and cross databaseevaluation protocols demonstrated PG-CNN outperforms otherstate-of-the-art methods. Ablation analyses show PG-CNN iscapable of shifting attention from occluded patch to otherrelated ones. For future work, we will study how to generateattention parts in face without landmarks, as PG-CNN relies onrobust face detection and facial landmark localization modules.\\nA\\nCKNOWLEDGMENT\\nThis work was partially supported by National Key R&D\\nProgram of China under contracts NO.2017YFB1002802,Natural Science Foundation of China (grants 61702481 and61702486), and External Cooperation Program of CAS (grantGJHZ1843). Thanks also to Xin Liu for discussions.\\nR\\nEFERENCES\\n[1] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews,\\n“The extended cohn-kanade dataset (ck+): A complete dataset for actionunit and emotion-speciﬁed expression,” in CVPRW, 2010 .\\n[2] M. Pantic, M. V alstar, R. Rademaker, and L. Maat, “Web-based database\\nfor facial expression analysis,” in ICME, 2005 .\\n[3] G. Zhao, X. Huang, M. Taini, S. Z. Li, and M. Pietik ¨aInen, “Facial\\nexpression recognition from near-infrared videos,” Image & Vision\\nComputing , vol. 29, no. 9, pp. 607–619, 2011.\\n[4] S. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-\\npreserving learning for expression recognition in the wild,” in CVPR,\\n2017 .[5] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database\\nfor facial expression, valence, and arousal computing in the wild,” arXiv\\npreprint arXiv:1708.03985 , 2017.\\n[6] J. Wright, A. Y . Yang, A. Ganesh, S. Sastry, and Y . Ma, “Robust face\\nrecognition via sparse representation,” IEEE TPAMI , vol. 31, no. 2, pp.\\n210–227, 2009.\\n[7] E. Osherov and M. Lindenbaum, “Increasing cnn robustness to occlu-\\nsions by reducing ﬁlter support,” in CVPR, 2017 .\\n[8] M. Ranzato, J. Susskind, V . Mnih, and G. E. Hinton, “On deep generative\\nmodels with applications to recognition,” in CVPR, 2011 .\\n[9] X. Mao, Y . Xue, Z. Li, K. Huang, and S. Lv, “Robust facial expression\\nrecognition based on rpca and adaboost,” in WIAMIS, 2009 .\\n[10] I. Kotsia, I. Buciu, and I. Pitas, “An analysis of facial expression\\nrecognition under partial facial image occlusion,” Image and Vision\\nComputing , vol. 26, no. 7, pp. 1052–1067, 2008.\\n[11] A. M. Martinez, “Recognizing imprecisely localized, partially occluded,\\nand expression variant faces from a single sample per class,” IEEE\\nTPAMI , vol. 24, no. 6, pp. 748–763, 2002.\\n[12] L. Zhong, Q. Liu, P . Yang, B. Liu, J. Huang, and D. N. Metaxas,\\n“Learning active facial patches for expression analysis,” in CVPR, 2012 .\\n[13] A. Dapogny, K. Bailly, and S. Dubuisson, “Conﬁdence-weighted local\\nexpression predictions for occlusion handling in expression recognition\\nand action unit detection,” IJCV , pp. 1–17, 2017.\\n[14] W. Li, F. Abitahi, and Z. Zhu, “Action unit detection with region\\nadaptation, multi-labeling learning and optimal temporal fusing,” in\\nCVPR, 2017 .\\n[15] L. Zhang, D. Tjondronegoro, and V . Chandran, “Random gabor based\\ntemplates for facial expression recognition in images with facial occlu-\\nsion,” Neurocomputing , vol. 145, pp. 451–464, 2014.\\n[16] R. Min, A. Hadid, and J. L. Dugelay, “Improving the recognition of\\nfaces occluded by facial accessories,” in Automatic Face & Gesture\\nRecognition and Workshops, 2011 .\\n[17] X. Huang, G. Zhao, W. Zheng, and M. Pietikinen, “Towards a dynamic\\nexpression recognition system under facial occlusion,” Pattern Recogni-\\ntion Letters , vol. 33, no. 16, pp. 2181–2191, 2012.\\n[18] H. Towner and M. Slater, “Reconstruction and recognition of occluded\\nfacial expressions using pca,” in ACII, 2007 .\\n[19] Y . Deng, D. Li, X. Xie, K. Lam, and Q. Dai, “Partially occluded face\\ncompletion and recognition,” in ICIP , 2009 .\\n[20] H. Zheng, J. Fu, T. Mei, and J. Luo, “Learning multi-attention convo-\\nlutional neural network for ﬁne-grained image recognition,” in ICCV ,\\n2017 .\\n[21] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,\\nand Y . Bengio, “Show, attend and tell: Neural image caption generation\\nwith visual attention,” in ICML, 2015 .\\n[22] C. Zhu, Y . Zhao, S. Huang, K. Tu, and Y . Ma, “Structured attentions\\nfor visual question answering,” in\\nCVPR, 2017 .\\n[23] L. Zhao and W. Zhuang, Jingdong, “Part-aligned network for person\\nre-identiﬁcation,” in ICCV , 2017 .\\n[24] K. Simonyan and A. Zisserman, “V ery deep convolutional networks for\\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\\n[25] J. Zhang, M. Kan, S. Shan, and X. Chen, “Occlusion-free face alignment:\\ndeep regression networks coupled with de-corrupt autoencoders,” inCVPR, 2016 .\\n[26] C. F. Benitez-Quiroz, R. Srinivasan, Q. Feng, Y . Wang, and A. M.\\nMartinez, “Emotionet challenge: Recognition of facial expressions ofemotion in the wild,” arXiv preprint arXiv:1703.01210 , 2017.\\n[27] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for\\nfast feature embedding,” in ACM Multimedia, 2014 .\\n[28] A. Dapogny, K. Bailly, and S. Dubuisson, “Conﬁdence-weighted local\\nexpression predictions for occlusion handling in expression recognition\\nand action unit detection,” IJCV , pp. 1–17, 2016.\\n[29] A. Mollahosseini, D. Chan, and M. H. Mahoor, “Going deeper in facial\\nexpression recognition using deep neural networks,” in WACV , 2016 .\\n[30] C. Mayer, M. Eggers, and B. Radig, “Cross-database evaluation for facial\\nexpression recognition,” Pattern recognition and image analysis , vol. 24,\\nno. 1, pp. 124–132, 2014.\\n[31] X. Zhang, M. H. Mahoor, and S. M. Mavadati, “Facial expression\\nrecognition using {l}\\n{p}-norm mkl multiclass-svm,” Machine Vision\\nand Applications , vol. 26, no. 4, pp. 467–483, 2015.\\n[32] K. Simonyan, A. V edaldi, and A. Zisserman, “Deep inside convolutional\\nnetworks: Visualising image classiﬁcation models and saliency maps,”\\narXiv preprint arXiv:1312.6034 , 2013.2214\\n',\n",
       " \"Received July 1, 2020, accepted July 9, 2020, date of publication July 17, 2020, date of current version July 29, 2020.\\nDigital Object Identifier 10.1 109/ACCESS.2020.3010018\\nPyramid With Super Resolution for In-The-Wild\\nFacial Expression Recognition\\nTHANH-HUNG VO\\n , GUEE-SANG LEE\\n , (Member, IEEE),\\nHYUNG-JEONG YANG\\n , (Member, IEEE), AND SOO-HYUNG KIM\\n , (Member, IEEE)\\nDepartment of Electronic Computer Engineering, Chonnam National University, Gwangju 61186, South Korea\\nCorresponding author: Soo-Hyung Kim (shkim@jnu.ac.kr)\\nThis work was supported by the National Research Foundation of Korea (NRF) funded by the Ministry of Education through the Basic\\nScience Research Program under Grant NRF-2018R1D1A3A03000947 and Grant NRF-2020R1A4A1019191.\\nABSTRACT Facial Expression Recognition (FER) is a challenging task that improves natural\\nhuman-computer interaction. This paper focuses on automatic FER on a single in-the-wild (ITW) image.\\nITW images suffer real problems of pose, direction, and input resolution. In this study, we propose a\\npyramid with super-resolution (PSR) network architecture to solve the ITW FER task. We also introduce\\na prior distribution label smoothing (PDLS) loss function that applies the additional prior knowledge of the\\nconfusion about each expression in the FER task. Experiments on the three most popular ITW FER datasets\\nshowed that our approach outperforms all the state-of-the-art methods.\\nINDEX TERMS Emotion recognition, image resolution, human computer interaction.\\nI. INTRODUCTION\\nNon-verbal communication plays an essential role in\\nperson-person communication. These non-verbal signals can\\nadd clues, additional information, and meaning to spo-\\nken (verbal) communication. Some studies estimate that\\naround 60% to 80% of communication is non-verbal [1].\\nThese signals include facial expressions, eye contact, voice\\ntone and pitch, gestures, and physical distance, of which\\nfacial expression is the most popular input for analysis. The\\nfacial expression recognition (FER) task aims to recognize\\nthe emotion from the facial image.\\nIn psychology and computer vision, emotion can be\\ndivided into two kinds of model: discrete and dimensional\\ncontinuous [2]\\x15[4]. The dimensional continuous model\\nfocuses on arousal and valence, which values from -1.0 to\\n1.0, whereas the discrete emotion theory classi\\x1ces a few core\\nemotions such as happy, sad, angry, neutral, surprise, disgust,\\nfear and contempt. In our study, we attempted discrete emo-\\ntion recognition .\\nEkman and Friesen developed a Facial Action Coding Sys-\\ntem (FACS) to analyze human facial movements [5]. How-\\never, this scheme needs trained humans and is extensively\\ntime consuming. The recent advances of successful machine\\nThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Waleed Alsabhan\\n .learning in computer vision could help simplify and automate\\nthose processes. The scope of our study is automatic facial\\nexpression recognition, where emotional expression is in the\\ndiscrete model.\\nMany studies use traditional image processing and\\nmachine learning for the FER task. Shan et al. used\\nlocal statistical features, termed Local Binary Patterns,\\nfor person-independent facial expression recognition [6].\\nMa and Khorasani used one-hidden-layer feed forward\\nneural network on a two-dimensional discrete cosine\\ntransform [7]. Lien et al. combined facial feature point track-\\ning, dense \\x1dow tracking, and gradient component detec-\\ntion to detect FACS and calculate emotion [8]. In [9],\\nZhang et al. extracted scale-invariant feature transform and\\nused the deep neural network (DNN) as the classi\\x1cer. Aleksic\\nand Katsaggelos used hidden Markov models for automatic\\nFER [10].\\nRecently, deep learning (DL) has signi\\x1ccantly affected\\nmany \\x1celds, such as image, voice, and natural language\\nprocessing. In the Boosted Deep Belief Network [14] intro-\\nduced by Liu et al. , multiple deep belief networks learned\\nfeature representation from patches of image and some of\\nthem were selected to boost. In [15], Liu et al. ensembled\\nthree convolutional neural networks (CNN) subnets and con-\\ncatenated the outputs to predict the \\x1cnal results. Huang [16]\\nused a custom residual block of the ResNet architecture\\n131988This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nand late fusion to combine the results from the VGG and\\nthe ResNet models. Zeng et al. extracted image histogram\\nof oriented gradients and passed them through deep sparse\\nautoencoders to classify them [17]. Tozadore et al. grouped\\nemotions into several groups to help CNN classify with better\\naccuracy [18].\\nDespite these successes of in-the-lab datasets, the rise of\\nthe in-the-wild (ITW) dataset in recent years has raised new\\nchallenges for researchers. When in-the-lab datasets were\\ncollected under control, the data were clean, accurate, and\\nuniform. In contrast, ITW datasets are noisy, inaccurate, and\\nvariant. We outline the following two observations about ITW\\ndatasets for the FER task.\\nObservation 1: The images size of the ITW datasets\\nvaries . While the size of in-the-lab datasets images is con-\\ntrolled and nearly constant, ITW dataset images have various\\nsizes from too small to large. Figure 1 shows the image\\nsize distribution of the RAF-DB [11], [12] (Fig. 1a) and the\\nAffectNet [13] dataset (Fig. 1b). These two selected datasets\\nare the most popular ITW datasets for the FER task. Because\\nof the differences in width and length, the average of the\\ntwo is considered as the size of the image. In both datasets,\\nthe small images occur more frequently and this frequency\\nFIGURE 1. The image size distribution of the RAF-DB [11], [12] and\\nAffectNet [13] datasets.decreases with increasing size. The mean and variance of the\\nimage size in the RAF-DB are 193 and 144, which is a bit\\nlarge. The AffectNet dataset has larger image sizes, ranging\\nfrom 130 pixels to more than 2000 pixels. In the graph,\\nwe round all images larger than 2000 pixels to the \\x1cxed value\\nof 1000 pixels. Similar to the RAD-DB dataset, the number\\nof image decreases when the size of the image increases. The\\nthird most popular ITW datasets for the FER task is the FER C\\ndataset [19] extended from the FER2013 [20]. It also faces\\nthe different-image-size problem. Unfortunately, the original\\nimage size information was omitted when the author of the\\nFER data published. Most of the studies in this \\x1celd does\\nnot consider the image-size problem. They simply resized\\nall images to the same size, e.g. 128 \\x02128 or 224\\x02224.\\nThe \\x1crst reason is due to the DL framework itself, because\\nin the batch mode, each batch must have the same input\\nshape. Implementing different input sizes at the same time\\ntakes more effort, and is complicated and computationally\\ninef\\x1ccient. While CNN architecture was successful for many\\nimage classi\\x1ccation tasks, it is based on the assumption that\\ndespite the resizing of the images the network could learn to\\ndistinguish by itself. Nearest-neighbor interpolation, bilinear,\\nand bicubic algorithms are popular techniques to scale image\\nsizes.\\nObservation 2: The CNNs are usually sensitized with\\ninput image size. While CNN was very successful for many\\ntasks related to image classi\\x1ccation and segmentation, this\\narchitecture suffers from several weaknesses. One of CNN's\\nweaknesses is the sensitivity to the size of the input image.\\nZooming is one of the data augmentation techniques that\\nattempts to address this problem. The selected zooming scale\\nin most of the experiments ranged from 0.9 to 1.2 because\\nvalues outside this range degraded and damaged the network.\\nWith global pooling , CNN networks could support different\\ninput sizes, and the size incremental technique was used to\\ntrain the networks more quickly and gives coverage easier.\\nDespite the improvement offered by this process, the network\\nremains sensitive to the input size. Therefore, the network\\ntrained with this input size works poorly with the same images\\nbut on a different scale. Figure 2 shows the training and\\nvalidation loss for VGG16 when training with the RAF-DB\\nand the FERCin different scales: 50 \\x0250, 100\\x02100,\\n150\\x02150 and back to 50\\x0250 again in RAF-DB and 48 \\x0248,\\n96\\x0296, 192\\x02192 and again 48\\x0248 in the FERCfor every\\n20 epochs in the sequence. We use weights transfer from the\\nImageNet [21], and then, we freeze the whole CNN architec-\\nture except the fully connected layers. The freeze steps were\\ntrained in 20 epochs at the smallest input image size. At the\\npoint of image size change (epoch 41, 61, 81), the loss of\\nboth training and validation set a signi\\x1ccant increase. At the\\nepoch 81, although the input size returns to the size 48 \\x0248\\nthat was used to train to the network before, the loss value still\\nincreases because of the characteristics of convolution. The\\nconvolution layer uses a kernel (size 3 \\x023, 5\\x025, or similar) to\\nscan the ``pixel'' in the previous layer. Then, even though the\\nimage is the same but in a different scale, the next convolution\\nVOLUME 8, 2020 131989\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 2. The loss value for the training and validation during the\\ntraining process as the input size changed for the RAF-DB and the FER C\\n(VGG16 architecture [22]).\\nlayers learns very different features; therefore, increasing the\\nkernel size does not help here.\\nWhile currently, the super-resolution (SR) step was in\\nthe pre-processing for input, it could be a part of the DL\\narchitecture. SR approaches may be better than the older\\nalgorithms such as nearest-neighbor interpolation, bilinear,\\nand bicubic to solve the small-image-size problem. The SR\\ntask is used to make the larger image from a low-resolution\\nimage while trying to \\x1cll the lost pixels and to avoid the\\npixels becoming blurred. From a low-resolution image, e.g.\\nsizeW\\x02H, the SR task is used to make the larger image\\nkW\\x02kHwhere k\\x152, with the aim of making the\\nnew image as clear as possible. While down-scaling the\\nimage from high-resolution to low-resolution is an easy task,\\nthe reverse direction is not. The missing pixels that are\\nlost from low-resolution need to be recovered. Some recent\\nresearch has focused on this problem. Dong et al. intro-\\nduced the Super-Resolution Convolutional Neural Network\\n(SRCNN), a deep CNN model that works on low-resolution\\nand high-resolution feature maps and \\x1cnally generated a\\nhigh-resolution image [23]. The SRCNN is lightweight and\\noutperforms the bicubic interpolation. Very Deep Super Res-\\nolution (VDSR) has a similar structure as the SRCNN but\\nis more in-depth [24]. In [25], Shi et al. makes the ef\\x1ccient\\nsub-pixel convolutional neural network (ESPCN) that out-\\nperforms the SRCNN. ESPCN improves SRCNN by dealing\\nwith the feature maps at low-resolution and upsampling to the\\x1cnal image. Ledig et al. used resblocks to build SRResNet\\nin [26]. Lim et al. proposed enhanced deep super-resolution\\nnetwork (EDSR) [27]. The EDSR is a modi\\x1ced version of\\nthe SRResNet that removes all batch normalization layers to\\nreduce computing by 40% while improving the ef\\x1cciency.\\nThey also designed a multi-scale network from the base block\\nwith good results. Hu et al. published a Cascaded Multi-Scale\\nCross network that includes a sequence of the cascaded\\nsub-networks [28]. In recent years, the network for the SR has\\ndeepened, and the accuracy has been improved more. While\\nthe SRCNN is lightweight but low accuracy, the EDSR needs\\nmore computing but generates better results.\\nOur study has two highlight contributions. Firstly, we pro-\\npose a Pyramid with Super-Resolution (PSR) network archi-\\ntecture to deal with the different-image-size problem for the\\nITW FER task. Our approach aims to view an image on sev-\\neral scales and uses SR for up-scaling. With many small-size-\\nimage problems in real-world FER datasets, the SR improves\\nthe network performance. Viewing the image on many scales\\nalso helps the network to learn not only at a small local but\\nalso at the global view of input. We also discuss the loss\\nfunction and apply it to the FER task where the distribution\\nof confusion labels are known and can be used.\\nThe rest of this paper is organized as follows. We explain\\nour proposed methods in section II and introduce the\\nprior distribution label smoothing (PDLS) loss function in\\nsection III. Dataset information is presented in section IV.\\nSection V describes the experimental results and discussion.\\nFinally, we conclude our study in section VI.\\nII. PYRAMID WITH SUPER-RESOLUTION (PSR) NETWORK\\nWe deal with the various image-size problems by using\\na pyramid architecture, which is termed as the PSR net-\\nwork. Figure 3 shows the overall PSR network architecture.\\nThere are six blocks in our approach, including spatial trans-\\nformer network (STN), scaling, low-level feature extractor,\\nhigh-level feature extractor, fully connected, and the \\x1cnal\\nconcatenation block. STN is a simulator of an af\\x1cne transfor-\\nmation in a 2D image. The STN is used for face alignment.\\nThe scaling block is the main block, the fundamental idea of\\nour approach. The details about this block are explained in\\nthe next subsection. After the scaling block, there are several\\ninternal outputs, each of which is one image of the original\\ninput, but in different scales, and hence has different sizes.\\nLow and high-level feature extractors are two usual parts\\nin most of the CNN. The fully connected block includes\\nseveral fully connected layers and dropout layers. Finally,\\nwe combine all branch outputs with a late fusion technique.\\nA. THE SPATIAL TRANSFORMER NETWORK (STN) BLOCK\\nThe STN was introduced by Jaderberg et al. [29] and\\nDaiet al. [30]. The main idea of STN is to align the input\\nby learning the transformer. This block is comprised of three\\nparts: localization net, grid generator, and a sampler [29]. The\\nlocalization net has several convolution layers, and \\x1cnally,\\na fully connected layer to output \\x12, where\\x12is a matrix size\\n131990 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 3. Overall network architecture.\\nof 2\\x023, a representation of an af\\x1cne transform in a 2D\\nimage. The grid generator then accepts \\x12and makes a grid,\\nand \\x1cnally, the sampler uses this grid and generates the output\\nimage. The output image is from the input image with rotate,\\nscale, and transforms operators. The input and output of this\\nblock are images with the same size and the same number of\\nchannels.\\nDifferent from in-the-lab images, ITW images are very\\ndifferent from the head pose direction. We add the STN block\\nto help the network learn to align the face and make it easier\\nto recognize.\\nOur implementation details follow the previously pub-\\nlished paper [29]. Table 1 shows the details of the internal\\nlayers of this block. For the convolution layers, the parameters\\nare the input channel, output channel, kernel size, and stride.\\nThe kernel size and stride are needed for the maxpool2d layer.\\nFor the linear layer, only two parameters are needed: the\\nnumber of input nodes and that of output nodes. After the\\nlocalization, the feature map is \\x1dattened and passed through\\nthe fully connected part. Our algorithm calculates the size\\nof the feature map dynamically based on the input size. So,\\nthe block is adaptive to different sizes of the input images.\\nTABLE 1. The details of STN block.\\nB. THE SCALING BLOCK\\nThe scaling block is the leading block in our architecture. The\\nmain idea of this block is to view the input image on different\\nscale from small to large. Belong to that, super-resolution was\\nused to upscale the image size. As in many CNNs, to ensurethe ef\\x1cciency of memory and computing, the input images\\nare kept at the same size. And to use the best information\\nfrom the input images, they are passed to the network at the\\nlargest attainable size. The input size may be limited by the\\ncomputational limit and based on each dataset. While passing\\nthe same size images, as in the \\x1crst observation, many of them\\nwere in low-resolution and were up-scaled by using some\\ntraditional algorithm. However, our approach down-scales\\nthem and then up-scales them again using the SR technique.\\nThis block is to view the overall context in the low-resolution\\nimages, along with the high-resolution image to consider the\\noriginal features.\\nIn the scaling block the network branches to three or more\\nsub-networks. All sub-networks work with the same input\\nimage but on a different scale. The latest branch received\\nthe original input images, which had the highest resolution\\nfor the network. Due to the computational limit, most studies\\nin the \\x1celd of image classi\\x1ccation use the input image from\\n100 to maximum 312. For the larger input size, the higher\\nresolution does not improve the performance. For the batch\\nmode, all images were resized to the central size before being\\npassed through to the network. The larger image size is then\\ndown-scaled, and the smaller images needed to be up-scaled.\\nWe call the original input size W\\x02H. This process of scale\\ninput is the traditional algorithm, such as Nearest-neighbor\\ninterpolation, bilinear, and bicubic. While the down-scaled\\nimage is safe, the up-scaling from small size images to the\\nlarger size using the traditional algorithm is complicated and\\ninaccurate. Our approach tends to overcome this issue. The\\n\\x1crst branch is applied to the lowest resolution image, which\\nwas down-scaled from the original input by the simple oper-\\nator using mean pooling to implement. We declare the value\\nstepandkstep for the step scale value between two neighbors.\\nBy the limit of DL, stepis set to 2. A large kstep can be used,\\nbut due to the computational limitation, we restrict kstep to\\nonly 1 or 2. The size of image for the \\x1crst branch is\\nW\\n2kstep\\x02H\\n2kstep\\nVOLUME 8, 2020 131991\\nT.-H. Vo et al. : PSR for ITW FER\\nBetween the \\x1crst and the last branch, there are kstep of\\nSR branches, each of which is a SR block with the scale size\\nof 2;4;8;:::from the lowest resolution image from the \\x1crst\\nbranch. The size of ithSR is given by equation 1.\\nW\\n2kstep\\x00i\\x02H\\n2kstep\\x00i(1)\\nIn case kD1, there is only one SR branch in the scaling\\nblock, and the output size is the same as the original input\\nsize. In case kD2, there are two SR branches, which\\nhave the sizes of [ W=2,H=2] and [ W,H]. Our setup always\\nensures that the last SR part has the same size as the original\\ninput size. For the SR task, we use the EDSR architecture\\nintroduced by Lim et al. [27].\\nBy learning how to resample the image size, we assume\\nthat this block can add useful information to this particular\\ntask, and thereby increases the accuracy of the prediction\\nmodel.\\nC. LOW AND HIGH-LEVEL FEATURE EXTRACTOR\\nTypically, low and high-level feature extractors are combined\\nin a base network architecture. We choose VGG16 [22] as\\nthe base network because this network is still used as the\\nbase of many recent network for the FER task [31]\\x15[33].\\nFrom the base network, VGG16 [22], we separated into two\\nparts for two levels of input. The low-level feature extractor\\nreceives the images as input and generates the feature map\\ncorresponding to the data. This block works at low level\\nof features, e.g., edge, corner, and so on. The high-level\\nfeature extractor receives the feature map from the low-level\\npart and makes a more in-depth, high-level features for the\\ninput.\\nWhile the input is passed through both extractors in this\\norder, we separated them as two to share across branches.\\nAs in the second observation, we know that the CNNs are\\nvery sensitized with the input size, and here, each branch has\\ndifferent input sizes. The low-level features for each branch\\nare quite different and cannot be shared because sharing\\nlow-level layers damages the network. The high-level feature\\nblock is in another environment. At this level, a high-level\\nfeature needs to be learned and is less dependent on the size\\nof the input. Then the weight of this block can be shared\\nacross branches. The shared weights also act in a similar way\\nto multi-task learning where the combination helps each task\\nobtain better results.\\nThe position of the cutting point denotes pos, which is the\\nposition of the convolution layers in the base network, where\\nwe separate the two parts. A lower posvalue means that all\\nbranches share the weights in most of the internal layers,\\nwhile the highest value of posseparates all branches. From\\nthe second observation, we assume that the low posvalue\\ndegrades the network. Since the base network is VGG16,\\nwhich has 12 convolution layers, the cutting position pos\\nshould be in 0\\x0012, which is the position of corresponding\\nconvolution layers. We analyze the effect of the cutting point\\n(theposvalue) in the experiments.D. FULLY CONNECTED BLOCK AND CONCATENATION\\nBLOCK\\nThe fully connected block includes two fully connected layers\\n(Linear, FC) and several additional layers. The output feature\\nfrom the high-level block then passes through this block to get\\nthe vector to represent the score for each label. Depending\\non the experiment, we use either seven or eight emotions,\\nand then the output vector sizes are set to seven or eight,\\nrespectively. We also use BatchNorm1d for the last feature\\nmap, and two dropout layers with pvalues of 0:25 and 0:5 for\\nthe \\x1crst and the second FC layers, respectively. The ReLU\\nactivation function was applied after the \\x1crst FC layer.\\nSimilar to the high-level feature extractor block, the fully\\nconnected block was also shared among branches.\\nAll branches were fused with the weighted late fusion\\nstrategy. The weight of each branch has been determined\\naccording to the contribution to the \\x1cnal score of the whole\\nnetwork.\\nIII. THE PRIOR DISTRIBUTION LABEL\\nSMOOTHING (PDLS) LOSS FUNCTION\\nFER for basic emotion is a classi\\x1ccation problem, where each\\ninput image is classi\\x1ced into one of seven or eight classes.\\nSoftmax Cross-Entropy is the most popular loss function for\\nclassi\\x1ccation tasks. The cross entropy (CE) loss function is\\ngiven in equation 2.\\nCED\\x00X\\nc2Ctc\\x03log(\\x1b(zc)) (2)\\nwhere :\\n\\x0fCE: cross entropy\\n\\x0fC: set of classes (labels)\\n\\x0ftc: the distribution value of the label cin the ground truth\\nwhereP\\nc2CtcD1\\n\\x0f\\x1b(zc): softmax function for zc\\n\\x0fzc: raw value score for class cfrom the model\\nIn the real world, it is dif\\x1ccult to get the ground truth\\ndistribution for the labels for each sample; therefore, the all\\nin one assumption was used in most cases. In the ideal case,\\nthe sample belongs to one and only one class; therefore,\\nthe one-hot vector is widely used in the classi\\x1ccation task\\nfor labeling, so that equation 2 becomes the simple case of\\n\\x00log(\\x1b(zk)), where tcD0 for all c2Cbut the correct label\\nk(tkD1). Then, all parts except the label kare omitted.\\nThe Label Smoothing (LS) loss function has been intro-\\nduced in other studies [34], [35], and [36]. The formula for LS\\nis given as equation 3. The main idea here is the contribution\\nof all incorrect labels. The parameter \\x0bwas set around 0 :9,\\nmeaning that the contribution for other labels is very small;\\ne.g., for FER task,jCjD8, then the weight for each of them is\\n0:1=8\\x190:0125 and for the correct label is 0 :9125. Although\\nthe weight of the incorrect labels is small, the LS has been\\nused successfully in many classi\\x1ccation tasks. The advantage\\nof the LS over CE with one-hot is that all label scores pre-\\ndicted by the model are activated. Then the backpropagation\\n131992 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nTABLE 2. The prior distribution of the emotions on the FER task.\\nprocess can learn not only how to increase the score for the\\ncorrect label but also how to decrease for the incorrect ones.\\nLSD\\x00log(\\x1b(zk))\\x03\\x0bC\\x00X\\nc2Clog(\\x1b(zc))\\x03(1\\x00\\x0b)\\njCj(3)\\nwhere :\\n\\x0fjCj: size of label set\\n\\x0f\\x0b: parameter control the weight for each part\\nIn the LS loss function, all labels except the correct one\\nare given equal, i.e., they have a small role and are all equal.\\nLS can be used extensively in many tasks when there is no\\ninformation about the distribution. However, in many tasks\\nlike FER, for a particular correct label, the confusion to other\\nclasses are not uniform. The FER task has two advantages: the\\nnumber of labels is small, just seven or eight and, more impor-\\ntantly, we know that for the particular label, the confusion\\nfor some speci\\x1cc classes is higher than others. For example,\\nthe correct label fearis very likely to be confused with sur-\\nprise than with disgust . Another example is the disgust facial,\\nwhich can easily be mistaken as neutral , orsadness than\\nanger orfear. If we have this prior knowledge, the smoothing\\npart should not be a uniform distribution. So, we proposed\\nan extended version of LS with additional prior knowledge\\nof the label's confusion call PDLS. The PDLS loss function\\nwas given by two parts: the one-hot and the prior distribution,\\nas shown in equation 4.\\nPDLSD\\x00X\\nc2C(tc\\x03\\x0bCdkc\\x03(1\\x00\\x0b))\\x03log(\\x1b(zc)) (4)\\nwhere :\\n\\x0f\\x0bis a parameter to control the weight of one-hot and\\ndistribution.\\n\\x0fdkcthe prior distribution for the correct label kand the\\nconfusion label c.\\nAll notations in equation 4 are similar to those in equation 2\\nand 3. The dkcvalue is the new operand in this formula, and it\\nreplaced the uniform distribution1\\njCjin the LS loss function.\\nThedmatrix has the following properties V\\nsizeDjCj\\x02jCjX\\nc2CdkcD1;8k2C\\nargmax (dk1;dk2;:::; dkjCj)Dk;8k2C\\nThe most important part is how to calculate the dkc. Using\\nBarsoum et al. [19], when correcting the labels for theFER2013 dataset [20], the authors of FER Calso provided the\\nlabels distribution information for every sample. In FER C,\\neach sample was labeled by ten people, who need to classify\\neach image to eight basic classes plus two additional classes,\\nunknown andnon-face . While the correct label's distribution\\nfor each sample is dif\\x1ccult to obtain, we assumed that the\\nmethod for making the FER Cis a good approximation for\\nthe ground truth distribution. For each sample s2S,Sis\\nthe FERCdataset, we have the approximate distribution ads.\\nSince unknown and non-face images are omitted, we only use\\ninformation for eight basic emotions, denote by E. Then ads\\nis a vector in R8when 8 is the sizejEj, andPadsD1.\\nEquation 5 is to calculate the average distribution for each\\nground truth emotion k. In this calculation, we used only the\\ntraining set in the FER C.\\ndkDP\\ns2Skads\\njSkj(5)\\nwhere :\\n\\x0fdk: the average distribution of the label k,dk2R8\\n\\x0fjSkj: the size of the subset Sk,Sk\\x1aS, where the ground\\ntruth emotion is k.[k2ESkDS.\\nThe \\x1cnal prior distribution dkifor the FER task is provided\\nin table 2. Each row in the table is dk, and kis one in eight\\nemotion labels. The columns are the confusion labels, and\\nthere are also eight emotion labels. E.g. dneutral;sadnessD\\n0:114 means when image in neutral , there is 11.4% chance\\nto confuse it as sadness . The number in the main diagonal is\\nalways higher than 0 :5 that represents the distribution for its\\nown emotion. The happiness emotion is very clear and easy\\nto detect: dhappiness;happinessD0:918, whereas fearand the\\ndisgust are dif\\x1ccult to detect and easy to be confused.\\nIV. DATASETS\\nThere are three popular ITW datasets for the FER task,\\nincluding the FERC[19], RAF-DB [11], [12] and Affect-\\nNet [13] datasets. In this study, the experiments are con-\\nducted with all of them. The eight discrete emotions for\\nthe classi\\x1ccation are neutral ,happiness ,surprise ,sadness ,\\nanger ,disgust ,fearandcontempt . Some previous datasets\\nand studies used seven of them as they excluded contempt\\nbecause it is dif\\x1ccult and rare in the real world. The details\\nfor each dataset are given below.\\nFERCdataset. The FERCdataset [19] is the \\x1crst\\nITW dataset among them. The original version is the\\nVOLUME 8, 2020 131993\\nT.-H. Vo et al. : PSR for ITW FER\\nTABLE 3. Number of images in training/testing/validation subsets of the FER C, RAF-DB, and AffectNet datasets.\\nFER2013 [20] by Goodfellow et al. , released for the ICML\\n2013 Workshop on Challenges in Representation Learning.\\nBut as the labeling accuracy of the FER2013 dataset is not\\nreliable, Barsoum et al. reassigned the labels [19]. Ten people\\nassigned manually the basic emotion for each image in the\\nFER2013 dataset. The subset of the original images was\\nexcluded if it is classi\\x1ced as unknown ornon-face . The \\x1cnal\\nemotion label was assigned based on the voting from the ten\\npeople. The number of people voting for each emotion for\\neach image was given, which was then used to calculate the\\napproximate distribution of the emotion over that image.\\nThe dataset includes all the images, each of which has one\\nperson's face aligned. The dataset images were collected from\\nthe Internet by querying many related expression keywords.\\nThere are many kinds of face in the real-world environment,\\nand their pose and rotation make them more challenging to\\nrecognize. The images were aligned and centered, and they\\nwere scaled slightly differently. All images are low-resolution\\nand in grayscale with a size of 48 \\x0248 pixels. Each corre-\\nsponding label for each image is also given. The eight basic\\nemotions are used in this dataset.\\nTable 3 and \\x1cgure 4 show the distribution of train, test\\nand validation on the FER Cdataset. The number of neutral\\nimages is highest, 9,030 on the train set, and 1,102 on the test\\nset. The disgust emotion has the lowest number of images:\\nonly 107 on train and 15 on test. The contempt emotion has\\na similar number of images with disgust : only 115 on train\\nand 13 on test. Disgust ,contempt andfearhave few images,\\ncompared with the other \\x1cve emotions. This is normal in\\nnatural communication where people are usually in neutral\\nandhappy state and only rarely experience disgust ,contempt\\nFIGURE 4. The FER Cdata distribution of train/test/valid.orfear. Figure 4 shows that the distribution of emotions on\\ntraining, testing, and validation on the FER Care similar.\\nRAF-DB dataset . Shan Li, Weihong Deng, and Jun-\\nPing Du provided the Real-world Affective Faces Database\\n(RAF-DB) for emotion recognition [11], [12]. The dataset\\ncontains about 30,000 images downloaded from the Internet.\\nAbout 40 trained annotators labeled carefully the image. The\\ndataset has two parts: the single-label subset (basic emotions)\\nand the two-tab subset (compound emotions). We used the\\nsingle-label subset with seven classes of basic emotions. This\\nsubset has 12,271 samples in the training set and 3,068 in the\\ntest set. The number of samples for each emotion is given in\\ntable 3. Notably, the RAF-DB dataset does not include the\\ncontempt expression. Figure 1 shows that images sizes in the\\nRAF-DB vary from tiny to large, which makes it dif\\x1ccult for\\nthe DL model to deal with.\\nAffectNet dataset . The AffectNet [13] is the largest\\ndataset for the FER task. The dataset contains more than one\\nmillion images queried from the Internet by using related\\nexpression keywords. There are about 450,000 images man-\\nually annotated by trained persons. It also includes train,\\nvalidation, and test sets. The test set has not yet been pub-\\nlished, so most previous studies used validation set as the\\ntest set [13], [37]\\x15[40]. Because the contempt emotion is\\nrare in the natural world, some studies [40] used only seven\\nemotions while other studies [13], [38], [39] analyzed all\\neight emotions. Another study used both eight and seven\\nexpressions [37]. Therefore, to compare our results with the\\nprevious studies, we performed experiments with both eight\\nclasses and seven classes.\\nTable 3 shows the number of samples for each emotion\\nclass on each subset train, validation, and test on the FER C,\\nRAF-DB, and AffectNet datasets. The name they use for\\nlabels are a little different but can be mapped to the eight\\nbasic emotions as the emotion column. The FER Chas three\\nseparate subsets for training, validation, and testing, while\\ntwo others have only two subsets. The AffectNet dataset\\nhas not published the testing subset, so as for most of the\\nstudies in this dataset, the validation is taken as the testing\\nsubset, and the validation subset during the training process\\nshould be randomly selected from the training subset. Similar\\nto the RAF-DB, the training subset is randomly separated\\nand then applied to get the training and validation subsets.\\nOnly AffectNet exhibits balanced validation (as the testing),\\n131994 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nwhile the FERCand RAF-DB are highly unbalanced. Both\\nthe FERCand AffectNet datasets have eight emotions labels,\\nand the RAF-DB has only seven emotion classes without\\ncontempt emotion expression.\\nFigure 5 gives some sample images for each class from the\\nthree datasets. In this \\x1cgure, each column presents one emo-\\ntion expression. The images in the \\x1crst two rows (\\x1cgure 5a)\\nis from the FERCdataset, \\x1cgure 5b is from RAF-DB, and\\nthe rest (\\x1cgure 5c) are from AffectNet. The last column of\\nRAF-DB is empty because the RAF-DB dataset has seven\\nemotions without contempt expression.\\nFIGURE 5. Sample images from the (a) FER C, (b) RAF-DB and\\n(c) AffectNet datasets.\\nV. EXPERIMENTS AND RESULTS\\nThis section reports our experiments and results. Subsec-\\ntion V-A gives the experimental setup. Results are shown in\\nsubsection V-B. Finally, subsection V-C presents the discus-\\nsion about our approach and limitations.\\nA. EXPERIMENTAL SETUP\\nFor all experiments, Fastai [41] and PyTorch [42] were used.\\nThose toolboxes make DL experiments easier, with many\\nbuild-in classes, functions, and also pre-trained models to\\nreuse.\\nIn DL, the network initialization has a signi\\x1ccant impact on\\nthe training process. Commonly, weights are initially random.\\nHaving a good initialization strategy helps the networks to\\nlearn better and more smoothly. In our case, we carefully\\ninitialize the network weights. The STN block was set to\\nidentical transformation. The SR layers were initialized from\\npreviously published pre-trained model [27]. The base net-\\nwork, VGG16, was trained with different scale input images.\\nThe model weights were then saved and reloaded to our\\narchitecture. The careful initialization step has several advan-\\ntages. It is easier to train the network, gives quicker network\\ncoverage, and makes a more stable network, leading to fewer\\nvariants.We use Adam optimization algorithm [43] with an adap-\\ntive learning rate using The One Cycle Policy suggested by\\nSmith [44]. The learning rates were set to 1e-3 for some later\\nlayers of the network, and 1e-4 for the STN block. The lower\\nlearning rate for STN with the transformation aims to keep\\nthis bock with little change.\\nThe validation set is used to optimize the hyper-parameters,\\nand then we collected the best models. The hyper-parameters\\nfor all our experiments include the learning rate and the\\nnumber of epoch where the network gets the best result.\\nThose models were used to evaluate the test set. We applied\\nTest Time Augmentation on the test step. Eight randomly\\nrotated, zoomed images are generated from each image and\\nthen passed through the model to get the raw score to predict.\\nThe \\x1cnal raw score is the average of their outputs.\\nFor basic emotions recognition, several metrics are used to\\nevaluate the results. The \\x1crst and most widely used metric is\\naccuracy , orweighted accuracy (WA), which is the number\\nof correct answers divided by the total number of the test\\nsamples. But, when the number of samples for each class is\\nhighly unbalanced, WA may have poor performance, partic-\\nularly FER task, because the emotions in the real world are\\nusually unbalanced. Some emotions such as neutral, happy,\\nor sad are more common than disgust, fear, or contempt. In\\nthis case, unweighted accuracy (UA) should be considered for\\nthe additional evaluation of the system. The UA metric is an\\nunbiased version of WA. The UA is calculated by the average\\nof the accuracy of each class. For comparison with other\\nstudies, both WA and UA are adopted in the experiments.\\nAll experiments were run on Ubuntu 18.04, 32G RAM,\\nGeForce RTX 2080 Ti GPU with 11G GPU RAM.\\nB. EXPERIMENTAL RESULTS\\nWe report the experimental results for the RAF-DB, FER C,\\nand AffectNet datasets.\\n1) RAF-DB DATASET\\nTable 4 gives the results for the RAF-DB dataset. In previous\\nstudies, the methods in [38], [39], [45] report results in WA\\nmetric, and others [46], [47] report UA metric. We report and\\ncompare with previous \\x1cndings in both WA and UA metrics.\\nOur approach produces signi\\x1ccantly better results than the\\nrecent studies on both metrics. For WA, we get 88.98%, which\\nis improved by more than 2% in absolute terms or 2.4%\\nrelatively, compared to Wang et al. [39]. In the UA metric,\\nour approach is 4.05% better in absolute terms compared to\\n[46] or 5.28% relatively.\\nFigure 6 shows the confusion matrix for the RAF-DB. It is\\nshown that the model gives very good accuracy for happiness\\nandneutral , but the results for disgust andfear are only\\n54% and 59%, respectively. Disgust images were predicted\\nasneutral by 17% and fearwas predicted as surprise by 16%.\\n2) FERCDATASET\\nTable 5 shows the experimental results on the FER Ctest set.\\nThe highest accuracy is from the PSR model, which achieved\\nVOLUME 8, 2020 131995\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 6. The confusion matrix on the test set for the RAF-DB, FER Cand AffectNet datasets.\\nTABLE 4. RAF-DB accuracy comparison (%).\\n89.75%. Compared to the best previous result in the literature\\nby Albanie et al. [48], our approach is improved by 0.65%.\\nThe average accuracy for our proposed architecture is\\n69.54% and F1 score (macro) is 74.88% . The low accuracyTABLE 5. FERCaccuracy comparison (%).\\nondisgust andfearmakes the F1 score and average accuracy\\nfar lower than the average. Future work should consider\\nfocusing on increasing the number of sample of disgust and\\nfearto improve the accuracy for these two expressions.\\n131996 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 7. Cumulative accuracy by size on the test set of RAF-DB dataset with the VGG16 (base-line) and the PSR architecture.\\nFigure 6c shows the confusions matrix on the test set of\\nthe PSR architecture: happiness has the highest accuracy\\nof 96%, followed by neutral ,surprise andanger . All four\\nexpressions had accuracy above 90%. The lowest accuracy\\nwas for contempt , 23% accuracy. Due to the lack of contempt\\nimages, the model could not learn to distinguish it from neu-\\ntral,anger , orsadness . Some emotions have high likelihood\\nof wrong classi\\x1ccation: fearpredicted as surprise by 37%,\\ndisgust classi\\x1ced as anger by 33% and sadness classi\\x1ced as\\nneutral by 22%. These high levels of confusion are typical in\\nthe real world because even for humans, it can be dif\\x1ccult to\\ndistinguish these pairs of emotions.\\n3) AffectNet DATASET\\nWe compared both eight and seven classes in the AffectNet\\ndataset. Table 6 shows the results in classi\\x1ccation accuracy\\n(WA). In the classi\\x1ccation of eight emotions, our model\\narchived the accuracy of 60.68%, outperformed the current\\nstate-of-the-art 59.58% achieved by Georgescu et al. [37].\\nIn the seven-emotion task, our model archived the accuracy\\nof 63.77%, slightly improved relative to the current highest\\none at 63.31% [37]. Figure 6b and \\x1cgure 6d present the confu-\\nsion matrix for the AffectNet in the seven-class task and eight-\\nclass task, respectively. The happy expression has the highest\\ndetection rate in both cases, followed by the fearemotion.\\nSurprise, anger, and disgust have a similar performance in\\nboth cases. In the eight-expression task, contempt has the\\nlowest performance just at 49%.\\nFigure 7 shows the cumulative accuracy according to the\\nsize of the original image on the base-line network and the\\nPSR architecture. The PSR was run with the three branchesTABLE 6. AffectNet accuracy comparison (%).\\n[1;2;1] and the cutting point at the sixth convolution layer,\\nwith the original input size of 100 pixels. The image size\\nranged from 23 pixels to about 1200 pixels. Because the\\nlarge images were resized to a \\x1cxed size at 100 pixels,\\nwe consider only those images smaller than 100 pixels to see\\nhow our approach is affected. We omitted the \\x1crst twenty\\npoints because they are unstable to calculate the accuracy.\\nThe \\x1cgure shows that initially, with tiny image sizes less\\nthan 40 pixels, both base-line and PSR are unstable. But after\\n40 pixels, the PSR architecture is improved and works better\\nthan the base-line network. The PSR maintained this trend to\\nthe end of the dataset because, in our approach, we added the\\nsuper-resolution module with double size for a small image\\nin one of the three branches, and another branch for half size\\n100=2D50 improved the recognition accuracy.\\nFigure 8 shows the accuracy discrepancy by size between\\nPSR and VGG16 on the test set of the RAF-DB dataset. The\\nblue points are raw values, and yellow ones are the smoothed\\nversion. The accuracy discrepancy represents the speed of\\nthe improvement of the PSR over the baseline network. It is\\nclear that the improvement had the highest speed when the\\nVOLUME 8, 2020 131997\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 8. The discrepancy of accuracy by size on the test set of the\\nRAF-DB dataset between PSR and baseline.\\nTABLE 7. Analysis of the effectiveness of each block on the RAF-DB (%).\\noriginal image size ranged from 40 pixels to about 55 pix-\\nels; it slows down when the size reached between 55 pixels\\nand 75 pixels, and it becomes lower for 75-85 pixels. After\\n85 pixels, the improvement continues but at a slow speed.\\nNotably, in the experiments for RAF-DB, the original input\\nsize is 100 pixels resolution, then 50 pixels is half of the input\\nsize.\\n4) THE EFFECTIVENESS OF EACH BLOCK\\nTable 7 shows a comparison between some variations of the\\nPSR on the RAF-DB dataset. The second row presents the\\nresult of the PSR without the STN block, which means that\\nthere are only the pyramid structure on top of the baseline\\nnetwork with three branches ( kstepD2). It is clear that on\\nboth WA and UA metrics, this network architecture gets better\\nresults than the VGG16. The improvements are signi\\x1ccant\\nin both cases of metrics, 2.73% for WA and 3.30% for UA.\\nThis implies that our pyramid with SR has an important role.\\nWhen adding STN block to make the full PSR architecture,\\nwe can get a little improvement, about 0.36% in WA and\\n0.81% in WA metrics. We analyzed the effectiveness of the\\nsuper-resolution reconstruction module by breaking down the\\nPSR without the STN block to three separate branches to see\\nthe contribution of each to the \\x1cnal fusion. Figure 9 shows the\\naccuracy of each separated branch and also the fusion of them\\non the PSR architecture. As expected, the small size branch\\ngot the lowest accuracy, and the fusion gets the best one when\\ncombining all three branches. The SR branch and original\\ninput size use the same scale input size, one is SR from\\nthe haft size and another is the original input size. Although\\nusing the same scale size, the SR branch performs better\\nFIGURE 9. The accuracy by original image size on each branch of the PSR\\nwithout the STN block on the RAF-DB test-set.\\nFIGURE 10. The boxplots of performance with different cutting points\\n(accuracy).\\nthan the original size branch. The discrepancy between SR\\nand original input size branch is large for the small images,\\nand it decreases as the size increase. The results clearly\\nrecon\\x1crm that the SR branch helps the network improve the\\nperformance when the original image size is small.\\nFigure 10 shows the performance on the RAF-DB dataset\\nby the cutting point posof the convolution layers from\\nVGG16. The network exhibits the lowest performance at the\\npoint posD0, indicating that all the convolution layers are\\nshared. The accuracy increases as the posvalue increases\\nbut this improvement ceases after the particular cutting point\\natposD5. After the \\x1cfth layer cutting, the accuracy\\nremains stable around the particular value. This result sup-\\nports the second observation, i.e. the CNN is sensitized with\\nthe input size. Sharing some early convolution layers causes\\nthe network to crash. On the other hand, the deeper layers can\\nbe shared because the former convolution layers are learning\\nthe low-level features, and the later convolution layers are\\nworking on more abstract, high-level features.\\n5) THE SENSITIVITY OF THE NETWORK TO THE DIFFERENT\\nINPUT IMAGE SIZE\\nFigure 11 shows the comparison between PSR and\\nVGG16 about the sensitivity when changing the input image\\n131998 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\nFIGURE 11. Visualization of training loss of the PSR and baseline during\\nthe training process of the RAF-DB when changing the original input size\\nin the sequence 50, 100, 150, and back to 50 pixels again.\\nTABLE 8. The loss function comparison (accuracy %).\\nsize on the RAF-DB dataset. The training process is similar\\nas in \\x1cgure 2a with the \\x1crst 20 freeze steps were omitted.\\nThe changing points are in epoch 20, 40, and 60. The graph\\nexhibits that the PSR is less sensitive than the baseline. After\\nthe changing point, the loss of PSR architecture is slightly\\nincreasing. But the VGG16 has a large increase of loss values.\\nThe results con\\x1crm that our approach has the robustness\\nfor the ITW FER task where the original image size varies,\\nalthough the CNNs usually sensitized to the input image.\\n6) THE COMPARISON OF THE THREE DIFFERENT LOSS\\nFUNCTIONS\\nTable 8 compares three loss functions, including the CE,\\nLS and PDLS on the RAF-DB dataset. For each type of\\nloss function, we conducted on experiment on the baseline\\narchitecture, VGG16, and our proposed network architecture.\\nIn both cases of the VGG16 and PSR network architecture,\\nthe CE loss function gets the lowest accuracy. For the baseline\\nnetwork, the LS is slightly better than PDLS, by 0.12%. For\\nthe PSR architecture, however, the PDLS is slightly better\\nthan LS with a margin of 0.42%.\\nFigure 12 shows some sample images from the RAF-DB\\ndataset that PSR predict the correct emotions while the base-\\nline network gave the incorrect emotions. All three images are\\nin the low-resolution which the size ranged between 45 and\\n56 pixels.\\nC. DISCUSSION\\nThe experiments have demonstrated the signi\\x1ccant improve-\\nment of our approach in FER task on all the three datasets.\\nCompared to the base network, VGG16, our pyramid archi-\\ntecture with additional SR block and late fusion greatly\\nimproves the performance. On the RAF-DB dataset, our\\naccuracy is better by about 2% in WA metric and 4.05% in\\nFIGURE 12. Sample images in low-resolution in the RAF-DB dataset\\nwhere PSR recognizes better than the baseline network.\\nUA metric, compared to the state-of-the-art results. The most\\nsubstantial improvement in accuracy has been obtained on the\\nRAF-DB dataset. On the AffectNet dataset, PSR improves the\\naccuracy by 1.01% and 0.46%, compared to the best previous\\nstudy, respectively. Although the given input is in a small\\nsize (48\\x0248) as the FERCdataset, our PSR model gener-\\nated better results. Among the three datasets, the RAF-DB\\nexhibited the most improvement because the RAF-DB has\\nmany image sizes from 23-100. The AffectNet dataset shows\\nless improvement. For the FER Cdataset, the dataset includes\\nthe resized and cropped version of images; using the original\\nversion, if it were available, PSR would give better results.\\nNotable, the different accuracy discrepancies versus the sec-\\nond best algorithm in tables 4, 5, and 6 might be due to\\neach of these tables having a different set of algorithms..\\nOverall, the pyramid with SR has a signi\\x1ccant improvement\\nfor the FER task on the ITW dataset. The SR branch helps\\nthe network performance on the low-resolution image and\\nthen combining with other branches makes the whole network\\nbetter. The STN block also has some improvement.\\nAs in the second observation, the DL networks are sen-\\nsitized with the image input size, and the low-level block\\nin each branch is very different. The result shown in\\n\\x1cgure 10 supports our assumption. When the posvalue is\\ndecreased, indicated that more layers are shared, including\\nsome low-level convolution layers, the network is degraded.\\nWhen the posvalue is increased, indicating that the low-level\\nfeatures are less shared, the network exhibits better results.\\nDue to the trade-off between the performance and the com-\\nputing cost in real practice, the results in \\x1cgure 10 are useful\\nfor selecting the cutting point.\\nThe experimental results in table 8 recon\\x1crmed that LS\\nloss function is better than CE as in many previous studies\\n[34]\\x15[36]. Both the LS and PDLS have better performance\\ncompared to CE, and in the case of the PSR architecture,\\nPDLS shows a signi\\x1ccant boost. The PDLS loss function\\ngave a slight improvement over the original LS function in\\nthe FER task, but it varies case by case, it depends on the\\nnetwork architecture. In the case of VGG16, the experiments\\nshow that the PDLS is nearly equal to LS, which suggests\\nthat future improvements should be needed. The results from\\nPSR model suggests that either LS or PDLS is good for the\\nloss function in the FER task, instead of CE.\\nDespite the signi\\x1ccant improvements presented in our\\nstudy, some limitations warrant further research. The \\x1crst is\\nVOLUME 8, 2020 131999\\nT.-H. Vo et al. : PSR for ITW FER\\nthe step of the scale-up from the lowest resolution. The pyra-\\nmid architecture has viewed the input on several scales, but a\\nstep is an integer number larger than one, and 2 is a starting\\nvalue. But, the double scale is still a tremendous value. While\\nthe scale 1:2 is a good point for most of the augmentation\\ntechniques, and1\\n1:2(\\x190:83) in the reverse case, we suggest\\nthat the scale step should be 1 :22D1:44, or approximated\\nas 1:5. For the traditional algorithm, a decimal scaling value\\nis possible, but it cannot be used for the DL approaches.\\nThe second weakness is the baseline network architecture.\\nAlthough several network architectures, more reliable than\\nVGG16, such as ResNet [49] and SENet [50] have been\\nreported, we chose the VGG16 as the base network. Although\\nour approach is general, we can apply many kinds of CNN,\\nand the re-implementation is needed for each base network.\\nOur approach is not a simple module, so extra efforts must be\\ntaken to implement case by case. The innovation of another\\narchitecture is left for future work.\\nVI. CONCLUSION\\nIn this study, we addressed the various different-image-size\\nproblem in the FER task for ITW datasets, where the original\\ninput image size varies. Although the CNNs could work on\\nthe image with a small rotate and scale, they are worthless\\nwhen the scale is enormous. The main contribution of this\\nstudy is the development of a pyramid network architec-\\nture with several branches, each of which works on one\\nlevel of input scale. The proposed network is based on the\\nVGG16 model, but it can be extended to another baseline net-\\nwork architecture. In the PSR architecture, the SR method is\\napplied for up-scaling the low-resolution input. Experiments\\non three ITW FER datasets show that our proposed method\\noutperforms all the current state-of-the-art methods.\\nREFERENCES\\n[1] A. Mehrabian, Nonverbal Communication . New Brunswick, NJ, USA:\\nAldine Transaction, 1972.\\n[2] P. Ekman, ``Are there basic emotions?'' Psychol. Rev. , vol. 99, no. 3,\\npp. 550\\x15553, 1992.\\n[3] P. Ekman, ``Basic emotions.,'' in Handbook Cognition Emotion . New York,\\nNY, USA: Wiley, 1999, pp. 45\\x1560.\\n[4] J. A. Russell, ``A circumplex model of affect.,'' J. Personality Social\\nPsychol. , vol. 39, no. 6, pp. 1161\\x151178, 1980.\\n[5] P. Ekman and W. Friesen, Facial Action Coding System , vol. 1.\\nMountain View, CA, USA: Consulting Psychologists Press, 1978.\\n[6] C. Shan, S. Gong, and P. W. McOwan, ``Facial expression recognition\\nbased on local binary patterns: A comprehensive study,'' Image Vis. Com-\\nput., vol. 27, no. 6, pp. 803\\x15816, May 2009.\\n[7] L. Ma and K. Khorasani, ``Facial expression recognition using con-\\nstructive feedforward neural networks,'' IEEE Trans. Syst., Man,\\nCybern. B. Cybern. , vol. 34, no. 3, pp. 1588\\x151595, Jun. 2004.\\n[8] J. J. Lien, T. Kanade, J. F. Cohn, and C.-C. Li, ``Automated facial expres-\\nsion recognition based on FACS action units,'' in Proc. 3rd IEEE Int. Conf.\\nAutom. Face Gesture Recognit. , 1998, pp. 390\\x15395.\\n[9] T. Zhang, W. Zheng, Z. Cui, Y. Zong, J. Yan, and K. Yan, ``A deep neural\\nnetwork-driven feature learning method for multi-view facial expression\\nrecognition,'' IEEE Trans. Multimedia , vol. 18, no. 12, pp. 2528\\x152536,\\nDec. 2016.\\n[10] P. S. Aleksic and A. K. Katsaggelos, ``Automatic facial expression recog-\\nnition using facial animation parameters and multistream HMMs,'' IEEE\\nTrans. Inf. Forensics Security , vol. 1, no. 1, pp. 3\\x1511, Mar. 2006.[11] S. Li and W. Deng, ``Reliable crowdsourcing and deep locality-preserving\\nlearning for unconstrained facial expression recognition,'' IEEE Trans.\\nImage Process. , vol. 28, no. 1, pp. 356\\x15370, Jan. 2019.\\n[12] S. Li, W. Deng, and J. P. Du, ``Reliable crowdsourcing and deep locality-\\npreserving learning for expression recognition in the wild,'' in Proc. 30th\\nIEEE Conf. Comput. Vis. Pattern Recognit. , Oct. 2017, pp. 2584\\x152593.\\n[13] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ``AffectNet: A database\\nfor facial expression, valence, and arousal computing in the wild,'' IEEE\\nTrans. Affect. Comput. , vol. 10, no. 1, pp. 18\\x1531, Jan. 2019.\\n[14] P. Liu, S. Han, Z. Meng, and Y. Tong, ``Facial expression recognition via\\na boosted deep belief network,'' in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. , Jun. 2014, pp. 1805\\x151812.\\n[15] K. Liu, M. Zhang, and Z. Pan, ``Facial expression recognition with CNN\\nensemble,'' in Proc. Int. Conf. Cyberworlds (CW) , Sep. 2016, pp. 163\\x15166.\\n[16] C. Huang, ``Combining convolutional neural networks for emotion recog-\\nnition,'' in Proc. IEEE MIT Undergraduate Res. Technol. Conf. (URTC) ,\\nNov. 2017, pp. 1\\x154.\\n[17] N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, and A. M. Dobaie, ``Facial\\nexpression recognition via learning deep sparse autoencoders,'' Neurocom-\\nputing , vol. 273, pp. 643\\x15649, Jan. 2018.\\n[18] D. C. Tozadore, C. M. Ranieri, G. V. Nardari, R. A. F. Romero, and\\nV. C. Guizilini, ``Effects of emotion grouping for recognition in human-\\nrobot interactions,'' in Proc. 7th Brazilian Conf. Intell. Syst. (BRACIS) ,\\nOct. 2018, pp. 438\\x15443.\\n[19] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang, ``Training deep\\nnetworks for facial expression recognition with crowd-sourced label dis-\\ntribution,'' in Proc. 18th ACM Int. Conf. Multimodal Interact. , 2016,\\npp. 279\\x15283.\\n[20] I. J. Goodfellow, ``Challenges in representation learning: A report on three\\nmachine learning contests,'' Neural Netw. , vol. 64, pp. 59\\x1563, Apr. 2015.\\n[21] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ``ImageNet:\\nA large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. , Jun. 2009, pp. 248\\x15255.\\n[22] K. Simonyan and A. Zisserman, ``Very deep convolutional networks for\\nlarge-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Avail-\\nable: http://arxiv.org/abs/1409.1556\\n[23] C. Dong, ``Learning a deep convolutional network for image super-\\nresolution,'' in Computer Vision , vol. 8692, D. Fleet, Ed. Cham, Switzer-\\nland: Springer, 2014, pp. 184\\x15199.\\n[24] J. Kim, J. K. Lee, and K. M. Lee, ``Accurate image super-resolution using\\nvery deep convolutional networks,'' in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit. (CVPR) , Jun. 2016, pp. 1646\\x151654.\\n[25] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueck-\\nert, and Z. Wang, ``Real-time single image and video super-resolution\\nusing an ef\\x1ccient sub-pixel convolutional neural network,'' in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 1874\\x151883.\\n[26] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,\\nA. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, ``Photo-realistic\\nsingle image super-resolution using a generative adversarial network,''\\ninProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017,\\npp. 4681\\x154690.\\n[27] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ``Enhanced deep residual\\nnetworks for single image super-resolution,'' in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. Workshops (CVPRW) , Jul. 2017, pp. 1132\\x151140.\\n[28] Y. Hu, X. Gao, J. Li, Y. Huang, and H. Wang, ``Single image\\nsuper-resolution via cascaded multi-scale cross network,'' 2018,\\narXiv:1802.08808 . [Online]. Available: http://arxiv.org/abs/1802.08808\\n[29] M. Jaderberg, K. Simonyan, A. Zisserman, others, and K. Kavukcuoglu,\\n``Spatial transformer networks,'' in Proc. Adv. Neural Inf. Process. Syst. ,\\n2015, pp. 2017\\x152025.\\n[30] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, ``Deformable\\nconvolutional networks,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\\nOct. 2017, pp. 764\\x15773.\\n[31] M. Hu, H. Wang, X. Wang, J. Yang, and R. Wang, ``Video facial emo-\\ntion recognition based on local enhanced motion history image and\\nCNN-CTSLSTM networks,'' J. Vis. Commun. Image Represent. , vol. 59,\\npp. 176\\x15185, Feb. 2019.\\n[32] S. Li, W. Zheng, Y. Zong, C. Lu, C. Tang, X. Jiang, J. Liu, and W. Xia, ``Bi-\\nmodality fusion for emotion recognition in the wild,'' in Proc. Int. Conf.\\nMultimodal Interact. , Oct. 2019, pp. 589\\x15594.\\n[33] A. Sepas-Moghaddam, A. Etemad, F. Pereira, and P. L. Correia, ``Facial\\nemotion recognition using light \\x1celd images with deep attention-based\\nbidirectional LSTM,'' in Proc. ICASSP - IEEE Int. Conf. Acoust., Speech\\nSignal Process. (ICASSP) , May 2020, pp. 3367\\x153371.\\n132000 VOLUME 8, 2020\\nT.-H. Vo et al. : PSR for ITW FER\\n[34] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ``Rethinking\\nthe inception architecture for computer vision,'' in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 2818\\x152826.\\n[35] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ``Learning transferable\\narchitectures for scalable image recognition,'' in Proc. IEEE/CVF Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2018, pp. 8697\\x158710.\\n[36] R. Müller, S. Kornblith, and G. Hinton, ``When does label smoothing\\nhelp?'' in Proc. NIPS , 2019, pp. 4694\\x154703.\\n[37] M.-I. Georgescu, R. T. Ionescu, and M. Popescu, ``Local learning with deep\\nand handcrafted features for facial expression recognition,'' IEEE Access ,\\nvol. 7, pp. 64827\\x1564836, 2018.\\n[38] J. Zeng, S. Shan, and X. Chen, ``Facial expression recognition with incon-\\nsistently annotated datasets,'' in Proc. Eur. Conf. Comput. Vis. , vol. 11217,\\n2018, pp. 227\\x15243.\\n[39] K. Wang, X. Peng, J. Yang, D. Meng, and Y. Qiao, ``Region attention\\nnetworks for pose and occlusion robust facial expression recognition,''\\nIEEE Trans. Image Process. , vol. 29, pp. 4057\\x154069, 2020.\\n[40] W. Hua, F. Dai, L. Huang, J. Xiong, and G. Gui, ``HERO: Human emotions\\nrecognition for realizing intelligent Internet of Things,'' IEEE Access ,\\nvol. 7, pp. 24321\\x1524332, 2019.\\n[41] J. Howard. (2018). Fastai . [Online]. Available: https://github.\\ncom/fastai/fastai\\n[42] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\\nA. Desmaison, L. Antiga, and A. Lerer, ``Automatic differentiation in\\nPyTorch,'' in Proc. NIPS , 2017, pp. 1\\x154.\\n[43] D. P. Kingma and J. Ba, ``Adam: A method for stochastic\\noptimization,'' 2014, arXiv:1412.6980 . [Online]. Available:\\nhttp://arxiv.org/abs/1412.6980\\n[44] L. N. Smith, ``Cyclical learning rates for training neural networks,''\\ninProc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) , Mar. 2017,\\npp. 464\\x15472.\\n[45] J. Cai, Z. Meng, A. S. Khan, Z. Li, J. O'Reilly, and Y. Tong,\\n``Probabilistic attribute tree in convolutional neural networks for facial\\nexpression recognition,'' 2018, arXiv:1812.07067 . [Online]. Available:\\nhttp://arxiv.org/abs/1812.07067\\n[46] Y. Fan, J. C. Lam, and V. O. Li, ``Multi-region ensemble convolutional\\nneural network for facial expression recognition,'' in Arti\\x1ccial Neural\\nNetworks and Machine Learning (Lecture Notes in Computer Science),\\nvol. 11139. Berlin, Germany: Springer, 2018, pp. 84\\x1594.\\n[47] F. Lin, R. Hong, W. Zhou, and H. Li, ``Facial expression recognition with\\ndata augmentation and compact feature learning,'' in Proc. 25th IEEE Int.\\nConf. Image Process. (ICIP) , Oct. 2018, pp. 1957\\x151961.\\n[48] S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, ``Emotion recog-\\nnition in speech using cross-modal transfer in the wild,'' in Proc. ACM\\nMultimedia Conf. Multimedia Conf. , 2018, pp. 292\\x15301.\\n[49] K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image\\nrecognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,\\nJun. 2016, pp. 770\\x15778.\\n[50] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ``Squeeze-and-excitation\\nnetworks,'' IEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 8,\\npp. 2011\\x152023, Aug. 2020.\\nTHANH-HUNG VO received the B.Eng. degree\\nfrom the Ho Chi Minh City University of Technol-\\nogy, in 2010, and the M.Eng. degree from Vietnam\\nNational University, Vietnam, in 2013, all in com-\\nputer science. He is currently pursuing the Ph.D.\\ndegree with the Pattern Recognition Laboratory,\\nSchool of Electronics and Computer Engineering,\\nChonnam National University, South Korea. Since\\n2011, he has been working as a Lecturer with the\\nHo Chi Minh City University of Technology. His\\nresearch interests include natural language processing, speech, and computer\\nvision applying machine learning, and deep learning techniques.\\nGUEE-SANG LEE (Member, IEEE) received the\\nB.S. degree in electrical engineering and the\\nM.S. degree in computer engineering from Seoul\\nNational University, South Korea, in 1980 and\\n1982, respectively, and the Ph.D. degree in com-\\nputer science from Pennsylvania State Univer-\\nsity, in 1991. He is currently a Professor with\\nthe Department of Electronics and Computer\\nEngineering, Chonnam National University, South\\nKorea. His primary research interests include\\nimage processing, computer vision, and video technology\\nHYUNG-JEONG YANG (Member, IEEE) received\\nthe B.S., M.S., and Ph.D. degrees from Chonbuk\\nNational University, South Korea. She is currently\\na Professor with the Department of Electronics and\\nComputer Engineering, Chonnam National Uni-\\nversity, Gwangju, South Korea. Her main research\\ninterests include multimedia data mining, medical\\ndata analysis, social network service data mining,\\nand video data understanding.\\nSOO-HYUNG KIM (Member, IEEE) received the\\nB.S. degree in computer engineering from Seoul\\nNational University, in 1986, and the M.S. and\\nPh.D. degrees in computer science from the Korea\\nAdvanced Institute of Science and Technology,\\nin 1988 and 1993, respectively. Since 1997, he has\\nbeen a Professor with the School of Electronics\\nand Computer Engineering, Chonnam National\\nUniversity, South Korea. His research interests\\ninclude pattern recognition, document image pro-\\ncessing, medical image processing, and deep learning applications.\\nVOLUME 8, 2020 132001\\n\",\n",
       " 'Real Time Emotion Recognition from Facial \\nExpressions Using CNN Architecture  \\n \\nMehmet Akif OZDEMIR1,Berkay ELAGOZ1, AysegulALAYBEYOGLU2, Reza SADIGHZADEH3and Aydin AKAN1 \\n1Department of Biomedical Engineering, 2Department of Computer Engineering, 3Business Administration  \\nIzmir Katip Celebi University  \\nIzmir, Turkey  \\nmakif.ozdemir@ikc.edu.tr , berkayelagoz@gmail.com , aysegul.alaybeyoglu@ikc.edu.tr , riza@taimaksan.com , aydin.akan@ikc.edu.tr  \\n \\n \\n \\nAbstract—Emotion is an important topic in different fields \\nsuch as biomedical engineering, psychology, neuroscience and \\nhealth . Emotion recognition could be useful for diagnosis of brain \\nand psychological disorders. In recent ye ars, deep learning has \\nprogressed much in the field of image classification. In this study, \\nwe proposed a Convolutional Neural Network (CNN ) based \\nLeNet architecture for facial expression recognition. First of all, \\nwe merged 3 dataset s (JAFFE, KDEF and our  custom dataset). \\nThen we trained our LeNet architecture for emotion states \\nclassification. In this study, we achieved  accuracy of 96.43% and \\nvalidation accuracy of 91.81% for classification of 7 different \\nemotions through facial expressions.  \\nKeywords —Convolutional Neural Network;  Deep Learning; \\nEmotion Recognition; Facial Expressions , Real Time Detection . \\nI. INTRODUCTION  \\nAlthough there are many studies in the literature on \\nemotion, there is no common or singular definition in the \\nliterature  about emotion [1]. Emotion is the appearance or \\nreflection of a feeling. Distinct from feeling, emotion can be \\neither real or sham. For example, feeling of pain can directly \\nrepresent the feeling. But emotions are not felt exactly. \\nEmotions present inner situations psychologically [2, 3] . \\nEmotion is an important, complex and extensive research \\ntopic in the fields of biomedical engineering  [4], psychology  \\n[5], neuroscience  [6] and health  [7]. Emotion detection is an \\nimportant research area in biomedical engineering. Studies in \\nthis area focus on predicting human emotion and computer -\\nassisted diagnosis of psychological disorders. There are \\ndifferent methods in literature to detect emotional states such \\nas electroencephalography (EEG ), galvanic  skin response \\n(GSR), speech analysis,  facial expression, multimodal, visual \\nscanning behavior  [8-10]. \\nIn recent years, with the popularization of deep learning, \\ngreat progress has been made in image classification. \\nConvolutional neural networks (CNNs) is an artificial neural \\nnetwork type that proposed by Yann  LeChun in 1988  [11]. \\nConvolutional neural networks are one of the most popular \\ndeep learning architectures  for image classification, \\nrecognition, and segmentation.  \\nConvolutional neural networks built like a human brain \\nwith artificial neurons and consist of hierarchical multiply hidden layers. These artificial neurons take input from image, \\nmultiply weight, add bias and then apply activation function. \\nSo that, artifi cial neurons can be used in image classification, \\nrecognition, and segmentation by perform simple \\nconvolutions. By feeding the convolutional neural network \\nwith more data (huge amount of data), a better and highly \\naccurate deep learning model can be achiev ed. \\nDeep learning based facial expression recognition is one of \\nthese methods to detect emotion state (e.g., anger, fear, \\nneutral, happiness, disgust, sadness and surprise ) of human. \\nThis method aims to detect facial expressions automatically to  \\nidentify e motional state with high accuracy. In this method, \\nlabeled facial images from facial expression dataset are sent to \\nCNN and CNN is trained by these images. Then, proposed \\nCNN model makes  a determination which facial expression is \\nperformed.  \\nChang et al. u sed CNN model based on ResNet to extract \\nfeature from Fer2013 and CK+ dataset. Proposed complexity \\nperception classification algorithm (CPC) was applied with \\ndifferent classifiers (Softmax, LinearSVM, and \\nRandomForest). CNN+Softmax with CPC has achieved \\n71.35% and 98.78% recognition accuracies for Fer2013 and \\nCK+ respectively  [12]. \\nClawson et al. proposed two human centric CNN \\narchitecture for facial expression recognitio n on CK+ dataset. \\nCNN A consists of 1 convolutional layer and 1 max pooling \\nlayer. CNN B consists of 2 convolutional layers and 2 max \\npooling layers. These architectures trained with 0.0001 initial \\nlearning rate, 300 epochs  and 10 batch size. According to \\nresults, proposed model has achieved 93.3% accuracy on CK+ \\nimages  [13]. \\nNguyen et al. propos ed multi -level18 -layer  CNN model \\nsimilar to VGG. These model does not take only high-level  \\nfeatures also takes mid-level  features. Plain CNN model has \\nreached 69.21% accuracy and proposed multi -level  CNN \\nmodel has reached 73.03% accuracy on Fer2013 dataset  [14].  \\nCao et al. proposed CNN model with K -means clustering \\nidea and SVM classifier which has  achieved 80.29% accuracy. \\nK-means clustering model determines initial value of the \\nconvolution ke rnel of CNN. SVM layers takes features from \\ntrained CNN model to classify Fer2013 images  [15]. \\n978-1-7281 -2420 -9/19/$31.00 ©2019 IEEE  \\nAhmed et al. merged different facial expression datasets \\nwhich are CK, CK+, Fer2013, the MUG facial expression \\ndatabase, KDEF, AKDEF, and KinFaceW -I/II. Data \\naugmentation was applied merged dataset. Proposed CNN \\nmodel consists of 3 convolutional layers with 32, 64,  128 \\nfilters and kernel size are 3x3.  According to results, proposed \\nmodel has reached 96.24% accuracy  [16]. \\nChristou et al. proposed 13 layer CNN model that used on \\nFer2013 dataset and achieved 91.12% accuracy on validation \\ndataset  [17]. \\nSajjanhar et al. worked on CK+, JAFFE and FACES \\ndatasets. They trained and used pre -trained CNN models such \\nas Inception -V3, VGG -16, VGG -19 and VGG -Face. \\nAccording to results, highest accuracy (97.16%) was obtained \\nwith VGG -19 model  on FACES dataset  [18]. \\nChen et al. proposed two -stage framework based on \\nDifference Convolutional Neural Network (DCNN) that \\ntrained with CK+ and BU -4DFE datasets. Results showed \\nproposed model achieved 95.4% accuracy on CK+ dataset and \\n77.4% on BU -4DFE  [19]. \\nIn this study, we proposed CNN based LeNet architecture \\nfor facial expression recognition to estimate emotion states of \\nhuman. We merged 3 different datasets  (KDEF, JAFFE and \\nour custom dataset). Then, proposed LeNet architecture was \\ntrained with final dataset for classification of 7 emotion states \\n(happy, sad, surprised, angry, disgust, afraid and neutral). The \\naim of the study is to obtain deep learning mode l that achieve  \\nhigher accuracy rate for  emotion recognition through  facial \\nexpression.  \\nII. METHODS  \\nA. Facial Expression Dataset  \\nThere are many open accesses  facial expression dataset in \\nliterature. We used 3 facial expression datasets . These are \\nJAFFE, KDEF and o ur custom dataset . \\nJAFFE dataset contains 213 images with 7 facial \\nexpressions (happy, sad, surprised, angry, disgust, afraid and \\nneutral). These images were taken from 10 Japanese female \\nmodels  [20]. An example of images from JAFFE dataset are \\nshown in Fig. 1.  \\nKDEF dataset contains 4900 images with 7 facial \\nexpressions (happy, sad, surprised, angry, disgust, afraid and \\nneutral). Participants are 35 males and 35 females. Dataset \\ncontains 5 different angles. We used only straight position in \\nthis study  [21]. An example of images from KDEF dataset are \\nshown in Fig. 2.  \\n \\nFig.1. Example of images from JAFFE facial expression dataset.    \\nFig.2. Example of images from KDEF facial expression dataset.  \\n Our custom dataset contains 140 images with 7 facial \\nexpressions (happy, sad, surprised, angry, disgust, afraid and \\nneutral). Participants are 1 male and 1 female. Each facial \\nexpression was expressed 10 times by 1 participant.  \\nB. Image Preprocessing  \\n Containing approximately equal numb ers of face images \\nwhich is seven different facial expressions  were different \\nresolutions, because of there were 3 different databases. \\nTherefore, first of all, the face circumference was detected \\nusing the Haar Cascade  library from the pictures. Then, thes e \\ndetected rectangular facial expressions were clipped and \\nrecorded to the same size. Also, the pixel values in the images \\nwere converted to gray images size of  64x64 to be placed in \\nneural networks. This process was done to avoid unnecessary \\ndensity in th e neur al networks.  \\nC. Convolutional Neural  Network Architecture  \\nWith the proposed CNN architecture, it is aimed to educate \\nthe pixel values in the rectangular region containing facial \\nexpressions quickly and functionally and to make quick queries \\nwith the dee p artificial neur al network model formed . The \\nproposed CNN structure is summarized in Fig. 3.  The network \\nmimics the LeNet  structure used in classification of 2D facial \\nexpression data and includes the two convolutional layers, two \\nmax-pooling layers, and one fully connected layer. The \\nconvolutional layers with kernel size of 2x2 are stacked \\ntogether which are followed by max -pooling layer with kernel \\nsize of 2x2 and stride of 2. After all operations of convolutional \\nlayers and max -pooling layers, each frame feeds to the fully \\nconnected layers and prediction of frames was processed with \\nSoftmax classifier as seven different facial emotional state.  \\nD. Network Training  \\nIn training of network, test size determined as 25%. Batch \\nsize ha s been set as 32 and epoch number was found as 500 to \\nconverge parameters of network. Learning rate defined as 10-3. \\nAll k ernel size defined as 2x2 with stride of 2 for convolutional \\nlayers and max -pooling layers respectively. Number of \\nconvolutional layer s represented as 16  and 32 respectively.  \\nSummary of proposed CNN architecture is shown in Table  I. \\nE. Real Time Testing  \\n After training of proposed CNN architecture, the trained \\nmodel was tested in real time.  First of all , human faces were \\ndetected with the Haar Cascade  library within 30 images per \\nsecond of the computer camera.  After that , the detected \\nimages were sent to the model and the classes they belong to \\nwere queried.  As a result of the predictions, the possibility of \\nbelonging to which class the facial expression was shown on a \\n\\n \\nFig. 3. Propos ed CNN model diagram for  facial emotio n recognition . \\nseparate screen and the emotion in which class was higher was \\noverwritten on the Haar Cascade  frame. This process was \\nperformed on every 30 frames that occurred every second of \\nthe camera image obtained in real tim e. \\nTABLE I. SUMMARY  OF PROPOSED  CNN  ARCHITECTURE  \\nLayer (type)  Output Shape  Param #  \\nconv2d_1 (Conv2D)  (None, 64, 64, 20) 520 \\nactivation_1 (Activation)  (None, 64, 64, 20) 0 \\nmax_pooling2d_1 (MaxPooling2)  (None, 32, 64, 20) 0 \\nconv2d_2 (Conv2D)  (None, 32, 32, 50) 25050  \\nactivation_2 (Activation)  (None, 32, 32, 50) 0 \\nmax_pooling2d_2 (MaxPooling2)  (None, 16, 16, 50) 0 \\nflatten_1 (Flatten)  (None, 12800 ) 0 \\ndense_1 (Dense)  (None, 500) 6400500  \\nactivation_ 3 (Activation)  (None, 500) 0 \\ndense_2 (Dense)  (None, 7) 3507  \\nactivation_ 4 (Activation)  (None, 7) 0 \\nTotal params: 6,429,577    \\nTrainable params: 6,429,577    \\nNon-trainable params: 0    \\nNone    \\n*Conv2D: 2D Convolutional Layer , Maxpooling2: 2D Max pooling Layer  III. RESULT AND DISCUSSION  \\nIn this study, Keras  and TensorFlow  libraries were used \\nfor training LeNet CNN architecture and prediction of \\nemotion states with proposed deep learning model. Intel I7 \\n8300 CPU was used for all experiments and training custom \\ndataset. Proposed LeNet CNN model was set wi th mentioned \\nparameters. Fig. 4 . shows performance metrics (training \\naccuracy and  training  loss, validation accuracy and validation \\nloss) of proposed architecture during training and testing. \\nAccording to experiment results, training loss was found \\n0.0887;  training accuracy was found 96.43%; validation loss \\nwas found 0.2725 and validation accuracy was found 91.81%. \\nWhen we look at our results, we get better results than \\nmentioned studies in introduction section [9,11,12,14,16].  \\n \\nFig. 4. Performance metric s of proposed architecture.  \\n\\n According to Fig. 5 . confusion matrix, proposed LeNet \\nmodel more accurate at prediction of surprised, fear, neutral \\nemotion states and less accurate at prediction of sad emotion \\nstate.  \\n \\nFig. 5. Confusion matrix of propoesed architecture . \\nIV. CONCLUSION  \\n This paper proposed a low cost and functionality method \\nfor real time classification seven different emotions by facial \\nexpression based on LeNet CNN architecture. In this study, \\nfacial expression pictures, which can be said has a  small \\nnumber, were successfully trained in CNN and achieved high \\nclassification accuracy . Using the Haar Cascade  library, the \\neffect of unimportant pixels which is outside facial \\nexpressions was reduced.  In addition, single -depth placement \\nof the pixels i n the pictures to networks did not only result in \\nloss of success rate, but also reduced training time and number \\nof networks.  Using a custom database has provided higher \\nvalidation and test accuracy  than training in existing \\ndatabases.  The real -time test model has the functionality to \\nquery each image that occurs in every second . \\n Emotion estimation from facial expressions is the area of \\ninterest of many researchers in the literature. It is hoped that \\nthis study will be a source of studies that will help i n the early \\ndetection of diseases from facial expressions and also studies \\nof consumer behavior analysis.  \\nACKNOWLEDGMENT  \\n This work was supported by Scientific Research Projects \\nCoordinatorship of Izmir Katip Celebi University.  Project \\nnumber: 2019 -ÖNAP -MÜMF -0003 . REFERENCES  \\n[1] M. Cabanac, \"What is emotion?,\" Behavioural proc esses, vol. 60, \\npp. 69 -83, 2002.  \\n[2] R. Roberts, \"What an Emotion Is: a Sketch,\" The Philosophical \\nReview, vol. 97, 1988.  \\n[3] E. Shouse, \"Feeling, emotion, affect,\" M/c journal, vol. 8, no. 6, p. \\n26, 2005.  \\n[4] J. Zhao, X. Mao, and L. Chen, \"Speech emotion recognition using \\ndeep 1D & 2D CNN LSTM networks,\" Biomedical Signal \\nProcessing and Control, vol. 47, pp. 312 -323, 2019.  \\n[5] J. M. B. Fugate, A. J. O\\'Hare, and W. S. Emmanuel, \"Emotion \\nwords: Facing change,\" Journal of Experimental Social \\nPsychology, vol. 79, pp. 264 -274, 2018.  \\n[6] J. P. Powers and K. S. LaBar, \"Regulating emotion through \\ndistancing: A taxonomy, neur ocognitive model, and supporting \\nmeta -analysis,\" Neuroscience & Biobehavioral Reviews, vol. 96, \\npp. 155 -173, 2019.  \\n[7] R. B. Lopez and B. T. Denny, \"Negative affect mediates the \\nrelationship between use of emotion regulation strategies and \\ngeneral health in college -aged students,\" Personality and Individual \\nDifferences, vol. 151, p. 109529, 2019.  \\n[8] S. Albanie, A. Nagrani, A. Vedaldi, and A. J. a. p. a. Zisserman, \\n\"Emotion recognition in speech using cross -modal transfer in the \\nwild,\" pp. 292 -301, 2018.  \\n[9] K.-Y. Huang, C. -H. Wu, Q. -B. Hong, M. -H. Su, and Y. -H. Chen, \\n\"Speech Emotion Recognition Using Deep Neural Network \\nConsidering Verbal and Nonverbal Speech Sounds,\" in ICASSP \\n2019 -2019 IEEE International Conference on Acoustics, Sp eech \\nand Signal Processing (ICASSP), 2019, pp. 5866 -5870: IEEE.  \\n[10] M. Degirmenci, M. A. Ozdemir, R. Sadighzadeh, and A. Akan, \\n\"Emotion Recognition from EEG Signals by Using Empirical \\nMode Decomposition,\" in 2018 Medical Technologies National \\nCongress (TI PTEKNO), 2018, pp. 1 -4. \\n[11] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient -Based \\nLearning Applied to Document Recognition,\" Proceedings of the \\nIEEE , vol. 86, pp. 2278 -2324, 1998.  \\n[12] T. Chang, G. Wen, Y. Hu, and J. Ma, \"Facial Expression  \\nRecognition Based on Complexity Perception Classification \\nAlgorithm,\" arXiv e -prints, Accessed on: February 01, 2018  \\nAvailable: \\nhttps://ui.adsabs.harvard.edu/abs/2018arXiv180300185C  \\n[13] K. Clawson, L. Delicato, and C. Bowerman, \"Human Centric \\nFacial Expr ession Recognition ,\" 2018.  \\n[14] H.-D. Nguyen, S. Yeom, G. -S. Lee, H. -J. Yang, I. Na, and S. H. \\nKim, \"Facial Emotion Recognition Using an Ensemble of Multi -\\nLevel Convolutional Neural Networks,\" International Journal of \\nPattern Recognition and Artificial Int elligence,  2018.  \\n[15] T. Cao and M. Li, \"Facial Expression Recognition Algorithm \\nBased on the Combination of CNN and K -Means,\" presented at the \\nProceedings of the 2019 11th International Conference on Machine \\nLearning and Computing, Zhuhai, China, 20 19.  \\n[16] T. Ahmed, S. Hossain, M. Hossain, R. Islam, and K. Andersson, \\n\"Facial Expression Recognition using Convolutional Neural \\nNetwork with Data Augmentation ,\" pp. 1 -17, 2019.  \\n[17] N. Christou and N. Kanojiya, \"Human Facial Expression \\nRecognition with Convolution  Neural Networks,\" Singapore, 2019, \\npp. 539 -545: Springer Singapore.  \\n[18] A. Sajjanhar, Z. Wu, and Q. Wen, \"Deep learning models for facial \\nexpression recognition,\" in 2018 Digital Image Computing: \\nTechniques and Applications (DICTA), 2018, pp. 1 -6: IEEE.  \\n[19] J. Chen, Y. Lv, R. Xu, and C. Xu, \"Automatic social signal \\nanalysis: Facial expression recognition using difference \\nconvolution neural network,\" Journal of Parallel and Distributed \\nComputing, vol. 131, pp. 97 -102, 2019.  \\n[20] M. Lyons, S. A kamatsu, M. Kamachi, and J. Gyoba, \"Coding \\nFacial Expressions with Gabor Wavelets ,\" pp. 200 -205, 1998 . \\n[21] D. Lundqvist, A. Flykt, and A. Öhman, \"The Karolinska directed \\nemotional faces (KDEF),\" CD ROM from Department of Clinical \\nNeuroscience, Psychology section, Karolinska Institutet, vol. 91, p. \\n630, 1998.  \\n\\n',\n",
       " 'Recognition of Affect in the wild using Deep Neural Networks\\nDimitrios Kollias⋆Mihalis A. Nicolaou†Irene Kotsia1,2Guoying Zhao3\\nStefanos Zafeiriou⋆,3\\n⋆Department of Computing, Imperial College London, UK\\n†Department of Computing, Goldsmiths, University of London, UK\\n3Center for Machine Vision and Signal Analysis, University of Oulu, Finland\\n1School of Science and Technology, International Hellenic University, Greece\\n2Department of Computer Science, Middlesex University, UK\\n⋆{s.zafeiriou, dimitrios.kollias15 }@imperial.ac.uk,†m.nicolaou@gold.ac.uk\\nAbstract\\nIn this paper we utilize the ﬁrst large-scale ”in-the-wild”\\n(Aff-Wild) database, which is annotated in terms of the\\nvalence-arousal dimensions, to train and test an end-to-end\\ndeep neural architecture for the estimation of continuous\\nemotion dimensions based on visual cues. The proposed\\narchitecture is based on jointly training convolutional\\n(CNN) and recurrent neural network (RNN) layers, thus\\nexploiting both the invariant properties of convolutional\\nfeatures, while also modelling temporal dynamics that\\narise in human behaviour via the recurrent layers. V arious\\npre-trained networks are used as starting structures which\\nare subsequently appropriately ﬁne-tuned to the Aff-Wild\\ndatabase. Obtained results show premise for the utilization\\nof deep architectures for the visual analysis of human\\nbehaviour in terms of continuous emotion dimensions and\\nanalysis of different types of affect.\\n1. Introduction\\nBehavioral modeling and analysis constitute a crucial as-\\npect of Human Computer Interaction. Emotion recognition\\nis a key issue, dealing with multimodal patterns, such as\\nfacial expressions, head pose, hand and body gestures, lin-\\nguistic and paralinguistic acoustic cues, as well as physi-\\nological data [ 19][5][18]. However, building machines\\nwhich are able to recognize human emotions is a very chal-\\nlenging problem. This is due to the fact that the emotion\\npatterns are complex, time-varying, user and context de-\\npendent, especially when considering uncontrolled environ-\\nments, i.e., in-the-wild.\\nCurrently deep neural network architectures are the\\nmethod of choise for learning-based computer vision,\\nspeech recognition and natural language processing tasks.They have also achieved great performances in emotion\\nrecognition challenges and contests [ 23][14][13][8].\\nMoreover, end-to-end architectures, i.e., networks designed\\n- trained, tested and subsequently used - as whole systems,\\naccepting the raw input data and learning to produce the de-\\nsired outputs, seem very promising for implementing plat-\\nforms that can reach the market and be easily used by cus-\\ntomers and users.\\nIn this paper, we make a considerable effort to go beyond\\ncurrent practices in facial behaviour analysis, by training\\nmodels on large scale data gathered in ”in-the-wild”, that is\\nin entirely uncontrolled conditions. In more detail, we uti-\\nlize the ﬁrst, annotated in terms of continuous emotion di-\\nmensions, large scale ”in-the-wild” database of facial affect,\\ni.e. the Aff-Wild database [ 27]. Exploiting the abundance\\nof data available in video-sharing websites, the database is\\nenriched with spontaneous behaviours (such as subjects re-\\nacting to an unexpected development in a movie or a series,\\na disturbing clip, etc.). The database contains more than 30\\nhours of video, and around 200 subjects.\\nGiven the Aff-Wild data, we show that it is possible to\\nbuild upon the recent breakthroughs in deep learning and\\npropose, the ﬁrst, to the best of our knowledge, end-to-end\\ntrainable system for valence and arousal estimation using\\n”in-the-wild” visual data1.\\nIn the rest of the paper, we ﬁrst describe brieﬂy the Aff-\\nWild database (Section 2), afterwards we present the pro-\\nposed end-to-end deep CNN and CNN-RNN architectures\\n(Section 3) and then the experimental results (Section 4).\\nFinally, conclusions and future work are presented in Sec-\\n1An end-to-end trainable Convolutional Neural Network (CNN) plus\\nRecurrent Neural Network (RNN) for valence and arousal estimation from\\nspeech has been recently proposed in [ 25]. Furthermore, the recent method\\nin [12] combines CNN with RNN (but independently trained) for valence\\nand arousal in the A VEC data that have been captured in controlled condi-\\ntions [ 22].\\n2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops\\n2160-7516/17 $31.00 © 2017 IEEE\\nDOI 10.1109/CVPRW.2017.2471972\\n\\nFigure 1: Annotated valence and arousal (Person A)\\ntion 5.\\n2. The Aff-Wild Database\\nTraining and testing of the deep neural architectures\\nhas been performed using the Aff-Wild database [ 27]; this\\ndatabase shows spontaneous facial behaviors in arbitrary\\nrecording conditions, which should be analyzed so as to\\ndetect the valence and arousal emotion parameters. The\\ndatabase contains 298 videos of 200 people in total.\\nFigures 1,2show two characteristic sequences of facial\\nimages, taken from different videos of Aff-Wild. They in-\\nclude the respective video frame numbers and the valence\\nand arousal annotations for each of them. A visual repre-\\nsentation of the valence and arousal values is also depicted\\non the 2-D emotion space, showing the change in the reac-\\ntions/behavior of the person among these time instances of\\nthe video. Time evolution is indicated, by using a larger size\\nfor the more recent frames and a smaller size for the older\\nones.\\nFigure 3shows the histogram of valence and arousal an-\\nnotations. It can be seen that the amount of annotated pos-\\nitive reactions, corresponding to positive valence values, is\\nlarger than that of negative ones. Similarly, the amount of\\nannotated ’evident’ reactions, with positive arousal values,\\nis larger than the less ’evident’, or hidden ones, with nega-\\ntive arousal values. We examine in more detail this issue in\\nthe experimental section of the paper.\\n2.1. Database Pre-processing\\nThe whole database contains more than 1,180,000\\nframes (around 900K for training and 300K for testing).\\nFrom each frame, we detected faces using the method de-\\nFigure 2: Annotated valence and arousal (Person B)\\nFigure 3: Histogram of Annotations\\nscribed in [ 15] and cropped the faces. Next, we applied the\\nbest performing method of [ 3] in order to track 68 facial\\nlandmark. Since, many of the current pre-trained networks,\\nsuch as VGG series of networks, operate on images with\\nresolution of 224×224×3we have always resized the\\nfacial images to this resolution.\\nIn order to have a more balanced dataset for training,\\nwe performed data augmentation, mainly through oversam-\\npling by re-sampling and duplicating [ 16] some data from\\nthe Aff-Wild database. To be more precise, we re-sampled\\ndata that had negative valence and arousal values, as well\\n1973\\nas positive valence and negative arousal values. As a conse-\\nquence, the training set consisted of about 43% of positive\\nvalence and arousal values, 24% of negative valence and\\npositive arousal values, 19% of positive valence and nega-\\ntive arousal values and 14% of negative valence and arousal\\nvalues.\\n3. The End-to-End Deep Neural Architectures\\nWe have developed end-to-end architectures, i.e., archi-\\ntectures that trained all-together, accepting raw data colour\\nimages, learn to produce 2-D predictions of valence and\\narousal.\\nIn particular, we have evaluated the following architec-\\ntures:\\n(1) An architecture based on the structure of the ResNet\\nL50 network [ 9].\\n(2) An architecture based on the structure of the VGG\\nFace network [ 20].\\n(3) An architecture based on the structure of the VGG-16\\nnetwork [ 24].\\nWe also considered two different approaches (a) an only\\nframe based approach where only CNNs are trained and (b)\\nCNN plus RNN end-to-end approaches that can exploit the\\ndynamic information of the video data. In both settings, we\\npresent experimental results based on the following scenar-\\nios:\\n(1) The network is applied directly on cropped facial video\\nframes of the generated database, trained to produce\\nboth valence and arousal (V , A) predictions.\\n(2) The network is trained on both the facial appearance\\nvideo frames, as well as the facial landmarks corre-\\nsponding to the same frame.\\nRegarding the CNN-RNN architecture, we utilize Long\\nShort Term Memory (LSTM) [ 10] and Gated Recurrent\\nUnit (GRU) [ 4] layers, stacked on top of the last fully con-\\nnected layer. The RNN-LSTM/GRU consists of one or two\\nhidden layers, along with the output layer that provides the\\nﬁnal 2-D emotion predictions. We note that all deep learn-\\ning architectures have been implemented in the Tensorﬂow\\nplatform [ 1].\\nFor training, we utilize the Adam optimizer, that pro-\\nvides slightly better overall performance in comparison to\\nother methods, such as the stochastic gradient descent. Fur-\\nthermore, the utilized loss functions for evaluation and\\ntraining include the Concordance Correlation Coefﬁcient\\n(CCC) and Mean Squared Error (MSE). We primarily fo-\\ncus on optimizing the CCC, since it can provide better in-\\nsight on whether the prediction follows the structure of theground truth annotation. In more detail, the CCC is deﬁned\\nas\\nρc=2sxy\\ns2x+s2y+( ¯x−¯y)2(1)\\nwheresxandsyare the variances of the predicted and\\nground truth values respectively, ¯xand¯yare the correspond-\\ning mean values, while sxandsyare the respective covari-\\nance values.\\nRegarding initialization, in our experiments we trained\\nthe proposed deep architectures by either (i) randomly ini-\\ntializing the weight values, or (ii) using pre-trained weights\\nfrom networks having been pre-trained on large databases,\\nsuch as the ImageNet [ 6]. For the second approach we used\\ntransfer learning [ 17], especially of the convolutional and\\npooling part of the pre-trained networks. In more detail,\\nwe utilized the ResNet L50 and VGG-16 networks, which\\nhave been pre-trained for object detection tasks, along with\\nVGG-Face, which has been pre-trained for face recognition\\ntasks. In all experiments, the VGG-Face provided much\\nbetter results, so, in the following we focus on the transfer\\nlearning methodology with weight initialization using the\\nVGG-Face network; this has been pre-trained on the Face-\\nV alue dataset [ 2]. It should be noted that when utilizing\\npre-trained networks, we experimented based on two ap-\\nproaches: either performing ﬁne-tuning, i.e., training the\\nentire architecture with a relatively small learning rate, or\\nfreezing the pre-trained part of the architecture and retrain-\\ning the rest (i.e., the fully connected layers of the CNN,\\nas well as the hidden layers of the RNN). In general, the\\nprocedure of freezing a part of the network and ﬁne-tuning\\n[11] the rest can be deemed very useful, in particular when\\nthe given dataset is incremented with more videos. This in-\\ncreases the ﬂexibility of the architecture, as ﬁne-tuning can\\nbe performed by simply considering only the new videos.\\nTraining was performed on a single TITAN X (Pascal) GPU\\nand the training time was around 5 days.\\n3.1. Implementing the CNN Architectures\\nIn the following we provide speciﬁc information on the\\nselected structure and parameters in the used end-to-end\\nneural architectures, with reference to the results obtained\\nfor each case in our experimental study.\\nExtensive testing and evaluation has been performed by\\nselecting different network parameter values, including (1)\\nthe number of neurons in the CNN fully connected layers,\\n(2) the batch size used for network parameter updating, (3)\\nthe value of the learning rate and the strategy for reducing it\\nduring training (e.g. exponential decay in ﬁxed number of\\nepochs), (4) the weight decay parameter value, and ﬁnally\\n(5) the dropout probability value.\\nWith respect to parameter selection in the CNN archi-\\ntectures, we used a batch size in the range 10 −100 and an\\ninitial learning rate value in the range 0.0001 −0.01 with ex-\\n1974\\nponential decay. The best results have been obtained with\\nbatch size 80 and initial learning rate 0.001. The dropout\\nprobability value was 0.5, the decay steps of the exponen-\\ntial decay of the learning rate were 400, while the learning\\nrate decay factor was 0.97. The number of neurons per layer\\nper CNN type is described in the next subsections.\\n3.1.1 CNN architecture based on ResNet L50\\nTable 1shows the conﬁguration of the CNN architecture\\nbased on ResNet L50. This is composed of 9 blocks.\\nFor each convolutional layer the parameters are denoted\\nas (channels, kernel, stride) and for the max pooling layer\\nas (kernel, stride). The bottleneck modules are deﬁned as\\nin [9]. For the fully connected layers, Table 1shows the\\nrespective number of units. Using three Fully-Connected\\n(FC) layers was found to provide best results.\\nTable 1: Architecture for CNN network based on ResNet\\nL50\\nblock 1 1×conv layer (64,7×7,2×2)\\nbatch norm layer\\n1×max pooling (3×3,2×2)\\nblock 2 3×bottleneck [(64, 1×1),\\n(64,3×3),\\n(256, 1×1)]\\nblock 3 4×bottleneck [(128, 1×1),\\n(128, 3×3),\\n(512, 1×1)]\\nblock 4 6×bottleneck [(256, 1×1),\\n(256, 3×3),\\n(1024, 1×1)]\\nblock 5 6×bottleneck [(512, 1×1),\\n(512, 3×3),\\n(2048, 1×1)]\\nblock 6 1×average pooling\\nblock 7 fully connected 1 1500\\ndropout layer\\nblock 8 fully connected 2 256\\ndropout layer\\nblock 9 fully connected 3 2\\nTable 1refers to our second scenario where both the out-\\nputs of the last pooling layer of the CNN, as well as the 68\\nlandmark 2-D positions ( 68×2values) were provided as\\ninputs to the ﬁrst of the three fully connected (FC) layers of\\nthe architecture. In the contrary, in scenario (1), the outputs\\nof the last pooling layer of the CNN were the only inputs of\\nthe fully connected layer of our architecture. In this case,\\nthe architecture included only two fully connected layers,\\ni.e., the 1st and 3rd fully connected ones.\\nIn general, we used 1000 −1500 units in the ﬁrst FC\\nlayer and 200 −500 units in the second FC layer. The lastlayer consisted of 2 output units, providing the (V , A) pre-\\ndictions. A linear activation function was used in this last\\nFC layer, providing the ﬁnal estimates. All units in the other\\nFC layers were equipped with the rectiﬁcation (ReLU) non-\\nlinearity.\\n3.1.2 CNN architecture based on VGG-Face/VGG-16\\nTable 2shows the conﬁguration of the CNN architecture\\nbased on VGG-Face or VGG-16. It is also composed of 9\\nblocks. For each convolutional layer the parameters are de-\\nnoted as (channels, kernel, stride) and for the max pooling\\nlayer as (kernel, stride). Table 2shows the respective num-\\nber of units of each fully connected layer. Using four fully\\nconnected layers was found to provide best results.\\nTable 2: Architecture for CNN network based on VGG-\\nFace/VGG-16\\nblock 1 2×conv layer (64,3×3,1×1)\\n1×max pooling (2×2,2×2)\\nblock 2 2×conv layer (128, 3×3,1×1)\\n1×max pooling (2×2,2×2)\\nblock 3 3×conv layer (256, 3×3,1×1)\\n1×max pooling (2×2,2×2)\\nblock 4 3×conv layer (512, 3×3,1×1)\\n1×max pooling (2×2,2×2)\\nblock 5 3×conv layer (512, 3×3,1×1)\\n1×max pooling (2×2,2×2)\\nblock 6 fully connected 1 4096\\ndropout layer\\nblock 7 fully connected 2 4096\\ndropout layer\\nblock 8 fully connected 3 2622\\ndropout layer\\nblock 9 fully connected 4 2\\nTable 2also refers to the second scenario. In this case,\\nhowever, best results were obtained, when the 68 landmark\\n2-D positions ( 68×2values) were provided, together with\\nthe outputs of the ﬁrst FC layer of the CNN, as inputs to\\nthe second of the four FC layers of the architecture. In sce-\\nnario 1, the outputs of the ﬁrst FC layer of the CNN were\\nthe only inputs to the second fully connected layer of our\\narchitecture. In this case, the architecture included only 3\\nFC layers, i.e., the 1st, 2nd and 4th FC layers. A linear acti-\\nvation function was used in the last FC layer, providing the\\nﬁnal estimates. All units in the rest FC layers were equipped\\nwith the rectiﬁcation (ReLU) non-linearity.\\n3.2. Implementing the CNN-RNN architectures\\nWhen developing the CNN-RNN architecture, the RNN\\npart was fed with the outputs of either the ﬁrst, or the second\\n1975\\nfully connected layer of the respective CNN network. The\\nstructure of the RNN, which we examined, consisted of one\\nor two hidden layers, with 100 -150 units, following either\\nthe LSTM neuron model allowing peephole connections, or\\nthe GRU neuron model. Using one fully connected layer in\\nthe CNN part and two hidden layers in the RNN part was\\nfound to provide the best results.\\nTable 3shows the conﬁguration of the CNN-RNN ar-\\nchitecture. The CNN part of this architecture is based on\\nthe convolutional and pooling layers of the CNN architec-\\ntures described above (in subsection 3.1). It is followed by\\na fully connected layer. Note that in the case of the second\\nscenario, both the outputs of the last pooling layer of the\\nCNN, as well as the 68 landmark 2-D positions ( 68×2val-\\nues) were provided as inputs to this fully connected layer.\\nFor the RNN and fully connected layers, Table 3shows the\\nrespective number of units.\\nTable 3: Architecture for CNN-RNN network based on con-\\nvolution and pooling layers of previously described CNN\\narchitectures\\nblock 1 CNN’s conv & pooling parts\\nblock 2 fully connected 1 4096\\ndropout layer\\nblock 3 RNN layer 1 128\\ndropout layer\\nblock 4 RNN layer 2 128\\ndropout layer\\nblock 5 fully connected 2 2\\nLong evaluation has been performed by selecting differ-\\nent network parameter values. These parameters included:\\nthe batch size used for network parameter updating; the\\nvalue of the learning rate and the strategy for reducing it\\nduring training (e.g. exponential decay in ﬁxed number\\nof epochs); the weight decay parameter value; the dropout\\nprobability value. Final selection of these parameters was\\nsimilar to the CNN cases, apart from the batch size which\\nwas selected in the range 100 (≈3 seconds) - 300 (≈9 sec-\\nonds). Best results have been obtained with batch size 100 .\\n4. Experimental Results\\nIn the following, we provide the main outcomes of the\\nexperimental study, illustrating the above-described cases\\nand scenarios. In all experiments training and validation\\nwas performed in the training set of the Aff-Wild database,\\nwhile testing was performed in the test set of Aff-Wild. The\\nﬁrst approach we tried was based on extracting SIFT fea-\\ntures [ 26] from the facial region and then using an Support\\nV ector Regression (SVR) [ 7] for valence and arousal esti-\\nmation. For training and testing the SVRs, we utilized the\\nscikit-learn library [ 21]. Obtained results were very poor. Inparticular, in all cases, the obtained CCC values were very\\nlow, while very low variance was present in the correspond-\\ning predictions (we do not present the performance of SVR\\nin order not to clutter the results).\\n4.1. Only-CNN architectures\\nTable 4summarizes the obtained CCC and MSE values\\non the test set of Aff-Wild using each of the three afore-\\nmentioned CNN structures as pre-trained networks. The\\nbest results have been obtained using the VGG Face pre-\\ntrained CNN for initialization as shown in Table 4. There-\\nfore, we focus on utilizing this conﬁguration for the results\\npresented in the rest of this section. Moreover, Table 5\\nshows that there is a signiﬁcant improvement in the per-\\nformance, when we also use the 68 2-D landmark positions\\nas input data (case with landmarks). It should be also noted\\nthat we have examined the following scenarios (a) having\\none network for joined estimation of valence and arousal\\nand (b) estimation of the values of valence and arousal us-\\ning two different networks (one for valence and one for\\narousal). Slightly better results were obtained on the lat-\\nter case; so, this architecture is being used in the following\\nresults.\\nFurthermore, we have trained the networks with two dif-\\nferent annotations. The ﬁrst is the annotation provided by\\nthe Aff-Wild database, which is the average over some an-\\nnotators (please see the Aff-Wild[ 27]). The second is the\\nannotation produced by only one annotator (the one with the\\nhighest correlation to the landmarks). Annotations coming\\nfrom a single annotator are generally less smooth than av-\\nerage over annotators. Hence, they are more difﬁcult to be\\nlearned. The results are summarized in Table 6.A s i t w a s\\nexpected it is better to train over the annotation provided by\\nAff-Wild[ 27].\\nTable 4: CCC and MSE evaluation of valence & arousal\\npredictions provided by the CNN architecture when using 3\\ndifferent pre-trained networks for initialization\\nCCC MSE\\nV alence Arousal V alence Arousal\\nVGG Face 0.46 0.35 0.10 0.09\\nVGG-16 0.40 0.30 0.13 0.11\\nResNet-50 0.33 0.24 0.16 0.13\\n4.2. CNN plus RNN architectures\\nLet us now turn to the application of CNN plus RNN\\nend-to-end neural architecture on Aff-Wild. We ﬁrst per-\\nform a comparison between two different units that can be\\nused in an RNN network, i.e. an LSTM vs GRU. Table\\n9summarises the CCC and MSE values when using LSTM\\nand GRU. It can be seen that best results have been obtained\\n1976\\nTable 5: CCC and MSE evaluation of valence & arousal\\npredictions provided by the best CNN network (based on\\nVGG Face), with/without landmarks\\nWith Landmarks Without Landmarks\\nV alence Arousal V alence Arousal\\nCCC 0.46 0.35 0.38 0.31\\nMSE 0.10 0.09 0.14 0.11\\nTable 6: CCC and MSE evaluation of valence & arousal\\npredictions provided by the best CNN network (based on\\nVGG Face), using either one annotators values or the mean\\nof annotators values\\n1 Annotator Mean of Annotators\\nV alence Arousal V alence Arousal\\nCCC 0.35 0.25 0.46 0.35\\nMSE 0.18 0.14 0.10 0.09\\nTable 7: Comparison of best CCC and MSE values of va-\\nlence & arousal provided by best CNN and CNN-RNN ar-\\nchitectures\\nCCC MSE\\nV alence Arousal V alence Arousal\\nCNN 0.46 0.35 0.10 0.09\\nCNN-RNN 0.57 0.43 0.08 0.06\\nTable 8: Effect of Changing Number of Hidden Units &\\nHidden Layers for CCC valence & arousal values in the\\nCNN-RNN architecture\\n1 Hidden Layer 2 Hidden Layers\\nHidden Units V alence Arousal V alence Arousal\\n100 0.40 0.33 0.47 0.40\\n128 0.49 0.40 0.57 0.43\\n150 0.44 0.37 0.50 0.41\\nwhen the GRU model was used. All results reported in the\\nfollowing are, therefore, based on the GRU model. Table\\n7shows the improvement in the CCC and MSE values ob-\\ntained when using the best CNN-RNN end-to-end neural ar-\\nchitecture compared to the best only-CNN one. In particu-\\nlar, we compare networks that take as input facial landmarks\\nand are based on the pre-trained VGG Face network. It can\\nbe seen that this improvement is about 24% in valence esti-\\nmation and about 23% in arousal estimation, which clearly\\nindicates the ability of the CNN-RNN architecture to better\\ncapture the dynamic phenomenon.\\nWe have tested various numbers of hidden layers and\\nhidden units per layer when training and testing the CNN-\\nRNN network. Some characteristic selections and the cor-\\nFigure 4: Predictions vs Ground Truth for valence for a part of\\na video\\nFigure 5: Predictions vs Ground Truth for arousal for a part of\\na video\\nresponding CNN-RNN performances are shown in Table 8.\\nTable 9: CCC and MSE evaluation of valence & arousal\\npredictions provided by the best CNN-GRU and CNN-\\nLSTM architectures that had same network conﬁgurations\\n(2 hidden layers with 128 units each)\\nCCC MSE\\nV alence Arousal V alence Arousal\\nCNN-GRU 0.57 0.43 0.08 0.06\\nCNN-LSTM 0.49 0.38 0.10 0.09\\nIn Figures 4and 5, we qualitatively illustrate some of the\\nobtained results, comparing a segment of the obtained va-\\nlence/arousal predictions compared to the ground truth val-\\nues, in over 6000 consecutive frames of test data.\\n1977\\n5. Conclusions and future work\\nIn this paper, we present the design, implementation\\nand testing of deep learning architectures for the problem\\nof analysing human behaviour utilizing continuous dimen-\\nsional emotions. In more detail, we present, to the best\\nof our knowledge, the ﬁrst such architecture that is trained\\non hundred thousands of data, gathered ”in-the-wild” (i.e.,\\nin entirely uncontrolled conditions) and annotated in terms\\nof continuous emotion dimensions. It should be empha-\\nsized that a major challenge in facial expression and emo-\\ntion recognition lies in the large variability of spontaneous\\nexpressions and emotions, arising in uncontrolled environ-\\nments. This prevents pre-trained models and classiﬁers to\\nbe successfully utilized in new settings and unseen datasets.\\nIn the current paper, our focus has been on experiments in-\\nvestigating the ability of the proposed deep CNN-RNN ar-\\nchitectures to provide accurate predictions of the 2D emo-\\ntion labels in a variety of scenarios, as well as with cross-\\ndatabase experiments. Presented results are very encourag-\\ning, and illustrate the ability of the presented architectures\\nto predict the values of continuous emotion dimensions on\\ndata gathered ”in-the-wild”. Planned future work lies in\\nextending the analysis to simultaneously interpret the be-\\nhaviour of multiple subjects appearing in videos, as well\\nas to further extend the derived representations obtained by\\nthe CNN-RNN architectures for subject and setting speciﬁc\\nadaptation.\\n6. Acknowledgments\\nThe work of Stefanos Zafeiriou has been partially\\nfunded by the FiDiPro program of Tekes (project num-\\nber: 1849/31/2015). The work of Dimitris Kollias was\\nfunded by a Teaching Fellowship of Imperial College Lon-\\ndon. We would like also to acknowledge the contribution of\\nthe Y outube users that gave us the permission to use their\\nvideos (especially Zalzar and Eddie from The1stTake).\\nReferences\\n[1] M. Abadi, A. Agarwal, P . Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia,\\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ´e,\\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P . Tucker,\\nV . V anhoucke, V . V asudevan, F. Vi ´egas, O. Vinyals, P . War-\\nden, M. Wattenberg, M. Wicke, Y . Y u, and X. Zheng. Tensor-\\nFlow: Large-scale machine learning on heterogeneous sys-\\ntems, 2015. Software available from tensorﬂow.org.\\n[2] S. Albanie and A. V edaldi. Learning grimaces by watching\\ntv.arXiv preprint arXiv:1610.02255 , 2016.\\n[3] G. G. Chrysos, E. Antonakos, P . Snape, A. Asthana, and\\nS. Zafeiriou. A comprehensive performance evaluationof deformable face tracking” in-the-wild”. arXiv preprint\\narXiv:1603.06015 , 2016.\\n[4] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical\\nevaluation of gated recurrent neural networks on sequence\\nmodeling. arXiv preprint arXiv:1412.3555 , 2014.\\n[5] C. Corneanu, M. Oliu, J. Cohn, and S. Escalera. Survey on\\nrgb, 3d, thermal, and multimodal approaches for facial ex-\\npression recognition: History, trends, and affect-related ap-\\nplications. IEEE transactions on pattern analysis and ma-\\nchine intelligence , 2016.\\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. Imagenet: A large-scale hierarchical image database.\\nInComputer Vision and Pattern Recognition, 2009. CVPR\\n2009. IEEE Conference on , pages 248–255. IEEE, 2009.\\n[7] H. Drucker, C. J. Burges, L. Kaufman, A. Smola, V . V ap-\\nnik, et al. Support vector regression machines. Advances in\\nneural information processing systems , 9:155–161, 1997.\\n[8] I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning .\\nMIT Press, 2016. http://www.deeplearningbook.\\norg .\\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\\ning for image recognition. In Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recognition , pages\\n770–778, 2016.\\n[10] S. Hochreiter and J. Schmidhuber. Long short-term memory.\\nNeural computation , 9(8):1735–1780, 1997.\\n[11] H. Jung, S. Lee, J. Yim, S. Park, and J. Kim. Joint ﬁne-tuning\\nin deep neural networks for facial expression recognition. In\\nProceedings of the IEEE International Conference on Com-\\nputer Vision , pages 2983–2991, 2015.\\n[12] P . Khorrami, T. Le Paine, K. Brady, C. Dagli, and T. S.\\nHuang. How deep neural networks can improve emotion\\nrecognition on video data. In Image Processing (ICIP), 2016\\nIEEE International Conference on , pages 619–623. IEEE,\\n2016.\\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in neural information processing systems , pages\\n1097–1105, 2012.\\n[14] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning. Nature ,\\n521(7553):436–444, 2015.\\n[15] M. Mathias, R. Benenson, M. Pedersoli, and L. V an Gool.\\nFace detection without bells and whistles. In European Con-\\nference on Computer Vision , pages 720–735. Springer, 2014.\\n[16] A. More. Survey of resampling techniques for improving\\nclassiﬁcation performance in unbalanced datasets. arXiv\\npreprint arXiv:1608.06048 , 2016.\\n[17] H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler.\\nDeep learning for emotion recognition on small datasets us-\\ning transfer learning. In Proceedings of the 2015 ACM on\\nInternational Conference on Multimodal Interaction , pages\\n443–449. ACM, 2015.\\n[18] M. A. Nicolaou, H. Gunes, and M. Pantic. Continuous pre-\\ndiction of spontaneous affect from multiple cues and modal-\\nities in valence-arousal space. Affective Computing, IEEE\\nTransactions on , 2(2):92–105, 2011.\\n1978\\n[19] M. Pantic and L. J. Rothkrantz. Automatic analysis of fa-\\ncial expressions: The state of the art. Pattern Analysis and\\nMachine Intelligence, IEEE Transactions on , 22(12):1424–\\n1445, 2000.\\n[20] O. M. Parkhi, A. V edaldi, and A. Zisserman. Deep face\\nrecognition. In BMVC , volume 1, page 6, 2015.\\n[21] F. Pedregosa, G. V aroquaux, A. Gramfort, V . Michel,\\nB. Thirion, O. Grisel, M. Blondel, P . Prettenhofer, R. Weiss,\\nV . Dubourg, J. V anderplas, A. Passos, D. Cournapeau,\\nM. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-\\nchine learning in Python. Journal of Machine Learning Re-\\nsearch , 12:2825–2830, 2011.\\n[22] F. Ringeval, B. Schuller, M. V alstar, R. Cowie, and M. Pan-\\ntic. Avec 2015: The 5th international audio/visual emotion\\nchallenge and workshop. In Proceedings of the 23rd ACM\\ninternational conference on Multimedia , pages 1335–1336.\\nACM, 2015.\\n[23] J. Schmidhuber. Deep learning in neural networks: An\\noverview. Neural Networks , 61:85–117, 2015.\\n[24] K. Simonyan and A. Zisserman. V ery deep convolutional\\nnetworks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556 , 2014.\\n[25] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A.\\nNicolaou, B. Schuller, and S. Zafeiriou. Adieu features?\\nend-to-end speech emotion recognition using a deep convo-\\nlutional recurrent network. In Acoustics, Speech and Signal\\nProcessing (ICASSP), 2016 IEEE International Conference\\non, pages 5200–5204. IEEE, 2016.\\n[26] A. V edaldi and B. Fulkerson. Vlfeat: An open and portable\\nlibrary of computer vision algorithms. In Proceedings of the\\n18th ACM international conference on Multimedia , pages\\n1469–1472. ACM, 2010.\\n[27] S. Zafeiriou, D. Kollias, M. Nicolaou, A. Papaioannou,\\nG. Zhao, and I. Kotsia. Aff-wild: V alence and arousal ‘in-\\nthe-wild’ challenge. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition Workshop ,\\n2017.\\n1979\\n',\n",
       " \"Recognizing Action Units for\\nFacial Expression Analysis\\nYing-li Tian, Member ,IEEE , Takeo Kanade, Fellow ,IEEE , and Jeffrey F. Cohn, Member ,IEEE\\nAbstract ÐMost automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as\\nhappiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentionsare more often communicated by changes in one or a few discrete facial features. In this paper, we develop an Automatic Face\\nAnalysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial\\nfeatures (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes\\nin facial expression into action units (AUs) of the Facial Action Coding System (FACS), instead of a few prototypic expressions.\\nMultistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes,\\nbrows, cheeks, and furrows. During tracking, detailed parametric descriptions of the facial features are extracted. With these\\nparameters as the inputs, a group of action units (neutral expression, six upper face AUs and 10 lower face AUs) are recognized\\nwhether they occur alone or in combinations. The system has achieved average recognition rates of 96.4 percent (95.4 percent if\\nneutral expressions are excluded) for upper face AUs and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face\\nAUs. The generalizability of the system has been tested by using independent image databases collected and FACS-coded for\\nground-truth by different research teams.\\nIndex Terms ÐComputer vision, multistate face and facial component models, facial expression analysis, facial action coding system,\\naction units, AU combinations, neural network.\\næ\\n1I NTRODUCTION\\nFACIAL expression is one of the most powerful, natural, and\\nimmediate means for human beings to communicate their\\nemotions and intentions. The face can express emotion sooner\\nthan people verbalize or even realize their feelings. In the past\\ndecade, much progress has been made to build computersystems to understand and use this natural form of human\\ncommunication [4], [3], [8], [10], [16], [18], [24], [26], [28], [32],\\n[37], [38], [36], [40]. Most such systems attempt to recognize asmall set of prototypic emotional expressions, i.e., joy,\\nsurprise, anger, sadness, fear, and disgust. This practice may\\nfollow from the work of Darwin [9] and more recently Ekmanand Friesen [13], Friesen [12], and Izard et al. [19] whoproposed that basic emotions have corresponding prototypic\\nfacial expressions. In everyday life, however, such prototypic\\nexpressions occur relatively infrequently. Instead, emotionmore often is communicated by subtle changes in one or a few\\ndiscrete facial features, such as a tightening of the lips in anger\\nor obliquely lowering the lip corners in sadness [7]. Change inisolated features, especially in the area of the eyebrows\\nor eyelids, is typical of paralinguistic displays; for\\ninstance, raising the brows signals greeting [11]. To capturesuch subtlety of human emotion and paralinguisticcommunication, automated recognition of fine-grained\\nchanges in facial expression is needed.1.1 Facial Action Coding System\\nEkman and Friesen [14] developed the Facial Action Coding\\nSystem (FACS) for describing facial expressions by actionunits (AUs). Of 44 FACS AUs that they defined, 30 AUs areanatomically related to the contractions of specific facialmuscles: 12 are for upper face, and 18 are for lower face. AUscan occur either singly or in combination. When AUs occur incombination they may be additive , in which the combination\\ndoes not change the appearance of the constituent AUs, ornonadditive, in which the appearance of the constituents doeschange. Although the number of atomic action units isrelatively small, more than 7,000 different AU combinationshave been observed [30]. FACS provides the descriptivepower necessary to describe the details of facial expression.\\nCommonly occurring AUs and some of the additive and\\nnonadditive AU combinations are shown in Tables 1 and 2.As an example of a nonadditive effect, AU 4 appearsdifferently depending on whether it occurs alone or incombination with AU 1 (as in AU 1\\x874). When AU 4 occurs\\nalone, the brows are drawn together and lowered. In\\nAU1\\x874, the brows are drawn together but are raised due\\nto the action of AU 1. AU 1\\x872is another example of\\nnonadditive combinations. When AU 2 occurs alone, it notonly raises the outer brow, but also often pulls up the innerbrow which results in a very similar appearance toAU1\\x872. These effects of the nonadditive AU combinations\\nincrease the difficulties of AU recognition.\\n1.2 Automated Facial Expression Analysis\\nMost approaches to automated facial expression analysis sofar attempt to recognize a small set of prototypic emotionalexpressions. Suwa et al. [31] presented an early attempt toanalyze facial expressions by tracking the motion of20 identified spots on an image sequence. Essa andIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001 97\\n.Y.-l. Tian and T. Kanade are with the Robotics Institute, Carnegie Mellon\\nUniversity, Pittsburgh, PA 15213. E-mail: {yltian, tk}@cs.cmu.edu.\\n.J.F. Cohn is with the Robotics Institute, Carnegie Mellon University,\\nPittsburgh, and the Department of Psychology, University of Pittsburgh,Pittsburgh, PA 15260. E-mail: jeffcohn@pitt.edu.\\nManuscript received 26 Apr. 2000; revised 5 Oct. 2000; accepted 14 Oct.\\n2000.\\nRecommended for acceptance by K.W. Bowyer.For information on obtaining reprints of this article, please send e-mail to:tpami@computer.org, and reference IEEECS Log Number 112006.\\n0162-8828/01/$10.00 ß2001 IEEE\\nPentland [16] developed a dynamic parametric model based\\non a 3D geometric mesh face model to recognize fiveprototypic expressions. Mase [26] manually selected facialregions that corresponded to facial muscles and computedmotion within these regions using optical flow. The workby Yacoob and Davis [37] used optical flow like Mase'swork, but tracked the motion of the surface regions of facialfeatures (brows, eyes, nose, and mouth) instead of that ofthe underlying muscle groups. Zhang [40] investigated theuse of two types of facial features: the geometric positions of34 fiducial points on a face and a set of multiscale,multiorientation Gabor wavelet coefficients at these pointsfor facial expression recognition.\\nAutomatic recognition of FACS action units (AU) is a\\ndifficult problem, and relatively little work has beenreported. AUs have no quantitative definitions and, as noted,\\ncan appear in complex combinations. Mase [26] and Essa [16]\\ndescribed patterns of optical flow that corresponded toseveral AUs, but did not attempt to recognize them. Bartlettet al. [2] and Donato et al. [10] reported some of the mostextensive experimental results of upper and lower face AUrecognition. They both used image sequences that were freeof head motion, manually aligned faces using three coordi-nates, rotated the images so that the eyes were in horizontal,scaled the images and, finally, cropped a window of60\\x0290pixels. Their system was trained and tested using\\nthe leave-one-out cross-validation procedure, and the meanclassification accuracy was calculated across all of the testcases. Bartlett et al. [2] recognized six single upper face AUs(AU 1, AU 2, AU 4, AU 5, AU 6, and AU 7) but no AUsoccurring in combinations. They achieved 90.9 percentaccuracy by combining holistic spatial analysis and opticalflow with local feature analysis in a hybrid system. Donatoet al. [10] compared several techniques for recognizing actionunits. These techniques included optical flow, principal\\ncomponent analysis, independent component analysis, local\\nfeature analysis, and Gabor wavelet representation. The best\\nperformances were obtained by using Gabor waveletrepresentation and independent component analysis withwhich a 95.5 percent average recognition rate was reported\\nfor six single upper face AUs (AU 1, AU 2, AU 4, AU 5, AU 6,\\nand AU 7) and two lower face AUs and four AU combina-tions (AU 17, AU 18, AU 9\\x8725,A U 10\\x8725,A U 16\\x8725,\\nAU 20\\x8725). For analysis purpose, they treated each\\ncombination as if it were a separate new AU.\\nThe authors' group has developed a few versions of the\\nfacial expression analysis system. Cohn et al. [8] and\\nLien et al. [24] used dense-flow, feature-point tracking,\\nand edge extraction to recognize four upper face AUs andtwo combinations (AU 4, AU 5, AU 6, AU 7, AU 1\\x872,\\nand AU 1\\x874) and four lower face AUs and five\\ncombinations (AU 12, AU 25, AU 26, AU 27, AU 12\\x8725,\\nAU 20\\x8725\\x0616,A U 15\\x8717,A U 17\\x8723\\x8724,a n d\\nAU 9\\x8717\\x0625). Again, each AU combination was\\nregarded as a separate new AU. The average recognition\\nrate ranged from 80 percent to 92 percent depending on\\nthe method used and AUs recognized.\\nThese previous versions have several limitations:\\n1. They require manual marking of 38 to 52 feature\\npoints around face landmarks in the initial input\\nframe. A more automated system is desirable.\\n2. The initial input image is aligned with a standard\\nface image by affine transformation, which assumesthat any rigid head motion is in-plane.\\n3. The extraction of dense flow is relatively slow, which\\nlimits its usefulness for large databases and real-time\\napplications.98 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nTABLE 1\\nUpper Face Action Units and Some Combinations\\n\\n4. Lip and eye feature tracking is not reliable because\\nof the aperture problem and when features undergoa large amount of change in appearance, such asopen to tightly closed mouth or eyes.\\n5. While they used three separate feature extraction\\nmodules, they were not integrated for the purpose ofAU recognition. By integrating their outputs, it islikely that even higher accuracy could be achieved.\\n6. A separate hidden Markov model is necessary for\\neach single AU and each AU combination. Because\\nFACS consists of 44 AUs and potential combinations\\nnumbering in the thousands, a more efficientapproach will be needed.\\nThe current AFA system addresses many of these\\nlimitations:1. Degree of manual preprocessing is reduced by using\\nautomatic face detection [29]. Templates of face\\ncomponents are quickly adjusted in the first frame\\nand then tracked automatically.\\n2. No image alignment is necessary, and in-plane and\\nlimited out-of-plane head motion can be handled.\\n3. To decrease processing time, the system uses a more\\nefficient facial feature tracker instead of a computa-tionally intensive dense-flow extractor. Processingnow requires less than 1 second per frame pair.\\n4. To increase the robustness and accuracy of the\\nfeature extraction, multistate face-component mod-els are devised. Facial feature tracking can cope witha large change of appearance and limited out-of-plane head motion.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 99\\nTABLE 2\\nLower Face Action Units and Some Combinations\\nSingle AU 23 and AU 24 are not included in this table because our database happens to contain only occurences of their combination, but not\\nindividual ones.\\n5. Extracted features are represented and normalized\\nbased on an explicit face model that is invariant toimage scale and in-plane head motion.\\n6. More AUs are recognized and they are recognized\\nwhether they occur alone or in combinations. Insteadof one HMM for each AU or AU combination, thecurrent system employs two Artificial Neural Net-works (one for the upper face and one for thelower face) for AU recognition. It recognizes 16 of the30 AUs that have a specific anatomic basis and\\noccur frequently in emotion and paralinguistic\\ncommunication.\\n2M ULTISTATE FEATURE -BASED AU R ECOGNITION\\nAn automated facial expression analysis system must solve\\ntwo problems: facial feature extraction and facial expression\\nclassification. In this paper, we describe our multistatefeature-based AU recognition system, which explicitly\\nanalyzes appearance changes in localized facial features in\\na nearly frontal image sequence. Since each AU is\\nassociated with a specific set of facial muscles, we believe\\nthat accurate geometrical modeling and tracking of facialfeatures will lead to better recognition results. Furthermore,\\nthe knowledge of exact facial feature positions could be\\nuseful for the area-based [37], holistic analysis [2], and\\noptical-flow-based [24] classifiers.\\nFig. 1 depicts the overall structure of the AFA system.\\nGiven an image sequence, the region of the face and\\napproximate location of individual face features are\\ndetected automatically in the initial frame [29]. Thecontours of the face features and components then are\\nadjusted manually in the initial frame. Both permanent\\n(e.g., brows, eyes, lips) and transient (lines and furrows)\\nface feature changes are automatically detected and tracked\\nin the image sequence. Informed by FACS AUs, we groupthe facial features into separate collections of feature\\nparameters because the facial actions in the upper and\\nlower face are relatively independent for AU recognition\\n[14]. In the upper face, 15 parameters describe shape,\\nmotion, eye state, motion of brow and cheek, and furrows.\\nIn the lower face, nine parameters describe shape, motion,\\nlip state, and furrows. These parameters are geometricallynormalized to compensate for image scale and in-plane\\nhead motion.\\nThe facial feature parameters are fed to two neural-\\nnetwork-based classifiers. One recognizes six upper faceAUs (AU 1, AU 2, AU 4, AU 5, AU 6, AU 7) and\\nNEUTRAL , and the other recognizes 10 lower face AUs\\n(AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU 25, AU 26,AU 27, AU 23\\x8724) andNEUTRAL . These classifiers are\\ntrained to respond to the designated AUs whether theyoccur singly or in combination. When AUs occur incombination, multiple output nodes could be excited. Forthe upper face, we have achieved an average recognitionrate of 96.4 percent for 50 sample sequences of 14 subjectsperforming seven AUs (including NEUTRAL ) singly or in\\ncombination. For the lower face, our system has achieved anaverage recognition rate of 96.7 percent for 63 sample\\nsequences of 32 subjects performing 11 AUs (including\\nNEUTRAL ) singly or in combination. The generalizability\\nof AFA has been tested further on an independent databaserecorded under different conditions and ground-truthcoded by an independent laboratory. A 93.3 percentaverage recognition rate has been achieved for 122 samplesequences of 21 subjects for neutral expression and 16 AUswhether they occurred individually or in combinations.\\n3F ACIAL FEATURE EXTRACTION\\nContraction of the facial muscles produces changes in thedirection and magnitude of the motion on the skin surfaceand in the appearance of permanent and transient facialfeatures. Examples of permanent features are the lips, eyes,and any furrows that have become permanent with age.Transient features include facial lines and furrows that arenot present at rest but appear with facial expressions. Even\\nin a frontal face, the appearance and location of the facial\\nfeatures can change dramatically. For example, the eyeslook qualitatively different when open and closed. Differentcomponents require different extraction and detectionmethods. Multistate models of facial components havebeen introduced to detect and track both transient andpermanent features in an image sequence.\\n3.1 Multistate Face Component Models\\nTo detect and track changes of facial components in nearfrontal images, we develop multistate facial componentmodels. The models are illustrated in Table 3, whichincludes both permanent (i.e., lips, eyes, brows, and cheeks)and transient components (i.e., furrows). A three-state lipmodel describes lip state: open, closed, and tightly closed. A\\ntwo-state model (open or closed) is used for each of the\\neyes. Each brow and cheek has a one-state model. Transientfacial features, such as nasolabial furrows, have two states:present and absent.\\n3.2 Permanent Features\\nLips: A three-state lip model represents open, closed, and\\ntightly closed lips. A different lip contour template isprepared for each lip state. The open and closed lipcontours are modeled by two parabolic arcs, which aredescribed by six parameters: the lip center position (xc, yc),the lip shape ( h1,h2, andw), and the lip orientation ( \\x12). For100 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nFig. 1. Feature-based automatic facial action analysis (AFA) system.\\ntightly closed lips, the dark mouth line connecting the lip\\ncorners represents the position, orientation, and shape.\\nTracking of lip features uses color, shape, and motion. In\\nthe first frame, the approximate position of the lip templateis detected automatically. Then, it is adjusted manually bymoving four key points. A Gaussian mixture modelrepresents the color distribution of the pixels inside of thelip template [27]. The details of our lip tracking algorithmhave been presented in [33].\\nEyes: Most eye trackers developed so far are for open\\neyes and simply track the eye locations [23], [39]. Torecognize facial AUs, however, we need to detect whetherthe eyes are open or closed, the degree of eye opening, andthe location and radius of the iris. For an open eye, the eyetemplate (Table 3), is composed of a circle with threeparameters\\x85x\\n0;y0;r\\x86to model the iris and two parabolic\\narcs with six parameters \\x85xc;yc;h1;h2;w;\\x12\\x86to model the\\nboundaries of the eye. This template is the same as Yuille's[39] except for the two points located at the center of the\\nwhites of the eyes. For a closed eye, the template is reduced\\nto four parameters: two for the position of each of the eyecorners.\\nThe open-eye template is adjusted manually in the first\\nframe by moving six points for each eye. We found that theouter corners are more difficult to track than the innercorners; for this reason, the inner corners of the eyes are\\ntracked first. The outer corners then are located on the line\\nthat connects the inner corners at a distance of the eye widthas estimated in the first frame.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 101\\nTABLE 3\\nMultistate Facial Component Models of a Frontal Face\\n\\nThe iris provides important information about the eye\\nstate. Part of the iris is normally visible if the eye is open.\\nIntensity and edge information are used to detect the iris.We have observed that the eyelid edge is noisy even in a\\ngood quality image. However, the lower part of the iris is\\nalmost always visible and its edge is relatively clear if theeye is open. Thus, we use a half circle mask to filter the irisedge (Fig. 2). The radius of the iris circle template r\\n0is\\ndetermined in the first frame, since it is stable except for\\nlarge out-of-plane head motion. The radius of the circle isincreased or decreased slightly ( \\x0er) fromr\\n0so that it can\\nvary between minimum radius \\x85r0ÿ\\x0er\\x86and maximum\\nradius\\x85r0\\x87\\x0er\\x86. The system determines that the iris is\\nfound when the following two conditions are satisfied: One\\nis that the edges in the mask are at their maximum. The\\nother is that the change in the average intensity is less than athreshold. Once the iris is located, the eye is determined to\\nbe open and the iris center is the iris mask center \\x85x\\n0;y0\\x86.\\nThe eyelid contours then are tracked. For a closed eye, a lineconnecting the inner and outer corners of the eye is used as\\nthe eye boundary. The details of our eye-tracking algorithm\\nhave been presented in [34].\\nBrow and cheek: Features in the brow and cheek areas\\nare also important for expression analysis. Each left or right\\nbrow has one modelÐa triangular template with six\\nparameters\\x85x1;y1\\x86,\\x85x2;y2\\x86, and\\x85x3;y3\\x86. Also, each cheek\\nhas a similar six parameter downward triangular template\\nmodel. Both brow and cheek templates are tracked using\\nthe Lucas-Kanade algorithm [25].\\n3.3 Transient Features\\nIn addition to permanent features that move and change\\ntheir shape and positions, facial motion also produces\\ntransient features that provide crucial information forrecognition of certain AUs. Wrinkles and furrows appear\\nperpendicular to the direction of the motion of the activated\\nmuscles. Contraction of the corrugator muscle, for instance,\\nproduces vertical furrows between the brows, which is\\ncoded in FACS as AU 4, while contraction of the medial\\nportion of the frontalis muscle (AU 1) causes horizontal\\nwrinkling in the center of the forehead.\\nSome of these transient features may become permanent\\nwith age. Permanent crow's-feet wrinkles around the\\noutside corners of the eyes, which are characteristic of\\nAU 6, are common in adults but not in children. When\\nwrinkles and furrows become permanent, contraction of the\\ncorresponding muscles produces only changes in their\\nappearance, such as deepening or lengthening. The pre-\\nsence or absence of the furrows in a face image can be\\ndetermined by edge feature analysis [22], [24], or by eigen-\\nimage analysis [21], [35]. Terzopoulos and Waters [32]detected the nasolabial furrows for driving a face animator,\\nbut with artificial markers. Kwon and Lobo [22] detectedfurrows using snakes to classify pictures of people intodifferent age groups. Our previous system [24] detectedhorizontal, vertical, and diagonal edges using a complex\\nface template.\\nIn our current system, we detect wrinkles in the\\nnasolabial region, the nasal root, and the areas lateral to\\nthe outer corners of the eyes (Fig. 3). These areas are located\\nusing the tracked locations of the corresponding permanentfeatures. We classify each of the wrinkles into one of twostates: present and absent. Compared with the neutralframe, the wrinkle state is classified as present if wrinklesappear, deepen, or lengthen. Otherwise, it is absent.\\nWe use a Canny edge detector to quantify the amount\\nand orientation of furrows [6]. For nasal root wrinkles andcrow's-feet wrinkles, we compare the number of edge pixelsEin the wrinkle areas of the current frame with the number\\nof edge pixels E\\n0of the first frame. If the ratio E=E 0is larger\\nthan a threshold, the furrows are determined to be present.Otherwise, the furrows are absent. For nasolabial furrows,the existence of vertical to diagonal connected edges is usedfor classification. If the connected edge pixels are largerthan a threshold, the nasolabial furrow is determined to bepresent and is modeled as a line. The orientation of the\\nfurrow is represented as the angle between the furrow line\\nand line connecting the eye inner corners. This anglechanges according to different AUs. For example, thenasolabial furrow angle of AU 9 or AU 10 is larger thanthat of AU 12.\\n3.4 Examples of Feature Extraction\\nPermanent Features: Fig. 4 shows the results of tracking\\npermanent features for the same subject with different\\nexpressions. In Figs. 4a, 4b, and 4d, the lips are tracked asthey change in state from open to closed and tightly closed.\\nThe iris position and eye boundaries are tracked while the\\neye changes from widely opened to tightly closed and blink(Figs. 4b, 4c, and 4d). Notice that the semicircular iris model\\ntracks the iris even when the iris is only partially visible.\\nFigs. 5 and 6 show examples of tracking in subjects whovary in age, sex, skin color, and in amount of out-plane\\nhead motion. Difficulty occurs in eye tracking when the eye\\nbecomes extremely narrow. For example, in Fig. 5a, theleft eye in the last image is mistakenly determined to be\\nclosed because the iris was too small to be detected. In\\nthese examples, face size varies between 80\\x0290and102 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nFig. 2. Half circle iris mask. \\x85x0;y0\\x86is the iris center, r0is the iris radius\\nr1is the minimum radius of the mask, and r2is the maximum radius of\\nthe mask.\\nFig. 3. The areas for nasolabial furrows, nasal root, and outer eye\\ncorners.\\n200\\x02220 pixels. For display purpose, images have been\\ncropped to reduce space. Additional results can be found athttp://www.cs.cmu.edu/~face.\\nTransient Features: Fig. 7 shows the results of nasolabial\\nfurrow detection for different subjects and AUs. Thenasolabial furrow angles systematically vary between AU 9and AU 12 (Fig. 7a and 7b). For some images, the nasolabialfurrow is detected only on one side. In the first image ofFig. 7d, only the left nasolabial furrow exists, and it iscorrectly detected. In the middle image of Fig. 7b, the rightnasolabial furrow is missed because the length of thedetected edges is less than threshold. The results of nasalroot and crow's-feet wrinkle detection are shown in Fig. 8.Generally, the crow's-feet wrinkles are present for AU 6,and the nasal root wrinkles appear for AU 9.4F ACIAL FEATURE REPRESENTATION AND AU\\nRECOGNITION by N EURAL NETWORKS\\nWe transform the extracted features into a set of parameters\\nfor AU recognition. We first define a face coordinate\\nsystem. Because the inner corners of the eyes are most\\nreliably detected and their relative position is unaffected by\\nmuscle contraction, we define the x- a x i sa st h el i n e\\nconnecting two inner corners of eyes and the y-axis as\\nperpendicular to it. We split the facial features into twogroups (upper face and lower face) of parameters because\\nfacial actions in the upper face have little interaction with\\nfacial motion in lower face and vice versa [14].\\nUpper Face Features: We represent the upper face features\\nby 15 parameters, which are defined in Table 4. Of these,\\n12 parameters describe the motion and shape of the eyes,TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 103\\nFig. 4. Permanent feature tracking results for different expressions of same subject. Note appearance changes in eye and mouth states. In this and\\nthe following figures, images have been cropped for display purpose. Face size varies between 80\\x0290and200\\x02220pixels.\\nbrows, and cheeks, two parameters describe the state of the\\ncrow's-feet wrinkles, and one parameter describes the\\ndistance between the brows. To remove the effects of\\nvariation in planar head motion and scale between image\\nsequences in face size, all parameters are computed as ratiosof their current values to that in the initial frame. Fig. 9 shows\\nthe coordinate system and the parameter definitions.\\nLower Face Features: Nine parameters represent the lower\\nface features ( Table 5 and Fig. 10). Of these, six parameters\\ndescribe lip shape, state and motion, and three describe thefurrows in the nasolabial and nasal root regions. These\\nparameters are normalized by using the ratios of the current\\nfeature values to that of the neutral frame.AU Recognition by Neural Networks: We use three-\\nlayer neural networks with one hidden layer to recognizeAUs by a standard back-propagation method [29]. Separatenetworks are used for the upper- and lower face. For\\nAU recognition in the upper face, the inputs are the\\n15 parameters shown in Table 4. The outputs are the sixsingle AUs (AU 1, AU 2, AU 4, AU 5, AU 6, and AU7) andNEUTRAL . In the lower face, the inputs are the seven\\nparameters shown in Table 5 and the outputs are 10 single\\nAUs ( AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU23\\x8724, AU 25, AU 26, and AU 27) and NEUTRAL . These\\nnetworks are trained to respond to the designated AUs\\nwhether they occur singly or in combination. When AUs\\noccur in combination, multiple output nodes are excited.104 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nFig. 5. Permanent feature tracking results for different subjects.\\n5E XPERIMENTAL EVALUATIONS\\nWe conducted three experiments to evaluate the performance\\nof our system. The first is AU recognition in the upper facewhen image data contain only single AUs. The second isAU recognition in the upper and lower face when image datacontain both single AUs and combinations. The thirdexperiment evaluates the generalizability of our system byusing completely disjointed databases for training andtesting, while image data contain both single AUs andcombinations. Finally, we compared the performance of oursystem with that of other AU recognition systems.\\n5.1 Facial Expression Image Databases\\nTwo databases were used to evaluate our system: the Cohn-\\nKanade AU-Coded Face Expression Image Database [20]\\nand Ekman-Hager Facial Action Exemplars [15].\\nCohn-Kanade AU-Coded Face Expression Image Data-\\nbase: We have been developing a large-scale database for\\npromoting quantitative study of facial expression analysis[20]. The database currently contains a recording of thefacial behavior of 210 adults who are 18 to 50 years old,\\n69 percent female and 31 percent male, and 81 percent\\nCaucasian, 13 percent African, and 6 percent other groups.Over 90 percent of the subjects had no prior experience inFACS. Subjects were instructed by an experimenter toperform single AUs and AU combinations. Subjects' facial\\nbehavior was recorded in an observation room. Imagesequences with in-plane and limited out-of-plane motionwere included.\\nThe image sequences began with a neutral face and were\\ndigitized into 640\\x02480 pixel arrays with either 8-bit\\ngray-scale or 24-bit color values. To date, 1,917 imagesequences of 182 subjects have been FACS coded by certified\\nFACS coders for either the entire sequence or target AUs.\\nApproximately 15 percent of these sequences were coded bytwo independent certified FACS coders to validate theaccuracy of the coding. Interobserver agreement was quanti-fied with coefficient kappa, which is the proportion of\\nagreement above what would be expected to occur by chance\\n[17]. The mean kappas for interobserver agreement were 0.82for target AUs and 0.75 for frame-by-frame coding.\\nEkman-Hager Facial Action Exemplars: This database\\nwas provided by P. Ekman at the Human Interaction\\nLaboratory, University of California, San Francisco, and\\ncontains images that were collected by Hager, Methvin, andIrwin. Bartlett et al. [2] and Donato et al. [10] used thisdatabase to train and test their AU recognition systems. TheEkman-Hager database includes 24 Caucasian subjects\\n(12 males and 12 females). Each image sequence consists\\nof six to eight frames that were sampled from a longerimage sequence. Image sequences begin with a neutralTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 105\\nFig. 6. Permanent feature tracking results with head motions. (a) Head yaw. (b) Head pitch. (c) Head up and left with background motion.\\nexpression (or weak facial actions) and end with stronger\\nfacial actions. AUs were coded for each frame. Sequences\\ncontaining rigid head motion detectable by a human\\nobserver were excluded. Some of the image sequencescontain large lighting changes between frames and we\\nnormalized intensity to keep the average intensity constant\\nthroughout the image sequence.\\n5.2 Upper Face AU Recognition for Image Data\\nContaining Only Single AUs\\nIn the first experiment, we used a neural network-based\\nrecognizer having the structure shown in Fig. 11. The inputsto the network were the upper face feature parametersshown in Table 4. The outputs were the same set of sixsingle AUs (AU 1, AU 2, AU 4, AU 5, AU 6, AU 7); these arethe same set that were used by Bartlett and Donato. In\\naddition, we included an output node for NEUTRAL . The\\noutput node that showed the highest value was interpreted\\nas the recognized AU. We tested various numbers of hiddenunits and found that six hidden units gave the best\\nperformance.\\nFrom the Ekman-Hager database, we selected image\\nsequences in which only a single AU occurred in the upperface. 99 image sequences from 23 subjects met this criterion.\\nThese 99 image sequences we used are the superset of the\\n80 image sequences used by Bartlett and Donato. The initialand final two frames in each image sequence were used. As\\nshown in Table 6, the image sequences were assigned to\\ntraining and testing sets in two ways. In S1, the sequences106 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nFig. 7. Nasolabial furrow detection results.\\nwere randomly selected, so the same subject was allowed to\\nappear in both training and testing sets. In S2, no subject\\ncould appear in both training and testing sets; testing wasperformed done with novel faces.\\nTable 7 shows the recognition results with the S1testing\\nset. The average recognition rate was 88.5 percent whensamples of NEUTRAL were excluded (Recognizing neutral\\nfaces is easier), and 92.3 percent when samples ofNEUTRAL were included. For the S2test set (i.e., novel\\nfaces), the recognition rate remained virtually identical:\\n89.4 percent ( NEUTRAL exclusive) and 92.9 percent\\n(NEUTRAL inclusive), which is shown in Table 8.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 107\\nFig. 8. Nasal root and crow's-feet wrinkle detection. For the left image of (a), (b), and (c), crow's-feet wrinkles are present. For the right image of ( a),\\n(b), and (c), the nasal root wrinkles appear.\\nTABLE 4\\nUpper Face Feature Representation for AU Recognition\\n\\n5.3 Upper and Lower Face AU Recognition for\\nImage Sequences Containing Both Single AUs\\nand Combinations\\nBecause AUs can occur either singly or in combinations, an\\nAU recognition system must have the ability to recognize\\nthem however they occur. All previous AU recognition\\nsystems [2], [10], [24] were trained and tested on single AUs\\nonly. In these systems, even when AU combinations wereincluded, each combination was treated as if it were a\\nseparate AU. Because potential AU combinations number\\nin the thousands, this method of separately treating\\nAU combinations is impractical. In our second experiment,\\nwe trained a neural network to recognize AUs singly and incombinations by allowing multiple output units of the\\nnetworks to fire when the input consists of AU combinations.\\nUpper Face AUs: The neural network-based recognition\\nsystem for AU combination is shown in Fig. 12. Thenetwork has a similar structure to that used in Experiment 1,where the output nodes correspond to six single AUs plusNEUTRAL . However, the network for recognizing AU\\ncombinations is trained so that when an AU combination ispresented, multiple output nodes that correspond to thecomponent AUs are excited. In training, all of the outputnodes that correspond to the input AU components are setto have the same value. For example, when a training inputis AU 1\\x872\\x874, the output values are trained to be 1.0 for\\nAU 1, AU 2, and AU 4; 0.0 for the remaining AUs and\\nNEUTRAL . At the runtime, AUs whose output nodes\\nshow values higher than the threshold are considered to berecognized.\\nA total of 236 image sequences of 23 subjects from the\\nEkman-Hager database (99 image sequences containingonly single AUs and 137 image sequences containingAU combinations) were used for recognition of AUs inthe upper face. We split them into training (186 sequences)108 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nFig. 9. Upper face features. hl\\x85hl1\\x87hl2\\x86andhr\\x85hr1\\x87hr2\\x86are the\\nheight of left eye and right eye; Dis the distance between brows; cland\\ncrare the motion of left cheek and right cheek. bliandbriare the motion\\nof the inner part of left brow and right brow. bloandbroare the motion of\\nthe outer part of left brow and right brow. flandfrare the left and right\\ncrow's-feet wrinkle areas.\\nTABLE 5\\nLower Face Feature Representation for AUs Recognition\\nFig. 10. Lower face features. h1andh2are the top and bottom lip\\nheights;wis the lip width; Dleftis the distance between the left lip corner\\nand eye inner corners line; Drightis the distance between the right lip\\ncorner and eye inner corners line; n1is the nasal root area.\\nFig. 11. Neural network-based recognizer for single AUs in the upper\\nface. The inputs are the feature parameters, and the output is one labelout of six single AUs and NEUTRAL .\\nand testing (50 sequences) sets by subjects (9 subjects for\\ntraining and 14 subjects for testing) to ensure that the samesubjects did not appear in both training and testing. Testing,\\ntherefore, was done with ªnovel faces.º From experiments,we have found that it was necessary to increase the number\\nof hidden units from six to 12 to obtain optimized\\nperformance.\\nBecause input sequences could contain one or more AUs,\\nseveral outcomes were possible. Correct denotes that the\\nrecognized results were completely identical to the input\\nsamples.Partiallycorrect denotes that some, but not all of\\nthe AUs were recognized ( MissingAUs ) or that AUs that\\ndid not occur were misrecognized in addition to the one(s)\\nthat did (ExtraAUs ). If none of the AUs that occurred were\\nrecognized, the result was Incorrect .\\nUsing (1) and (2), we calculated recognition- and false-\\nalarm rates for input samples and input AU components,\\nrespectively. Human FACS coders typically use the latter to\\ncalculate percentage agreement. We believe, however, thatTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 109\\nTABLE 6\\nDetails of Training and Testing Data from Ekman-Hager Database that Are Used for Single AU Recognition in the Upper Face\\nInS1, some subjects appear in both training and testing sets. In S2, no subject appears in both training and testing sets.\\nTABLE 7\\nAU Recognition for Single AUs on S1Training and Testing Sets in Experiment 1\\nA same subject could appear in both training and testing sets. The numbers in bold are results excluding NEUTRAL .\\nTABLE 8\\nAU Recognition for Single AUs on S2Train and Testing Sets in Experiment 1\\nNo subject appears in both training and testing sets. The numbers in bold are results excluding NEUTRAL .\\nFig. 12. Neural network-based recognizer for AU combinations in the\\nupper face.\\nthe recognition rates based on input samples are the more\\nconservative measures.\\nRecognitionrate \\x88\\nTotalnumberof correctlyrecognized samples\\nTotalnumberof samplesbased on input samples\\nTotalnumberof correctlyrecognized AUs\\nTotalnumberof AUsbased on AU components(\\n\\x851\\x86\\nFalsealarmrate \\x88\\nTotalnumberof recognized sampleswithextraAUs\\nTotalnumberof samplesbased on input samples\\nTotalnumberof extra AUs\\nTotalnumberof AUsbased on AU components :\\x1a\\x852\\x86\\nTable 9 shows a summary of the AU combination\\nrecognition results of 50 test image sequences of 14 subjects\\nfrom the Ekman-Hager database. For input samples, we\\nachieved average recognition and false alarm rates of88 percent and 6.7 percent, respectively, when NEUTRAL\\nwas included, and 82 percent and 12 percent, respectively,\\nwhenNEUTRAL was excluded. AU component-wise, an\\naverage recognition rate of 96.4 percent and a false alarm rateof 6.3 percent were achieved when NEUTRAL was included\\nand a recognition rate of 95.4 percent and a false alarm rate of8.2 percent was obtained when NEUTRAL was excluded.\\nRecognition rates in Experiment 2 were slightly higher\\nthan those in Experiment 1. There are two possible reasons:One is that in the neural network used in Experiment 2,\\nmultiple output nodes could be excited to allow for recogni-\\ntion of AUsoccurring in combinations. Another reason maybethat a larger training data set was used in Experiment 2.\\nLower Face AUs: The same structure of the neural\\nnetwork-based recognition scheme, as shown in Fig. 12,was used, except that the input feature parameters and the\\noutput component AUs now are those for the lower face. The\\ninputs were the lower face feature parameters shown in Table5. The outputs of the neural network were the 11 single AUs(AU 9, AU 10, AU 12, AU 15, AU 17, AU 20, AU 25, AU 26,AU 27, AU 23\\x8724, andNEUTRAL ) (see Table 2). Note that\\nAU23\\x8724is modeled as a single unit, instead of as AU 23\\nand AU 24 separately, because they almost always occurred\\ntogether in our data. Use of 12 hidden units achieved the bestperformance in this experiment.110 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nTABLE 9\\nUpper Face AU Recognition with AU Combinations in Experiment 2\\nThe numbers in bold are results excluding NEUTRAL . TheMissingAUs column shows the AUs that are missed. The ExtraAUs column lists the\\nextra AUs that are misrecognized. The recognized AU with ª*º indicates that it includes both MissingAUs andExtraAUs .\\nA total of 463 image sequences from the Cohn-Kanade\\nAU-Coded Face Expression Image Database were used for\\nlower face AU recognition. Of these, 400 image sequences\\nwere used as the training data and 63 sequences were usedas the testing data. The test data set included 10 single AUs,NEUTRAL , and 11 AU combinations (such as AU 12\\x8725,\\nAU15\\x8717\\x8723,A U 9\\x8717\\x8723\\x8724, and AU 17\\x8720\\x8726)\\nfrom 32 subjects; none of these subjects appeared in trainingdata set. Some of the image sequences contained limitedplanar and out-of-plane head motions.\\nTable 10 shows a summary of the AU recognition results\\nfor the lower face when image sequences contain both\\nsingle AUs and AU combinations. As above, we report the\\nrecognition and false alarm rates based on both the numberof input samples and the number of AU components (see(1) and (2)). With respect to the input samples, an average\\nrecognition rate of 95.8 percent was achieved with a false\\nalarm rate of 4.2 percent when NEUTRAL was included\\nand a recognition rate of 93.7 percent and a false alarm rateof 6.4 percent when NEUTRAL was excluded. With respect\\nto AU components, an average recognition rate of 96.7 per-\\ncent was achieved with a false alarm rate of 2.9 percent\\nwhenNEUTRAL was included, and a recognition rate of\\n95.6 percent with a false alarm rate of 3.9 percent wasobtained when NEUTRAL was excluded.\\nMajor Causes of the Misidentifications: Most of the\\nmisidentifications come from confusions between similar\\nAUs: AU1 and AU2, AU6 and AU7, and AU25 and AU26.\\nThe confusions between AU 1 and AU 2 were caused by thestrong correlation between them. The action of AU 2, whichTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 111\\nTABLE 10\\nLower Face AU Recognition Results in Experiment 2\\n\\nraises the outer portion of the brow, tends to pull the inner\\nbrow up as well (see Table 1). Both AU 6 and AU 7 raise the\\nlower eyelids and are often confused by human AU coders\\nas well [8]. All the mistakes of AU 26 were due to confusionwith AU 25. AU 25 and AU 26 contain parted lips but differonly with respect to motion of the jaw, but jaw motion was\\nnot detected or used in the current system.\\n5.4 Generalizability between Databases\\nTo evaluate the generalizability of our system, we trainedthe system on one database and tested it on another\\nindependent image database that was collected and FACScoded for ground-truth by a different research team. Onewas Cohn-Kanade database and the other was the Ekman-\\nHager database. This procedure ensured a more rigorous\\ntest of generalizability than more usual methods whichdivide a single database into training and testing sets.Table 11 summarizes the generalizability of our system.\\nFor upper face AU recognition, the network was trained\\non 186 image sequences of nine subjects from the Ekman-\\nHager database and tested on 72 image sequences of sevensubjects from the Cohn-Kanade database. Of the 72 imagesequences, 55 consisted of single AUs (AU 1, AU 2, AU 4,AU 5, AU 6, and AU 7) and the others contained\\nAU combinations such as AU 1\\x872,A U 1\\x872\\x874, and\\nAU6\\x877. We achieved a recognition rate of 93.2 percent\\nand a false alarm of 2 percent (when samples of NEUTRAL\\nwere included), which is only slightly (3-4 percent) lower\\nthan the case when the Ekman-Hager database was used for\\nboth training and testing.\\nFor lower face AU recognition, the network was trained\\non 400 image sequences of 46 subjects from the Cohn-Kanade database and tested on 50 image sequences of\\n14 subjects from the Ekman-Hager database. Of the 50 image\\nsequences, half contained AU combinations, such asAU 10\\x8717,A U 10\\x8725,A U 12\\x8725,A U 15\\x8717, and\\nAU20\\x8725. No instances of AU 23\\x8724were available in\\nthe Ekman-Hager database. We achieved a recognition rate\\nof 93.4 percent (when samples of NEUTRAL were\\nincluded). These results were again only slightly lowerthan those of using the same database. The system showedhigh generalizability.\\n5.5 Comparison with Other AU Recognition\\nSystems\\nWe compare the current AFA system's performance with\\nthat of Cohn et al. [8], Lien et al. [24], Bartlett et al. [2], andDonato et al. [10]. The comparisons are summarized in\\nTable 12. When performing comparison of recognition\\nresults in general, it is important to keep in mind\\ndifferences in experimental procedures between systems.For example, scoring methods may be either by dividing thedata set into training and testing sets [8], [24] or by using a\\nleave-one-out cross-validation procedure [2], [10]. Even\\nwhen the same data set is used, the particular AUs thatwere recognized or the specific image sequence that wereused for evaluation are not necessarily the same. Therefore,minor differences in recognition rates between systems are\\nnot meaningful.\\nIn Table 12, the systems were compared along several\\ncharacteristics: feature extraction methods, recognition rates,treatment of AU combinations, AUs recognized, and data-bases used. The terms ªold facesº and ªnovel facesº in the\\nthird column requires some explanation. ªOld facesº means\\nthat in obtaining the recognition rates, some subjects appearin both training and testing sets. ªNovel facesº means nosame subject appears in both training and testing sets; this is\\nobviously a little more difficult case than ªOld faces.º In the\\nfourth column, the terms ª No,º ªYes=Yes ,º and ªYes=No º\\nare used to describe how the AU combinations are treated.ªNoº means that no AU combination was recognized.\\nªYes=Yes º means that AU combinations were recognized\\nand AUs in combination were recognizable individually.\\nªYes=No º means that AU combinations were recognized but\\neach AU combination was treated as if it were a separate newAU. Our current AFA system, while being able to recognize a\\nlarger number of AUs and AU combinations, shows the best\\nor near the best recognition rates even for the tests with ªnovelfacesº or in tests where independent different databases areused for training and testing.\\n6C ONCLUSION\\nAutomatically recognizing facial expressions is important tounderstand human emotion and paralinguistic communica-tion, to design multimodal user interfaces, and to relate\\napplications, such as human identification. The facial action\\ncoding system (FACS) developed by Ekman and Friesen[14] is considered to be one of the best and acceptedfoundations for recognizing facial expressions. Our feature-\\nbased automatic face analysis (AFA) system has shown\\nimprovement in AU recognition over previous systems.\\nIt has been reported [2], [5], [40] that holistic template-\\nbased methods (including image decomposition with image112 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\nTABLE 11\\nGeneralizability to Independent Databases\\nThe numbers in bold are results from independent databases.\\nkernels such as Gabors, Eigenfaces, and Independent\\nComponent Images) outperform explicit parameterization\\nof facial features. Our comparison indicates that a feature-\\nbased method performs just as well as the best holistic\\ntemplate-based method and in more complex data. It may\\nbe premature to conclude that one or the other approach issuperior. Recovering FACS-AUs from video using auto-\\nmatic computer vision techniques is not an easy task and\\nnumerous challenges remain [20]. We feel that furtherefforts will be required for combining both approaches in\\norder to achieve the optimal performance, and that tests\\nwith a substantially large database are called for [1].ACKNOWLEDGMENTS\\nThe authors would like to thank Paul Ekman, at the Human\\nInteraction Laboratory, University of California, San Fran-\\ncisco for providing the Ekman-Hager database. The authors\\nalso thank Zara Ambadar, Bethany Peters, and Michelle\\nLemenager for processing the images. The authors appreci-\\nate the helpful comments and suggestions of Marian\\nBartlett, Simon Baker, Karen Schmidt, and anonymous\\nreviewers. This work was supported by the National\\nInstitute of Mental Health grant R01 MH51435.TIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 113\\nTABLE 12\\nComparison with Other AU Recognition Systems\\nIn the fourth column, ª Noº means that no AU combination was recognized. ª Yes=Yes º means that AU combinations were recognized and AUs in\\ncombination were recognizable individually. ª Yes=No º means that AU combinations were recognized but each AU combination was treated as if it\\nwere a separate new AU.\\nREFERENCES\\n[1] Facial Expression Coding Project, cooperation and competition\\nbetween Carnegie Mellon Univ. and Univ. of California, SanDiego, unpublished, 2000.\\n[2] M. Bartlett, J. Hager, P. Ekman, and T. Sejnowski, ªMeasuring\\nFacial Expressions by Computer Image Analysis,º Psychophysiol-\\nogy, vol. 36, pp. 253-264, 1999.\\n[3] M.J. Black and Y. Yacoob, ªTracking and Recognizing Rigid and\\nNonrigid Facial Motions Using Local Parametric Models of ImageMotion,º Proc. Int'l Conf. Computer Vision, pp. 374-381, 1995.\\n[4] M.J. Black and Y. Yacoob, ªRecognizing Facial Expressions in\\nImage Sequences Using Local Parameterized Models of Image\\nMotion,º Int'l J. Computer Vision, vol. 25, no. 1, pp. 23-48, Oct. 1997.\\n[5] R. Brunelli and T. Poggio, ªFace Recognition: Features versus\\nTemplates,º IEEE Trans. Pattern Analysis and Machine Intelligence,\\nvol. 15, no. 10, pp. 1042-1052, Oct. 1993.\\n[6] J. Canny, ªA Computational Approach to Edge Detection,º IEEE\\nTrans. Pattern Analysis Machine Intelligence, vol. 8, no. 6, June 1986.\\n[7] J.M. Carroll and J. Russell, ªFacial Expression in Hollywood's\\nPortrayal of Emotion,º J. Personality and Social Psychology, vol. 72,\\npp. 164-176, 1997.\\n[8] J.F. Cohn, A.J. Zlochower, J. Lien, and T. Kanade, ªAutomated\\nFace Analysis by Feature Point Tracking has High Concurrent\\nValidity with Manual Faces Coding,º Psychophysiology, vol. 36,\\npp. 35-43, 1999.\\n[9] C. Darwin, The Expression of Emotions in Man and Animals, John\\nMurray, reprinted by Univ. of Chicago Press, 1965, 1872.\\n[10] G. Donato, M.S. Bartlett, J.C. Hager, P. Ekman, and T.J. Sejnowski,\\nªClassifying Facial Actions,º IEEE Trans. Pattern Analysis and\\nMachine Intelligence, vol. 21, no. 10, pp. 974-989, Oct. 1999.\\n[11] I. Eibl-Eihesfeldt, Human Ethology. New York: Aldine de Gruvter,\\n1989.\\n[12] P. Ekman, ªFacial Expression and Emotion,º Am. Psychologist,\\nvol. 48, pp. 384-392, 1993.\\n[13] P. Ekman and W.V. Friesen, Pictures of Facial Affect. Palo Alto,\\nCalif.: Consulting Psychologist, 1976.\\n[14] P. Ekman and W.V. Friesen, The Facial Action Coding System: A\\nTechnique for The Measurement of Facial Movement. San Francisco:\\nConsulting Psychologists Press, 1978.\\n[15] P. Ekman, J. Hager, C.H. Methvin, and W. Irwin, ªEkman-Hager\\nFacial Action Exemplars,º unpublished data, Human InteractionLaboratory, Univ. of California, San Francisco.\\n[16] I.A. Essa and A.P. Pentland, ªCoding, Analysis, Interpretation,\\nand Recognition of Facial Expressions,º IEEE Trans. Pattern\\nAnalysis and Machine Intelligence, vol. 19, no. 7, pp. 757-763, July\\n1997.\\n[17] J. Fleiss, Statistical Methods for Rates and Proportions. New York:\\nWiley, 1981.\\n[18] K. Fukui and O. Yamaguchi, ªFacial Feature Point Extraction\\nMethod Based on Combination of Shape Extraction and PatternMatching,º Systems and Computers in Japan, vol. 29, no. 6, pp. 49-58,\\n1998.\\n[19] C. Izard, L. Dougherty, and E.A. Hembree, ªA System for\\nIdentifying Affect Expressions by Holistic Judgments,º unpub-lished manuscript, Univ. of Delaware, 1983.\\n[20] T. Kanade, J. Cohn, and Y. Tian, ªComprehensive Database for\\nFacial Expression Analysis,º Proc. Int'l Conf. Face and Gesture\\nRecognition, pp. 46-53, Mar. 2000.\\n[21] M. Kirby and L. Sirovich, ªApplication of the k-l Procedure for the\\nCharacterization of Human Faces,º IEEE Trans. Pattern Analysis\\nand Machine Intelligence, vol. 12, no. 1, pp. 103-108, Jan. 1990.\\n[22] Y. Kwon and N. Lobo, ªAge Classification from Facial Images,º\\nProc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 762-\\n767, 1994.\\n[23] K. Lam and H. Yan, ªLocating and Extracting the Eye in Human\\nFace Images,º Pattern Recognition, vol. 29, no. 5, pp. 771-779, 1996.\\n[24] J.-J.J. Lien, T. Kanade, J.F. Cohn, and C.C. Li, ªDetection, Tracking,\\nand Classification of Action Units in Facial Expression,º J. Robotics\\nand Autonomous System, vol. 31, pp. 131-146, 2000.\\n[25] B. Lucas and T. Kanade, ªAn Interative Image Registration\\nTechnique with an Application in Stereo Vision,º Proc. Seventh\\nInt'l Joint Conf. Artificial Intelligence, pp. 674-679, 1981.\\n[26] K. Mase, ªRecognition of Facial Expression from Optical Flow,º\\nIEICE Trans., vol. E74, no. 10, pp. 3474-3483, Oct. 1991.\\n[27] R.R. Rao, ªAudio-Visual Interaction in Multimedia,º PhD thesis,\\nElectrical Eng., Georgia Inst. of Technology, 1998.[28] M. Rosenblum, Y. Yacoob, and L.S. Davis, ªHuman Expression\\nRecognition from Motion Using a Radial Basis Function NetworkArchtecture,º IEEE Trans. Neural Network, vol. 7, no. 5, pp. 1121-\\n1138, 1996.\\n[29] H.A. Rowley, S. Baluja, and T. Kanade, ªNeural Network-Based\\nFace Detection,º IEEE Trans. Pattern Analysis and Machine\\nintelligence, vol. 20, no. 1, pp. 23-38, Jan. 1998.\\n[30] K. Scherer and P. Ekman, Handbook of Methods in Nonverbal\\nBehavior Research. Cambridge, UK: Cambridge Univ. Press, 1982.\\n[31] M. Suwa, N. Sugie, and K. Fujimora, ªA Preliminary Note on\\nPattern Recognition of Human Emotional Expression,º Proc. Int'l\\nJoint Conf. Pattern Recognition, pp. 408-410, 1978.\\n[32] D. Terzopoulos and K. Waters, ªAnalysis of Facial Images Using\\nPhysical and Anatomical Models,º Proc. IEEE Int'l Conf. Computer\\nVision, pp. 727-732, 1990.\\n[33] Y. Tian, T. Kanade, and J. Cohn, ªRobust Lip Tracking by\\nCombining Shape, Color, and Motion,º Proc. Asian Conf. Computer\\nVision, pp. 1040-1045, 2000.\\n[34] Y. Tian, T. Kanade, and J. Cohn, ªDual-State Parametric Eye\\nTracking,º Proc. Int'l Conf. Face and Gesture Recognition, pp. 110-\\n115, Mar. 2000.\\n[35] M. Turk and A. Pentland, ªFace Recognition Using Eigenfaces,º\\nProc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 586-\\n591, 1991.\\n[36] Y. Yacoob and M.J. Black, ªParameterized Modeling and\\nRecognition of Activities,º Proc. of the Sixth Int'l Conf. Computer\\nVision, pp. 120-127, 1998.\\n[37] Y. Yacoob and L.S. Davis, ªRecognizing Human Facial Expression\\nfrom Long Image Sequences Using Optical Flow,º IEEE Trans.\\nPattern Analysis and Machine Intelligence, vol. 18, no. 6, pp. 636-642,\\nJune 1996.\\n[38] Y. Yacoob, H. Lam, and L. Davis, ªRecognizing Face Showing\\nExpressions,º Proc. Int'l Workshop Automatic Face and Gesture\\nRecognition, 1995.\\n[39] A. Yuille, P. Haallinan, and D.S. Cohen, ªFeature Extraction from\\nFaces Using Deformable Templates,º Int'l J. Computer Vision, vol. 8,\\nno. 2, pp. 99-111, 1992.\\n[40] Z. Zhang, ªFeature-Based Facial Expression Recognition: Sensi-\\ntivity Analysis and Experiments with a Multilayer Perceptron,ºInt'l J. Pattern Recognition and Artificial Intelligence, vol. 13, no. 6,\\npp. 893-911, 1999.\\nYing-li Tian received the BS and MS degrees in\\noptical engineering from TianJin University,China, in 1987 and 1990, and the PhD degreein electrical engineering from the Chinese\\nUniversity of Hong Kong, in 1996. After holding\\na faculty position at National Laboratory ofPattern Recognition, Chinese Academy ofSciences, Beijing, she joined Carnegie MellonUniversity in 1998, where she is currently apostdoctoral fellow of the Robotics Institute. Her\\ncurrent research focuses on a wide range of computer vision problems\\nfrom photometric modeling and shape from shading, to human\\nidentification, 3D reconstruction, motion analysis, and facial expressionanalysis. She is a member of the IEEE.114 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 23, NO. 2, FEBRUARY 2001\\n\\nTakeo Kanade received the Doctoral degree in\\nelectrical engineering from Kyoto University,Japan, in 1974. After holding a faculty positionat Department of Information Science, KyotoUniversity, he joined Carnegie Mellon University\\nin 1980. He is currently U.A. Helen Whitaker\\nUniversity Professor of computer science anddirector of the Robotics Institute. Dr. Kanade hasworked in multiple areas of robotics: computervision, manipulators, autonomous mobile robots,\\nand sensors. He has written more than 200 technical papers and reports\\nin these areas, as well as more than 10 patents. He has been the\\nprincipal investigator of a dozen major vision and robotics projects atCarnegie Mellon. Dr. Kanade has been elected to the National Academyof Engineering. He is a fellow of the IEEE, the ACM, a founding fellow ofthe American Association of Artificial Intelligence, and the foundingeditor of the International Journal of Computer Vision . He has received\\nseveral awards including the C&C Award, the Joseph Engelberger\\nAward, JARA Award, Otto Franc Award, Yokogawa Prize, and Marr\\nPrize Award. Dr. Kanade has served the government, industry, and as auniversity advisory or on consultant committees, including Aeronauticsand Space Engineering Board (ASEB) of the National ResearchCouncil, NASA's Advanced Technology Advisory Committee, and theAdvisory Board of Canadian Institute for Advanced Research.Jeffrey F. Cohn received the PhD degree in\\nclinical psychology from the University of Mas-sachusetts at Amherst in 1983. He is anassociate professor of psychology and psychia-try at the University of Pittsburgh and an adjunct\\nfaculty member at the Robotics Institute, Carne-\\ngie Mellon University. Dr. Cohn's primary re-search interests are emotion and paralinguisticcommunication, developmental psychopathol-\\nogy, face image and prosodic analysis, and human-computer interac-tion. He has published more than 60 peer reviewed articles and\\nconference proceedings on these topics. He is member of the IEEE.\\nTIAN ET AL.: RECOGNIZING ACTION UNITS FOR FACIAL EXPRESSION ANALYSIS 115\\n\\n\",\n",
       " 'Recognizing Facial Expression: Machine Learning and Application to\\nSpontaneous Behavior\\nMarian Stewart Bartlett1, Gwen Littlewort1,M a r kF r a n k2, Claudia Lainscsek1,\\nIan Fasel1, Javier Movellan1\\nInstitute for Neural Computation, University of California, San Diego\\n2Rutgers University, New Brunswick, NJ\\nmbartlett@ucsd.edu\\nAbstract\\nW e present a systematic comparison of machine learning\\nmethods applied to the problem of fully automatic recogni-tion of facial expressions. W e report results on a series of\\nexperiments comparing recognition engines, including Ad-\\naBoost, support vector machines, linear discriminant anal-\\nysis. W e also explored feature selection techniques, includ-\\ning the use of AdaBoost for feature selection prior to clas-siﬁcation by SVM or LDA. Best results were obtained by\\nselecting a subset of Gabor ﬁlters using AdaBoost followed\\nby classiﬁcation with Support V ector Machines. The system\\noperates in real-time, and obtained 93% correct general-\\nization to novel subjects for a 7-way forced choice on the\\nCohn-Kanade expression dataset. The outputs of the clas-siﬁers change smoothly as a function of time and thus can\\nbe used to measure facial expression dynamics. W e applied\\nthe system to to fully automated recognition of facial ac-\\ntions (F ACS). The present system classiﬁes 17 action units,\\nwhether they occur singly or in combination with other ac-\\ntions, with a mean accuracy of 94.8%. W e present prelimi-nary results for applying this system to spontaneous facial\\nexpressions.\\n1 Introduction\\nWe present results on a user independent fully automatic\\nsystem for real time recognition of basic emotional expres-\\nsions from video. The system automatically detects frontal\\nfaces in the video stream and codes each frame with respect\\nto 7 dimensions: Neutral, anger, disgust, fear, joy, sadness,\\nsurprise. A second version of the system detects 17 actionunits of the Facial Action Coding System (FACS). We con-\\nducted empirical investigations of machine learning meth-\\nods applied to this problem, including comparison of recog-nition engines and feature selection techniques. Best results\\nwere obtained by selecting a subset of Gabor ﬁlters using\\nAdaBoost and then training Support V ector Machines onthe outputs of the ﬁlters selected by AdaBoost. The com-\\nbination of AdaBoost and SVM’s enhanced both speed andaccuracy of the system. The system presented here is fully\\nautomatic and operates in real-time. We present prelimi-\\nnary results for recognizing spontaneous expressions in an\\ninterview setting.\\n2 Facial Expression Data\\nThe facial expression system was trained and tested on\\nCohn and Kanade’s DFA T-504 dataset [7]. This dataset con-sists of 100 university students ranging in age from 18 to\\n30 years. 65% were female, 15% were African-American,\\nand 3% were Asian or Latino. Videos were recoded in ana-\\nlog S-video using a camera located directly in front of the\\nsubject. Subjects were instructed by an experimenter to per-\\nform a series of 23 facial expressions. Subjects began eachdisplay with a neutral face. Before performing each display,\\nan experimenter described and model ed the desired display.\\nImage sequences from neutral to target display were digi-\\ntized into 640 by 480 pixel arrays with 8-bit precision for\\ngrayscale values. For our study, we selected the 313 se-\\nquences from the dataset that were labeled as one of the 6\\nbasic emotions. The sequences came from 90 subjects, with\\n1 to 6 emotions per subject. The ﬁrst and last frames (neu-tral and peak) were used as training images and for testing\\ngeneralization to new subjects, for a total of 626 examples.\\nThe trained classiﬁers were later applied to the entire se-quence.\\n2.1 Real-time Face Detection\\nWe developed a real-time face detection system that em-\\nploys boosting techniques in a generative framework [5]\\nand extends work by [17]. Enhancements to [17] include\\nemploying Gentleboost instead of Adaboost, smart featuresearch, and a novel cascade training procedure, combined\\nin a generative framework. Source code for the face detec-\\ntor is freely available at http://kolmogorov.sourceforge.net.\\nAccuracy on the CMU-MIT dataset, a standard public data\\nset for benchmarking frontal face detection systems, is 90%\\ndetections and 1/million false alarms, which is state-of-the-\\nart accuracy. The CMU test set has unconstrained lightingand background. With controlled lighting and background,\\nsuch as the facial expression data employed here, detection\\naccuracy is much higher. The system presently operates at24 frames/second on a 3 GHz Pentium IV for 320x240 im-\\nages.\\nAll faces in the DFA T-504 dataset were successfully de-\\ntected. The automatically located faces were rescaled to\\n48x48 pixels. The typical distance between the centers ofthe eyes was roughly 24 pixels. No further registration was\\nperformed. The images were converted into a Gabor mag-\\nnitude representation, using a bank of Gabor ﬁlters at 8 ori-entations and 9 spatial frequencies (2:32 pixels per cycle at\\n1/2 octave steps) (See [9] and [10]).\\n3 Classiﬁcation of Full Expressions\\n3.1 Support V ector Machines\\nWe ﬁrst examined facial expression classiﬁcation based\\non support vector machines (SVM’s). SVM’s are well\\nsuited to this task because the high dimensionality of theGabor representation O( 10\\n5) does not affect training time,\\nwhich depends only on the number of training examples\\nO(102). The system performed a 7-way forced choice be-\\ntween the following emotion categories: Happiness, sad-\\nness, surprise, disgust, fear, anger, neutral. Methods for\\nmulticlass decisions with SVM’s were investigated in [10].\\nHere, the seven-way forced choice was performed in two\\nstages. In stage I, support vector machines performed bi-nary decision tasks using one-versus-all partitioning of the\\ndata, where each SVM discriminated one emotion from ev-\\nerything else. Stage II converted the representation pro-duced by the ﬁrst stage into a probability distribution over\\nthe seven expression categories. This was achieved by pass-\\ning the 7 SVM outputs through a softmax competition.\\nGeneralization to novel subjects was tested using leave-\\none-subject-out cross-validation, in which all images of the\\ntest subject were excluded from training. Linear, polyno-mial, and radial basis function (RBF) kernels with Lapla-\\ncian, and Gaussian basis functions were explored. Lin-\\near and RBF kernels employing a unit-width Gaussian per-formed best, and are presented here. Results are given in\\nTable 1.\\n3.2 Adaboost\\nSVM performance was next compared to Adaboost for\\nemotion classiﬁcation. The features employed for the Ad-\\naboost emotion classiﬁer were the individual Gabor ﬁlters.\\nThis gave 9x8x48x48= 165,888 possible features. A subsetof these features was chosen using Adaboost. On each train-\\ning round, the Gabor feature with the best expression clas-\\nsiﬁcation performance for the current boosting distributionwas chosen. The performance measure was a weighted sum\\nof errors on a binary classiﬁcation t ask, where the weighting\\ndistribution (boosting) was updated at every step to reﬂect\\nhow well each training vector was classiﬁed.\\nAdaboost training continued until the classiﬁer output\\ndistributions for the positive and negative samples were\\ncompletely separated by a gap proportional to the widths\\nof the two distributions. The union of all features selected\\nfor each of the 7 emotion classiﬁers resulted in a total of\\n900 features.\\nClassiﬁcation results are given in Table 1. The general-\\nization performance with Adaboost was comparable to lin-\\near SVM performance. Adaboost had a substantial speed\\nadvantage. There was a 180-fold reduction in the number\\nof Gabor ﬁlters used. Because the system employed a sub-\\nset of ﬁlter outputs at speciﬁc image locations the convo-lutions were calculated in pixel space rather than Fourier\\nspace which reduced the speed advantage, but it neverthe-\\nless resulted in a speed beneﬁt of over 3 times faster thanthe linear SVM.\\n3.3 Linear Discriminant Analysis\\nA previous successful approach to basic emotion recog-\\nnition used Linear Discriminant Analysis (LDA) to classify\\nGabor representations of images [11]. While LDA may be\\noptimal when the class distributions are Gaussian, SVM’smay be more effective when the class distributions are not\\nGaussian. Table 1 compares LDA with SVM’s and Ad-\\naboost. A small ridge term was used in LDA.\\nThe performance results for LDA were dramatically\\nlower than SVMs. Performance with LDA improved by ad-\\njusting the decision threshold for each emotion so as to bal-ance the number of false detects and false negatives. This\\nform of threshold adjustment is commonly employed with\\nLDA classiﬁers, but it uses post-hoc information, whereasthe SVM performance was without post-hoc information.\\nEven with the threshold adjustment, the linear SVM per-\\nformed signiﬁcantly better than LDA. (See Tables 1 and 2.)\\n3.4 Feature selection using PCA\\nMany approaches to LDA a lso employ PCA to perform\\nfeature selection prior to classiﬁcation. For each classi-ﬁer we searched for the number of PCA components which\\ngave maximum LDA performance, which was typically 40\\nto 70 components. The PCA step resulted in a substantial\\nimprovement. The combination of PCA and threshold ad-\\njustment gave performance accuracy of 80.7% for the 7-alternative forced choice, which was comparable to other\\nLDA results in the literature [11]. Nevertheless, the lin-\\near SVM outperformed LDA even with the combination of\\nPCA and threshold adjustment. SVM performance on the\\nPCA representation was signiﬁcantly reduced, indicating an\\nincompatibility between PCA and SVM’s for the problem.\\n3.5 Feature selection by Adaboost\\nAdaboost is not only a fast classiﬁer, it is also a feature\\nselection technique. An advantage of feature selection byAdaboost is that features are selected contingent on the fea-\\ntures that have already been selected. In feature selection by\\nAdaboost, each Gabor ﬁlter is a treated as a weak classiﬁer.\\nAdaboost picks the best of those classiﬁers, and then boosts\\nthe weights on the examples to weight the errors more. Thenext ﬁlter is selected as the one that gives the best perfor-\\nmance on the errors of the previous ﬁlter. At each step, the\\nchosen ﬁlter can be shown to be uncorrelated with the out-put of the previous ﬁlters [6, 15].\\nWe explored training SVM and LDA classiﬁers on the\\nfeatures selected by Adaboost. Here, the classiﬁers were\\nt r a i n e do nt h e continuous outputs of the selected Gabor\\nfeatures, in contrast to the Adaboost classiﬁer which em-\\nployed thresholded outputs. Adaboost was used to select\\n900 features from 9x8x48x48=165888 possible Gabor fea-\\ntures, which were then classiﬁed by the SVM or LDA.\\nThe results are shown in Table 1 and 2. Best performance\\nwas obtained with the combination of Adaboost and SVM’s.\\nWe informally call these combined classiﬁers AdaSVM. We\\ninformally call these combined classiﬁers AdaSVM. The re-sults are shown in Table 1. AdaSVM’s outperformed both\\nAdaboost (z=2 .1,p =0 .2)and SVM’s (z=2 .6,p <\\n.01),w h e r e zis the Z-statistic for comparing success rates\\nof Bernoulli random variables, and pis probability that the\\ntwo performances come from the same distribution. The re-sult of 93.3% accuracy for a user-independent 7-alternative\\nforced choice was encouraging given that previously pub-\\nlished results on this database were 81-83% accuracy (e.g.\\n[2]). AdaSVM’s also carried a substantial speed advantage\\nover SVM’s. The nonlinear AdaSVM was over 400 times\\nfaster than the nonlinear SVM.\\nRegarding LDA, feature selection with Adaboost gave\\nbetter performance than feature selection by PCA and re-\\nduced the difference in performance between LDA and\\nSVM’s. Nevertheless, SVM’s continued to outperform\\nLDA.\\nTable 1. Leave-one-out generalization performance of Ad-\\naboost,SVM’s and AdaSVM’s. AdaSVM: Feature selection by\\nAdaBoost followed by classiﬁcation with SVM’s. LDA pca:L i n -\\near Discriminant analysis with feature selection based on principle\\ncomponent analysis, as commonly implemented in the literature.\\nKernel\\n Adaboost SVM AdaSVM LDA pca\\nLinear\\n 90.1 88.0 93.3 80.7\\nRBF\\n 89.1 93.3Table 2. Comparing SVM performance to LDA with different fea-\\nture selection techniques. The two classiﬁers are compared with\\nno feature selection, with feature selection by PCA, and feature\\nselection by Adaboost.\\nLDA SVM (linear)\\nFeature selection\\nNone\\n 44.4 88.0\\nPCA\\n 80.7 75.5\\nAdaboost\\n 88.2 93.3\\n4 Application to Spontaneous Behavior\\nIn order to objectively capture the richness and complex-\\nity of facial expressions, behavioral scientists have found it\\nnecessary to develop objective coding standards. The facialaction coding system (FACS) [4] is the most objective and\\ncomprehensive coding system in the behavioral sciences.\\nA human coder decomposes facial expressions in terms of46 component movements, which roughly correspond to\\nthe 44 facial muscles. Several research groups have rec-\\nognized the importance of automatically recognizing FACS[3, 16, 14, 8]. Here we apply the system described above to\\nthe problem of fully automated facial action coding.\\n4.1 Spontaneous Expression Database\\nOur collaborators at Rutgers University have collected\\na dataset of spontaneous facial behavior consisting of 100subjects participating in a ’false opinion’ paradigm. In this\\nparadigm, subjects ﬁrst ﬁll out a questionnaire regarding\\ntheir opinions about a social or political issue. Subjects arethen asked to either tell the truth or take the opposite opinion\\non an issue where they rated strong feelings, and convince\\nan interviewer they are telling the truth. This paradigm hasbeen shown to elicit a wide range of emotional expressions\\nas well as speech-related facial expressions. This dataset\\nis particularly challenging both because of speech-related\\nmouth movements, and also because of out-of-plane head\\nrotations which tend to be present during discourse.\\nTwo minutes of each subject’s behavior is being FACS\\ncoded by two certiﬁed FACS coders. FACS codes include\\nthe apex frame as well as the onset and offset frame foreach action unit (AU). Here we present preliminary results\\nfor a system trained on two large datasets of FACS-coded\\nposed expressions, and tested on the spontaneous expres-sion database.\\n4.2 FACS Training\\nThe system was trained on FACS-coded images from 2\\ndatasets. The ﬁrst dataset was the Cohn Kanade dataset,\\nwhich contains FACS scores by two certiﬁed FACS coders\\nin addition to the basic emotion labels. The second dataset\\nconsisted of directed facial actions collected by Hager and\\nEkman. (See [3].) The combined dataset contained 2568\\ntraining examples from 119 subjects. As above, the sys-tem was fully automated. Automatic eye detection [5] was\\nemployed to align the eyes in each image. Images were\\nscaled to 192x192, passed through a bank of Gabor ﬁltersat 8 orientations and 7 spatial frequencies (4:32 pixels per\\ncyc). Output magnitudes were then passed to nonlinear sup-\\nport vector machines using RBF kernels. No feature selec-\\ntion was performed, although we plan to evaluate feature\\nselection by AdaBoost in the near future.\\nSeparate support vector machines, one for each AU,\\nwere trained to perform context-independent recognition.\\nIn context-independent recognition, the system detects the\\npresence of a given AU regardless of the co-occurring AU’s.\\nPositive examples consisted of the last frame of each se-quence which contained the expression apex. Negative ex-\\namples consisted of all apex frames that did not contain the\\ntarget AU plus neutral images obtained from the ﬁrst frameof each sequence, for a total of 2568-N negative examples\\nfor each AU.\\n4.3 Generalization Performance Within Dataset\\nWe ﬁrst report performance for generalization to\\nnovel subjects within the Cohn-Kanade and Ekman-Hager\\ndatabases. Generalization to new subjects was tested us-\\ning leave-one-subject-out cross-validation. The results areshown in Table 3. All system outputs above threshold were\\ntreated as detections. Performance was evaluated for thresh-\\nolds of 0 in the SVM, and then evaluated again for the opti-\\nmal threshold that maximized percent correct.\\nThe system obtained a mean of 94.8% agreement with\\nhuman FACS labels. System outputs for full image se-\\nquences of test subjects are shown in Figure 1. Althougheach individual image is separately processed and classi-\\nﬁed, the outputs change smoothly as a function of expres-\\nsion magnitude in the successive frames of each sequence,enabling applications for measuring the magnitude and dy-\\nnamics of facial expressions.\\nFigure 1. Automated FACS measurements for full image se-\\nquences. Shown are 4 subjects from the Cohn-Kanade dataset pos-\\ning disgust containing AU’s 4,7 and 9. These are test sequences\\nnot used for training.\\nOver 7000 action unit combinations have been reportedTable 3. Performance for fully automatic recognition of 17 facial\\nactions, generalization to novel subjects in the Cohn-Kanade and\\nEkman-Hager databases. N: Total number of positive examples. P:\\nPercent agreement with Human FACS codes (positive and negative\\nexamples classed correctly). P opt: Same with optimal threshold.\\nFA, Hit: Hit and false alarm rates with optimal threshold.\\nAU Name N P P opt FA Hit\\n1 Inn. brow raise 409 90.3 92.9 0.4 71.3\\n2 Out. brow raise 315 91.8 92.8 1.6 62.6\\n4 Brow lower 412 82.7 86.8 6.9 41.05 Upper lid raise 286 91.2 92.9 2.1 61.9\\n6 Cheek raise 278 92.8 93.5 1.4 70.1\\n7 Lower lid tight 403 85.7 88.5 4.6 52.1\\n9 Nose wrinkle 68 98.7 98.8 0.04 85.3\\n10 Lip Raise 50 97.7 98.1 13.9 26.012 Lip crnr. pull 196 97.8 98.0 0.04 93.4\\n15 Lip crnr. depr. 100 97.0 97.2 1.0 72.0\\n17 Chin raise 203 87.0 92.8 7.0 40.420 Lip stretch 99 94.4 96.2 6.6 41.4\\n23 Lip tighten 57 97.0 97.9 11.0 36.8\\n24 Lip press 49 98.4 98.5 1.7 61.225 Lips part 376 89.7 91.2 2.2 64.9\\n26 Jaw drop 86 96.7 97.1 5.9 45.3\\n27 Mouth stretch 81 99.2 99.2 0.04 97.5\\nMean 93.4 94.8 3.9 60.2\\nin the psychology literature, and the problem of how to han-\\ndle recognition of action unit combinations has received\\nconsiderable discussion (e.g. [16, 13]). Here we address\\nrecognition of combinations by training a data-driven sys-\\ntem to detect a given action regardless of whether it appearssingly or in combination with other actions (context inde-\\npendent recognition). A strength of data-driven systems is\\nthat they learn the variations due to combinations, and theyalso learn the most likely contexts of an action. Nonlin-\\near support vector machines have the added advantage of\\nbeing able to handle multimodal data distributions which\\ncan arise with action combinations.\\n1It is an open question\\nwhether building classiﬁers for speciﬁc combinations im-\\nproves recognition performance, and that is a topic of futurework.\\n4.4 Generalization to Spontaneous Expressions\\nThe system described in Section 4.2 was then tested on\\nthe spontaneous expression database. Preliminary results\\n1when the class of kernel is well matched to the problem. The distribu-\\ntion of facial expression data is not well known, and this question requiresempirical study. Several labs in addition to ours have found a range of RBF\\nkernels to be effective for face classiﬁcation tasks.\\nare presented for 12 subjects. This data contained a total of\\n1689 labeled events, consisting of 33 distinct action units,\\n16 of which were AU’s for which we had trained classiﬁers.\\nThe face detector operates for frontal faces of ±10 deg ,\\nwhereas unconstrained head movements during discourse\\ncan rotate outside that range. Face detections were accepted\\nif the face box was greater than 150 pixels width, both eyeswere detected with positive position, and the distance be-\\ntween the eyes was >40 pixels. This resulted in faces found\\nfor 95% of the video frames. All detected faces were passed\\nto the AU recognition system.\\na\\nb\\nFigure 2. Sample system outputs for a 10-second segment con-\\ntaining a brow-raise (FACS code 1+2). System output is shown\\nfor AU 1 (left) and AU 2 (right). Human codes are overlayed for\\ncomparison (onset, apex, offset).\\nHere we present benchmark performance of the basic\\nframe-by-frame system on the video data. Figure 2 shows\\nsample system outputs for one subject, and performance is\\nshown in Table 4. Performance was assessed several ways.\\nFirst, we assessed overall percent correct for each actionunit on a frame-by-frame basis, where system outputs that\\nwere above threshold inside the onset and offset interval\\nindicated by the human FACS codes, and below thresholdoutside that interval were considered correct. This gave an\\noverall accuracy of 90.5% correct across AU’s.\\n2\\nNext an interval analysis was performed which measured\\npercent correct detections on intervals of length I. Here we\\npresent performance for intervals of length 21 (10 on either\\nside of the apex), but performance was stable for a range of\\nchoices of I. A target AU was treated as present if at least\\n6/21 frames were above threshold. An SVM threshold of\\n2Overall percent correct can give high numbers since the AU’s are\\npresent for a small percentage of frames.Table 4. Recognition of spontaneous facial actions. AU: Action\\nunit number. N: Total number of testing examples. Dur.: Mean\\nduration of the AU in frames. P: percent correct over all frames;\\nHit apex : Hit rate for AU apex frame. P ∆: Percent correct for\\ninterval analysis (see text). FA, Hit: Hit and false alarm rates for\\ninterval analysis.\\nAU\\n ND u r . P P ∆ FA Hit\\n1\\n 166 30 84 81 17 48\\n2\\n 138 23 88 79 20 55\\n4\\n 33 23 93 78 22 55\\n5\\n 34 26 98 80 20 33\\n6\\n 56 112 91 86 13 79\\n7\\n 48 78 83 76 22 33\\n9\\n 2 12 100 79 21 100\\n10\\n 53 69 95 76 23 29\\n12\\n 112 102 86 84 11 58\\n15\\n 73 18 98 80 19 40\\n17\\n 88 39 93 78 20 48\\n20\\n 8 8 99 80 20 18\\n23\\n 29 46 94 79 21 36\\n24\\n 66 27 92 77 22 17\\n25\\n 131 65 65 74 21 34\\n26\\n 105 55 92 73 23 27\\nMean\\n 90.5 78.8 19.7 44.4\\n1 standard deviation above the mean was employed. Neg-\\native examples consisted of the remaining 2 minute video\\nstream for each subject, outside the FACS coded onset and\\noffset intervals for the target AU, parsed into intervals of\\n21 frames. Mean percent correct for the interval analysiswas 79%, with hit and false alarm rates of 44% and 20%\\nrespectively.\\n5 Conclusions\\nWe presented a systematic comparison of machine learn-\\ning methods applied to the problem of fully automaticrecognition of facial expressions, including AdaBoost, sup-\\nport vector machines, and linear discriminant analysis, as\\nwell as feature selection methods. Best results were ob-tained by selecting a subset of Gabor ﬁlters using AdaBoost\\nand then training Support V ector Machines on the outputs\\nof the ﬁlters selected by AdaBoost. The combination of\\nAdaboost and SVM’s enhanced both speed and accuracy of\\nthe system. The full system operates in real time. Face de-\\ntection runs at 24 frames/second in 320x240 images on a 3\\nGHz Pentium IV . The expression recognition step operates\\nin less than 10 msec.\\nThe generalization performance to new subjects for\\nrecognition of full facial expressions of emotion in a 7-way\\nforced choice was 93.3%, which is the best performance\\nreported so far on this publicly available dataset. Our re-\\nsults suggest that user independent, fully automatic real\\ntime coding of facial expressions in the continuous video\\nstream is an achievable goal with present computer power,at least for applications in which frontal views can be as-\\nsumed.\\nThe machine-learning based system presented here can\\nbe applied to recognition of any facial expression dimen-\\nsion given a training dataset. Here we applied the sys-\\ntem to fully automated facial action coding, and obtained\\na mean agreement rate of 94.8% for 17 AU’s from the Fa-\\ncial Action Coding System. The outputs of the expressionclassiﬁers change smoothly as a function of time, provid-\\ning information about expression dynamics that was previ-\\nously intractable by hand coding. The system is fully auto-mated, and performance rates are similar to or better than\\nother systems tested on this dataset that employed varying\\nlevels of manual registration. The approach to automaticFACS coding presented here, in addition to being fully au-\\ntomated, also differs from approaches such as [13] and [16]\\nin that instead of designing special purpose image featuresfor each facial action, we explore general purpose learning\\nmechanisms for data-driven facial expression classiﬁcation.\\nThe approach detects not only changes in position of fea-\\nture points, but also changes in image texture such as those\\ncreated by wrinkles, bulges, and changes in feature shapes.\\nHere we presented preliminary r esults for the perfor-\\nmance of the system on spontaneous expressions. The sys-\\ntem was able to detect facial actions in this database de-\\nspite the presence of speech, out-of-plane head movements\\nthat occur during discourse, and the fact that many of the\\naction units occurred in combination. These results pro-vide a benchmark for frame-by-frame analysis by a system\\ntrained for frontal views. The output sequence contains in-\\nformation about dynamics that can be exploited for decid-ing the presence of a facial action [1]. Future work will\\nexplore these dynamics, and compare improvement to the\\nbenchmark provided here. The accuracy of automated fa-\\ncial expression measurement may also be considerably im-\\nproved by 3D alignment of faces. Moreover, information\\nabout head movement dynamics is an important component\\nof nonverbal behavior, and is measured in FACS. Members\\nof this group have developed techniques for automaticallyestimating 3D head pose in a generative model [12] and for\\naligning face images in 3D. These techniques will be inte-\\ngrated into future versions of our system.\\nAcknowledgments. Support for this work was provided\\nby NSF-ITR IIS-0220141, California Digital Media Inno-\\nvation Program DiMI 01-10130, and NRL Advanced Infor-\\nmation Technology 55-05-03.\\nReferences\\n[1] M.S. Bartlett, G. Littlewort, B. Braathen, T.J. Sejnowski,\\nand J.R. Movellan. A protot ype for automatic recognitionof spontaneous facial actions. In S. Becker, S. Thrun, and\\nK. Obermayer, editors, Advances in Neural Information Pro-\\ncessing Systems , volume 15, pages 1271–1278, Cambridge,\\nMA, 2003. MIT Press.\\n[2] I. Cohen, N. Sebe, F. Cozman, M. Cirelo, and T. Huang.\\nLearning baysian network classiﬁers for facial expression\\nrecognition using both lab eled and unlabeled data. Computer\\nVision and Pattern Recognition. , 2003.\\n[3] G. Donato, M. Bartlett, J. Hager, P. Ekman, and T. Se-\\njnowski. Classifying facial actions. IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 21(10):974–989,\\n1999.\\n[4] P. Ekman and W. Friesen. Facial Action Coding System: A\\nTechnique for the Measurement of Facial Movement . Con-\\nsulting Psychologists Press, Palo Alto, CA, 1978.\\n[5] I. R. Fasel, B. Fortenberry, and J. R. Movellan. GBoost: A\\ngenerative framework for boosting with applications to real-\\ntime eye coding. Computer Vision and Image Understand-\\ning, in press.\\n[6] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic\\nregression: a statistical view of boosting, 1998.\\n[7] T. Kanade, J.F. Cohn, and Y . Tian. Comprehensive database\\nfor facial expression analysis. In Proceedings of the fourth\\nIEEE International conference on automatic face and ges-ture recognition (FG’00) , pages 46–53, Grenoble, France,\\n2000.\\n[8] A. Kapoor, Y . Qi, and R.W.Picard. Fully automatic upper\\nfacial action recognition. IEEE International Workshop on\\nAnalysis and Modeling of Faces and Gestures. , 2003.\\n[9] M. Lades, J. V orbr¨ uggen, J. Buhmann, J. Lange, W. Konen,\\nC. von der Malsburg, and R. W ¨ urtz. Distortion invariant\\nobject recognition in the dynamic link architecture. IEEE\\nTransactions on Computers , 42(3):300–311, 1993.\\n[10] G. Littlewort, M.S. Bartlett, I. Fasel, J. Susskind, and J.R.\\nMovellan. Dynamics of facial expression extracted automat-\\nically from video. In IEEE Conference on Computer Vision\\nand Pattern Recognition, Workshop on Face Processing in\\nVideo , 2004.\\n[11] M. Lyons, J. Budynek, A. Plante, and S. Akamatsu. Classify-\\ning facial attributes using a 2-d gabor wavelet representation\\nand discriminant analysis. In Proceedings of the 4th interna-\\ntional conference on automatic face and gesture recognition ,\\npages 202–207, 2000.\\n[12] T. K. Marks, J. Hershey, J. Cooper Roddey, and J. R. Movel-\\nlan. 3d tracking of morphable objects using condition-ally gaussian nonlinear ﬁlters. Computer Vision and Image\\nUnderstanding , under review. See also CVPR04 workshop:\\nGenerative-Model Based Vision.\\n[13] M. Pantic and J.M. Rothkrantz. Automatic analysis of fa-\\ncial expressions: State of the art. IEEE Transactions on Pat-\\ntern Analysis and Machine Intelligence , 22(12):1424–1445,\\n2000.\\n[14] M. Pantic and J.M. Rothkrantz. Facial action recognition\\nfor facial expression analysis from static face images. IEEE\\nTransactions on Systems, Man and Cybernetics , 34(3):1449–\\n1461, 2004.\\n[15] R. E. Schapire. A brief introduction to boosting. In IJCAI ,\\npages 1401–1406, 1999.\\n[16] Y .L. Tian, T. Kanade, and J.F. Cohn. Recognizing ac-\\ntion units for facial expression analysis. IEEE Transactions\\non Pattern Analysis and Machine Intelligence , 23:97–116,\\n2001.\\n[17] Paul Viola and Michael Jones. Robust real-time object detec-\\ntion. Technical Report CRL 20001/01, Cambridge Research-\\nLaboratory, 2001.\\n',\n",
       " 'Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression\\nRecognition in the Wild\\nShan Li, Weihong Deng, and JunPing Du\\nBeijing University of Posts and Telecommunications\\n{ls1995, whDeng, junpingd }@bupt.edu.cn\\nAbstract\\nPast research on facial expressions have used relative-\\nly limited datasets, which makes it unclear whether cur-\\nrent methods can be employed in real world. In this pa-\\nper , we present a novel database, RAF-DB , which contains\\nabout 30000 facial images from thousands of individuals.\\nEach image has been individually labeled about 40 times,\\nthen EM algorithm was used to ﬁlter out unreliable label-\\ns. Crowdsourcing reveals that real-world faces often ex-\\npress compound emotions, or even mixture ones. F or all\\nwe know, RAF-DB is the ﬁrst database that contains com-\\npound expressions in the wild. Our cross-database study\\nshows that the action units of basic emotions in RAF-DB are\\nmuch more diverse than, or even deviate from, those of lab-\\ncontrolled ones. To address this problem, we propose a new\\nDLP-CNN (Deep Locality-Preserving CNN) method, which\\naims to enhance the discriminative power of deep features\\nby preserving the locality closeness while maximizing the\\ninter-class scatters. The benchmark experiments on the 7-\\nclass basic expressions and 11-class compound expression-\\ns, as well as the additional experiments on SFEW and CK+\\ndatabases, show that the proposed DLP-CNN outperforms\\nthe state-of-the-art handcrafted features and deep learning\\nbased methods for the expression recognition in the wild.\\n1. Introduction\\nMillions of images are being uploaded every day by user-\\ns from different events and social gatherings. There is an\\nincreasing interest in designing systems capable of under-\\nstanding human manifestations of emotional attributes and\\naffective displays. To automatic learn the affective state of\\nface images from the Internet, large annotated databases are\\nrequired. However, the complexity of annotations of emo-\\ntion categories has hindered the collection of large annotat-\\ned databases. On the other side, popular AU coding [12]\\nrequires speciﬁc expertise to take months to learn and be\\nperfected, hence, alternative solutions are needed. And dueto the cultural difference in the way of perceiving facial e-\\nmotion [13], it is difﬁcult for psychologists to deﬁne deﬁnite\\nprototypical AUs for each facial expressions. Therefore, it\\nis also worth to study the emotion of social images from the\\njudgments of a large common population, besides from the\\nprofessional knowledge of a few experts.\\nIn this paper, we propose to study the common ex-\\npression perception by a reliable crowdsourcing approach.\\nSpeciﬁcally, our well-trained annotators are asked to label\\nface images with one of the seven basic categories [11],\\nand each face is annotated enough times independently, i.e.\\nabout 40 times in our experiment. Then, the noisy labels\\nare ﬁltered by an EM based reliability evaluation algorithm,\\nthrough which each image can be represented reliably by a\\n7-dimensional emotion probability vector. By analyzing 1.2\\nmillion labels of 29672 great-diverse facial images down-\\nloaded from the Internet, these Real-world Affective Faces\\n(RAF)1are naturally categorized into two types: basic ex-\\npression with single-modal distribution and compound e-\\nmotions with bimodal distribution, an observation support-\\ning a recent ground-breaking ﬁnding in the lab-controlled\\ncondition [10]. To the best of our knowledge, the real-\\nworld expression database RAF-DB is the ﬁrst large-scale\\ndatabase providing the labels of common expression per-\\nception and compound emotions in unconstrained environ-\\nment.\\nThe cross-database experiment and AU analysis on\\nRAF-DB indicates that AUs of real-world expressions are\\nmuch more diverse than, or even deviate from, those of\\nlab-controlled ones guided by psychologists. To address\\nthis ambiguity of unconstrained emotion, we further pro-\\npose a novel Deep Locality-preserving CNN (DLP-CNN).\\nInspired by [17], we develop a practical back-propagation\\nalgorithm which creates a locality preserving loss (LP loss)\\naiming to pull the locally neighboring faces of the same\\nclass together. Jointly trained with the classical softmax\\nloss which forces different classes to stay apart, locality p-\\nreserving loss drives the intra-class local clusters of each\\n1http://whdeng.cn/RAF/model1.html\\n2017 IEEE Conference on Computer Vision and Pattern Recognition\\n1063-6919/17 $31.00 © 2017 IEEE\\nDOI 10.1109/CVPR.2017.2772584\\n\\n/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g38/g286/g258/g396/g296/g437/g367\\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282\\n/g44/g258/g393/g393/g455\\n/g94/g258/g282\\n/;#23#23#23#23/g374/g336/g396/g455\\n/g69/g286/g437/g410/g396/g258/g367\\n(a) DCNN without LP loss\\n/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g38/g286/g258/g396/g296/g437/g367\\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282\\n/g44/g258/g393/g393/g455\\n/g94/g258/g282\\n/;#23#23#23#23/g374/g336/g396/g455\\n/g69/g286/g437/g410/g396/g258/g367\\n(b) DLP-CNN\\n/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g38/g286/g258/g396/g296/g437/g367\\n/g44/g258/g393/g393/g455\\n/g94/g258/g282\\n/;#23#23#23#23/g374/g336/g396/g455\\n(c) Ampliﬁed face images in color from Subﬁgure (b)\\nFigure 1. The distribution of deeply learned features in (a) “DCNN without LP loss” and (b) “DLP-CNN”. As can be seen, locality preserv-\\ning loss layer helps the network to learn features with more discrimination. Moreover, it can be clearly seen that non-neutral expressions\\nwhich have obvious intensity variations, such as Happiness, Sadness, Fear, Surprise and Anger, change the intensity continuously and\\nsmoothly, from low to high, from center to periphery. And images with Disgust label, which is the most confused expression, are assem-\\nbled in the middle. With the neighborhood preserving character of DLP-CNN, the deep feature seems to be able to capture the intrinsic\\nexpression manifold structure to a large extent. Best viewed in color.\\nclass to become compact, and thus the discriminative pow-\\ner of the deeply learned features can be highly enhanced.\\nMoreover, locally neighboring faces tend to share similar e-\\nmotion intensity by using DLP-CNN, which can derive the\\ndiscriminative deep feature with smooth emotion intensity\\ntransition. Figure 1 (b) shows the resulting 2-dimensional\\ndeep features learnt from our DLP-CNN model, where we\\nattach example face images with various intensity in differ-\\nent expression classes.\\nExtensive experiments on RAF-DB and other related\\ndatabases show that the proposed DLP-CNN outperform-\\ns other state-of-the-art methods. Moreover, the activation\\nfeatures trained on RAF-DB can be re-purposed to new\\ndatabases with small-sample training data, suggesting that\\nthe DLP-CNN is a powerful tool to handle the cross-culture\\nproblem on perception of emotion (POE).\\n2. Related Work\\n2.1. Expression image datasets\\nFacial expression recognition largely relies on well-\\ndeﬁned databases, however, several limitations exist.\\nMany available databases were produced in tightly con-\\ntrolled environments without diversity on subjects and con-\\nditions. Subjects in them were taught to act expressions in\\na uniform way. Besides, the majority of current databas-\\nes only include six basic categories or less. However, im-\\nages captured in real-life scenarios often present complex,\\ncompound or even ambiguous emotions rather than simple\\nand prototypical ones [3]. What’s more, labelers in these\\ndatabases are too few, which would reduce the reliability\\nand validity of the emotion labels.\\nWe then focus on discussing image databases with spon-\\ntaneous expressions. SFEW 2.0 [7] contains 700 images\\nextracted from movies, and images were labelled by twoindependent labelers. The database covers unconstrained\\nfacial expressions, varied head poses, large age range, oc-\\nclusions, varied focus, different resolution of face. FER-\\n2013 [16] contains 35887 images collected and labelled us-\\ning the Google image search API. Cropped images are pro-\\nvided in 48 ×48 pixels and converted to grayscale. BP4D-\\nSpontaneous [47] contains plenty of images from 41 sub-\\njects revealing a range of spontaneous expressions elicit-\\ned through eight tasks. However, the database organiza-\\ntion were lab-controlled. AM-FED [30] is collected in real\\nworld with sufﬁcient samples, however, without speciﬁcal\\nemotion labels, it’s more suited for researches on AUs. E-\\nmotioNet [1] is a large database of one million facial expres-\\nsion images in the wild created by an automatic AU detec-\\ntion algorithm. Unlike these databases, RAF-DB simulta-\\nneously satisﬁes multiple requirements: sufﬁcient data, var-\\nious environments, group perceiving on facial expressions\\nand data labels with the least noise.\\n2.2. The framework for expression recognition\\nFacial expression analysis can be generally divided into\\nthree main parts [14]: face aquisition, facial feature extrac-\\ntion and facial expression classiﬁcation.\\nIn face aquisition stage, an automatic face detector is\\nused to locate faces in complex scenes. Feature points are\\nthen used to crop and align faces into a uniﬁed template\\nby geometric transformations. For facial feature extrac-\\ntion, previous methods can be generally categorized into t-\\nwo groups: Appearance-based methods [29] and AU-based\\nmethods [42]. The former uses common feature extraction\\nmethods such as LBP [38], Haar [44]. The latter recog-\\nnizes expression by detecting AUs. Feature classiﬁcation is\\nperformed in the last stage. The commonly used methods\\ninclude SVM, nearest neighbor, LDA, DBN and decision-\\nlevel fusion on these classiﬁers [46]. The extracted facial\\n2585\\nexpression information is either classiﬁed as a set of facial\\nactions or a particular basic emotion [34]. Most focus onthe latter and is based on Ekman’s theory of six basic emo-tions [12]. Indeed, without making additional assumptionsabout how to determine what action units constitute an ex-pression, there can be no exact deﬁnition for the expressioncategory. The basic emotional expressions is therefore notuniversal enough to generalize expressions displayed on hu-man face [37].\\n2.3. Deep learning for expression recognition\\nRecently, deep learning algorithms have been applied\\nto visual object recognition, face veriﬁcation and detec-tion, image classiﬁcation and many other problems, whichachieve state-of-the-art results. So far, there have been afew deep neural networks used in facial expression recogni-tion due to the lack of sufﬁcient training samples. In ICM-L 2013 competition [16], the winner [41] was based onDeep Convolutional Neural Network (DCNN) plus SVM.In EmotiW 2013 competition [6], the winner [19] combinedmodality speciﬁc deep neural network models. In EmotiW2015 [8], more competitors have tried deep learning meth-ods: transfer learning was used to solve the problem ofsmall database in [32], hierarchical committee of multi-column DCNNs in [20] gained the best result on SFEWdatabase, LBP features combined with DCNNs structurewere proposed in [22]. In [24], AU-aware Deep Networks(AUDN) was proposed to learn features with the interpreta-tion of facial AUs. In [31], a DCNN with inception layerswas proposed to gain comparable results.\\n3. Real-world Expression Database: RAF-DB\\n3.1. Creating RAF-DB\\nData collection. At the very beginning, the images’\\nURLs collected from Flickr were fed into an automat-ic open-source downloader to download images in batch-es. Considering that the results returned by Flickr’s im-age search API were in well-structured XML format, fromwhich the URLs can be easily parsed, we then used a setof keywords (for example: smile, giggle, cry, rage, scared,frightened, terriﬁed, shocked, astonished, disgust, expres-sionless) to pick out images that were related with the sixbasic emotions plus the neutral emotion. At last, a to-tal of 29672 real-world facial images are presented in ourdatabase. Figure 2 shows the pipeline of data collection.\\nDatabase annotation. Annotating nearly 30000 images\\nof expression is an extremely difﬁcult and time-consumingtask. Considering the compounded property of real-worldexpressions, multiple views of images’ expression stateshould be collected from different labelers. We thereforeemployed 315 annotators (students and staffs from univer-sities) who have been instructed with one-hour tutorial of\\nFigure 2. Overview of construction and annotation of RAF-DB.\\npsychological knowledge on emotion for an online facialexpression annotation assignment, where they were askedto classify the image into the most apparent one from sevenclasses. We developed a website for RAF-DB annotation,which shows each image with exclusive attribute options.Images were randomly and equally assigned to each label-er, ensuring that there were no direct correlation among theimages labeled by one person. And each image was assuredto be labeled by about 40 independent labelers. After that,a multi-label annotation result is obtained for each image,i.e., a seven dimensional vector that each dimension indi-cates the votes of relevant emotion.\\nMetadata. The data is provided with precise locations\\nand size of the face region, as well as the manually locatedﬁve landmark points (the central of two eyes, the tips of thenose and two corners of the mouth) on the face. Besides, anautomatic landmark annotation mode without manual labelis included: 37 landmarks were picked out from the annota-tion results provided by Face++ API [18]. We also manual-ly annotated the basic attributes (gender, age (5 ranges) andrace) of all RAF faces. In summary, subjects in our databaserange in age from 0 to 70 years old. They are 52% female,43% male, and 5% remains unsure. For racial distribution,there are 77% Caucasian, 8% African-American, and 15%Asian. The pose of each image, including pitch, yaw androll parameters, is computed from the manually labeled lo-cations of the ﬁve facial landmarks.\\nReliability estimation. Due to subjectivity and varied\\nexpertise of labelers and wide ranging levels of images’ d-ifﬁculty, there were some disagreements among annotators.To get rid of noisy labels, motivated by [45], a ExpectationMaximization (EM) framework was used to assess each la-beler’s reliability.\\nLetD={(x\\nj,yj,t1\\nj,t2j,...,tRj)}nj=1denote a set of n la-\\nbeled inputs, where yjis the gold standard label (hidden\\nvariable) for the jth samplesxj,tij∈{1,2,3,4,5,6,7}is\\nthe corresponding label given by the ithannotator. The cor-\\nrect probability of tijare formulated as a sigmoid function:\\np(tij=yj|αi,βj)=( 1 + e x p ( −αiβj))−1, where 1/β jis\\nthe difﬁculty of the jth images,αiis the reliability of ith\\nannotators.\\nOur goal is to optimize the log-likelihood of the given\\n2586\\n/g44/g258/g393/g393/g349/g367/g455/;#23#23#23/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g1010/g1013/g1011/;#23#23#23/g894/g1005/g1011/g856/g1009 ϵ/g1081/g895\\n/g44/g258/g393/g393/g349/g367/g455/;#23#23#23/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282\\n/g1006/g1010/g1010/;#23#23#23/g894/g1010/g856/g1011/g1005/g1081/g895\\n/g94/g258/g282/g367/g455/;#23#23#23/g38/g286/g258/g396/g296/g437/g367\\n/g1005/g1006/g1013/;#23#23#23/g894/g1007/g856/g1006ϲ/g1081/g895\\n/g94/g258/g282/g367/g455/;#23#23#23/g258/g374/g336/g396/g455\\n/g1005/g1010ϯ/;#23#23#23/g894/g1008/g856/g1005ϭ/g1081/g895\\n/g94/g258/g282/g367/g455/;#23#23#23/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g1012/g1010/;#23#23#23/g894/g1006/g856/g1005/g1011/g1081/g895\\n/g94/g258/g282/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282\\n/g1011/g1007/g1012/;#23#23#23/g894/g1005/g1012/g856/g1010 ϯ/g1081/g895\\n/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g258/g374/g336/g396/g455\\n/g1005/g1009/g1004/;#23#23#23/g894/g1007/g856/g1011ϵ/g1081/g895\\n/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282 \\x03\\n/g1009/g1010Ϭ/;#23#23#23/g894/g1005/g1008/g856/g1005ϯ/g1081/g895\\n/g38/g286/g258/g396/g296/g437/g367/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282\\n/g1012/;#23#23#23/g894/g1004/g856/g1006/g1004/g1081/g895\\n/;#23#23#23#23/g374/g336/g396/g349/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282\\n/g1005/g1011/g1010/;#23#23#23/g894/g1008/g856/g1008/g1008/g1081/g895\\n/;#23#23#23#23/g374/g336/g396/g349/g367/g455/;#23#23#23/g282/g349/g400/g336/g437/g400/g410/g286/g282\\n/g1012/g1008ϭ/g894/g1006/g1005/g856/g1006ϯ/g1081/g895\\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282/g367/g455/;#23#23#23/g400/g437/g396/g393/g396/g349/g400/g286/g282 \\x03\\n/g1005ϰϴ/g894/g1007/g856/g1011ϰ/g1081/g895/g94/g437/g396/g393/g396/g349/g400/g286/g282\\n/g1005/g1010/g1005/g1013/;#23#23#23/g894/g1005/g1004/g856/g1009/g1009/g1081/g895\\n/g38/g286/g258/g396/g296/g437/g367\\n/g1007/g1009/g1009/;#23#23#23/g894/g1006/g856/g1007/g1005/g1081/g895\\n/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/g286/g282\\n/g1012/g1011/g1011/;#23#23#23/g894/g1009/g856/g1011/g1006/g1081/g895\\n/g44/g258/g393/g393/g455\\n/g1009/g1013/g1009/g1011/;#23#23#23/g894/g1007/g1012/g856/g1012/g1008/g1081/g895\\n/g94/g258/g282\\n/g1006/g1008/g1010/g1004/;#23#23#23/g894/g1005/g1010/g856/g1004/g1008/g1081/g895\\n/;#23#23#23#23/g374/g336/g396/g455\\n/g1012/g1010/g1011/;#23#23#23/g894/g1009/g856/g1010/g1009/g1081/g895\\nFigure 3. Examples of six-class basic emotions and twelve-class compound emotions from RAF-DB. Detailed data distribution of RAF-DB\\nhas been attached to each expression classes.\\nlabels:\\nmax\\nβ>0l(α,β)=/summationdisplay\\njlnp(t|α,β)=/summationdisplay\\njln/summationdisplay\\nyp(t,y|α,β)\\n=/summationdisplay\\njln/summationdisplay\\nyQj(y)p(t,y|α,β)\\nQj(y)\\n≥/summationdisplay\\nj/summationdisplay\\nyQj(y)l np(t,y|α,β)\\nQj(y)\\nwhereQj(y)is a certain distribution of hidden variable y,\\nQj(yj)=p(tj,yj|α,β)\\n/summationtext\\nyp(tj,yj|α,β)=p(tj,yj|α,β)\\np(tj|α,β)=p(yj|tj,α,β )\\nAfter revision, 285 annotators’ labels have been remained\\nand Cronbach’s Alpha score of all labels is 0.966.\\nSubset Partitions. LetGj={g1,g2,...,g 7}denotes\\nthe 7-dimensional ground truth of the jth image, where\\ngk=R/summationtext\\ni=1αi1ti\\nj=k(αimeans the ith annotators reliabili-\\nty.1Ais an indicator function that evaluates to “1” if the\\nBoolean expression A is true and “0” otherwise.), and label\\nk∈{1,2,3,4,5,6,7}refer to surprise, fear, disgust, hap-\\npiness, sadness, anger and neutral, respectively. We then\\ndivided RAF-DB into different subsets according to the 7-\\ndimensional ground truth. For Single-label Subset, we ﬁrst\\ncalculated the mean distribution value gmean =7/summationtext\\nk=1gk/7\\nfor each image, then picked out label k w.r.t. gk>g mean\\nas the valid label. Images who have single valid label are\\nclassiﬁed into Single-label Subset. For Two-tab Subset, the\\npartition rule is similar. The only difference is that we took\\nout images with neutral label before partition. Figure 3 ex-\\nhibits speciﬁc samples of 6-class basic emotions and 12-\\nclass compound emotions.\\n3.2. CK+ and RAF Cross-Database Study\\nWe then conducted a CK+ [26] and RAF cross-database\\nstudy to explore the speciﬁc difference between expression-Algorithm 1 Label reliability estimation algorithm.\\nInput: Training set D={(xj,t1\\nj,t2\\nj,...,tR\\nj)}n\\nj=1\\nOutput: Each annotator’s reliability α∗\\ni\\nInitialize:\\n∀j=1,...,n , initialize the true label yjusing majority voting\\nβj:=−R/summationtext\\ni=1p(ti\\nj)l np(ti\\nj),αi:= 1 ,\\nThe initial value of βjis image j’s entropy. The higher the en-\\ntropy, the more uncertain the image.\\nRepeat:\\nE-step:\\nQj(yj): =/productdisplay\\nip(yj|tj,αi,βj)\\nM-step:\\nαi:= arg max\\nαi/summationdisplay\\nj/summationdisplay\\nyjQj(yj)l np(tj,yj|αi,βj)\\nQj(yj)\\nWe also optimize βjalong with αiduring M-step. However, the\\ngoal is to get each labeler’s reliability, so we didn’t include it in\\nthis step. For optimization, we take a derivative with respect to\\nβjandαirespectively.\\nUntil convergence\\ns of real-world affective face and the lab-controlled posed\\nface guided by psychologist. Here, “cross-database” mean-\\ns we use all of the images from one database for training\\nand the images from the other for testing. In order to elimi-\\nnate the bias caused by different training size, the single-tab\\nsubset of RAF-DB has been sub-sampled for experiment to\\nbalance the size of two databases.\\nTo ensure the generalization capabilities of the classiﬁer-\\ns, we applied support vector machine for classiﬁcation and\\ntried HOG descriptor [5] for representation. Speciﬁcally,\\noriginal images were ﬁrst aligned to the size of 100 ×100.\\nThen, we got a 4000-dimensional HOG feature vector per\\naligned image. Finally, SVM with RBF kernel implemented\\n2587\\n98.2\\n36.5\\n0.3\\n0.0\\n7.4\\n11.80.0\\n16.4\\n0.0\\n0.0\\n0.8\\n5.20.3\\n24.3\\n76.4\\n0.2\\n30.2\\n61.61.0\\n6.3\\n6.8\\n99.7\\n0.0\\n0.60.3\\n3.9\\n4.4\\n0.1\\n61.2\\n1.60.1\\n12.5\\n12.1\\n0.0\\n0.4\\n19.2\\nSur Fea Dis Hap Sad AngSur\\nFea\\nDis\\nHap\\nSad\\nAng\\n(a) RAF− →CK+73.7\\n20.5\\n10.1\\n5.8\\n4.6\\n1.74.5\\n14.9\\n4.2\\n4.6\\n11.5\\n0.03.3\\n11.8\\n56.0\\n20.6\\n28.1\\n47.80.3\\n22.0\\n0.5\\n62.7\\n2.8\\n0.78.0\\n30.5\\n6.5\\n0.4\\n18.9\\n42.810.1\\n0.3\\n22.7\\n6.0\\n34.1\\n7.0\\nSur Fea Dis Hap Sad AngSur\\nFea\\nDis\\nHap\\nSad\\nAng\\n(b) CK+ − →RAF\\nFigure 4. Confusion matrixes for cross-database experiments using\\nHOG features. The true labels (training data) are on the vertical\\naxis, the predicted labels (test data) are on the horizontal axis.\\nby LibSVM [4] was applied for classiﬁcation. Parameters\\nwere optimized using grid search.\\nWe then performed a cross-database experiment based\\non six-class expression. Multiclass support vector machine\\n(mSVM) and confusion matrix were used as the classiﬁca-\\ntion method and the assessment criteria respectively. Fig-\\nure 4 shows the results of this experiment.\\nAnalyzing the diagonal of these two matrixes, we can\\nsee that surprise, happiness and disgust are the top three\\nthat have the highest recognition rates in both cases. This\\nresult is in line with many single database tests based on\\nCK+, such as [26], [35] and [38]. After calculating the\\naverage of the diagonals, Matrix I was detected with 62%\\naccuracy while Matrix II with only 39%, which indicates\\nthat data collected from real world is more multiple and ef-\\nfective than lab-controlled one. This is particularly evident\\nin the expression of sadness, then happiness and surprise.\\nBesides, anger and disgust are usually confused with each\\nother in both cases, which conforms to the survey in [2].\\nIn order to explain the phenomena above, a more de-\\ntailed research must be conducted to ﬁnd out the speciﬁcal\\ndifferences of each expression between these two databas-\\nes. Therefore, a facial action coding system (FACS) anal-\\nysis has been employed. FACS was ﬁrst presented in [12],\\nwhere the changes on facial behaviors are described by a\\nset of action units (AUs). AUs of sub-sampled images in\\nRAF-DB were ﬁrst labeled by our FACS coders. We then\\nquantitatively analyzed the AU presence for different emo-\\ntions in CK+ and RAF. Some examples from CK+ and RAF\\nare shown in Figure 5. Besides, probabilities of AUs’ oc-\\ncurrence for each expression from sub-sampled images in\\nRAF-DB have been shown in Table 1.\\n4. Deep Locality-Preserving Feature Learning\\nBesides the “in-the-wild” difﬁculties such as variable\\nlighting, poses and occlusions, real-world affective faces\\nat least pose two challenges that demand new algorithm-\\n/;#23#23#23#23/g104/g1006/g1008/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1005/g1011\\n/;#23#23#23#23/g104/g1005/g853/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1006/g1004/g853\\n/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1010\\n/;#23#23#23#23/g104/g1005/g1006\\n/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1013\\n/;#23#23#23#23/g104/g1005/g1011/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1005/g1009\\n/;#23#23#23#23/g104/g1005/g1011/;#23#23#23#23/g104/g1005/g853/g1008\\n/;#23#23#23#23/g104/g1005/g853/g1006\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1006/g1009/g853\\n/;#23#23#23#23/g104/g1006/g1011\\n/;#23#23#23#23/g104/g1005/g853/g1006\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1005/g853/g1006\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1006/g1009\\n /;#23#23#23#23/g104/g1006/g1009/g853\\n/;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1005/g853/g1006\\n /;#23#23#23#23/g104/g1005/g853/g1006\\n/;#23#23#23#23/g104/g1006/g1010\\n/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1009/;#23#23#23#23/g104/g1005/g853/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1006/g1010\\n/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1005/g853/g1008/;#23#23#23#23/g104/g1005\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1006/g1010\\n/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1006/g1004/g853\\n/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010\\n/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010\\n/;#23#23#23#23/g104/g1006/g1010\\n /;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010\\n/;#23#23#23#23/g104/g1005/g1006/;#23#23#23#23/g104/g1010\\n/;#23#23#23#23/g104/g1005/g1011\\n/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1006/g1008\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1005/g1004\\n/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1006/g1010/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1013/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1005/g1004\\n/;#23#23#23#23/g104/g1006/g1010\\n/;#23#23#23#23/g104/g1006/g1011/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1005/g853/g1008\\n/;#23#23#23#23/g104/g1005/g1009\\n/;#23#23#23#23/g104/g1005/g1009\\n/;#23#23#23#23/g104/g1005/g1011\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1006/g1009\\n/;#23#23#23#23/g104/g1005/g1011\\n/;#23#23#23#23/g104/g1005/g853/g1008\\n/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1005/g1004\\n/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1011\\n/;#23#23#23#23/g104/g1013\\n/;#23#23#23#23/g104/g1005/g1011\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1005/g1004\\n/;#23#23#23#23/g104/g1008\\n/;#23#23#23#23/g104/g1009\\n/;#23#23#23#23/g104/g1005/g1004\\n/g90/;#23#23#23#23/g38 /;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g60/g1085/g94/g437/g396/g393/g396/g349/g400/g286\\n/g94/g258/g282/g374/g286/g400/g400/;#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23#23/g349/g400/g336/g437/g400/g410/;#23#23#23#23/g374/g336/g286/g396/g38/g286/g258/g396/g58/g381/g455\\n/;#23#23#23#23/g104/g1006/g1004/g853\\n/;#23#23#23#23/g104/g1006/g1009/;#23#23#23#23/g104/g1011\\nFigure 5. Comparison of six basic emotions from CK+ and RAF.\\nIt’s evident that expression AUs in RAF are more diverse than\\nthose in CK+.\\nTable 1. Probabilities of AUs’ occurrence for each expression in\\nRAF-DB\\n(%) AU1 AU2 AU4 AU5 AU6 AU7 AU9 AU10 AU12 AU15 AU 17 AU20 AU25 AU 26 AU27\\nSur 97 97 84 98 53∗\\nFea 78 42 74 79 50 30∗61∗43∗\\nDis 51 34∗89∗82 26 55∗\\nHap 98 85 97 23\\nSad 88 84 21∗54 49∗\\nAng 9672∗94 36 87 79∗72∗\\nThe empty data indicates the probability is less than 10%\\nAn asterisk(*) indicates the AU’s probability is quite different from CK+’s (at least 40% disparity).\\ns to address. First, as indicated by our cross-database s-\\ntudy, real world expression may associate with various AU\\ncombinations that require classiﬁcation algorithms to model\\nthe multi-modality distribution of each emotion in the fea-\\nture space. Second, as suggested by our crowdsourcing re-\\nsults, a large amount of real-world affective faces express\\ncompound, or even multiple emotions. So traditional hand-\\nengineered representations which perform well on the lab-\\ncontrolled databases are no longer suitable for expression\\nrecognition tasks in the wild.\\nNowadays, DCNN has been proved to outperform hand-\\ncrafted features on lager-scale visual recognition tasks. N-\\nevertheless, conventional DCNN uses only the softmax loss\\nlayer to supervise the training process. The softmax layer\\nhelps keeping the deeply learned features of different class-\\nes separable, however, still remains serious intra-class vari-\\nation. On the contrary, facial expressions in real world show\\nsigniﬁcant intra-class difference on account of varied occlu-\\nsions, illuminations, resolutions and head positions. What’s\\nmore, individual variation can also lead to big difference for\\nthe same category expression, for example, laugh v.s. smile.\\nHence, we proposed a novel DLP-CNN to address the am-\\nbiguity and multi-modality of real-world facial expression-\\ns. In DLP-CNN, we added a new supervised layer on the\\nfundamental architecture shown in Table 2, namely locali-\\nty preserving loss (LP loss), to improve the discrimination\\nability of the deep features.\\nThe basic idea is to preserve the locality of each sample\\nxiand make the local neighborhoods within each class as\\n2588\\nTable 2. The conﬁguration parameters in the fundamental architecture (baseDCNN).\\nLayer\\nType1 2 3 4 5 6 7 8 91 01 11 2 1 3 1 4 1 5 1 6 1 7 1 8\\nConv ReLu MPool Conv ReLu MPool Conv ReLu Conv ReLu MPool Conv ReLu Conv ReLu FC ReLu FC\\nKernel 3 - 2 3 - 2 3 - 3 - 2 3 - 3 - -\\noutput 64 - - 96 - - 128 - 128 - - 256 - 256 - 2000 - 7\\nStride 1 1 2 1 1 2 1111 2 1111 1\\nP a d 10 0 10 0 1010 0 1010 0\\ncompact as possible. To formulate our goal:\\nmin\\nW/summationdisplay\\ni,jSij||xi−xj||2\\n2 (1)\\nwhereWis the network parameters, and the matrix Sis a\\nsimilarity matrix. The deep feature x∈Rddenotes Deep\\nConvolutional activation features (DeCaf) [9] taken from\\nthe ﬁnal hidden layer, i.e., just before the softmax layer that\\nproduces the class prediction. A possible way of deﬁning S\\nis as follows.\\nSij=⎧\\n⎨\\n⎩1,xjis among knearest neighbors of xi\\norxiis among knearest neighbors of xj\\n0,otherwise(2)\\nwherexiandxjbelong to the same class of expression, k\\ndeﬁnes the size of the local neighborhood.\\nThis formulation effectively characterizes the intra-class\\nlocal scatters. Note that xishould be updated as the itera-\\ntive optimization of the CNN. To compute the summation of\\nthe pairwise distance, we need to take the entire training set\\nin each iteration, which is inefﬁcient to implement. To ad-\\ndress this difﬁculty, we do the approximation by searching\\ntheknearest neighbors for each sample xi, and the locality\\npreserving loss function of xiis deﬁned as follow:\\nLlp=1\\n2n/summationdisplay\\ni=1||xi−1\\nk/summationdisplay\\nx∈Nk{xi}x||2\\n2 (3)\\nwhereNk{xi}denotes the ensemble of the knearest neigh-\\nbors of sample xiwith the same class.\\nThe gradients of Llpwith respect to xiis computed as:\\n∂Llp\\n∂xi=xi−1\\nk/summationdisplay\\nx∈Nk{xi}x (4)\\nIn this manner, we can perform the update based on mini-\\nbatch. Note that, the recently proposed center loss [43] can\\nbe considered as a special case of the locality preserving\\nloss, ifk=nc−1(ncis the number of the training samples\\nin class c to which xibelong). While center loss simply\\npulls the samples to a single centroid, the proposed locality\\npreserving loss is more ﬂexible especially when the class\\nconditional distribution is multi-modal.\\nWe then adopt the joint supervision of softmax loss\\nwhich characterizes the global scatter and the locality p-\\nreserving loss which characterizes the local scatters within\\nclass, to train the CNNs for discriminative feature learning.The objective function is formulated as follow: L=\\nLs+λLlp, whereLsdenotes the softmax loss and Llpde-\\nnotes the locality preserving loss. The hyper parameter λis\\nused to balance the two loss functions. Algorithm 2 sum-\\nmarizes the learning process in the deep locality preserving\\nCNN.\\nAlgorithm 2 Optimization algorithm of DLP-CNN.\\nInput: Training data {xi}n\\ni=1,\\nn is the size of mini-batch\\nOutput: Network layer parameters W\\nInitialize: t=0\\nNetwork learning rate μ, hyper parameter λ, Network layer pa-\\nrameters W, softmax loss parameters θ, neighboring nodes k.\\nRepeat:\\n1:t=t+1\\n2: Computer the center of k-nearest neighbor for xi:\\nCt\\ni=1\\nk/summationtextn\\nj=1xt\\njSt\\nij\\n3: Update the softmax loss parameters:\\nθt+1=θt−μt∂Lt\\ns\\n∂θt\\n4: Update the backpropagation error:\\n∂Lt\\n∂xt\\ni=∂Lt\\ns\\n∂xt\\ni+λ∂Lt\\nlp\\n∂xt\\ni5: Computer the network layer parameters:\\nWt+1=Wt−μt∂Lt\\n∂Wt=Wt−μt/summationtextn\\ni=1∂Lt\\n∂xt\\ni∂xt\\ni\\n∂Wt\\nUntil convergence\\n5. Baseline System\\nTo facilitate translating the research from laboratory en-\\nvironments to the real world, we performed two challenging\\nbenchmark experiments on RAF-DB: 7-class basic expres-\\nsion classiﬁcation and 11-class compound expression clas-\\nsiﬁcation, and presented afﬁliated baseline algorithms and\\nperformances. We also conducted comparative experiments\\non two small and popular datasets, CK+ and JAFFE [28].\\nWe followed up the experimental setup in cross-database\\nexperiments, and tried LBP [33], HOG [5] and Gabor [23]\\nfeatures. The LBP descriptor applied the 59-bin LBPu2\\n8,2\\noperator, and then concatenated the histograms from 10 ×10\\npixel cells, generating a 5,900 dimensional feature vector.\\nThe HOG feature used this shape-based segmentation di-\\nviding the image into 10 ×10 pixel blocks of four 5 ×5 pixel\\ncells with no overlapping. By setting 10 bins for each his-\\ntograms, we extract a 4000-dimensional HOG feature vec-\\ntor for each image. For Gabor wavelet, we used a bank of\\n2589\\nTable 3. Basic expression class performance comparison of CK+,\\nJAFFE and RAF along with Compound expression performance\\nof RAF, based on LBP , HOG and Gabor descriptors, and SVM,\\nLDA+kNN classiﬁcation. The metric is the mean diagonal value\\nof the confusion matrix.\\nbasic compound\\nCK+ JAFFE RAF RAF\\nmSVMLBP 88.92 78.81 55.98 28.84\\nHOG 90.50 84.76 58.45 33.65\\nGabor 91.98 88.95 65.12 35.76\\nLDALBP 85.84 77.74 50.97 22.89\\nHOG 91.77 80.12 51.36 24.01\\nGabor 92.33 83.45 56.93 23.81\\n40 Gabor ﬁlters at ﬁve spatial scales and eight orientation-\\ns. The downsample image’s size was set to 10*10, yielding\\n4000-dimensional features.\\nIn order to objectively measure the performance for the\\nfollowers entries, we split the dataset into a training set\\nand a test set with the idea of ﬁve-fold cross-validation,\\nwhich means the size of training set is ﬁve times larger than\\ntest set, and expressions in both sets have a near-identical\\ndistribution. Considering expressions in the wild have\\nimbalanced distribution, the accuracy metric which is\\nespecially sensitive to bias and no longer effective for\\nimbalanced data [15], is no longer used in RAF. Instead,\\nwe use the mean diagonal value of the confusion matrix as\\nthe ultima metric.\\nBasic emotions . In this experiment, seven basic\\nemotion classes were detected using the whole 15339\\nimages from the single-label subset. The best classiﬁcation\\naccuracy (output by SVM) was 72.71% for LBP , 74.35%\\nfor HOG, and 77.28% for Gabor. Results declined to\\n55.98%, 58.45% and 65.12% respectively when using the\\nmean diagonal value of the confusion matrix as metric. To\\nassess the reliability of the basic emotion labels, we also\\nassigned a uniform random label to each sample, which\\nwe call a naive emotion detector. And the best result for\\nthe naive classiﬁer was 16.07% when using Gabor feature,\\nwhich is much lower than the former value.\\nFor comparison, we employed the same methods on\\nCK+ with person-independent 5-fold cross-validation and\\nJAFFE with leave-one-subject-out strategy. The results\\nshown in Table 3 certify that expressions in real world\\nare more difﬁcult for recognition and the current common\\nmethods which perform well on the existing databases\\ncannot solve the expression recognition problem in the\\nchallenging real-world condition.\\nTo evaluate effectiveness of different classiﬁers, we have\\nalso trained LDA with nearest neighbor (NN) classiﬁcation.\\nWe found that LDA+NN were inferior to mSVM obviouslywhen training on RAF, a extremely large database. Nev-\\nertheless, it performed better when training on small-scale\\ndatasets (CK+ and JAFFE), even outperformed mSVM in\\nsome cases. Concrete results can be viewed in Table 3.\\nCompound emotions. For compound emotions clas-\\nsiﬁcation, we got rid of fearfully disgusted emotion as it’s\\ntoo few, leaving 11 classes of compound emotion, 3954\\nin total. The best classiﬁcation accuracy (output by SVM)\\nwas 45.51% for LBP , 51.89% for HOG, and 53.54% for\\nGabor. Results declined to 28.84%, 33.65% and 35.76%\\nrespectively when using the mean diagonal value of the\\nconfusion matrix as metric. Again, to demonstrate the\\nreliability of the compound emotion labels, we computed\\nthe baseline for the naive emotion detector, which declined\\nto 5.79% when using Gabor feature.\\nAs expected, the overall performance dropped sig-\\nniﬁcantly when more expressions are involved for\\nclassiﬁcation. The signiﬁcantly lower results compared to\\nthat of basic emotions indicate that compound emotions\\nare more difﬁcult to detect and new methods should be\\ninvented to solve this problem. Besides the multi-modality,\\nlack of training samples of compound expressions from\\nreal world is another great technical challenge.\\n6. Deep Learning System\\nNowadays, deep learning has been applied to lager-scale\\nvisual recognition tasks and perform exceedingly well with\\nlager amounts of training data. However, fully-supervised\\ndeep models are easy to be overﬁtting on facial expression\\nrecognition task due to the insufﬁcient training samples for\\nthe model learning. Therefore, most deep learning frame-\\nworks employed on facial expression recognition [22, 32,\\n36] are base on pre-trained models. These pre-trained mod-\\nels, such as VGG network [40] and AlexNet [21], are ini-\\ntially designed for face recognition, which are short of dis-\\ncrimination ability of expression characteristic. So in this\\npaper, we directly trained our deep learning system on the\\nbig enough self-collected database RAF from scratch, with-\\nout using other databases.\\nWhen conducting experiments, we followed the same\\ndataset partition standards, image processing methods and\\nclassiﬁcation methods as in the baseline system. Related\\nresearches [9, 39] have proved that well-trained deep con-\\nvolutional network can work as a feature extraction tool\\nwith generalization ability for the classiﬁcation task. Fol-\\nlowing up this idea, we ﬁrst trained each DCNNs for basic\\nemotion recognition task, and then directly used the already\\ntrained DCNN models to extract deep features for both ba-\\nsic and compound expressions. 2000-dimensional deep fea-\\ntures learnt from raw data were extracted from the penulti-\\nmate fully connected layer of the DCNNs and then classi-\\nﬁed by SVM.\\n2590\\nTable 4. Expression recognition performance of different DCNNs on RAF. The metric is the mean diagonal value of the confusion matrix.\\nbasic compound\\nAnger Disgust Fear Happiness Sadness Surprise Neutral Average Average\\nmSVMVGG 68.52 27.50 35.13 85.32 64.85 66.32 59.88 58.22 31.63\\nAlexNet 58.64 21.87 39.19 86.16 60.88 62.31 60.15 55.60 28.22\\nbaseDCNN 70.99 52.50 50.00 92.91 77.82 79.64 83.09 72.42 40.17\\ncenter loss 68.52 53.13 54.05 93.08 78.45 79.63 83.24 72.87 39.97\\nDLP-CNN 71.60 52.15 62.16 92.83 80.13 81.16 80.29 74.20 44.55\\nLDAVGG 66.05 25.00 37.84 73.08 51.46 53.49 47.21 50.59 16.27\\nAlexNet 43.83 27.50 37.84 75.78 39.33 61.70 48.53 47.79 15.56\\nbaseDCNN 66.05 47.50 51.35 89.45 74.27 76.90 77.50 69.00 28.23\\ncenter loss 64.81 49.38 54.05 92.41 74.90 76.29 77.21 69.86 27.33\\nDLP-CNN 77.51 55.41 52.50 90.21 73.64 74.07 73.53 70.98 32.29\\nFrom the results in Table 4, we have the following obser-\\nvations. First, DCNNs which achieve quite reasonable re-\\nsults for large-scale image recognition setting, such as VGG\\nnetwork and AlexNet, are not efﬁcient for facial expression\\nrecognition. Second, all of the deep features learnt on RAF-\\nDB outperform the unlearned features used in the baseline\\nsystem by a signiﬁcant margin, which indicates that deep\\nlearning architecture is more robust and applicable for both\\nbasic and compound expression. At last, our new locali-\\nty preserving loss model achieves better performance than\\nthe based one and the center loss one. Note that, the center\\nloss, which efﬁciently converges unimodal class, can help\\nenhance the network performance on basic emotion, but it\\nfails on compound emotion. This shows the advantage of\\nthe locality preserving loss on multi-modal facial expres-\\nsion recognition, including both basic and compound one.\\nTo see the generalization ability of our well-trained DLP-\\nCNN model on other databases, we then employed it to di-\\nrectly extract ﬁxed-length feature of CK+ and SFEW 2.0\\nwithout ﬁnetune. For the lab-controlled databases CK+, we\\nfollowed the experimental principle in the baseline system.\\nFor the real-world database SFEW 2.0, we followed the rule\\nin EmotiW 2015 [8], and the “SFEW best” is the result of\\nthe single best model used in the winner [20] of EmotiW\\n2015. Note that, in [20], the Authors trained their model\\nwith extra data from SFEW. From the comparison results\\nin Table 5, we can see that our network can also achieve\\ncomparable or even better performance than other state-of-\\nthe-art methods, not only for RAF, but also other databases.\\nThis indicates that our proposed network can be used as an\\nefﬁcient and effective feature extraction tool for facial ex-\\npression databases, without a signiﬁcant amount of time to\\nexecute in traditional DCNNs.\\n7. Conclusions and Future Work\\nThe main contribution of this paper is presenting a novel\\noptimized algorithm for crowdsourcing and a new locali-Table 5. Comparison results of DLP-CNN and other state-of-the-\\nart deep learning methods on CK+ and SFEW 2.0.\\nAUDN\\n[25]FP+SAE\\n[27][31]SFEW best\\n[20]DLP-CNN\\n(without ﬁnetune)\\nCK+ 93.70 91.11 93.2 – 95.78\\nSFEW 2.0 30.14 – 47.7 52.5 51.05\\nty preserving loss layer for deep learning, based on a real-\\nworld publicly available facial expression database RAF-\\nDB. The optimized algorithm helps to keep the best anno-\\ntated results from labelers. The new DCNN can learn more\\ndiscriminative feature for expression recognition task. The\\nRAF-DB contains, 1) 29672 real-world images labeled for\\ndifferent expressions, age range, gender and posture fea-\\nture, 2) a 7-dimensional expression distribution vector for\\neach image, 3) two different subsets: single-label subset,\\nincluding seven classes of basic emotions; two-tab subset,\\nincluding twelve classes of compound emotions, 4) loca-\\ntions of ﬁve manually accurate detect landmark points, 5)\\nbaseline classiﬁer outputs for basic emotions and compound\\nemotions. We hope that the release of this database will en-\\ncourage more researches on the effect of real-world expres-\\nsion distribution or detection and be a useful benchmark re-\\nsource for researchers to compare the validity of their facial\\nexpression analysis algorithms in challenge conditions.\\n8. Acknowledgments\\nThis work was partially supported by the Na-\\ntional Natural Science Foundation of China (Project\\n6157306861471048, 61375031 and 61532006), Beijing\\nNova Program under Grant No. Z161100004916088, the\\nFundamental Research Funds for the Central Universities\\nunder Grant No. 2014ZD03-01, and the Program for New\\nCentury Excellent Talents in University(NCET-13-0683).\\n2591\\nReferences\\n[1] C. F. Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. E-\\nmotionet: An accurate, real-time algorithm for the automatic\\nannotation of a million facial expressions in the wild. In\\nProceedings of IEEE International Conference on Comput-\\ner Vision & Pattern Recognition (CVPR16), Las V egas, NV ,\\nUSA , 2016.\\n[2] V . Bettadapura. Face expression recognition and analysis:\\nthe state of the art. arXiv preprint arXiv:1203.6722 , 2012.\\n[3] J. C. Borod. The neuropsychology of emotion . Oxford Uni-\\nversity Press New Y ork, 2000.\\n[4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for sup-\\nport vector machines. ACM Transactions on Intelligen-\\nt Systems and Technology , 2:27:1–27:27, 2011. Soft-\\nware available at http://www.csie.ntu.edu.tw/\\n˜cjlin/libsvm .\\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In Computer Vision and Pattern Recogni-\\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\\non, volume 1, pages 886–893. IEEE, 2005.\\n[6] A. Dhall, R. Goecke, J. Joshi, M. Wagner, and T. Gedeon.\\nEmotion recognition in the wild challenge 2013. In Pro-\\nceedings of the 15th ACM on International conference on\\nmultimodal interaction , pages 509–516. ACM, 2013.\\n[7] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon. Static fa-\\ncial expression analysis in tough conditions: Data, evalua-\\ntion protocol and benchmark. In Computer Vision Workshops\\n(ICCV Workshops), 2011 IEEE International Conference on ,\\npages 2106–2112. IEEE, 2011.\\n[8] A. Dhall, O. Ramana Murthy, R. Goecke, J. Joshi, and\\nT. Gedeon. Video and image based emotion recognition chal-\\nlenges in the wild: Emotiw 2015. In Proceedings of the 2015\\nACM on International Conference on Multimodal Interac-\\ntion , pages 423–426. ACM, 2015.\\n[9] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\\nvation feature for generic visual recognition. In ICML , pages\\n647–655, 2014.\\n[10] S. Du, Y . Tao, and A. M. Martinez. Compound facial expres-\\nsions of emotion. Proceedings of the National Academy of\\nSciences , 111(15):E1454–E1462, 2014.\\n[11] P . Ekman. Facial expression and emotion. American psy-\\nchologist , 48(4):384, 1993.\\n[12] P . Ekman and W. V . Friesen. Facial action coding system.\\n1977.\\n[13] P . Ekman, W. V . Friesen, M. O’Sullivan, A. Chan,\\nI. Diacoyanni-Tarlatzis, K. Heider, R. Krause, W. A.\\nLeCompte, T. Pitcairn, P . E. Ricci-Bitti, et al. Universals and\\ncultural differences in the judgments of facial expressions\\nof emotion. Journal of personality and social psychology ,\\n53(4):712, 1987.\\n[14] B. Fasel and J. Luettin. Automatic facial expression analysis:\\na survey. Pattern recognition , 36(1):259–275, 2003.\\n[15] C. Ferri, J. Hern ´andez-Orallo, and R. Modroiu. An experi-\\nmental comparison of performance measures for classiﬁca-\\ntion. Pattern Recognition Letters , 30(1):27–38, 2009.[16] I. J. Goodfellow, D. Erhan, P . L. Carrier, A. Courville,\\nM. Mirza, B. Hamner, W. Cukierski, Y . Tang, D. Thaler, D.-\\nH. Lee, et al. Challenges in representation learning: A report\\non three machine learning contests. In Neural information\\nprocessing , pages 117–124. Springer, 2013.\\n[17] X. He and P . Niyogi. Locality preserving projections. In\\nNIPS , volume 16, 2003.\\n[18] M. Inc. Face++ research toolkit. www.faceplusplus.com,\\nDec. 2013.\\n[19] S. E. Kahou, C. Pal, X. Bouthillier, P . Froumenty,\\nC¸.G ¨ulc ¸ehre, R. Memisevic, P . Vincent, A. Courville, Y . Ben-\\ngio, R. C. Ferrari, et al. Combining modality speciﬁc deep\\nneural networks for emotion recognition in video. In Pro-\\nceedings of the 15th ACM on International conference on\\nmultimodal interaction , pages 543–550. ACM, 2013.\\n[20] B.-K. Kim, J. Roh, S.-Y . Dong, and S.-Y . Lee. Hierarchical\\ncommittee of deep convolutional neural networks for robust\\nfacial expression recognition. Journal on Multimodal User\\nInterfaces , pages 1–17, 2016.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\\nclassiﬁcation with deep convolutional neural networks. In\\nAdvances in neural information processing systems , pages\\n1097–1105, 2012.\\n[22] G. Levi and T. Hassner. Emotion recognition in the wild via\\nconvolutional neural networks and mapped binary pattern-\\ns. In Proceedings of the 2015 ACM on International Con-\\nference on Multimodal Interaction , pages 503–510. ACM,\\n2015.\\n[23] C. Liu and H. Wechsler. Gabor feature based classiﬁca-\\ntion using the enhanced ﬁsher linear discriminant model for\\nface recognition. Image processing, IEEE Transactions on ,\\n11(4):467–476, 2002.\\n[24] M. Liu, S. Li, S. Shan, and X. Chen. Au-aware deep net-\\nworks for facial expression recognition. In Automatic Face\\nand Gesture Recognition (FG), 2013 10th IEEE Internation-\\nal Conference and Workshops on , pages 1–6. IEEE, 2013.\\n[25] M. Liu, S. Li, S. Shan, and X. Chen. Au-inspired deep net-\\nworks for facial expression feature learning. Neurocomput-\\ning, 159:126–136, 2015.\\n[26] P . Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and\\nI. Matthews. The extended cohn-kanade dataset (ck+): A\\ncomplete dataset for action unit and emotion-speciﬁed ex-\\npression. In Computer Vision and Pattern Recognition Work-\\nshops (CVPRW), 2010 IEEE Computer Society Conference\\non, pages 94–101. IEEE, 2010.\\n[27] Y . Lv, Z. Feng, and C. Xu. Facial expression recognition via\\ndeep learning. In Smart Computing (SMARTCOMP), 2014\\nInternational Conference on , pages 303–308. IEEE, 2014.\\n[28] M. J. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, and\\nJ. Budynek. The japanese female facial expression (jaffe)\\ndatabase. 1998.\\n[29] M. J. Lyons, J. Budynek, and S. Akamatsu. Automatic clas-\\nsiﬁcation of single facial images. IEEE Transactions on\\nPattern Analysis & Machine Intelligence , (12):1357–1362,\\n1999.\\n[30] D. McDuff, R. El Kaliouby, T. Senechal, M. Amr, J. F. Cohn,\\nand R. Picard. Affectiva-mit facial expression dataset (am-\\n2592\\nfed): Naturalistic and spontaneous facial expressions collect-\\ned in-the-wild. In Computer Vision and Pattern Recogni-\\ntion Workshops (CVPRW), 2013 IEEE Conference on , pages\\n881–888. IEEE, 2013.\\n[31] A. Mollahosseini, D. Chan, and M. H. Mahoor. Going deeper\\nin facial expression recognition using deep neural networks.\\nIn2016 IEEE Winter Conference on Applications of Com-\\nputer Vision (WACV) , pages 1–10. IEEE, 2016.\\n[32] H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler.\\nDeep learning for emotion recognition on small datasets us-\\ning transfer learning. In Proceedings of the 2015 ACM on\\nInternational Conference on Multimodal Interaction , pages\\n443–449. ACM, 2015.\\n[33] T. Ojala, M. Pietik ¨ainen, and T. M ¨aenp ¨a¨a. Multiresolution\\ngray-scale and rotation invariant texture classiﬁcation with\\nlocal binary patterns. Pattern Analysis and Machine Intelli-\\ngence, IEEE Transactions on , 24(7):971–987, 2002.\\n[34] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of\\nfacial expressions: the state of the art. IEEE Transactions\\non Pattern Analysis and Machine Intelligence , 22(12):1424–\\n1445, Dec 2000.\\n[35] M. Pard `as and A. Bonafonte. Facial animation param-\\neters extraction and expression recognition using hidden\\nmarkov models. Signal Processing: Image Communication ,\\n17(9):675–688, 2002.\\n[36] X. Peng, Z. Xia, L. Li, and X. Feng. Towards facial expres-\\nsion recognition in the wild: A new database and deep recog-\\nnition system. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition Workshops , pages\\n93–99, 2016.\\n[37] J. A. Russell. Is there universal recognition of emotion from\\nfacial expressions? a review of the cross-cultural studies.\\nPsychological bulletin , 115(1):102, 1994.\\n[38] C. Shan, S. Gong, and P . W. McOwan. Facial expression\\nrecognition based on local binary patterns: A comprehensive\\nstudy. Image and Vision Computing , 27(6):803–816, 2009.\\n[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-\\nson. Cnn features off-the-shelf: an astounding baseline for\\nrecognition. In Proceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition Workshops , pages 806–\\n813, 2014.\\n[40] K. Simonyan and A. Zisserman. V ery deep convolutional\\nnetworks for large-scale image recognition. arXiv preprint\\narXiv:1409.1556 , 2014.\\n[41] Y . Tang. Deep learning using linear support vector machines.\\narXiv preprint arXiv:1306.0239 , 2013.\\n[42] Y . I. Tian, T. Kanade, and J. F. Cohn. Recognizing action u-\\nnits for facial expression analysis. IEEE Transactions on Pat-\\ntern Analysis and Machine Intelligence , 23(2):97–115, Feb\\n2001.\\n[43] Y . Wen, K. Zhang, Z. Li, and Y . Qiao. A discrimina-\\ntive feature learning approach for deep face recognition. In\\nEuropean Conference on Computer Vision , pages 499–515.\\nSpringer, 2016.\\n[44] J. Whitehill and C. W. Omlin. Haar features for facs au\\nrecognition. In Automatic Face and Gesture Recognition,\\n2006. FGR 2006. 7th International Conference on , page 5–p-\\np. IEEE, IEEE, 2006.[45] J. Whitehill, T.-f. Wu, J. Bergsma, J. R. Movellan, and P . L.\\nRuvolo. Whose vote should count more: Optimal integration\\nof labels from labelers of unknown expertise. In Advances\\nin neural information processing systems , pages 2035–2043,\\n2009.\\n[46] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A sur-\\nvey of affect recognition methods: Audio, visual, and spon-\\ntaneous expressions. Pattern Analysis and Machine Intelli-\\ngence, IEEE Transactions on , 31(1):39–58, 2009.\\n[47] X. Zhang, L. Yin, J. F. Cohn, S. Canavan, M. Reale,\\nA. Horowitz, P . Liu, and J. M. Girard. Bp4d-spontaneous:\\nahigh-resolution spontaneous 3d dynamic facial expression\\ndatabase. Image and Vision Computing , 32(10):692–706,\\n2014.\\n2593\\n',\n",
       " '2108 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\nSingle Sample Face Recognition via Learning Deep\\nSupervised Autoencoders\\nShenghua Gao, Yuting Zhang, Kui Jia, Jiwen Lu, Member, IEEE , and Yingying Zhang\\nAbstract — This paper targets learning robust image\\nrepresentation for single training sample per person facerecognition. Motivated by the success of deep learning in imagerepresentation, we propose a supervised autoencoder, which isa new type of building block for deep architectures. There aretwo features distinct our supervised autoencoder from standardautoencoder. First, we enforce the faces with variants to bemapped with the canonical face of the person, for example,frontal face with neutral expression and normal illumination;Second, we enforce features corresponding to the same person tobe similar. As a result, our supervised autoencoder extracts thefeatures which are robust to variances in illumination, expression,occlusion, and pose, and facilitates the face recognition. We stacksuch supervised autoencoders to get the deep architecture and useit for extracting features in image representation. Experimentalresults on the AR, Extended Yale B, CMU-PIE, and Multi-PIEdata sets demonstrate that by coupling with the commonly usedsparse representation-based classiﬁcation, our stacked supervisedautoencoders-based face representation signiﬁcantly outperformsthe commonly used image representations in single sampleper person face recognition, and it achieves higher recognitionaccuracy compared with other deep learning models, includingthe deep Lambertian network, in spite of much less trainingdata and without any domain information. Moreover, supervisedautoencoder can also be used for face veriﬁcation, which furtherdemonstrates its effectiveness for face representation.\\nIndex Terms — Single training sample per person, face\\nrecognition, supervised auto-encoder, deep architecture.\\nI. I NTRODUCTION\\nSINGLE sample per pers on (SSPP) face recogni-\\ntion [1], [2]1is a very important research topic in\\ncomputer vision because of its potential applications in\\nmany realistic scenarios like passport identiﬁcation, gate\\nID identiﬁcation, video surveillance, etc.H o w e v e r ,a ss h o w n\\nin Fig. 1, there is only one training image (gallery image)\\nin SSPP, and the faces to be recognized may contain lots of\\nManuscript received December 9, 2014; revised March 16, 2015; accepted\\nMay 30, 2015. Date of publication June 16, 2015; date of current version\\nAugust 6, 2015. This work was supported by the Shanghai Pujiang Program\\nunder Grant 15PJ1405700. The associate e ditor coordinating the review of\\nthis manuscript and approving it for pub lication was Prof. Patrizio Campisi.\\nS. Gao and Y . Zhang are with ShanghaiTech University, Shanghai 200444,\\nChina (e-mail: gaoshh@shanghaitech.edu.cn; zyt@zju.edu.cn).\\nK. Jia is with the University of Macau, Macau 999078, China (e-mail:\\nkuijia@gmail.com).\\nJ. Lu is with Advanced Digital Sciences Center, Singapore 138632 (e-mail:\\njiwen.lu@adsc.com.sg).\\nY . Zhang is with Zhejiang University, Hangzhou 310027, China (e-mail:\\nzhangyy2@shanghaitech.edu.cn).\\nColor versions of one or more of the ﬁgures in this paper are available\\nonline at http://ieeexplore.ieee.org.\\nDigital Object Identiﬁer 10.1109/TIFS.2015.2446438\\n1SSPP is one speciﬁc task of one shot learning [3]\\nFig. 1. Samples of gallery images (the ﬁrst column) and probe images (the\\nrest faces) on the AR, Extended Yale B, CMU-PIE, and Multi-PIE datasets.\\n(a) The AR dataset. (b) The Extende d Yale B dataset. (c) The CMU-PIE\\ndataset. (d) The Multi-PIE dataset.\\nvariances in, for example, illumination, expression, occlusion,\\npose, etc. Therefore, SSPP face recognition is an extremely\\nchallenging task.\\nBecause of the presence of such challenging intra-class\\nvariances, a robust face representation which can overcome\\nthe effect of these variances is extremely desirable, and will\\ngreatly facilitate SSPP face recognition. However, restricted\\nby having only one tr aining sample for each person, the\\ncommonly used subspace analysis based face representationmethods, like Eigenfaces [4] and Fisherfaces [5], are no longer\\nsuitable nor applicable to SSPP. To seek a good SSPP image\\nrepresentation, lots of endeavors have been made whichachieve some good performance under certain settings. For\\nexample, with the help of manually generated virtual faces [6]\\nor an external dataset [1], traditional face representation\\nmethods can be extended to the SSPP scenario. However,\\nthe performance of these methods is still not unsatisfactoryfor challenging real data. After representing each face\\nwith a feature vector, a classiﬁcation technique, like Nearest\\nNeighbor, sparse representation based classiﬁcation (SRC) [7],can be used to predict the labels of the probe images.\\nDeep neural networks have demonstrated their great\\nsuccesses in image representation [8], [9], and their\\nfundamental ingredient is the training of a nonlinear feature\\nextractor at each layer [10]–[12]. After the layer-wise trainingof each building block and the bu ilding of a deep architecture,\\nthe output of the network is used for the image representation\\nin the subsequent task. As a typical building block in deepneural networks, Denoising Auto-Encoder [11] extracts the\\n1556-6013 © 2015 I EEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\\nGAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2109\\nfeatures through a deterministic nonlinear mapping, and it\\nis robust to the noises of the data. Image representations\\nbased on the denoising auto-encoder have shown good\\nperformance in many tasks, like object recognition, digitrecognition, etc. Motivated by the success of denoising\\nauto-encoder based deep neural networks, and driven by the\\nSSPP face recognition, we propose a supervised auto-encoderto build the deep neural network. We ﬁrst treat the faces with\\nall type of variants (for example, illumination, expression,\\nocclusion, or poses) as images contaminated by noises. With\\na supervised auto-encoder, we can recover the face without\\nthe variant, meanwhile, such a supervised auto-encoderalso extracts robust features for image representation in\\nSSPP scenario, i.e., the featur es corresponding to the same\\nperson should have the same (ideally) or similar features,and such features should be stable to the intra-class variances\\nwhich commonly exist in the SSPP scenario.\\nThe contributions of this paper are two-fold. Firstly,\\nwe propose a new type of building block – supervised\\nauto-encoder, for building the deep neural network. Differentfrom standard Auto-Encoder, on the one hand, we enforce all\\nthe faces with variances to be mapped with the canonical face\\nof the person, for example, frontal face with neutral expressionand normal illumination. Such strategy helps remove the\\nvariances in face recognition. On the other hand, by imposing\\nthe similarity preservation constraints on the extracted\\nfeatures, the supervised auto-encoder makes the features\\ncorresponding to the same person similar, therefore it extractsmore robust features for face representation. Secondly, by\\nleveraging the supervised auto-encoder, robust features can be\\nextracted for image representation in SSPP face recognition,therefore improves the recognition accuracy of SSPP.\\nThe rest of the paper is organized as follows: we will\\nreview the related work, including the commonly used image\\nrepresentation methods for SSPP scenario as well as the\\nauto-encoder and its variants in Section II. In Section III,we will introduce our supervised auto-encoder, and discuss\\nits application in SSPP. We will experimentally evaluate the\\nproposed technique and its parameters in Section IV, andconclude our work in Section V.\\nII. R\\nELATED WORK\\nA. Work Related to SSPP Face Representation\\nThough subspace analysis based methods are usually\\nadopted for effective and efﬁcient face representation in\\ngeneral face recognition, they are no longer suitable norapplicable to the SSPP scenario. On the one hand, because\\nof the limited number of gallery images and uncertainty\\nof the variances between probe images and gallery image,\\nit is not easy to estimate the data distribution and get the\\nproper projection matrix for unsupervised methods, like\\nEigenfaces [4], 2DPCA [13], etc. To make these unsupervised\\nmethods more suitable for SSPP , by taking advantage of\\nsome virtual faces generated by d ifferent methods, Projection-\\nCombined Principal Component Analysis ((PC)\\n2A) [14],\\nEnhanced (PC)2A( E ( P C )2A) [15], etc., have been proposed.\\nOn the other hand, to make FLDA [5] based imagerepresentation able to be used in the SSPP case where the\\nintra-class variance is impo ssible to be directly estimated\\nbecause only one training sample is provided for each person,\\nvirtual samples are usually generated by using small per-turbation [16], transformation [17], SVD decomposition [6],\\nor subimages generated by di viding each image into small\\npatches [18]. Moreover, the intra-class variances can alsobe estimated from a generic dataset [1], [19], where each\\nsubject contains images with variances in pose, expression,\\nillumination, occlusion, etc. Recently, with the emergence\\nof deep learning technique, Tang et al. propose a Deep\\nLambertian Network (DLN) [20] which combines DeepBelief Nets [10] with Lambertian reﬂection assumption.\\nTherefore DLN extracts illumination invariant features for\\nface representation and it also shows good performance underSSPP setting, but it is not able to handle other variances,\\nlike expressions, poses, occlusions, etc., which restricts its\\napplication in real world problems.\\nB. Work Related to Auto-Encoder\\nAuto-encoder which is also termed as autoassociator or\\nDiabolo network, is one of the commonly used building\\nblocks in deep neural networks. It contains two modules.\\n(i) A encoder maps the input xto the hidden nodes\\nthrough some deterministic mapping function f:h=f(x).\\n(ii) A decoder maps the hidden nodes back to the original\\ninput space through another deterministic mapping function\\ng:x\\n/prime=g(h). For real-valued input, by minimizing the recon-\\nstruction error /bardblx−g(f(x))/bardbl2\\n2, the parameters of encoder and\\ndecoder can be learnt. Then the output of the hidden layer is\\nused as the feature for image representation. It has been shown\\nthat such a nonlinear auto-encoder is different from PCA [21],\\nand it has been proven that “training an auto-encoder tominimize reconstruction error amounts to maximizing a lower\\nbound on the mutual information between input and the\\nlearnt representation” [11] . To further boos t the ability of\\nauto-encoder for image representation in building deep\\nnetworks, Vincent et al. [11] propose a denoising auto-encoder\\nwhich enhances its generalization by training with locally\\ncorrupted inputs. Rafai et al. enhance the robustness of\\nauto-encoder to noises by adding the Jacobian [22],or Jacobian and Hessian at the same time [23], into the\\nobjective of basic auto-encoder. Zou et al. [24] use the pooling\\noperation after the Reconstr uction Independent Component\\nAnalysis [25] encoding process, and enforce the pooled\\nfeatures to be similar for instances with the same class label.\\nThese extensions improve the performance of auto-encoder\\nbased neural networks for image representation in object and\\ndigit recognition.\\nC. Deep Learning Based Face Veriﬁcation Systems\\nIn [26], a Siamese Networks is proposed for face veriﬁcation\\nand such SN is based on Convolutional Neural Network\\nin [26]. In the experiment, we have tried different settingsand use the one with the bes t performance. As the SN is\\nproposed for face veriﬁcation, to do the face recognition,\\nwe run face veriﬁcation over all pairs of faces in the\\n2110 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\ntest gallery set and choose the pair that’s most similar.\\nIn this way, we can predict the label of the probe. Recently,\\nTaigman et al. also propose a Convolutional Neural Networks\\nbased face veriﬁcation system [27], which signiﬁcantlyoutperforms the existing hand-crafted features based systems\\non the Labeled Face in the Wild (LFW) database. Similarly,\\nFan et al. also use another variant of convolutional neural\\nnetwork which is termed as Pyramid CNN, for face\\nveriﬁcation, and also achieve similar performance on the\\nLFW database. But these works are proposed for face\\nveriﬁcation, where a pair of f aces are given and the algorithm\\nidentiﬁes whether the two faces belonging to the same personor not (a random guess is 50%). Our task solves face recogni-\\ntion in which, given a test image , it tries to identify precisely\\nthe correct person among many. Random guess gives oneover the number of subjects. Besides the tasks to be solved\\nare different, the network architecture of our paper is also\\ndifferent with these all existing works. Moreover, our work\\nis also closely related to the work of Zhu et al. [28], [29].\\nIn [28], a supervised training is deployed to train a networkconsisting of encoding layer and pooling layer. In [29],\\nmultiple CNN networks trained on different regions of face\\nand a regression layer are trained to solve the face veriﬁcationtask. Similar to these work, our work also makes faces of\\nthe same person be represented similarly, but we are based\\non different architectures. Moreover, different from [28], [29],\\nfeatures learnt by our method is not designed for some speciﬁc\\ntask. As shown later, our model can also be used for facerecognition or face veriﬁcation.\\nIII. S\\nUPERVISED AUTO-ENCODER FOR\\nSSPP F ACE REPRESENTATION\\nThe motivation of our supervised auto-encoder for\\nSSPP face representation comes from the denoising auto-\\nencoder [11]. In this section, we will ﬁrst revisit the denoising\\nauto-encoder. Then we will propose our supervised\\nauto-encoder, its formulation, its optimization, its differences\\nwith denoising auto-encoder, and its application in SSPP facerepresentation.\\nA. A Revisit of Denoising Auto-Encoder\\nA denoising auto-encoder tries to reconstruct the clean\\ninput data by using the manually corrupted version of it.Mathematically, denote the input as x. In denoising\\nauto-encoder, input xis ﬁrst corrupted by some pre-\\ndeﬁned noise, for example, Additive Gaussian noise(˜x|x∼N(x,σ\\n2I)), masking noise (a fraction of xis forced\\nto 0), or salt-and-pepper noise (a fraction of xis forced to\\nbe 0 or 1). Such a corrupted ˜xis used as the input of the\\nencoder h=f(˜x)=sf(W˜x+bf). Then the output of\\nencoder his input into the decoder ˆx=g(h)=sg(W/primeh+bg).\\nHere sfandsgare predeﬁned activation functions of encoder\\nand decoder respectively, which can be sigmoid functions,\\nhyperbolic tangent functions, or rectiﬁer functions [30], etc.\\nW∈Rdh×dxandbf∈Rdhare the parameters of encoder, and\\nW/prime∈Rdx×dhand bg∈Rdxare the parameters of decoder.\\ndxanddhare the dimensionality of input data and the numberof hidden nodes respectively. Based on the above deﬁnitions,\\nthe objective of denoising auto-encoder is given as follows:\\nmin\\nW,W/prime,bf,bg/summationdisplay\\nx∈XL(x,ˆx) (1)\\nHere Lis the reconstruction error, typically squared error\\nL(x,ˆx)=/bardblx−ˆx/bardbl2for real-valued inputs. After learning\\nfand g, the output of clean input ( f(x))i su s e da st h e\\ninput of the next layer. By training such denoising auto-\\nencoder layer by layer, stacked denoising auto-encoders are\\nbuilt. Experimental results show that stacked denoising auto-\\nencoders greatly improve the gen eralization performance of\\nthe neural network. Even when the fraction of corrupted pixels(corrupted by zero masking noises) reaches up to 55%, the\\nrecognition accuracy is still better or comparable with that of\\na network trained without corruptions.\\nB. Supervised Auto-Encoder\\nIn real applications, like passport or Gate ID identiﬁcation,\\nthe only training sample (gallery image) for each person is\\nusually a frontal face with frontal/uniform lighting, neutral\\nexpression, and no occlusion. However, the test faces\\n(probe images) are usually accompanied by variances in\\nillumination, occlusion, expression, pose, etc.Compared to the\\ndenoising auto-encoder, these gallery images can be seen as\\nclean data and these probe images can be seen as corrupteddata. For robust face recognition, we desire to learn the\\nfeatures which are robust to these variances. The success\\nof denoising auto-encoder convinces us of the possibility to\\nlearn such features. Then the problem becomes: How do we\\nlearn a mapping function which captures the discriminativestructures of the faces of different persons , while staying\\nrobust to the possible variances of these faces? Once such\\na function is learnt, robust features can be extracted forimage presentation, with an expected improvement to the\\nperformance of SSPP face recognition.\\nGiven a set of data which contain the gallery images (clean\\ndata), probe images (corrupted data) as well as their labels,\\nwe use them to train a deep neural network for featureextraction. We denote each probe image in this dataset as ˜x\\ni,\\nand its corresponding gallery image as xi(i=1,..., N).\\nIt is desirable that xiand˜xishould be represented similarly.\\nTherefore the following formulation is proposed (following the\\nwork [11], [22], only the tied weights case is explored in this\\npaper, i.e., W/prime=WT):\\nmin\\nW,bf,bg1\\nN/summationdisplay\\ni/parenleftbig\\n/bardblxi−g(f(˜xi))/bardbl2\\n2+λ/bardblf(xi)−f(˜xi)/bardbl2\\n2/parenrightbig\\n+α/parenleftbig\\nKL(ρx||ρ0)+KL(ρ˜x||ρ0)/parenrightbig\\n(2)\\nwhere\\nρx=1\\nN/summationdisplay\\ni1\\n2(f(xi)+1),\\nρ˜x=1\\nN/summationdisplay\\ni1\\n2(f(˜xi)+1),\\nKL(ρ||ρ0)=/summationdisplay\\nj/parenleftbig\\nρ0log(ρ0\\nρj)+(1−ρ0)log(1−ρ0\\n1−ρj)/parenrightbig\\n.(3)\\nGAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2111\\nFig. 2. Architecture of Staked Supervised Auto-Enc oders. The left ﬁgure: The basic supervised auto -encoder, which is comprised of the clean/“corru pted”\\nfaces, there features (hidden layer), as well as the reconstructed clean face by using the “corrupted face”. The middle ﬁgure: The output of previous h idden\\nlayer is used as the input to train the next supervis ed auto-encoder. We repeat such training several times until the desired number of hidden layers is reached.\\nIn this paper, only two hidden layers are used. The right ﬁgure: Once the netwo rk is trained, given any input face, the output of the last hidden layer is u sed\\nas the feature for image representation.\\nIn this paper, the activation functions used are the hyperbolic\\ntangent, i.e., h= f(x)= tanh(Wx+bf),a n d\\ng(h)=tanh(WTh+bg).\\nWe list the properties of the supervised auto-encoder as\\nfollows:\\n1) The ﬁrst term in equation (2) is the reconstruction\\nerror. It means that though gallery images contain somevariances, after passing through the encoder and the\\ndecoder, they will be repaired. In this way, our learnt\\nmodel is robust to the variances of expression, occlusion,\\npose, etc., which are quite different to the noises in the\\nDenoising Auto-encoder.\\n2) The second term in equation (2) is the similarity\\npreservation term. Since the output of the hidden layer\\nis used as the feature, f(x\\ni)and f(˜xi)correspond to\\nthe features of the same person. It is desirable that\\nthey should be the same (ideally) or similar. Such a\\nconstraint enforces the learning of a nonlinear mapping\\nrobust to the variances that commonly appear in SSPP.\\n3) The third and the fourth term, the Kullback-Leiber\\ndivergence (KL divergence) terms in equation (2)\\nintroduce sparsity in the hidden layer. Work from\\nbiological studies shows that the percentage of theactivated neurons of human brain at the same time is\\naround 1% to 4% [31]. Therefore the sparsity constraint\\non the activation of the hidden layer is commonly used\\nin the auto-encoder based neural networks and results\\nshow that sparse auto-encoder often achieves betterperformance [32] than that trained without the sparsity\\nconstraint. Since the activation function we used is the\\nhyperbolic tangent, its output is between −1 and 1,\\nw h e r et h ev a l u eo f −1 is regarded as non-activated [33].\\nTherefore we map the output of the encoder to the\\nrange (0,1) ﬁrst in equation (3). Here ρ\\nxandρ˜xare the\\nmapped average activations of the clean data and\\nthe corrupted data respectively. By choosing a small ρ0,\\nthe KL divergence regularizer enforces that only a few\\nfraction of neurons are activated [33], [34]. Following\\nthe work [33], [34], we also set ρ0to 0.05.\\n4) Weighting the ﬁrst and the second term with1\\nN\\n(Nis the total number of training samples) helps\\nbalance the contributions of the the ﬁrst two terms andthe last terms in the optimization. Otherwise we may\\nneed to tune αon different datasets because the training\\nsamples for different datasets may be different.\\n5) We aim at learning an featur e extractor to represent\\nfaces corresponding to the same person similarly,\\nbut [26] and [35] focus on learning a distance metric\\nin the last layer of their res pective DNN architectures.\\nOur work is different from [26] and [35] in terms of\\nnetwork architecture and application.\\n6) Since the labels of the faces are used while training this\\nbuilding block, we term our proposed formulation as the\\nSupervised Auto-Encoder (SAE) . We illustrate the idea\\nof such supervised auto-encoder in Fig 2 (the left ﬁgure).\\nC. Optimization of Supervised Auto-Encoder\\nThe optimization method is very important for good\\nperformance of deep neural networks. Following the\\nwork [36], [37], the entries of Ware randomly sampled\\nfrom the uniform distribution between [−/radicalBig\\n6\\ndh+dx,/radicalBig\\n6\\ndh+dx],\\nand bfand bgare initialized with zero vectors. It is worth\\nnoting that such normalized initialization is very important\\nfor the good performance of Auto-Encoder based method“presumably because the layer -to-layer transformations\\nmaintain magnitudes of activations (ﬂowing upward) and\\ngradients (ﬂowing backward) [36]”. Then the Limited memoryBroydenCFletcherCGoldfarbCShanno (L-BFGS) algorithm\\nis used for learning the parameters because of its faster\\nconvergence rate and better performance compared to\\nstochastic gradient descent methods and conjugate\\ngradient [38] methods.\\nAs the computational cost of the similarity preservation\\nterm (the 2\\nndterm in equation (2)) and the calculation of its\\ngradient with respect to the unknown parameters is almost thesame with that of the reconstruction error term, the overall\\ncomputational complexity is still O(d\\nx×dh)[22] in our\\nsupervised auto-encoder.\\nD. Stacking Supervised Auto-Encoders\\nto Build Deep Architecture\\nDeep neural networks demonstrate better performance for\\nimage representation than shallow neural networks [32].\\n2112 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\nTherefore we also train the supervised auto-encoder in a\\nlayer-wise manner and get the deep architecture. Here we\\nterm such an architecture as the Stacked Supervised\\nAuto-Encoders (SSAE) . After learning the activation function\\nof the encoder in previous layer, it is applied to the clean\\ndata and corrupted data respectively, and the outputs serve\\nas the clean input data and corrupted input data to train thesupervised auto-encoder in the next layer. Once the whole\\nnetwork is trained, the output of the highest layer will be\\nused as the feature for image representation. We illustrate this\\nlearning strategy in Fig. 2.\\nE. The Differences Between Stacked Supervised\\nAuto-Encoders and Stacked Denoising Auto-Encoders\\nOur stacked auto-encoders are different from the stacked\\ndenoising auto-encoders in the following three aspects:\\n1) Input: Denoising auto-encoder is an unsupervised feature\\nlearning method, and the corrupted data are generated by\\nmanually corrupting the clean data with predeﬁned noises.\\nTherefore the trained model is robust to these predeﬁnednoises. However, in our supervised denoising auto-encoder, the\\ncorrupted data are the photos ta ken under different settings,\\nand they are physically meaningful data. By enforcing thereconstructed probe image to be similar to its gallery image,\\nwe can overcome the possible variances which appear\\nin SSPP face recognition, and w hich are quite different\\nfrom the noises in denoising auto-encoder. This is why our\\nsupervised auto-encoder needs the labels of data to train thenetwork.\\n2) Formulation: The similarity preservation term is used to\\nfurther emphasize the desired property of SSPP image\\nrepresentation. Though we enforce the reconstructed probe\\nimage to be similar to the gallery image, it cannot beguaranteed that the extracted features of them are similar\\nbecause the variances in pose, expression, illumination, etc.\\ncan make the probe and the gallery quite different. But inSSPP face recognition, it is desirable that the images of the\\nsame person have similar representation. To this end, the\\nsimilarity preservation term is added to the objective of SAE\\nin equation (2). Such term further makes the extracted features\\nbe robust to the variances in face recognition. As shownin Section IV-F, this term greatly improves the recognition\\naccuracy, especially for the cases with large intra-class\\nvariances.\\n3) Building a Deep Architecture: In stacked denoising\\nauto-encoders, once the activation function of the encoder inprevious layer is trained, it maps the clean data to the hidden\\nlayer, and the outputs serve as the clean input data of next\\nlayer SAE. Then the noises which are generated in the sameway with that in the previous layer are added on the clean\\ndata to serve as the corrupted data. As claimed in [34], since\\nthe features in the hidden layer are no longer in the same\\nfeature space with the input data, the meaning of applying the\\nnoises generated in the same way with that in previous layerto the output of the hidden layer is no longer clear. Similar\\nto [34], we apply the activation function of the encoder to\\nboth the clean data and the corrupted data, and their outputsserve as the clean and corrupted input data of the next layer.\\nHence this way of stacking the basic supervised auto-encoders\\nis more natural for build ing a deep architecture.\\nIV . E\\nXPERIMENTS\\nIn this section, we will experimentally evaluate the\\nstacked supervised auto-encoder for extracting the features in\\nSSPP face recognition task on Extended Yale B, CMU-PIE,\\nand AR datasets. Important parameters will also be exper-imentally evaluated. Besides face recognition, we also use\\nstacked supervised auto-encoder for face veriﬁcation on the\\nLFW dataset.\\nA. Experimental Setup\\nWe use the Extended Yale B, AR, and CMU-PIE, and\\nMulti-PIE datasets for evaluation of the effectiveness of the\\nproposed SSAE model. All the images are gray-scale images\\nand are manually aligned.\\n2The image size is 32 ×32. Thus the\\ndimensionality of the input vect or is 1024. Each input feature\\nis normalized with the /lscript2normalization. We set λ=λ0×dx\\ndh.\\nFurther, we set λ0=10 on the CMU-PIE and Multi-PIE\\ndataset, and set λ0=5 on the AR and the Extended Yale\\nB datasets. The reason for this is that the variances are more\\nsigniﬁcant on the CMU-PIE and Multi-PIE dataset than that\\nof the AR or the Extended Yale B datasets. The weightcorresponding to the KL divergency term ( α)i sﬁ x e dt o\\nbe 10\\n−4. Following the work [11], we ﬁx the number of\\nthe hidden nodes in all the hidden layers to be 2048. In our\\nexperiments, we found that two hidden layers already give\\na sufﬁciently good performance. After extracting featureswith stacked supervised auto-encoders for face representation,\\nfollowing the work [7], sparse representation based classiﬁca-\\ntion is used for face recognition. The features are normalizedto make their /lscript\\n2norms equal to 1 before sparse coding.\\n1) Baselines: We compare our Stacked Supervised\\nAuto-Encoders (SSAE) with th e following work because of\\ntheir close relationships. It is also worth noting that all the\\ncomparisons are based on the sam e training/test set, and the\\nsame generic data if they are used.\\n1) Sparse Representation for Classiﬁcation (SRC) [7] with\\nraw pixels;\\n2) LBP feature followed by SRC;\\n3) Collaborative Representation for Classiﬁcation\\n(CRC) [39].\\n4) AGL [1];\\n5) One-Shot Similarity Kernel (OSS) [40]. The generic\\ndata are used as the negative data in OSS, i.e., we use\\nthe same generic/training/testing split for both OSS and\\nour SAE.\\n2In all our experiments, faces are cropped based on manually labeled\\nlandmark points, and aligned based la ndmark points. We also tested our work\\nwith the faces cropped with the OpenCV face detector on the AR dataset,\\nbut the performance of our work on such data is less than 50% on AR.One possible reason for such poor performance is that we don’t have enough\\ndata to train a deep network to be robust to the misalignment. The combination\\nof our work with automatic face alignment method is our future work alongthis direction.\\nGAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2113\\n6) Denoising Auto-Encoder (DAE) [11] with 10% masking\\nnoises followed by SRC;\\n7) Modiﬁed Denoising Auto-Encoder (MDAE).\\nWe propose to use the reconstruction error term onlyin equation (2) ( λ=α=0), and term such baseline\\nmethod as Modiﬁed Denoising Auto-Encoder (MDAE);\\n8) Siamese network (SN) [26].\\n3\\nIn all these baseline methods, 1-3 correspond to the verypopular sparse coding related methods. 4-5 are speciallydesigned for SSPP, and 6-8 are most related deep learning\\nmethods.\\n4\\nB. Dataset Description\\nThe CMU-PIE dataset [42] contains 41,368 images\\nof 68 subjects. For each subject, the images are taken under\\n13 different poses, 4 different illumination conditions, and4 different expressions. For each subject, we use the face\\nimages taken with the frontal pose, neutral expression, and\\nnormal lighting condition as the galleries, and use the rest of\\nthe images taken with the poses C27, C29, C07, C05, C09 as\\nprobes. We use images of 20 subjects to learn the SSAE, anduse the remaining 48 subjects for evaluation.\\nThe AR dataset [43] contains over 4,000 frontal faces taken\\nfrom 126 subjects (70 men and 56 women) in two differentsessions, and the images contain variances in occlusion\\n(sunglasses or scarves), expression (neutral expression, smile,\\nangry, scream), and illumination. Some images contains both\\nocclusion and illumination variances. In our experiments,\\n20 subjects from session 1 are used as the generic set fortraining the SSAE, and another 80 subjects also from session 1\\nare used for evaluation.\\nThe Extended Yale B dataset [44] contains 38 categories.\\nFor each subject, we use the frontal faces whose light source\\ndirection with respect to the camera axis is 0 degree azimuth\\n(‘A+000’) and 0 degree elevation (‘E +00’) as gallery\\nimages, and use the rest of the images with different lighting\\nconditions as the probe images. Following the work of deepLambertian networks (DLN) [20], 28 categories are used to\\ntrain the SSAE and the remaining 10 categories which are\\nfrom the original Yale B dataset are used for evaluation.\\nThe Multi-PIE dataset [45] contain images of 337 persons\\ntaken under the four sessions over the span of 5 months.\\n3We tried several different network architectures in order to get the best\\nexperimental result for the Siamese network. Surprisingly, the one of the\\nbest performers is the two-layer netwo rk which take a fully connected layer\\n(with 500 hidden units and the tanh non- linearity) as the ﬁrst layer and the\\nSiamese as the second one. The origin al deep architecture proposed in [26]\\n(i.e. “the basic architecture is C1-S2-C3-S4-C5-F6”) doesn’t work as well as\\nthis simpler ones (With the architecture listed in [1], the accuracy of SiameseNetwork on AR is below 60%). Probabl y, the limited size of our generic\\ntraining set make it difﬁcult to train a good convolutional neural network\\nwith deep architecture. Also as the image we used are well-aligned which is\\ndifferent from [26], the convolutional l ayers in the original architecture might\\nbecome redundant.\\n4Because the codes of deepface [27] and Pyramid CNN [41] are not\\navailable, and the implementation and preprocessing involves lots of tricks,\\nsay very sophisticated alignment met hod and lots of outside data required\\nfor network training, here we don’t compare our method with these works.\\nAnother reason for not comparing with [27] and [41] is that the task solved\\nby these methods is face veriﬁcation, but our work solves the face veriﬁcationtask.TABLE I\\nP\\nERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE\\nEXTENDED YALE BAND THE AR D ATASETS (%)\\nTABLE II\\nPERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE\\nCMU-PIE D ATAS ET (%)\\nTABLE III\\nPERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS ON THE\\nMULTI -PIE D ATAS ET (%)\\nFor each persons, images are taken under 15 different view\\nangles and 19 different illuminations while displaying different\\nfacial expressions. In our experiments, only the images withthe neutral expression, near frontal pose (0°, 15°, −15°), and\\ndifferent illuminations are used. For each person, the frontal\\nface with neutral expression, frontal illumination is used asthe gallery and all the images are used as the probe. The\\n249 persons in session 1 are used as the evaluation, and the ﬁrst\\ntime appearance images corresponding to the other 88 persons\\nwhich only appear in session 2-4 are used to train the\\nnetwork.\\nC. Performance Evaluation\\nThe performance of different methods on the AR, Extended\\nYale B, CMU-PIE, and Multi-PIE datasets is listed in Table I,Table II, and Table III. We can see that our SSAE outperforms\\nall the rest of the methods including those specially designed\\nmethods for SSPP image representation, and it achieves the\\n2114 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\nbest performance, which proves the effectiveness of SSAE for\\nextracting robust features for face representation. Needless to\\nsay, SSAE based face represen tation can also be combined\\nwith ESRC if another set of labeled data are provided.\\n1) SAE vs. Hand-Crafted Features: The improvement of the\\nfeatures learnt from our method over the popular LBP features\\nis between about 10% (C27) and 31% (C29) on theCMU-PIE dataset, with larger improvements for subsets of\\nlarger poses. On the AR dataset and the Extended Yale B\\ndataset where there is little pose variation, the improvement\\nof our method over LBP is still around 5%. Our method\\nalso outperforms ESRC, which introduces a pre-trainedintra-class variance dictionary to extend the SRC method\\nto the SSPP case, and the improvement is very evident for\\ncases of large pose variance (e.g., CMU-PIE C07, C09, C29,Multi-PIE 15, −15). Compared with ESRC, another advantage\\nof our method is speed. ESRC relies on the intra-class variance\\ndictionary to achieve good performance, and the size of such\\ndictionary is usually big. Such a big dictionary will increase\\ncomputational costs, but in our method, the dictionary sizein the sparse coding is the same with the number of gallery\\nfaces. Thus our method is signiﬁcantly faster than ESRC\\nin testing. For example, ESRC costs about 4.63 seconds tosolve sparse coding for each probe face on Extended Yale B,\\nwhile our method can solve the sparse coding in about\\n1.8×10\\n−3seconds. Here the number of atoms in intra-class\\nvariance dictionary is 1746 on the Extended Yale B. Therefore\\nthe total atoms in the dictionary is 1756 for ESRC. In ourmethod, the number of atoms in the dictionary of SRC is\\nonly 10. All the methods are based on Matlab\\nimplementations, and run on a Windows Server (64bit)with a 2.13GHz CPU and 16GB RAM. It is worth noting\\nthat as the intra-class variance dictionary is very large on\\nMulti-PIE, the optimi zation is very slow, therefore we don’t\\ninclude its performance on the Multi-PIE dataset.\\n2) SAE vs. Other DNNs: Compared with DAE, the improve-\\nment of our method is over 30% on all the datasets. The reason\\nfor the poor performance of DAE f or SSPP face representation\\nis that the training of DAE is unsupervised, and zero maskingnoise is used to train the DAE. Therefore it is natural\\nthat the trained DAE cannot handle the variances in poses,\\nexpressions, etc. Different from DAE, MDAE enforces the\\nreconstructed faces of the probe images, which contain\\nthe variances in expression, pose, etc, to be well aligned\\nto their gallery images (frontal faces), therefore its perfor-\\nmance is better than DAE, but it is still inferior to that\\nof stacked SAE, especially for the cases with variance inpose and expression on AR and CMU-PIE. In contrast,\\nbesides using more appropriate reconstruction error term,\\nour SAE also enforces extracted features with the sameperson to be similar. Therefore, SAE can overcome these\\nvariances in SSPP and is more suitable for extracting features\\nfor face recognition. Moreover, on the Extended Yale B\\ndataset, the performance of our method (83.97% when\\nthe number of hidden nodes is 1024, and 82.22% whenthe number of hidden nodes is 2048) is better than the\\n81% obtained by Deep Lambertian Network (DLN) [20],\\nwhich is specially designed for removing the illuminationeffect in SSPP face recogn ition [20]. In addition to the\\n28 categories from the Extended Yale B, DLN also uses the\\nToronto Face Database, which is a very large dataset to train\\nthe network. Although we use much less data to train the SAEand it does not use any domain information about illumination\\nmodel as in DLN, performance of SAE is still better.\\n5\\nIn addition, our SAE also outperforms SN on all the datasets.\\nThese experiments clearly demonstrate the superiority of SAE\\nfor recognition tasks over other DNN building blocks such as\\nDAE and DLN.\\n3) More Observations: We notice that as the poses of\\nprobe images change, the performance drops notably onthe CMU-PIE. For example, the recognition accuracy drops\\nat least 10.83% (C09) compared with that of the frontal\\nface (C27) pose. Interestingly, we also notice that the perfor-\\nmance of C07 (look up) and C09 (look down) is slightly better\\nthan C05 (look right) and C29 (look left). A possible reasonis that the missing parts of face are larger for C05 and C29\\nthan that in C07 and C09 (please refer to Fig. 1). Similar\\nobservations can also be found on the Multi-PIE datasetin Table III where the recognition accuracy also drops by\\naround 30% if the poses are non-frontal.\\nMoreover, some probe images and their reconstructed\\nimages on the AR dataset are also shown in Fig. 3. We can\\nsee our method can remove the illumination, and recoverthe neutral face from the faces with different expressions.\\nFor the faces with occlusion, our method can also simulate\\nthe faces without occlusion, but compared with illuminationand expression, it is more difﬁcult to recover the face from\\nthe occluded face because too much information is lost.\\nSuch results are natural because human can infer the faces\\nwith normal illumination and neutral expression from the\\nexperience (For the deep neural networks, the experience islearnt from the generic set). But it is also almost impossible\\nfor our human to infer the occluded face parts because too\\nmuch information is missing.\\nMoreover, some probe images and their reconstructed\\nimages on the AR dataset are also shown in Fig. 3. We can\\nsee our method can remove the illumination, and recover\\nthe neutral face from the faces with different expressions.\\nFor the faces with occlusion, our method can also simulatethe faces without occlusion, but compared with illumination\\nand expression, it is more difﬁcult to recover the face from\\nthe occluded face because too much information is lost.Such results are natural because human can infer the faces\\nwith normal illumination and neutral expression from the\\nexperience (For the deep neural networks, the experience islearnt from the generic set). But it is also almost impossible\\nfor our human to infer the occluded face parts because too\\nmuch information is missing.\\n5Because the results on the AR and CMU-PIE datasets and the codes for\\nDLN are not available for comparison, we don’t report its performance on\\nthe AR and CMU-PIE datasets. But it is unlikely that DLN works well on\\nthe AR and CMU-PIE as it is not designe d to handle other variations in the\\ndata such as poses, occlusions, expressions, etc. Moreover, the state-of-the-art\\nperformance on Extended Yale B has reached 93.6% in terms of accuracy\\nunder the SSPP setting in [46]. But the e xperimental setup in [46] is a little\\ndifferent from that in our paper.\\nGAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2115\\nFig. 3. A comparison between the original images and the reconstructed ones on the AR dataset. (a): Corrupted faces used for training the network.\\n(b): Corrupted faces used for evaluation. (c): Reconstructed faces of (a). (d): Reconstructed faces of (b).\\nFig. 4. The Effect of the Similarity Preservation Term on the CMU-PIE\\nDataset (%).\\nD. Evaluation of Similarity Preservation Term\\nThe similarity preservation term, the second term in\\nequation (2), is very important in the formulation of SAE. Here\\nwe list the performance of differ ent networks trained with the\\nformulation with and without this term in Fig. 4. It can be\\nseen that this term improves the recognition by 3% for thebenign frontal face case (C27), about 10% for C09 and C07,\\nand about 29% for C05 and C29. We can see that improvement\\nin performance increases with the increase of the poses (themisalignment of faces is larger for C05 and C29 than that\\nin C07 and C09). This validates that the similarity preservation\\nterm indeed enhances the robustness of face representation,\\nespecially for pose variations.\\nE. The Effect of Deep Architecture\\nAs an example, we show the performance of the SSAE\\nwith different layers on the Multi-PIE dataset in Fig. 5. We cansee that 2-layer SSAE network outperforms the single layer\\nnetwork, which demonstrates the effectiveness of the deep\\narchitecture.Fig. 5. The effect of SSAE with different depth on the Multi-PIE dataset.\\nF . Parameter Evaluation\\n1) Number of Hidden Nodes in Hidden Layers: We change\\nthe number of hidden nodes from 512 to 4096 on the\\nExtended Yale B, and show their performancein Fig. 6 (top left). Results show that the recognition\\naccuracy is higher when the number of hidden nodes\\nis 1024 or 2048, which is the same or larger than the\\ndimensionality of the data. The reason for this is that more\\nhidden nodes usually increase the expressiveness of theauto-encoder [32]. We also not e that too many hidden nodes\\nactually decreases the accuracy. A possible reason is that\\nthe data used for learning the network in our setting isnot enough (only images of 28 subjects are used to learn\\nthe parameters), which may limit stability of the learnt\\nnetwork.\\n2) Weight of KL Terms ( α):We plot the performance with\\ndifferent αin Fig. 6 (top right). The poor performance when\\nαis too small (10\\n−6)proves the importance of the sparsity\\nterm. But if we impose too large weight on α(10−2), more\\nhidden nodes will hibernate for a given input, which affects\\n2116 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\nFig. 6. The effect of different parameters in supervised auto-encoder on the\\nExtended Yale B dataset.\\nthe sensitiveness of the model to the input, and may make\\nfaces of different persons be represented similarly. This may\\nbe the reason that for a value of 102, our model has the lowest\\nperformance. So properly setting αis a requirement for the\\ngood performance of our model. Similarly phenomenon also\\nhappens in sparse representation based face recognition [47],\\ntoo small or too large sparsity both will reduce the recognition\\naccuracy.\\n3) Weight of Similarity Preservation Term ( λ):We plot the\\nperformance of SSAE with different λin Fig. 6 (bottom). The\\npoor performance with smaller λ0(0.1 or 1) also demonstrates\\nthe importance of the similarity preservation term. But if λis\\ntoo big, the learnt network might become less discriminative\\nfor different subjects as it enforces too strongly the similarityof the learnt features for diverse samples. That leads to a drop\\nin performance for very large λ. Moreover, from the plot,\\nwe see that good performance can be obtained for a fairlylarge range of λ.\\nG. The Comparison of Different Activation Functions\\nBesides hyperbolic tangent, we also evaluate the perfor-\\nmance of SSAE with other activation functions, including\\nsigmoid and Rectiﬁed Linear Units (ReLU) [48]. To achievethe sparsity in the hidden layer, different strategies are used\\nfor different activation functions. Speciﬁcally, if the activation\\nfunction is sigmoid, the objective function is rewritten\\nas follows:\\nmin\\nW,bf,bg1\\nN/summationdisplay\\ni/parenleftbig\\n/bardblxi−g(f(˜xi))/bardbl2\\n2+λ/bardblf(xi)−f(˜xi)/bardbl2\\n2/parenrightbig\\n+α/parenleftbig\\nKL(ρx||ρ0)+KL(ρ˜x||ρ0)/parenrightbig\\n(4)\\nwhere\\nρx=1\\nN/summationdisplay\\nif(xi),TABLE IV\\nPERFORMANCE COMPARISON WITHDIFFERENT\\nACTIV A TION FUNCTIONS (%)\\nTABLE V\\nPERFORMANCE COMPARISON BETWEEN DIFFERENT METHODS\\nON THE LFW D ATASETS .( % )\\nρ˜x=1\\nN/summationdisplay\\nif(˜xi),\\nKL(ρ||ρ0)=/summationdisplay\\nj/parenleftbig\\nρ0log(ρ0\\nρj)+(1−ρ0)log(1−ρ0\\n1−ρj)/parenrightbig\\n.(5)\\nIf the activation function is ReLU, the objective function is\\nrewritten as follows:\\nmin\\nW,bf,bg1\\nN/summationdisplay\\ni/parenleftbig\\n/bardblxi−g(f(˜xi))/bardbl2\\n2+λ/bardblf(xi)−f(˜xi)/bardbl2\\n2/parenrightbig\\n+α/parenleftbig\\n/bardblf(xi)/bardbl1+/bardblf(˜xi)/bardbl1/parenrightbig\\n(6)\\nWe list the performance of our SAE based on different\\nactivations on the AR dataset in Table IV. We can see that\\nhyperbolic tangent usually achieves the best performance.It is worth noting that ReLU is usually used for Convolu-\\ntional Neural Networks (CNN) and demonstrate good perfor-\\nmance for image classiﬁcation. But many tricks, includingthe momentum, weight decay, early stopping, are used to\\noptimize the objective function in CNN. In our objective\\nfunction optimization, we simply use the L-BFGS to opti-\\nmize the objective function. Many existing works [38], [49]\\nhave shown that different optimization methods will greatlyaffect the performance of deep neural networks. Maybe more\\nadvanced optimization method and more tricks help improve\\nthe performance of ReLU.\\nH. Extension: SSAE for Face Veriﬁcation\\nBesides face recognition, our model can also be used\\nfor face veriﬁcation. Specially, we tested our work on the\\nLFW dataset under the constrained without outside dataprotocol. The performance of different methods under such\\nprotocol is listed in Table V . Interestingly, though our\\nwork is designed for learning features to make faces of the\\nsame person represented similarly, the performance of our\\nmodel for face veriﬁcation is not bad. By learning moresophisticated distance metric [50] with our face representation\\nsimultaneously, the performance of our method on LFW\\nprobably can be further boosted.\\nGAO et al. : SINGLE SAMPLE FACE RECOGNITION VIA LEARNING DEEP SUPERVISED AUTOENCODERS 2117\\nV. C ONCLUSION AND FUTURE WORK\\nIn this paper, we propose a supervised auto-encoder,\\nand use it to build deep neural network architecture for\\nextracting robust features for SSPP face representation.\\nBy introducing a similarity preservation term, our supervised\\nauto-encoder enforces faces corresponding to the same person\\nto be represented similarly. Experimental results on the AR,Extended Yale B, and CMU-PIE datasets demonstrate clear\\nsuperiority of this module over other conventional modules\\nsuch as DAE or DLN.\\nIn view of the size of images and training sets, we restrict\\nthe image size to be 32 ×32, and only images of a handful\\nof subjects are used to train the network. For example, only20 subjects are used on the CMU -PIE and AR datasets, and\\nonly 28 subjects on the Extended Yale B dataset. Obviously\\nmore training samples will improve the stability of the learnt\\nnetwork and larger images will improve the face recognition\\naccuracy [57].\\nR\\nEFERENCES\\n[1] Y . Su, S. Shan, X. Chen, and W. Gao, “Adaptive generic learning for\\nface recognition from a single sample per person,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , Jun. 2010, pp. 2699–2706.\\n[2] X. Tan, S. Chen, Z.-H. Zhou, and F. Zhang, “Face recognition from a\\nsingle image per person: A survey,” Pattern Recognit. , vol. 39, no. 9,\\npp. 1725–1745, 2006.\\n[3] L. Fei-Fei, R. Fergus, and P. Perona, “One-shot learning of object\\ncategories,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 28, no. 4,\\npp. 594–611, Apr. 2006.\\n[4] M. A. Turk and A. P. Pentland, “Face recognition using eigenfaces,”\\ninProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. ,\\nJun. 1991, pp. 586–591.\\n[5] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.\\nFisherfaces: Recognition using class speciﬁc linear projection,” IEEE\\nTrans. Pattern Anal. Mach. Intell. , vol. 19, no. 7, pp. 711–720,\\nJul. 1997.\\n[6] Q.-X. Gao, L. Zhang, and D. Zhang, “Face recognition using FLDA\\nwith single training image per person,” Appl. Math. Comput. , vol. 205,\\nno. 2, pp. 726–734, 2008.\\n[7] J. Wright, A. Y . Yang, A. Ganesh, S. S. Sastry, and Y . Ma, “Robust face\\nrecognition via sparse representation,” IEEE Trans. Pattern Anal. Mach.\\nIntell. , vol. 31, no. 2, pp. 210–227, Feb. 2009.\\n[8] Q. V . Le et al. , “Building high-level features using large scale unsuper-\\nvised learning,” in Proc. 29th Int. Conf. Mach. Learn. , 2012, pp. 81–88.\\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation\\nwith deep convolutional neural networks,” in Proc. Adv. Neural Inf.\\nProcess. Syst. , 2012, pp. 1106–1114.\\n[10] G. E. Hinton, S. Osindero, and Y . -W. Teh, “A fast learning algorithm\\nfor deep belief nets,” Neural Comput. , vol. 18, no. 7, pp. 1527–1554,\\n2006.\\n[11] P. Vincent, H. Larochelle, I. Laj oie, Y . Bengio, and P.-A. Manzagol,\\n“Stacked denoising autoencoders: L earning useful representations in a\\ndeep network with a local denoising criterion,” J. Mach. Learn. Res. ,\\nvol. 11, no. 5, pp. 3371–3408, 2010.\\n[12] D. Erhan, Y . Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and\\nS. Bengio, “Why does unsupervised pr e-training help deep learning?”\\nJ. Mach. Learn. Res. , vol. 11, pp. 625–660, Feb. 2010.\\n[13] J. Yang, D. Zhang, A. F. Frangi, and J.-Y . Yang, “Two-dimensional\\nPCA: A new approach to appearance-based face representation andrecognition,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 21, no. 1,\\npp. 131–137, Jan. 2004.\\n[14] J. Wu and Z.-H. Zhou, “Face recognition with one training image\\nper person,” Pattern Recognit. Lett. , vol. 23, no. 14, pp. 1711–1719,\\n2002.\\n[15] S. Chen, D. Zhang, and Z.-H. Zhou, “Enhanced (PC)2A for face\\nrecognition with one training image per person,” Pattern Recognit. Lett. ,\\nvol. 25, no. 10, pp. 1173–1181, 2004.\\n[16] A. M. Martinez, “Recognizing impr ecisely localized, partially occluded,\\nand expression variant faces from a single sample per class,” IEEE Trans.\\nPattern Anal. Mach. Intell. , vol. 24, no. 6, pp. 748–763, Jun. 2002.[17] S. Shan, B. Cao, W. Gao, and D. Zhao, “Extended Fisherface for face\\nrecognition from a single example image per person,” in Proc. IEEE\\nInt. Symp. Circuits Syst. , May 2002, pp. II-81–II-84.\\n[18] S. Chen, J. Liu, and Z.-H. Zhou, “Making FLDA applicable to face\\nrecognition with one sample per person,” Pattern Recognit. , vol. 37,\\nno. 7, pp. 1553–1555, 2004.\\n[19] T.-K. Kim and J. Kittler, “Locally linear discriminant analysis for\\nmultimodally distributed classes for face recognition with a single\\nmodel image,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 27, no. 3,\\npp. 318–327, Mar. 2005.\\n[20] Y . Tang, R. Salakhutdinov, and G. E. Hinton, “Deep Lambertian net-\\nworks,” in Proc. 29th Int. Conf. Mach. Learn. , 2012, pp. 1623–1630.\\n[21] N. Japkowicz, S. J. Hanson, and M. A. Gluck, “Nonlinear autoasso-\\nciation is not equivalent to PCA,” Neural Comput. , vol. 12, no. 3,\\npp. 531–545, 2000.\\n[22] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y . Bengio, “Contractive\\nauto-encoders: Explicit invariance during feature extraction,” in Proc.\\n28th Int. Conf. Mach. Learn. , 2011, pp. 833–840.\\n[23] S. Rifai et al. , “Higher order contractive auto-encoder,” in Proc. Eur.\\nConf. Mach. Learn. , 2011, pp. 645–660.\\n[24] W. Y . Zou, A. Y . Ng, S. Zhu, and K. Yu, “Deep learning of invariant\\nfeatures via simulated ﬁxations in video,” in Proc. Adv. Neural Inf.\\nProcess. Syst. , 2012, pp. 3203–3211.\\n[25] Q. V . Le, A. Karpenko, J. Ngiam, and A. Y . Ng, “ICA with recon-\\nstruction cost for efﬁcient overcomplete feature learning,” in Proc. Adv.\\nNeural Inf. Process. Syst. , 2011.\\n[26] S. Chopra, R. Hadsell, and Y . LeCun, “Learning a similarity met-\\nric discriminatively, with application to face veriﬁcation,” in Proc.\\nIEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. , Jun. 2005,\\npp. 539–546.\\n[27] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf, “Deepface: Closing\\nthe gap to human-level performance in face veriﬁcation,” in Proc. IEEE\\nConf. Comput. Vis. Pattern Recognit. , Jun. 2014, pp. 1701–1708.\\n[28] Z. Zhu, P. Luo, X. Wang, and X. Tang, “Deep learning identity-\\npreserving face space,” in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\\nDec. 2013, pp. 113–120.\\n[29] Z. Zhu, P. Luo, X. Wang, and X. Tang. (2014). “Recover canonical-\\nview faces in the wild with deep neural networks.” [Online]. Available:http://arxiv.org/abs/1404.3543\\n[30] V . Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted\\nBoltzmann machines,” in Proc. 27th Int. Conf. Mach. Learn. , 2010,\\npp. 807–814.\\n[31] P. Lennie, “The cost of cortical computation,” Current Biol. , vol. 13,\\nno. 6, pp. 493–497, 2003.\\n[32] Y . Bengio, “Learning deep architectures for AI,” Found. Trends Mach.\\nLearn. , vol. 2, no. 1, pp. 1–127, 2009.\\n[33] A. Ng, “Sparse autoencoder,” Stanford Univ., Stanford, CA, USA,\\nLecture Notes CS294A, 2011.\\n[34] J. Xie, L. Xu, and E. Chen, “Image denoising and inpainting with\\ndeep neural networks,” in Proc. Adv. Neural Inf. Process. Syst. , 2012,\\npp. 350–358.\\n[35] R. Salakhutdinov and G. E. Hint on, “Learning a nonlinear embedding\\nby preserving class neighbourhood structure,” in Proc. Int. Conf. Artif.\\nIntell. Statist.\\n, 2007, pp. 412–419.\\n[36] X. Glorot and Y . Bengio, “Understa nding the difﬁculty of training deep\\nfeedforward neural networks,” in Proc. Int. Conf. Artif. Intell. Statist. ,\\n2010, pp. 249–256.\\n[37] Y . Bengio, “Practical recommendati ons for gradient-based training of\\ndeep architectures,” in Neural Networks: Tricks of the Trade . Berlin,\\nGermany: Springer-Verlag, 2012, pp. 437–478.\\n[38] Q. V . Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A. Y . Ng,\\n“On optimization methods for deep learning,” in Proc. 28th Int. Conf.\\nMach. Learn. , 2011, pp. 265–272.\\n[39] L. Zhang, M. Yang, and X. Feng, “Spa rse representation or collaborative\\nrepresentation: Which helps face recognition?” in Proc. IEEE Int. Conf.\\nComput. Vis. , Nov. 2011, pp. 471–478.\\n[40] L. Wolf, T. Hassner, and Y . Taigman, “The one-shot similarity kernel,” in\\nProc. IEEE 12th Int. Conf. Comput. Vis. , Sep./Oct. 2009, pp. 897–902.\\n[41] H. Fan, Z. Cao, Y . Jiang, Q. Yin, and C. Doudou. (2014). “Learning deep\\nface representation.” [Online]. Available: http://arxiv.org/abs/1403.2802\\n[42] T. Sim, S. Baker, and M. Bsat, “The CMU pose, illumination, and\\nexpression (PIE) database,” in Proc. 5th IEEE Int. Conf. FG , May 2002,\\npp. 46–51.\\n[43] A. Martínez and R. Benavente, “The AR face database,”\\nCVC Tech. Rep. #24, Jun. 1998.\\n2118 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 10, NO. 10, OCTOBER 2015\\n[44] A. S. Georghiades, P. N. Belhumeur, and D. Kriegman, “From few to\\nmany: Illumination cone models for face recognition under variablelighting and pose,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 23,\\nno. 6, pp. 643–660, Jun. 2001.\\n[45] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-PIE,”\\nImage Vis. Comput. , vol. 28, no. 5, pp. 807–813, 2010.\\n[46] Y . Tang, R. Salakhutdinov, and G. E. Hinton, “Tensor analyzers,” in\\nProc. 30th Int. Conf. Mach. Learn. , 2013, pp. 163–171.\\n[47] S. Gao, I. W.-H. Tsang, and L.-T. Chia, “Sparse representation with\\nkernels,” IEEE Trans. Image Process. , vol. 22, no. 2, pp. 423–434,\\nFeb. 2013.\\n[48] X. Glorot, A. Bordes, and Y . Be ngio, “Deep sparse rectiﬁer neural\\nnetworks,” in Proc. 14th Int. Conf. Artif. Intell. Statist. , 2011,\\npp. 315–323.\\n[49] G. E. Hinton and R. R. Salakhutdi nov, “Reducing the dimensionality of\\ndata with neural networks,” Science , vol. 313, no. 5786, pp. 504–507,\\n2006.\\n[50] J. Hu, J. Lu, and Y .-P. Tan, “Discriminative deep metric learning for\\nface veriﬁcation in the wild,” in Proc. IEEE Conf. Comput. Vis. Pattern\\nRecognit. (CVPR) , Jun. 2014, pp. 1875–1882.\\n[51] N. Pinto, J. J. DiCarlo, and D. D. Cox, “How far can you get with\\na modern face recognition test set using only simple features?” in\\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2009,\\npp. 2591–2598.\\n[52] H. Li, G. Hua, Z. Lin, J. Brandt, and J. Yang, “Probabilistic elastic\\nmatching for pose variant face veriﬁcation,” in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. (CVPR) , Jun. 2013, pp. 3499–3506.\\n[53] S. R. Arashloo and J. Kittler, “Efﬁcient processing of MRFs for\\nunconstrained-pose face recognition,” in Proc. IEEE 6th Int. Conf.\\nBiometrics, Theory, Appl. Syst. (BTAS) , Sep./Oct. 2013, pp. 1–8.\\n[54] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Fisher\\nvector faces in the wild,” in Proc. Brit. Mach. Vis. Conf. , 2013,\\npp. 8.1–8.12.\\n[55] H. Li, G. Hua, X. Shen, Z. Lin, an d J. Brandt, “Eigen-PEP for video face\\nrecognition,” in Computer Vision (Lecture Notes in Computer Science),\\nvol. 9005. Berlin, Germany: Springer-Verlag, 2015, pp. 17–33.\\n[56] S. R. Arashloo and J. Kittler, “Class-speciﬁc kernel fusion of multiple\\ndescriptors for face veriﬁcation using multiscale binarised statisticalimage features,” IEEE Trans. Inf. Forensics Security , vol. 9, no. 12,\\npp. 2100–2109, Dec. 2014.\\n[57] M. D. Zeiler and R. Fergus, “ Visualizing and understanding\\nconvolutional networks,” in Computer Vision (Lecture Notes\\nin Computer Science), vol. 8689. New York, NY , USA:\\nSpringer-Verlag, 2014, pp. 818–833.\\nShenghua Gao received the B.E. (Hons.) degree\\nfrom the University of Science and Technologyof China, in 2008, and the Ph.D. degree from\\nNanyang Technological University, in 2012. He is\\ncurrently an Assistant Professor with ShanghaiTechUniversity, China. From 2012 to 2014, he was\\na Research Scientist with the Advanced Digital\\nSciences Center, Singapore. He has authored over30 papers on object and face recognition related top-ics in many international conferences and journals,\\nincluding the IEEE T\\nRANSACTIONS ON PATTERN\\nANALYSIS AND MACHINE INTELLIGENCE ,t h e International Journal of\\nComputer Vision , the IEEE T RANSACTIONS ON IMAGE PROCESSING ,t h e\\nIEEE T RANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS ,\\nthe IEEE T RANSACTIONS ON MULTIMEDIA , the IEEE T RANSACTIONS ON\\nCIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY ,Computer Vision and\\nPattern Recognition , and European Conference on Computer Vision. His\\nresearch interests include computer vision and machine learning. He received\\nthe Microsoft Research Fellowship in 2010 and the ACM Shanghai YoungScientist Award in 2015.\\nYuting Zhang received the B.E. degree in computer\\nscience from Zhejiang University, in 2009, wherehe is currently pursuing the Ph.D. degree with\\nthe Department of Computer Science, advised\\nby G. Pan. He is currently a Visiting Student withthe Department of Electronic Engineering andComputer Science, University of Michigan, USA.\\nHe was also a Junior Research Assistant with the\\nAdvanced Digital Sciences Center, Singapore, andthe University of Illinois at Urbana–Champaign\\nin 2012. His research interests include machine\\nlearning, computer vision, and in partic ular, application of deep graph model.\\nKui Jia received the B.Eng. degree in marine\\nengineering from Northwestern Polytechnic\\nUniversity, China, in 2001, the M.Eng. degreein electrical and computer engineering from theNational University of Singapore, in 2003, and the\\nPh.D. degree in computer science from Queen Mary,\\nUniversity of London, London, U.K., in 2007. He iscurrently a Visiting Assistant Professor with the\\nUniversity of Macau, Macau, China. He also holds\\na Research Scientist position with the AdvancedDigital Sciences Center, Singapore. His research\\ninterests are computer vision, machine learning, and image processing.\\nJiwen Lu (S’10–M’11) received the B.Eng. degree\\nin mechanical engineering and the M.Eng. degree inelectrical engineering from the Xi’an University of\\nTechnology, Xi’an, China, and the Ph.D. degree in\\nelectrical engineering from Nanyang TechnologicalUniversity, Singapore. He is currently a ResearchScientist with the Advanced Digital Sciences Cen-\\nter, Singapore. His research interests include com-\\nputer vision, pattern recognition, and machine learn-ing.\\nHe has authored or coauthored over 100 scientiﬁc\\npapers in these areas, with 23 papers published in IEEE Transactions journals(IEEE T\\nRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTEL -\\nLIGENCE , the IEEE T RANSACTIONS ON IMAGE PROCESSING , the IEEE\\nTRANSACTIONS ON INFORMATION FORENSICS AND SECURITY , the IEEE\\nTRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY ,\\nand IEEE T RANSACTIONS ON MULTIMEDIA ), and 12 papers published in\\ntop-tier computer vision conferences (ICCV/CVPR/ECCV). He serves as an\\nArea Chair for ICME 2015 and ICB 2015, and a Special Session Chair forVCIP 2015. He was a recipient of the Firs t-Prize National Scholarship and\\nthe National Outstanding Student Awar d from the Ministry of Education of\\nChina in 2002 and 2003, the Best Student Paper Award from the PREMIA of\\nSingapore in 2012, and the Top 10% Best Paper Award from MMSP 2014.Recently, he has given tutorials at some conferences, such as CVPR 2015,\\nFG 2015, ACCV 2014, ICME 2014, and IJCB 2014.\\nYingying Zhang received the B.E. degree from\\nXi’an Jiaotong University, China, in 2014. He is\\ncurrently pursuing the master’s degree withShanghaiTech University, China. His research\\ninterests are computer vision and machine learning.\\n',\n",
       " 'Nothing found',\n",
       " 'Nothing found',\n",
       " 'Nothing found',\n",
       " 'Nothing found',\n",
       " 'Nothing found',\n",
       " 'Nothing found']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame({'links':all_links, 'texts':full_texts})\n",
    "full_df.to_csv('full_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/4563052</td>\n",
       "      <td>3D Facial Expression Recognition Based on Auto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/1640921</td>\n",
       "      <td>3D Facial Expression Recognition Based on Prim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/4813304</td>\n",
       "      <td>3DFacial Expr ession Recognition Based onPrope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/1613022</td>\n",
       "      <td>A 3D Facial Expression Database  For Facial Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/5975141</td>\n",
       "      <td>A Multimodal Database for\\nAffect Recognition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/6130508</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/5543262</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/5771374</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/4459336</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://ieeexplore.ieee.org/document/5653653</td>\n",
       "      <td>Nothing found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           links  \\\n",
       "0   https://ieeexplore.ieee.org/document/4563052   \n",
       "1   https://ieeexplore.ieee.org/document/1640921   \n",
       "2   https://ieeexplore.ieee.org/document/4813304   \n",
       "3   https://ieeexplore.ieee.org/document/1613022   \n",
       "4   https://ieeexplore.ieee.org/document/5975141   \n",
       "..                                           ...   \n",
       "71  https://ieeexplore.ieee.org/document/6130508   \n",
       "72  https://ieeexplore.ieee.org/document/5543262   \n",
       "73  https://ieeexplore.ieee.org/document/5771374   \n",
       "74  https://ieeexplore.ieee.org/document/4459336   \n",
       "75  https://ieeexplore.ieee.org/document/5653653   \n",
       "\n",
       "                                                texts  \n",
       "0   3D Facial Expression Recognition Based on Auto...  \n",
       "1   3D Facial Expression Recognition Based on Prim...  \n",
       "2   3DFacial Expr ession Recognition Based onPrope...  \n",
       "3   A 3D Facial Expression Database  For Facial Be...  \n",
       "4   A Multimodal Database for\\nAffect Recognition ...  \n",
       "..                                                ...  \n",
       "71                                      Nothing found  \n",
       "72                                      Nothing found  \n",
       "73                                      Nothing found  \n",
       "74                                      Nothing found  \n",
       "75                                      Nothing found  \n",
       "\n",
       "[76 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in full_texts:\n",
    "    if i == 'Error' or i == 'Nothing found':\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
