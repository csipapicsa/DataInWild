<html lang="en-US"><head>
<meta name="citation_pii" content="S0031320316301753">
<meta name="citation_issn" content="0031-3203">
<meta name="citation_volume" content="61">
<meta name="citation_lastpage" content="628">
<meta name="citation_publisher" content="Pergamon">
<meta name="citation_firstpage" content="610">
<meta name="citation_journal_title" content="Pattern Recognition">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.patcog.2016.07.026">
<meta name="dc.identifier" content="10.1016/j.patcog.2016.07.026">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromâ€¦">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316X00118-cov150h.gif">
<meta name="citation_title" content="Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order">
<meta property="og:title" content="Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order">
<meta name="citation_publication_date" content="2017/01/01">
<meta name="citation_online_date" content="2016/07/19">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
<title>Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order - ScienceDirect</title>
<link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0031320316301753">
<meta property="og:type" content="article">
<meta name="viewport" content="initial-scale=1">
<meta name="SDTech" content="Proudly brought to you by the SD Technology team">
<script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
<link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
<link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
<link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/arp.css">
<link href="//cdn.pendo.io" rel="dns-prefetch">
<link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
<link rel="dns-prefetch" href="https://smetrics.elsevier.com">
<script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S0031320316301753?componentVersion=V10&amp;jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwMDMxMzIwMzE2MzAxNzUzIiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiZDNlOTE2NTMxZGUwYzg0OTE2MGI3M2Y0NzkyNTBiODU5NGNhZ3hycWIiLCJleHAiOjE3Mjg5OTI1NjUsImlhdCI6MTcyODk5MDc2NSwidmVyc2lvbiI6MSwianRpIjoiNDRlOGZlZWUtYjczMC00MTBkLWFlZDQtMzk3ZTBjMzk1YTM1In0.-33UH45f-7kdvFDosRSUfYpDOHYzmWPfbT2U8yt-djE" type="text/javascript"></script>
<script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"3195E19EB152C7C4-6193D3A958742172","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Pattern Recognition","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
<script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement.min.js" async=""></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement_Module_ActivityMap.min.js" async=""></script><script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/6a62c1bc1779/RCa16d232f95a944c0aabdea6621a2ef94-source.min.js" async=""></script><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202410100101/pubads_impl.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link id="plx-css-summary" type="text/css" rel="stylesheet" href="//cdn.plu.mx/summary.css"><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//cdn.plu.mx/extjs/xss.js"></script><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:bold;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:bold}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none !important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0, 0, 0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none !important}.onetrust-pc-dark-filter.ot-hide{display:none !important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width: 426px)and (max-width: 896px)and (orientation: landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(0.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:normal;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:none;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width: 400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width: 550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width: 550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em 0;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:rgba(0,0,0,0);border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style><link type="text/css" rel="stylesheet" href="https://pendo-static-5661679399600128.storage.googleapis.com/guide.-323232.1721046486120.css" id="_pendo-css_"><style type="text/css" scoped="scoped" class=" pendo-style-D_T2uHq_M1r-XQq8htU6Z3GjHfE" style="white-space: pre-wrap;"></style><style type="text/css">.MathJax_SVG_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax_SVG .MJX-monospace {font-family: monospace}
.MathJax_SVG .MJX-sans-serif {font-family: sans-serif}
#MathJax_SVG_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax_SVG {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax_SVG * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_SVG > div {display: inline-block}
.mjx-svg-href {fill: blue; stroke: blue}
.MathJax_SVG_Processing {visibility: hidden; position: absolute; top: 0; left: 0; width: 0; height: 0; overflow: hidden; display: block!important}
.MathJax_SVG_Processed {display: none!important}
.MathJax_SVG_test {font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.MathJax_SVG_test.mjx-test-display {display: table!important}
.MathJax_SVG_test.mjx-test-inline {display: inline!important; margin-right: -1px}
.MathJax_SVG_test.mjx-test-default {display: block!important; clear: both}
.MathJax_SVG_ex_box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
.mjx-test-inline .MathJax_SVG_left_box {display: inline-block; width: 0; float: left}
.mjx-test-inline .MathJax_SVG_right_box {display: inline-block; width: 0; float: right}
.mjx-test-display .MathJax_SVG_right_box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
</style></head>
<body data-sd-ui-layer-boundary="true" class="toolbar-stuck" style=""><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_SVG_Hidden"></div><svg><defs id="MathJax_SVG_glyphs"><path stroke-width="1" id="MJMATHI-48" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path stroke-width="1" id="MJMAIN-2A" d="M215 721Q216 732 225 741T248 750Q263 750 273 742T284 721L270 571L327 613Q383 654 388 657T399 660Q412 660 423 650T435 624T424 600T376 575Q363 569 355 566L289 534L355 504L424 470Q435 462 435 447Q435 431 424 420T399 409Q393 409 388 412T327 456L270 498L277 423L284 348Q280 320 250 320T215 348L229 498L172 456Q116 415 111 412T100 409Q87 409 76 420T64 447Q64 461 75 470L144 504L210 534L144 566Q136 570 122 576Q83 593 74 600T64 624Q64 639 75 649T100 660Q106 660 111 657T172 613L229 571Q229 578 222 643T215 721Z"></path><path stroke-width="1" id="MJMATHI-57" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path stroke-width="1" id="MJMAIN-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path stroke-width="1" id="MJMATHI-3C3" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path stroke-width="1" id="MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width="1" id="MJMAIN-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path stroke-width="1" id="MJMATHI-3BC" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path stroke-width="1" id="MJMAIN-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width="1" id="MJMATHI-3B1" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path><path stroke-width="1" id="MJMAIN-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width="1" id="MJMAIN-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path stroke-width="1" id="MJMAIN-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path stroke-width="1" id="MJMAIN-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path><path stroke-width="1" id="MJMATHI-78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width="1" id="MJMAIN-2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path stroke-width="1" id="MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width="1" id="MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="MJMATHI-68" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path stroke-width="1" id="MJMATHI-67" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path stroke-width="1" id="MJMATHI-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path stroke-width="1" id="MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="1" id="MJMATHI-7A" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path stroke-width="1" id="MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width="1" id="MJMAIN-6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path stroke-width="1" id="MJMAIN-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path stroke-width="1" id="MJMAIN-78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path stroke-width="1" id="MJMAIN-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width="1" id="MJMATHI-43" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path stroke-width="1" id="MJMATHI-63" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path stroke-width="1" id="MJMATHI-6C" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width="1" id="MJMATHI-61" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path stroke-width="1" id="MJMATHI-73" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path stroke-width="1" id="MJSZ1-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path stroke-width="1" id="MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width="1" id="MJMATHI-45" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path><path stroke-width="1" id="MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width="1" id="MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="1" id="MJMATHI-54" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path stroke-width="1" id="MJMATHI-62" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path stroke-width="1" id="MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="1" id="MJMATHI-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path stroke-width="1" id="MJMAIN-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path stroke-width="1" id="MJMAIN-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path stroke-width="1" id="MJMAIN-25" d="M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z"></path><path stroke-width="1" id="MJMAIN-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path stroke-width="1" id="MJMATHI-3B8" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path><path stroke-width="1" id="MJMAIN-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path></defs></svg></div><div id="MathJax_Message" style="display: none;"></div>
<script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"sect0005"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"sp0165"},"#name":"simple-para","_":"Facial expression recognition has been an active research area in the past 10 years, with growing application areas including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for machine learning methods, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in computer vision. In this work, we propose a simple solution for facial expression recognition that uses a combination of Convolutional Neural Network and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with big data. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods â€“ 96.76% of accuracy in the CK+ database â€“ it is fast to train, and it allows for real time facial expression recognition with standard computers."}],"$":{"view":"all","id":"abs0010"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab0010","class":"author"},"#name":"abstract"},{"$$":[{"$$":[{"$":{"id":"sect0010"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0005"},"#name":"para","_":"A CNN based approach for facial expression recognition."}],"$":{"id":"u0005"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0010"},"#name":"para","_":"A set of pre-processing steps allowing for a simpler CNN architecture."}],"$":{"id":"u0010"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0015"},"#name":"para","_":"A study of the impact of each pre-processing step in the accuracy."}],"$":{"id":"u0015"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0020"},"#name":"para","_":"A study for lowering the impact of the sample presentation order during training."}],"$":{"id":"u0020"},"#name":"list-item"},{"$$":[{"#name":"label","_":"â€¢"},{"$":{"view":"all","id":"p0025"},"#name":"para","_":"High facial expression recognition accuracy (96.76%) with real time evaluation."}],"$":{"id":"u0025"},"#name":"list-item"}],"$":{"id":"li0005"},"#name":"list"}],"$":{"view":"all","id":"sp0170"},"#name":"simple-para"}],"$":{"view":"all","id":"abs0015"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab0015","class":"author-highlights"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banner":{"id":"Banner"},"banners":[{"id":"Banner"},{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"50401","accountName":"IT University of Copenhagen","loginStatus":"logged in","userId":"71970787","isLoggedIn":true},"cid":"272206","content-family":"serial","copyright-line":"Â© 2016 Elsevier Ltd. All rights reserved.","cover-date-years":["2017"],"cover-date-start":"2017-01-01","cover-date-text":"January 2017","document-subtype":"fla","document-type":"article","entitledToken":"77DC43F82B5EAE855502CF452F1DA6C567B355A535A91DEB15D926692E29C5E4808F8D94D42DD242","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMwMDMxMzIwMzE2MzAxNzUzIiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiZDNlOTE2NTMxZGUwYzg0OTE2MGI3M2Y0NzkyNTBiODU5NGNhZ3hycWIiLCJleHAiOjE3Mjg5OTI1NjUsImlhdCI6MTcyODk5MDc2NSwidmVyc2lvbiI6MSwianRpIjoiNDRlOGZlZWUtYjczMC00MTBkLWFlZDQtMzk3ZTBjMzk1YTM1In0.-33UH45f-7kdvFDosRSUfYpDOHYzmWPfbT2U8yt-djE","eid":"1-s2.0-S0031320316301753","doi":"10.1016/j.patcog.2016.07.026","first-fp":"610","hub-eid":"1-s2.0-S0031320316X00118","issuePii":"S0031320316X00118","item-weight":"FULL-TEXT","language":"en","last-lp":"628","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au0020","biographyid":"bio0020"},"$$":[{"#name":"given-name","_":"Thiago"},{"#name":"surname","_":"Oliveira-Santos"},{"#name":"cross-ref","$":{"id":"cr0125","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"cr0130","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZWFkZDAwMjAlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlN0QlMkMlMjJfJTIyJTNBJTIydG9kc2FudG9zJTQwaW5mLnVmZXMuYnIlMjIlN0Q="}]}]},"normalized-first-auth-initial":"A","normalized-first-auth-surname":"LOPES","pages":[{"last-page":"628","first-page":"610"}],"pii":"S0031320316301753","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2018-10-13T00:00:00.000Z"},{"#name":"sa-embargo-status","_":"UnderEmbargo"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Pattern Recognition","suppl":"C","timestamp":"2017-09-23T09:30:33.424391-04:00","title":{"content":[{"#name":"title","$":{"id":"tit0005"},"_":"Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"61","vol-iss-suppl-text":"Volume 61","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":82,"freeHtmlGiven":false,"userProfile":{"departmentName":"Library","webUserId":"71970787","accountName":"IT University of Copenhagen","shibProfile":{"canRegister":false},"departmentId":"71310","hasMultipleOrganizations":false,"accountNumber":"C000050401","userName":"Gergo Gyori","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLqqwvNhq3slt9cD6pF7tpgA*","accessType":"SHIBREG","accountId":"50401","userType":"NORMAL","privilegeType":"BASIC","email":"gegy@itu.dk"},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"authenticationMethod":"SHIBBOLETH","entitled":true,"isCasaUser":false,"usageInfo":"(71970787,U|71310,D|50401,A|26,S|34,P|2,PL)(SDFE,CON|d3e916531de0c849160b73f479250b8594cagxrqb,SSO|REG_SHIBBOLETH,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"19 July 2016","Received":"30 January 2016","Revised":["15 July 2016"],"Accepted":"16 July 2016","Publication date":"1 January 2017","Version of Record":"13 October 2016"},"downloadFullIssue":true,"entitlementReason":"package","hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/ci/pta/login/redirect/home/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLqqwvNhq3slt9cD6pF7tpgA*","contactUrl":"https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLqqwvNhq3slt9cD6pF7tpgA*","userName":"Gergo Gyori","userEmail":"gegy@itu.dk","orgName":"IT University of Copenhagen","webUserId":"71970787","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":true,"hasMultiOrg":false,"userType":"SHIBREG","userAnonymity":"INDIVIDUAL","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLqqwvNhq3slt9cD6pF7tpgA*","institutionName":"your institution"},"isCorpReq":false,"isPdfFullText":false,"issn":"00313203","issn-primary-formatted":"0031-3203","issRange":"","isThirdParty":false,"pageCount":19,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"3652c422d2874205c7eb2b753a3aa71b","pid":"1-s2.0-S0031320316301753-main.pdf"},"pii":"S0031320316301753","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Pergamon","id":"67"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":299,"openArchiveStatus":false,"openArchiveArticleCount":20,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0031320316X00118-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0031320316X00118/cover/DOWNSAMPLED200/image/gif/516c7fdfa3c910cb4f085758653503b6/cov200h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316X00118/cover/DOWNSAMPLED200/image/gif/516c7fdfa3c910cb4f085758653503b6/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"13947","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S0031320316X00118-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0031320316X00118/cover/DOWNSAMPLED/image/gif/5c2e8e7a65c1a238f763c47027073ef0/cov150h.gif","https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316X00118/cover/DOWNSAMPLED/image/gif/5c2e8e7a65c1a238f763c47027073ef0/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"11054","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S00313203.gif","title":"pattern-recognition","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S0031320316X00118-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0031320316X00118-cov150h.gif"},"volRange":"61","features":["keywords","references","biography","preview"],"open-research":{},"titleString":"Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"referenceLinks":true,"references":true,"referredToBy":true,"recommendations-entitled":true,"toc":true,"body":true,"recommendations":true,"citingArticles":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"ag0005"},"$$":[{"#name":"author","$":{"id":"au0005","biographyid":"bio1"},"$$":[{"#name":"given-name","_":"AndrÃ© Teixeira"},{"#name":"surname","_":"Lopes"},{"#name":"cross-ref","$":{"id":"cr0110","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZWFkZDAwMDUlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlN0QlMkMlMjJfJTIyJTNBJTIyYW5kcmV0ZWl4ZWlyYWxvcGVzJTQwZ21haWwuY29tJTIyJTdE"}]},{"#name":"author","$":{"id":"au0010","biographyid":"bio0010"},"$$":[{"#name":"given-name","_":"Edilson"},{"#name":"surname","_":"de Aguiar"},{"#name":"cross-ref","$":{"id":"cr0115","refid":"aff0010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZWFkZDAwMTAlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlN0QlMkMlMjJfJTIyJTNBJTIyZWRpbHNvbi5kZS5hZ3VpYXIlNDBnbWFpbC5jb20lMjIlN0Q="}]},{"#name":"author","$":{"id":"au0015","biographyid":"bio0015"},"$$":[{"#name":"given-name","_":"Alberto F."},{"#name":"surname","_":"De Souza"},{"#name":"cross-ref","$":{"id":"cr0120","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZWFkZDAwMTUlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlN0QlMkMlMjJfJTIyJTNBJTIyYWxiZXJ0byU0MGxjYWQuaW5mLnVmZXMuYnIlMjIlN0Q="}]},{"#name":"author","$":{"id":"au0020","biographyid":"bio0020"},"$$":[{"#name":"given-name","_":"Thiago"},{"#name":"surname","_":"Oliveira-Santos"},{"#name":"cross-ref","$":{"id":"cr0125","refid":"aff0005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"cr0130","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"âŽ"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMmlkJTIyJTNBJTIyZWFkZDAwMjAlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlN0QlMkMlMjJfJTIyJTNBJTIydG9kc2FudG9zJTQwaW5mLnVmZXMuYnIlMjIlN0Q="}]},{"#name":"affiliation","$":{"id":"aff0005"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Informatics, Universidade Federal do EspÃ­rito Santo (Campus VitÃ³ria), 514 Fernando Ferrari Avenue, 29075910Â Goiabeiras, VitÃ³ria, EspÃ­rito Santo, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Informatics, Universidade Federal do EspÃ­rito Santo (Campus VitÃ³ria)"},{"#name":"address-line","_":"514 Fernando Ferrari Avenue"},{"#name":"city","_":"Goiabeiras, VitÃ³ria"},{"#name":"state","_":"EspÃ­rito Santo"},{"#name":"postal-code","_":"29075910"},{"#name":"country","_":"Brazil"}]}]},{"#name":"affiliation","$":{"id":"aff0010"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Computing and Electronics, Universidade Federal do EspÃ­rito Santo (Campus SÃ£o Mateus), BR 101 North highway, km 60, 29932540 Bairro LitorÃ¢neo, SÃ£o Mateus, EspÃ­rito Santo, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing and Electronics, Universidade Federal do EspÃ­rito Santo (Campus SÃ£o Mateus)"},{"#name":"address-line","_":"BR 101 North highway, km 60"},{"#name":"city","_":"Bairro LitorÃ¢neo, SÃ£o Mateus"},{"#name":"state","_":"EspÃ­rito Santo"},{"#name":"postal-code","_":"29932540"},{"#name":"country","_":"Brazil"}]}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Correspondence to: Departamento de InformÃ¡tica, Centro TecnolÃ³gico, Universidade Federal do EspÃ­rito Santo, Campus UniversitÃ¡rio de Goiabeiras, PrÃ©dio: CT-VII, Sala 28, Av. Fernando Ferrari, 541, 29075910 VitÃ³ria â€“ ES, Brazil."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Departamento de InformÃ¡tica, Centro TecnolÃ³gico, Universidade Federal do EspÃ­rito Santo, Campus UniversitÃ¡rio de Goiabeiras"},{"#name":"address-line","_":"PrÃ©dio: CT-VII, Sala 28, Av. Fernando Ferrari, 541"},{"#name":"city","_":"VitÃ³ria"},{"#name":"state","_":"ES"},{"#name":"postal-code","_":"29075910"},{"#name":"country","_":"Brazil"}]}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff0005":{"#name":"affiliation","$":{"id":"aff0005"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Informatics, Universidade Federal do EspÃ­rito Santo (Campus VitÃ³ria), 514 Fernando Ferrari Avenue, 29075910Â Goiabeiras, VitÃ³ria, EspÃ­rito Santo, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Informatics, Universidade Federal do EspÃ­rito Santo (Campus VitÃ³ria)"},{"#name":"address-line","_":"514 Fernando Ferrari Avenue"},{"#name":"city","_":"Goiabeiras, VitÃ³ria"},{"#name":"state","_":"EspÃ­rito Santo"},{"#name":"postal-code","_":"29075910"},{"#name":"country","_":"Brazil"}]}]},"aff0010":{"#name":"affiliation","$":{"id":"aff0010"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Computing and Electronics, Universidade Federal do EspÃ­rito Santo (Campus SÃ£o Mateus), BR 101 North highway, km 60, 29932540 Bairro LitorÃ¢neo, SÃ£o Mateus, EspÃ­rito Santo, Brazil"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computing and Electronics, Universidade Federal do EspÃ­rito Santo (Campus SÃ£o Mateus)"},{"#name":"address-line","_":"BR 101 North highway, km 60"},{"#name":"city","_":"Bairro LitorÃ¢neo, SÃ£o Mateus"},{"#name":"state","_":"EspÃ­rito Santo"},{"#name":"postal-code","_":"29932540"},{"#name":"country","_":"Brazil"}]}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"âŽ"},{"#name":"text","_":"Correspondence to: Departamento de InformÃ¡tica, Centro TecnolÃ³gico, Universidade Federal do EspÃ­rito Santo, Campus UniversitÃ¡rio de Goiabeiras, PrÃ©dio: CT-VII, Sala 28, Av. Fernando Ferrari, 541, 29075910 VitÃ³ria â€“ ES, Brazil."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Departamento de InformÃ¡tica, Centro TecnolÃ³gico, Universidade Federal do EspÃ­rito Santo, Campus UniversitÃ¡rio de Goiabeiras"},{"#name":"address-line","_":"PrÃ©dio: CT-VII, Sala 28, Av. Fernando Ferrari, 541"},{"#name":"city","_":"VitÃ³ria"},{"#name":"state","_":"ES"},{"#name":"postal-code","_":"29075910"},{"#name":"country","_":"Brazil"}]}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{"content":[{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio1","view":"all"},"$$":[{"#name":"link","$":{"type":"simple","href":"pii:S0031320316301753/fx1","locator":"fx1"}},{"#name":"simple-para","$":{"id":"sp0175","view":"all"},"$$":[{"#name":"bold","_":"AndrÃ© Teixeira Lopes"},{"#name":"__text__","_":" was born in Cachoeiro de Itapemirim, ES, Brazil, on March 19, 1992. He received the B.Sc. degree in computer science in 2013 from the Universidade Federal do EspÃ­rito Santo (UFES). He received the M.Sc. degree in computer science from the same university in 2016. Currently, he is a Ph.D. student and a member of the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory), both at UFES, in VitÃ³ria, ES, Brazil. His research interests include the following topics computer vision, image processing, machine learning and computer graphics."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio0010","view":"all"},"$$":[{"#name":"link","$":{"type":"simple","href":"pii:S0031320316301753/fx2","locator":"fx2"}},{"#name":"simple-para","$":{"id":"sp0180","view":"all"},"$$":[{"#name":"bold","_":"Edilson de Aguiar"},{"#name":"__text__","_":" was born in Vila Velha, ES, Brazil, on June 11, 1979. In March 2002, he received the B.Sc. degree in computer engineering from the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He received the M.Sc. degree in computer science in December 2003 and the Ph.D. degree in computer science in December 2008, both from the Saarland University and the Max-Planck Institute for Computer Science, in SaarbrÃ¼cken, Saarland, Germany. After that he worked as researcher from 2009 to 2010 at the Disney Research laboratory in Pittsburgh, USA. Since then, he has been with the Departamento de ComputaÃ§Ã£o e EletrÃ´nica of UFES, in SÃ£o Mateus, ES, Brazil, where he is an adjunct professor and researcher at the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory). His research interests are in the areas of computer graphics, computer vision, image processing and robotics. He has been involved in research projects financed through Brazilian research agencies, such as State of EspÃ­rito Santo Research Foundation (FundaÃ§Ã£o de Apoio a Pesquisa do Estado do EspÃ­rito Santo â€“ FAPES). He has also been in the program committee and organizing committee of national and international conferences in computer science."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio0015","view":"all"},"$$":[{"#name":"link","$":{"type":"simple","href":"pii:S0031320316301753/fx3","locator":"fx3"}},{"#name":"simple-para","$":{"id":"sp0185","view":"all"},"$$":[{"#name":"bold","_":"Alberto F. De Souza"},{"#name":"__text__","_":" was born in Cachoeiro de Itapemirim, ES, Brazil, on October 27, 1963. He received the B. Eng. (Cum Laude) degree in electronics engineering and M.Sc. in systems engineering in computer science from the Universidade Federal do Rio de Janeiro (COPPE/UFRJ), in Rio de Janeiro, RJ, Brazil, in 1988 and 1993, respectively; and Doctor of Philosophy (Ph.D.) in computer science from the University College London, in London, United Kingdom, in 1999. He is a professor of computer science and coordinator of the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory) at the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He has authored/co-authored one USA patent and over 90 publications. He has edited proceedings of four conferences (two IEEE sponsored conferences), is a standing member of the Steering Committee of the International Conference in Computer Architecture and High Performance Computing (SBAC-PAD), senior member of the IEEE, and comendador of the order of Rubem Braga."}]}]},{"#name":"biography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bio0020","view":"all"},"$$":[{"#name":"link","$":{"type":"simple","href":"pii:S0031320316301753/fx4","locator":"fx4"}},{"#name":"simple-para","$":{"id":"sp0190","view":"all"},"$$":[{"#name":"bold","_":"Thiago Oliveira-Santos"},{"#name":"__text__","_":" was born in VitÃ³ria, ES, Brazil, on December 13, 1979. In 2004, he received the B.Sc. degree in computer engineering from the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He received the M.Sc. degree in computer science from the same university in 2006. In 2011, he received a Ph.D. degree in biomedical engineering from the University of Bern in Switzerland, where he also worked as a post-doctoral researcher until 2013. Since then, he has been working as an adjunct professor at theÂ Department of Computer Science of UFES in Vitoria, ES, Brazil. His research activities are performed at the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory) and include the following topics computer vision, image processing, computer graphics, and robotics."}]}]}],"floats":[],"footnotes":[],"attachments":[{"attachment-eid":"1-s2.0-S0031320316301753-fx1.sml","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx1/THUMBNAIL/image/gif/f9cfdc53605ec04deb970cd1dac9d6c6/fx1.sml","file-basename":"fx1","filename":"fx1.sml","extension":"sml","filesize":"25186","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0031320316301753-fx2.sml","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx2/THUMBNAIL/image/gif/ceeb9695eec128be9aa735d918fd344d/fx2.sml","file-basename":"fx2","filename":"fx2.sml","extension":"sml","filesize":"25362","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0031320316301753-fx3.sml","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx3/THUMBNAIL/image/gif/7b5c570e3f90f0e34718d586a2c95239/fx3.sml","file-basename":"fx3","filename":"fx3.sml","extension":"sml","filesize":"22764","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0031320316301753-fx4.sml","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx4/THUMBNAIL/image/gif/98f76a78a2dd63782fd3d0e90c6dc81c/fx4.sml","file-basename":"fx4","filename":"fx4.sml","extension":"sml","filesize":"22006","pixel-height":"164","pixel-width":"123","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S0031320316301753-fx1.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx1/DOWNSAMPLED/image/jpeg/8076f3871fd88555f1099cceb19cb08a/fx1.jpg","file-basename":"fx1","filename":"fx1.jpg","extension":"jpg","filesize":"27533","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0031320316301753-fx2.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx2/DOWNSAMPLED/image/jpeg/0f252820b6ff19511a6a61dd9ce16b31/fx2.jpg","file-basename":"fx2","filename":"fx2.jpg","extension":"jpg","filesize":"25035","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0031320316301753-fx3.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx3/DOWNSAMPLED/image/jpeg/61f2c4f10f20ed1902be3cd445c534c7/fx3.jpg","file-basename":"fx3","filename":"fx3.jpg","extension":"jpg","filesize":"23085","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0031320316301753-fx4.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx4/DOWNSAMPLED/image/jpeg/25fd76d9924201be9dcd065008a4f46f/fx4.jpg","file-basename":"fx4","filename":"fx4.jpg","extension":"jpg","filesize":"20955","pixel-height":"151","pixel-width":"113","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S0031320316301753-fx1_lrg.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx1/HIGHRES/image/jpeg/b0e6f37098f1e36d3124d20fb34e193f/fx1_lrg.jpg","file-basename":"fx1","filename":"fx1_lrg.jpg","extension":"jpg","filesize":"57364","pixel-height":"400","pixel-width":"300","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0031320316301753-fx2_lrg.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx2/HIGHRES/image/jpeg/e61e4c7cdf08ba62f74eb9c2f8b3a247/fx2_lrg.jpg","file-basename":"fx2","filename":"fx2_lrg.jpg","extension":"jpg","filesize":"45193","pixel-height":"400","pixel-width":"300","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0031320316301753-fx3_lrg.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx3/HIGHRES/image/jpeg/0efd2906bd35f585d22d9c504a7e5110/fx3_lrg.jpg","file-basename":"fx3","filename":"fx3_lrg.jpg","extension":"jpg","filesize":"41454","pixel-height":"400","pixel-width":"300","attachment-type":"IMAGE-HIGH-RES"},{"attachment-eid":"1-s2.0-S0031320316301753-fx4_lrg.jpg","ucs-locator":"https://s3.amazonaws.com/prod-ucs-content-store-us-east/content/pii:S0031320316301753/fx4/HIGHRES/image/jpeg/a9eb0329ac7e95cce6f3b1f7c688fd99/fx4_lrg.jpg","file-basename":"fx4","filename":"fx4_lrg.jpg","extension":"jpg","filesize":"35804","pixel-height":"400","pixel-width":"300","attachment-type":"IMAGE-HIGH-RES"}]},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":true},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"keys0005","lang":"en","class":"keyword","view":"all"},"$$":[{"#name":"section-title","$":{"id":"sect0015"},"_":"Keywords"},{"#name":"keyword","$":{"id":"key0005"},"$$":[{"#name":"text","_":"Facial expression recognition"}]},{"#name":"keyword","$":{"id":"key0010"},"$$":[{"#name":"text","_":"Convolutional Neural Networks"}]},{"#name":"keyword","$":{"id":"key0015"},"$$":[{"#name":"text","_":"Computer vision"}]},{"#name":"keyword","$":{"id":"key0020"},"$$":[{"#name":"text","_":"Machine learning"}]},{"#name":"keyword","$":{"id":"key0025"},"$$":[{"#name":"text","_":"Expression specific features"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"isOpen":true},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}};
      </script>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1728990766050?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AREG_SHIBBOLETH&c1=ae%3A50401&c12=ae%3A71970787 />
    </noscript>
<a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
<div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container u-hide-from-print"><div id="gh-profile-cnt" class="u-flex-center-ver gh-move-to-spine"><div class="popover" id="gh-profile-dropdown"><div id="popover-trigger-gh-profile-dropdown"><input type="hidden"><button class="button-link gh-icon-btn gh-user-icon u-margin-l-left gh-truncate text-s button-link-secondary button-link-medium button-link-icon-left" title="Gergo Gyori" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-person"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="button-link-text-container"><span class="button-link-text">Gergo Gyori</span></span></button></div></div></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id="institution-popover"><div id="popover-trigger-institution-popover"><input type="hidden"><button class="button-link gh-icon-btn gh-has-institution gh-truncate text-s button-link-secondary u-margin-l-left button-link-medium button-link-icon-left" id="gh-inst-icon-btn" aria-expanded="false" aria-label="Institutional Access" title="IT University of Copenhagen" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-institution gh-inst-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="button-link-text-container"><span class="button-link-text">IT University of Copenhagen</span></span></button></div></div></div><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S0031320316301753/pdfft?md5=3652c422d2874205c7eb2b753a3aa71b&amp;pid=1-s2.0-S0031320316301753-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="search" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ab0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry" id="ai-components-toc-entry"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#keys0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction"><span class="anchor-text-container"><span class="anchor-text">1. Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0010" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Related work"><span class="anchor-text-container"><span class="anchor-text">2. Related work</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0015" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Facial expression recognition system"><span class="anchor-text-container"><span class="anchor-text">3. Facial expression recognition system</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0050" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Experiments and discussions"><span class="anchor-text-container"><span class="anchor-text">4. Experiments and discussions</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0085" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Conclusion"><span class="anchor-text-container"><span class="anchor-text">5. Conclusion</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0090" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Conflict of interest"><span class="anchor-text-container"><span class="anchor-text">Conflict of interest</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ack0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgment"><span class="anchor-text-container"><span class="anchor-text">Acknowledgment</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#s0100" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Appendix A"><span class="anchor-text-container"><span class="anchor-text">Appendix A. </span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#bibliog0005" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#bio1" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Vitae"><span class="anchor-text-container"><span class="anchor-text">Vitae</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (671)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (12)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.1. Three different subjects with the happy expression" class="u-display-block" height="85px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr1.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0010" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.2. Overview of the proposed facial expression recognition system" class="u-display-block" height="140px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr2.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0015" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.3. Illustration of the synthetic sample generation" class="u-display-block" height="164px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr3.sml" width="142px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0020" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.4. Rotation correction example" class="u-display-block" height="75px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr4.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0025" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.5. Image cropping example" class="u-display-block" height="89px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr5.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#f0030" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig.6. Illustration of the intensity normalization" class="u-display-block" height="106px" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr6.sml" width="219px"></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 6 more figures</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (20)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Preprocessing steps tuning for the CK+ database: (a) no pre-processing; (b) just cropping; (c) just rotation correction; (d) cropping and rotation correction; (e) only intensity normalization; (f) bot..."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0010" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Impact of the presentation order in the accuracy."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0015" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Accuracy for both classifiers using all processing steps and the synthetic samples for six expressions on the CK+ database."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;3</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0020" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Training parameters."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;4</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0025" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Confusion matrix using both normalizations and synthetic samples for six expressions on the CK+ database."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;5</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#t0030" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Accuracy for both classifiers using all processing steps and the synthetic samples for seven expressions."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;6</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-tables" type="button"><span class="button-link-text-container"><span class="button-link-text">Show all tables</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/pattern-recognition" title="Go to Pattern Recognition on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/pattern-recognition" title="Go to Pattern Recognition on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Pattern Recognition</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/pattern-recognition/vol/61/suppl/C" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volume 61</span></span></a>, <!-- -->January 2017<!-- -->, Pages 610-628</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/pattern-recognition/vol/61/suppl/C"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316X00118-cov150h.gif" alt="Pattern Recognition"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">AndrÃ© Teixeira</span> <span class="text surname">Lopes</span> </span><span class="author-ref" id="baff0005"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0010" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Edilson</span> <span class="text surname">de Aguiar</span> </span><span class="author-ref" id="baff0010"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0015" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Alberto F.</span> <span class="text surname">De Souza</span> </span><span class="author-ref" id="baff0005"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0020" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Thiago</span> <span class="text surname">Oliveira-Santos</span> </span><span class="author-ref" id="baff0005"><sup>a</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.patcog.2016.07.026" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.patcog.2016.07.026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0031320316301753&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author-highlights" id="ab0015"><div id="abs0015"><h3 class="u-h4 u-margin-m-top u-margin-xs-bottom" id="sect0010">Highlights</h3><div class="u-margin-s-bottom" id="sp0170"><ul class="list"><li class="react-xocs-list-item"><span class="list-label">â€¢</span><span><div class="u-margin-s-bottom" id="p0005">A CNN based approach for facial expression recognition.</div></span></li><li class="react-xocs-list-item"><span class="list-label">â€¢</span><span><div class="u-margin-s-bottom" id="p0010">A set of pre-processing steps allowing for a simpler CNN architecture.</div></span></li><li class="react-xocs-list-item"><span class="list-label">â€¢</span><span><div class="u-margin-s-bottom" id="p0015">A study of the impact of each pre-processing step in the accuracy.</div></span></li><li class="react-xocs-list-item"><span class="list-label">â€¢</span><span><div class="u-margin-s-bottom" id="p0020">A study for lowering the impact of the sample presentation order during training.</div></span></li><li class="react-xocs-list-item"><span class="list-label">â€¢</span><span><div class="u-margin-s-bottom" id="p0025">High facial expression recognition accuracy (96.76%) with real time evaluation.</div></span></li></ul></div></div></div><div class="abstract author" id="ab0010"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="abs0010"><div class="u-margin-s-bottom" id="sp0165"><span>Facial expression recognition has been an active research area in the past 10 years, with growing <a href="/topics/computer-science/application-area" title="Learn more about application areas from ScienceDirect's AI-generated Topic Pages" class="topic-link">application areas</a> including avatar animation, neuromarketing and sociable robots. The recognition of facial expressions is not an easy problem for </span><a href="/topics/engineering/machine-learning-method" title="Learn more about machine learning methods from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning methods</a><span><span><span>, since people can vary significantly in the way they show their expressions. Even images of the same person in the same facial expression can vary in brightness, background and pose, and these variations are emphasized if considering different subjects (because of variations in shape, ethnicity among others). Although facial expression recognition is very studied in the literature, few works perform fair evaluation avoiding mixing subjects while training and testing the proposed algorithms. Hence, facial expression recognition is still a challenging problem in <a href="/topics/engineering/computervision" title="Learn more about computer vision from ScienceDirect's AI-generated Topic Pages" class="topic-link">computer vision</a>. In this work, we propose a simple solution for facial expression recognition that uses a combination of </span><a href="/topics/engineering/convolutional-neural-network" title="Learn more about Convolutional Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">Convolutional Neural Network</a> and specific image pre-processing steps. Convolutional Neural Networks achieve better accuracy with </span><a href="/topics/engineering/big-data" title="Learn more about big data from ScienceDirect's AI-generated Topic Pages" class="topic-link">big data</a>. However, there are no publicly available datasets with sufficient data for facial expression recognition with deep architectures. Therefore, to tackle the problem, we apply some pre-processing techniques to extract only expression specific features from a face image and explore the presentation order of the samples during training. The experiments employed to evaluate our technique were carried out using three largely used public databases (CK+, JAFFE and BU-3DFE). A study of the impact of each image pre-processing operation in the accuracy rate is presented. The proposed method: achieves competitive results when compared with other facial expression recognition methods â€“ 96.76% of accuracy in the CK+ database â€“ it is fast to train, and it allows for real time facial expression recognition with standard computers.</span></div></div></div></div><div id="reading-assistant-main-body-section"></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" href="/science/article/pii/S0031320316301625"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S0031320316301650"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="keys0005" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="key0005" class="keyword"><span>Facial expression recognition</span></div><div id="key0010" class="keyword"><span>Convolutional Neural Networks</span></div><div id="key0015" class="keyword"><span>Computer vision</span></div><div id="key0020" class="keyword"><span>Machine learning</span></div><div id="key0025" class="keyword"><span>Expression specific features</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="s0005"><h2 id="sect0020" class="u-h4 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div class="u-margin-s-bottom" id="p0030">Facial expression is one of the most important features of human emotion recognition <a class="anchor anchor-primary" href="#bib1" name="bbib1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib1"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a>. It was introduced as a research field by Darwin in his book â€œThe Expression of the Emotions in Man and Animals' <a class="anchor anchor-primary" href="#bib2" name="bbib2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib2"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a>. According to Li and Jain <a class="anchor anchor-primary" href="#bib3" name="bbib3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib3"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a><span>, it can be defined as the facial changes in response to a person's internal <a href="/topics/engineering/emotional-state" title="Learn more about emotional state from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional state</a>, intentions, or social communication. Nowadays, automated facial expression recognition has a large variety of applications, such as data-driven animation, neuromarketing, interactive games, sociable robotics and many other humanâ€“computer interaction systems.</span></div><div class="u-margin-s-bottom" id="p0035">Expression recognition is a task that humans perform daily and effortlessly <a class="anchor anchor-primary" href="#bib3" name="bbib3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib3"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a><span>, but it is not yet easily performed by computers, despite recent methods have presented with accuracies larger than 95% in some conditions (frontal face, controlled environments, and high-resolution images). Many works in the literature do not perform a consistent evaluation methodology (e.g. without subject overlap in training and testing) and therefore present a misleading high-accuracy, but do not represent most of the face expression recognition problems real scenarios. On the other hand, low accuracy has been reported on databases with uncontrolled environments and in cross-database evaluations. Trying to cope with these limitations, several <a href="/topics/engineering/research-work" title="Learn more about research works from ScienceDirect's AI-generated Topic Pages" class="topic-link">research works</a><span> have tried to make computers reach the same accuracy of humans, and some examples of these works are highlighted below. This problem is still a challenge for computers because it is very hard to separate the expressionsâ€™ <a href="/topics/engineering/feature-space" title="Learn more about feature space from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature space</a><span>, i.e. <a href="/topics/computer-science/facial-feature" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a> from one subject in two different expressions may be very close in the feature space, while facial features from two subjects with the same expression may be very far from each other. In addition, some expressions like â€œsadâ€ and â€œfearâ€, for example, are, in some cases, very similar.</span></span></span></div><div class="u-margin-s-bottom"><div id="p0040"><a class="anchor anchor-primary" href="#f0005" name="bf0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0005"><span class="anchor-text-container"><span class="anchor-text">Fig. 1</span></span></a> shows three subjects with a happy expression. As it can be seen in the figure, the images vary a lot from each other not only in the way that the subjects show their expression, but also in lighting, brightness, pose and background. This figure also exemplifies another challenge related to the facial expression recognition that is the uncontrolled trainingâ€“testing scenarios (training images can be very different in terms of environmental conditions and subject ethnicity from the testing images). One approach to evaluate the facial expression recognition under these scenarios is to train the method with one database and to test it with another (possibly from different ethnic groups). We present results following this approach.</div><figure class="figure text-xs" id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr1.jpg" height="263" alt="Fig.&nbsp;1" aria-describedby="cap0005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (347KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (347KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0005"><p id="sp0005"><span class="label">Fig.&nbsp;1</span>. Three different subjects with the happy expression. As it can be seen, the images vary a lot from each other not only in the way that the subjects show their expression but also in light, brightness, position and background. The images are from the following databases: CK+ database <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, JAFFE database <a class="anchor anchor-primary" href="#bib5" name="bbib5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib5"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a> and BU-3DFE database <a class="anchor anchor-primary" href="#bib6" name="bbib6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib6"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a>, in this order.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0045">Facial expression recognition systems can be divided into two main categories: those that work with static images <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, <a class="anchor anchor-primary" href="#bib9" name="bbib9" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib9"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a>, <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>, <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a>, <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a> and those that work with dynamic image sequences <a class="anchor anchor-primary" href="#bib14" name="bbib14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib14"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>, <a class="anchor anchor-primary" href="#bib15" name="bbib15" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib15"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a>, <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, <a class="anchor anchor-primary" href="#bib17" name="bbib17" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib17"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a>. Static-based methods do not use temporal information, i.e. the feature vector comprises information about the current input image only. Sequence based methods, in the other hand, use temporal information of images to recognize the expression captured from one or more frames. Automated systems for facial expression recognition receive the expected input (static image or image sequence) and typically give as output one of six basic expressions (anger, sad, surprise, happy, disgust and fear, for example); some systems also recognize the neutral expression. This work will focus on methods based on static images and it will consider the six and seven expressions sets (six basic plus neutral), for controlled and uncontrolled scenarios.</div><div class="u-margin-s-bottom" id="p0050">As described by Li and Jain <a class="anchor anchor-primary" href="#bib3" name="bbib3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib3"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, automatic facial expression analysis comprises three steps: face acquisition, facial data extraction and representation, and facial expression recognition. Face acquisition can be split in two major steps: face detection <a class="anchor anchor-primary" href="#bib18" name="bbib18" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib18"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a>, <a class="anchor anchor-primary" href="#bib19" name="bbib19" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib19"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a>, <a class="anchor anchor-primary" href="#bib20" name="bbib20" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib20"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>, <a class="anchor anchor-primary" href="#bib21" name="bbib21" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib21"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a><span> and <a href="/topics/engineering/head-pose-estimation" title="Learn more about head pose estimation from ScienceDirect's AI-generated Topic Pages" class="topic-link">head pose estimation</a> </span><a class="anchor anchor-primary" href="#bib22" name="bbib22" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib22"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a>, <a class="anchor anchor-primary" href="#bib23" name="bbib23" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib23"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a>, <a class="anchor anchor-primary" href="#bib24" name="bbib24" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib24"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a>. After the face acquisition, the facial changes caused by facial expressions need to be extracted. These changes are usually extracted using geometric feature-based methods <a class="anchor anchor-primary" href="#bib25" name="bbib25" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib25"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>, <a class="anchor anchor-primary" href="#bib26" name="bbib26" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib26"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a>, <a class="anchor anchor-primary" href="#bib27" name="bbib27" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib27"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a>, <a class="anchor anchor-primary" href="#bib21" name="bbib21" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib21"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a> or appearance-based methods <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, <a class="anchor anchor-primary" href="#bib9" name="bbib9" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib9"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a>, <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>, <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a>, <a class="anchor anchor-primary" href="#bib25" name="bbib25" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib25"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>, <a class="anchor anchor-primary" href="#bib28" name="bbib28" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib28"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a><span>. The extracted features are often represented in vectors, referred as feature vectors. Geometric feature-based methods work with shape and location of facial components like mouth, eyes, nose and eyebrows. The feature vector that represents the face geometry is composed of facial components or facial feature points. Appearance-based methods work with feature vectors extracted from the whole face, or from specific regions; these feature vectors are acquired using <a href="/topics/earth-and-planetary-sciences/image-filter" title="Learn more about image filters from ScienceDirect's AI-generated Topic Pages" class="topic-link">image filters</a> applied to the whole face image </span><a class="anchor anchor-primary" href="#bib3" name="bbib3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib3"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0055">Once feature vectors related to the facial expression are available, expression recognition can be performed. According to Liu et al. <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a><span>, expression recognition systems basically use a three-stage training procedure: <a href="/topics/computer-science/representation-learning" title="Learn more about feature learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature learning</a>, feature selection and classifier construction, in this order. The feature learning stage is responsible for the extraction of all features related to the facial expression. The feature selection selects the best features to represent the facial expression. They should minimize the intra-class variation of expressions while maximizing the inter-class variation </span><a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>. Minimizing the intra-class variation of expressions is a problem because images of different individuals with the same expression are far from each other in the pixel's space. Maximizing the inter-class variation is also difficult because images of the same person in different expressions may be very close to one another in the pixel's space <a class="anchor anchor-primary" href="#bib29" name="bbib29" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib29"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a>. At the end of the whole process, a classifier (or a set of classifiers, with one for each expression) is used to infer the facial expression, given the selected features.</div><div class="u-margin-s-bottom" id="p0060"><span>One of the techniques that has been successfully applied to the facial expression recognition problem was the deep multi-layer <a href="/topics/chemical-engineering/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> </span><a class="anchor anchor-primary" href="#bib30" name="bbib30" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib30"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a>, <a class="anchor anchor-primary" href="#bib31" name="bbib31" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib31"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a>, <a class="anchor anchor-primary" href="#bib32" name="bbib32" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib32"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a>, <a class="anchor anchor-primary" href="#bib14" name="bbib14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib14"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>, <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>, <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a><span>. This technique comprises the three steps of facial expression recognition (learning and selection of features and classification) in one single step. In the last decade, <a href="/topics/computer-science/neural-network-research" title="Learn more about neural network researches from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network researches</a><span> were motivated to find a way to train deep multi-layer <a href="/topics/computer-science/neural-network" title="Learn more about neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural networks</a> (i.e. networks with more than one or two hidden layers) in order to increase their accuracy </span></span><a class="anchor anchor-primary" href="#bib33" name="bbib33" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib33"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a>, <a class="anchor anchor-primary" href="#bib34" name="bbib34" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib34"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a>. According to Bengio <a class="anchor anchor-primary" href="#bib35" name="bbib35" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib35"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a><span>, until 2006, many new attempts have shown little success. Although somewhat old, the <a href="/topics/computer-science/convolutional-neural-network" title="Learn more about Convolutional Neural Networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">Convolutional Neural Networks</a> (CNNs) proposed in 1998 by Lecun et al. </span><a class="anchor anchor-primary" href="#bib36" name="bbib36" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib36"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a><span> has shown to be very effective in learning features with a high level of abstraction when using deeper architectures (i.e. with a lot of layers) and new training techniques. In general, this type of hierarchical network has alternating types of layers, including <a href="/topics/engineering/convolutional-layer" title="Learn more about convolutional layers from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional layers</a>, sub-sampling layers and fully connected layers. Convolutional layers are characterized by the kernel's size and the number of generated maps. The kernel is shifted over the valid region of the input image generating one map. Sub-sampling layers are used to increase the position invariance of the kernels by reducing the map size </span><a class="anchor anchor-primary" href="#bib37" name="bbib37" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib37"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a><span>. The main types of sub-sampling layers are maximum-pooling and <a href="/topics/computer-science/average-pooling" title="Learn more about average pooling from ScienceDirect's AI-generated Topic Pages" class="topic-link">average pooling</a> </span><a class="anchor anchor-primary" href="#bib37" name="bbib37" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib37"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a><span><span>. Fully connected layers on CNN's are similar to the ones in general neural networks, its neurons are fully connected with the previous layer (generally: <a href="/topics/computer-science/convolution-layer" title="Learn more about convolution layer from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution layer</a>, sub-sampling layer or even a fully connected layer). The learning procedure of CNNs consists of finding the best synapsesâ€™ weights. Supervised learning can be performed using a </span><a href="/topics/computer-science/gradient-descent-method" title="Learn more about gradient descent method from ScienceDirect's AI-generated Topic Pages" class="topic-link">gradient descent method</a>, like the one proposed by Lecun et al. </span><a class="anchor anchor-primary" href="#bib36" name="bbib36" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib36"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a>. One of the main advantages of CNN, is that the models' input is a raw image rather than a set of hand-coded features.</div><div class="u-margin-s-bottom" id="p0065">Besides the methods using deep architecture, there are many others in the literature, but some aspects of the evaluation of these methods still deserve attention. For example, validation methods could be improved in <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#bib30" name="bbib30" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib30"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a>, <a class="anchor anchor-primary" href="#bib38" name="bbib38" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib38"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib39" name="bbib39" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib39"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>, <a class="anchor anchor-primary" href="#bib40" name="bbib40" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib40"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a>, <a class="anchor anchor-primary" href="#bib41" name="bbib41" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib41"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a>, <a class="anchor anchor-primary" href="#bib42" name="bbib42" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib42"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a> in order to consider situations where the subject in the test set is not in the training set (i.e. test without subject overlap), accuracy is somewhat low in <a class="anchor anchor-primary" href="#bib38" name="bbib38" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib38"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, <a class="anchor anchor-primary" href="#bib1" name="bbib1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib1"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a>, <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>, <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>, and the recognition time in <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>, <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a> could be improved so as to perform real time evaluations.</div><div class="u-margin-s-bottom" id="p0070"><span>Trying to cope with some of these limitations while keeping a simple solution, in this paper, we present a <a href="/topics/chemical-engineering/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> approach combining standard methods, like image normalizations, synthetic training-samples generation (for example, real images with artificial rotations, translation and scaling) and Convolutional Neural Network, into a simple solution that is able to achieve a very high accuracy rate of 96.76% in the CK+ database for 6 expressions, which is the state-of -the-art. The training time is significantly smaller if compared with other methods in the literature and the whole facial expression recognition system can operate in real time in standard computers. We have examined the performance of our system using the Extensive Cohnâ€“Kanade (CK+) database </span><a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, the Japanese Female Facial Expression (JAFFE) database <a class="anchor anchor-primary" href="#bib5" name="bbib5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib5"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a> and the Binghamton University 3D Facial Expression (BU-3DFE) database <a class="anchor anchor-primary" href="#bib6" name="bbib6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib6"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a><span>, achieving a better accuracy in the CK+ database, which contains more samples (important for <a href="/topics/computer-science/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> techniques) than the JAFFE and BU-3DFE databases. In addition, we have performed an extensive validation, with cross-database tests (i.e. training the method using one database and evaluating its accuracy using another one). In summary, the main contributions of this work are:</span><ul class="list"><li class="react-xocs-list-item"><span class="list-label">i.</span><span><div class="u-margin-s-bottom" id="p0075">an efficient method for facial expression recognition that operates in real time;</div></span></li><li class="react-xocs-list-item"><span class="list-label">ii.</span><span><div class="u-margin-s-bottom" id="p0080">a study of the effects of image pre-processing operations in the facial expression recognition problem;</div></span></li><li class="react-xocs-list-item"><span class="list-label">iii.</span><span><div class="u-margin-s-bottom" id="p0085">a set of pre-processing operations for face normalization (spatial and intensity) in order to decrease the need of controlled environments and to cope with the lack of data;</div></span></li><li class="react-xocs-list-item"><span class="list-label">iv.</span><span><div class="u-margin-s-bottom" id="p0090">a study to handle the variability in the accuracy caused by the presentation order of the samples during training; and</div></span></li><li class="react-xocs-list-item"><span class="list-label">v.</span><span><div class="u-margin-s-bottom" id="p0095">a study of the performance of the proposed system with different cultures and environments (cross-database evaluation).</div></span></li></ul></div><div class="u-margin-s-bottom" id="p0100">This work extends the one presented in the 28th SIBGRAPI (Conference on Graphics, Patterns and Images) <a class="anchor anchor-primary" href="#bib45" name="bbib45" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib45"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a> as follows:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">i.</span><span><div class="u-margin-s-bottom" id="p0105">it presents a deeper literature review which were used to enlarge the comparisons of the results;</div></span></li><li class="react-xocs-list-item"><span class="list-label">ii.</span><span><div class="u-margin-s-bottom" id="p0110">it presents the results using a new implementation based on a different framework (from ConvNet, a Matlab based implementation <a class="anchor anchor-primary" href="#bib46" name="bbib46" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib46"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a>, to Caffe, a C++ based implementation <a class="anchor anchor-primary" href="#bib47" name="bbib47" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib47"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a>), which consequently reduced the total training time by almost a factor of four;</div></span></li><li class="react-xocs-list-item"><span class="list-label">iii.</span><span><div class="u-margin-s-bottom" id="p0115">it presents results showing a reduced recognition time, which is now real time;</div></span></li><li class="react-xocs-list-item"><span class="list-label">iv.</span><span><div class="u-margin-s-bottom" id="p0120">it presents results showing better accuracy due to longer training and small changes (see below) in the method;</div></span></li><li class="react-xocs-list-item"><span class="list-label">v.</span><span><div class="u-margin-s-bottom" id="p0125">it includes improved experimental methodology, as for example, using training, validation and test sets, instead of training and test sets only; and</div></span></li><li class="react-xocs-list-item"><span class="list-label">vi.</span><span><div class="u-margin-s-bottom" id="p0130">it presents a more complete evaluation including cross-database tests.</div></span></li></ul>The changes in the method that allowed better accuracy were as follows:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">a.</span><span><div class="u-margin-s-bottom" id="p0135">The synthetic samples have a slight different generation process, allowing larger variation among them (now synthetic samples can be the original image rotated, scaled or translated, instead of only rotated);</div></span></li><li class="react-xocs-list-item"><span class="list-label">b.</span><span><div class="u-margin-s-bottom" id="p0140">We increased the number of synthetic samples, from 30 to 70 (motivated by the previous item); and</div></span></li><li class="react-xocs-list-item"><span class="list-label">c.</span><span><div class="u-margin-s-bottom" id="p0145"><span>The <a href="/topics/computer-science/logistic-regression" title="Learn more about logistic regression from ScienceDirect's AI-generated Topic Pages" class="topic-link">logistic regression</a> loss function was replaced by a SoftmaxWithLoss function (described in </span><a class="anchor anchor-primary" href="#s0015" name="bs0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0015"><span class="anchor-text-container"><span class="anchor-text">Section 3</span></span></a>).</div></span></li></ul></div><div class="u-margin-s-bottom" id="p0150">The remainder of this paper is organized as follows: the next section presents the most recent related work, while <a class="anchor anchor-primary" href="#s0015" name="bs0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0015"><span class="anchor-text-container"><span class="anchor-text">Section 3</span></span></a> describes the proposed approach. In <a class="anchor anchor-primary" href="#s0050" name="bs0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0050"><span class="anchor-text-container"><span class="anchor-text">Section 4</span></span></a>, the experiments we have performed to evaluate our system are presented and compared with several recent facial expression recognition methods. Finally, we conclude in <a class="anchor anchor-primary" href="#s0085" name="bs0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0085"><span class="anchor-text-container"><span class="anchor-text">Section 5</span></span></a>.</div></section><section id="s0010"><h2 id="sect0025" class="u-h4 u-margin-l-top u-margin-xs-bottom">2. Related work</h2><div class="u-margin-s-bottom" id="p0155"><span>Several facial expression recognition approaches were developed in the last decades with an increasing progress in recognition performance. An important part of this recent progress was achieved thanks to the emergence of <a href="/topics/chemical-engineering/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> methods </span><a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a> and more specifically with Convolutional Neural Networks <a class="anchor anchor-primary" href="#bib14" name="bbib14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib14"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>, <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a><span><span>, which is one of the <a href="/topics/computer-science/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> approaches. These approaches became feasible due to: the larger amount of data available nowadays to train learning methods and the advances in </span><a href="/topics/engineering/graphics-processing-unit" title="Learn more about GPU from ScienceDirect's AI-generated Topic Pages" class="topic-link">GPU</a><span> technology. The former is crucial for training networks with deep architectures, whereas the latter is crucial for the low cost high-performance <a href="/topics/computer-science/numerical-computation" title="Learn more about numerical computations from ScienceDirect's AI-generated Topic Pages" class="topic-link">numerical computations</a> required for the training procedure. Surveys of the facial expression recognition research can be found in </span></span><a class="anchor anchor-primary" href="#bib3" name="bbib3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib3"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>, <a class="anchor anchor-primary" href="#bib48" name="bbib48" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib48"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0160">Some recent approaches for facial expression recognition have focused on uncontrolled environments (e.g. not frontal face, images partially overlapped, spontaneous expressions and others), which is still a challenging problem <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a>, <a class="anchor anchor-primary" href="#bib49" name="bbib49" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib49"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a>, <a class="anchor anchor-primary" href="#bib50" name="bbib50" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib50"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a>. This work will focus on more controlled environments and evaluation among different ethnic groups, the latter is a more challenging scenario in facial expression recognition. This section discusses recent methods that achieve high accuracy in facial expression recognition using a comparable experimental methodology (as explained in <a class="anchor anchor-primary" href="#s0050" name="bs0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0050"><span class="anchor-text-container"><span class="anchor-text">Section 4</span></span></a><span>) or methods that are based on <a href="/topics/chemical-engineering/deep-neural-network" title="Learn more about deep neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep neural networks</a>.</span></div><div class="u-margin-s-bottom" id="p0165">Liu et al. <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a><span><span> proposed a novel approach called boosted <a href="/topics/engineering/deep-belief-network" title="Learn more about deep belief network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep belief network</a> (BDBN). The BDBN is composed by a set of classifiers, named by the authors as </span><a href="/topics/computer-science/weak-classifier" title="Learn more about weak classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">weak classifiers</a>. Each weak classifier is responsible for classifying one expression. Their approach performs the three learning stages (feature learning, feature selection and classifier construction) iteratively in a unique framework. Their experiments were conducted using two public databases of static images, Cohnâ€“Kanade </span><a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a> and JAFFE <a class="anchor anchor-primary" href="#bib51" name="bbib51" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib51"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a><span>, and achieved an accuracy of 96.7% and 91.8%, respectively. They also performed experiments on less controlled scenarios, using a cross-database configuration (training with the CK+ and testing in JAFFE) and achieved an accuracy of 68.0%. All images were firstly preprocessed based on the given eye coordinates, i.e. performing alignment and crop. The training and the test adopted a one-versus-all classification strategy, i.e. they used a <a href="/topics/computer-science/binary-classifier" title="Learn more about binary classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary classifier</a> for each expression. The time required to train the network was about 8 days. The recognition was calculated as a function of the weak classifiers. In their method, they use six or seven classifiers, depending on the amount of expressions to be recognized (one for each expression). Each classifier took 30</span>&nbsp;ms to recognize each expression, with a total recognition time of about 0.21&nbsp;s. The recognition time was reported by the authors using a 6-core 2.4&nbsp;GHz PC.</div><div class="u-margin-s-bottom" id="p0170">Song et al. <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a><span><span><span>, developed a facial expression recognition system that uses a <a href="/topics/computer-science/deep-convolutional-neural-networks" title="Learn more about deep Convolutional Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep Convolutional Neural Network</a> and runs on a smartphone. The proposed network is composed of five layers and 65,000 neurons. According to the authors, it is common to have an overfitting when using a small amount of </span><a href="/topics/computer-science/training-data" title="Learn more about training data from ScienceDirect's AI-generated Topic Pages" class="topic-link">training data</a> and such a big network. Therefore, the authors applied </span><a href="/topics/computer-science/data-augmentation" title="Learn more about data augmentation from ScienceDirect's AI-generated Topic Pages" class="topic-link">data augmentation</a> techniques to increase the amount of training data and used the drop-out </span><a class="anchor anchor-primary" href="#bib52" name="bbib52" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib52"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a> during the network training. The experiments were performed using the CK+ <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a> dataset, and other three datasets created by the authors. The images of the CK+ dataset were firstly cropped to focus on regions that contains facial changes caused by an expression. The experiments performed by the authors follow a 10-fold cross validation, but they do not mention if there were images of the same subject in more than one fold. Therefore, we assumed that there was an overlap among subjects in the training and test sets. An accuracy of 99.2% was achieved in the CK+ database while recognizing only five expressions (anger, happy, sad, surprise and neutral).</div><div class="u-margin-s-bottom" id="p0175">Burkert et al. <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a><span><span> also proposed a method based on Convolutional Neural Networks. The authors claim that their method is independent of any hand-crafted feature extraction (i.e. uses the raw image as input). Their <a href="/topics/computer-science/network-architecture" title="Learn more about network architecture from ScienceDirect's AI-generated Topic Pages" class="topic-link">network architecture</a> consists of four parts. The first part is responsible for the automatic </span><a href="/topics/computer-science/data-preprocessing" title="Learn more about data preprocessing from ScienceDirect's AI-generated Topic Pages" class="topic-link">data preprocessing</a>, while the remaining parts carried out the feature extraction process. The extracted features are classified into a given expression by a fully connected layer at the end of the network. The proposed architecture comprises 15 layers (7 convolutions, 5 poolings, 2 concatenations and 1 normalization layer). They evaluated their method with the CK+ database and the MMI database, achieving an accuracy of 99.6% and 98.63%, respectively. Despite the high accuracy, in their experimental methodology they did not guarantee that subjects used in training were not used in test. As will be discussed in </span><a class="anchor anchor-primary" href="#s0050" name="bs0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0050"><span class="anchor-text-container"><span class="anchor-text">Section 4</span></span></a>, this is an important restriction that should be enforced in order to perform a fair evaluation of facial expression recognition methods <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>, <a class="anchor anchor-primary" href="#bib53" name="bbib53" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib53"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0180">Liu et al. <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a> proposed an action unit (AU) inspired deep networks (AUDN) in order to explore a psychological theory that expressions can be decomposed into multiple facial expression action units. The authors claim that the method is able to learn: (i) informative local appearance variation; (ii) an optimal way to combine local variations; (iii) and a high-level representation for the final expression recognition. Experiments were performed in the CK+ <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, MMI <a class="anchor anchor-primary" href="#bib54" name="bbib54" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib54"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a> and SFEW <a class="anchor anchor-primary" href="#bib55" name="bbib55" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib55"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a> datasets. The latter contains images captured from various movies under uncontrolled scenarios, representing the real-world environment. Experiments were performed using a cross-validation approach without subject overlap between training and test groups, and evaluating the six basic expressions. The method achieves an accuracy of 93.70% in the CK+ database, 75.85% in the MMI database and 30.14% in SFEW database.</div><div class="u-margin-s-bottom" id="p0185">Ali et al. <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a><span> proposed a collection of boosted <a href="/topics/computer-science/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> ensembles for multiethnic facial expression recognition. The proposed model is composed by three main steps: firstly a set of binary neural networks are trained, secondly the predictions of these neural networks are combined to compose the ensemble's collection and finally these collections are used to detect the presence of an expression. The multicultural facial expression database was created by the authors with images from three different databases, which contains images from Japanese (JAFFE), Taiwanese (TFEID), Caucasians (RaFD), and Moroccans subjects. The authors reported the result of recognizing five expressions (anger, happy, sad, surprise and fear) in two different experimental approaches. In the first, they trained and evaluated the system in the multicultural database achieving an accuracy of 93.75%. The second experiment was performed to evaluate the proposed method in a less controlled environment. The method was trained with two databases (TFEID and RaFD) and evaluated in the JAFFE database, achieving an accuracy of 48.67%.</span></div><div class="u-margin-s-bottom" id="p0190">Shan et al. <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a><span> performed a study using <a href="/topics/engineering/local-binary-pattern" title="Learn more about local binary patterns from ScienceDirect's AI-generated Topic Pages" class="topic-link">local binary patterns</a><span><span> (LBP) as feature extractor. They combined and compared different <a href="/topics/engineering/machine-learning-technique" title="Learn more about machine learning techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning techniques</a> like </span><a href="/topics/computer-science/template-matching" title="Learn more about template matching from ScienceDirect's AI-generated Topic Pages" class="topic-link">template matching</a><span>, <a href="/topics/computer-science/support-vector-machine" title="Learn more about support vector machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">support vector machine</a><span><span> (SVM), <a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about linear discriminant analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear discriminant analysis</a><span> and linear programming to recognize facial expressions. The authors also conducted a study to analyze the impact of image resolution in the accuracy result and concluded that methods based on <a href="/topics/engineering/geometric-feature" title="Learn more about geometric features from ScienceDirect's AI-generated Topic Pages" class="topic-link">geometric features</a><span> do not handle low-resolution images very well, whereas those based on appearance, like <a href="/topics/engineering/gabor-wavelet" title="Learn more about Gabor wavelets from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gabor wavelets</a> and LBP, are not so sensitive to the image resolution. The best result achieved in their work was an accuracy of 95.1% using </span></span></span><a href="/topics/engineering/support-vector-machine" title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class="topic-link">SVM</a> and LBP in the CK+ database. Using a cross-database validation (training with the CK+ and testing with JAFFE) to evaluate the proposed system in a less controlled scenario, the authors achieved an accuracy of 41.3%. The images were firstly cropped using the eye positions. The experimental setup used was a 10-fold cross validation scheme without subject overlap. The training and the recognition times were not mentioned by the authors.</span></span></span></span></div><div class="u-margin-s-bottom" id="p0195">A video-based facial expression recognition system was proposed by Byeon and&nbsp;Kwak <a class="anchor anchor-primary" href="#bib14" name="bbib14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib14"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a><span>. They developed a 3D-CNN having an image sequence (from neutral to final expression) using 5 successive frames as 3D input. Therefore, the <a href="/topics/engineering/neural-network-input" title="Learn more about CNN input from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN input</a> is </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>H</mi><mo is=&quot;true&quot;>*</mo><mi is=&quot;true&quot;>W</mi><mo is=&quot;true&quot;>*</mo><mn is=&quot;true&quot;>5</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.051ex" height="2.202ex" viewBox="0 -846.5 4327.4 947.9" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-48"></use></g><g is="true" transform="translate(1110,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(1833,0)"><use xlink:href="#MJMATHI-57"></use></g><g is="true" transform="translate(3104,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(3826,0)"><use xlink:href="#MJMAIN-35"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">H</mi><mo is="true">*</mo><mi is="true">W</mi><mo is="true">*</mo><mn is="true">5</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-1"><math><mrow is="true"><mi is="true">H</mi><mo is="true">*</mo><mi is="true">W</mi><mo is="true">*</mo><mn is="true">5</mn></mrow></math></script></span> (where <em>H</em> and <em>W</em><span> are the image height and width, respectively, and 5 is the number of frames). The authors claim that the <a href="/topics/computer-science/3d-convolutional-neural-networks" title="Learn more about 3D CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D CNN</a> method can handle some degrees of shift and deformation invariance. With this approach, they achieved an accuracy of 95%, but the method relies on a sequence containing the full movement from the neutral to the expression. The experiments were carried out with 10 persons only on a non-usual dataset. The training and recognition times were not mentioned by the authors.</span></div><div class="u-margin-s-bottom" id="p0200">Another video-based approach, proposed by Fan and Tjahjadi <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a><span>, used a spatialâ€“temporal framework based on histogram of gradients and optical flow. Their method comprises three phases: pre-processing, feature extraction and classification. In the pre-processing phase, the detection of facial landmarks was performed and a face alignment was carried out (in order to reduce variations in the head pose). In the feature extraction phase, a framework that integrates dynamic information extracted from the variation in the facial shape caused by the expressions was employed. In the last phase, the classification, a SVM classifier with a <a href="/topics/computer-science/radial-basis-function" title="Learn more about RBF from ScienceDirect's AI-generated Topic Pages" class="topic-link">RBF</a> kernel was used. The experiments were carried out using the CK+ and the MMI databases. The accuracy achieved by the authors in the CK+ database for seven expressions was 83.7% and in the MMI database was 74.3%. The training time was not mentioned, while the recognition time was about 350</span>&nbsp;ms per image in the CK+ database and 520&nbsp;ms in the MMI database.</div><div class="u-margin-s-bottom" id="p0205">In comparison with the methods above, this work: presents a higher accuracy in the CK+ and JAFFE databases (including the cross-database validation) and a smaller training and evaluation time than Liu et al. <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a>, Shan et al. <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a> and Fan and Tjahjadi <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>; a more robust evaluation methodology (without subject overlap between training and test) than Song et al. <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a> and Bukert et al. <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>; recognition of six and seven expression, instead of only five or six as done by Song et al. <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a>, Bukert et al. <a class="anchor anchor-primary" href="#bib11" name="bbib11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib11"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a> and Ali et al. <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a><span>; validates the proposed method on three largely used databases, to allow for a <a href="/topics/computer-science/fair-comparison" title="Learn more about fair comparison from ScienceDirect's AI-generated Topic Pages" class="topic-link">fair comparison</a> with other methods in the literature, instead of using unusual non-public databases like Byeon and Kwak </span><a class="anchor anchor-primary" href="#bib14" name="bbib14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib14"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>. Many of the works mentioned here present a very high accuracy that cannot be fairly compared with our method because they allow for subject overlap in the training and test sets. Preliminary experiments performed with our method considering such overlapping scenarios also showed accuracies closer to 100% without much effort.</div></section><section id="s0015"><h2 id="sect0030" class="u-h4 u-margin-l-top u-margin-xs-bottom">3. Facial expression recognition system</h2><div class="u-margin-s-bottom" id="p0210"><span>Our system for facial expression recognition performs the three learning stages in just one classifier (CNN). The proposed system operates in two main phases: training and test. During training, the system receives a <a href="/topics/computer-science/training-data" title="Learn more about training data from ScienceDirect's AI-generated Topic Pages" class="topic-link">training data</a> comprising </span><a href="/topics/engineering/grayscale-image" title="Learn more about grayscale images from ScienceDirect's AI-generated Topic Pages" class="topic-link">grayscale images</a> of faces with their respective expression id and eye center locations and learns a set of weights for the network. To ensure that the training performance is not affected by the order of presentation of the examples, a few images are separated as validation and are used to choose the final best set of weights out of a set of trainings performed with samples presented in different orders. During test, the system receives a grayscale image of a face along with its respective eye center locations, and outputs the predicted expression by using the final network weights learned during training.</div><div class="u-margin-s-bottom"><div id="p0215">An overview of the system is illustrated in&nbsp;<a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig. 2</span></span></a><span>. In the training phase, new images are synthetically generated to increase the database size. After that, a rotation correction is carried out to align the eyes with the horizontal axis. Subsequently, the image is cropped to remove <a href="/topics/computer-science/background-information" title="Learn more about background information from ScienceDirect's AI-generated Topic Pages" class="topic-link">background information</a><span> and to keep only expression specific features. A down-sampling procedure is carried out to get the features in different images in the same location. Thereafter, the image intensity is normalized. The normalized images are used to train the Convolutional Neural Network. The output of the training phase is the set of weights of the round that achieved the best result with the validation data after a few training rounds considering data in different orders. The testing phase use the same methodology as the training phase: spatial normalization, cropping, down-sampling and intensity normalization. Its output is a single number â€“ the id â€“ of one of the six basic expressions. The expressions are represented as <a href="/topics/engineering/integer-number" title="Learn more about integer numbers from ScienceDirect's AI-generated Topic Pages" class="topic-link">integer numbers</a> (0 â€“ angry, 1 â€“ disgust, 2 â€“ fear, 3 â€“ happy, 4 â€“ sad and 5 â€“ surprise).</span></span></div><figure class="figure text-xs" id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr2.jpg" height="516" alt="Fig.&nbsp;2" aria-describedby="cap0010"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (682KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (682KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0010"><p id="sp0010"><span class="label">Fig.&nbsp;2</span>. Overview of the proposed facial expression recognition system. The system operates in two main phases: training and test. The training phase takes as input a set of images with a face, its eyes locations and expression id, and outputs the set of weights of the round that achieved the best result with the validation data after a few training rounds considering the data in different orders. The test phase receives the weight set from the learning step and a face image with its eyes locations, and outputs the expression id of the image.</p></span></span></figure></div><section id="s0020"><h3 id="sect0035" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.1. Synthetic sample generation</h3><div class="u-margin-s-bottom" id="p0220">Unfortunately, the spatial normalization employed is not enough to ensure that all faces will have the eyes correctly aligned due to imperfections in the eye detection procedure. Fortunately, CNN's are very good at learning transformation invariant functions (i.e. they can handle distorted images <a class="anchor anchor-primary" href="#bib56" name="bbib56" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib56"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a>). However, one of the main problems of deep learning methods is that they need a lot of data in the training phase to perform this task properly <a class="anchor anchor-primary" href="#bib56" name="bbib56" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib56"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a>. Unfortunately, the amount of data available in public datasets is not enough to achieve such behavior in our application.</div><div class="u-margin-s-bottom" id="p0225">To address this problem Simard et al. <a class="anchor anchor-primary" href="#bib56" name="bbib56" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib56"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a><span> proposed the generation of <a href="/topics/computer-science/synthetic-image" title="Learn more about synthetic images from ScienceDirect's AI-generated Topic Pages" class="topic-link">synthetic images</a><span> (i.e. real images with artificial rotations, translations and skewing) to increase the database, this process is referred as <a href="/topics/computer-science/data-augmentation" title="Learn more about data augmentation from ScienceDirect's AI-generated Topic Pages" class="topic-link">data augmentation</a><span>. The authors show the benefits of applying combinations of translations, rotations and skewing for increasing the database. Following this idea, in this paper, we use a 2D <a href="/topics/earth-and-planetary-sciences/normal-density-functions" title="Learn more about Gaussian distribution from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian distribution</a> (</span></span></span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>3</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.591ex" height="1.971ex" viewBox="0 -747.2 2407.1 848.5" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3C3"></use></g><g is="true" transform="translate(850,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1906,0)"><use xlink:href="#MJMAIN-33"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-2"><math><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></script></span> pixels and <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>0</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.663ex" height="2.432ex" viewBox="0 -747.2 2438.1 1047.3" role="img" focusable="false" style="vertical-align: -0.697ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3BC"></use></g><g is="true" transform="translate(881,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1937,0)"><use xlink:href="#MJMAIN-30"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Î¼</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-3"><math><mrow is="true"><mi is="true">Î¼</mi><mo is="true">=</mo><mn is="true">0</mn></mrow></math></script></span><span>) to introduce random noise in the locations of the center of the eyes. Synthetic images are generated by considering the normalized versions of noisy eye locations. For each image, 70 additional <a href="/topics/engineering/synthetic-image" title="Learn more about synthetic images from ScienceDirect's AI-generated Topic Pages" class="topic-link">synthetic images</a> were generated.</span></div><div class="u-margin-s-bottom"><div id="p0230">As it can be seen in <a class="anchor anchor-primary" href="#f0015" name="bf0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0015"><span class="anchor-text-container"><span class="anchor-text">Fig. 3</span></span></a><span>, points are generated for both eyes following a Gaussian distribution centered in their original location. The new eye center position is therefore equivalent to the original one but disturbed by a <a href="/topics/computer-science/gaussian-white-noise" title="Learn more about Gaussian noise from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian noise</a>. As the new values given to the normalization procedure are not the real eye center, the resulting images will be either disturbed by a translation, a rotation and/or a scale, or not disturbed at all. The specific value of the Gaussian standard deviation needs to be carefully chosen. A very small deviation could cause no variation in the original data and generate a lot of useless equal images. On the other hand, a big deviation for each eye could introduce too much translation, rotation and/or scale noise in the images making the scenario more complex for the classifier to learn the expression features. A standard deviation of </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>3</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.591ex" height="1.971ex" viewBox="0 -747.2 2407.1 848.5" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3C3"></use></g><g is="true" transform="translate(850,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1906,0)"><use xlink:href="#MJMAIN-33"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-4"><math><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></script></span><span> pixels was empirically chosen. It is important to note that the <a href="/topics/computer-science/synthetic-data" title="Learn more about synthetic data from ScienceDirect's AI-generated Topic Pages" class="topic-link">synthetic data</a> is only used in the training.</span></div><figure class="figure text-xs" id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr3.jpg" height="935" alt="Fig.&nbsp;3" aria-describedby="cap0015"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (776KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (776KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0015"><p id="sp0015"><span class="label">Fig.&nbsp;3</span>. Illustration of the synthetic sample generation. The Gaussian synthetic sample generation procedure increases the database size and variation, adding a noise (Gaussian with <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>3</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.591ex" height="1.971ex" viewBox="0 -747.2 2407.1 848.5" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3C3"></use></g><g is="true" transform="translate(850,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1906,0)"><use xlink:href="#MJMAIN-33"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-5"><math><mrow is="true"><mi is="true">Ïƒ</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></script></span> pixels) in the images considering a controlled environment. The new images are generated using the normalization step and the new eye points. The green crosses are the original eye points, while the red ones are the synthetic points. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</p></span></span></figure></div></section><section id="s0025"><h3 id="sect0040" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.2. Rotation correction</h3><div class="u-margin-s-bottom" id="p0235"><span>The images in the databases, and also in real environments, vary in rotation, brightness and size even for images of the same subject. These variations are not related to the face expression and can affect the accuracy rate of the system. To address this problem, the face region is aligned (with rotation normalization) with the horizon and a center point in order to correct possible geometric issues like rotations and translations. To perform this alignment two information are needed, the <a href="/topics/computer-science/facial-image" title="Learn more about facial image from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial image</a> and the center of both eyes. There are already a lot of methods in the literature that are able to find the eyes, and the others facial points, with high precision </span><a class="anchor anchor-primary" href="#bib57" name="bbib57" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib57"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a>, <a class="anchor anchor-primary" href="#bib58" name="bbib58" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib58"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a>, <a class="anchor anchor-primary" href="#bib59" name="bbib59" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib59"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a>, <a class="anchor anchor-primary" href="#bib60" name="bbib60" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib60"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a>, <a class="anchor anchor-primary" href="#bib61" name="bbib61" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib61"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a>, and this is not the focus of this work. Cheng et al. <a class="anchor anchor-primary" href="#bib61" name="bbib61" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib61"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a> developed a CUDA version of DRMF <a class="anchor anchor-primary" href="#bib60" name="bbib60" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib60"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a><span>, allowing for real-time facial <a href="/topics/engineering/keypoints" title="Learn more about keypoints from ScienceDirect's AI-generated Topic Pages" class="topic-link">keypoints</a> detection, while keeping an accurate detection of these keypoint even with the face partially occluded (a shape RMSE below 0.05 fraction of inter-ocular distance).</span></div><div class="u-margin-s-bottom"><div id="p0240"><span><span>To perform the face alignment, a rotation transformation is applied to align the eyes with the horizontal axis of the image and an affine translation defined by the locations of the eyes to centralize the face in a specific point of the image. The rotation makes the angle formed by the <a href="/topics/engineering/line-segment" title="Learn more about line segment from ScienceDirect's AI-generated Topic Pages" class="topic-link">line segment</a> going from one eye center to the other, and the horizontal axis to be zero. Rotations and translations in the images are not related to the facial expression and therefore should be removed to avoid negatively affecting the accuracy rate of the system. The rotation </span><a href="/topics/engineering/correction-procedure" title="Learn more about correction procedure from ScienceDirect's AI-generated Topic Pages" class="topic-link">correction procedure</a> is shown in </span><a class="anchor anchor-primary" href="#f0020" name="bf0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0020"><span class="anchor-text-container"><span class="anchor-text">Fig. 4</span></span></a>.</div><figure class="figure text-xs" id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr4.jpg" height="193" alt="Fig.&nbsp;4" aria-describedby="cap0020"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (234KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (234KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0020"><p id="sp0020"><span class="label">Fig.&nbsp;4</span>. Rotation correction example. The non-corrected input image (left) is rotated (right) using the <a href="/topics/engineering/line-segment" title="Learn more about line segment from ScienceDirect's AI-generated Topic Pages" class="topic-link">line segment</a> going from one eye center to the other (red line) and the horizontal axis (blue line). The 10Â° is just an example of a possible correction. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0245">The input to this procedure can be an original or a synthetic image. The rotation correction, for the synthetic generated images, may not carry out a perfect align with the horizontal axis because the eye center is the real position disturbed by a random <a href="/topics/engineering/gaussian-white-noise" title="Learn more about Gaussian noise from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian noise</a>. Therefore, it will generate images disturbed by rotations and translations, which increases the variation in the training examples.</div></section><section id="s0030"><h3 id="sect0045" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.3. Image cropping</h3><div class="u-margin-s-bottom"><div id="p0250">As shown in <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig. 2</span></span></a>, the original image has a lot of background information that is not important to the expression classification procedure. This information could decrease the accuracy of the classification because the classifier has one more problem to solve, i.e. discriminating between background and foreground. After the cropping, all image parts that do not have expression specific information are removed. The cropping region also tries to remove facial parts that do not contribute for the expression (e.g. ears, part of the forehead, etc.). Therefore, the region of interest is defined based on a ratio of the inter-eyes distance. Consequently, our method is able to handle different persons and image sizes without human intervention. The cropping region is delimited by a vertical factor of 4.5 (considering 1.3 for the region above the eyes and 3.2 for the region below) applied to the distance between the eyes middle point and the right eye center. The horizontal cropping region is delimited by a factor of 2.4 applied to this same distance. These factor values were determined empirically. An example of this procedure is illustrated in <a class="anchor anchor-primary" href="#f0025" name="bf0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0025"><span class="anchor-text-container"><span class="anchor-text">Fig. 5</span></span></a>.</div><figure class="figure text-xs" id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr5.jpg" height="231" alt="Fig.&nbsp;5" aria-describedby="cap0025"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (326KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (326KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0025"><p id="sp0025"><span class="label">Fig.&nbsp;5</span>. <span>Image cropping example. The <a href="/topics/engineering/spatial-normalization" title="Learn more about spatial normalization from ScienceDirect's AI-generated Topic Pages" class="topic-link">spatial normalization</a> is carried out using half of the inter-eyes distance (</span><em>Î±</em>). The input image (left) is cropped (right) using a horizontal factor (<span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo is=&quot;true&quot;>*</mo><mn is=&quot;true&quot;>2.4</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.654ex" height="2.202ex" viewBox="0 -846.5 2864.9 947.9" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(862,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(1585,0)"><use xlink:href="#MJMAIN-32"></use><use xlink:href="#MJMAIN-2E" x="500" y="0"></use><use xlink:href="#MJMAIN-34" x="779" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Î±</mi><mo is="true">*</mo><mn is="true">2.4</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-6"><math><mrow is="true"><mi is="true">Î±</mi><mo is="true">*</mo><mn is="true">2.4</mn></mrow></math></script></span>) to crop in the horizontal and a vertical factor (<span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B1;</mi><mo is=&quot;true&quot;>*</mo><mn is=&quot;true&quot;>4.5</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.654ex" height="2.202ex" viewBox="0 -846.5 2864.9 947.9" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3B1"></use></g><g is="true" transform="translate(862,0)"><use xlink:href="#MJMAIN-2A"></use></g><g is="true" transform="translate(1585,0)"><use xlink:href="#MJMAIN-34"></use><use xlink:href="#MJMAIN-2E" x="500" y="0"></use><use xlink:href="#MJMAIN-35" x="779" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Î±</mi><mo is="true">*</mo><mn is="true">4.5</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-7"><math><mrow is="true"><mi is="true">Î±</mi><mo is="true">*</mo><mn is="true">4.5</mn></mrow></math></script></span> considering 1.3 for the region above the eyes and 3.2 for the region below) to crop in the vertical. This operation aims to remove all non-expression features, such as background and hair.</p></span></span></figure></div></section><section id="s0035"><h3 id="sect0050" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.4. Down-sampling</h3><div class="u-margin-s-bottom" id="p0255"><span><span>The down-sampling operation is performed to reduce the image size for the network and to ensure scale normalization, i.e. the same location for the face components (eyes, mouth, eyebrow, etc.) in all images. The down-sampling uses a <a href="/topics/computer-science/linear-interpolation" title="Learn more about linear interpolation from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear interpolation</a> approach. After this re-sampling, one can guarantee that the eye center will be approximately in the same position. This procedure helps the CNN to learn which regions are related to each specific expression. The down-sampling also enables the convolutions to be performed in the </span><a href="/topics/engineering/graphics-processing-unit" title="Learn more about GPU from ScienceDirect's AI-generated Topic Pages" class="topic-link">GPU</a><span> since most of the graphics card nowadays have limited memory. The final image is down-sampled, using a <a href="/topics/engineering/linear-interpolation" title="Learn more about linear interpolation from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear interpolation</a>, to 32 </span></span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-8"><math><mo is="true">Ã—</mo></math></script></span>32 pixels.</div></section><section id="s0040"><h3 id="sect0055" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.5. Intensity normalization</h3><div class="u-margin-s-bottom"><div id="p0260"><span>The <a href="/topics/engineering/image-brightness" title="Learn more about image brightness from ScienceDirect's AI-generated Topic Pages" class="topic-link">image brightness</a> and contrast can vary even in images of the same person in the same expression, therefore, increasing the variation in the feature vector. Such variations increase the complexity of the problem that the classifier has to solve for each expression. In order to reduce these issues an intensity normalization was applied. A method adapted from a bio-inspired technique described in </span><a class="anchor anchor-primary" href="#bib62" name="bbib62" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib62"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a><span>, called contrastive equalization, was used. Basically, the normalization is a two step procedure: firstly a subtractive <a href="/topics/computer-science/local-contrast" title="Learn more about local contrast from ScienceDirect's AI-generated Topic Pages" class="topic-link">local contrast</a> normalization is performed; and secondly, a divisive local contrast normalization is applied. In the first step, the value of every pixel is subtracted from a Gaussian-weighted average of its neighbors. In the second step, every pixel is divided by the standard deviation of its neighborhood. The neighborhood for both procedures uses a kernel of 7 </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-9"><math><mo is="true">Ã—</mo></math></script></span>7 pixels (empirically chosen). An example of this procedure is illustrated in <a class="anchor anchor-primary" href="#f0030" name="bf0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0030"><span class="anchor-text-container"><span class="anchor-text">Fig. 6</span></span></a>.</div><figure class="figure text-xs" id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr6.jpg" height="275" alt="Fig.&nbsp;6" aria-describedby="cap0030"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (206KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (206KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0030"><p id="sp0030"><span class="label">Fig.&nbsp;6</span>. Illustration of the intensity normalization. The figure shows the image with the original intensity (left) and its intensity normalized version (right).</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0265">Eq. <a class="anchor anchor-primary" href="#eq0005" name="beq0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="eq0005"><span class="anchor-text-container"><span class="anchor-text">(1)</span></span></a> shows how each new pixel value is calculated in the intensity normalization procedure:<span class="display"><span id="eq0005" class="formula"><span class="label">(1)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msup superscriptshift=&quot;75%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi></mrow><mrow is=&quot;true&quot;><mo accent=&quot;true&quot; is=&quot;true&quot;>&amp;#x2032;</mo></mrow></msup><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi><mo is=&quot;true&quot;>&amp;#x2212;</mo><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nhgx</mi></mrow></msub></mrow></mrow><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nhgx</mi></mrow></msub></mrow></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.909ex" height="6.586ex" viewBox="0 -1641.4 5127.6 2835.7" role="img" focusable="false" style="vertical-align: -2.774ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-78"></use></g></g><g is="true" transform="translate(572,750)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-2032"></use></g></g></g><g is="true" transform="translate(1145,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1923,0)"><g transform="translate(397,0)"><rect stroke="none" width="2686" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,1156)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-78"></use></g><g is="true" transform="translate(404,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2212"></use></g><g is="true" transform="translate(955,0)"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-3BC"></use></g></g><g is="true" transform="translate(426,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-68" x="600" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-67" x="1177" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-78" x="1654" y="0"></use></g></g></g></g></g><g is="true" transform="translate(548,-345)"><g is="true"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-3C3"></use></g></g><g is="true" transform="translate(404,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-68" x="600" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-67" x="1177" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-78" x="1654" y="0"></use></g></g></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msup superscriptshift="75%" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo accent="true" is="true">â€²</mo></mrow></msup><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">âˆ’</mo><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Î¼</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></mrow></mrow><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Ïƒ</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></mrow></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-10"><math><mrow is="true"><msup superscriptshift="75%" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo accent="true" is="true">â€²</mo></mrow></msup><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mi is="true">x</mi><mo is="true">âˆ’</mo><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Î¼</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></mrow></mrow><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Ïƒ</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></mrow></mrow></mfrac></mrow></math></script></span></span></span>where <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup superscriptshift=&quot;75%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>x</mi></mrow><mrow is=&quot;true&quot;><mo accent=&quot;true&quot; is=&quot;true&quot;>&amp;#x2032;</mo></mrow></msup></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.014ex" height="3.125ex" viewBox="0 -1244 867.3 1345.3" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-78"></use></g></g><g is="true" transform="translate(572,750)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-2032"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup superscriptshift="75%" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo accent="true" is="true">â€²</mo></mrow></msup></math></span></span><script type="math/mml" id="MathJax-Element-11"><math><msup superscriptshift="75%" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo accent="true" is="true">â€²</mo></mrow></msup></math></script></span> is the new pixel value, <em>x</em> is the original pixel value, <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3BC;</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nhgx</mi></mrow></msub></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.291ex" height="3.24ex" viewBox="0 -498.8 2278.2 1395" role="img" focusable="false" style="vertical-align: -2.082ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3BC"></use></g></g><g is="true" transform="translate(603,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-68" x="600" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-67" x="1177" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-78" x="1654" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Î¼</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></math></span></span><script type="math/mml" id="MathJax-Element-12"><math><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Î¼</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></math></script></span> is the Gaussian-weighted average of the neighbors of <em>x</em>, and <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3C3;</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nhgx</mi></mrow></msub></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.217ex" height="3.24ex" viewBox="0 -498.8 2246.2 1395" role="img" focusable="false" style="vertical-align: -2.082ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3C3"></use></g></g><g is="true" transform="translate(571,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-68" x="600" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-67" x="1177" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-78" x="1654" y="0"></use></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Ïƒ</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></math></span></span><script type="math/mml" id="MathJax-Element-13"><math><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">Ïƒ</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nhgx</mi></mrow></msub></math></script></span> is the standard deviation of the neighbors of <em>x</em>.</div></section><section id="s0045"><h3 id="sect0060" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.6. Convolutional Neural Network</h3><div class="u-margin-s-bottom"><div id="p0270">The architecture of our Convolutional Neural Network is represented in&nbsp;<a class="anchor anchor-primary" href="#f0035" name="bf0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0035"><span class="anchor-text-container"><span class="anchor-text">Fig. 7</span></span></a><span>. The network receives as input a 32x32 grayscale image and outputs the confidence of each expression. The class with the maximum value is used as the expression in the image. Our <a href="/topics/engineering/neural-network-architecture" title="Learn more about CNN architecture from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN architecture</a><span> comprises 2 convolutional layers, 2 sub-sampling layers and one fully connected layer. The first layer of the CNN is a convolution layer, that applies a <a href="/topics/computer-science/convolution-kernel" title="Learn more about convolution kernel from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution kernel</a> of 5 </span></span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-14"><math><mo is="true">Ã—</mo></math></script></span>5 and outputs 32 images of 28 <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-15"><math><mo is="true">Ã—</mo></math></script></span>28 pixels. This layer is followed by a sub-sampling layer that uses max-pooling (with kernel size 2 <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-16"><math><mo is="true">Ã—</mo></math></script></span>2) to reduce the image to half of its size. Subsequently, a new convolution layer performs 64 convolutions with a 7 <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-17"><math><mo is="true">Ã—</mo></math></script></span>7 kernel to map of the previous layer and is followed by another sub-sampling, again with a 2 <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-18"><math><mo is="true">Ã—</mo></math></script></span>2 kernel. The outputs are given to a fully connected hidden layer that has 256 neurons. Finally, the network has six or seven output nodes (one for each expression that outputs their confidence level) that are fully connected to the previous layer.</div><figure class="figure text-xs" id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr7.jpg" height="170" alt="Fig.&nbsp;7" aria-describedby="cap0035"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr7_lrg.jpg" target="_blank" download="" title="Download high-res image (226KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (226KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr7.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0035"><p id="sp0035"><span class="label">Fig.&nbsp;7</span>. Architecture of the proposed Convolutional Neutral Network. It comprises of five layers: two <a href="/topics/computer-science/convolutional-layer" title="Learn more about convolutional layers from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional layers</a>, two sub-sampling layers and one fully connected layer.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0275">The first layer of the network (a convolution layer) aims to extract elementary visual features, like oriented edges, end-point, corners and shapes in general, like described by Lecun et al.&nbsp;<a class="anchor anchor-primary" href="#bib36" name="bbib36" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib36"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a><span>. In the facial expression recognition problem, the features detected are mainly the shapes, corners and edges of eyes, eyebrow and lips. Once the features are detected, its exact location is not so important, just its <a href="/topics/computer-science/relative-position" title="Learn more about relative position from ScienceDirect's AI-generated Topic Pages" class="topic-link">relative position</a> compared to the other features. For example, the absolute position of the eyebrows is not important, but their distances from the eyes are, because a big distance may indicate, for instance, the surprise expression. This precise position is not only irrelevant but it can also pose a problem, because it can naturally vary for different subjects in the same expression. The second layer (a sub-sampling layer) reduces the spatial resolution of the feature map. According to Lecun et al. </span><a class="anchor anchor-primary" href="#bib36" name="bbib36" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib36"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a><span>, this operation aims to reduce the precision with which the position of the features extracted by the previous layer are encoded in the new map. The next two layers, one convolutional and one sub-sampling, aim to do the same operations that the first ones, but handling features in a lower level, recognizing contextual elements (face elements) instead of simple shapes, edges and corners. The concatenation of sets of convolution and sub-sampling layers achieve a high degree of invariance to <a href="/topics/computer-science/geometric-transformation" title="Learn more about geometric transformation from ScienceDirect's AI-generated Topic Pages" class="topic-link">geometric transformation</a> of the input. The last hidden layer (a fully connected layer) receives the set of features learned and outputs the confidence level of the given features in each one of the considered expressions.</span></div><div class="u-margin-s-bottom" id="p0280"><span><span>This network uses the stochastic <a href="/topics/computer-science/gradient-descent-method" title="Learn more about gradient descent method from ScienceDirect's AI-generated Topic Pages" class="topic-link">gradient descent method</a> to calculate the </span><a href="/topics/engineering/synaptic-weight" title="Learn more about synaptic weights from ScienceDirect's AI-generated Topic Pages" class="topic-link">synaptic weights</a> between the neurons, this method was proposed by Buttou </span><a class="anchor anchor-primary" href="#bib63" name="bbib63" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib63"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a><span>. The <a href="/topics/engineering/initial-value" title="Learn more about initial value from ScienceDirect's AI-generated Topic Pages" class="topic-link">initial value</a> of these synapses for the convolutions and for the fully connected layer are generated using the Xavier filler, proposed by Glorot et al.&nbsp;</span><a class="anchor anchor-primary" href="#bib64" name="bbib64" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib64"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a><span>, that automatically determines the scale of the initialization based on the number of input and output neurons. The loss is calculated using a <a href="/topics/engineering/logistic-function" title="Learn more about logistic function from ScienceDirect's AI-generated Topic Pages" class="topic-link">logistic function</a> of the soft-max output (known as </span><em>SoftmaxWithLoss</em><span>). The <a href="/topics/computer-science/activation-function" title="Learn more about activation function from ScienceDirect's AI-generated Topic Pages" class="topic-link">activation function</a> of the neurons is a ReLu (rectified linear unit), defined as </span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>f</mi><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>z</mi><mo is=&quot;true&quot;>)</mo><mo is=&quot;true&quot;>=</mo><mi is=&quot;true&quot;>max</mi><mo is=&quot;true&quot;>(</mo><mi is=&quot;true&quot;>z</mi><mo is=&quot;true&quot;>,</mo><mn is=&quot;true&quot;>0</mn><mo is=&quot;true&quot;>)</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="16.694ex" height="2.779ex" viewBox="0 -846.5 7187.7 1196.3" role="img" focusable="false" style="vertical-align: -0.812ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-66"></use></g><g is="true" transform="translate(550,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(940,0)"><use xlink:href="#MJMATHI-7A"></use></g><g is="true" transform="translate(1408,0)"><use xlink:href="#MJMAIN-29"></use></g><g is="true" transform="translate(2075,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3132,0)"><use xlink:href="#MJMAIN-6D"></use><use xlink:href="#MJMAIN-61" x="833" y="0"></use><use xlink:href="#MJMAIN-78" x="1334" y="0"></use></g><g is="true" transform="translate(4994,0)"><use xlink:href="#MJMAIN-28"></use></g><g is="true" transform="translate(5384,0)"><use xlink:href="#MJMATHI-7A"></use></g><g is="true" transform="translate(5852,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true" transform="translate(6297,0)"><use xlink:href="#MJMAIN-30"></use></g><g is="true" transform="translate(6798,0)"><use xlink:href="#MJMAIN-29"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">f</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">)</mo><mo is="true">=</mo><mi is="true">max</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-19"><math><mrow is="true"><mi is="true">f</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">)</mo><mo is="true">=</mo><mi is="true">max</mi><mo is="true">(</mo><mi is="true">z</mi><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo></mrow></math></script></span>. The ReLu function generally learns much faster in deep architectures <a class="anchor anchor-primary" href="#bib65" name="bbib65" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib65"><span class="anchor-text-container"><span class="anchor-text">[65]</span></span></a>.</div></section></section><section id="s0050"><h2 id="sect0065" class="u-h4 u-margin-l-top u-margin-xs-bottom">4. Experiments and discussions</h2><div class="u-margin-s-bottom" id="p0285">The experiments were performed using three publicly available databases in the facial expression recognition research field: The Extended Cohnâ€“Kanade (CK+) database <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, the Japanese Female Facial Expressions (JAFFE) database <a class="anchor anchor-primary" href="#bib5" name="bbib5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib5"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a> and the Binghamton University 3D Facial Expression (BU-3DFE) database <a class="anchor anchor-primary" href="#bib6" name="bbib6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib6"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a><span><span>. Accuracy is computed considering one classifier to classify all learned expressions. In addition, to allow for a <a href="/topics/computer-science/fair-comparison" title="Learn more about fair comparison from ScienceDirect's AI-generated Topic Pages" class="topic-link">fair comparison</a> with some methods in the literature, accuracy is also computed considering one </span><a href="/topics/computer-science/binary-classifier" title="Learn more about binary classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary classifier</a> for each expression, as used in </span><a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0290">The implementation of the pre-processing steps was done in-house using OpenCV, C++ and a GPU based CNN library (Caffe <a class="anchor anchor-primary" href="#bib47" name="bbib47" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib47"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a>). All the experiments were carried out using an Intel Core i7 3.4&nbsp;GHz with a NVIDA GeForce GTX 660 CUDA Capable that has 1.5&nbsp;Gb of memory in the GPU. The environment of the experiments was Linux Ubuntu 12.04, with the NVIDIA CUDA Framework 6.5 and the cuDNN library installed. The preprocessing step (rotation correction, cropping, down-sampling and intensity normalization) took only 0.02&nbsp;<span>s and the <a href="/topics/computer-science/recognition-network" title="Learn more about network recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">network recognition</a> (classification step) took in average 0.01</span>&nbsp;s.</div><div class="u-margin-s-bottom" id="p0295">In this section, a study is carried out showing the impact of every normalization step in the accuracy of the method. Firstly, we describe the databases used for the experiments. Secondly, the metrics used to evaluate the system accuracy are explained. Thirdly, the results of the tuning experiments and the influence of each pre-processing step is presented. Fourthly, the results with different databases are shown and discussed in details. Finally, a comparison with several recent facial expression recognition methods that uses the same evaluation methodology is presented and the limitations of our method are discussed.</div><section id="s0055"><h3 id="sect0070" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Database</h3><div class="u-margin-s-bottom"><div id="p0300">The presented system was trained and tested using the CK+ database <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a>, the JAFFE database <a class="anchor anchor-primary" href="#bib5" name="bbib5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib5"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a> and the BU-3DFE database <a class="anchor anchor-primary" href="#bib6" name="bbib6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib6"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a><span><span><span>. The CK+ database comprises 100 university students with age between 18 and 30 years old. The subjects in the database are 65% female, 15% are African-American and 3% are Asian or south American. The images were captured from a camera located directly in front of the subject. The students were instructed to perform a series of expressions. Each sequence begins and ends with the neutral expression. All images in the database are 640 by 480 pixel arrays with 8-bit precision for <a href="/topics/engineering/grayscale-value" title="Learn more about grayscale values from ScienceDirect's AI-generated Topic Pages" class="topic-link">grayscale values</a>. Each image has a </span><a href="/topics/computer-science/file-descriptor" title="Learn more about descriptor file from ScienceDirect's AI-generated Topic Pages" class="topic-link">descriptor file</a> with its facial points, these points were used to normalize the facial expression image. The facial points in the database are coded using the </span><a href="/topics/computer-science/facial-action-coding-system" title="Learn more about facial action coding system from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial action coding system</a> (FACS) </span><a class="anchor anchor-primary" href="#bib66" name="bbib66" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib66"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a><span>. The database creators used the <a href="/topics/computer-science/active-appearance-model" title="Learn more about active appearance models from ScienceDirect's AI-generated Topic Pages" class="topic-link">active appearance models</a> (AAMs) to automatically extract the facial points. The database contains images for the following expressions: neutral, angry, contempt, disgust, fear, happy, sad and surprise. To do a fair comparison with the major part of the recent methods </span><a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>, <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>, <a class="anchor anchor-primary" href="#bib67" name="bbib67" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib67"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, in our experiments the contempt expression images were not used. Some examples of the CK+ database images are shown in <a class="anchor anchor-primary" href="#f0040" name="bf0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0040"><span class="anchor-text-container"><span class="anchor-text">Fig. 8</span></span></a>.</div><figure class="figure text-xs" id="f0040"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr8.jpg" height="175" alt="Fig.&nbsp;8" aria-describedby="cap0040"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr8_lrg.jpg" target="_blank" download="" title="Download high-res image (381KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (381KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr8.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0040"><p id="sp0040"><span class="label">Fig.&nbsp;8</span>. Example of the images in the CK+ database. In (1), the subject is in the neutral expression. In (2), the subject is in the surprise expression. In (3), the subject is in the disgust expression. In (4), the subject is in the fear expression.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0305">To perform a fair evaluation of the proposed method, the database was separated in 8 groups without subject overlap between groups (i.e. if an image of one subject is in one group, no image of the same subject will be in any other group). Each group contains about 12 subjects. This methodology ensures that testing groups do not have subjects from the training group and is also used by many methods in the literature <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>, <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a>, <a class="anchor anchor-primary" href="#bib67" name="bbib67" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib67"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a>, <a class="anchor anchor-primary" href="#bib68" name="bbib68" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib68"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a>, <a class="anchor anchor-primary" href="#bib69" name="bbib69" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib69"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib70" name="bbib70" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib70"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a>. As discussed by Girard et al.&nbsp;<a class="anchor anchor-primary" href="#bib53" name="bbib53" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib53"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a><span>, this methodology (different subjects in training/testing groups and cross-validation) ensures the <a href="/topics/computer-science/generalizability" title="Learn more about generalizability from ScienceDirect's AI-generated Topic Pages" class="topic-link">generalizability</a> of classifiers. Zavaschi et al.&nbsp;</span><a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a> also discuss and support this data separation procedure. They conduct two experiments using the same methodology, just changing the way that the data groups are separated. In one experiment the groups contain images of the same subject (not the same images), while in the other experiment they guarantee that images of the same subject are not in the training and testing groups at same time. In the first experiment an accuracy of 99.40% was achieved, while in the second the accuracy goes down to 88.90%. This result shows that methods which are evaluated without the same subjects in training/testing groups (which we believe to be a fairer evaluation) generally presents a lower accuracy than those that do not guarantee this constraint. We also confirmed these results with preliminary experiments using our method.</div><div class="u-margin-s-bottom"><div id="p0310">To verify the generalization of the proposed method some cross-database experiments were also performed. These experiments used the JAFFE and the BU-3DFE databases. The JAFFE database consists of 213 images from 10 Japanese female subjects. In this database, there are about 4 images in each one of the six basic expressions and one image of the neutral expression from each subject. All images in the dataset are 256 by 256 pixel arrays with 8-bit precision for grayscale values. As this database is smaller, the groups are separated by subject, i.e. each group contains images of just one subject, so 10 groups were formed. Some examples of the JAFFE database images are shown in <a class="anchor anchor-primary" href="#f0045" name="bf0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0045"><span class="anchor-text-container"><span class="anchor-text">Fig. 9</span></span></a>.</div><figure class="figure text-xs" id="f0045"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr9.jpg" height="201" alt="Fig.&nbsp;9" aria-describedby="cap0045"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr9_lrg.jpg" target="_blank" download="" title="Download high-res image (452KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (452KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr9.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0045"><p id="sp0045"><span class="label">Fig.&nbsp;9</span>. Example of the images in the JAFFE database. In (1), the subject is in the surprise expression. In (2), the subject is in the happy expression. In (3), the subject is in the sad expression. In (4), the subject is in the sad expression.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0315">Another database used to evaluate the proposed method is the Binghamton University 3D Facial Expression (BU-3DFE) <a class="anchor anchor-primary" href="#bib6" name="bbib6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib6"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a><span>. The BU-3DFE database contains 64 subjects (56% female and 44% male), ranging age between 18 years to 70 years old, with a variety of ethnic/racial <a href="/topics/earth-and-planetary-sciences/ancestry" title="Learn more about ancestries from ScienceDirect's AI-generated Topic Pages" class="topic-link">ancestries</a>, including White, Black, East-Asian, Middle-east Asian, Indian and Hispanic Latino. All images in the dataset are 156 by 209 pixel arrays. This database was also separated in 8 groups, without subject overlap between groups. Each group contains about 8 subjects. Some examples of the BU-3DFE database images are shown in </span><a class="anchor anchor-primary" href="#f0050" name="bf0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0050"><span class="anchor-text-container"><span class="anchor-text">Fig. 10</span></span></a>.</div><figure class="figure text-xs" id="f0050"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr10.jpg" height="263" alt="Fig.&nbsp;10" aria-describedby="cap0050"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr10_lrg.jpg" target="_blank" download="" title="Download high-res image (339KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (339KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr10.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0050"><p id="sp0050"><span class="label">Fig.&nbsp;10</span>. Example of the images in the BU-3DFE database. In (1) the subject is in the fear expression. In (2) the subject is in the neutral expression. In (3) the subject is in the fear expression. In (4) the subject is in the fear expression.</p></span></span></figure></div><div class="u-margin-s-bottom" id="p0320">All databases contain a lot of images of the same subjects for each expression and these images are very similar to each other. So, only 3 frames of each expression (i.e. most expressive frames) and 1 frame for the neutral expression (i.e. least expressive frame), of each subject, were used in this work. Following this methodology, the resulting database sizes are as follows, for the CK+ database 2100 samples (without the synthetic samples) and 147,000 samples (with the synthetic samples), for the JAFFE database 213 samples (without the synthetic samples) and 14,910 samples (with the synthetic samples) and for the BU-3DFE database 1344 samples (without the synthetic samples) and 94,080 samples (with the synthetic samples).</div></section><section id="s0060"><h3 id="sect0075" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Metrics</h3><div class="u-margin-s-bottom" id="p0325">To allow for a fair comparison of the presented method with the literature, the accuracy was computed in two different ways. In the first, one classifier for all basic expression is used. The accuracy is computed simply using the average, <em>C</em><sub><em>nclass</em></sub>, of the <em>n</em>-class classifier accuracy per expression, <em>C</em><sub><em>nclassE</em></sub>, i.e. number of hits of an expression per amount of data of that expression, see the following equation:<span class="display"><span id="eq0010" class="formula"><span class="label">(2)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nclass</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msubsup subscriptshift=&quot;90%&quot; superscriptshift=&quot;90%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi></mrow></msubsup></mrow><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nclassE</mi></mrow></msub></mrow></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi></mrow></mfrac><mo is=&quot;true&quot;>,</mo><mo is=&quot;true&quot;></mo><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>nclassE</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>Hit</mi></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>E</mi></mrow></msub></mrow></mrow><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>T</mi></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>E</mi></mrow></msub></mrow></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="35.117ex" height="8.317ex" viewBox="0 -2486 15119.6 3580.9" role="img" focusable="false" style="vertical-align: -2.543ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-63" x="600" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="1034" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="1332" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1861" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="2331" y="0"></use></g></g></g><g is="true" transform="translate(3073,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(3852,0)"><g transform="translate(397,0)"><rect stroke="none" width="3751" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,1303)"><g is="true"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g></g><g is="true" transform="translate(747,900)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use></g></g><g is="true" transform="translate(747,-900)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMAIN-31"></use></g></g></g></g><g is="true" transform="translate(1284,0)"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(505,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-63" x="600" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-6C" x="1034" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-61" x="1332" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-73" x="1862" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-73" x="2331" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-45" x="2801" y="0"></use></g></g></g></g></g><g is="true" transform="translate(1663,-345)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g></g></g></g><g is="true" transform="translate(8121,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true"></g><g is="true" transform="translate(8566,0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-63" x="600" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="1034" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="1332" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1861" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="2331" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-45" x="2801" y="0"></use></g></g></g><g is="true" transform="translate(12162,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(12940,0)"><g transform="translate(397,0)"><rect stroke="none" width="1660" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,1053)"><g is="true"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-48"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-69" x="831" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-74" x="1177" y="0"></use></g></g><g is="true" transform="translate(1087,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-45"></use></g></g></g></g></g><g is="true" transform="translate(397,-383)"><g is="true"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g><g is="true" transform="translate(413,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-45"></use></g></g></g></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclass</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msubsup subscriptshift="90%" superscriptshift="90%" is="true"><mrow is="true"><mo is="true">âˆ‘</mo></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></msubsup></mrow><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclassE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">n</mi></mrow></mfrac><mo is="true">,</mo><mo is="true"></mo><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclassE</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub></mrow></mrow><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub></mrow></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-20"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclass</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msubsup subscriptshift="90%" superscriptshift="90%" is="true"><mrow is="true"><mo is="true">âˆ‘</mo></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></msubsup></mrow><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclassE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">n</mi></mrow></mfrac><mo is="true">,</mo><mo is="true"></mo><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">nclassE</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub></mrow></mrow><mrow is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">T</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub></mrow></mrow></mfrac></mrow></math></script></span></span></span>where <em>Hit</em><sub><em>E</em></sub> is the number of hits in the expression <em>E</em>, <em>T</em><sub><em>E</em></sub> is total number of samples of that expression and <em>n</em> is the number of expressions to be considered.</div><div class="u-margin-s-bottom" id="p0330">In the second, one binary classifier for each expression performs a one-versus-all classification, as proposed in <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>. Using this approach, the images are presented to <em>n</em> binary classifiers, where <em>n</em> is the number of expressions being classified. Each classifier aims to answer â€œyesâ€ if the image contains one specific expression, or â€œnoâ€ otherwise. For example, if one image contains the surprise expression, the surprise classifier should answer â€œyesâ€ and all the other five classifiers should answer â€œnoâ€. The only difference for this classifier from the architecture presented in <a class="anchor anchor-primary" href="#s0015" name="bs0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0015"><span class="anchor-text-container"><span class="anchor-text">Section 3</span></span></a> is that only two outputs are required for each classifier. The accuracy is computed using the average, <em>C</em><sub><em>bin</em></sub>, of the binary classifier accuracy per expression, <em>C</em><sub><em>binE</em></sub>, i.e. the number of hits of an expression plus the number of hits of a non-expression divided per total amount of data, see the following equation:<span class="display"><span id="eq0015" class="formula"><span class="label">(3)</span><span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>bin</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><mrow is=&quot;true&quot;><msubsup subscriptshift=&quot;90%&quot; superscriptshift=&quot;90%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mo is=&quot;true&quot;>&amp;#x2211;</mo></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>1</mn></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi></mrow></msubsup></mrow><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>binE</mi></mrow></msub></mrow></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>n</mi></mrow></mfrac><mo is=&quot;true&quot;>,</mo><mspace width=&quot;1.0em&quot; is=&quot;true&quot; /><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>binE</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mfrac is=&quot;true&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>Hit</mi></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>E</mi></mrow></msub><mo is=&quot;true&quot;>+</mo><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>Hit</mi></mrow><mrow is=&quot;true&quot;><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>NE</mi></mrow></msub></mrow></mrow><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>T</mi></mrow></mfrac></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="36.862ex" height="7.509ex" viewBox="0 -2486 15871 3233.2" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-69" x="429" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6E" x="774" y="0"></use></g></g></g><g is="true" transform="translate(2065,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(2844,0)"><g transform="translate(397,0)"><rect stroke="none" width="3038" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,1303)"><g is="true"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJSZ1-2211"></use></g></g><g is="true" transform="translate(747,900)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-6E"></use></g></g><g is="true" transform="translate(747,-900)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMAIN-31"></use></g></g></g></g><g is="true" transform="translate(1284,0)"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(505,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-62"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-69" x="429" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-6E" x="775" y="0"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-45" x="1375" y="0"></use></g></g></g></g></g><g is="true" transform="translate(1306,-345)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-6E"></use></g></g></g></g><g is="true" transform="translate(6400,0)"><use xlink:href="#MJMAIN-2C"></use></g><g is="true"></g><g is="true" transform="translate(7845,0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-62"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-69" x="429" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6E" x="774" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-45" x="1375" y="0"></use></g></g></g><g is="true" transform="translate(10433,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(11212,0)"><g transform="translate(397,0)"><rect stroke="none" width="4140" height="60" x="0" y="220"></rect><g is="true" transform="translate(60,1053)"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-48"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-69" x="831" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-74" x="1177" y="0"></use></g></g><g is="true" transform="translate(1087,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-45"></use></g></g></g><g is="true" transform="translate(1540,0)"><use transform="scale(0.707)" xlink:href="#MJMAIN-2B"></use></g><g is="true" transform="translate(2091,0)"><g is="true"><g is="true"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-48"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-69" x="831" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-74" x="1177" y="0"></use></g></g><g is="true" transform="translate(1087,-650)"><g is="true"><use transform="scale(0.5)" xlink:href="#MJMATHI-4E"></use><use transform="scale(0.5)" xlink:href="#MJMATHI-45" x="803" y="0"></use></g></g></g></g></g><g is="true" transform="translate(1821,-383)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMATHI-54"></use></g></g></g></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">bin</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msubsup subscriptshift="90%" superscriptshift="90%" is="true"><mrow is="true"><mo is="true">âˆ‘</mo></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></msubsup></mrow><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">binE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">n</mi></mrow></mfrac><mo is="true">,</mo><mspace width="1.0em" is="true"></mspace><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">binE</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub><mo is="true">+</mo><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">NE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">T</mi></mrow></mfrac></mrow></math></span></span><script type="math/mml" id="MathJax-Element-21"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">bin</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mrow is="true"><msubsup subscriptshift="90%" superscriptshift="90%" is="true"><mrow is="true"><mo is="true">âˆ‘</mo></mrow><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></msubsup></mrow><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">binE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">n</mi></mrow></mfrac><mo is="true">,</mo><mspace width="1.0em" is="true"></mspace><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">binE</mi></mrow></msub><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi is="true">E</mi></mrow></msub><mo is="true">+</mo><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi mathvariant="italic" is="true">Hit</mi></mrow><mrow is="true"><mi mathvariant="italic" is="true">NE</mi></mrow></msub></mrow></mrow><mrow is="true"><mi is="true">T</mi></mrow></mfrac></mrow></math></script></span></span></span>where <em>Hit</em><sub><em>E</em></sub> is the number of hits in the expression <em>E</em>, i.e. number of times the classifier <em>E</em> responded â€œyesâ€ and the tested image was of the expression <em>E. Hit</em><sub><em>NE</em></sub> is the number of times the classifier <em>E</em> responded â€œnoâ€ and the tested image was not the expression <em>E. T</em> is the total number of tested images and <em>n</em> is the number of expressions to be considered.</div></section><section id="s0065"><h3 id="sect0080" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.3. Pre-processing tuning</h3><div class="u-margin-s-bottom" id="p0335"><span><span>As described earlier, the proposed method combines a pre-processing step, that aims to remove non-expression specific features of a <a href="/topics/computer-science/facial-image" title="Learn more about facial image from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial image</a> and a Convolutional Neural Network to classify this preprocessed image in one of the six (or seven) expressions. In this section, we present the impact in the </span><a href="/topics/engineering/classification-accuracy" title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification accuracy</a> of each operation in the preprocessing step. As these experiments aim only to show the impact of the operations, a simplified version of our methodology was employed. Here, we randomly generate the order of the samples to the network and use a simple </span><em>k</em>-fold cross validation between the 8 groups of the CK+ database. The database was divided into two sets, training (with 7 of the groups) and test (with 1 of the groups). The training was performed 8 times using only 2000 epochs for each of them. The accuracy was computed per expression (<em>C</em><sub>6<em>classE</em></sub>) and overall average for all expressions (<em>C</em><sub>6<em>class</em></sub>).</div><div class="u-margin-s-bottom"><div id="p0340">(<em>a</em>) <em>No pre-processing</em> . This first experiment was carried out using the original database, without any intervention or image pre-processing, just a down-sampling to the image to be of the same size as the input of the CNN. In this experiment, the average accuracy for all expressions was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>53.57</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-35"></use><use xlink:href="#MJMAIN-33" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-35" x="1279" y="0"></use><use xlink:href="#MJMAIN-37" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">53.57</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-22"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">53.57</mn><mo is="true">%</mo></mrow></math></script></span>. The accuracy per expression is shown in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>. The accuracy shown is an average of Eq. <a class="anchor anchor-primary" href="#eq0010" name="beq0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="eq0010"><span class="anchor-text-container"><span class="anchor-text">(2)</span></span></a> for all runs.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0005"><span class="captions text-s"><span id="cap0065"><p id="sp0065"><span class="label">Table&nbsp;1</span>. Preprocessing steps tuning for the CK+ database: (a) no pre-processing; (b) just cropping; (c) just rotation correction; (d) cropping and rotation correction; (e) only intensity normalization; (f) both normalizations; (g) spatial normalization and synthetic samples; and (h) both normalizations and synthetic samples.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Preprocessing Step</strong></th><th scope="col"><strong>Angry</strong> (%)</th><th scope="col"><strong>Disgust</strong> (%)</th><th scope="col"><strong>Fear</strong> (%)</th><th scope="col"><strong>Happy</strong> (%)</th><th scope="col"><strong>Sad</strong> (%)</th><th scope="col"><strong>Surprise</strong> (%)</th><th scope="col"><strong>Average</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th class="align-char" scope="row">(a)</th><td class="align-char">28.10</td><td class="align-char">51.23</td><td class="align-char">17.91</td><td class="align-left">70.68</td><td class="align-char">20.99</td><td class="align-char">77.52</td><td class="align-char">53.57</td></tr><tr class="valign-top"><th class="align-char" scope="row">(b)</th><td class="align-char">68.60</td><td class="align-char">79.01</td><td class="align-char">23.37</td><td class="align-left">86.39</td><td class="align-char">23.46</td><td class="align-char">87.16</td><td class="align-char">71.67</td></tr><tr class="valign-top"><th class="align-char" scope="row">(c)</th><td class="align-char">17.17</td><td class="align-char">79.09</td><td class="align-char">00.00</td><td class="align-left">48.92</td><td class="align-char">05.05</td><td class="align-char">91.25</td><td class="align-char">61.55</td></tr><tr class="valign-top"><th class="align-char" scope="row">(d)</th><td class="align-char">81.82</td><td class="align-char">90.74</td><td class="align-char">73.13</td><td class="align-left">95.81</td><td class="align-char">66.67</td><td class="align-char">94.50</td><td class="align-char">87.86</td></tr><tr class="valign-top"><th class="align-char" scope="row">(e)</th><td class="align-char">27.27</td><td class="align-char">52.94</td><td class="align-char">08.22</td><td class="align-left">79.10</td><td class="align-char">18.29</td><td class="align-char">85.54</td><td class="align-char">57.00</td></tr><tr class="valign-top"><th class="align-char" scope="row">(f)</th><td class="align-char">78.51</td><td class="align-char">93.21</td><td class="align-char">53.73</td><td class="align-left">95.29</td><td class="align-char">75.31</td><td class="align-char">93.12</td><td class="align-char">86.67</td></tr><tr class="valign-top"><th class="align-char" scope="row">(g)</th><td class="align-char">86.05</td><td class="align-char">88.30</td><td class="align-char">69.33</td><td class="align-left">96.60</td><td class="align-char">77.11</td><td class="align-char">95.34</td><td class="align-char">87.10</td></tr><tr class="valign-top"><th class="align-char" scope="row">(h)</th><td class="align-char"><strong>79.34</strong></td><td class="align-char"><strong>94.44</strong></td><td class="align-char"><strong>73.13</strong></td><td class="align-left"><strong>99.48</strong></td><td class="align-char"><strong>72.84</strong></td><td class="align-char"><strong>94.94</strong></td><td class="align-char"><strong>89.76</strong></td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0345">As it can be seen in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>, using only the CNN without any image pre-processing, the recognition rate is very low compared with methods in the literature. We believe the variation and amount of samples in the CK+ database was small which did not allow the Convolutional Neural Network learn how to deal with pose, environment and subject variance. Additionally, it does not use the full range of the image space available in the network input to represent the face.</div><div class="u-margin-s-bottom" id="p0350">(<em>b</em>) <em>Image cropping</em>. In order to increase our method performance, as explained in <a class="anchor anchor-primary" href="#s0015" name="bs0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0015"><span class="anchor-text-container"><span class="anchor-text">Section 3</span></span></a>, the image is automatically cropped to remove non-expression specific regions, in both training and testing steps. As a results, the average accuracy for all expressions increased to <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>71.67</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-37"></use><use xlink:href="#MJMAIN-31" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-36" x="1279" y="0"></use><use xlink:href="#MJMAIN-37" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">71.67</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-23"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">71.67</mn><mo is="true">%</mo></mrow></math></script></span>. The accuracy per expression is shown in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>. Here, the down-sampling is also performed, because the input of the proposed network is a fixed 32 <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo is=&quot;true&quot;>&amp;#xD7;</mo></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.808ex" height="1.394ex" viewBox="0 -548.5 778.5 600.2" role="img" focusable="false" style="vertical-align: 0.019ex; margin-bottom: -0.139ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><use xlink:href="#MJMAIN-D7"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">Ã—</mo></math></span></span><script type="math/mml" id="MathJax-Element-24"><math><mo is="true">Ã—</mo></math></script></span>32 pixels image.</div><div class="u-margin-s-bottom" id="p0355">Compared with the result shown before, we can note a significant increase of the recognition rate by adding only the cropping process. The main reason for the accuracy increase is that with the cropping, we remove a lot of unnecessary information that the classifier will need to handle to determine the subject expression and use better the image space available in the network input.</div><div class="u-margin-s-bottom" id="p0360">(<em>c</em>)<em>Rotation correction</em>. A rotation correction (and the down-sampling) is performed in the image to remove rotations that are not related to expression facial changes (that can be pose-specific or caused by a camera movement), in both training and testing steps. The average accuracy for all expressions was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>61.55</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-36"></use><use xlink:href="#MJMAIN-31" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-35" x="1279" y="0"></use><use xlink:href="#MJMAIN-35" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">61.55</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-25"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">61.55</mn><mo is="true">%</mo></mrow></math></script></span>.</div><div class="u-margin-s-bottom" id="p0365">Note that, this result is applying just the rotation correction, but not the cropping. Compared with the result of no pre-processing we can note an increase of the accuracy in about 8.00%. This increase might be caused by the lower variation that the network needs to handle. With the rotation correction, the facial elements (eyes, mouth, and eyebrows) stay mostly in the same pixel space, but still has the influence of the background and does not use the full range of the image space available in the network input to represent the face, as in a).</div><div class="u-margin-s-bottom" id="p0370">(<em>d</em>) <em>Spatial normalization</em>. As seen before, the image cropping and the rotation correction applied separately, increase the classifier accuracy. This happens because both procedures reduce the problem complexity. Here, we discuss the full spatial normalization, composed by the image cropping, rotation correction and down-sampling. With the operations combined, the average accuracy for all expressions was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>87.86</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-38"></use><use xlink:href="#MJMAIN-37" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-38" x="1279" y="0"></use><use xlink:href="#MJMAIN-36" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">87.86</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-26"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">87.86</mn><mo is="true">%</mo></mrow></math></script></span>.</div><div class="u-margin-s-bottom" id="p0375">As expected, joining both procedures in the pre-processing step increase the accuracy. This happens because a lot of variation not related to the expression was removed from the image. Although the Convolutional Neural Network could handle these variations, we would need a bigger database (that we do not have) and maybe a more complex architecture.</div><div class="u-margin-s-bottom" id="p0380">(<em>e</em>) <em>Intensity normalization</em>. The spatial normalization procedure significantly increases the overall accuracy of the system. The intensity normalization is used to remove brightness variation in the images in both steps, training and testing. This experiment was performed using just the intensity normalization. It uses the same methodology described before. The average accuracy for all expressions was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>57.00</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-35"></use><use xlink:href="#MJMAIN-37" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-30" x="1279" y="0"></use><use xlink:href="#MJMAIN-30" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">57.00</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-27"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">57.00</mn><mo is="true">%</mo></mrow></math></script></span>.</div><div class="u-margin-s-bottom" id="p0385">As it can be seen, by just applying the intensity normalization the classifier accuracy was also slightly increased.</div><div class="u-margin-s-bottom" id="p0390">(<em>f</em>) <em>Spatial and intensity normalization</em>. Combining the spatial (rotation correction, cropping and down-sampling) and intensity normalizations, we remove a big part of the variations unrelated to the facial expression and leave just the expression specific variation that is not related to the pose or environment. This experiment was done using the same methodology described before. The average accuracy for all expression was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>86.67</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-38"></use><use xlink:href="#MJMAIN-36" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-36" x="1279" y="0"></use><use xlink:href="#MJMAIN-37" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">86.67</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-28"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">86.67</mn><mo is="true">%</mo></mrow></math></script></span>. The accuracy per expression is shown in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>.</div><div class="u-margin-s-bottom" id="p0395">As it can be seen, the accuracy of applying both normalization procedures is lower than the one that uses only the spatial normalization. The result of the fear expression has a very low accuracy, which reduces the overall recognition average.</div><div class="u-margin-s-bottom" id="p0400">(<em>g</em>) <em>Spatial normalization and synthetic samples</em>. The result of the spatial and intensity normalization and only the spatial normalization gives a false impression that the intensity normalization might decrease the accuracy of the method. To verify this assumption, a new experiment was conducted using only the spatial normalization procedure and the additional synthetic samples for training. For the synthetic samples generation, thirty more samples were generated for each image using a Gaussian standard deviation of 3 pixels (<span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-29-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>&amp;#x3B8;</mi><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>3</mn></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.351ex" height="2.086ex" viewBox="0 -796.9 2304.1 898.2" role="img" focusable="false" style="vertical-align: -0.235ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><use xlink:href="#MJMATHI-3B8"></use></g><g is="true" transform="translate(747,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(1803,0)"><use xlink:href="#MJMAIN-33"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><mi is="true">Î¸</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></span></span><script type="math/mml" id="MathJax-Element-29"><math><mrow is="true"><mi is="true">Î¸</mi><mo is="true">=</mo><mn is="true">3</mn></mrow></math></script></span>). The average accuracy for all expression was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-30-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>89.11</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-38"></use><use xlink:href="#MJMAIN-39" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-31" x="1279" y="0"></use><use xlink:href="#MJMAIN-31" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">89.11</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-30"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">89.11</mn><mo is="true">%</mo></mrow></math></script></span>. The accuracy per expression is shown in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>.</div><div class="u-margin-s-bottom" id="p0405">This result increases the accuracy of applying just the spatial normalization (87.86%). However, this accuracy is outperformed by the next experiment, that apply both normalizations and the synthetic sample generation, showing that the synthetic sample generation procedure indeed increases the robustness of the classifier (motivated by the increase of the samples and its variation).</div><div class="u-margin-s-bottom" id="p0410">(<em>h</em>) <em>Spatial normalization, intensity normalization and synthetic samples</em>. The best result achieved in our method applies the three image pre-processing steps: spatial normalization, intensity normalization and synthetic samples generation. This experiment is performed using the same methodology described before. The average accuracy for all expression was <span class="math"><span class="MathJax_Preview" style=""></span><span class="MathJax_SVG" id="MathJax-Element-31-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow is=&quot;true&quot;><msub subscriptshift=&quot;65%&quot; is=&quot;true&quot;><mrow is=&quot;true&quot;><mi is=&quot;true&quot;>C</mi></mrow><mrow is=&quot;true&quot;><mn is=&quot;true&quot;>6</mn><mi mathvariant=&quot;italic&quot; is=&quot;true&quot;>class</mi></mrow></msub><mo is=&quot;true&quot;>=</mo><mn is=&quot;true&quot;>89.79</mn><mo is=&quot;true&quot;>%</mo></mrow></math>" role="presentation" style="font-size: 90%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.048ex" height="3.702ex" viewBox="0 -846.5 7340.1 1593.7" role="img" focusable="false" style="vertical-align: -1.735ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g is="true"><g is="true"><g is="true"><g is="true"><use xlink:href="#MJMATHI-43"></use></g></g><g is="true" transform="translate(715,-650)"><g is="true"><use transform="scale(0.707)" xlink:href="#MJMAIN-36"></use></g><g is="true" transform="translate(520,0)"><use transform="scale(0.707)" xlink:href="#MJMATHI-63"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-6C" x="433" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-61" x="732" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1261" y="0"></use><use transform="scale(0.707)" xlink:href="#MJMATHI-73" x="1731" y="0"></use></g></g></g><g is="true" transform="translate(3169,0)"><use xlink:href="#MJMAIN-3D"></use></g><g is="true" transform="translate(4226,0)"><use xlink:href="#MJMAIN-38"></use><use xlink:href="#MJMAIN-39" x="500" y="0"></use><use xlink:href="#MJMAIN-2E" x="1001" y="0"></use><use xlink:href="#MJMAIN-37" x="1279" y="0"></use><use xlink:href="#MJMAIN-39" x="1780" y="0"></use></g><g is="true" transform="translate(6506,0)"><use xlink:href="#MJMAIN-25"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">89.79</mn><mo is="true">%</mo></mrow></math></span></span><script type="math/mml" id="MathJax-Element-31"><math><mrow is="true"><msub subscriptshift="65%" is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mn is="true">6</mn><mi mathvariant="italic" is="true">class</mi></mrow></msub><mo is="true">=</mo><mn is="true">89.79</mn><mo is="true">%</mo></mrow></math></script></span>. The accuracy of this experiment shows that combining the three techniques (spatial normalization, intensity normalization and synthetic samples) is better than using them individually.</div><div class="u-margin-s-bottom" id="p0415"><a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a> shows the mean accuracy for each expression using all the preprocessing steps already discussed for the CK+ database: (a) no preprocessing, (b) cropping, (c) rotation correction, (d) spatial normalization (cropping and rotation correction), (e) intensity normalization, (f) both normalizations (spatial and intensity), (g) spatial normalization using the synthetic samples and (h) both normalizations and the synthetic samples. The accuracy is computed using the six class classifier (<em>C</em><sub>6<em>classE</em></sub>).. The best accuracies achieved are highlighted in bold.</div></section><section id="s0070"><h3 id="sect0085" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.4. Results</h3><div class="u-margin-s-bottom" id="p0420">As discussed before, a simplified training/testing methodology was used to evaluate the impact of the pre-processing steps. In contrast to the tuning experiments that used only training and test sets, this section separates the databases for each experiment in three main sets: training set, validation set and test set.</div><div class="u-margin-s-bottom"><div id="p0425">In addition, it was also mentioned that the gradient descent method was used for training the network. Such methods might be influenced by the order of presentation of the samples to the network during training, which causes a variation in accuracy. As can be seen in <a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a><span> the accuracy increases (or decreases) in about 4.00% only with the order change of the <a href="/topics/computer-science/training-sample" title="Learn more about training samples from ScienceDirect's AI-generated Topic Pages" class="topic-link">training samples</a> (as it can be seen in the values highlighted in bold). </span><a class="anchor anchor-primary" href="#t0010" name="bt0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0010"><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a> shows the accuracy for the same training, performed 10 times, each one with a random presenting order of the training samples. To be less affected by this accuracy variation, we propose a training methodology that uses a validation set to choose the best network weights based on different trainings, illustrated in <a class="anchor anchor-primary" href="#f0010" name="bf0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0010"><span class="anchor-text-container"><span class="anchor-text">Fig. 2</span></span></a>. Therefore, the final accuracy result of our experiments is computed using the network weights of the best run out of 10 runs, having a validation set for accuracy measurement. Each run has a random presentation order of the training samples. The weights of the best run in the validation set are later used to evaluate the test set and compute the final accuracy.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0010"><span class="captions text-s"><span id="cap0070"><p id="sp0070"><span class="label">Table&nbsp;2</span>. Impact of the presentation order in the accuracy.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Presentation order</strong></th><th scope="col"><strong>Accuracy</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th scope="row"><strong>1</strong></th><td class="align-left"><strong>88.18</strong></td></tr><tr class="valign-top"><th scope="row">2</th><td class="align-left">86.36</td></tr><tr class="valign-top"><th scope="row">3</th><td class="align-left">86.36</td></tr><tr class="valign-top"><th scope="row">4</th><td class="align-left">86.36</td></tr><tr class="valign-top"><th scope="row">5</th><td class="align-left">88.18</td></tr><tr class="valign-top"><th scope="row"><strong>6</strong></th><td class="align-left"><strong>84.55</strong></td></tr><tr class="valign-top"><th scope="row">7</th><td class="align-left">85.45</td></tr><tr class="valign-top"><th scope="row">8</th><td class="align-left">84.55</td></tr><tr class="valign-top"><th scope="row">9</th><td class="align-left">87.27</td></tr><tr class="valign-top"><th scope="row">10</th><td class="align-left">88.18</td></tr><tr><th scope="row" colspan="2"><br></th></tr><tr class="valign-top"><th scope="row" colspan="2"><strong>Average: 86.71%</strong></th></tr><tr><th scope="row" colspan="2"><br></th></tr><tr class="valign-top"><th scope="row" colspan="2"><strong>Standard deviation: 1.43%</strong></th></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0430">To verify that the proposed approach handles well other databases and even in unknown environments, some experiments with the BU-3DFE database, the JAFFE database and cross-databases experiments were performed. The experiments with other databases follow the same approach that the CK+ database. In the cross-database experiments, the network was trained with the CK+ database and the accuracy evaluation was calculated using the BU-3DFE database or in the JAFFE database. In the cross-database experiments, no images from the BU-3DFE database or JAFFE database was used during the network training.</div><div class="u-margin-s-bottom" id="p0435"><em>CK+ database experiment</em>. The database was separated into 8 groups of non-overlapping subjects (with about 12 subjects each). Seven groups were used to compose the training set, whereas the eighth group (also with about 12 subjects) was shared between the validation set and test set, where 11 subjects were used for validation and 1 subject for testing. Our experiments follow a <em>k</em>-fold cross-validation configuration, in which, each time, one group is separated for validation/test and the other seven for training. When a group is selected for validation/test, a leave-one-out procedure is performed within the 12 subjects (having 11 for validation and 1 for test) for each of the training runs. For each configuration of validation and test subjects, the training is carried out 10 times, changing the presentation order of the training images. The validation group is used to select the best epoch for each run during training and to select the run with the best presentation order. Based on these information (best epoch and presentation order), the best network weights are selected and used to compute the accuracy of the test set. With this experimental configuration, the training of the network is performed about 960 times (8 groups * 12 subjects per group * 10 runs with different presenting order). The training of each run took only 2&nbsp;min, therefore the total training of the system takes about 20&nbsp;min (2&nbsp;min * 10 runs). The overall experiment time was about 32&nbsp;hs (including all combinations of training, validation and test). The results of this experiment are computed as an average of the 96 runs (8 folds * 12 subjects per group * 1 best configuration out of 10 runs).</div><div class="u-margin-s-bottom"><div id="p0440"><a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a> shows the best result achieved (using both normalizations and the synthetic samples) for <em>C</em><sub>6class</sub> and <em>C</em><sub><em>bin</em></sub>. As it can be seen, the binary classifier approach increases the accuracy. It happens because in this approach the hit can be achieved six times (one for each classifier), instead of using just one classifier, where each sample has just one chance to be properly classified. The binary classifier approach was employed to allow a fair comparison with some methods in the literature that just report these results. We think that the six-class classifier (<em>C</em><sub>6class</sub>) is a fairer evaluation method. However, the <em>C</em><sub><em>bin</em></sub><span> <a href="/topics/computer-science/classification-approach" title="Learn more about classification approach from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification approach</a> is a useful method when we are interested in just one expression. The standard deviation reported in the table is based on the runs of all subjects.</span></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0015"><span class="captions text-s"><span id="cap0075"><p id="sp0075"><span class="label">Table&nbsp;3</span>. Accuracy for both classifiers using all processing steps and the synthetic samples for six expressions on the CK+ database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>Angry</strong> (%)</th><th scope="col"><strong>Disgust</strong> (%)</th><th scope="col"><strong>Fear</strong> (%)</th><th scope="col"><strong>Happy</strong> (%)</th><th scope="col"><strong>Sad</strong> (%)</th><th scope="col"><strong>Surprise</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th scope="row"><em>C</em><sub>6<em>classE</em></sub></th><td class="align-char">93.33</td><td class="align-char">100.00</td><td class="align-char">96.00</td><td class="align-char">98.55</td><td class="align-char">84.52</td><td class="align-char">99.20</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>binE</em></sub></th><td class="align-char">98.27</td><td class="align-char">99.37</td><td class="align-char">99.24</td><td class="align-char">99.68</td><td class="align-char">98.17</td><td class="align-char">98.81</td></tr><tr><th scope="row" colspan="7"><br></th></tr><tr class="valign-top"><th scope="row" colspan="7"><strong>Average of</strong> <em>C</em><sub><strong>6</strong><em>class</em></sub>: <strong>96.76</strong>%Â±<strong>0.07</strong></th></tr><tr><th scope="row" colspan="7"><br></th></tr><tr class="valign-top"><th scope="row" colspan="7"><strong>Average of</strong> <em>C</em><sub><em>bin</em></sub>: <strong>98.92</strong>%Â±<strong>0.02</strong></th></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="p0445">The training parameter values that achieve the results shown in <a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a> are shown in <a class="anchor anchor-primary" href="#t0020" name="bt0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0020"><span class="anchor-text-container"><span class="anchor-text">Table 4</span></span></a>. The same parameter values are used in the experiments on other databases.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0020"><span class="captions text-s"><span id="cap0080"><p id="sp0080"><span class="label">Table&nbsp;4</span>. Training parameters.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Parameter</strong></th><th scope="col"><strong>Value</strong></th></tr></thead><tbody><tr class="valign-top"><th scope="row">Momentum</th><td>0.95</td></tr><tr class="valign-top"><th scope="row">Learning rate</th><td>0.01</td></tr><tr class="valign-top"><th scope="row">Epochs</th><td>10,000</td></tr><tr class="valign-top"><th scope="row">Loss function</th><td><em>SoftmaxWithLoss</em></td></tr><tr class="valign-top"><th scope="row">Gaussian standard deviation</th><td>3</td></tr><tr class="valign-top"><th scope="row">Synthetic samples amount</th><td>70</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="p0450">Using the result shown in <a class="anchor anchor-primary" href="#t0015" name="bt0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0015"><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a><span>, the <a href="/topics/engineering/confusion-matrix" title="Learn more about confusion matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">confusion matrix</a> shown in </span><a class="anchor anchor-primary" href="#t0025" name="bt0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0025"><span class="anchor-text-container"><span class="anchor-text">Table 5</span></span></a> was created for the six-class classifier.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0025"><span class="captions text-s"><span id="cap0085"><p id="sp0085"><span class="label">Table&nbsp;5</span>. <a href="/topics/computer-science/confusion-matrix" title="Learn more about Confusion matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">Confusion matrix</a> using both normalizations and synthetic samples for six expressions on the CK+ database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">126</td><td class="align-char">6</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">0</td><td class="align-char">177</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">72</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">204</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">71</td><td class="align-char">9</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">247</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="p0455"><span>Based on the results of the six-class classifier, we can note that the disgust, happy and surprise expressions achieve an accuracy rate higher than 98%. While the angry and fear expression were about 93% and 96%, respectively. The sad expression achieves the smallest recognition rate, with only 84.52%. Looking at the <a href="/topics/computer-science/confusion-matrix" title="Learn more about confusion matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">confusion matrix</a>, the sad expression was confused in the majority of the time with the surprise expression. This shows that the features of these two expressions are not well separated in the pixel space, i.e. they are very similar to each other in some cases. </span><a class="anchor anchor-primary" href="#f0055" name="bf0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0055"><span class="anchor-text-container"><span class="anchor-text">Fig. 11</span></span></a><span> shows some examples of the <a href="/topics/engineering/misclassification" title="Learn more about misclassification from ScienceDirect's AI-generated Topic Pages" class="topic-link">misclassification</a>.</span></div><figure class="figure text-xs" id="f0055"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr11.jpg" height="168" alt="Fig.&nbsp;11" aria-describedby="cap0055"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr11_lrg.jpg" target="_blank" download="" title="Download high-res image (405KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (405KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr11.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0055"><p id="sp0055"><span class="label">Fig.&nbsp;11</span>. In (1), the expected expression was sad, but the method returned fear. In (2), the expected expression was angry, but the method returned fear. In (3), the expected expression was sad, but the method returned angry. In (4), the expected expression was angry, but the method returned sad.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0460"><a class="anchor anchor-primary" href="#f0060" name="bf0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0060"><span class="anchor-text-container"><span class="anchor-text">Fig. 12</span></span></a> shows an illustration of the learned kernels and the generated maps for each convolution layer. In the first convolution layer, the input image is processed by the 32 learned kernels and generates 32 output maps. In the second convolution layer, the 64 learned kernels are used to generate new maps for each one of the 32 maps of the previous layer. The kernels shown in <a class="anchor anchor-primary" href="#f0060" name="bf0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="f0060"><span class="anchor-text-container"><span class="anchor-text">Fig. 12</span></span></a> were learned in the training using the CK+ database for the six basic expressions. As it can be seen in the figure, after the second convolutional layer, the generated maps are focused on regions near the eyes, mouth and nose. Indeed, these regions are more critical for facial expression analysis, as suggested by psychological studies <a class="anchor anchor-primary" href="#bib71" name="bbib71" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib71"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a>.</div><figure class="figure text-xs" id="f0060"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr12.jpg" height="303" alt="Fig.&nbsp;12" aria-describedby="cap0060"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr12_lrg.jpg" target="_blank" download="" title="Download high-res image (495KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (495KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-gr12.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0060"><p id="sp0060"><span class="label">Fig.&nbsp;12</span>. Illustration of the learned kernels and the generated maps for each <a href="/topics/computer-science/convolution-layer" title="Learn more about convolution layer from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolution layer</a>. In the first convolution layer, the input image is processed by the 32 learned kernels and generates 32 output maps. In the second convolution layer, the 64 learned kernels are used to generate new maps for each one of the 32 maps of the previous layer. The sub-sampling layers are not represented in this image. Only a subset of the 32 kernels for the first layer and of the 64 kernels for the second layer are shown. The generated maps were equalized to allow for a better visualization.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="p0465">Instead of recognizing only six expressions, we can also recognize the neutral expression, which results in a classifier that recognizes seven expressions. The result of the seven expressions classifier to the CK+ database, using the same methodology of the six-class is shown in <a class="anchor anchor-primary" href="#t0030" name="bt0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0030"><span class="anchor-text-container"><span class="anchor-text">Table 6</span></span></a>. The standard deviation reported in the table is between the runs of all subjects.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0030"><span class="captions text-s"><span id="cap0090"><p id="sp0090"><span class="label">Table&nbsp;6</span>. Accuracy for both classifiers using all processing steps and the synthetic samples for seven expressions.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>Neutral</strong> (%)</th><th scope="col"><strong>Angry</strong> (%)</th><th scope="col"><strong>Disgust</strong> (%)</th><th scope="col"><strong>Fear</strong> (%)</th><th scope="col"><strong>Happy</strong> (%)</th><th scope="col"><strong>Sad</strong> (%)</th><th scope="col"><strong>Surprise</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th scope="row"><em>C</em><sub><strong>7</strong><em>classE</em></sub></th><td class="align-char">95.15</td><td class="align-char">91.11</td><td class="align-char">99.44</td><td class="align-char">92.00</td><td class="align-char">100.0</td><td class="align-char">82.14</td><td class="align-char">98.80</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>binE</em></sub></th><td class="align-char">97.49</td><td class="align-char">97.82</td><td class="align-char">99.76</td><td class="align-char">99.11</td><td class="align-char">99.76</td><td class="align-char">98.79</td><td class="align-char">98.87</td></tr><tr><th scope="row" colspan="8"><br></th></tr><tr class="valign-top"><th scope="row" colspan="8"><strong>Average of</strong> <em>C</em><sub>7<em>class</em></sub>: <strong>95.79</strong>%Â±<strong>0.06</strong></th></tr><tr><th scope="row" colspan="8"><br></th></tr><tr class="valign-top"><th scope="row" colspan="8"><strong>Average of</strong> <em>C</em><sub><em>bin</em></sub>: <strong>98.80</strong>%Â±<strong>0.01</strong></th></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="p0470">As it can be seen, for the binary classifier approach, we have a slight decrease in accuracy, from 98.90% to 98.80%. On the other hand, in the seven-class classifier the decrease was larger, from 96.7% to 95.7%. It happens because, in the seven-class classifier approach, one more output was included in the network. On the other hand, in the binary-class approach, one new classifier was inserted, keeping the others unchanged. The confusion matrix for the seven expressions is shown in <a class="anchor anchor-primary" href="#t0035" name="bt0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0035"><span class="anchor-text-container"><span class="anchor-text">Table 7</span></span></a>.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0035"><span class="captions text-s"><span id="cap0095"><p id="sp0095"><span class="label">Table&nbsp;7</span>. Confusion matrix using both normalizations and synthetic samples for seven expressions on the CK+ database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Neutral</th><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Neutral</th><td class="align-char">294</td><td class="align-char">11</td><td class="align-char">1</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">2</td></tr><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">8</td><td class="align-char">123</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">176</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">6</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">69</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">207</td><td class="align-char">0</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">69</td><td class="align-char">9</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">246</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom"><div id="p0475"><em>BU-3DFE database experiment</em>. This experiment follows the same approach as the CK+ database, the only difference is that in the BU-3DFE database the groups have about only eight subjects. The results of this experiment is computed as an average of the 64 runs (8 folds * 8 subjects per group * 1 best configuration out of 10 runs). The result for six and seven expressions for both classifiers is shown in <a class="anchor anchor-primary" href="#t0040" name="bt0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0040"><span class="anchor-text-container"><span class="anchor-text">Table 8</span></span></a>. The standard deviation reported in the table is between the runs of all subjects.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0040"><span class="captions text-s"><span id="cap0100"><p id="sp0100"><span class="label">Table&nbsp;8</span>. Accuracy using six and seven (six basic plus neutral) expressions for the BU-3DFE and JAFFE databases.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><th scope="col"><strong>Train</strong></th><th scope="col"><strong>Test</strong></th><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>6-expressions</strong></th><th scope="col"><strong>7-expressions</strong></th></tr></thead><tbody><tr class="valign-top"><th scope="row">BU-3DFE</th><td>BU-3DFE</td><td><em>C</em><sub><em>nclass</em></sub></td><td>72.89%Â±0.05</td><td>71.62%Â±0.04</td></tr><tr class="valign-top"><th scope="row">BU-3DFE</th><td>BU-3DFE</td><td><em>C</em><sub><em>bin</em></sub></td><td>90.96%Â±0.01</td><td>91.89%Â±0.01</td></tr><tr class="valign-top"><th scope="row">JAFFE</th><td>JAFFE</td><td><em>C</em><sub><em>nclass</em></sub></td><td>53.44%Â±0.15</td><td>53.57%Â±0.13</td></tr><tr class="valign-top"><th scope="row">JAFFE</th><td>JAFFE</td><td><em>C</em><sub><em>bin</em></sub></td><td>84.48%Â±0.05</td><td>86.74%Â±0.03</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0480">As it can be seen, the accuracy for the BU-3DFE decreased compared with the CK+ database. One possible reason is that this database has more subjects from different ethnicities and light conditions, and is smaller than the CK+. In addition, we have an increase in the accuracy for the <em>C</em><sub><em>bin</em></sub> classifier on seven expressions, whereas we have a decrease in accuracy for the <em>C</em><sub><em>nclass</em></sub> classifier on seven expressions. The confusion matrices for this experiment, on six and seven expressions, can be seen in <a class="anchor anchor-primary" href="#t0045" name="bt0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0045"><span class="anchor-text-container"><span class="anchor-text">Table 13</span></span></a> and in <a class="anchor anchor-primary" href="#t0050" name="bt0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0050"><span class="anchor-text-container"><span class="anchor-text">Table 14</span></span></a>, respectively, in the appendices (<a class="anchor anchor-primary" href="#s0100" name="bs0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0100"><span class="anchor-text-container"><span class="anchor-text">Appendix A</span></span></a>).</div><div class="u-margin-s-bottom" id="p0485"><span><span>The BU-3DFE database contains 3D models of faces in the six basic expressions and the neutral expressions. Commonly, this database is used for <a href="/topics/computer-science/3d-reconstruction" title="Learn more about 3D reconstruction from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D reconstruction</a> and facial expression recognition with 3D data. However, in this work, just the 2D image of the subjects are used to recognize the expressions. There are other works in the literature that presents higher facial expression </span><a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracies from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracies</a>, but using 3D information to infer the expressions </span><a class="anchor anchor-primary" href="#bib72" name="bbib72" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib72"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a>, <a class="anchor anchor-primary" href="#bib73" name="bbib73" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib73"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a>, <a class="anchor anchor-primary" href="#bib74" name="bbib74" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib74"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a>.</div><div class="u-margin-s-bottom" id="p0490">A better evaluation of the proposed method in real environments is the cross-database experiments, i.e. train the method with one database and test with another (in this case the BU-3DFE).</div><div class="u-margin-s-bottom"><div id="p0495">To perform the cross-database experiment, seven groups of the CK+ database were used to train the network and one was used to be the validation set (to choose the best network weights based on the best epoch presentation order of the training set). The BU-3DFE was used to test the network. The training was done eight times, each one with a different validation set. We ran each configuration (training set plus validation set) 10 times, each one with a different presenting order in the training samples. The result in the cross-database experiment is computed as an average of the 8 runs to consider different combinations of training and validation sets (8 folds * 1 best configuration out of 10 runs) showing all the BU-3DFE images to test the network. The results are shown in <a class="anchor anchor-primary" href="#t0055" name="bt0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0055"><span class="anchor-text-container"><span class="anchor-text">Table 9</span></span></a>.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0055"><span class="captions text-s"><span id="cap0115"><p id="sp0115"><span class="label">Table&nbsp;9</span>. Cross-database experiment for the BU-3DFE and JAFFE databases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col"><strong>Train</strong></th><th scope="col"><strong>Test</strong></th><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>6-expressions</strong> (%)</th><th scope="col"><strong>7-expressions</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th scope="row">CK+</th><td>BU-3DFE</td><td><em>C</em><sub><em>nclass</em></sub></td><td class="align-char">45.91</td><td class="align-char">42.25</td></tr><tr class="valign-top"><th scope="row">CK+</th><td>BU-3DFE</td><td><em>C</em><sub><em>bin</em></sub></td><td class="align-char">81.97</td><td class="align-char">83.50</td></tr><tr class="valign-top"><th scope="row">CK+</th><td>JAFFE</td><td><em>C</em><sub><em>nclass</em></sub></td><td class="align-char">38.80</td><td class="align-char">37.36</td></tr><tr class="valign-top"><th scope="row">CK+</th><td>JAFFE</td><td><em>C</em><sub><em>bin</em></sub></td><td class="align-char">79.60</td><td class="align-char">82.10</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0500">The confusion matrices for this experiment, on six and seven expressions, can be seen in <a class="anchor anchor-primary" href="#t0060" name="bt0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0060"><span class="anchor-text-container"><span class="anchor-text">Table 15</span></span></a> and in <a class="anchor anchor-primary" href="#t0065" name="bt0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0065"><span class="anchor-text-container"><span class="anchor-text">Table 16</span></span></a> respectively, in the appendices (<a class="anchor anchor-primary" href="#s0100" name="bs0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0100"><span class="anchor-text-container"><span class="anchor-text">Appendix A</span></span></a>).</div><div class="u-margin-s-bottom" id="p0505"><em>JAFFE database experiment</em>. This experiment follows a slightly different approach from the CK+ and BU-3DFE experiments in regards to the number of groups. The JAFFE database contains images from only 10 subjects, therefore, as done by other works in the literature <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, the images were separated in 10 groups, each one with just one subject. The test was carried out using a 10-fold cross validation, the training group contains eight subjects, the validation group one subject and the testing group one subject. The result of this experiment is computed as an average of the 10 runs (10 folds * 1 subjects per group * 1 best configuration out of 10 runs). The result for six and seven expressions for both classifiers is shown in <a class="anchor anchor-primary" href="#t0040" name="bt0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0040"><span class="anchor-text-container"><span class="anchor-text">Table 8</span></span></a>. The standard deviation reported in the table is between the runs of all subjects.</div><div class="u-margin-s-bottom" id="p0510">As it can be seen in <a class="anchor anchor-primary" href="#t0040" name="bt0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0040"><span class="anchor-text-container"><span class="anchor-text">Table 8</span></span></a>, compared with the CK+ and BU-3DFE results, the accuracy decreased considerably. It also happens in other works <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a><span>, motivated mainly due to the small database. The required data amount that methods for <a href="/topics/computer-science/facial-action" title="Learn more about facial action from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial action</a> (or expression) recognition need in the training phase to achieve a good accuracy is studied by Girard et al.&nbsp;</span><a class="anchor anchor-primary" href="#bib53" name="bbib53" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib53"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a>. In one of their experiments, the recognition rate varies from 63% to 94% as the training set increased from 8 subjects to 64 subjects. Once the JAFFE dataset contains images from only 10 subjects, it is unfair to compare its result with the CK+ or the BU3DFE datasets that contain, respectively, 100 subjects and 64 subjects.</div><div class="u-margin-s-bottom" id="p0515">This problem of small amount of data is more emphasized in the technique used in this work. Convolutional Neural Networks, generally, requires a big amount of data to adjust its parameters. Therefore, there are other approaches in the literature that do not require a so high amount of data and consequently achieve better results than the method presented here <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a>, <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>. The confusion matrices for this experiment, on six and seven expressions, can be seen in <a class="anchor anchor-primary" href="#t0070" name="bt0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0070"><span class="anchor-text-container"><span class="anchor-text">Table 17</span></span></a> and in <a class="anchor anchor-primary" href="#t0075" name="bt0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0075"><span class="anchor-text-container"><span class="anchor-text">Table 18</span></span></a>, respectively, in the appendices (<a class="anchor anchor-primary" href="#s0100" name="bs0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0100"><span class="anchor-text-container"><span class="anchor-text">Appendix A</span></span></a>).</div><div class="u-margin-s-bottom" id="p0520">The cross-database experiment was also performed to the JAFFE database. In this experiment the network is trained and validated only on the CK+ database and the tests were carried out on the JAFFE database. This experiment follows the same approach described for the BU-3DFE. The result in the cross-database experiment is computed as an average of the 8 runs to consider different combinations of training and validation sets (8 folds * 1 best configuration out of 10 runs) showing all the JAFFE images to test the network. The results for this experiment are shown in <a class="anchor anchor-primary" href="#t0055" name="bt0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0055"><span class="anchor-text-container"><span class="anchor-text">Table 9</span></span></a>.</div><div class="u-margin-s-bottom" id="p0525">One motivation for the small accuracy reported in <a class="anchor anchor-primary" href="#t0055" name="bt0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0055"><span class="anchor-text-container"><span class="anchor-text">Table 9</span></span></a> is the cultural differences over the training subjects and the testing subjects. While in the BU-3DFE we have some subjects of the same ethnicity of the CK+ database, the same does not happen with the JAFFE database. The confusion matrices for this experiment, on six and seven expressions, can be seen in <a class="anchor anchor-primary" href="#t0080" name="bt0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0080"><span class="anchor-text-container"><span class="anchor-text">Table 19</span></span></a> and in <a class="anchor anchor-primary" href="#t0085" name="bt0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0085"><span class="anchor-text-container"><span class="anchor-text">Table 20</span></span></a>, respectively, in the appendices (<a class="anchor anchor-primary" href="#s0100" name="bs0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0100"><span class="anchor-text-container"><span class="anchor-text">Appendix A</span></span></a>).</div></section><section id="s0075"><h3 id="sect0090" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.5. Comparisons</h3><div class="u-margin-s-bottom"><div id="p0530"><a class="anchor anchor-primary" href="#t0090" name="bt0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0090"><span class="anchor-text-container"><span class="anchor-text">Table 10</span></span></a> summarizes all results presented in this section, showing the evolution of the proposed method by changing the image pre-processing steps.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0090"><span class="captions text-s"><span id="cap0150"><p id="sp0150"><span class="label">Table&nbsp;10</span>. Preprocessing comparison.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col"><strong>Preprocessing</strong></th><th scope="col"><strong>Accuracy</strong> (%)</th></tr></thead><tbody><tr class="valign-top"><th scope="row">None</th><td class="align-char">53.57</td></tr><tr class="valign-top"><th scope="row">Cropping</th><td class="align-char">71.67</td></tr><tr class="valign-top"><th scope="row">Rotation correction</th><td class="align-char">61.55</td></tr><tr class="valign-top"><th scope="row">Spatial normalization</th><td class="align-char">87.86</td></tr><tr class="valign-top"><th scope="row">Intensity normalization</th><td class="align-char">57.00</td></tr><tr class="valign-top"><th scope="row">Spatial and intensity normalization</th><td class="align-char">86.67</td></tr><tr class="valign-top"><th scope="row">Spatial normalization and synthetic samples</th><td class="align-char">89.11</td></tr><tr class="valign-top"><th scope="row"><strong>Spatial and intensity normalization and synthetic samples</strong></th><td class="align-char"><strong>89.79</strong></td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0535">As it can be seen in <a class="anchor anchor-primary" href="#t0090" name="bt0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0090"><span class="anchor-text-container"><span class="anchor-text">Table 10</span></span></a>, the best performance was achieved using both normalization procedures and the synthetic samples (as it can be seen in the row highlighted in bold). This table just summarizes the experiments performed to choose the best configuration to the proposed classifier, using a simplified training methodology.</div><div class="u-margin-s-bottom"><div id="p0540"><a class="anchor anchor-primary" href="#t0095" name="bt0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0095"><span class="anchor-text-container"><span class="anchor-text">Table 11</span></span></a> shows the results of the CK+ database using the full validation methodology (with training, validation and test sets) explained before. In this table, a comparison between the proposed method and other methods in the literature that use the same experimental methodology (i.e. perform a <em>k</em>-fold cross-validation and guarantee that the same subject is not in the training and testing groups) is presented.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0095"><span class="captions text-s"><span id="cap0155"><p id="sp0155"><span class="label">Table&nbsp;11</span>. Comparison for the CK+ database.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col"><strong>Method</strong></th><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>6-expressions</strong></th><th scope="col"><strong>7-expressions</strong></th></tr></thead><tbody><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Fan and Tjahjadi <a class="anchor anchor-primary" href="#bib16" name="bbib16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib16"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a><a class="anchor anchor-primary" href="#tblfn1" name="btblfn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tblfn1"><span class="anchor-text-container"><span class="anchor-text"><sup>a</sup></span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>83.70</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Zavaschi et al. <a class="anchor anchor-primary" href="#bib44" name="bbib44" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib44"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>â€“</td><td>88.90</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Rivera et al. <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>â€“</td><td>89.30</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Gu et al. <a class="anchor anchor-primary" href="#bib67" name="bbib67" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib67"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>91.51</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Zhong et al. <a class="anchor anchor-primary" href="#bib68" name="bbib68" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib68"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>93.30</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">AUDN <a class="anchor anchor-primary" href="#bib12" name="bbib12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib12"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>93.70</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Liu et al. <a class="anchor anchor-primary" href="#bib76" name="bbib76" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib76"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a><a class="anchor anchor-primary" href="#tblfn1" name="btblfn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tblfn1"><span class="anchor-text-container"><span class="anchor-text"><sup>a</sup></span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>94.19</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Lee et al. <a class="anchor anchor-primary" href="#bib69" name="bbib69" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib69"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a><a class="anchor anchor-primary" href="#tblfn1" name="btblfn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tblfn1"><span class="anchor-text-container"><span class="anchor-text"><sup>a</sup></span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>94.90</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">LBP+SVM <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>95.10</td><td>91.40</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">BDBN <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>â€“</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>96.70</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">IntraFace <a class="anchor anchor-primary" href="#bib70" name="bbib70" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib70"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a><a class="anchor anchor-primary" href="#tblfn1" name="btblfn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tblfn1"><span class="anchor-text-container"><span class="anchor-text"><sup>a</sup></span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>96.40</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2"><strong>Proposed</strong></th><td><em>C</em><sub><em>nclass</em></sub></td><td><strong>96.76</strong></td><td><strong>95.75</strong></td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td><strong>98.92</strong></td><td><strong>98.80</strong></td></tr></tbody></table></div><dl class="footnotes"><dt id="tblfn1">a</dt><dd><div class="u-margin-s-bottom" id="ntp0005">The authors also recognize the contempt expression. They were not placed in the 7-expression column because our 7-expression recognition system is based on the six basic expression plus the neural expression (not the contempt expression).</div></dd></dl></div></div><div class="u-margin-s-bottom" id="p0545">As it can be seen in <a class="anchor anchor-primary" href="#t0095" name="bt0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0095"><span class="anchor-text-container"><span class="anchor-text">Table 11</span></span></a>, the proposed method achieves competitive results in the CK+ database for all experiment configurations (as it can be seen in the row highlighted in bold). Besides, the training and recognition times are also smaller than the others. The total time to train the system is with the CK+ is about 20&nbsp;min. The whole experimentation time including all the <em>k</em>-fold configurations of the proposed method was 32&nbsp;h, and the recognition is real time (only 0.01&nbsp;s for each image), almost 100 images can be processed per second. In comparison with Liu et al. <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, their training took eight days and the recognition was about 0.21&nbsp;s per image. Note that we disregard the hardware used for computing the time. Shan et al. <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a> and Zhon et al. <a class="anchor anchor-primary" href="#bib68" name="bbib68" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib68"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a> did not report the training and recognition time. The method proposed by Lee et al. <a class="anchor anchor-primary" href="#bib69" name="bbib69" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib69"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a> requires manual image pre-processing before the facial expression recognition.</div><div class="u-margin-s-bottom" id="p0550">It is important to note that although there are some other methods in the literature that report accuracies higher than ours (98.92%), the results are not comparable. As discussed before, a fair evaluation method for facial expression recognition should guarantee that images of a subject are not present in both training and testing sets at the same time. Accuracy achieved without this constraint overcomes the presented result. Some of these methods randomly select data to the folds <a class="anchor anchor-primary" href="#bib39" name="bbib39" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib39"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>, <a class="anchor anchor-primary" href="#bib40" name="bbib40" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib40"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a>, <a class="anchor anchor-primary" href="#bib41" name="bbib41" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib41"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a>, <a class="anchor anchor-primary" href="#bib42" name="bbib42" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib42"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a>. Some other do not mention if this constraint was kept in the images chosen for each fold <a class="anchor anchor-primary" href="#bib77" name="bbib77" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib77"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a>, <a class="anchor anchor-primary" href="#bib78" name="bbib78" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib78"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a>, <a class="anchor anchor-primary" href="#bib10" name="bbib10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib10"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a> therefore they were assumed to have an overlap. There are also methods which select only a subset of the database (or a subset of the six basic expressions) to evaluate the accuracy <a class="anchor anchor-primary" href="#bib79" name="bbib79" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib79"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a>, <a class="anchor anchor-primary" href="#bib80" name="bbib80" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib80"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a>, <a class="anchor anchor-primary" href="#bib81" name="bbib81" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib81"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a>, <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a>. Rivera et al. <a class="anchor anchor-primary" href="#bib43" name="bbib43" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib43"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a> also reported an accuracy of 99.50% in the CK <a class="anchor anchor-primary" href="#bib82" name="bbib82" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib82"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a> database, which is a previous version of the CK+ <a class="anchor anchor-primary" href="#bib4" name="bbib4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib4"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a> database with less subjects and image sequences. However, as it can be seen in&nbsp;<a class="anchor anchor-primary" href="#t0095" name="bt0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0095"><span class="anchor-text-container"><span class="anchor-text">Table 11</span></span></a>, the results with CK+ tend to be worse than the ones with the CK.</div><div class="u-margin-s-bottom"><div id="p0555">Some methods in the literature <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a>, <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, <a class="anchor anchor-primary" href="#bib67" name="bbib67" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib67"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a> also performed the cross-database experiment in the JAFFE database (training in the CK+ database and testing in the JAFFE). A comparison of this work with these methods, for the cross-database experiment, is shown in <a class="anchor anchor-primary" href="#t0100" name="bt0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0100"><span class="anchor-text-container"><span class="anchor-text">Table 12</span></span></a>. In our literature review, we did not find any work using a cross-database evaluation with the BU-3DFE for expression recognition. The results achieved by the proposed method in the cross-database experiments with the JAFFE database are highlighted in bold.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0100"><span class="captions text-s"><span id="cap0160"><p id="sp0160"><span class="label">Table&nbsp;12</span>. Comparison for the JAFFE cross-database experiment.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col"><strong>Method</strong></th><th scope="col"><strong>Classifier</strong></th><th scope="col"><strong>6-expressions</strong></th><th scope="col"><strong>7-expressions</strong></th></tr></thead><tbody><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">LBP+SVM <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>â€“</td><td>41.30</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">BDBN <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>â€“</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>68.00</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2">Gu et al. <a class="anchor anchor-primary" href="#bib67" name="bbib67" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib67"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a></th><td><em>C</em><sub><em>nclass</em></sub></td><td>55.87</td><td>â€“</td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td>â€“</td><td>â€“</td></tr><tr><th scope="row"><br></th><td><br></td><td><br></td><td><br></td></tr><tr class="valign-top"><th class="valign-top" scope="row" rowspan="2"><strong>Proposed</strong></th><td><em>C</em><sub><em>nclass</em></sub></td><td><strong>38.80</strong></td><td><strong>37.36</strong></td></tr><tr class="valign-top"><th scope="row"><em>C</em><sub><em>bin</em></sub></th><td><strong>79.60</strong></td><td><strong>82.10</strong></td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="p0560">Comparing the presented method with Shan et al. <a class="anchor anchor-primary" href="#bib8" name="bbib8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib8"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a> our accuracy was about 4% smaller using 7 expressions and the <em>C</em><sub><em>nclass</em></sub> classifier. They did not report the result of the six basic expressions. On the other hand, compared with <em>Liu et al.</em> <a class="anchor anchor-primary" href="#bib7" name="bbib7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib7"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a>, using the binary classifier approach the proposed method significantly increases the accuracy, from 68.0% to 82.0%. Ali et al. <a class="anchor anchor-primary" href="#bib13" name="bbib13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib13"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a> also perform a cross-database validation in their work, achieving an accuracy of 48.67% training in the RAFD database and testing with the JAFFE database. Unfortunately, we cannot compare with our results due to the discrepancy in the experimental methodology. The first is the database, in our case the training was carried out with the CK+ database, while theirs with the RAFD database. Secondly, they recognize only five expressions (anger, happy, surprise, sad and fear).</div></section><section id="s0080"><h3 id="sect0095" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.6. Limitations</h3><div class="u-margin-s-bottom" id="p0565">As discussed before, the presented method needs the locations of each eye for the image pre-processing steps. The eye detection can be easily included in the system adopting methods available in the literature <a class="anchor anchor-primary" href="#bib59" name="bbib59" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib59"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a>, <a class="anchor anchor-primary" href="#bib61" name="bbib61" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib61"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a>, while still keeping whole method (including the eyes detection) in real time. In addition, as shown in <a class="anchor anchor-primary" href="#t0005" name="bt0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0005"><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a>, the accuracy of some expressions, e.g. sad, was about 84%, whereas the accuracy of the whole method was about 96%. This suggests that the variation between these classes are not enough to separate them. One approach to address this problem is to create a specialized classifier for those expressions, to be used as a second classifier. Another limitation of the presented method is the controlled environment of the input images with the frontal face of the subjects. All these limitations, could be addressed with a large set of training data, which will allow for a deeper network, capable of handling such constraints.</div></section></section><section id="s0085"><h2 id="sect0100" class="u-h4 u-margin-l-top u-margin-xs-bottom">5. Conclusion</h2><div class="u-margin-s-bottom" id="p0570">In this paper, we propose a facial expression recognition system that uses a combination of standard methods, like Convolutional Neural Network and specific image pre-processing steps. Experiments showed that the combination of the normalization procedures improve significantly the method's accuracy. As shown in the results, in comparison with the recent methods in the literature, that use the same facial expression database and experimental methodology, our method achieves competitive results and presents a simpler solution. In addition, it takes less time to train and its recognition is performed in real time. Finally, the cross-database experiments show that the proposed approach also works in unknown environments, where the testing image acquisition conditions and subjects vary from the training images, but&nbsp;still has room left&nbsp;for improvement.</div><div class="u-margin-s-bottom" id="p0575">As explained in <a class="anchor anchor-primary" href="#s0005" name="bs0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0005"><span class="anchor-text-container"><span class="anchor-text">Section 1</span></span></a><span>, the use of Convolutional Neural Networks aims to decrease the need for hand-coded features. Its input can be raw images, instead of an already selected set of features. It happens because this <a href="/topics/computer-science/neural-network-model" title="Learn more about neural network model from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network model</a> is able to learn the set of features that best models the desired classification. To perform such learning, Convolutional Neural Networks need a large amount of data, that we do not have. This is a constraint of deep architectures, motivated by the large amount of parameters that need adjustment during training. To address this problem (our limited data), the pre-processing operations were applied to the images, in order to decrease the variations between images and to select a subset of the features to be learned, which reduces the need for a large amount of data. If we had a better set of images, with more variation and more samples (millions), these pre-processing operations could not be necessary to achieve the reported accuracy and even the cross-database validation could be improved.</span></div><div class="u-margin-s-bottom" id="p0580"><span>Preliminary experiments were performed with deeper architectures, trained with a big amount of data. In these experiments, a <a href="/topics/computer-science/deep-convolutional-neural-networks" title="Learn more about deep Convolutional Neural Network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep Convolutional Neural Network</a> composed by 38 layers and trained with about 982,800 images from 2662 subjects, proposed by Parkhi et al. </span><a class="anchor anchor-primary" href="#bib83" name="bbib83" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib83"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a><span> to recognize faces, was briefly studied. The already trained model was used as a pre-trained feature extractor plugged as input of a simple two-layered <a href="/topics/computer-science/trained-neural-network" title="Learn more about neural network trained from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network trained</a> with the CK+ database. In this experiment, no preprocessing operation was applied. Despite the experiment simplicity, the results achieved were promising and even increase the accuracy achieved in the cross-database experiments (reported in </span><a class="anchor anchor-primary" href="#s0050" name="bs0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="s0050"><span class="anchor-text-container"><span class="anchor-text">Section 4</span></span></a>), with the cost of decreasing the accuracy in the same database experiments. These results indicate that a deep learning approach of such type can be a better way to produce a discriminative model for facial expression recognition, allowing it to work in uncontrolled scenarios, which is one of the current challenges in this field.</div><div class="u-margin-s-bottom" id="p0585">As future work, the application of this feature extraction method will be investigated in other problems. In addition, we want to investigate other learning methods in order to increase the method robustness in unknown environments (e.g. with varying light conditions, culture and others). Also, more tests will be performed using the face descriptor proposed by Parkhi et al. <a class="anchor anchor-primary" href="#bib83" name="bbib83" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib83"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a><span>, using fine adjustment techniques, which aims to tune an already trained <a href="/topics/computer-science/deep-neural-network" title="Learn more about deep neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep neural network</a> in order to focus on more specific features (in our case, expressions).</span></div></section><section id="s0090"><h2 id="sect0105" class="u-h4 u-margin-l-top u-margin-xs-bottom">Conflict of interest</h2><div class="u-margin-s-bottom" id="p0590">We wish to confirm that there are no known conflicts of interest associated with this work.</div></section></div><section id="ack0005"><h2 id="sect0115" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgment</h2><div class="u-margin-s-bottom" id="p0600">We would like to thank <span id="gs1">Universidade Federal do EspÃ­rito Santos â€“ UFES</span> (project SIEEPEF, <a class="anchor anchor-primary" href="#gs1"><span class="anchor-text-container"><span class="anchor-text">5911/2015</span></span></a>), <span id="gs2">FundaÃ§Ã£o de Amparo Pesquisa do EspÃ­rito Santo â€“ FAPES</span> (grants <a class="anchor anchor-primary" href="#gs2"><span class="anchor-text-container"><span class="anchor-text">65883632/14</span></span></a>, <a class="anchor anchor-primary" href="#gs2"><span class="anchor-text-container"><span class="anchor-text">53631242/11</span></span></a>, and <a class="anchor anchor-primary" href="#gs3"><span class="anchor-text-container"><span class="anchor-text">60902841/13</span></span></a>), <span id="gs3">CoordenaÃ§Ã£o de AperfeiÃ§oamento de Pessoal de NÃ­vel Superior â€“ CAPES</span> (grant <a class="anchor anchor-primary" href="#gs3"><span class="anchor-text-container"><span class="anchor-text">11012/13-7</span></span></a> and scholarship) and <span id="gs4">Conselho Nacional de Desenvolvimento CientÃ­fico e TecnolÃ³gico â€“ CNPq</span> (grants <a class="anchor anchor-primary" href="#gs4"><span class="anchor-text-container"><span class="anchor-text">552630/2011-0</span></span></a> and <a class="anchor anchor-primary" href="#gs4"><span class="anchor-text-container"><span class="anchor-text">312786/2013-1</span></span></a>).</div></section><div class="Appendices"><section id="s0100"><h2 class="u-h4 u-margin-l-top u-margin-xs-bottom">Appendix A. </h2><div class="u-margin-s-bottom"><div id="p0605">See <a class="anchor anchor-primary" href="#t0045" name="bt0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0045"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;13</span></span></a>, <a class="anchor anchor-primary" href="#t0050" name="bt0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0050"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;14</span></span></a>, <a class="anchor anchor-primary" href="#t0060" name="bt0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0060"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;15</span></span></a>, <a class="anchor anchor-primary" href="#t0065" name="bt0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0065"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;16</span></span></a>, <a class="anchor anchor-primary" href="#t0070" name="bt0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0070"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;17</span></span></a>, <a class="anchor anchor-primary" href="#t0075" name="bt0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0075"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;18</span></span></a>, <a class="anchor anchor-primary" href="#t0080" name="bt0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0080"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;19</span></span></a>, <a class="anchor anchor-primary" href="#t0085" name="bt0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="t0085"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;20</span></span></a>.</div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0045"><span class="captions text-s"><span id="cap0105"><p id="sp0105"><span class="label">Table&nbsp;13</span>. Confusion matrix for six expressions on the BU-3DFE database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">109</td><td class="align-char">26</td><td class="align-char">7</td><td class="align-char">2</td><td class="align-char">10</td><td class="align-char">3</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">15</td><td class="align-char">12</td><td class="align-char">14</td><td class="align-char">11</td><td class="align-char">0</td><td class="align-char">7</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">12</td><td class="align-char">21</td><td class="align-char">61</td><td class="align-char">10</td><td class="align-char">9</td><td class="align-char">19</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">0</td><td class="align-char">6</td><td class="align-char">6</td><td class="align-char">159</td><td class="align-char">3</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">17</td><td class="align-char">3</td><td class="align-char">14</td><td class="align-char">5</td><td class="align-char">124</td><td class="align-char">5</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">7</td><td class="align-char">6</td><td class="align-char">6</td><td class="align-char">5</td><td class="align-char">11</td><td class="align-char">134</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0050"><span class="captions text-s"><span id="cap0110"><p id="sp0110"><span class="label">Table&nbsp;14</span>. Confusion matrix for seven expressions on the BU-3DFE database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Neutral</th><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Neutral</th><td class="align-char">191</td><td class="align-char">11</td><td class="align-char">4</td><td class="align-char">5</td><td class="align-char">1</td><td class="align-char">14</td><td class="align-char">6</td></tr><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">22</td><td class="align-char">93</td><td class="align-char">24</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">15</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">0</td><td class="align-char">9</td><td class="align-char">117</td><td class="align-char">20</td><td class="align-char">6</td><td class="align-char">0</td><td class="align-char">7</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">21</td><td class="align-char">3</td><td class="align-char">23</td><td class="align-char">47</td><td class="align-char">6</td><td class="align-char">10</td><td class="align-char">22</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">5</td><td class="align-char">0</td><td class="align-char">6</td><td class="align-char">2</td><td class="align-char">160</td><td class="align-char">0</td><td class="align-char">1</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">28</td><td class="align-char">10</td><td class="align-char">0</td><td class="align-char">8</td><td class="align-char">3</td><td class="align-char">113</td><td class="align-char">6</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">13</td><td class="align-char">0</td><td class="align-char">6</td><td class="align-char">7</td><td class="align-char">6</td><td class="align-char">5</td><td class="align-char">132</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0060"><span class="captions text-s"><span id="cap0120"><p id="sp0120"><span class="label">Table&nbsp;15</span>. Confusion matrix for six expressions on the BU-3DFE database in the cross-database experiment.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">55</td><td class="align-char">30</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">23</td><td class="align-char">49</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">12</td><td class="align-char">57</td><td class="align-char">9</td><td class="align-char">6</td><td class="align-char">5</td><td class="align-char">71</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">4</td><td class="align-char">3</td><td class="align-char">13</td><td class="align-char">12</td><td class="align-char">24</td><td class="align-char">76</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">6</td><td class="align-char">8</td><td class="align-char">5</td><td class="align-char">111</td><td class="align-char">22</td><td class="align-char">22</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">6</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">3</td><td class="align-char">51</td><td class="align-char">105</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">5</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">7</td><td class="align-char">154</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0065"><span class="captions text-s"><span id="cap0125"><p id="sp0125"><span class="label">Table&nbsp;16</span>. Confusion matrix for seven expressions on the BU-3DFE database in the cross-database experiment.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Neutral</th><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Neutral</th><td class="align-char">116</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">4</td><td class="align-char">0</td><td class="align-char">24</td><td class="align-char">87</td></tr><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">77</td><td class="align-char">28</td><td class="align-char">25</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">5</td><td class="align-char">22</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">23</td><td class="align-char">8</td><td class="align-char">62</td><td class="align-char">3</td><td class="align-char">9</td><td class="align-char">10</td><td class="align-char">44</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">29</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">7</td><td class="align-char">11</td><td class="align-char">25</td><td class="align-char">57</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">36</td><td class="align-char">3</td><td class="align-char">8</td><td class="align-char">2</td><td class="align-char">100</td><td class="align-char">14</td><td class="align-char">11</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">21</td><td class="align-char">7</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">3</td><td class="align-char">54</td><td class="align-char">80</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">1</td><td class="align-char">3</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">5</td><td class="align-char">155</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0070"><span class="captions text-s"><span id="cap0130"><p id="sp0130"><span class="label">Table&nbsp;17</span>. Confusion matrix for six expressions on the JAFFE database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">168</td><td class="align-char">32</td><td class="align-char">26</td><td class="align-char">10</td><td class="align-char">32</td><td class="align-char">2</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">79</td><td class="align-char">129</td><td class="align-char">21</td><td class="align-char">1</td><td class="align-char">30</td><td class="align-char">1</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">38</td><td class="align-char">17</td><td class="align-char">98</td><td class="align-char">6</td><td class="align-char">72</td><td class="align-char">57</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">9</td><td class="align-char">6</td><td class="align-char">6</td><td class="align-char">224</td><td class="align-char">19</td><td class="align-char">24</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">53</td><td class="align-char">26</td><td class="align-char">42</td><td class="align-char">14</td><td class="align-char">123</td><td class="align-char">21</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">7</td><td class="align-char">1</td><td class="align-char">61</td><td class="align-char">17</td><td class="align-char">21</td><td class="align-char">163</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0075"><span class="captions text-s"><span id="cap0135"><p id="sp0135"><span class="label">Table&nbsp;18</span>. Confusion matrix for seven expressions on the JAFFE database.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Neutral</th><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Neutral</th><td class="align-char">156</td><td class="align-char">33</td><td class="align-char">3</td><td class="align-char">4</td><td class="align-char">22</td><td class="align-char">8</td><td class="align-char">35</td></tr><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">6</td><td class="align-char">175</td><td class="align-char">17</td><td class="align-char">28</td><td class="align-char">2</td><td class="align-char">39</td><td class="align-char">3</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">5</td><td class="align-char">84</td><td class="align-char">118</td><td class="align-char">12</td><td class="align-char">0</td><td class="align-char">41</td><td class="align-char">1</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">24</td><td class="align-char">30</td><td class="align-char">0</td><td class="align-char">103</td><td class="align-char">0</td><td class="align-char">80</td><td class="align-char">43</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">46</td><td class="align-char">12</td><td class="align-char">1</td><td class="align-char">3</td><td class="align-char">210</td><td class="align-char">6</td><td class="align-char">10</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">8</td><td class="align-char">55</td><td class="align-char">27</td><td class="align-char">42</td><td class="align-char">12</td><td class="align-char">116</td><td class="align-char">19</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">66</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">36</td><td class="align-char">11</td><td class="align-char">6</td><td class="align-char">149</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0080"><span class="captions text-s"><span id="cap0140"><p id="sp0140"><span class="label">Table&nbsp;19</span>. Confusion matrix for six expressions on the JAFFE database in the cross-database experiment.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">2</td><td class="align-char">2</td><td class="align-char">2</td><td class="align-char">2</td><td class="align-char">11</td><td class="align-char">11</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">1</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">7</td><td class="align-char">18</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">9</td><td class="align-char">20</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">2</td><td class="align-char">18</td><td class="align-char">11</td><td class="align-char">0</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">0</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">18</td><td class="align-char">10</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">3</td><td class="align-char">25</td></tr></tbody></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id="t0085"><span class="captions text-s"><span id="cap0145"><p id="sp0145"><span class="label">Table&nbsp;20</span>. Confusion matrix for seven expressions on the JAFFE database in the cross-database experiment.</p></span></span><div class="groups"><table><thead><tr class="valign-top rowsep-1"><td scope="col"><span class="screen-reader-only">Empty Cell</span></td><th scope="col">Neutral</th><th scope="col">Angry</th><th scope="col">Disgust</th><th scope="col">Fear</th><th scope="col">Happy</th><th scope="col">Sad</th><th scope="col">Surprise</th></tr></thead><tbody><tr class="valign-top"><th scope="row">Neutral</th><td class="align-char">4</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">20</td><td class="align-char">5</td></tr><tr class="valign-top"><th scope="row">Angry</th><td class="align-char">5</td><td class="align-char">4</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">10</td><td class="align-char">6</td></tr><tr class="valign-top"><th scope="row">Disgust</th><td class="align-char">3</td><td class="align-char">2</td><td class="align-char">4</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">7</td><td class="align-char">12</td></tr><tr class="valign-top"><th scope="row">Fear</th><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">3</td><td class="align-char">0</td><td class="align-char">9</td><td class="align-char">20</td></tr><tr class="valign-top"><th scope="row">Happy</th><td class="align-char">2</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">2</td><td class="align-char">17</td><td class="align-char">8</td><td class="align-char">1</td></tr><tr class="valign-top"><th scope="row">Sad</th><td class="align-char">1</td><td class="align-char">2</td><td class="align-char">2</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">18</td><td class="align-char">7</td></tr><tr class="valign-top"><th scope="row">Surprise</th><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">0</td><td class="align-char">0</td><td class="align-char">1</td><td class="align-char">2</td><td class="align-char">26</td></tr></tbody></table></div></div></div></section></div></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="bibliog0005"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="bb0005"><ol class="references" id="reference-links-bb0005"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib1" id="ref-id-bib1" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a></span><span class="reference" id="othref0005"><div class="other-ref"><span>Y. Wu, H. Liu, H. Zha, Modeling facial expression space for recognition, in: 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, 2005 (IROS 2005), 2005, pp. 1968â€“1973.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Y.%20Wu%2C%20H.%20Liu%2C%20H.%20Zha%2C%20Modeling%20facial%20expression%20space%20for%20recognition%2C%20in%3A%202005%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%2C%202005%20(IROS%202005)%2C%202005%2C%20pp.%201968%E2%80%931973." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib2" id="ref-id-bib2" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a></span><span class="reference" id="othref0010"><div class="other-ref"><span>C. Darwin, The Expression of the Emotions in Man and Animals, CreateSpace Independent Publishing Platform, 2012.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=C.%20Darwin%2C%20The%20Expression%20of%20the%20Emotions%20in%20Man%20and%20Animals%2C%20CreateSpace%20Independent%20Publishing%20Platform%2C%202012." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib3" id="ref-id-bib3" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></span><span class="reference" id="othref0015"><div class="other-ref"><span>S.Z. Li, A.K. Jain, Handbook of Face Recognition, Springer Science &amp; Business Media, Secaucus, NJ, USA, 2011.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.Z.%20Li%2C%20A.K.%20Jain%2C%20Handbook%20of%20Face%20Recognition%2C%20Springer%20Science%20%26%20Business%20Media%2C%20Secaucus%2C%20NJ%2C%20USA%2C%202011." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib4" id="ref-id-bib4" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a></span><span class="reference" id="othref0020"><div class="other-ref"><span>P. Lucey, J. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews, The extended Cohnâ€“Kanade dataset (CK+): a complete dataset for action unit and emotion-specified expression, in: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2010, pp. 94â€“101.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Lucey%2C%20J.%20Cohn%2C%20T.%20Kanade%2C%20J.%20Saragih%2C%20Z.%20Ambadar%2C%20I.%20Matthews%2C%20The%20extended%20Cohn%E2%80%93Kanade%20dataset%20(CK%2B)%3A%20a%20complete%20dataset%20for%20action%20unit%20and%20emotion-specified%20expression%2C%20in%3A%202010%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20Workshops%20(CVPRW)%2C%202010%2C%20pp.%2094%E2%80%93101." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib5" id="ref-id-bib5" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a></span><span class="reference" id="sbref1"><div class="contribution"><div class="authors u-font-sans">M. Lyons, J. Budynek, S. Akamatsu</div><div id="ref-id-sbref1" class="title text-m">Automatic classification of single facial images</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 21 (12) (1999), pp. 1357-1362</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033352182&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref1"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20classification%20of%20single%20facial%20images&amp;publication_year=1999&amp;author=M.%20Lyons&amp;author=J.%20Budynek&amp;author=S.%20Akamatsu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref1"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib6" id="ref-id-bib6" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a></span><span class="reference" id="othref0025"><div class="other-ref"><span>L. Yin, X. Wei, Y. Sun, J. Wang, M. Rosato, A 3d facial expression database for facial behavior research, in: 7th International Conference on Automatic Face and Gesture Recognition (FGR06), Institute of Electrical &amp; Electronics Engineers (IEEE), Southampton, UK, 2006.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=L.%20Yin%2C%20X.%20Wei%2C%20Y.%20Sun%2C%20J.%20Wang%2C%20M.%20Rosato%2C%20A%203d%20facial%20expression%20database%20for%20facial%20behavior%20research%2C%20in%3A%207th%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20(FGR06)%2C%20Institute%20of%20Electrical%20%26%20Electronics%20Engineers%20(IEEE)%2C%20Southampton%2C%20UK%2C%202006." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib7" id="ref-id-bib7" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></span><span class="reference" id="othref0030"><div class="other-ref"><span>P. Liu, S. Han, Z. Meng, Y. Tong, Facial expression recognition via a boosted deep belief network, in: 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 1805â€“1812.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Liu%2C%20S.%20Han%2C%20Z.%20Meng%2C%20Y.%20Tong%2C%20Facial%20expression%20recognition%20via%20a%20boosted%20deep%20belief%20network%2C%20in%3A%202014%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20(CVPR)%2C%202014%2C%20pp.%201805%E2%80%931812." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib8" id="ref-id-bib8" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></span><span class="reference" id="sbref2"><div class="contribution"><div class="authors u-font-sans">C. Shan, S. Gong, P.W. McOwan</div><div id="ref-id-sbref2" class="title text-m">Facial expression recognition based on local binary patterns: a comprehensive study</div></div><div class="host u-font-sans">Image Vis. Comput., 27 (6) (2009), pp. 803-816</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0262885608001844/pdfft?md5=f193c98d97c488b5b841cc25dd4c0b50&amp;pid=1-s2.0-S0262885608001844-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref2"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0262885608001844" aria-describedby="ref-id-sbref2"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-63449136395&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref2"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%20local%20binary%20patterns%3A%20a%20comprehensive%20study&amp;publication_year=2009&amp;author=C.%20Shan&amp;author=S.%20Gong&amp;author=P.W.%20McOwan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref2"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib9" id="ref-id-bib9" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a></span><span class="reference" id="othref0035"><div class="other-ref"><span>W. Liu, C. Song, Y. Wang, Facial expression recognition based on discriminative dictionary learning, in: 2012 21st International Conference on Pattern Recognition (ICPR), 2012, pp. 1839â€“1842.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=W.%20Liu%2C%20C.%20Song%2C%20Y.%20Wang%2C%20Facial%20expression%20recognition%20based%20on%20discriminative%20dictionary%20learning%2C%20in%3A%202012%2021st%20International%20Conference%20on%20Pattern%20Recognition%20(ICPR)%2C%202012%2C%20pp.%201839%E2%80%931842." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib10" id="ref-id-bib10" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a></span><span class="reference" id="othref0040"><div class="other-ref"><span>I. Song, H.-J. Kim, P.B. Jeon, Deep learning for real-time robust facial expression recognition on a smartphone, in: International Conference on Consumer Electronics (ICCE), Institute of Electrical &amp; Electronics Engineers (IEEE), Las Vegas, NV, USA, 2014.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=I.%20Song%2C%20H.-J.%20Kim%2C%20P.B.%20Jeon%2C%20Deep%20learning%20for%20real-time%20robust%20facial%20expression%20recognition%20on%20a%20smartphone%2C%20in%3A%20International%20Conference%20on%20Consumer%20Electronics%20(ICCE)%2C%20Institute%20of%20Electrical%20%26%20Electronics%20Engineers%20(IEEE)%2C%20Las%20Vegas%2C%20NV%2C%20USA%2C%202014." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib11" id="ref-id-bib11" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a></span><span class="reference" id="othref0045"><div class="other-ref"><span>P. Burkert, F. Trier, M.Z. Afzal, A. Dengel, M. Liwicki, Dexpression: Deep Convolutional Neural Network for Expression Recognition, CoRR abs/1509.05371 (URL ã€ˆ<a class="anchor anchor-primary" href="http://arxiv.org/abs/1509.05371" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/1509.05371</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>ã€‰).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Burkert%2C%20F.%20Trier%2C%20M.Z.%20Afzal%2C%20A.%20Dengel%2C%20M.%20Liwicki%2C%20Dexpression%3A%20Deep%20Convolutional%20Neural%20Network%20for%20Expression%20Recognition%2C%20CoRR%20abs%2F1509.05371%20(URL%20%E3%80%88http%3A%2F%2Farxiv.org%2Fabs%2F1509.05371%E3%80%89)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib12" id="ref-id-bib12" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></span><span class="reference" id="sbref3"><div class="contribution"><div class="authors u-font-sans">M. Liu, S. Li, S. Shan, X. Chen</div><div id="ref-id-sbref3" class="title text-m">Au-inspired deep networks for facial expression feature learning</div></div><div class="host u-font-sans">Neurocomputing, 159 (2015), pp. 126-136, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2015.02.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2015.02.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231215001605/pdfft?md5=88aa205d563f728728330b60e4c22aec&amp;pid=1-s2.0-S0925231215001605-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref3"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231215001605" aria-describedby="ref-id-sbref3"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84933280308&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref3"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Au-inspired%20deep%20networks%20for%20facial%20expression%20feature%20learning&amp;publication_year=2015&amp;author=M.%20Liu&amp;author=S.%20Li&amp;author=S.%20Shan&amp;author=X.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref3"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib13" id="ref-id-bib13" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></span><span class="reference" id="sbref4"><div class="contribution"><div class="authors u-font-sans">G. Ali, M.A. Iqbal, T.-S. Choi</div><div id="ref-id-sbref4" class="title text-m">Boosted NNE collections for multicultural facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit., 55 (2016), pp. 14-27, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2016.01.032" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2016.01.032</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320316000534/pdfft?md5=288b0abf1f3aef5d496e7ac31e432bd2&amp;pid=1-s2.0-S0031320316000534-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref4"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320316000534" aria-describedby="ref-id-sbref4"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84969352788&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref4"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Boosted%20NNE%20collections%20for%20multicultural%20facial%20expression%20recognition&amp;publication_year=2016&amp;author=G.%20Ali&amp;author=M.A.%20Iqbal&amp;author=T.-S.%20Choi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref4"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib14" id="ref-id-bib14" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a></span><span class="reference" id="othref0050"><div class="other-ref"><span>Y.-H. Byeon, K.-C. Kwak, Facial expression recognition using 3d convolutional neural network. International Journal of Advanced Computer Science and Applications(IJACSA), 5 (2014).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Y.-H.%20Byeon%2C%20K.-C.%20Kwak%2C%20Facial%20expression%20recognition%20using%203d%20convolutional%20neural%20network.%20International%20Journal%20of%20Advanced%20Computer%20Science%20and%20Applications(IJACSA)%2C%205%20(2014)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib15" id="ref-id-bib15" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a></span><span class="reference" id="othref0055"><div class="other-ref"><span>J.-J.J. Lien, T. Kanade, J. Cohn, C. Li, Detection, tracking, and classification of action units in facial expression, J. Robot. Auton. Syst. 31(3), 2000, 131-146</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.-J.J.%20Lien%2C%20T.%20Kanade%2C%20J.%20Cohn%2C%20C.%20Li%2C%20Detection%2C%20tracking%2C%20and%20classification%20of%20action%20units%20in%20facial%20expression%2C%20J.%20Robot.%20Auton.%20Syst.%2031(3)%2C%202000%2C%20131-146" target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib16" id="ref-id-bib16" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></span><span class="reference" id="sbref5"><div class="contribution"><div class="authors u-font-sans">X. Fan, T. Tjahjadi</div><div id="ref-id-sbref5" class="title text-m">A spatialâ€“temporal framework based on histogram of gradients and optical flow for facial expression recognition in video sequences</div></div><div class="host u-font-sans">Pattern Recognit., 48 (11) (2015), pp. 3407-3416</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320315001648/pdfft?md5=574d3e1fff0e6bd89e82e8090bd61700&amp;pid=1-s2.0-S0031320315001648-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref5"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320315001648" aria-describedby="ref-id-sbref5"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84937818516&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref5"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20spatialtemporal%20framework%20based%20on%20histogram%20of%20gradients%20and%20optical%20flow%20for%20facial%20expression%20recognition%20in%20video%20sequences&amp;publication_year=2015&amp;author=X.%20Fan&amp;author=T.%20Tjahjadi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref5"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib17" id="ref-id-bib17" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a></span><span class="reference" id="sbref6"><div class="contribution"><div class="authors u-font-sans">W. Zhang, Y. Zhang, L. Ma, J. Guan, S. Gong</div><div id="ref-id-sbref6" class="title text-m">Multimodal learning for facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit., 48 (10) (2015), pp. 3191-3202</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S003132031500151X/pdfft?md5=f6d72b12453b5adc205c7c8d1b6b3238&amp;pid=1-s2.0-S003132031500151X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref6"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S003132031500151X" aria-describedby="ref-id-sbref6"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84931567876&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref6"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20learning%20for%20facial%20expression%20recognition&amp;publication_year=2015&amp;author=W.%20Zhang&amp;author=Y.%20Zhang&amp;author=L.%20Ma&amp;author=J.%20Guan&amp;author=S.%20Gong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref6"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib18" id="ref-id-bib18" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a></span><span class="reference" id="sbref7"><div class="contribution"><div class="authors u-font-sans">C.-R. Chen, W.-S. Wong, C.-T. Chiu</div><div id="ref-id-sbref7" class="title text-m">A 0.64&nbsp;mm real-time cascade face detection design based on reduced two-field extraction</div></div><div class="host u-font-sans">IEEE Trans. Very Large Scale Integr. (VLSI) Syst., 19 (11) (2011), pp. 1937-1948</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052880084&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref7"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%200.64mm%20real-time%20cascade%20face%20detection%20design%20based%20on%20reduced%20two-field%20extraction&amp;publication_year=2011&amp;author=C.-R.%20Chen&amp;author=W.-S.%20Wong&amp;author=C.-T.%20Chiu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref7"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib19" id="ref-id-bib19" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a></span><span class="reference" id="sbref8"><div class="contribution"><div class="authors u-font-sans">C. Garcia, M. Delakis</div><div id="ref-id-sbref8" class="title text-m">Convolutional face finder: a neural architecture for fast and robust face detection</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 26 (11) (2004), pp. 1408-1423, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2004.97" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2004.97</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-12844263684&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref8"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Convolutional%20face%20finder%3A%20a%20neural%20architecture%20for%20fast%20and%20robust%20face%20detection&amp;publication_year=2004&amp;author=C.%20Garcia&amp;author=M.%20Delakis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref8"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib20" id="ref-id-bib20" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a></span><span class="reference" id="sbref9"><div class="contribution"><div class="authors u-font-sans">Z. Zhang, D. Yi, Z. Lei, S. Li</div><div id="ref-id-sbref9" class="title text-m">Regularized transfer boosting for face detection across spectrum</div></div><div class="host u-font-sans">IEEE Signal Process. Lett., 19 (3) (2012), pp. 131-134</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Regularized%20transfer%20boosting%20for%20face%20detection%20across%20spectrum&amp;publication_year=2012&amp;author=Z.%20Zhang&amp;author=D.%20Yi&amp;author=Z.%20Lei&amp;author=S.%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref9"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib21" id="ref-id-bib21" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a></span><span class="reference" id="othref0060"><div class="other-ref"><span>M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, J. Movellan, Recognizing facial expression: machine learning and application to spontaneous behavior, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005 (CVPR 2005), vol. 2, 2005, pp. 568â€“573.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Bartlett%2C%20G.%20Littlewort%2C%20M.%20Frank%2C%20C.%20Lainscsek%2C%20I.%20Fasel%2C%20J.%20Movellan%2C%20Recognizing%20facial%20expression%3A%20machine%20learning%20and%20application%20to%20spontaneous%20behavior%2C%20in%3A%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202005%20(CVPR%202005)%2C%20vol.%202%2C%202005%2C%20pp.%20568%E2%80%93573." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib22" id="ref-id-bib22" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></span><span class="reference" id="othref0065"><div class="other-ref"><span>P. Liu, M. Reale, L. Yin, 3d head pose estimation based on scene flow and generic head model, in: 2012 IEEE International Conference on Multimedia and Expo (ICME), 2012, pp. 794â€“799.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Liu%2C%20M.%20Reale%2C%20L.%20Yin%2C%203d%20head%20pose%20estimation%20based%20on%20scene%20flow%20and%20generic%20head%20model%2C%20in%3A%202012%20IEEE%20International%20Conference%20on%20Multimedia%20and%20Expo%20(ICME)%2C%202012%2C%20pp.%20794%E2%80%93799." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib23" id="ref-id-bib23" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></span><span class="reference" id="othref0070"><div class="other-ref"><span>W.W. Kim, S. Park, J. Hwang, S. Lee, Automatic head pose estimation from a single camera using projective geometry, in: 2011 8th International Conference on Information, Communications and Signal Processing (ICICS), 2011, pp. 1â€“5.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=W.W.%20Kim%2C%20S.%20Park%2C%20J.%20Hwang%2C%20S.%20Lee%2C%20Automatic%20head%20pose%20estimation%20from%20a%20single%20camera%20using%20projective%20geometry%2C%20in%3A%202011%208th%20International%20Conference%20on%20Information%2C%20Communications%20and%20Signal%20Processing%20(ICICS)%2C%202011%2C%20pp.%201%E2%80%935." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib24" id="ref-id-bib24" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a></span><span class="reference" id="othref0075"><div class="other-ref"><span>M. Demirkus, D. Precup, J. Clark, T. Arbel, Multi-layer temporal graphical model for head pose estimation in real-world videos, in: 2014 IEEE International Conference on Image Processing (ICIP), 2014, pp. 3392â€“3396.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Demirkus%2C%20D.%20Precup%2C%20J.%20Clark%2C%20T.%20Arbel%2C%20Multi-layer%20temporal%20graphical%20model%20for%20head%20pose%20estimation%20in%20real-world%20videos%2C%20in%3A%202014%20IEEE%20International%20Conference%20on%20Image%20Processing%20(ICIP)%2C%202014%2C%20pp.%203392%E2%80%933396." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib25" id="ref-id-bib25" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a></span><span class="reference" id="othref0080"><div class="other-ref"><span>Z. Zhang, M. Lyons, M. Schuster, S. Akamatsu, Comparison between geometry-based and gabor-wavelets-based facial expression recognition using multi-layer perceptron, in: 1998 Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition, 1998, pp. 454â€“459.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Z.%20Zhang%2C%20M.%20Lyons%2C%20M.%20Schuster%2C%20S.%20Akamatsu%2C%20Comparison%20between%20geometry-based%20and%20gabor-wavelets-based%20facial%20expression%20recognition%20using%20multi-layer%20perceptron%2C%20in%3A%201998%20Proceedings%20of%20the%20Third%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%2C%201998%2C%20pp.%20454%E2%80%93459." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib26" id="ref-id-bib26" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a></span><span class="reference" id="othref0085"><div class="other-ref"><span>P. Yang, Q. Liu, D. Metaxas, Boosting coded dynamic features for facial action units and facial expression recognition, in: IEEE Conference on Computer Vision and Pattern Recognition, 2007 (CVPRâ€™07), 2007, pp. 1â€“6.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Yang%2C%20Q.%20Liu%2C%20D.%20Metaxas%2C%20Boosting%20coded%20dynamic%20features%20for%20facial%20action%20units%20and%20facial%20expression%20recognition%2C%20in%3A%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202007%20(CVPR%E2%80%9907)%2C%202007%2C%20pp.%201%E2%80%936." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib27" id="ref-id-bib27" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></span><span class="reference" id="othref0090"><div class="other-ref"><span>S. Jain, C. Hu, J. Aggarwal, Facial expression recognition with temporal modeling of shapes, in: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 2011, pp. 1642â€“1649, <a class="anchor anchor-primary" href="http://dx.doi.org/10.1109/ICCVW.2011.6130446" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCVW.2011.6130446</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Jain%2C%20C.%20Hu%2C%20J.%20Aggarwal%2C%20Facial%20expression%20recognition%20with%20temporal%20modeling%20of%20shapes%2C%20in%3A%202011%20IEEE%20International%20Conference%20on%20Computer%20Vision%20Workshops%20(ICCV%20Workshops)%2C%202011%2C%20pp.%201642%E2%80%931649%2C%2010.1109%2FICCVW.2011.6130446." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib28" id="ref-id-bib28" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a></span><span class="reference" id="sbref10"><div class="contribution"><div class="authors u-font-sans">Y. Lin, M. Song, D.T.P. Quynh, Y. He, C. Chen</div><div id="ref-id-sbref10" class="title text-m">Sparse coding for flexible, robust 3d facial-expression synthesis</div></div><div class="host u-font-sans">IEEE Comput. Graph. Appl., 32 (2) (2012), pp. 76-88</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84863125366&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref10"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sparse%20coding%20for%20flexible%2C%20robust%203d%20facial-expression%20synthesis&amp;publication_year=2012&amp;author=Y.%20Lin&amp;author=M.%20Song&amp;author=D.T.P.%20Quynh&amp;author=Y.%20He&amp;author=C.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref10"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib29" id="ref-id-bib29" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a></span><span class="reference" id="othref0095"><div class="other-ref"><span>S. Rifai, Y. Bengio, A. Courville, P. Vincent, M. Mirza, Disentangling factors of variation for facial expression recognition, in: A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato, C. Schmid (Eds.), Computer Vision â€“ ECCV 2012, Lecture Notes in Computer Science, vol. 7577, Springer, Berlin Heidelberg, 2012, pp. 808â€“822.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Rifai%2C%20Y.%20Bengio%2C%20A.%20Courville%2C%20P.%20Vincent%2C%20M.%20Mirza%2C%20Disentangling%20factors%20of%20variation%20for%20facial%20expression%20recognition%2C%20in%3A%20A.%20Fitzgibbon%2C%20S.%20Lazebnik%2C%20P.%20Perona%2C%20Y.%20Sato%2C%20C.%20Schmid%20(Eds.)%2C%20Computer%20Vision%20%E2%80%93%20ECCV%202012%2C%20Lecture%20Notes%20in%20Computer%20Science%2C%20vol.%207577%2C%20Springer%2C%20Berlin%20Heidelberg%2C%202012%2C%20pp.%20808%E2%80%93822." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib30" id="ref-id-bib30" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a></span><span class="reference" id="othref0100"><div class="other-ref"><span>B. Fasel, Robust face analysis using convolutional neural networks, in: Proceedings of the 16th International Conference on Pattern Recognition, 2002, vol. 2, 2002, pp. 40â€“43.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=B.%20Fasel%2C%20Robust%20face%20analysis%20using%20convolutional%20neural%20networks%2C%20in%3A%20Proceedings%20of%20the%2016th%20International%20Conference%20on%20Pattern%20Recognition%2C%202002%2C%20vol.%202%2C%202002%2C%20pp.%2040%E2%80%9343." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib31" id="ref-id-bib31" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a></span><span class="reference" id="othref0105"><div class="other-ref"><span>F. Beat, Head-pose invariant facial expression recognition using convolutional neural networks, in: Proceedings of the Fourth IEEE International Conference on Multimodal Interfaces, 2002, 2002, pp. 529â€“534.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=F.%20Beat%2C%20Head-pose%20invariant%20facial%20expression%20recognition%20using%20convolutional%20neural%20networks%2C%20in%3A%20Proceedings%20of%20the%20Fourth%20IEEE%20International%20Conference%20on%20Multimodal%20Interfaces%2C%202002%2C%202002%2C%20pp.%20529%E2%80%93534." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib32" id="ref-id-bib32" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a></span><span class="reference" id="sbref11"><div class="contribution"><div class="authors u-font-sans">M. Matsugu, K. Mori, Y. Mitari, Y. Kaneda</div><div id="ref-id-sbref11" class="title text-m">Subject independent facial expression recognition with robust face detection using a convolutional neural network</div></div><div class="host u-font-sans">Neural Netw.: Off. J. Int. Neural Netw. Soc., 16 (5) (2003), pp. 555-559</div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0893608003001151/pdfft?md5=fe80ce0a82acc47c9ce0b687973c438f&amp;pid=1-s2.0-S0893608003001151-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref11"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0893608003001151" aria-describedby="ref-id-sbref11"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037707305&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref11"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Subject%20independent%20facial%20expression%20recognition%20with%20robust%20face%20detection%20using%20a%20convolutional%20neural%20network&amp;publication_year=2003&amp;author=M.%20Matsugu&amp;author=K.%20Mori&amp;author=Y.%20Mitari&amp;author=Y.%20Kaneda" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref11"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib33" id="ref-id-bib33" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a></span><span class="reference" id="othref0110"><div class="other-ref"><span>Y. Bengio, Y. LeCun, Scaling learning algorithms towards AI, in: L. Bottou, O. Chapelle, D. DeCoste, J. Weston (Eds.), Large-Scale Kernel Machines, MIT Press, Cambridge, Massachusetts, USA, 2007 (URL ã€ˆ<a class="anchor anchor-primary" href="http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a>ã€‰).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Y.%20Bengio%2C%20Y.%20LeCun%2C%20Scaling%20learning%20algorithms%20towards%20AI%2C%20in%3A%20L.%20Bottou%2C%20O.%20Chapelle%2C%20D.%20DeCoste%2C%20J.%20Weston%20(Eds.)%2C%20Large-Scale%20Kernel%20Machines%2C%20MIT%20Press%2C%20Cambridge%2C%20Massachusetts%2C%20USA%2C%202007%20(URL%20%E3%80%88http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Fbengio-lecun-07.pdf%E3%80%89)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib34" id="ref-id-bib34" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></span><span class="reference" id="sbref12"><div class="contribution"><div class="authors u-font-sans">P.E. Utgoff, D.J. Stracuzzi</div><div id="ref-id-sbref12" class="title text-m">Many-layered learning</div></div><div class="host u-font-sans">Neural Comput., 14 (10) (2002), pp. 2497-2529</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0036782663&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref12"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Many-layered%20learning&amp;publication_year=2002&amp;author=P.E.%20Utgoff&amp;author=D.J.%20Stracuzzi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref12"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib35" id="ref-id-bib35" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></span><span class="reference" id="othref0115"><div class="other-ref"><span>Y. Bengio, I.J. Goodfellow, A. Courville, Deep Learning, MIT Press, Cambridge, Massachusetts, USA, 2015.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Y.%20Bengio%2C%20I.J.%20Goodfellow%2C%20A.%20Courville%2C%20Deep%20Learning%2C%20MIT%20Press%2C%20Cambridge%2C%20Massachusetts%2C%20USA%2C%202015." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib36" id="ref-id-bib36" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></span><span class="reference" id="sbref13"><div class="contribution"><div class="authors u-font-sans">Y. Lecun, L. Bottou, Y. Bengio, P. Haffner</div></div><div class="host u-font-sans" id="ref-id-sbref13">Gradient-based Learn. Appl. Doc. Recognit., 86 (11) (1998), pp. 2278-2324, <a class="anchor anchor-primary" href="https://doi.org/10.1109/5.726791" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/5.726791</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib37" id="ref-id-bib37" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a></span><span class="reference" id="othref0120"><div class="other-ref"><span>D.C. Cirean, U. Meier, J. Masci, L.M. Gambardella, J. Schmidhuber, Flexible, high performance convolutional neural networks for image classification, in: Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI'11), vol. 2, AAAI Press, Barcelona, Catalonia, Spain, 2011, pp. 1237â€“1242.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=D.C.%20Cirean%2C%20U.%20Meier%2C%20J.%20Masci%2C%20L.M.%20Gambardella%2C%20J.%20Schmidhuber%2C%20Flexible%2C%20high%20performance%20convolutional%20neural%20networks%20for%20image%20classification%2C%20in%3A%20Proceedings%20of%20the%20Twenty-Second%20International%20Joint%20Conference%20on%20Artificial%20Intelligence%20(IJCAI'11)%2C%20vol.%202%2C%20AAAI%20Press%2C%20Barcelona%2C%20Catalonia%2C%20Spain%2C%202011%2C%20pp.%201237%E2%80%931242." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib38" id="ref-id-bib38" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a></span><span class="reference" id="othref0125"><div class="other-ref"><span>P. Zhao-yi, W. Zhi-qiang, Z. Yu, Application of mean shift algorithm in real-time facial expression recognition, in: International Symposium on Computer Network and Multimedia Technology, 2009 (CNMT 2009), 2009, pp. 1â€“4.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Zhao-yi%2C%20W.%20Zhi-qiang%2C%20Z.%20Yu%2C%20Application%20of%20mean%20shift%20algorithm%20in%20real-time%20facial%20expression%20recognition%2C%20in%3A%20International%20Symposium%20on%20Computer%20Network%20and%20Multimedia%20Technology%2C%202009%20(CNMT%202009)%2C%202009%2C%20pp.%201%E2%80%934." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib39" id="ref-id-bib39" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a></span><span class="reference" id="sbref14"><div class="contribution"><div class="authors u-font-sans">H.Y. Patil, A.G. Kothari, K.M. Bhurchandi</div><div id="ref-id-sbref14" class="title text-m">Expression invariant face recognition using local binary patterns and contourlet transform</div></div><div class="host u-font-sans">Opt.-Int. J. Light Electron Opt., 127 (5) (2016), pp. 2670-2678, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ijleo.2015.11.187" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ijleo.2015.11.187</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0030402615018203/pdfft?md5=c6afe073397164f65f90520db419241e&amp;pid=1-s2.0-S0030402615018203-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref14"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0030402615018203" aria-describedby="ref-id-sbref14"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959010208&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref14"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Expression%20invariant%20face%20recognition%20using%20local%20binary%20patterns%20and%20contourlet%20transform&amp;publication_year=2016&amp;author=H.Y.%20Patil&amp;author=A.G.%20Kothari&amp;author=K.M.%20Bhurchandi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref14"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib40" id="ref-id-bib40" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></span><span class="reference" id="othref0130"><div class="other-ref"><span>J.Y.R. Cornejo, H. Pedrini, F. Florez-Revuelta, Facial expression recognition with occlusions based on geometric representation, in: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: Proceedings of the 20th Iberoamerican Congress (CIARP 2015), Montevideo, Uruguay, November 9â€“12, 2015,&nbsp;Springer International Publishing, Cham, 2015,&nbsp;pp. 263â€“270.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.Y.R.%20Cornejo%2C%20H.%20Pedrini%2C%20F.%20Florez-Revuelta%2C%20Facial%20expression%20recognition%20with%20occlusions%20based%20on%20geometric%20representation%2C%20in%3A%20Progress%20in%20Pattern%20Recognition%2C%20Image%20Analysis%2C%20Computer%20Vision%2C%20and%20Applications%3A%20Proceedings%20of%20the%2020th%20Iberoamerican%20Congress%20(CIARP%202015)%2C%20Montevideo%2C%20Uruguay%2C%20November%209%E2%80%9312%2C%202015%2C%C2%A0Springer%20International%20Publishing%2C%20Cham%2C%202015%2C%C2%A0pp.%20263%E2%80%93270." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib41" id="ref-id-bib41" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></span><span class="reference" id="sbref15"><div class="contribution"><div class="authors u-font-sans">Z. Wang, Q. Ruan, G. An</div><div id="ref-id-sbref15" class="title text-m">Facial expression recognition using sparse local fisher discriminant analysis</div></div><div class="host u-font-sans">Neurocomputing, 174 (Part B) (2016), pp. 756-766, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2015.09.083" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2015.09.083</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231215014162/pdfft?md5=56b8718d9eb7cc3b9acd3436cd5ac84d&amp;pid=1-s2.0-S0925231215014162-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref15"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231215014162" aria-describedby="ref-id-sbref15"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84949682126&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref15"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20sparse%20local%20fisher%20discriminant%20analysis&amp;publication_year=2016&amp;author=Z.%20Wang&amp;author=Q.%20Ruan&amp;author=G.%20An" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref15"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib42" id="ref-id-bib42" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></span><span class="reference" id="othref0135"><div class="other-ref"><span>S. Arivazhagan, R.A. Priyadharshini, S. Sowmiya, Facial expression recognition based on local directional number pattern and anfis classifier, in: 2014 International Conference on Communication and Network Technologies (ICCNT),&nbsp;2014, pp. 62â€“67 (<a class="anchor anchor-primary" href="https://doi.org/10.1109/CNT.2014.7062726" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1109/CNT.2014.7062726).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Arivazhagan%2C%20R.A.%20Priyadharshini%2C%20S.%20Sowmiya%2C%20Facial%20expression%20recognition%20based%20on%20local%20directional%20number%20pattern%20and%20anfis%20classifier%2C%20in%3A%202014%20International%20Conference%20on%20Communication%20and%20Network%20Technologies%20(ICCNT)%2C%C2%A02014%2C%20pp.%2062%E2%80%9367%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FCNT.2014.7062726)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib43" id="ref-id-bib43" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></span><span class="reference" id="sbref16"><div class="contribution"><div class="authors u-font-sans">A.R. Rivera, J.R. Castillo, O.O. Chae</div><div id="ref-id-sbref16" class="title text-m">Local directional number pattern for face analysis: face and expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 22 (5) (2013), pp. 1740-1752</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84883711470&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref16"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Local%20directional%20number%20pattern%20for%20face%20analysis%3A%20face%20and%20expression%20recognition&amp;publication_year=2013&amp;author=A.R.%20Rivera&amp;author=J.R.%20Castillo&amp;author=O.O.%20Chae" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref16"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib44" id="ref-id-bib44" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></span><span class="reference" id="sbref17"><div class="contribution"><div class="authors u-font-sans">T.H. Zavaschi, A.S. Britto, L.E. Oliveira, A.L. Koerich</div><div id="ref-id-sbref17" class="title text-m">Fusion of feature sets and classifiers for facial expression recognition</div></div><div class="host u-font-sans">Expert Syst. Appl., 40 (2) (2013), pp. 646-655, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2012.07.074" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2012.07.074</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S095741741200930X/pdfft?md5=ecd92a8345b3c5edc167132213b87d3b&amp;pid=1-s2.0-S095741741200930X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref17"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S095741741200930X" aria-describedby="ref-id-sbref17"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84867676745&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref17"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fusion%20of%20feature%20sets%20and%20classifiers%20for%20facial%20expression%20recognition&amp;publication_year=2013&amp;author=T.H.%20Zavaschi&amp;author=A.S.%20Britto&amp;author=L.E.%20Oliveira&amp;author=A.L.%20Koerich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref17"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib45" id="ref-id-bib45" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></span><span class="reference" id="othref0140"><div class="other-ref"><span>A.T. Lopes, E. de Aguiar, T.O. Santos, A facial expression recognition system using convolutional networks, in: 2015 28th SIBGRAPI Conference on Graphics, Patterns and Images, Institute of Electrical &amp; Electronics Engineers (IEEE), Salvador, Bahia, Brasil, 2015.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.T.%20Lopes%2C%20E.%20de%20Aguiar%2C%20T.O.%20Santos%2C%20A%20facial%20expression%20recognition%20system%20using%20convolutional%20networks%2C%20in%3A%202015%2028th%20SIBGRAPI%20Conference%20on%20Graphics%2C%20Patterns%20and%20Images%2C%20Institute%20of%20Electrical%20%26%20Electronics%20Engineers%20(IEEE)%2C%20Salvador%2C%20Bahia%2C%20Brasil%2C%202015." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib46" id="ref-id-bib46" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a></span><span class="reference" id="othref0145"><div class="other-ref"><span>S. Demyanov, J. Bailey, R. Kotagiri, C. Leckie, Invariant Backpropagation: How To Train a Transformation-Invariant Neural Network (<a class="anchor anchor-primary" href="http://arXiv:1502.04434/physics.ins-det/" target="_blank"><span class="anchor-text-container"><span class="anchor-text">arXiv:1502.04434[cs, stat]).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Demyanov%2C%20J.%20Bailey%2C%20R.%20Kotagiri%2C%20C.%20Leckie%2C%20Invariant%20Backpropagation%3A%20How%20To%20Train%20a%20Transformation-Invariant%20Neural%20Network%20(arXiv%3A1502.04434%5Bcs%2C%20stat%5D)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib47" id="ref-id-bib47" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></span><span class="reference" id="othref0150"><div class="other-ref"><span>Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, Caffe: Convolutional Architecture for Fast Feature Embedding (<a class="anchor anchor-primary" href="http://arXiv:1408.5093" target="_blank"><span class="anchor-text-container"><span class="anchor-text">arXiv:1408.5093).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Y.%20Jia%2C%20E.%20Shelhamer%2C%20J.%20Donahue%2C%20S.%20Karayev%2C%20J.%20Long%2C%20R.%20Girshick%2C%20S.%20Guadarrama%2C%20T.%20Darrell%2C%20Caffe%3A%20Convolutional%20Architecture%20for%20Fast%20Feature%20Embedding%20(arXiv%3A1408.5093)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib48" id="ref-id-bib48" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a></span><span class="reference" id="othref0155"><div class="other-ref"><span>C.-D. Caleanu, Face expression recognition: a brief overview of the last decade, in: 2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI), 2013, pp. 157â€“161.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=C.-D.%20Caleanu%2C%20Face%20expression%20recognition%3A%20a%20brief%20overview%20of%20the%20last%20decade%2C%20in%3A%202013%20IEEE%208th%20International%20Symposium%20on%20Applied%20Computational%20Intelligence%20and%20Informatics%20(SACI)%2C%202013%2C%20pp.%20157%E2%80%93161." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib49" id="ref-id-bib49" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></span><span class="reference" id="sbref18"><div class="contribution"><div class="authors u-font-sans">M.K.A.E. Meguid, M.D. Levine</div><div id="ref-id-sbref18" class="title text-m">Fully automated recognition of spontaneous facial expressions in videos using random forest classifiers</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 5 (2) (2014), pp. 141-154, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2014.2317711" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2014.2317711</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fully%20automated%20recognition%20of%20spontaneous%20facial%20expressions%20in%20videos%20using%20random%20forest%20classifiers&amp;publication_year=2014&amp;author=M.K.A.E.%20Meguid&amp;author=M.D.%20Levine" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref18"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib50" id="ref-id-bib50" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></span><span class="reference" id="othref0160"><div class="other-ref"><span>C. Turan, K. M. Lam, Region-based feature fusion for facial-expression recognition, in: 2014 IEEE International Conference on Image Processing (ICIP), 2014, pp. 5966â€“5970 (<a class="anchor anchor-primary" href="https://doi.org/10.1109/ICIP.2014.7026204" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1109/ICIP.2014.7026204).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=C.%20Turan%2C%20K.%20M.%20Lam%2C%20Region-based%20feature%20fusion%20for%20facial-expression%20recognition%2C%20in%3A%202014%20IEEE%20International%20Conference%20on%20Image%20Processing%20(ICIP)%2C%202014%2C%20pp.%205966%E2%80%935970%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FICIP.2014.7026204)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib51" id="ref-id-bib51" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></span><span class="reference" id="othref0165"><div class="other-ref"><span>M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, Coding facial expressions with gabor wavelets, in: Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition, 1998, 1998, pp. 200â€“205.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Lyons%2C%20S.%20Akamatsu%2C%20M.%20Kamachi%2C%20J.%20Gyoba%2C%20Coding%20facial%20expressions%20with%20gabor%20wavelets%2C%20in%3A%20Proceedings%20of%20the%20Third%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%2C%201998%2C%201998%2C%20pp.%20200%E2%80%93205." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib52" id="ref-id-bib52" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a></span><span class="reference" id="sbref19"><div class="contribution"><div class="authors u-font-sans">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov</div><div id="ref-id-sbref19" class="title text-m">Dropout: a simple way to prevent neural networks from overfitting</div></div><div class="host u-font-sans">J. Mach. Learn. Res., 15 (1) (2014), pp. 1929-1958</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84904163933&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref19"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&amp;publication_year=2014&amp;author=N.%20Srivastava&amp;author=G.%20Hinton&amp;author=A.%20Krizhevsky&amp;author=I.%20Sutskever&amp;author=R.%20Salakhutdinov" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref19"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib53" id="ref-id-bib53" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a></span><span class="reference" id="othref0170"><div class="other-ref"><span>J.M. Girard, J.F. Cohn, L.A. Jeni, S. Lucey, F.D. la Torre, How much training data for facial action unit detection?, in: 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), vol. 1, 2015, pp. 1â€“8 (<a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2015.7163106" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1109/FG.2015.7163106).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.M.%20Girard%2C%20J.F.%20Cohn%2C%20L.A.%20Jeni%2C%20S.%20Lucey%2C%20F.D.%20la%20Torre%2C%20How%20much%20training%20data%20for%20facial%20action%20unit%20detection%3F%2C%20in%3A%202015%2011th%20IEEE%20International%20Conference%20and%20Workshops%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20(FG)%2C%20vol.%201%2C%202015%2C%20pp.%201%E2%80%938%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FFG.2015.7163106)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib54" id="ref-id-bib54" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a></span><span class="reference" id="othref0175"><div class="other-ref"><span>M. Valstar, M. Pantic, Induced disgust, happiness and surprise: an addition to the mmi facial expression database, in: Proceedings of the 3rd International Workshop on EMOTION (Satellite of LREC): Corpora for Research on Emotion and Affect, 2010, p. 65.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Valstar%2C%20M.%20Pantic%2C%20Induced%20disgust%2C%20happiness%20and%20surprise%3A%20an%20addition%20to%20the%20mmi%20facial%20expression%20database%2C%20in%3A%20Proceedings%20of%20the%203rd%20International%20Workshop%20on%20EMOTION%20(Satellite%20of%20LREC)%3A%20Corpora%20for%20Research%20on%20Emotion%20and%20Affect%2C%202010%2C%20p.%2065." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib55" id="ref-id-bib55" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a></span><span class="reference" id="othref0180"><div class="other-ref"><span>A. Dhall, R. Goecke, S. Lucey, T. Gedeon, Static facial expression analysis in tough conditions: data, evaluation protocol and benchmark, in: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops),&nbsp;IEEE, Barcelona, Catalonia, Spain, 2011, pp. 2106â€“2112.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Dhall%2C%20R.%20Goecke%2C%20S.%20Lucey%2C%20T.%20Gedeon%2C%20Static%20facial%20expression%20analysis%20in%20tough%20conditions%3A%20data%2C%20evaluation%20protocol%20and%20benchmark%2C%20in%3A%202011%20IEEE%20International%20Conference%20on%20Computer%20Vision%20Workshops%20(ICCV%20Workshops)%2C%C2%A0IEEE%2C%20Barcelona%2C%20Catalonia%2C%20Spain%2C%202011%2C%20pp.%202106%E2%80%932112." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib56" id="ref-id-bib56" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a></span><span class="reference" id="othref0185"><div class="other-ref"><span>P. Simard, D. Steinkraus, J.C. Platt, Best practices for convolutional neural networks applied to visual document analysis, in: 2003&nbsp;Proceedings of the Seventh International Conference on Document Analysis and Recognition,&nbsp;2003, pp. 958â€“963.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Simard%2C%20D.%20Steinkraus%2C%20J.C.%20Platt%2C%20Best%20practices%20for%20convolutional%20neural%20networks%20applied%20to%20visual%20document%20analysis%2C%20in%3A%202003%C2%A0Proceedings%20of%20the%20Seventh%20International%20Conference%20on%20Document%20Analysis%20and%20Recognition%2C%C2%A02003%2C%20pp.%20958%E2%80%93963." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib57" id="ref-id-bib57" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></span><span class="reference" id="othref0190"><div class="other-ref"><span>J.-I. Choi, C.-W. La, P.-K. Rhee, Y.-L. Bae, Face and eye location algorithms for visual user interface, in: Proceedings of First Signal Processing Society Workshop on Multimedia Signal Processing, Institute of Electrical &amp; Electronics Engineers (IEEE), Princeton, NJ, USA, 1997.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.-I.%20Choi%2C%20C.-W.%20La%2C%20P.-K.%20Rhee%2C%20Y.-L.%20Bae%2C%20Face%20and%20eye%20location%20algorithms%20for%20visual%20user%20interface%2C%20in%3A%20Proceedings%20of%20First%20Signal%20Processing%20Society%20Workshop%20on%20Multimedia%20Signal%20Processing%2C%20Institute%20of%20Electrical%20%26%20Electronics%20Engineers%20(IEEE)%2C%20Princeton%2C%20NJ%2C%20USA%2C%201997." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib58" id="ref-id-bib58" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></span><span class="reference" id="othref0195"><div class="other-ref"><span>G. Li, X. Cai, X. Li, Y. Liu, An efficient face normalization algorithm based on eyes detection, in: 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems, Institute of Electrical &amp; Electronics Engineers (IEEE), Beijing, China, 2006.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=G.%20Li%2C%20X.%20Cai%2C%20X.%20Li%2C%20Y.%20Liu%2C%20An%20efficient%20face%20normalization%20algorithm%20based%20on%20eyes%20detection%2C%20in%3A%202006%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%2C%20Institute%20of%20Electrical%20%26%20Electronics%20Engineers%20(IEEE)%2C%20Beijing%2C%20China%2C%202006." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib59" id="ref-id-bib59" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a></span><span class="reference" id="sbref20"><div class="contribution"><div class="authors u-font-sans">J.M. Saragih, S. Lucey, J.F. Cohn</div><div id="ref-id-sbref20" class="title text-m">Deformable model fitting by regularized landmark mean-shift</div></div><div class="host u-font-sans">Int. J. Comput. Vision., 91 (2) (2010), pp. 200-215</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deformable%20model%20fitting%20by%20regularized%20landmark%20mean-shift&amp;publication_year=2010&amp;author=J.M.%20Saragih&amp;author=S.%20Lucey&amp;author=J.F.%20Cohn" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref20"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib60" id="ref-id-bib60" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a></span><span class="reference" id="othref0200"><div class="other-ref"><span>A. Asthana, S. Zafeiriou, S. Cheng, M. Pantic, Robust discriminative response map fitting with constrained local models, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 3444â€“3451.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Asthana%2C%20S.%20Zafeiriou%2C%20S.%20Cheng%2C%20M.%20Pantic%2C%20Robust%20discriminative%20response%20map%20fitting%20with%20constrained%20local%20models%2C%20in%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202013%2C%20pp.%203444%E2%80%933451." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib61" id="ref-id-bib61" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></span><span class="reference" id="othref0205"><div class="other-ref"><span>S. Cheng, A. Asthana, S. Zafeiriou, J. Shen, M. Pantic, Real-time generic face tracking in the wild with cuda, in: Proceedings of the 5th ACM Multimedia Systems Conference, ACM, Singapore, Singapore 2014, pp. 148â€“151.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Cheng%2C%20A.%20Asthana%2C%20S.%20Zafeiriou%2C%20J.%20Shen%2C%20M.%20Pantic%2C%20Real-time%20generic%20face%20tracking%20in%20the%20wild%20with%20cuda%2C%20in%3A%20Proceedings%20of%20the%205th%20ACM%20Multimedia%20Systems%20Conference%2C%20ACM%2C%20Singapore%2C%20Singapore%202014%2C%20pp.%20148%E2%80%93151." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib62" id="ref-id-bib62" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a></span><span class="reference" id="othref0210"><div class="other-ref"><span>B.A. Wandell, Foundations of Vision, 1st ed., Sinauer Associates Inc, Sunderland, Mass, 1995.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=B.A.%20Wandell%2C%20Foundations%20of%20Vision%2C%201st%20ed.%2C%20Sinauer%20Associates%20Inc%2C%20Sunderland%2C%20Mass%2C%201995." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib63" id="ref-id-bib63" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a></span><span class="reference" id="othref0215"><div class="other-ref"><span>L. Bottou, Stochastic Gradient Descent Tricks, Springer, New York, NY, USA. 2012.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=L.%20Bottou%2C%20Stochastic%20Gradient%20Descent%20Tricks%2C%20Springer%2C%20New%20York%2C%20NY%2C%20USA.%202012." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib64" id="ref-id-bib64" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a></span><span class="reference" id="othref0220"><div class="other-ref"><span>X. Glorot, Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in: Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10), Society for Artificial Intelligence and Statistics, Sardinia, Italy, 2010.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=X.%20Glorot%2C%20Y.%20Bengio%2C%20Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks%2C%20in%3A%20Proceedings%20of%20the%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20(AISTATS10)%2C%20Society%20for%20Artificial%20Intelligence%20and%20Statistics%2C%20Sardinia%2C%20Italy%2C%202010." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib65" id="ref-id-bib65" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[65]</span></span></a></span><span class="reference" id="othref0225"><div class="other-ref"><span>X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectifier neural networks, in: G.J. Gordon, D.B. Dunson (Eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), vol. 15, 2011, pp. 315â€“323.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=X.%20Glorot%2C%20A.%20Bordes%2C%20Y.%20Bengio%2C%20Deep%20sparse%20rectifier%20neural%20networks%2C%20in%3A%20G.J.%20Gordon%2C%20D.B.%20Dunson%20(Eds.)%2C%20Proceedings%20of%20the%20Fourteenth%20International%20Conference%20on%20Artificial%20Intelligence%20and%20Statistics%20(AISTATS-11)%2C%20vol.%2015%2C%202011%2C%20pp.%20315%E2%80%93323." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib66" id="ref-id-bib66" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></span><span class="reference" id="sbref21"><div class="contribution"><div class="authors u-font-sans">P. Ekman, W. Friesen</div><div id="ref-id-sbref21" class="title text-m">Facial Action Coding System: A Technique for the Measurement of Facial Movement</div></div><div class="host u-font-sans">Consulting Psychologists Press, Palo Alto (1978)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20Action%20Coding%20System%3A%20A%20Technique%20for%20the%20Measurement%20of%20Facial%20Movement&amp;publication_year=1978&amp;author=P.%20Ekman&amp;author=W.%20Friesen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref21"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib67" id="ref-id-bib67" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a></span><span class="reference" id="sbref22"><div class="contribution"><div class="authors u-font-sans">W. Gu, C. Xiang, Y. Venkatesh, D. Huang, H. Lin</div><div id="ref-id-sbref22" class="title text-m">Facial expression recognition using radial encoding of local gabor features and classifier synthesis</div></div><div class="host u-font-sans">Pattern Recognit., 45 (1) (2012), pp. 80-91, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2011.05.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2011.05.006</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320311002056/pdfft?md5=8042c6b6eae2ceb1ef5da9e097293204&amp;pid=1-s2.0-S0031320311002056-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref22"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320311002056" aria-describedby="ref-id-sbref22"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052781929&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref22"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20radial%20encoding%20of%20local%20gabor%20features%20and%20classifier%20synthesis&amp;publication_year=2012&amp;author=W.%20Gu&amp;author=C.%20Xiang&amp;author=Y.%20Venkatesh&amp;author=D.%20Huang&amp;author=H.%20Lin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref22"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib68" id="ref-id-bib68" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></span><span class="reference" id="othref0230"><div class="other-ref"><span>L. Zhong, Q. Liu, P. Yang, B. Liu, J. Huang, D. Metaxas, Learning active facial patches for expression analysis, in: 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2562â€“2569.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=L.%20Zhong%2C%20Q.%20Liu%2C%20P.%20Yang%2C%20B.%20Liu%2C%20J.%20Huang%2C%20D.%20Metaxas%2C%20Learning%20active%20facial%20patches%20for%20expression%20analysis%2C%20in%3A%202012%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20(CVPR)%2C%202012%2C%20pp.%202562%E2%80%932569." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib69" id="ref-id-bib69" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a></span><span class="reference" id="sbref23"><div class="contribution"><div class="authors u-font-sans">S.H. Lee, W.J. Baddar, Y.M. Ro</div><div id="ref-id-sbref23" class="title text-m">Collaborative expression representation using peak expression and intra class variation face images for practical subject-independent emotion recognition in videos</div></div><div class="host u-font-sans">Pattern Recognit., 54 (2016), pp. 52-67, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2015.12.016" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2015.12.016</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320316000108/pdfft?md5=9ce9a8e992407cbf0b9557950393865e&amp;pid=1-s2.0-S0031320316000108-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref23"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320316000108" aria-describedby="ref-id-sbref23"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84956671145&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref23"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Collaborative%20expression%20representation%20using%20peak%20expression%20and%20intra%20class%20variation%20face%20images%20for%20practical%20subject-independent%20emotion%20recognition%20in%20videos&amp;publication_year=2016&amp;author=S.H.%20Lee&amp;author=W.J.%20Baddar&amp;author=Y.M.%20Ro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref23"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib70" id="ref-id-bib70" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a></span><span class="reference" id="othref0235"><div class="other-ref"><span>F.D. la Torre, W.S. Chu, X. Xiong, F. Vicente, X. Ding, J. Cohn, Intraface, in: 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG),&nbsp;vol. 1, 2015, pp. 1â€“8 (<a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2015.7163082" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1109/FG.2015.7163082).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=F.D.%20la%20Torre%2C%20W.S.%20Chu%2C%20X.%20Xiong%2C%20F.%20Vicente%2C%20X.%20Ding%2C%20J.%20Cohn%2C%20Intraface%2C%20in%3A%202015%2011th%20IEEE%20International%20Conference%20and%20Workshops%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20(FG)%2C%C2%A0vol.%201%2C%202015%2C%20pp.%201%E2%80%938%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FFG.2015.7163082)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib71" id="ref-id-bib71" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a></span><span class="reference" id="othref0240"><div class="other-ref"><span>J. Cohn A. Zlochower, A Computerized Analysis of Facial Expression: Feasibility of Automated Discrimination, vol. 2.&nbsp;American Psychological Society, 1995, p. 6.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.%20Cohn%20A.%20Zlochower%2C%20A%20Computerized%20Analysis%20of%20Facial%20Expression%3A%20Feasibility%20of%20Automated%20Discrimination%2C%20vol.%202.%C2%A0American%20Psychological%20Society%2C%201995%2C%20p.%206." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib72" id="ref-id-bib72" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a></span><span class="reference" id="othref0245"><div class="other-ref"><span>M. Xue, A. Mian, W. Liu, L. Li, Fully automatic 3d facial expression recognition using local depth features, in: IEEE Winter Conference on Applications of Computer Vision, 2014, pp. 1096â€“1103 <a class="anchor anchor-primary" href="https://doi.org/10.1109/WACV.2014.6835736" target="_blank"><span class="anchor-text-container"><span class="anchor-text">(http://dx.doi.org/10.1109/WACV.2014.6835736).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Xue%2C%20A.%20Mian%2C%20W.%20Liu%2C%20L.%20Li%2C%20Fully%20automatic%203d%20facial%20expression%20recognition%20using%20local%20depth%20features%2C%20in%3A%20IEEE%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%2C%202014%2C%20pp.%201096%E2%80%931103%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FWACV.2014.6835736)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib73" id="ref-id-bib73" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a></span><span class="reference" id="sbref24"><div class="contribution"><div class="authors u-font-sans">T. Sha, M. Song, J. Bu, C. Chen, D. Tao</div><div id="ref-id-sbref24" class="title text-m">Feature level analysis for 3d facial expression recognition</div></div><div class="host u-font-sans">Neurocomputing, 74 (12â€“13) (2011), pp. 2135-2141, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2011.01.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2011.01.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231211001044/pdfft?md5=3a7e649f6209eccd38ac29915a59c0c4&amp;pid=1-s2.0-S0925231211001044-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref24"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231211001044" aria-describedby="ref-id-sbref24"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79956068273&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref24"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20level%20analysis%20for%203d%20facial%20expression%20recognition&amp;publication_year=2011&amp;author=T.%20Sha&amp;author=M.%20Song&amp;author=J.%20Bu&amp;author=C.%20Chen&amp;author=D.%20Tao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref24"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib74" id="ref-id-bib74" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a></span><span class="reference" id="sbref25"><div class="contribution"><div class="authors u-font-sans">A. Maalej, B.B. Amor, M. Daoudi, A. Srivastava, S. Berretti</div><div id="ref-id-sbref25" class="title text-m">Shape analysis of local facial patches for 3d facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit., 44 (8) (2011), pp. 1581-1589, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2011.02.012" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2011.02.012</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320311000756/pdfft?md5=926ff1c0b39f7cae4ebf746be6dbf5cf&amp;pid=1-s2.0-S0031320311000756-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref25"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320311000756" aria-describedby="ref-id-sbref25"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79953063681&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref25"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Shape%20analysis%20of%20local%20facial%20patches%20for%203d%20facial%20expression%20recognition&amp;publication_year=2011&amp;author=A.%20Maalej&amp;author=B.B.%20Amor&amp;author=M.%20Daoudi&amp;author=A.%20Srivastava&amp;author=S.%20Berretti" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref25"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib76" id="ref-id-bib76" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></span><span class="reference" id="othref0250"><div class="other-ref"><span>M. Liu, S. Shan, R. Wang, X. Chen, Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1749â€“1756.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Liu%2C%20S.%20Shan%2C%20R.%20Wang%2C%20X.%20Chen%2C%20Learning%20expressionlets%20on%20spatio-temporal%20manifold%20for%20dynamic%20facial%20expression%20recognition%2C%20in%3A%20Proceedings%20of%20the%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%2C%202014%2C%20pp.%201749%E2%80%931756." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib77" id="ref-id-bib77" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a></span><span class="reference" id="sbref27"><div class="contribution"><div class="authors u-font-sans">D. Mery, K. Bowyer</div><div id="ref-id-sbref27" class="title text-m">Automatic facial attribute analysis via adaptive sparse representation of random patches</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 68 (Part 2) (2015), pp. 260-269</div><div class="comment">(Special Issue on â€œSoft Biometricsâ€, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2015.05.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://dx.doi.org/10.1016/j.patrec.2015.05.005).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865515001506/pdfft?md5=adb0d2f063d4dcc99b81b5f2c26ff664&amp;pid=1-s2.0-S0167865515001506-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref27"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865515001506" aria-describedby="ref-id-sbref27"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84948121918&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref27"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20attribute%20analysis%20via%20adaptive%20sparse%20representation%20of%20random%20patches&amp;publication_year=2015&amp;author=D.%20Mery&amp;author=K.%20Bowyer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref27"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib78" id="ref-id-bib78" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a></span><span class="reference" id="sbref28"><div class="contribution"><div class="authors u-font-sans">M.H. Siddiqi, R. Ali, A.M. Khan, Y.T. Park, S. Lee</div><div id="ref-id-sbref28" class="title text-m">Human facial expression recognition using stepwise linear discriminant analysis and hidden conditional random fields</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 24 (4) (2015), pp. 1386-1398, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2015.2405346" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2015.2405346</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84924605599&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref28"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Human%20facial%20expression%20recognition%20using%20stepwise%20linear%20discriminant%20analysis%20and%20hidden%20conditional%20random%20fields&amp;publication_year=2015&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=A.M.%20Khan&amp;author=Y.T.%20Park&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref28"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib79" id="ref-id-bib79" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a></span><span class="reference" id="othref0255"><div class="other-ref"><span>A. Zafer, R. Nawaz, J. Iqbal, Face recognition with expression variation via robust ncc, in: 2013 IEEE 9th International Conference on Emerging Technologies (ICET), 2013, pp. 1â€“5 <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICET.2013.6743520" target="_blank"><span class="anchor-text-container"><span class="anchor-text">(http://dx.doi.org/10.1109/ICET.2013.6743520).</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Zafer%2C%20R.%20Nawaz%2C%20J.%20Iqbal%2C%20Face%20recognition%20with%20expression%20variation%20via%20robust%20ncc%2C%20in%3A%202013%20IEEE%209th%20International%20Conference%20on%20Emerging%20Technologies%20(ICET)%2C%202013%2C%20pp.%201%E2%80%935%20(http%3A%2F%2Fdx.doi.org%2F10.1109%2FICET.2013.6743520)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib80" id="ref-id-bib80" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a></span><span class="reference" id="sbref29"><div class="contribution"><div class="authors u-font-sans">M.H. Siddiqi, R. Ali, A.M. Khan, E.S. Kim, G.J. Kim, S. Lee</div><div id="ref-id-sbref29" class="title text-m">Facial expression recognition using active contour-based face detection, facial movement-based feature extraction, and non-linear feature selection</div></div><div class="host u-font-sans">Multimed. Syst., 21 (6) (2014), pp. 541-555, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s00530-014-0400-2" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s00530-014-0400-2</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20active%20contour-based%20face%20detection%2C%20facial%20movement-based%20feature%20extraction%2C%20and%20non-linear%20feature%20selection&amp;publication_year=2014&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=A.M.%20Khan&amp;author=E.S.%20Kim&amp;author=G.J.%20Kim&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref29"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib81" id="ref-id-bib81" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a></span><span class="reference" id="sbref30"><div class="contribution"><div class="authors u-font-sans">M.H. Siddiqi, R. Ali, M. Idris, A.M. Khan, E.S. Kim, M.C. Whang, S. Lee</div><div id="ref-id-sbref30" class="title text-m">Human facial expression recognition using curvelet feature extraction and normalized mutual information feature selection</div></div><div class="host u-font-sans">Multimed. Tools Appl., 75 (2) (2014), pp. 935-959, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11042-014-2333-3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11042-014-2333-3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Human%20facial%20expression%20recognition%20using%20curvelet%20feature%20extraction%20and%20normalized%20mutual%20information%20feature%20selection&amp;publication_year=2014&amp;author=M.H.%20Siddiqi&amp;author=R.%20Ali&amp;author=M.%20Idris&amp;author=A.M.%20Khan&amp;author=E.S.%20Kim&amp;author=M.C.%20Whang&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref30"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib82" id="ref-id-bib82" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a></span><span class="reference" id="othref0260"><div class="other-ref"><span>T. Kanade, Y. Tian, J.F. Cohn, Comprehensive database for facial expression analysis, in: Proceedings of the Fourth IEEE International Conference on Automatic Face and Gesture Recognition 2000 (FGâ€™00), IEEE Computer Society, Washington, DC, USA, 2000, p. 46.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=T.%20Kanade%2C%20Y.%20Tian%2C%20J.F.%20Cohn%2C%20Comprehensive%20database%20for%20facial%20expression%20analysis%2C%20in%3A%20Proceedings%20of%20the%20Fourth%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%202000%20(FG%E2%80%9900)%2C%20IEEE%20Computer%20Society%2C%20Washington%2C%20DC%2C%20USA%2C%202000%2C%20p.%2046." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib83" id="ref-id-bib83" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a></span><span class="reference" id="othref0265"><div class="other-ref"><span>O.M. Parkhi, A. Vedaldi, A. Zisserman, Deep face recognition, in: British Machine Vision Conference, 2015, 46-53.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=O.M.%20Parkhi%2C%20A.%20Vedaldi%2C%20A.%20Zisserman%2C%20Deep%20face%20recognition%2C%20in%3A%20British%20Machine%20Vision%20Conference%2C%202015%2C%2046-53." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (671)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S0031320320300856"><span class="anchor-text-container"><span class="anchor-text">Binary neural networks: A survey</span></span></a></h3><div>2020, Pattern Recognition</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0002">The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S0031320319303401"><span class="anchor-text-container"><span class="anchor-text">Underwater scene prior inspired deep underwater image and video enhancement</span></span></a></h3><div>2020, Pattern Recognition</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp0004">In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater imaging physical model with optical properties of underwater scenes, we first synthesize underwater image degradation datasets which cover a diverse set of water types and degradation levels. Then, a light-weight CNN model is designed for enhancing each underwater scene type, which is trained by the corresponding training data. At last, this UWCNN model is directly extended to underwater video enhancement. Experiments on real-world and synthetic underwater images and videos demonstrate that our method generalizes well to different underwater scenes.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="/science/article/pii/S016786551930008X"><span class="anchor-text-container"><span class="anchor-text">Extended deep neural network for facial emotion recognition</span></span></a></h3><div>2019, Pattern Recognition Letters</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-2-title" aria-controls="citing-articles-article-2" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="spara0005">Humans use facial expressions to show their emotional states. However, facial expression recognition has remained a challenging and interesting problem in computer vision. In this paper we present our approach which is the extension of our previous work for facial emotion recognition <a class="anchor anchor-primary" href="#bib0001" name="bbib0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0001"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a>. The aim of this work is to classify each image into one of six facial emotion classes. The proposed model is based on single Deep Convolutional Neural Networks (DNNs), which contain convolution layers and deep residual blocks. In the proposed model, firstly the image label to all faces has been set for the training. Secondly, the images go through proposed DNN model. This model trained on two datasets Extended Cohnâ€“Kanade (CK+) and Japanese Female Facial Expression (JAFFE) Dataset. The overall results show that, the proposed DNN model can outperform the recent state-of-the-art approaches for emotion recognition. Even the proposed model has accuracy improvement in comparison with our previous model.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="/science/article/pii/S0031320317304120"><span class="anchor-text-container"><span class="anchor-text">Recent advances in convolutional neural networks</span></span></a></h3><div>2018, Pattern Recognition</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-3-title" aria-controls="citing-articles-article-3" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="spara0013">In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/TCYB.2017.2788081" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Spatial-Temporal Recurrent Neural Network for Emotion Recognition</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2019, IEEE Transactions on Cybernetics</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2018.00231" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Facial Expression Recognition by De-expression Residue Learning</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2018, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-84991821737&amp;md5=359c174347d6374cb86537bb1939ced5" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><div class="article-biography article-biography-has-image" id="bio1"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-fx1.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="sp0175"><strong>AndrÃ© Teixeira Lopes</strong> was born in Cachoeiro de Itapemirim, ES, Brazil, on March 19, 1992. He received the B.Sc. degree in computer science in 2013 from the Universidade Federal do EspÃ­rito Santo (UFES). He received the M.Sc. degree in computer science from the same university in 2016. Currently, he is a Ph.D. student and a member of the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory), both at UFES, in VitÃ³ria, ES, Brazil. His research interests include the following topics computer vision, image processing, machine learning and computer graphics.</div></div></div><div class="article-biography article-biography-has-image" id="bio0010"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-fx2.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="sp0180"><strong>Edilson de Aguiar</strong> was born in Vila Velha, ES, Brazil, on June 11, 1979. In March 2002, he received the B.Sc. degree in computer engineering from the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He received the M.Sc. degree in computer science in December 2003 and the Ph.D. degree in computer science in December 2008, both from the Saarland University and the Max-Planck Institute for Computer Science, in SaarbrÃ¼cken, Saarland, Germany. After that he worked as researcher from 2009 to 2010 at the Disney Research laboratory in Pittsburgh, USA. Since then, he has been with the Departamento de ComputaÃ§Ã£o e EletrÃ´nica of UFES, in SÃ£o Mateus, ES, Brazil, where he is an adjunct professor and researcher at the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory). His research interests are in the areas of computer graphics, computer vision, image processing and robotics. He has been involved in research projects financed through Brazilian research agencies, such as State of EspÃ­rito Santo Research Foundation (FundaÃ§Ã£o de Apoio a Pesquisa do Estado do EspÃ­rito Santo â€“ FAPES). He has also been in the program committee and organizing committee of national and international conferences in computer science.</div></div></div><div class="article-biography article-biography-has-image" id="bio0015"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-fx3.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="sp0185"><strong>Alberto F. De Souza</strong> was born in Cachoeiro de Itapemirim, ES, Brazil, on October 27, 1963. He received the B. Eng. (Cum Laude) degree in electronics engineering and M.Sc. in systems engineering in computer science from the Universidade Federal do Rio de Janeiro (COPPE/UFRJ), in Rio de Janeiro, RJ, Brazil, in 1988 and 1993, respectively; and Doctor of Philosophy (Ph.D.) in computer science from the University College London, in London, United Kingdom, in 1999. He is a professor of computer science and coordinator of the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory) at the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He has authored/co-authored one USA patent and over 90 publications. He has edited proceedings of four conferences (two IEEE sponsored conferences), is a standing member of the Steering Committee of the International Conference in Computer Architecture and High Performance Computing (SBAC-PAD), senior member of the IEEE, and comendador of the order of Rubem Braga.</div></div></div><div class="article-biography article-biography-has-image" id="bio0020"><div class="article-biography-image"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0031320316301753-fx4.jpg" height="151" alt=""></div><div class="article-biography-text"><div class="u-margin-s-bottom" id="sp0190"><strong>Thiago Oliveira-Santos</strong> was born in VitÃ³ria, ES, Brazil, on December 13, 1979. In 2004, he received the B.Sc. degree in computer engineering from the Universidade Federal do EspÃ­rito Santo (UFES), in VitÃ³ria, ES, Brazil. He received the M.Sc. degree in computer science from the same university in 2006. In 2011, he received a Ph.D. degree in biomedical engineering from the University of Bern in Switzerland, where he also worked as a post-doctoral researcher until 2013. Since then, he has been working as an adjunct professor at the&nbsp;Department of Computer Science of UFES in Vitoria, ES, Brazil. His research activities are performed at the LaboratÃ³rio de ComputaÃ§Ã£o de Alto Desempenho (LCAD â€“ High Performance Computing Laboratory) and include the following topics computer vision, image processing, computer graphics, and robotics.</div></div></div><a class="anchor abstract-link anchor-primary" href="/science/article/abs/pii/S0031320316301753"><span class="anchor-text-container"><span class="anchor-text">View Abstract</span></span></a><div class="Copyright"><span class="copyright-line">Â© 2016 Elsevier Ltd. All rights reserved.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865517302313" title="Multi angle optimal pattern-based deep learning for automatic facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Multi angle optimal pattern-based deep learning for automatic facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 139, 2020, pp. 157-165</div></div><div class="authors"><span>Deepak Kumar</span> <span>Jain</span>, â€¦, <span>Kaiqi</span> <span>Huang</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865517302313/pdfft?md5=4d53594be344ac11f69063c8524d6629&amp;pid=1-s2.0-S0167865517302313-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article0-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0031320316000534" title="Boosted NNE collections for multicultural facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Boosted NNE collections for multicultural facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition, Volume 55, 2016, pp. 14-27</div></div><div class="authors"><span>Ghulam</span> <span>Ali</span>, â€¦, <span>Tae-Sun</span> <span>Choi</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320316000534/pdfft?md5=288b0abf1f3aef5d496e7ac31e432bd2&amp;pid=1-s2.0-S0031320316000534-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article1-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231218308634" title="Spatio-temporal convolutional features with nested LSTM for facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Spatio-temporal convolutional features with nested LSTM for facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 317, 2018, pp. 50-57</div></div><div class="authors"><span>Zhenbo</span> <span>Yu</span>, â€¦, <span>Jiankang</span> <span>Deng</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231218308634/pdfft?md5=41149ff1659402937f3542b188491f6c&amp;pid=1-s2.0-S0925231218308634-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0925231220309838" title="Attention mechanism-based CNN for facial expression recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Attention mechanism-based CNN for facial expression recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 411, 2020, pp. 340-350</div></div><div class="authors"><span>Jing</span> <span>Li</span>, â€¦, <span>Zhaojie</span> <span>Ju</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231220309838/pdfft?md5=6060445b3bd2193d0e2fd0cc1bd93d5e&amp;pid=1-s2.0-S0925231220309838-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article3-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1877050916314752" title="Automatic Facial Expression Recognition Using DCNN"><span class="anchor-text-container"><span class="anchor-text"><span>Automatic Facial Expression Recognition Using DCNN</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Procedia Computer Science, Volume 93, 2016, pp. 453-461</div></div><div class="authors"><span>Veena</span> <span>Mayya</span>, â€¦, <span>M.M.</span> <span>Manohara Pai</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1877050916314752/pdf?md5=e1d1827351dd63536bb7ddbe9e3ea365&amp;pid=1-s2.0-S1877050916314752-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article4-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0743731518308827" title="Automatic social signal analysis: Facial expression recognition using difference convolution neural network"><span class="anchor-text-container"><span class="anchor-text"><span>Automatic social signal analysis: Facial expression recognition using difference convolution neural network</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of Parallel and Distributed Computing, Volume 131, 2019, pp. 97-102</div></div><div class="authors"><span>Jingying</span> <span>Chen</span>, â€¦, <span>Can</span> <span>Xu</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0743731518308827/pdfft?md5=b5c9970d12e21a85cfa52ad63527442f&amp;pid=1-s2.0-S0743731518308827-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article5-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plum-sciencedirect-theme"><div class="PlumX-Summary"><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-inverse-logo.png" class="plx-logo"></div><div class="pps-cols"><div class="pps-col plx-citation"><div class="plx-citation"><div class="pps-title">Citations</div><ul><li class="plx-citation"><span class="pps-label">Citation Indexes: </span><span class="pps-count">669</span></li></ul></div></div><div class="pps-col plx-capture"><div class="plx-capture"><div class="pps-title">Captures</div><ul><li class="plx-capture"><span class="pps-label">Readers: </span><span class="pps-count">629</span></li></ul></div></div><div class="pps-col plx-mention"><div class="plx-mention"><div class="pps-title">Mentions</div><ul><li class="plx-mention"><span class="pps-label">News Mentions: </span><span class="pps-count">1</span></li></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plx-logo"></div><a target="_blank" href="https://plu.mx/plum/a/?doi=10.1016/j.patcog.2016.07.026&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class="pps-seemore" title="PlumX Metrics Detail Page">View details<svg fill="currentColor" tabindex="-1" focusable="false" width="16" height="16" viewBox="0 0 16 16" class="svg-arrow"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0031320316301753" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLqqwvNhq3slt9cD6pF7tpgA*" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright Â© <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
<div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
<script async="" src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="text/javascript"></script>
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S0031320316301753","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0031-3203","volumeNumber":"61","suppl":"C","provider":"elsevier","entitlementType":"package"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1728990766011,"loadTime":""},"visitor":{"accessType":"ae:REG_SHIBBOLETH","accountId":"ae:50401","accountName":"ae:IT University of Copenhagen","loginStatus":"logged in","userId":"ae:71970787","ipAddress":"77.165.246.201","appSessionId":"6465a12a-1939-4136-ac19-1d08b328e0a8"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
<script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
<script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/arp.js" type="text/javascript"></script>
<script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:REG_SHIBBOLETH","countryCode":"NL"},"account":{"id":"ae:50401","name":"ae:IT University of Copenhagen"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
<span id="pendo-answer-rating"></span>
<script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
<script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
<script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
<script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'false' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>

<script>
var pageDataTracker = {
    eventCookieName: 'eventTrack',
    debugCookie: 'els-aa-debugmode',
    debugCounter: 1,
    warnings: [],
    measures: {},
    timeoffset: 0

    ,trackPageLoad: function(data) {
        if (window.pageData && ((pageData.page && pageData.page.noTracking == 'true') || window.pageData_isLoaded)) {
            return false;
        }

        this.updatePageData(data);

        this.initWarnings();
        if(!(window.pageData && pageData.page && pageData.page.name)) {
            console.error('pageDataTracker.trackPageLoad() called without pageData.page.name being defined!');
            return;
        }

        this.processIdPlusData(window.pageData);

        if(window.pageData && pageData.page && !pageData.page.loadTime) {
          pageData.page.loadTime = performance ? Math.round((performance.now())).toString() : '';
        }

        if(window.pageData && pageData.page) {
            var localTime = new Date().getTime();
            if(pageData.page.loadTimestamp) {
                // calculate timeoffset
                var serverTime = parseInt(pageData.page.loadTimestamp);
                if(!isNaN(serverTime)) {
                    this.timeoffset = pageData.page.loadTimestamp - localTime;
                }
            } else {
                pageData.page.loadTimestamp = localTime;
            }
        }

        this.validateData(window.pageData);

        try {
            var cookieTest = 'aa-cookie-test';
            this.setCookie(cookieTest, cookieTest);
            if(this.getCookie(cookieTest) != cookieTest) {
                this.warnings.push('dtm5');
            }
            this.deleteCookie(cookieTest);
        } catch(e){
            this.warnings.push('dtm5');
        }

        this.registerCallbacks();
        this.setAnalyticsData();

        // handle any cookied event data
        this.getEvents();

        window.pageData_isLoaded = true;

        this.debugMessage('Init - trackPageLoad()', window.pageData);

        _satellite.track('eventDispatcher', JSON.stringify({
          eventName: 'newPage',
          eventData: {eventName: 'newPage'},
          pageData: window.pageData
        }));
    }

    ,trackEvent: function(event, data, callback) {
        if (window.pageData && pageData.page && pageData.page.noTracking == 'true') {
            return false;
        }
        
        if(!window.pageData_isLoaded) {
            if(this.isDebugEnabled()) {
                console.log('[AA] pageDataTracker.trackEvent() called without calling trackPageLoad() first.');
            }
            return false;
        }

        if (event) {
            this.initWarnings();
            if(event === 'newPage') {
                // auto fillings
                if(data && data.page && !data.page.loadTimestamp) {
                    data.page.loadTimestamp = ''+(new Date().getTime() + this.timeoffset);
                }
                this.processIdPlusData(data);
            }

            window.eventData = data ? data : {};
            window.eventData.eventName = event;
            if(!_satellite.getVar('blacklisted')) {
                this.handleEventData(event, data);
    
                if(event === 'newPage') {
                    this.validateData(window.pageData);
                }
                this.debugMessage('Event: ' + event, data);
    
                _satellite.track('eventDispatcher', JSON.stringify({
                  eventName: event,
                  eventData: window.eventData,
                  pageData: window.pageData
                }));
            } else {
                this.debugMessage('!! Blocked Event: ' + event, data);
            }
        }

        if (typeof(callback) == 'function') {
            callback.call();
        }
    }

    ,processIdPlusData: function(data) {
        if(data && data.visitor && data.visitor.idPlusData) {
            var idPlusFields = ['userId', 'accessType', 'accountId', 'accountName'];
            for(var i=0; i < idPlusFields.length; i++) {
                if(typeof data.visitor.idPlusData[idPlusFields[i]] !== 'undefined') {
                    data.visitor[idPlusFields[i]] = data.visitor.idPlusData[idPlusFields[i]];
                }
            }
            data.visitor.idPlusData = undefined;
        }
    }

    ,validateData: function(data) {
        if(!data) {
            this.warnings.push('dv0');
            return;
        }

        // top 5
        if(!(data.visitor && data.visitor.accessType)) {
            this.warnings.push('dv1');
        }
        if(data.visitor && (data.visitor.accountId || data.visitor.accountName)) {
            if(!data.visitor.accountName) {
                this.warnings.push('dv2');
            }
            if(!data.visitor.accountId) {
                this.warnings.push('dv3');
            }
        }
        if(!(data.page && data.page.productName)) {
            this.warnings.push('dv4');
        }
        if(!(data.page && data.page.businessUnit)) {
            this.warnings.push('dv5');
        }
        if(!(data.page && data.page.name)) {
            this.warnings.push('dv6');
        }

        // rp mandatory
        if(data.page && data.page.businessUnit && (data.page.businessUnit.toLowerCase().indexOf('els:rp:') !== -1 || data.page.businessUnit.toLowerCase().indexOf('els:rap:') !== -1)) {
            if(!(data.page && data.page.loadTimestamp)) {
                this.warnings.push('dv7');
            }
            if(!(data.page && data.page.loadTime)) {
                this.warnings.push('dv8');
            }
            if(!(data.visitor && data.visitor.ipAddress)) {
                this.warnings.push('dv9');
            }
            if(!(data.page && data.page.type)) {
                this.warnings.push('dv10');
            }
            if(!(data.page && data.page.language)) {
                this.warnings.push('dv11');
            }
        }

        // other
        if(data.page && data.page.environment) {
            var env = data.page.environment.toLowerCase();
            if(!(env === 'dev' || env === 'cert' || env === 'prod')) {
                this.warnings.push('dv12');
            }
        }
        if(data.content && data.content.constructor !== Array) {
            this.warnings.push('dv13');
        }

        if(data.visitor && data.visitor.accountId && data.visitor.accountId.indexOf(':') == -1) {
            this.warnings.push('dv14');
            data.visitor.accountId = "data violation"
        }
    }

    ,initWarnings: function() {
        this.warnings = [];
        try {
            var hdn = document.head.childNodes;
            var libf = false;
            for(var i=0; i<hdn.length; i++) {
                if(hdn[i].src && (hdn[i].src.indexOf('satelliteLib') !== -1 || hdn[i].src.indexOf('launch') !== -1)) {
                    libf = true;
                    break;
                }
            }
            if(!libf) {
                this.warnings.push('dtm1');
            }
        } catch(e) {}

        try {
            for (let element of document.querySelectorAll('*')) {
                // Check if the element has a src attribute and if it meets the criteria
                if (element.src && element.src.includes('assets.adobedtm.com') && element.src.includes('launch')) {
                    if (element.tagName.toLowerCase() !== 'script') {
                        this.warnings.push('dtm5');
                    }
                    if (!element.hasAttribute('async')) {
                        this.warnings.push('dtm4');
                    }
                }
            }
        } catch (e) { }
    }

    ,getMessages: function() {
        return ['v1'].concat(this.warnings).join('|');
    }
    ,addMessage: function(message) {
        this.warnings.push(message);
    }

    ,getPerformance: function() {
        var copy = {};
        for (var attr in this.measures) {
            if(this.measures.hasOwnProperty(attr)) {
                copy[attr] = this.measures[attr];
            }
        }

        this.measures = {};
        return copy;
    }

    ,dtmCodeDesc: {
        dtm1: 'satellite-lib must be placed in the <head> section',
        dtm2: 'trackPageLoad() must be placed and called before the closing </body> tag',
        dtm3: 'trackEvent() must be called at a stage where Document.readyState=complete (e.g. on the load event or a user event)',
        dtm4: 'Embed codes need to be loaded in async mode',
        dtm5: 'Embed codes not in type script',
        dv1: 'visitor.accessType not set but mandatory',
        dv2: 'visitor.accountName not set but mandatory',
        dv3: 'visitor.accountId not set but mandatory',
        dv4: 'page.productName not set but mandatory',
        dv5: 'page.businessUnit not set but mandatory',
        dv6: 'page.name not set but mandatory',
        dv7: 'page.loadTimestamp not set but mandatory',
        dv8: 'page.loadTime not set but mandatory',
        dv9: 'visitor.ipAddress not set but mandatory',
        dv10: 'page.type not set but mandatory',
        dv11: 'page.language not set but mandatory',
        dv12: 'page.environment must be set to \'prod\', \'cert\' or \'dev\'',
        dv13: 'content must be of type array of objects',
        dv14: 'account number must contain at least one \':\', e.g. \'ae:12345\''
    }

    ,debugMessage: function(event, data) {
        if(this.isDebugEnabled()) {
            console.log('[AA] --------- [' + (this.debugCounter++) + '] Web Analytics Data ---------');
            console.log('[AA] ' + event);
            console.groupCollapsed("[AA] AA Data: ");
            if(window.eventData) {
                console.log("[AA] eventData:\n" + JSON.stringify(window.eventData, true, 2));
            }
            if(window.pageData) {
                console.log("[AA] pageData:\n" + JSON.stringify(window.pageData, true, 2));
            }
            console.groupEnd();
            if(this.warnings.length > 0) {
                console.groupCollapsed("[AA] Warnings ("+this.warnings.length+"): ");
                for(var i=0; i<this.warnings.length; i++) {
                    var error = this.dtmCodeDesc[this.warnings[i]] ? this.dtmCodeDesc[this.warnings[i]] : 'Error Code: ' + this.warnings[i];
                    console.log('[AA] ' + error);
                }
                console.log('[AA] More can be found here: https://confluence.cbsels.com/display/AA/AA+Error+Catalog');
                console.groupEnd();
            }
            console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        }
    }

    ,getTrackingCode: function() {
      var campaign = _satellite.getVar('Campaign - ID');
      if(!campaign) {
        campaign = window.sessionStorage ? sessionStorage.getItem('dgcid') : '';
      }
      return campaign;
    }

    ,isDebugEnabled: function() {
        if(typeof this.debug === 'undefined') {
            this.debug = (document.cookie.indexOf(this.debugCookie) !== -1) || (window.pageData && pageData.page && pageData.page.environment && pageData.page.environment.toLowerCase() === 'dev');
            //this.debug = (document.cookie.indexOf(this.debugCookie) !== -1);
        }
        return this.debug;
    }

    ,enableDebug: function(expire) {
        if (typeof expire === 'undefined') {
            expire = 86400;
        }
        console.log('You just enabled debug mode for Adobe Analytics tracking. This mode will persist for 24h.');
        console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        this.setCookie(this.debugCookie, 'true', expire, document.location.hostname);
        this.debug = true;
    }

    ,disableDebug: function() {
        console.log('Debug mode is now disabled.');
        this.deleteCookie(this.debugCookie);
        this.debug = false;
    }

    ,setAnalyticsData: function() {
        if(!(window.pageData && pageData.page && pageData.page.productName && pageData.page.name)) {
            return;
        }
        pageData.page.analyticsPagename = pageData.page.productName + ':' + pageData.page.name;

        var pageEls = pageData.page.name.indexOf(':') > -1 ? pageData.page.name.split(':') : [pageData.page.name];
        pageData.page.sectionName = pageData.page.productName + ':' + pageEls[0];
    }

    ,getEvents: function() {
        pageData.savedEvents = {};
        pageData.eventList = [];

        var val = this.getCookie(this.eventCookieName);
        if (val) {
            pageData.savedEvents = val;
        }

        this.deleteCookie(this.eventCookieName);
    }

    ,updatePageData(data) {
        window.pageData = window.pageData || {};
        if (data && typeof(data) === 'object') {
            for (var x in data) {
                if(data.hasOwnProperty(x) && data[x] instanceof Array) {
                    pageData[x] = data[x];
                } else if(data.hasOwnProperty(x) && typeof(data[x]) === 'object') {
                    if(!pageData[x]) {
                        pageData[x] = {};
                    }
                    for (var y in data[x]) {
                        if(data[x].hasOwnProperty(y)) {
                            pageData[x][y] = data[x][y];
                        }
                    }
                }
            }
        }
    }

    ,handleEventData: function(event, data) {
        var val;
        switch(event) {
            case 'newPage':
                this.updatePageData(data);
                this.setAnalyticsData();
            case 'saveSearch':
            case 'searchResultsUpdated':
                if (data) {
                    // overwrite page-load object
                    if (data.search && typeof(data.search) == 'object') {
                        window.eventData.search.resultsPosition = '';
                        pageData.search = pageData.search || {};
                        var fields = ['advancedCriteria', 'criteria', 'currentPage', 'dataFormCriteria', 'facets', 'resultsByType', 'resultsPerPage', 'sortType', 'totalResults', 'type', 'database',
                        'suggestedClickPosition','suggestedLetterCount','suggestedResultCount', 'autoSuggestCategory', 'autoSuggestDetails','typedTerm','selectedTerm', 'channel',
                        'facetOperation', 'details'];
                        for (var i=0; i<fields.length; i++) {
                            if (data.search[fields[i]]) {
                                pageData.search[fields[i]] = data.search[fields[i]];
                            }
                        }
                    }
                }
                this.setAnalyticsData();
                break;
            case 'navigationClick':
                if (data && data.link) {
                    window.eventData.navigationLink = {
                        name: ((data.link.location || 'no location') + ':' + (data.link.name || 'no name'))
                    };
                }
                break;
            case 'autoSuggestClick':
                if (data && data.search) {
                    val = {
                        autoSuggestSearchData: (
                            'letterct:' + (data.search.suggestedLetterCount || 'none') +
                            '|resultct:' + (data.search.suggestedResultCount || 'none') +
                            '|clickpos:' + (data.search.suggestedClickPosition || 'none')
                        ).toLowerCase(),
                        autoSuggestSearchTerm: (data.search.typedTerm || ''),
                        autoSuggestTypedTerm: (data.search.typedTerm || ''),
                        autoSuggestSelectedTerm: (data.search.selectedTerm || ''),
                        autoSuggestCategory: (data.search.autoSuggestCategory || ''),
                        autoSuggestDetails: (data.search.autoSuggestDetails || '')
                    };
                }
                break;
            case 'linkOut':
                if (data && data.content && data.content.length > 0) {
                    window.eventData.linkOut = data.content[0].linkOut;
                    window.eventData.referringProduct = _satellite.getVar('Page - Product Name') + ':' + data.content[0].id;
                }
                break;
            case 'socialShare':
                if (data && data.social) {
                    window.eventData.sharePlatform = data.social.sharePlatform || '';
                }
                break;
            case 'contentInteraction':
                if (data && data.action) {
                    window.eventData.action.name = pageData.page.productName + ':' + data.action.name;
                }
                break;
            case 'searchWithinContent':
                if (data && data.search) {
                    window.pageData.search = window.pageData.search || {};
                    pageData.search.withinContentCriteria = data.search.withinContentCriteria;
                }
                break;
            case 'contentShare':
                if (data && data.content) {
                    window.eventData.sharePlatform = data.content[0].sharePlatform;
                }
                break;
            case 'contentLinkClick':
                if (data && data.link) {
                    window.eventData.action = { name: pageData.page.productName + ':' + (data.link.type || 'no link type') + ':' + (data.link.name || 'no link name') };
                }
                break;
            case 'contentWindowLoad':
            case 'contentTabClick':
                if (data && data.content) {
                    window.eventData.tabName = data.content[0].tabName || '';
                    window.eventData.windowName = data.content[0].windowName || '';
                }
                break;
            case 'userProfileUpdate':
                if (data && data.user) {
                    if (Object.prototype.toString.call(data.user) === "[object Array]") {
                        window.eventData.user = data.user[0];
                    }
                }
                break;
            case 'videoStart':
                if (data.video) {
                    data.video.length = parseFloat(data.video.length || '0');
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.open(data.video.id, data.video.length, s.Media.playerName);
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoPlay':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoStop':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                }
                break;
            case 'videoComplete':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                    s.Media.close(data.video.id);
                }
                break;
            case 'addWebsiteExtension':
                if(data && data.page) {
                    val = {
                        wx: data.page.websiteExtension
                    }
                }
                break;
        }

        if (val) {
            this.setCookie(this.eventCookieName, val);
        }
    }

    ,registerCallbacks: function() {
        var self = this;
        if(window.usabilla_live) {
            window.usabilla_live('setEventCallback', function(category, action, label, value) {
                if(action == 'Campaign:Open') {
                    self.trackEvent('ctaImpression', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                } else if(action == 'Campaign:Success') {
                    self.trackEvent('ctaClick', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                }
            });
        }
    }

    ,getConsortiumAccountId: function() {
        var id = '';
        if (window.pageData && pageData.visitor && (pageData.visitor.consortiumId || pageData.visitor.accountId)) {
            id = (pageData.visitor.consortiumId || 'no consortium ID') + '|' + (pageData.visitor.accountId || 'no account ID');
        }

        return id;
    }

    ,getSearchClickPosition: function() {
        if (window.eventData && eventData.search && eventData.search.resultsPosition) {
            var pos = parseInt(eventData.search.resultsPosition), clickPos;
            if (!isNaN(pos)) {
                var page = pageData.search.currentPage ? parseInt(pageData.search.currentPage) : '', perPage = pageData.search.resultsPerPage ? parseInt(pageData.search.resultsPerPage) : '';
                if (!isNaN(page) && !isNaN(perPage)) {
                    clickPos = pos + ((page - 1) * perPage);
                }
            }
            return clickPos ? clickPos.toString() : eventData.search.resultsPosition;
        }
        return '';
    }

    ,getSearchFacets: function() {
        var facetList = '';
        if (window.pageData && pageData.search && pageData.search.facets) {
            if (typeof(pageData.search.facets) == 'object') {
                for (var i=0; i<pageData.search.facets.length; i++) {
                    var f = pageData.search.facets[i];
                    facetList += (facetList ? '|' : '') + f.name + '=' + f.values.join('^');
                }
            }
        }
        return facetList;
    }

    ,getSearchResultsByType: function() {
        var resultTypes = '';
        if (window.pageData && pageData.search && pageData.search.resultsByType) {
            for (var i=0; i<pageData.search.resultsByType.length; i++) {
                var r = pageData.search.resultsByType[i];
                resultTypes += (resultTypes ? '|' : '') + r.name + (r.results || r.values ? '=' + (r.results || r.values) : '');
            }
        }
        return resultTypes;
    }

    ,getJournalInfo: function() {
        var info = '';
        if (window.pageData && pageData.journal && (pageData.journal.name || pageData.journal.specialty || pageData.journal.section || pageData.journal.issn || pageData.journal.issueNumber || pageData.journal.volumeNumber || pageData.journal.family || pageData.journal.publisher)) {
            var journal = pageData.journal;
            info = (journal.name || 'no name') 
            + '|' + (journal.specialty || 'no specialty') 
            + '|' + (journal.section || 'no section') 
            + '|' + (journal.issn || 'no issn') 
            + '|' + (journal.issueNumber || 'no issue #') 
            + '|' + (journal.volumeNumber || 'no volume #')
            + '|' + (journal.family || 'no family')
            + '|' + (journal.publisher || 'no publisher');

        }
        return info;
    }

    ,getBibliographicInfo: function(doc) {
        if (!doc || !(doc.publisher || doc.indexTerms || doc.publicationType || doc.publicationRights || doc.volumeNumber || doc.issueNumber || doc.subjectAreas || doc.isbn)) {
            return '';
        }

        var terms = doc.indexTerms ? doc.indexTerms.split('+') : '';
        if (terms) {
            terms = terms.slice(0, 5).join('+');
            terms = terms.length > 100 ? terms.substring(0, 100) : terms;
        }

        var areas = doc.subjectAreas ? doc.subjectAreas.split('>') : '';
        if (areas) {
            areas = areas.slice(0, 5).join('>');
            areas = areas.length > 100 ? areas.substring(0, 100) : areas;
        }

        var biblio	= (doc.publisher || 'none')
            + '^' + (doc.publicationType || 'none')
            + '^' + (doc.publicationRights || 'none')
            + '^' + (terms || 'none')
            + '^' + (doc.volumeNumber || 'none')
            + '^' + (doc.issueNumber || 'none')
            + '^' + (areas || 'none')
            + '^' + (doc.isbn || 'none');

        return this.stripProductDelimiters(biblio).toLowerCase();
    }

    ,getContentItem: function() {
        var docs = window.eventData && eventData.content ? eventData.content : pageData.content;
        if (docs && docs.length > 0) {
            return docs[0];
        }
    }

    ,getFormattedDate: function(ts) {
        if (!ts) {
            return '';
        }

        var d = new Date(parseInt(ts) * 1000);

        // now do formatting
        var year = d.getFullYear()
            ,month = ((d.getMonth() + 1) < 10 ? '0' : '') + (d.getMonth() + 1)
            ,date = (d.getDate() < 10 ? '0' : '') + d.getDate()
            ,hours = d.getHours() > 12 ? d.getHours() - 12 : d.getHours()
            ,mins = (d.getMinutes() < 10 ? '0' : '') + d.getMinutes()
            ,ampm = d.getHours() > 12 ? 'pm' : 'am';

        hours = (hours < 10 ? '0' : '') + hours;
        return year + '-' + month + '-' + date;
    }

    ,getVisitorId: function() {
        var orgId = '4D6368F454EC41940A4C98A6@AdobeOrg';
        if(Visitor && Visitor.getInstance(orgId)) {
            return Visitor.getInstance(orgId).getMarketingCloudVisitorID();
        } else {
            return ''
        }
    }

    ,setProductsVariable: function() {
        var prodList = window.eventData && eventData.content ? eventData.content : pageData.content
            ,prods = [];
        if (prodList) {
            for (var i=0; i<prodList.length; i++) {
                if (prodList[i].id || prodList[i].type || prodList[i].publishDate || prodList[i].onlineDate) {
                    if (!prodList[i].id) {
                        prodList[i].id = 'no id';
                    }
                    var prodName = (pageData.page.productName || 'xx').toLowerCase();
                    if (prodList[i].id.indexOf(prodName + ':') != 0) {
                        prodList[i].id = prodName + ':' + prodList[i].id;
                    }
                    prodList[i].id = this.stripProductDelimiters(prodList[i].id);
                    var merch = [];
                    if (prodList[i].format) {
                        merch.push('evar17=' + this.stripProductDelimiters(prodList[i].format.toLowerCase()));
                    }
                    if (prodList[i].type) {
                        var type = prodList[i].type;
                        if (prodList[i].accessType) {
                            type += ':' + prodList[i].accessType;
                        }
                        merch.push('evar20=' + this.stripProductDelimiters(type.toLowerCase()));

                        if(type.indexOf(':manuscript') > 0) {
                            /*
                            var regex = /[a-z]+:manuscript:id:([a-z]+-[a-z]-[0-9]+-[0-9]+)/gmi;
                            var m = regex.exec(prodList[i].id);
                            if(m) {
                                merch.push('evar200=' + m[1]);
                            }
                            merch.push('evar200=' + prodList[i].id);
                            */
                            a = prodList[i].id.lastIndexOf(':');
                            if(a>0) {
                                merch.push('evar200=' + prodList[i].id.substring(a+1).toUpperCase());
                            }
                        } else if(type.indexOf(':submission') > 0) {
                            merch.push('evar200=' + prodList[i].id);
                        }
                    }
                    if(!prodList[i].title) {
                        prodList[i].title = prodList[i].name;
                    }
                    if (prodList[i].title) {
                        merch.push('evar75=' + this.stripProductDelimiters(prodList[i].title.toLowerCase()));
                    }
                    if (prodList[i].breadcrumb) {
                        merch.push('evar63=' + this.stripProductDelimiters(prodList[i].breadcrumb).toLowerCase());
                    }
                    var nowTs = new Date().getTime()/1000;
                    if (prodList[i].onlineDate && !isNaN(prodList[i].onlineDate)) {
                        if(prodList[i].onlineDate > 32503680000) {
                            prodList[i].onlineDate = prodList[i].onlineDate/1000;
                        }
                        merch.push('evar122=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate)));
                        var onlineAge = Math.floor((nowTs - prodList[i].onlineDate) / 86400);
                        onlineAge = (onlineAge === 0) ? 'zero' : onlineAge;
                        merch.push('evar128=' + onlineAge);
                    }
                    if (prodList[i].publishDate && !isNaN(prodList[i].publishDate)) {
                        if(prodList[i].publishDate > 32503680000) {
                            prodList[i].publishDate = prodList[i].publishDate/1000;
                        }
                        merch.push('evar123=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                        var publishAge = Math.floor((nowTs - prodList[i].publishDate) / 86400);
                        publishAge = (publishAge === 0) ? 'zero' : publishAge;
                        merch.push('evar127=' + publishAge);
                    }
                    if (prodList[i].onlineDate && prodList[i].publishDate) {
                        merch.push('evar38=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate) + '^' + pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                    }
                    if (prodList[i].mapId) {
                        merch.push('evar70=' + this.stripProductDelimiters(prodList[i].mapId));
                    }
					if (prodList[i].relevancyScore) {
						merch.push('evar71=' + this.stripProductDelimiters(prodList[i].relevancyScore));
					}
                    if (prodList[i].status) {
                        merch.push('evar73=' + this.stripProductDelimiters(prodList[i].status));
                    }
                    if (prodList[i].previousStatus) {
                        merch.push('evar111=' + this.stripProductDelimiters(prodList[i].previousStatus));
                    }
                    if (prodList[i].entitlementType) {
                        merch.push('evar80=' + this.stripProductDelimiters(prodList[i].entitlementType));
                    }
                    if (prodList[i].recordType) {
                        merch.push('evar93=' + this.stripProductDelimiters(prodList[i].recordType));
                    }
                    if (prodList[i].exportType) {
                        merch.push('evar99=' + this.stripProductDelimiters(prodList[i].exportType));
                    }
                    if (prodList[i].importType) {
                        merch.push('evar142=' + this.stripProductDelimiters(prodList[i].importType));
                    }
                    if (prodList[i].section) {
                        merch.push('evar100=' + this.stripProductDelimiters(prodList[i].section));
                    }
                    if (prodList[i].detail) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].detail.toLowerCase()));
                    } else if(prodList[i].details) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].details.toLowerCase()));
                    }
                    if (prodList[i].position) {
                        merch.push('evar116=' + this.stripProductDelimiters(prodList[i].position));
                    }
                    if (prodList[i].publicationTitle) {
                        merch.push('evar129=' + this.stripProductDelimiters(prodList[i].publicationTitle));
                    }
                    if (prodList[i].specialIssueTitle) {
                        merch.push('evar130=' + this.stripProductDelimiters(prodList[i].specialIssueTitle));
                    }
                    if (prodList[i].specialIssueNumber) {
                        merch.push('evar131=' + this.stripProductDelimiters(prodList[i].specialIssueNumber));
                    }
                    if (prodList[i].referenceModuleTitle) {
                        merch.push('evar139=' + this.stripProductDelimiters(prodList[i].referenceModuleTitle));
                    }
                    if (prodList[i].referenceModuleISBN) {
                        merch.push('evar140=' + this.stripProductDelimiters(prodList[i].referenceModuleISBN));
                    }
                    if (prodList[i].volumeTitle) {
                        merch.push('evar132=' + this.stripProductDelimiters(prodList[i].volumeTitle));
                    }
                    if (prodList[i].publicationSection) {
                        merch.push('evar133=' + this.stripProductDelimiters(prodList[i].publicationSection));
                    }
                    if (prodList[i].publicationSpecialty) {
                        merch.push('evar134=' + this.stripProductDelimiters(prodList[i].publicationSpecialty));
                    }
                    if (prodList[i].issn) {
                        merch.push('evar135=' + this.stripProductDelimiters(prodList[i].issn));
                    }
                    if (prodList[i].id2) {
                        merch.push('evar159=' + this.stripProductDelimiters(prodList[i].id2));
                    }
                    if (prodList[i].id3) {
                        merch.push('evar160=' + this.stripProductDelimiters(prodList[i].id3));
                    }
                    if (prodList[i].provider) {
                        merch.push('evar164=' + this.stripProductDelimiters(prodList[i].provider));
                    }
                    if (prodList[i].citationStyle) {
                        merch.push('evar170=' + this.stripProductDelimiters(prodList[i].citationStyle));
                    }

                    var biblio = this.getBibliographicInfo(prodList[i]);
                    if (biblio) {
                        merch.push('evar28=' + biblio);
                    }

                    if (prodList[i].turnawayId) {
                        pageData.eventList.push('product turnaway');
                    }

                    var price = prodList[i].price || '', qty = prodList[i].quantity || '', evts = [];
                    if (price && qty) {
                        qty = parseInt(qty || '1');
                        price = parseFloat(price || '0');
                        price = (price * qty).toFixed(2);

                        if (window.eventData && eventData.eventName && eventData.eventName == 'cartAdd') {
                            evts.push('event20=' + price);
                        }
                    }

                    var type = window.pageData && pageData.page && pageData.page.type ? pageData.page.type : '', evt = window.eventData && eventData.eventName ? eventData.eventName : '';
                    if (type.match(/^CP\-/gi) !== null && (!evt || evt == 'newPage' || evt == 'contentView')) {
                        evts.push('event181=1');
                    }
                    if (evt == 'contentDownload' || type.match(/^CP\-DL/gi) !== null) {
                        evts.push('event182=1');
                    }
                    if (evt == 'contentDownloadRequest') {
                        evts.push('event319=1');
                    }
                    if (evt == 'contentExport') {
                        evts.push('event184=1');
                    }
                    if (this.eventFires('recommendationViews')) {
                        evts.push('event264=1');
                    }

                    if(prodList[i].datapoints) {
                        evts.push('event239=' + prodList[i].datapoints);
                    }
                    if(prodList[i].documents) {
                        evts.push('event240=' + prodList[i].documents);
                    }
                    if(prodList[i].size) {
                        evts.push('event335=' + prodList[i].size);
                        evts.push('event336=1')
                    }

                    prods.push([
                        ''					// empty category
                        ,prodList[i].id		// id
                        ,qty				// qty
                        ,price				// price
                        ,evts.join('|')		// events
                        ,merch.join('|')	// merchandising eVars
                    ].join(';'));
                }
            }
        }

        return prods.join(',');
    }
    ,eventFires: function(eventName) {
      var evt = window.eventData && eventData.eventName ? eventData.eventName : '';
      if(evt == eventName) {
        return true;
      }
      // initial pageload and new pages
      if((!window.eventData || evt == 'newPage') && window.pageData && window.pageData.trackEvents) {
        var tEvents = window.pageData.trackEvents;
        for(var i=0; i<tEvents.length; i++) {
          if(tEvents[i] == eventName) {
            return true;
          }
        }
      }
      return false;
    }

    ,md5: function(s){function L(k,d){return(k<<d)|(k>>>(32-d))}function K(G,k){var I,d,F,H,x;F=(G&2147483648);H=(k&2147483648);I=(G&1073741824);d=(k&1073741824);x=(G&1073741823)+(k&1073741823);if(I&d){return(x^2147483648^F^H)}if(I|d){if(x&1073741824){return(x^3221225472^F^H)}else{return(x^1073741824^F^H)}}else{return(x^F^H)}}function r(d,F,k){return(d&F)|((~d)&k)}function q(d,F,k){return(d&k)|(F&(~k))}function p(d,F,k){return(d^F^k)}function n(d,F,k){return(F^(d|(~k)))}function u(G,F,aa,Z,k,H,I){G=K(G,K(K(r(F,aa,Z),k),I));return K(L(G,H),F)}function f(G,F,aa,Z,k,H,I){G=K(G,K(K(q(F,aa,Z),k),I));return K(L(G,H),F)}function D(G,F,aa,Z,k,H,I){G=K(G,K(K(p(F,aa,Z),k),I));return K(L(G,H),F)}function t(G,F,aa,Z,k,H,I){G=K(G,K(K(n(F,aa,Z),k),I));return K(L(G,H),F)}function e(G){var Z;var F=G.length;var x=F+8;var k=(x-(x%64))/64;var I=(k+1)*16;var aa=Array(I-1);var d=0;var H=0;while(H<F){Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=(aa[Z]| (G.charCodeAt(H)<<d));H++}Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=aa[Z]|(128<<d);aa[I-2]=F<<3;aa[I-1]=F>>>29;return aa}function B(x){var k="",F="",G,d;for(d=0;d<=3;d++){G=(x>>>(d*8))&255;F="0"+G.toString(16);k=k+F.substr(F.length-2,2)}return k}function J(k){k=k.replace(/rn/g,"n");var d="";for(var F=0;F<k.length;F++){var x=k.charCodeAt(F);if(x<128){d+=String.fromCharCode(x)}else{if((x>127)&&(x<2048)){d+=String.fromCharCode((x>>6)|192);d+=String.fromCharCode((x&63)|128)}else{d+=String.fromCharCode((x>>12)|224);d+=String.fromCharCode(((x>>6)&63)|128);d+=String.fromCharCode((x&63)|128)}}}return d}var C=Array();var P,h,E,v,g,Y,X,W,V;var S=7,Q=12,N=17,M=22;var A=5,z=9,y=14,w=20;var o=4,m=11,l=16,j=23;var U=6,T=10,R=15,O=21;s=J(s);C=e(s);Y=1732584193;X=4023233417;W=2562383102;V=271733878;for(P=0;P<C.length;P+=16){h=Y;E=X;v=W;g=V;Y=u(Y,X,W,V,C[P+0],S,3614090360);V=u(V,Y,X,W,C[P+1],Q,3905402710);W=u(W,V,Y,X,C[P+2],N,606105819);X=u(X,W,V,Y,C[P+3],M,3250441966);Y=u(Y,X,W,V,C[P+4],S,4118548399);V=u(V,Y,X,W,C[P+5],Q,1200080426);W=u(W,V,Y,X,C[P+6],N,2821735955);X=u(X,W,V,Y,C[P+7],M,4249261313);Y=u(Y,X,W,V,C[P+8],S,1770035416);V=u(V,Y,X,W,C[P+9],Q,2336552879);W=u(W,V,Y,X,C[P+10],N,4294925233);X=u(X,W,V,Y,C[P+11],M,2304563134);Y=u(Y,X,W,V,C[P+12],S,1804603682);V=u(V,Y,X,W,C[P+13],Q,4254626195);W=u(W,V,Y,X,C[P+14],N,2792965006);X=u(X,W,V,Y,C[P+15],M,1236535329);Y=f(Y,X,W,V,C[P+1],A,4129170786);V=f(V,Y,X,W,C[P+6],z,3225465664);W=f(W,V,Y,X,C[P+11],y,643717713);X=f(X,W,V,Y,C[P+0],w,3921069994);Y=f(Y,X,W,V,C[P+5],A,3593408605);V=f(V,Y,X,W,C[P+10],z,38016083);W=f(W,V,Y,X,C[P+15],y,3634488961);X=f(X,W,V,Y,C[P+4],w,3889429448);Y=f(Y,X,W,V,C[P+9],A,568446438);V=f(V,Y,X,W,C[P+14],z,3275163606);W=f(W,V,Y,X,C[P+3],y,4107603335);X=f(X,W,V,Y,C[P+8],w,1163531501);Y=f(Y,X,W,V,C[P+13],A,2850285829);V=f(V,Y,X,W,C[P+2],z,4243563512);W=f(W,V,Y,X,C[P+7],y,1735328473);X=f(X,W,V,Y,C[P+12],w,2368359562);Y=D(Y,X,W,V,C[P+5],o,4294588738);V=D(V,Y,X,W,C[P+8],m,2272392833);W=D(W,V,Y,X,C[P+11],l,1839030562);X=D(X,W,V,Y,C[P+14],j,4259657740);Y=D(Y,X,W,V,C[P+1],o,2763975236);V=D(V,Y,X,W,C[P+4],m,1272893353);W=D(W,V,Y,X,C[P+7],l,4139469664);X=D(X,W,V,Y,C[P+10],j,3200236656);Y=D(Y,X,W,V,C[P+13],o,681279174);V=D(V,Y,X,W,C[P+0],m,3936430074);W=D(W,V,Y,X,C[P+3],l,3572445317);X=D(X,W,V,Y,C[P+6],j,76029189);Y=D(Y,X,W,V,C[P+9],o,3654602809);V=D(V,Y,X,W,C[P+12],m,3873151461);W=D(W,V,Y,X,C[P+15],l,530742520);X=D(X,W,V,Y,C[P+2],j,3299628645);Y=t(Y,X,W,V,C[P+0],U,4096336452);V=t(V,Y,X,W,C[P+7],T,1126891415);W=t(W,V,Y,X,C[P+14],R,2878612391);X=t(X,W,V,Y,C[P+5],O,4237533241);Y=t(Y,X,W,V,C[P+12],U,1700485571);V=t(V,Y,X,W,C[P+3],T,2399980690);W=t(W,V,Y,X,C[P+10],R,4293915773);X=t(X,W,V,Y,C[P+1],O,2240044497);Y=t(Y,X,W,V,C[P+8],U,1873313359);V=t(V,Y,X,W,C[P+15],T,4264355552);W=t(W,V,Y,X,C[P+6],R,2734768916);X=t(X,W,V,Y,C[P+13],O,1309151649);Y=t(Y,X,W,V,C[P+4],U,4149444226);V=t(V,Y,X,W,C[P+11],T,3174756917);W=t(W,V,Y,X,C[P+2],R,718787259);X=t(X,W,V,Y,C[P+9],O,3951481745);Y=K(Y,h);X=K(X,E);W=K(W,v);V=K(V,g)}var i=B(Y)+B(X)+B(W)+B(V);return i.toLowerCase()}
    ,stripProductDelimiters: function(val) {
        if (val) {
            return val.replace(/\;|\||\,/gi, '-');
        }
    }

    ,setCookie: function(name, value, seconds, domain) {
        domain = document.location.hostname;
        var expires = '';
        var expiresNow = '';
        var date = new Date();
        date.setTime(date.getTime() + (-1 * 1000));
        expiresNow = "; expires=" + date.toGMTString();

        if (typeof(seconds) != 'undefined') {
            date.setTime(date.getTime() + (seconds * 1000));
            expires = '; expires=' + date.toGMTString();
        }

        var type = typeof(value);
        type = type.toLowerCase();
        if (type != 'undefined' && type != 'string') {
            value = JSON.stringify(value);
        }

        // fix scoping issues
        // keep writing the old cookie, but make it expire
        document.cookie = name + '=' + value + expiresNow + '; path=/';

        // now just set the right one
        document.cookie = name + '=' + value + expires + '; path=/; domain=' + domain;
    }

    ,getCookie: function(name) {
        name = name + '=';
        var carray = document.cookie.split(';'), value;

        for (var i=0; i<carray.length; i++) {
            var c = carray[i];
            while (c.charAt(0) == ' ') {
                c = c.substring(1, c.length);
            }
            if (c.indexOf(name) == 0) {
                value = c.substring(name.length, c.length);
                try {
                    value = JSON.parse(value);
                } catch(ex) {}

                return value;
            }
        }

        return null;
    }

    ,deleteCookie: function(name) {
        this.setCookie(name, '', -1);
        this.setCookie(name, '', -1, document.location.hostname);
    }

    ,mapAdobeVars: function(s) {
        var vars = {
            pageName		: 'Page - Analytics Pagename'
            ,channel		: 'Page - Section Name'
            ,campaign		: 'Campaign - ID'
            ,currencyCode	: 'Page - Currency Code'
            ,purchaseID		: 'Order - ID'
            ,prop1			: 'Visitor - Account ID'
            ,prop2			: 'Page - Product Name'
            ,prop4			: 'Page - Type'
            ,prop6			: 'Search - Type'
            ,prop7			: 'Search - Facet List'
            ,prop8			: 'Search - Feature Used'
            ,prop12			: 'Visitor - User ID'
            ,prop13			: 'Search - Sort Type'
            ,prop14			: 'Page - Load Time'
            ,prop15         : 'Support - Topic Name'
            ,prop16			: 'Page - Business Unit'
            ,prop21			: 'Search - Criteria'
            ,prop24			: 'Page - Language'
            ,prop25			: 'Page - Product Feature'
            ,prop28         : 'Support - Search Criteria'
            ,prop30			: 'Visitor - IP Address'
            ,prop33         : 'Page - Product Application Version'
            ,prop34         : 'Page - Website Extensions'
            ,prop60			: 'Search - Data Form Criteria'
            ,prop63			: 'Page - Extended Page Name'
            ,prop65         : 'Page - Online State'
            ,prop67         : 'Research Networks'
            ,prop40: 'Page - UX Properties'

            ,eVar3			: 'Search - Total Results'
            ,eVar7			: 'Visitor - Account Name'
            ,eVar15			: 'Event - Search Results Click Position'
            ,eVar19			: 'Search - Advanced Criteria'
            ,eVar21			: 'Promo - Clicked ID'
            ,eVar22			: 'Page - Test ID'
            ,eVar27			: 'Event - AutoSuggest Search Data'
            ,eVar157		: 'Event - AutoSuggest Search Typed Term'
            ,eVar156		: 'Event - AutoSuggest Search Selected Term'
            ,eVar162		: 'Event - AutoSuggest Search Category'
            ,eVar163		: 'Event - AutoSuggest Search Details'
            ,eVar33			: 'Visitor - Access Type'
            ,eVar34			: 'Order - Promo Code'
            ,eVar39			: 'Order - Payment Method'
            ,eVar41			: 'Visitor - Industry'
            ,eVar42			: 'Visitor - SIS ID'
            ,eVar43			: 'Page - Error Type'
            ,eVar44			: 'Event - Updated User Fields'
            ,eVar48			: 'Email - Recipient ID'
            ,eVar51			: 'Email - Message ID'
            ,eVar52			: 'Visitor - Department ID'
            ,eVar53			: 'Visitor - Department Name'
            ,eVar60			: 'Search - Within Content Criteria'
            ,eVar61			: 'Search - Within Results Criteria'
            ,eVar62			: 'Search - Result Types'
            ,eVar74			: 'Page - Journal Info'
            ,eVar59			: 'Page - Journal Publisher'
            ,eVar76			: 'Email - Broadlog ID'
            ,eVar78			: 'Visitor - Details'
            ,eVar80         : 'Visitor - Usage Path Info'
            ,eVar102		: 'Form - Name'
            ,eVar103        : 'Event - Conversion Driver'
            ,eVar105        : 'Search - Current Page'
            ,eVar106        : 'Visitor - App Session ID'
            ,eVar107        : 'Page - Secondary Product Name'
            ,eVar117        : 'Search - Database'
            ,eVar126        : 'Page - Environment'
            ,eVar141        : 'Search - Criteria Original'
            ,eVar143        : 'Page - Tabs'
            ,eVar161        : 'Search - Channel'
            ,eVar169        : 'Search - Facet Operation'
            ,eVar173        : 'Search - Details'
            ,eVar174        : 'Campaign - Spredfast ID'
            ,eVar175        : 'Visitor - TMX Device ID'
            ,eVar176        : 'Visitor - TMX Request ID'
            ,eVar148        : 'Visitor - Platform Name'
            ,eVar149        : 'Visitor - Platform ID'
            ,eVar152        : 'Visitor - Product ID'
            ,eVar153        : 'Visitor - Superaccount ID'
            ,eVar154        : 'Visitor - Superaccount Name'
            ,eVar177        : 'Page - Context Domain'
            ,eVar189    : 'Page - Experimentation User Id'
            ,eVar190    : 'Page - Identity User'
            ,eVar199    : 'Page - ID+ Parameters'

            ,list2			: 'Page - Widget Names'
            ,list3			: 'Promo - IDs'
        };

        for (var i in vars) {
            s[i] = s[i] ? s[i] : _satellite.getVar(vars[i]);
        }
    }
};

// async support fallback
(function(w) {
	var eventBuffer = [];
	if(w.appData) {
		if(Array.isArray(w.appData)) {
			eventBuffer = w.appData;
		} else {
			console.error('Elsevier DataLayer "window.appData" must be specified as array');
			return;
		}
    }

	w.appData = [];

	var oldPush = w.appData.push;

	var appDataPush = function() {
        oldPush.apply(w.appData, arguments);
        for(var i=0; i<arguments.length; i++) {
            var data = arguments[i];
            if(data.event) {
                if(data.event == 'pageLoad') {
                    w.pageDataTracker.trackPageLoad(data);
                } else {
                    w.pageDataTracker.trackEvent(data.event, data);
                }
            }
        }
	};

	w.appData.push = appDataPush;
	for(var i=0; i<eventBuffer.length; i++) {
	    var data = eventBuffer[i];
	    w.appData.push(data);
	}
})(window);

</script><script>_satellite["_runScript1"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><script src="https://cdn.plu.mx/widget-summary.js" async=""></script><script>_satellite["_runScript2"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-hide ot-fade-in"></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" checked="" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" checked="" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details Listâ€Ž</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" checked="" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details Listâ€Ž</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Searchâ€¦" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div><button aria-label="Feedback" type="button" id="_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E" data-layout="badgeBlank" class="_pendo-badge _pendo-badge_" style="z-index: 19000; margin: 0px; height: 32px; width: 128px; font-size: 0px; background: rgba(255, 255, 255, 0); padding: 0px; line-height: 1; min-width: auto; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; border: 0px; float: none; vertical-align: baseline; cursor: pointer; position: absolute; top: 63676px; left: 912px;"><img id="pendo-image-badge-19b66351" src="https://pendo-static-5661679399600128.storage.googleapis.com/D_T2uHq_M1r-XQq8htU6Z3GjHfE/guide-media-75af8ddc-3c43-49fd-8836-cfc7e2c3ea60" alt="Feedback" data-_pendo-image-1="" class="_pendo-image _pendo-badge-image" style="display: block; height: 32px; width: 128px; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; float: none; vertical-align: baseline;"></button></body></html>