<html lang="en-US"><head>
<meta name="citation_pii" content="S1566253522000367">
<meta name="citation_issn" content="1566-2535">
<meta name="citation_volume" content="83-84">
<meta name="citation_lastpage" content="52">
<meta name="citation_publisher" content="Elsevier">
<meta name="citation_firstpage" content="19">
<meta name="citation_journal_title" content="Information Fusion">
<meta name="citation_type" content="JOUR">
<meta name="citation_doi" content="10.1016/j.inffus.2022.03.009">
<meta name="dc.identifier" content="10.1016/j.inffus.2022.03.009">
<meta name="citation_article_type" content="Full-length article">
<meta property="og:description" content="Affective computing conjoins the research topics of emotion recognition and sentiment analysis, and can be realized with unimodal or multimodal data, …">
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522X0002X-cov150h.gif">
<meta name="citation_title" content="A systematic review on affective computing: emotion models, databases, and recent advances">
<meta property="og:title" content="A systematic review on affective computing: emotion models, databases, and recent advances">
<meta name="citation_publication_date" content="2022/07/01">
<meta name="citation_online_date" content="2022/03/25">
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR">
<title>A systematic review on affective computing: emotion models, databases, and recent advances - ScienceDirect</title>
<link rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S1566253522000367">
<meta property="og:type" content="article">
<meta name="viewport" content="initial-scale=1">
<meta name="SDTech" content="Proudly brought to you by the SD Technology team">
<script async="" src="https://cdn.pendo.io/agent/static/d6c1d995-bc7e-4e53-77f1-2ea4ecbb9565/pendo.js"></script><script type="text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814813181"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814813181",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
<link rel="shortcut icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
<link rel="icon" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/103/images/favSD.ico" type="image/x-icon">
<link rel="stylesheet" href="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/arp.css">
<link href="//cdn.pendo.io" rel="dns-prefetch">
<link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous">
<link rel="dns-prefetch" href="https://smetrics.elsevier.com">
<script async="" id="reading-assistant-script-tag" src="/feature/assets/ai-components/S1566253522000367?componentVersion=V10&amp;jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMxNTY2MjUzNTIyMDAwMzY3IiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiZDNlOTE2NTMxZGUwYzg0OTE2MGI3M2Y0NzkyNTBiODU5NGNhZ3hycWIiLCJleHAiOjE3Mjg5OTI1MjEsImlhdCI6MTcyODk5MDcyMSwidmVyc2lvbiI6MSwianRpIjoiNjAxYzQzYWEtZjM0Zi00ZmMxLWJhM2UtZDJiOTUwYjgyNTVjIn0.BJnzNfzRXzUG0ssdD1fT-DQ5NX1ZxAtXdIUOLz2tXKE" type="text/javascript"></script>
<script type="text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"086105B72B4A357C-217DA23D8EE0F9D5","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"eu-west-1","platform":"sdtech","entitled":true,"crawler":"","journal":"Information Fusion","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
<script data-cfasync="false" src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" data-domain-script="865ea198-88cc-4e41-8952-1df75d554d02"></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement.min.js" async=""></script><script src="https://assets.adobedtm.com/extensions/EP8757b503532a44a68eee17773f6f10a0/AppMeasurement_Module_ActivityMap.min.js" async=""></script><script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/6a62c1bc1779/RCa16d232f95a944c0aabdea6621a2ef94-source.min.js" async=""></script><script src="https://cdn.cookielaw.org/scripttemplates/202402.1.0/otBannerSdk.js" async="" type="text/javascript"></script><meta http-equiv="origin-trial" content="AlK2UR5SkAlj8jjdEc9p3F3xuFYlF6LYjAML3EOqw1g26eCwWPjdmecULvBH5MVPoqKYrOfPhYVL71xAXI1IBQoAAAB8eyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="Amm8/NmvvQfhwCib6I7ZsmUxiSCfOxWxHayJwyU1r3gRIItzr7bNQid6O8ZYaE1GSQTa69WwhPC9flq/oYkRBwsAAACCeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiV2ViVmlld1hSZXF1ZXN0ZWRXaXRoRGVwcmVjYXRpb24iLCJleHBpcnkiOjE3NTgwNjcxOTksImlzU3ViZG9tYWluIjp0cnVlfQ=="><meta http-equiv="origin-trial" content="A9wSqI5i0iwGdf6L1CERNdmsTPgVu44ewj8QxTBYgsv1LCPUVF7YmWOvTappqB1139jAymxUW/RO8zmMqo4zlAAAAACNeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+d7vJfYtay4OUbdtRPZA3y7bKQLsxaMEPmxgfhBGqKXNrdkCQeJlUwqa6EBbSfjwFtJWTrWIioXeMW+y8bWAgQAAACTeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiRmxlZGdlQmlkZGluZ0FuZEF1Y3Rpb25TZXJ2ZXIiLCJleHBpcnkiOjE3MzY4MTI4MDAsImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><script src="https://securepubads.g.doubleclick.net/pagead/managed/js/gpt/m202410100101/pubads_impl.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link id="plx-css-summary" type="text/css" rel="stylesheet" href="//cdn.plu.mx/summary.css"><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//cdn.plu.mx/extjs/xss.js"></script><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
</style><style id="onetrust-style">#onetrust-banner-sdk{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}#onetrust-banner-sdk .onetrust-vendors-list-handler{cursor:pointer;color:#1f96db;font-size:inherit;font-weight:bold;text-decoration:none;margin-left:5px}#onetrust-banner-sdk .onetrust-vendors-list-handler:hover{color:#1f96db}#onetrust-banner-sdk:focus{outline:2px solid #000;outline-offset:-2px}#onetrust-banner-sdk a:focus{outline:2px solid #000}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{outline-offset:1px}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{height:64px;width:64px}#onetrust-banner-sdk .ot-tcf2-vendor-count.ot-text-bold{font-weight:bold}#onetrust-banner-sdk .ot-close-icon,#onetrust-pc-sdk .ot-close-icon,#ot-sync-ntfy .ot-close-icon{background-size:contain;background-repeat:no-repeat;background-position:center;height:12px;width:12px}#onetrust-banner-sdk .powered-by-logo,#onetrust-banner-sdk .ot-pc-footer-logo a,#onetrust-pc-sdk .powered-by-logo,#onetrust-pc-sdk .ot-pc-footer-logo a,#ot-sync-ntfy .powered-by-logo,#ot-sync-ntfy .ot-pc-footer-logo a{background-size:contain;background-repeat:no-repeat;background-position:center;height:25px;width:152px;display:block;text-decoration:none;font-size:.75em}#onetrust-banner-sdk .powered-by-logo:hover,#onetrust-banner-sdk .ot-pc-footer-logo a:hover,#onetrust-pc-sdk .powered-by-logo:hover,#onetrust-pc-sdk .ot-pc-footer-logo a:hover,#ot-sync-ntfy .powered-by-logo:hover,#ot-sync-ntfy .ot-pc-footer-logo a:hover{color:#565656}#onetrust-banner-sdk h3 *,#onetrust-banner-sdk h4 *,#onetrust-banner-sdk h6 *,#onetrust-banner-sdk button *,#onetrust-banner-sdk a[data-parent-id] *,#onetrust-pc-sdk h3 *,#onetrust-pc-sdk h4 *,#onetrust-pc-sdk h6 *,#onetrust-pc-sdk button *,#onetrust-pc-sdk a[data-parent-id] *,#ot-sync-ntfy h3 *,#ot-sync-ntfy h4 *,#ot-sync-ntfy h6 *,#ot-sync-ntfy button *,#ot-sync-ntfy a[data-parent-id] *{font-size:inherit;font-weight:inherit;color:inherit}#onetrust-banner-sdk .ot-hide,#onetrust-pc-sdk .ot-hide,#ot-sync-ntfy .ot-hide{display:none !important}#onetrust-banner-sdk button.ot-link-btn:hover,#onetrust-pc-sdk button.ot-link-btn:hover,#ot-sync-ntfy button.ot-link-btn:hover{text-decoration:underline;opacity:1}#onetrust-pc-sdk .ot-sdk-row .ot-sdk-column{padding:0}#onetrust-pc-sdk .ot-sdk-container{padding-right:0}#onetrust-pc-sdk .ot-sdk-row{flex-direction:initial;width:100%}#onetrust-pc-sdk [type=checkbox]:checked,#onetrust-pc-sdk [type=checkbox]:not(:checked){pointer-events:initial}#onetrust-pc-sdk [type=checkbox]:disabled+label::before,#onetrust-pc-sdk [type=checkbox]:disabled+label:after,#onetrust-pc-sdk [type=checkbox]:disabled+label{pointer-events:none;opacity:.7}#onetrust-pc-sdk #vendor-list-content{transform:translate3d(0, 0, 0)}#onetrust-pc-sdk li input[type=checkbox]{z-index:1}#onetrust-pc-sdk li .ot-checkbox label{z-index:2}#onetrust-pc-sdk li .ot-checkbox input[type=checkbox]{height:auto;width:auto}#onetrust-pc-sdk li .host-title a,#onetrust-pc-sdk li .ot-host-name a,#onetrust-pc-sdk li .accordion-text,#onetrust-pc-sdk li .ot-acc-txt{z-index:2;position:relative}#onetrust-pc-sdk input{margin:3px .1ex}#onetrust-pc-sdk .pc-logo,#onetrust-pc-sdk .ot-pc-logo{height:60px;width:180px;background-position:center;background-size:contain;background-repeat:no-repeat;display:inline-flex;justify-content:center;align-items:center}#onetrust-pc-sdk .pc-logo img,#onetrust-pc-sdk .ot-pc-logo img{max-height:100%;max-width:100%}#onetrust-pc-sdk .screen-reader-only,#onetrust-pc-sdk .ot-scrn-rdr,.ot-sdk-cookie-policy .screen-reader-only,.ot-sdk-cookie-policy .ot-scrn-rdr{border:0;clip:rect(0 0 0 0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#onetrust-pc-sdk.ot-fade-in,.onetrust-pc-dark-filter.ot-fade-in,#onetrust-banner-sdk.ot-fade-in{animation-name:onetrust-fade-in;animation-duration:400ms;animation-timing-function:ease-in-out}#onetrust-pc-sdk.ot-hide{display:none !important}.onetrust-pc-dark-filter.ot-hide{display:none !important}#ot-sdk-btn.ot-sdk-show-settings,#ot-sdk-btn.optanon-show-settings{color:#68b631;border:1px solid #68b631;height:auto;white-space:normal;word-wrap:break-word;padding:.8em 2em;font-size:.8em;line-height:1.2;cursor:pointer;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease}#ot-sdk-btn.ot-sdk-show-settings:hover,#ot-sdk-btn.optanon-show-settings:hover{color:#fff;background-color:#68b631}.onetrust-pc-dark-filter{background:rgba(0,0,0,.5);z-index:2147483646;width:100%;height:100%;overflow:hidden;position:fixed;top:0;bottom:0;left:0}@keyframes onetrust-fade-in{0%{opacity:0}100%{opacity:1}}.ot-cookie-label{text-decoration:underline}@media only screen and (min-width: 426px)and (max-width: 896px)and (orientation: landscape){#onetrust-pc-sdk p{font-size:.75em}}#onetrust-banner-sdk .banner-option-input:focus+label{outline:1px solid #000;outline-style:auto}.category-vendors-list-handler+a:focus,.category-vendors-list-handler+a:focus-visible{outline:2px solid #000}#onetrust-pc-sdk .ot-userid-title{margin-top:10px}#onetrust-pc-sdk .ot-userid-title>span,#onetrust-pc-sdk .ot-userid-timestamp>span{font-weight:700}#onetrust-pc-sdk .ot-userid-desc{font-style:italic}#onetrust-pc-sdk .ot-host-desc a{pointer-events:initial}#onetrust-pc-sdk .ot-ven-hdr>p a{position:relative;z-index:2;pointer-events:initial}#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info a{margin-right:auto}#onetrust-pc-sdk .ot-pc-footer-logo img{width:136px;height:16px}#onetrust-pc-sdk .ot-pur-vdr-count{font-weight:400;font-size:.7rem;padding-top:3px;display:block}#onetrust-banner-sdk .ot-optout-signal,#onetrust-pc-sdk .ot-optout-signal{border:1px solid #32ae88;border-radius:3px;padding:5px;margin-bottom:10px;background-color:#f9fffa;font-size:.85rem;line-height:2}#onetrust-banner-sdk .ot-optout-signal .ot-optout-icon,#onetrust-pc-sdk .ot-optout-signal .ot-optout-icon{display:inline;margin-right:5px}#onetrust-banner-sdk .ot-optout-signal svg,#onetrust-pc-sdk .ot-optout-signal svg{height:20px;width:30px;transform:scale(0.5)}#onetrust-banner-sdk .ot-optout-signal svg path,#onetrust-pc-sdk .ot-optout-signal svg path{fill:#32ae88}#onetrust-banner-sdk,#onetrust-pc-sdk,#ot-sdk-cookie-policy,#ot-sync-ntfy{font-size:16px}#onetrust-banner-sdk *,#onetrust-banner-sdk ::after,#onetrust-banner-sdk ::before,#onetrust-pc-sdk *,#onetrust-pc-sdk ::after,#onetrust-pc-sdk ::before,#ot-sdk-cookie-policy *,#ot-sdk-cookie-policy ::after,#ot-sdk-cookie-policy ::before,#ot-sync-ntfy *,#ot-sync-ntfy ::after,#ot-sync-ntfy ::before{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}#onetrust-banner-sdk div,#onetrust-banner-sdk span,#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-banner-sdk p,#onetrust-banner-sdk img,#onetrust-banner-sdk svg,#onetrust-banner-sdk button,#onetrust-banner-sdk section,#onetrust-banner-sdk a,#onetrust-banner-sdk label,#onetrust-banner-sdk input,#onetrust-banner-sdk ul,#onetrust-banner-sdk li,#onetrust-banner-sdk nav,#onetrust-banner-sdk table,#onetrust-banner-sdk thead,#onetrust-banner-sdk tr,#onetrust-banner-sdk td,#onetrust-banner-sdk tbody,#onetrust-banner-sdk .ot-main-content,#onetrust-banner-sdk .ot-toggle,#onetrust-banner-sdk #ot-content,#onetrust-banner-sdk #ot-pc-content,#onetrust-banner-sdk .checkbox,#onetrust-pc-sdk div,#onetrust-pc-sdk span,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#onetrust-pc-sdk p,#onetrust-pc-sdk img,#onetrust-pc-sdk svg,#onetrust-pc-sdk button,#onetrust-pc-sdk section,#onetrust-pc-sdk a,#onetrust-pc-sdk label,#onetrust-pc-sdk input,#onetrust-pc-sdk ul,#onetrust-pc-sdk li,#onetrust-pc-sdk nav,#onetrust-pc-sdk table,#onetrust-pc-sdk thead,#onetrust-pc-sdk tr,#onetrust-pc-sdk td,#onetrust-pc-sdk tbody,#onetrust-pc-sdk .ot-main-content,#onetrust-pc-sdk .ot-toggle,#onetrust-pc-sdk #ot-content,#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk .checkbox,#ot-sdk-cookie-policy div,#ot-sdk-cookie-policy span,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy p,#ot-sdk-cookie-policy img,#ot-sdk-cookie-policy svg,#ot-sdk-cookie-policy button,#ot-sdk-cookie-policy section,#ot-sdk-cookie-policy a,#ot-sdk-cookie-policy label,#ot-sdk-cookie-policy input,#ot-sdk-cookie-policy ul,#ot-sdk-cookie-policy li,#ot-sdk-cookie-policy nav,#ot-sdk-cookie-policy table,#ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy tr,#ot-sdk-cookie-policy td,#ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy .ot-main-content,#ot-sdk-cookie-policy .ot-toggle,#ot-sdk-cookie-policy #ot-content,#ot-sdk-cookie-policy #ot-pc-content,#ot-sdk-cookie-policy .checkbox,#ot-sync-ntfy div,#ot-sync-ntfy span,#ot-sync-ntfy h1,#ot-sync-ntfy h2,#ot-sync-ntfy h3,#ot-sync-ntfy h4,#ot-sync-ntfy h5,#ot-sync-ntfy h6,#ot-sync-ntfy p,#ot-sync-ntfy img,#ot-sync-ntfy svg,#ot-sync-ntfy button,#ot-sync-ntfy section,#ot-sync-ntfy a,#ot-sync-ntfy label,#ot-sync-ntfy input,#ot-sync-ntfy ul,#ot-sync-ntfy li,#ot-sync-ntfy nav,#ot-sync-ntfy table,#ot-sync-ntfy thead,#ot-sync-ntfy tr,#ot-sync-ntfy td,#ot-sync-ntfy tbody,#ot-sync-ntfy .ot-main-content,#ot-sync-ntfy .ot-toggle,#ot-sync-ntfy #ot-content,#ot-sync-ntfy #ot-pc-content,#ot-sync-ntfy .checkbox{font-family:inherit;font-weight:normal;-webkit-font-smoothing:auto;letter-spacing:normal;line-height:normal;padding:0;margin:0;height:auto;min-height:0;max-height:none;width:auto;min-width:0;max-width:none;border-radius:0;border:none;clear:none;float:none;position:static;bottom:auto;left:auto;right:auto;top:auto;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;white-space:normal;background:none;overflow:visible;vertical-align:baseline;visibility:visible;z-index:auto;box-shadow:none}#onetrust-banner-sdk label:before,#onetrust-banner-sdk label:after,#onetrust-banner-sdk .checkbox:after,#onetrust-banner-sdk .checkbox:before,#onetrust-pc-sdk label:before,#onetrust-pc-sdk label:after,#onetrust-pc-sdk .checkbox:after,#onetrust-pc-sdk .checkbox:before,#ot-sdk-cookie-policy label:before,#ot-sdk-cookie-policy label:after,#ot-sdk-cookie-policy .checkbox:after,#ot-sdk-cookie-policy .checkbox:before,#ot-sync-ntfy label:before,#ot-sync-ntfy label:after,#ot-sync-ntfy .checkbox:after,#ot-sync-ntfy .checkbox:before{content:"";content:none}#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{position:relative;width:100%;max-width:100%;margin:0 auto;padding:0 20px;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{width:100%;float:left;box-sizing:border-box;padding:0;display:initial}@media(min-width: 400px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:90%;padding:0}}@media(min-width: 550px){#onetrust-banner-sdk .ot-sdk-container,#onetrust-pc-sdk .ot-sdk-container,#ot-sdk-cookie-policy .ot-sdk-container{width:100%}#onetrust-banner-sdk .ot-sdk-column,#onetrust-banner-sdk .ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-column,#onetrust-pc-sdk .ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-column,#ot-sdk-cookie-policy .ot-sdk-columns{margin-left:4%}#onetrust-banner-sdk .ot-sdk-column:first-child,#onetrust-banner-sdk .ot-sdk-columns:first-child,#onetrust-pc-sdk .ot-sdk-column:first-child,#onetrust-pc-sdk .ot-sdk-columns:first-child,#ot-sdk-cookie-policy .ot-sdk-column:first-child,#ot-sdk-cookie-policy .ot-sdk-columns:first-child{margin-left:0}#onetrust-banner-sdk .ot-sdk-two.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-two.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-two.ot-sdk-columns{width:13.3333333333%}#onetrust-banner-sdk .ot-sdk-three.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-three.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-three.ot-sdk-columns{width:22%}#onetrust-banner-sdk .ot-sdk-four.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-four.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-four.ot-sdk-columns{width:30.6666666667%}#onetrust-banner-sdk .ot-sdk-eight.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eight.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eight.ot-sdk-columns{width:65.3333333333%}#onetrust-banner-sdk .ot-sdk-nine.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-nine.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-nine.ot-sdk-columns{width:74%}#onetrust-banner-sdk .ot-sdk-ten.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-ten.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-ten.ot-sdk-columns{width:82.6666666667%}#onetrust-banner-sdk .ot-sdk-eleven.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-eleven.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-eleven.ot-sdk-columns{width:91.3333333333%}#onetrust-banner-sdk .ot-sdk-twelve.ot-sdk-columns,#onetrust-pc-sdk .ot-sdk-twelve.ot-sdk-columns,#ot-sdk-cookie-policy .ot-sdk-twelve.ot-sdk-columns{width:100%;margin-left:0}}#onetrust-banner-sdk h1,#onetrust-banner-sdk h2,#onetrust-banner-sdk h3,#onetrust-banner-sdk h4,#onetrust-banner-sdk h5,#onetrust-banner-sdk h6,#onetrust-pc-sdk h1,#onetrust-pc-sdk h2,#onetrust-pc-sdk h3,#onetrust-pc-sdk h4,#onetrust-pc-sdk h5,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h1,#ot-sdk-cookie-policy h2,#ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy h5,#ot-sdk-cookie-policy h6{margin-top:0;font-weight:600;font-family:inherit}#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem;line-height:1.2}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem;line-height:1.25}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem;line-height:1.3}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem;line-height:1.35}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem;line-height:1.5}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem;line-height:1.6}@media(min-width: 550px){#onetrust-banner-sdk h1,#onetrust-pc-sdk h1,#ot-sdk-cookie-policy h1{font-size:1.5rem}#onetrust-banner-sdk h2,#onetrust-pc-sdk h2,#ot-sdk-cookie-policy h2{font-size:1.5rem}#onetrust-banner-sdk h3,#onetrust-pc-sdk h3,#ot-sdk-cookie-policy h3{font-size:1.5rem}#onetrust-banner-sdk h4,#onetrust-pc-sdk h4,#ot-sdk-cookie-policy h4{font-size:1.5rem}#onetrust-banner-sdk h5,#onetrust-pc-sdk h5,#ot-sdk-cookie-policy h5{font-size:1.5rem}#onetrust-banner-sdk h6,#onetrust-pc-sdk h6,#ot-sdk-cookie-policy h6{font-size:1.5rem}}#onetrust-banner-sdk p,#onetrust-pc-sdk p,#ot-sdk-cookie-policy p{margin:0 0 1em 0;font-family:inherit;line-height:normal}#onetrust-banner-sdk a,#onetrust-pc-sdk a,#ot-sdk-cookie-policy a{color:#565656;text-decoration:underline}#onetrust-banner-sdk a:hover,#onetrust-pc-sdk a:hover,#ot-sdk-cookie-policy a:hover{color:#565656;text-decoration:none}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-button,#onetrust-banner-sdk button,#onetrust-pc-sdk .ot-sdk-button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy .ot-sdk-button,#ot-sdk-cookie-policy button{display:inline-block;height:38px;padding:0 30px;color:#555;text-align:center;font-size:.9em;font-weight:400;line-height:38px;letter-spacing:.01em;text-decoration:none;white-space:nowrap;background-color:rgba(0,0,0,0);border-radius:2px;border:1px solid #bbb;cursor:pointer;box-sizing:border-box}#onetrust-banner-sdk .ot-sdk-button:hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#onetrust-pc-sdk .ot-sdk-button:hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus,#ot-sdk-cookie-policy .ot-sdk-button:hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):hover,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:not(.ot-link-btn):focus{color:#333;border-color:#888;opacity:.7}#onetrust-banner-sdk .ot-sdk-button:focus,#onetrust-banner-sdk :not(.ot-leg-btn-container)>button:focus,#onetrust-pc-sdk .ot-sdk-button:focus,#onetrust-pc-sdk :not(.ot-leg-btn-container)>button:focus,#ot-sdk-cookie-policy .ot-sdk-button:focus,#ot-sdk-cookie-policy :not(.ot-leg-btn-container)>button:focus{outline:2px solid #000}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-banner-sdk button.ot-sdk-button-primary,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary,#onetrust-pc-sdk button.ot-sdk-button-primary,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary,#ot-sdk-cookie-policy button.ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary{color:#fff;background-color:#33c3f0;border-color:#33c3f0}#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-banner-sdk button.ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-banner-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-banner-sdk button.ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-banner-sdk input[type=button].ot-sdk-button-primary:focus,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:hover,#onetrust-pc-sdk button.ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:hover,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:hover,#onetrust-pc-sdk .ot-sdk-button.ot-sdk-button-primary:focus,#onetrust-pc-sdk button.ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=submit].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=reset].ot-sdk-button-primary:focus,#onetrust-pc-sdk input[type=button].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy button.ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:hover,#ot-sdk-cookie-policy .ot-sdk-button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy button.ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=submit].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=reset].ot-sdk-button-primary:focus,#ot-sdk-cookie-policy input[type=button].ot-sdk-button-primary:focus{color:#fff;background-color:#1eaedb;border-color:#1eaedb}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{height:38px;padding:6px 10px;background-color:#fff;border:1px solid #d1d1d1;border-radius:4px;box-shadow:none;box-sizing:border-box}#onetrust-banner-sdk input[type=text],#onetrust-pc-sdk input[type=text],#ot-sdk-cookie-policy input[type=text]{-webkit-appearance:none;-moz-appearance:none;appearance:none}#onetrust-banner-sdk input[type=text]:focus,#onetrust-pc-sdk input[type=text]:focus,#ot-sdk-cookie-policy input[type=text]:focus{border:1px solid #000;outline:0}#onetrust-banner-sdk label,#onetrust-pc-sdk label,#ot-sdk-cookie-policy label{display:block;margin-bottom:.5rem;font-weight:600}#onetrust-banner-sdk input[type=checkbox],#onetrust-pc-sdk input[type=checkbox],#ot-sdk-cookie-policy input[type=checkbox]{display:inline}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{list-style:circle inside}#onetrust-banner-sdk ul,#onetrust-pc-sdk ul,#ot-sdk-cookie-policy ul{padding-left:0;margin-top:0}#onetrust-banner-sdk ul ul,#onetrust-pc-sdk ul ul,#ot-sdk-cookie-policy ul ul{margin:1.5rem 0 1.5rem 3rem;font-size:90%}#onetrust-banner-sdk li,#onetrust-pc-sdk li,#ot-sdk-cookie-policy li{margin-bottom:1rem}#onetrust-banner-sdk th,#onetrust-banner-sdk td,#onetrust-pc-sdk th,#onetrust-pc-sdk td,#ot-sdk-cookie-policy th,#ot-sdk-cookie-policy td{padding:12px 15px;text-align:left;border-bottom:1px solid #e1e1e1}#onetrust-banner-sdk button,#onetrust-pc-sdk button,#ot-sdk-cookie-policy button{margin-bottom:1rem;font-family:inherit}#onetrust-banner-sdk .ot-sdk-container:after,#onetrust-banner-sdk .ot-sdk-row:after,#onetrust-pc-sdk .ot-sdk-container:after,#onetrust-pc-sdk .ot-sdk-row:after,#ot-sdk-cookie-policy .ot-sdk-container:after,#ot-sdk-cookie-policy .ot-sdk-row:after{content:"";display:table;clear:both}#onetrust-banner-sdk .ot-sdk-row,#onetrust-pc-sdk .ot-sdk-row,#ot-sdk-cookie-policy .ot-sdk-row{margin:0;max-width:none;display:block}#onetrust-banner-sdk{box-shadow:0 0 18px rgba(0,0,0,.2)}#onetrust-banner-sdk.otFlat{position:fixed;z-index:2147483645;bottom:0;right:0;left:0;background-color:#fff;max-height:90%;overflow-x:hidden;overflow-y:auto}#onetrust-banner-sdk.otFlat.top{top:0px;bottom:auto}#onetrust-banner-sdk.otRelFont{font-size:1rem}#onetrust-banner-sdk>.ot-sdk-container{overflow:hidden}#onetrust-banner-sdk::-webkit-scrollbar{width:11px}#onetrust-banner-sdk::-webkit-scrollbar-thumb{border-radius:10px;background:#c1c1c1}#onetrust-banner-sdk{scrollbar-arrow-color:#c1c1c1;scrollbar-darkshadow-color:#c1c1c1;scrollbar-face-color:#c1c1c1;scrollbar-shadow-color:#c1c1c1}#onetrust-banner-sdk #onetrust-policy{margin:1.25em 0 .625em 2em;overflow:hidden}#onetrust-banner-sdk #onetrust-policy .ot-gv-list-handler{float:left;font-size:.82em;padding:0;margin-bottom:0;border:0;line-height:normal;height:auto;width:auto}#onetrust-banner-sdk #onetrust-policy-title{font-size:1.2em;line-height:1.3;margin-bottom:10px}#onetrust-banner-sdk #onetrust-policy-text{clear:both;text-align:left;font-size:.88em;line-height:1.4}#onetrust-banner-sdk #onetrust-policy-text *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk #onetrust-policy-text a{font-weight:bold;margin-left:5px}#onetrust-banner-sdk #onetrust-policy-title,#onetrust-banner-sdk #onetrust-policy-text{color:dimgray;float:left}#onetrust-banner-sdk #onetrust-button-group-parent{min-height:1px;text-align:center}#onetrust-banner-sdk #onetrust-button-group{display:inline-block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{background-color:#68b631;color:#fff;border-color:#68b631;margin-right:1em;min-width:125px;height:auto;white-space:normal;word-break:break-word;word-wrap:break-word;padding:12px 10px;line-height:1.2;font-size:.813em;font-weight:600}#onetrust-banner-sdk #onetrust-pc-btn-handler.cookie-setting-link{background-color:#fff;border:none;color:#68b631;text-decoration:underline;padding-left:0;padding-right:0}#onetrust-banner-sdk .onetrust-close-btn-ui{width:44px;height:44px;background-size:12px;border:none;position:relative;margin:auto;padding:0}#onetrust-banner-sdk .banner_logo{display:none}#onetrust-banner-sdk.ot-bnr-w-logo .ot-bnr-logo{position:absolute;top:50%;transform:translateY(-50%);left:0px}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-policy{margin-left:65px}#onetrust-banner-sdk .ot-b-addl-desc{clear:both;float:left;display:block}#onetrust-banner-sdk #banner-options{float:left;display:table;margin-right:0;margin-left:1em;width:calc(100% - 1em)}#onetrust-banner-sdk .banner-option-input{cursor:pointer;width:auto;height:auto;border:none;padding:0;padding-right:3px;margin:0 0 10px;font-size:.82em;line-height:1.4}#onetrust-banner-sdk .banner-option-input *{pointer-events:none;font-size:inherit;line-height:inherit}#onetrust-banner-sdk .banner-option-input[aria-expanded=true]~.banner-option-details{display:block;height:auto}#onetrust-banner-sdk .banner-option-input[aria-expanded=true] .ot-arrow-container{transform:rotate(90deg)}#onetrust-banner-sdk .banner-option{margin-bottom:12px;margin-left:0;border:none;float:left;padding:0}#onetrust-banner-sdk .banner-option:first-child{padding-left:2px}#onetrust-banner-sdk .banner-option:not(:first-child){padding:0;border:none}#onetrust-banner-sdk .banner-option-header{cursor:pointer;display:inline-block}#onetrust-banner-sdk .banner-option-header :first-child{color:dimgray;font-weight:bold;float:left}#onetrust-banner-sdk .banner-option-header .ot-arrow-container{display:inline-block;border-top:6px solid rgba(0,0,0,0);border-bottom:6px solid rgba(0,0,0,0);border-left:6px solid dimgray;margin-left:10px;vertical-align:middle}#onetrust-banner-sdk .banner-option-details{display:none;font-size:.83em;line-height:1.5;padding:10px 0px 5px 10px;margin-right:10px;height:0px}#onetrust-banner-sdk .banner-option-details *{font-size:inherit;line-height:inherit;color:dimgray}#onetrust-banner-sdk .ot-arrow-container,#onetrust-banner-sdk .banner-option-details{transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-banner-sdk .ot-dpd-container{float:left}#onetrust-banner-sdk .ot-dpd-title{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-title,#onetrust-banner-sdk .ot-dpd-desc{font-size:.88em;line-height:1.4;color:dimgray}#onetrust-banner-sdk .ot-dpd-title *,#onetrust-banner-sdk .ot-dpd-desc *{font-size:inherit;line-height:inherit}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text *{margin-bottom:0}#onetrust-banner-sdk.ot-iab-2 .onetrust-vendors-list-handler{display:block;margin-left:0;margin-top:5px;clear:both;margin-bottom:0;padding:0;border:0;height:auto;width:auto}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk.ot-close-btn-link{padding-top:25px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container{top:15px;transform:none;right:15px}#onetrust-banner-sdk.ot-close-btn-link #onetrust-close-btn-container button{padding:0;white-space:pre-wrap;border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em}#onetrust-banner-sdk #onetrust-policy-text,#onetrust-banner-sdk .ot-dpd-desc,#onetrust-banner-sdk .ot-b-addl-desc{font-size:.813em;line-height:1.5}#onetrust-banner-sdk .ot-dpd-desc{margin-bottom:10px}#onetrust-banner-sdk .ot-dpd-desc>.ot-b-addl-desc{margin-top:10px;margin-bottom:10px;font-size:1em}@media only screen and (max-width: 425px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:6px;right:2px}#onetrust-banner-sdk #onetrust-policy{margin-left:0;margin-top:3em}#onetrust-banner-sdk #onetrust-button-group{display:block}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk .onetrust-close-btn-ui{top:auto;transform:none}#onetrust-banner-sdk #onetrust-policy-title{display:inline;float:none}#onetrust-banner-sdk #banner-options{margin:0;padding:0;width:100%}}@media only screen and (min-width: 426px)and (max-width: 896px){#onetrust-banner-sdk #onetrust-close-btn-container{position:absolute;top:0;right:0}#onetrust-banner-sdk #onetrust-policy{margin-left:1em;margin-right:1em}#onetrust-banner-sdk .onetrust-close-btn-ui{top:10px;right:10px}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:95%}#onetrust-banner-sdk.ot-iab-2 #onetrust-group-container{width:100%}#onetrust-banner-sdk.ot-bnr-w-logo #onetrust-button-group-parent{padding-left:50px}#onetrust-banner-sdk #onetrust-button-group-parent{width:100%;position:relative;margin-left:0}#onetrust-banner-sdk #onetrust-button-group button{display:inline-block}#onetrust-banner-sdk #onetrust-button-group{margin-right:0;text-align:center}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler{float:left}#onetrust-banner-sdk .has-reject-all-button #onetrust-reject-all-handler,#onetrust-banner-sdk .has-reject-all-button #onetrust-accept-btn-handler{float:right}#onetrust-banner-sdk .has-reject-all-button #onetrust-button-group{width:calc(100% - 2em);margin-right:0}#onetrust-banner-sdk .has-reject-all-button #onetrust-pc-btn-handler.cookie-setting-link{padding-left:0px;text-align:left}#onetrust-banner-sdk.ot-buttons-fw .ot-sdk-three button{width:100%;text-align:center}#onetrust-banner-sdk.ot-buttons-fw #onetrust-button-group-parent button{float:none}#onetrust-banner-sdk.ot-buttons-fw #onetrust-pc-btn-handler.cookie-setting-link{text-align:center}}@media only screen and (min-width: 550px){#onetrust-banner-sdk .banner-option:not(:first-child){border-left:1px solid #d8d8d8;padding-left:25px}}@media only screen and (min-width: 425px)and (max-width: 550px){#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group,#onetrust-banner-sdk.ot-iab-2 #onetrust-policy,#onetrust-banner-sdk.ot-iab-2 .banner-option{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-pc-btn-handler{width:100%}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-accept-btn-handler,#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group #onetrust-reject-all-handler{float:left}}@media only screen and (min-width: 769px){#onetrust-banner-sdk #onetrust-button-group{margin-right:30%}#onetrust-banner-sdk #banner-options{margin-left:2em;margin-right:5em;margin-bottom:1.25em;width:calc(100% - 7em)}}@media only screen and (min-width: 897px)and (max-width: 1023px){#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:75%;transform:translateY(-50%)}#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;padding:0;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{position:relative;margin:0;right:-22px;top:2px}}@media only screen and (min-width: 1024px){#onetrust-banner-sdk #onetrust-close-btn-container{top:50%;margin:auto;transform:translate(-50%, -50%);position:absolute;right:0}#onetrust-banner-sdk #onetrust-close-btn-container button{right:-12px}#onetrust-banner-sdk #onetrust-policy{margin-left:2em}#onetrust-banner-sdk.vertical-align-content #onetrust-button-group-parent{position:absolute;top:50%;left:60%;transform:translateY(-50%)}#onetrust-banner-sdk .ot-optout-signal{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-title{width:50%}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text,#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:1em;width:50%;border-right:1px solid #d8d8d8;padding-right:1rem}#onetrust-banner-sdk.ot-iab-2 #onetrust-policy-text{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 :not(.ot-dpd-desc)>.ot-b-addl-desc{margin-bottom:0;padding-bottom:1em}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-container{width:45%;padding-left:1rem;display:inline-block;float:none}#onetrust-banner-sdk.ot-iab-2 .ot-dpd-title{line-height:1.7}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group-parent{left:auto;right:4%;margin-left:0}#onetrust-banner-sdk.ot-iab-2 #onetrust-button-group button{display:block}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:auto;width:30%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:60%}#onetrust-banner-sdk #onetrust-button-group{margin-right:auto}#onetrust-banner-sdk #onetrust-accept-btn-handler,#onetrust-banner-sdk #onetrust-reject-all-handler,#onetrust-banner-sdk #onetrust-pc-btn-handler{margin-top:1em}}@media only screen and (min-width: 890px){#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group-parent{padding-left:3%;padding-right:4%;margin-left:0}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{margin-right:0;margin-top:1.25em;width:100%}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button{width:100%;margin-bottom:5px;margin-top:5px}#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type{margin-bottom:20px}}@media only screen and (min-width: 1280px){#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{width:55%}#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{width:44%;padding-left:2%;padding-right:2%}#onetrust-banner-sdk:not(.ot-iab-2).vertical-align-content #onetrust-button-group-parent{position:absolute;left:55%}}
        #onetrust-consent-sdk #onetrust-banner-sdk {background-color: #FFF;}
            #onetrust-consent-sdk #onetrust-policy-title,
                    #onetrust-consent-sdk #onetrust-policy-text,
                    #onetrust-consent-sdk .ot-b-addl-desc,
                    #onetrust-consent-sdk .ot-dpd-desc,
                    #onetrust-consent-sdk .ot-dpd-title,
                    #onetrust-consent-sdk #onetrust-policy-text *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk .ot-dpd-desc *:not(.onetrust-vendors-list-handler),
                    #onetrust-consent-sdk #onetrust-banner-sdk #banner-options *,
                    #onetrust-banner-sdk .ot-cat-header,
                    #onetrust-banner-sdk .ot-optout-signal
                    {
                        color: #2E2E2E;
                    }
            #onetrust-consent-sdk #onetrust-banner-sdk .banner-option-details {
                    background-color: #E9E9E9;}
             #onetrust-consent-sdk #onetrust-banner-sdk a[href],
                    #onetrust-consent-sdk #onetrust-banner-sdk a[href] font,
                    #onetrust-consent-sdk #onetrust-banner-sdk .ot-link-btn
                        {
                            color: #007398;
                        }#onetrust-consent-sdk #onetrust-accept-btn-handler,
                         #onetrust-banner-sdk #onetrust-reject-all-handler {
                            background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-banner-sdk *:focus,
            #onetrust-consent-sdk #onetrust-banner-sdk:focus {
               outline-color: #000000;
               outline-width: 1px;
            }
            #onetrust-consent-sdk #onetrust-pc-btn-handler,
            #onetrust-consent-sdk #onetrust-pc-btn-handler.cookie-setting-link {
                color: #6CC04A; border-color: #6CC04A;
                background-color:
                #FFF;
            }/*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/* 2023-12-04  Fix for button order in mobile view*/
@media (max-width: 550px) {
  #onetrust-accept-btn-handler {order: 1;  }
  #onetrust-reject-all-handler { order: 2;  }
  #onetrust-pc-btn-handler { order: 3;  }
}
#onetrust-pc-sdk.otPcCenter{overflow:hidden;position:fixed;margin:0 auto;top:5%;right:0;left:0;width:40%;max-width:575px;min-width:575px;border-radius:2.5px;z-index:2147483647;background-color:#fff;-webkit-box-shadow:0px 2px 10px -3px #999;-moz-box-shadow:0px 2px 10px -3px #999;box-shadow:0px 2px 10px -3px #999}#onetrust-pc-sdk.otPcCenter[dir=rtl]{right:0;left:0}#onetrust-pc-sdk.otRelFont{font-size:1rem}#onetrust-pc-sdk .ot-optout-signal{margin-top:.625rem}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus,#onetrust-pc-sdk .ot-hide-tgl{visibility:hidden}#onetrust-pc-sdk #ot-addtl-venlst .ot-arw-cntr *,#onetrust-pc-sdk #ot-addtl-venlst .ot-plus-minus *,#onetrust-pc-sdk .ot-hide-tgl *{visibility:hidden}#onetrust-pc-sdk #ot-gn-venlst .ot-ven-item .ot-acc-hdr{min-height:40px}#onetrust-pc-sdk .ot-pc-header{height:39px;padding:10px 0 10px 30px;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk #ot-pc-title,#onetrust-pc-sdk #ot-category-title,#onetrust-pc-sdk .ot-cat-header,#onetrust-pc-sdk #ot-lst-title,#onetrust-pc-sdk .ot-ven-hdr .ot-ven-name,#onetrust-pc-sdk .ot-always-active{font-weight:bold;color:dimgray}#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:55%;font-weight:700}#onetrust-pc-sdk .ot-cat-item p{clear:both;float:left;margin-top:10px;margin-bottom:5px;line-height:1.5;font-size:.812em;color:dimgray}#onetrust-pc-sdk .ot-close-icon{height:44px;width:44px;background-size:10px}#onetrust-pc-sdk #ot-pc-title{float:left;font-size:1em;line-height:1.5;margin-bottom:10px;margin-top:10px;width:100%}#onetrust-pc-sdk #accept-recommended-btn-handler{margin-right:10px;margin-bottom:25px;outline-offset:-1px}#onetrust-pc-sdk #ot-pc-desc{clear:both;width:100%;font-size:.812em;line-height:1.5;margin-bottom:25px}#onetrust-pc-sdk #ot-pc-desc a{margin-left:5px}#onetrust-pc-sdk #ot-pc-desc *{font-size:inherit;line-height:inherit}#onetrust-pc-sdk #ot-pc-desc ul li{padding:10px 0px}#onetrust-pc-sdk a{color:#656565;cursor:pointer}#onetrust-pc-sdk a:hover{color:#3860be}#onetrust-pc-sdk label{margin-bottom:0}#onetrust-pc-sdk #vdr-lst-dsc{font-size:.812em;line-height:1.5;padding:10px 15px 5px 15px}#onetrust-pc-sdk button{max-width:394px;padding:12px 30px;line-height:1;word-break:break-word;word-wrap:break-word;white-space:normal;font-weight:bold;height:auto}#onetrust-pc-sdk .ot-link-btn{padding:0;margin-bottom:0;border:0;font-weight:normal;line-height:normal;width:auto;height:auto}#onetrust-pc-sdk #ot-pc-content{position:absolute;overflow-y:scroll;padding-left:0px;padding-right:30px;top:60px;bottom:110px;margin:1px 3px 0 30px;width:calc(100% - 63px)}#onetrust-pc-sdk .ot-vs-list .ot-always-active,#onetrust-pc-sdk .ot-cat-grp .ot-always-active{float:right;clear:none;color:#3860be;margin:0;font-size:.813em;line-height:1.3}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-track{margin-right:20px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar{width:11px}#onetrust-pc-sdk .ot-pc-scrollbar::-webkit-scrollbar-thumb{border-radius:10px;background:#d8d8d8}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-pc-scrollbar{scrollbar-arrow-color:#d8d8d8;scrollbar-darkshadow-color:#d8d8d8;scrollbar-face-color:#d8d8d8;scrollbar-shadow-color:#d8d8d8}#onetrust-pc-sdk .save-preference-btn-handler{margin-right:20px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-right:10px}#onetrust-pc-sdk #ot-pc-desc .privacy-notice-link{margin-left:0;margin-right:8px}#onetrust-pc-sdk #ot-pc-desc .ot-imprint-handler{margin-left:0;margin-right:8px}#onetrust-pc-sdk .ot-subgrp-cntr{display:inline-block;clear:both;width:100%;padding-top:15px}#onetrust-pc-sdk .ot-switch+.ot-subgrp-cntr{padding-top:10px}#onetrust-pc-sdk ul.ot-subgrps{margin:0;font-size:initial}#onetrust-pc-sdk ul.ot-subgrps li p,#onetrust-pc-sdk ul.ot-subgrps li h5{font-size:.813em;line-height:1.4;color:dimgray}#onetrust-pc-sdk ul.ot-subgrps .ot-switch{min-height:auto}#onetrust-pc-sdk ul.ot-subgrps .ot-switch-nob{top:0}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr{display:inline-block;width:100%}#onetrust-pc-sdk ul.ot-subgrps .ot-acc-txt{margin:0}#onetrust-pc-sdk ul.ot-subgrps li{padding:0;border:none}#onetrust-pc-sdk ul.ot-subgrps li h5{position:relative;top:5px;font-weight:bold;margin-bottom:0;float:left}#onetrust-pc-sdk li.ot-subgrp{margin-left:20px;overflow:auto}#onetrust-pc-sdk li.ot-subgrp>h5{width:calc(100% - 100px)}#onetrust-pc-sdk .ot-cat-item p>ul,#onetrust-pc-sdk li.ot-subgrp p>ul{margin:0px;list-style:disc;margin-left:15px;font-size:inherit}#onetrust-pc-sdk .ot-cat-item p>ul li,#onetrust-pc-sdk li.ot-subgrp p>ul li{font-size:inherit;padding-top:10px;padding-left:0px;padding-right:0px;border:none}#onetrust-pc-sdk .ot-cat-item p>ul li:last-child,#onetrust-pc-sdk li.ot-subgrp p>ul li:last-child{padding-bottom:10px}#onetrust-pc-sdk .ot-pc-logo{height:40px;width:120px}#onetrust-pc-sdk .ot-pc-footer{position:absolute;bottom:0px;width:100%;max-height:160px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-refuse-all-handler{margin-bottom:0px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:160px}#onetrust-pc-sdk.ot-ftr-stacked .ot-pc-footer button{width:100%;max-width:none}#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:0 30px;width:calc(100% - 60px);padding-right:0}#onetrust-pc-sdk .ot-pc-footer-logo{height:30px;width:100%;text-align:right;background:#f4f4f4}#onetrust-pc-sdk .ot-pc-footer-logo a{display:inline-block;margin-top:5px;margin-right:10px}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo{direction:rtl}#onetrust-pc-sdk[dir=rtl] .ot-pc-footer-logo a{margin-right:25px}#onetrust-pc-sdk .ot-tgl{float:right;position:relative;z-index:1}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background-color:#468254;border:1px solid #fff}#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{-webkit-transform:translateX(20px);-ms-transform:translateX(20px);transform:translateX(20px);background-color:#fff;border-color:#fff}#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch{outline:#000 solid 1px}#onetrust-pc-sdk .ot-switch{position:relative;display:inline-block;width:45px;height:25px}#onetrust-pc-sdk .ot-switch-nob{position:absolute;cursor:pointer;top:0;left:0;right:0;bottom:0;background-color:#767676;border:1px solid #ddd;transition:all .2s ease-in 0s;-moz-transition:all .2s ease-in 0s;-o-transition:all .2s ease-in 0s;-webkit-transition:all .2s ease-in 0s;border-radius:20px}#onetrust-pc-sdk .ot-switch-nob:before{position:absolute;content:"";height:18px;width:18px;bottom:3px;left:3px;background-color:#fff;-webkit-transition:.4s;transition:.4s;border-radius:20px}#onetrust-pc-sdk .ot-chkbox input:checked~label::before{background-color:#3860be}#onetrust-pc-sdk .ot-chkbox input+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk .ot-chkbox label{position:relative;display:inline-block;padding-left:30px;cursor:pointer;font-weight:500}#onetrust-pc-sdk .ot-chkbox label::before,#onetrust-pc-sdk .ot-chkbox label::after{position:absolute;content:"";display:inline-block;border-radius:3px}#onetrust-pc-sdk .ot-chkbox label::before{height:18px;width:18px;border:1px solid #3860be;left:0px;top:auto}#onetrust-pc-sdk .ot-chkbox label::after{height:5px;width:9px;border-left:3px solid;border-bottom:3px solid;transform:rotate(-45deg);-o-transform:rotate(-45deg);-ms-transform:rotate(-45deg);-webkit-transform:rotate(-45deg);left:4px;top:5px}#onetrust-pc-sdk .ot-label-txt{display:none}#onetrust-pc-sdk .ot-chkbox input,#onetrust-pc-sdk .ot-tgl input{position:absolute;opacity:0;width:0;height:0}#onetrust-pc-sdk .ot-arw-cntr{float:right;position:relative;pointer-events:none}#onetrust-pc-sdk .ot-arw-cntr .ot-arw{width:16px;height:16px;margin-left:5px;color:dimgray;display:inline-block;vertical-align:middle;-webkit-transition:all 150ms ease-in 0s;-moz-transition:all 150ms ease-in 0s;-o-transition:all 150ms ease-in 0s;transition:all 150ms ease-in 0s}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw-cntr svg{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:#000 1px solid}#onetrust-pc-sdk .ot-tgl-cntr,#onetrust-pc-sdk .ot-arw-cntr{display:inline-block}#onetrust-pc-sdk .ot-tgl-cntr{width:45px;float:right;margin-top:2px}#onetrust-pc-sdk #ot-lst-cnt .ot-tgl-cntr{margin-top:10px}#onetrust-pc-sdk .ot-always-active-subgroup{width:auto;padding-left:0px !important;top:3px;position:relative}#onetrust-pc-sdk .ot-label-status{padding-left:5px;font-size:.75em;display:none}#onetrust-pc-sdk .ot-arw-cntr{margin-top:-1px}#onetrust-pc-sdk .ot-arw-cntr svg{-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s;transition:all 300ms ease-in 0s;height:10px;width:10px}#onetrust-pc-sdk input:checked~.ot-acc-hdr .ot-arw{transform:rotate(90deg);-o-transform:rotate(90deg);-ms-transform:rotate(90deg);-webkit-transform:rotate(90deg)}#onetrust-pc-sdk .ot-arw{width:10px;margin-left:15px;transition:all 300ms ease-in 0s;-webkit-transition:all 300ms ease-in 0s;-moz-transition:all 300ms ease-in 0s;-o-transition:all 300ms ease-in 0s}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0}#onetrust-pc-sdk .ot-hlst-cntr{margin-top:5px;display:inline-block;width:100%}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{clear:both;color:#3860be;margin-left:0;font-size:.813em;text-decoration:none;float:left;overflow:hidden}#onetrust-pc-sdk .category-vendors-list-handler:hover,#onetrust-pc-sdk .category-vendors-list-handler+a:hover,#onetrust-pc-sdk .category-host-list-handler:hover{text-decoration-line:underline}#onetrust-pc-sdk .category-vendors-list-handler+a{clear:none}#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{display:inline-block;height:13px;width:13px;background-repeat:no-repeat;margin-left:1px;margin-top:6px;cursor:pointer}#onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{margin-bottom:-1px}#onetrust-pc-sdk .back-btn-handler{font-size:1em;text-decoration:none}#onetrust-pc-sdk .back-btn-handler:hover{opacity:.6}#onetrust-pc-sdk #ot-lst-title h3{display:inline-block;word-break:break-word;word-wrap:break-word;margin-bottom:0;color:#656565;font-size:1em;font-weight:bold;margin-left:15px}#onetrust-pc-sdk #ot-lst-title{margin:10px 0 10px 0px;font-size:1em;text-align:left}#onetrust-pc-sdk #ot-pc-hdr{margin:0 0 0 30px;height:auto;width:auto}#onetrust-pc-sdk #ot-pc-hdr input::placeholder{color:#d4d4d4;font-style:italic}#onetrust-pc-sdk #vendor-search-handler{height:31px;width:100%;border-radius:50px;font-size:.8em;padding-right:35px;padding-left:15px;float:left;margin-left:15px}#onetrust-pc-sdk .ot-ven-name{display:block;width:auto;padding-right:5px}#onetrust-pc-sdk #ot-lst-cnt{overflow-y:auto;margin-left:20px;margin-right:7px;width:calc(100% - 27px);max-height:calc(100% - 80px);height:100%;transform:translate3d(0, 0, 0)}#onetrust-pc-sdk #ot-pc-lst{width:100%;bottom:100px;position:absolute;top:60px}#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr .ot-arw-cntr *{visibility:hidden}#onetrust-pc-sdk #ot-pc-lst .ot-tgl-cntr{right:12px;position:absolute}#onetrust-pc-sdk #ot-pc-lst .ot-arw-cntr{float:right;position:relative}#onetrust-pc-sdk #ot-pc-lst .ot-arw{margin-left:10px}#onetrust-pc-sdk #ot-pc-lst .ot-acc-hdr{overflow:hidden;cursor:pointer}#onetrust-pc-sdk .ot-vlst-cntr{overflow:hidden}#onetrust-pc-sdk #ot-sel-blk{overflow:hidden;width:100%;position:sticky;position:-webkit-sticky;top:0;z-index:3}#onetrust-pc-sdk #ot-back-arw{height:12px;width:12px}#onetrust-pc-sdk .ot-lst-subhdr{width:100%;display:inline-block}#onetrust-pc-sdk .ot-search-cntr{float:left;width:78%;position:relative}#onetrust-pc-sdk .ot-search-cntr>svg{width:30px;height:30px;position:absolute;float:left;right:-15px}#onetrust-pc-sdk .ot-fltr-cntr{float:right;right:50px;position:relative}#onetrust-pc-sdk #filter-btn-handler{background-color:#3860be;border-radius:17px;display:inline-block;position:relative;width:32px;height:32px;-moz-transition:.1s ease;-o-transition:.1s ease;-webkit-transition:1s ease;transition:.1s ease;padding:0;margin:0}#onetrust-pc-sdk #filter-btn-handler:hover{background-color:#3860be}#onetrust-pc-sdk #filter-btn-handler svg{width:12px;height:12px;margin:3px 10px 0 10px;display:block;position:static;right:auto;top:auto}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{color:#3860be;text-decoration:none;font-weight:100;display:inline-block;padding-top:10px;transform:translate(0, 1%);-o-transform:translate(0, 1%);-ms-transform:translate(0, 1%);-webkit-transform:translate(0, 1%);position:relative;z-index:2}#onetrust-pc-sdk .ot-ven-link *,#onetrust-pc-sdk .ot-ven-legclaim-link *{font-size:inherit}#onetrust-pc-sdk .ot-ven-link:hover,#onetrust-pc-sdk .ot-ven-legclaim-link:hover{text-decoration:underline}#onetrust-pc-sdk .ot-ven-hdr{width:calc(100% - 160px);height:auto;float:left;word-break:break-word;word-wrap:break-word;vertical-align:middle;padding-bottom:3px}#onetrust-pc-sdk .ot-ven-link,#onetrust-pc-sdk .ot-ven-legclaim-link{letter-spacing:.03em;font-size:.75em;font-weight:400}#onetrust-pc-sdk .ot-ven-dets{border-radius:2px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-ven-dets li:first-child p:first-child{border-top:none}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:not(:first-child){border-top:1px solid #ddd !important}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(odd){width:30%}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc:nth-child(n+3) p:nth-of-type(even){width:50%;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p,#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{padding-top:5px;padding-bottom:5px;display:block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc h4{display:inline-block}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-last-child(-n+1){padding-bottom:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc p:nth-child(-n+2):not(.disc-pur){padding-top:10px}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur-cont{display:inline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur{position:relative;width:50% !important;word-break:break-word;word-wrap:break-word;left:calc(30% + 17px)}#onetrust-pc-sdk .ot-ven-dets .ot-ven-disc .disc-pur:nth-child(-n+1){position:static}#onetrust-pc-sdk .ot-ven-dets p,#onetrust-pc-sdk .ot-ven-dets h4,#onetrust-pc-sdk .ot-ven-dets span{font-size:.69em;text-align:left;vertical-align:middle;word-break:break-word;word-wrap:break-word;margin:0;padding-bottom:10px;padding-left:15px;color:#2e3644}#onetrust-pc-sdk .ot-ven-dets h4{padding-top:5px}#onetrust-pc-sdk .ot-ven-dets span{color:dimgray;padding:0;vertical-align:baseline}#onetrust-pc-sdk .ot-ven-dets .ot-ven-pur h4{border-top:1px solid #e9e9e9;border-bottom:1px solid #e9e9e9;padding-bottom:5px;margin-bottom:5px;font-weight:bold}#onetrust-pc-sdk #ot-host-lst .ot-sel-all{float:right;position:relative;margin-right:42px;top:10px}#onetrust-pc-sdk #ot-host-lst .ot-sel-all input[type=checkbox]{width:auto;height:auto}#onetrust-pc-sdk #ot-host-lst .ot-sel-all label{height:20px;width:20px;padding-left:0px}#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{overflow:hidden;width:95%}#onetrust-pc-sdk .ot-host-hdr{position:relative;z-index:1;pointer-events:none;width:calc(100% - 125px);float:left}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-desc{display:inline-block;width:90%}#onetrust-pc-sdk .ot-host-name{pointer-events:none}#onetrust-pc-sdk .ot-host-hdr>a{text-decoration:underline;font-size:.82em;position:relative;z-index:2;float:left;margin-bottom:5px;pointer-events:initial}#onetrust-pc-sdk .ot-host-name+a{margin-top:5px}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a,#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{color:dimgray;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-name,#onetrust-pc-sdk .ot-host-name a{font-weight:bold;font-size:.82em;line-height:1.3}#onetrust-pc-sdk .ot-host-name a{font-size:1em}#onetrust-pc-sdk .ot-host-expand{margin-top:3px;margin-bottom:3px;clear:both;display:block;color:#3860be;font-size:.72em;font-weight:normal}#onetrust-pc-sdk .ot-host-expand *{font-size:inherit}#onetrust-pc-sdk .ot-host-desc,#onetrust-pc-sdk .ot-host-info{font-size:.688em;line-height:1.4;font-weight:normal}#onetrust-pc-sdk .ot-host-desc{margin-top:10px}#onetrust-pc-sdk .ot-host-opt{margin:0;font-size:inherit;display:inline-block;width:100%}#onetrust-pc-sdk .ot-host-opt li>div div{font-size:.8em;padding:5px 0}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(1){width:30%;float:left}#onetrust-pc-sdk .ot-host-opt li>div div:nth-child(2){width:70%;float:left;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-host-info{border:none;display:inline-block;width:calc(100% - 10px);padding:10px;margin-bottom:10px;background-color:#f8f8f8}#onetrust-pc-sdk .ot-host-info>div{overflow:auto}#onetrust-pc-sdk #no-results{text-align:center;margin-top:30px}#onetrust-pc-sdk #no-results p{font-size:1em;color:#2e3644;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk #no-results p span{font-weight:bold}#onetrust-pc-sdk #ot-fltr-modal{width:100%;height:auto;display:none;-moz-transition:.2s ease;-o-transition:.2s ease;-webkit-transition:2s ease;transition:.2s ease;overflow:hidden;opacity:1;right:0}#onetrust-pc-sdk #ot-fltr-modal .ot-label-txt{display:inline-block;font-size:.85em;color:dimgray}#onetrust-pc-sdk #ot-fltr-cnt{z-index:2147483646;background-color:#fff;position:absolute;height:90%;max-height:300px;width:325px;left:210px;margin-top:10px;margin-bottom:20px;padding-right:10px;border-radius:3px;-webkit-box-shadow:0px 0px 12px 2px #c7c5c7;-moz-box-shadow:0px 0px 12px 2px #c7c5c7;box-shadow:0px 0px 12px 2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-scrlcnt{overflow-y:auto;overflow-x:hidden;clear:both;max-height:calc(100% - 60px)}#onetrust-pc-sdk #ot-anchor{border:12px solid rgba(0,0,0,0);display:none;position:absolute;z-index:2147483647;right:55px;top:75px;transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);-webkit-transform:rotate(45deg);background-color:#fff;-webkit-box-shadow:-3px -3px 5px -2px #c7c5c7;-moz-box-shadow:-3px -3px 5px -2px #c7c5c7;box-shadow:-3px -3px 5px -2px #c7c5c7}#onetrust-pc-sdk .ot-fltr-btns{margin-left:15px}#onetrust-pc-sdk #filter-apply-handler{margin-right:15px}#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:25px;margin-left:15px;width:75%;position:relative}#onetrust-pc-sdk .ot-fltr-opt p{display:inline-block;margin:0;font-size:.9em;color:#2e3644}#onetrust-pc-sdk .ot-chkbox label span{font-size:.85em;color:dimgray}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]+label::after{content:none;color:#fff}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:checked+label::after{content:""}#onetrust-pc-sdk .ot-chkbox input[type=checkbox]:focus+label::before{outline-style:solid;outline-width:2px;outline-style:auto}#onetrust-pc-sdk #ot-selall-vencntr,#onetrust-pc-sdk #ot-selall-adtlvencntr,#onetrust-pc-sdk #ot-selall-hostcntr,#onetrust-pc-sdk #ot-selall-licntr,#onetrust-pc-sdk #ot-selall-gnvencntr{right:15px;position:relative;width:20px;height:20px;float:right}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label,#onetrust-pc-sdk #ot-selall-gnvencntr label{float:left;padding-left:0}#onetrust-pc-sdk #ot-ven-lst:first-child{border-top:1px solid #e2e2e2}#onetrust-pc-sdk ul{list-style:none;padding:0}#onetrust-pc-sdk ul li{position:relative;margin:0;padding:15px 15px 15px 10px;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk ul li h3{font-size:.75em;color:#656565;margin:0;display:inline-block;width:70%;height:auto;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk ul li p{margin:0;font-size:.7em}#onetrust-pc-sdk ul li input[type=checkbox]{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0}#onetrust-pc-sdk .ot-cat-item>button:focus,#onetrust-pc-sdk .ot-acc-cntr>button:focus,#onetrust-pc-sdk li>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-cat-item>button,#onetrust-pc-sdk .ot-acc-cntr>button,#onetrust-pc-sdk li>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=false]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-txt,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-cat-item>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk .ot-acc-cntr>button[aria-expanded=true]~.ot-acc-grpcntr,#onetrust-pc-sdk li>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-host-item>button:focus,#onetrust-pc-sdk .ot-ven-item>button:focus{outline:0;border:2px solid #000}#onetrust-pc-sdk .ot-hide-acc>button{pointer-events:none}#onetrust-pc-sdk .ot-hide-acc .ot-plus-minus>*,#onetrust-pc-sdk .ot-hide-acc .ot-arw-cntr>*{visibility:hidden}#onetrust-pc-sdk .ot-hide-acc .ot-acc-hdr{min-height:30px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt){padding-right:10px;width:calc(100% - 37px);margin-top:10px;max-height:calc(100% - 90px)}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk{background-color:#f9f9fc;border:1px solid #e2e2e2;width:calc(100% - 2px);padding-bottom:5px;padding-top:5px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt{border:unset;background-color:unset}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all-hdr{display:none}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all{padding-right:.5rem}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) #ot-sel-blk.ot-vnd-list-cnt .ot-sel-all .ot-chkbox{right:0}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all{padding-right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-sel-all-chkbox{width:auto}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) ul li{border:1px solid #e2e2e2;margin-bottom:10px}#onetrust-pc-sdk.ot-addtl-vendors #ot-lst-cnt:not(.ot-host-cnt) .ot-acc-cntr>.ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk.ot-addtl-vendors .ot-sel-all-chkbox{float:right}#onetrust-pc-sdk.ot-addtl-vendors .ot-plus-minus~.ot-sel-all-chkbox{right:34px}#onetrust-pc-sdk.ot-addtl-vendors #ot-ven-lst:first-child{border-top:none}#onetrust-pc-sdk .ot-acc-cntr{position:relative;border-left:1px solid #e2e2e2;border-right:1px solid #e2e2e2;border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr input{z-index:1}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr{background-color:#f9f9fc;padding:5px 0 5px 15px;width:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-plus-minus{vertical-align:middle;top:auto}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr .ot-arw-cntr{right:10px}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-hdr input{z-index:2}#onetrust-pc-sdk .ot-acc-cntr.ot-add-tech .ot-acc-hdr{padding:10px 0 10px 15px}#onetrust-pc-sdk .ot-acc-cntr>input[type=checkbox]:checked~.ot-acc-hdr{border-bottom:1px solid #e2e2e2}#onetrust-pc-sdk .ot-acc-cntr>.ot-acc-txt{padding-left:10px;padding-right:10px}#onetrust-pc-sdk .ot-acc-cntr button[aria-expanded=true]~.ot-acc-txt{width:auto}#onetrust-pc-sdk .ot-acc-cntr .ot-addtl-venbox{display:none}#onetrust-pc-sdk .ot-vlst-cntr{margin-bottom:0;width:100%}#onetrust-pc-sdk .ot-vensec-title{font-size:.813em;vertical-align:middle;display:inline-block}#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a{margin-left:0;margin-top:10px}#onetrust-pc-sdk #ot-selall-vencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-adtlvencntr.line-through label::after,#onetrust-pc-sdk #ot-selall-licntr.line-through label::after,#onetrust-pc-sdk #ot-selall-hostcntr.line-through label::after,#onetrust-pc-sdk #ot-selall-gnvencntr.line-through label::after{height:auto;border-left:0;transform:none;-o-transform:none;-ms-transform:none;-webkit-transform:none;left:5px;top:9px}#onetrust-pc-sdk #ot-category-title{float:left;padding-bottom:10px;font-size:1em;width:100%}#onetrust-pc-sdk .ot-cat-grp{margin-top:10px}#onetrust-pc-sdk .ot-cat-item{line-height:1.1;margin-top:10px;display:inline-block;width:100%}#onetrust-pc-sdk .ot-btn-container{text-align:right}#onetrust-pc-sdk .ot-btn-container button{display:inline-block;font-size:.75em;letter-spacing:.08em;margin-top:19px}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon{position:absolute;top:10px;right:0;z-index:1;padding:0;background-color:rgba(0,0,0,0);border:none}#onetrust-pc-sdk #close-pc-btn-handler.ot-close-icon svg{display:block;height:10px;width:10px}#onetrust-pc-sdk #clear-filters-handler{margin-top:20px;margin-bottom:10px;float:right;max-width:200px;text-decoration:none;color:#3860be;font-size:.9em;font-weight:bold;background-color:rgba(0,0,0,0);border-color:rgba(0,0,0,0);padding:1px}#onetrust-pc-sdk #clear-filters-handler:hover{color:#2285f7}#onetrust-pc-sdk #clear-filters-handler:focus{outline:#000 solid 1px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl,#onetrust-pc-sdk .ot-enbl-chr h4~.ot-always-active{right:45px}#onetrust-pc-sdk .ot-enbl-chr h4~.ot-tgl+.ot-tgl{right:120px}#onetrust-pc-sdk .ot-enbl-chr .ot-pli-hdr.ot-leg-border-color span:first-child{width:90px}#onetrust-pc-sdk .ot-enbl-chr li.ot-subgrp>h5+.ot-tgl-cntr{padding-right:25px}#onetrust-pc-sdk .ot-plus-minus{width:20px;height:20px;font-size:1.5em;position:relative;display:inline-block;margin-right:5px;top:3px}#onetrust-pc-sdk .ot-plus-minus span{position:absolute;background:#27455c;border-radius:1px}#onetrust-pc-sdk .ot-plus-minus span:first-of-type{top:25%;bottom:25%;width:10%;left:45%}#onetrust-pc-sdk .ot-plus-minus span:last-of-type{left:25%;right:25%;height:10%;top:45%}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-arw,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:first-of-type,#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{transform:rotate(90deg)}#onetrust-pc-sdk button[aria-expanded=true]~.ot-acc-hdr .ot-plus-minus span:last-of-type{left:50%;right:50%}#onetrust-pc-sdk #ot-selall-vencntr label,#onetrust-pc-sdk #ot-selall-adtlvencntr label,#onetrust-pc-sdk #ot-selall-hostcntr label,#onetrust-pc-sdk #ot-selall-licntr label{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk .ot-host-item .ot-plus-minus,#onetrust-pc-sdk .ot-ven-item .ot-plus-minus{float:left;margin-right:8px;top:10px}#onetrust-pc-sdk .ot-ven-item ul{list-style:none inside;font-size:100%;margin:0}#onetrust-pc-sdk .ot-ven-item ul li{margin:0 !important;padding:0;border:none !important}#onetrust-pc-sdk .ot-pli-hdr{color:#77808e;overflow:hidden;padding-top:7.5px;padding-bottom:7.5px;width:calc(100% - 2px);border-top-left-radius:3px;border-top-right-radius:3px}#onetrust-pc-sdk .ot-pli-hdr span:first-child{top:50%;transform:translateY(50%);max-width:90px}#onetrust-pc-sdk .ot-pli-hdr span:last-child{padding-right:10px;max-width:95px;text-align:center}#onetrust-pc-sdk .ot-li-title{float:right;font-size:.813em}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color{background-color:#f4f4f4;border:1px solid #d8d8d8}#onetrust-pc-sdk .ot-pli-hdr.ot-leg-border-color span:first-child{text-align:left;width:70px}#onetrust-pc-sdk li.ot-subgrp>h5,#onetrust-pc-sdk .ot-cat-header{width:calc(100% - 130px)}#onetrust-pc-sdk li.ot-subgrp>h5+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-acc-grpdesc{margin-bottom:5px}#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-acc-grpcntr .ot-vlst-cntr+.ot-subgrp-cntr{border-top:none}#onetrust-pc-sdk .ot-acc-hdr .ot-arw-cntr+.ot-tgl-cntr,#onetrust-pc-sdk .ot-acc-txt h4+.ot-tgl-cntr{padding-left:13px}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-subgrp>h5,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header{width:calc(100% - 145px)}#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item h5+.ot-tgl-cntr,#onetrust-pc-sdk .ot-pli-hdr~.ot-cat-item .ot-cat-header+.ot-tgl{padding-left:28px}#onetrust-pc-sdk .ot-sel-all-hdr,#onetrust-pc-sdk .ot-sel-all-chkbox{display:inline-block;width:100%;position:relative}#onetrust-pc-sdk .ot-sel-all-chkbox{z-index:1}#onetrust-pc-sdk .ot-sel-all{margin:0;position:relative;padding-right:23px;float:right}#onetrust-pc-sdk .ot-consent-hdr,#onetrust-pc-sdk .ot-li-hdr{float:right;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-li-hdr{max-width:100px;padding-right:10px}#onetrust-pc-sdk .ot-consent-hdr{max-width:55px}#onetrust-pc-sdk #ot-selall-licntr{display:block;width:21px;height:auto;float:right;position:relative;right:80px}#onetrust-pc-sdk #ot-selall-licntr label{position:absolute}#onetrust-pc-sdk .ot-ven-ctgl{margin-left:66px}#onetrust-pc-sdk .ot-ven-litgl+.ot-arw-cntr{margin-left:81px}#onetrust-pc-sdk .ot-enbl-chr .ot-host-cnt .ot-tgl-cntr{width:auto}#onetrust-pc-sdk #ot-lst-cnt:not(.ot-host-cnt) .ot-tgl-cntr{width:auto;top:auto;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox{position:relative;display:inline-block;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-chkbox label{position:absolute;padding:0;width:20px;height:20px}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-acc-grpdesc+.ot-leg-btn-container{padding-left:20px;padding-right:20px;width:calc(100% - 40px);margin-bottom:5px}#onetrust-pc-sdk .ot-subgrp .ot-leg-btn-container{margin-bottom:5px}#onetrust-pc-sdk #ot-ven-lst .ot-leg-btn-container{margin-top:10px}#onetrust-pc-sdk .ot-leg-btn-container{display:inline-block;width:100%;margin-bottom:10px}#onetrust-pc-sdk .ot-leg-btn-container button{height:auto;padding:6.5px 8px;margin-bottom:0;letter-spacing:0;font-size:.75em;line-height:normal}#onetrust-pc-sdk .ot-leg-btn-container svg{display:none;height:14px;width:14px;padding-right:5px;vertical-align:sub}#onetrust-pc-sdk .ot-active-leg-btn{cursor:default;pointer-events:none}#onetrust-pc-sdk .ot-active-leg-btn svg{display:inline-block}#onetrust-pc-sdk .ot-remove-objection-handler{text-decoration:underline;padding:0;font-size:.75em;font-weight:600;line-height:1;padding-left:10px}#onetrust-pc-sdk .ot-obj-leg-btn-handler span{font-weight:bold;text-align:center;font-size:inherit;line-height:1.5}#onetrust-pc-sdk.ot-close-btn-link #close-pc-btn-handler{border:none;height:auto;line-height:1.5;text-decoration:underline;font-size:.69em;background:none;right:15px;top:15px;width:auto;font-weight:normal}#onetrust-pc-sdk .ot-pgph-link{font-size:.813em !important;margin-top:5px;position:relative}#onetrust-pc-sdk .ot-pgph-link.ot-pgph-link-subgroup{margin-bottom:1rem}#onetrust-pc-sdk .ot-pgph-contr{margin:0 2.5rem}#onetrust-pc-sdk .ot-pgph-title{font-size:1.18rem;margin-bottom:2rem}#onetrust-pc-sdk .ot-pgph-desc{font-size:1rem;font-weight:400;margin-bottom:2rem;line-height:1.5rem}#onetrust-pc-sdk .ot-pgph-desc:not(:last-child):after{content:"";width:96%;display:block;margin:0 auto;padding-bottom:2rem;border-bottom:1px solid #e9e9e9}#onetrust-pc-sdk .ot-cat-header{float:left;font-weight:600;font-size:.875em;line-height:1.5;max-width:90%;vertical-align:middle}#onetrust-pc-sdk .ot-vnd-item>button:focus{outline:#000 solid 2px}#onetrust-pc-sdk .ot-vnd-item>button{position:absolute;cursor:pointer;width:100%;height:100%;margin:0;top:0;left:0;z-index:1;max-width:none;border:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=false]~.ot-acc-txt{margin-top:0;max-height:0;opacity:0;overflow:hidden;width:100%;transition:.25s ease-out;display:none}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-txt{transition:.1s ease-in;margin-top:10px;width:100%;overflow:auto;display:block}#onetrust-pc-sdk .ot-vnd-item>button[aria-expanded=true]~.ot-acc-grpcntr{width:auto;margin-top:0px;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{position:relative;border-radius:2px;margin:0;padding:0;border:1px solid #d8d8d8;border-top:none;width:calc(100% - 2px);float:left}#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{margin-top:10px;border-top:1px solid #d8d8d8}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc{padding-left:20px;padding-right:20px;width:calc(100% - 40px);font-size:.812em;margin-bottom:10px;margin-top:15px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul{padding-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpdesc>ul li{padding-top:0;line-height:1.5;padding-bottom:10px}#onetrust-pc-sdk .ot-accordion-layout div+.ot-acc-grpdesc{margin-top:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:first-child{margin-top:10px}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr:last-child,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr:last-child{margin-bottom:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding-top:11.5px;padding-bottom:11.5px;padding-left:20px;padding-right:20px;width:calc(100% - 40px);display:inline-block}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-txt{width:100%;padding:0}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp-cntr{padding-left:20px;padding-right:15px;padding-bottom:0;width:calc(100% - 35px)}#onetrust-pc-sdk .ot-accordion-layout .ot-subgrp{padding-right:5px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-grpcntr{z-index:1;position:relative}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr{position:absolute;top:50%;transform:translateY(-50%);right:20px;margin-top:-2px}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header+.ot-arw-cntr .ot-arw{width:15px;height:20px;margin-left:5px;color:dimgray}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{float:none;color:#2e3644;margin:0;display:inline-block;height:auto;word-wrap:break-word;min-height:inherit}#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr,#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding-left:20px;width:calc(100% - 20px);display:inline-block;margin-top:0;padding-bottom:2px}#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{position:relative;min-height:25px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl,#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{position:absolute;top:50%;transform:translateY(-50%);right:20px}#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl+.ot-tgl{right:95px}#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler,#onetrust-pc-sdk .ot-accordion-layout .category-vendors-list-handler+a{margin-top:5px}#onetrust-pc-sdk #ot-lst-cnt{margin-top:1rem;max-height:calc(100% - 96px)}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 2rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list,#onetrust-pc-sdk .ot-vnd-serv{width:auto;padding:1rem 1.25rem;padding-bottom:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:600;font-size:.95em;line-height:2;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item{border:none;margin:0;padding:0}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button{outline:none;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item button[aria-expanded=true],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item button[aria-expanded=true]{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:first-child{margin-top:.25rem;border-top:unset}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child{margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item:last-child button,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item:last-child button{border-bottom:none}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info-cntr{border:1px solid #d8d8d8;padding:.75rem 1.75rem;padding-bottom:0;width:auto;margin-top:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info{margin-bottom:1rem;padding-left:.75rem;padding-right:.75rem;display:flex;flex-direction:column}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOEmail]{border-top:1px solid #d8d8d8;padding-top:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink],#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info[data-vnd-info-key*=DPOLink]{border-bottom:1px solid #d8d8d8;padding-bottom:1rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-lbl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-lbl{font-weight:bold;font-size:.85em;margin-bottom:.5rem}#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-vnd-info .ot-vnd-cnt,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info .ot-vnd-cnt{margin-left:.5rem;font-weight:500;font-size:.85rem}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt{padding-left:40px}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-size:.8em}#onetrust-pc-sdk .ot-vs-list.ot-vnd-subgrp-cnt .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv.ot-vnd-subgrp-cnt .ot-cat-header{font-size:.8em}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv{margin-bottom:1rem;padding:1rem .95rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr{padding-bottom:.75rem;border-bottom:1px solid #d8d8d8}#onetrust-pc-sdk .ot-subgrp-cntr .ot-vnd-serv .ot-vnd-serv-hdr-cntr .ot-vnd-serv-hdr{font-weight:700;font-size:.8em;line-height:20px;margin-left:.82rem}#onetrust-pc-sdk .ot-subgrp-cntr .ot-cat-header{font-weight:700;font-size:.8em;line-height:20px}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-vnd-serv .ot-vnd-lst-cont .ot-accordion-layout .ot-acc-hdr div.ot-chkbox{margin-left:.82rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr{padding:.7rem 0;margin:0;display:flex;width:100%;align-items:center;justify-content:space-between}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:first-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:first-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:first-child{margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr div:last-child,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr div:last-child,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr div:last-child{margin-right:.5rem;margin-left:.5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-always-active,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-always-active{position:relative;right:unset;top:unset;transform:unset}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-plus-minus,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-plus-minus{top:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-arw-cntr,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-arw-cntr{float:none;top:unset;right:unset;transform:unset;margin-top:-2px;position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-cat-header,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-cat-header{flex:1;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-tgl,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-tgl{position:relative;transform:none;right:0;top:0;float:none}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox{position:relative;margin:0 .5rem}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label{padding:0}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox label::before,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox label::before{position:relative}#onetrust-pc-sdk .ot-vs-config .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk #ot-pc-lst .ot-vs-list .ot-vnd-item .ot-acc-hdr .ot-chkbox input,#onetrust-pc-sdk .ot-accordion-layout.ot-checkbox-consent .ot-acc-hdr .ot-chkbox input{position:absolute;cursor:pointer;width:100%;height:100%;opacity:0;margin:0;top:0;left:0;z-index:1}#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h5.ot-cat-header,#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp .ot-acc-hdr h4.ot-cat-header{margin:0}#onetrust-pc-sdk .ot-vs-config .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp h5{top:0;line-height:20px}#onetrust-pc-sdk .ot-vs-list{display:flex;flex-direction:column;padding:0;margin:.5rem 4px}#onetrust-pc-sdk .ot-vs-selc-all{display:flex;padding:0;float:unset;align-items:center;justify-content:flex-start}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf{justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf.ot-caret-conf .ot-sel-all-chkbox{margin-right:48px}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf .ot-sel-all-chkbox{margin:0;padding:0;margin-right:14px;justify-content:flex-end}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-chkbox,#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr.ot-tgl{display:inline-block;right:unset;width:auto;height:auto;float:none}#onetrust-pc-sdk .ot-vs-selc-all.ot-toggle-conf #ot-selall-vencntr label{width:45px;height:25px}#onetrust-pc-sdk .ot-vs-selc-all .ot-sel-all-chkbox{margin-right:11px;margin-left:.75rem;display:flex;align-items:center}#onetrust-pc-sdk .ot-vs-selc-all .sel-all-hdr{margin:0 1.25rem;font-size:.812em;line-height:normal;text-align:center;word-break:break-word;word-wrap:break-word}#onetrust-pc-sdk .ot-vnd-list-cnt #ot-selall-vencntr.ot-chkbox{float:unset;right:0}#onetrust-pc-sdk[dir=rtl] #ot-back-arw,#onetrust-pc-sdk[dir=rtl] input~.ot-acc-hdr .ot-arw{transform:rotate(180deg);-o-transform:rotate(180deg);-ms-transform:rotate(180deg);-webkit-transform:rotate(180deg)}#onetrust-pc-sdk[dir=rtl] input:checked~.ot-acc-hdr .ot-arw{transform:rotate(270deg);-o-transform:rotate(270deg);-ms-transform:rotate(270deg);-webkit-transform:rotate(270deg)}#onetrust-pc-sdk[dir=rtl] .ot-chkbox label::after{transform:rotate(45deg);-webkit-transform:rotate(45deg);-o-transform:rotate(45deg);-ms-transform:rotate(45deg);border-left:0;border-right:3px solid}#onetrust-pc-sdk[dir=rtl] .ot-search-cntr>svg{right:0}@media only screen and (max-width: 600px){#onetrust-pc-sdk.otPcCenter{left:0;min-width:100%;height:100%;top:0;border-radius:0}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk.ot-ftr-stacked .ot-btn-container{margin:1px 3px 0 10px;padding-right:10px;width:calc(100% - 23px)}#onetrust-pc-sdk .ot-btn-container button{max-width:none;letter-spacing:.01em}#onetrust-pc-sdk #close-pc-btn-handler{top:10px;right:17px}#onetrust-pc-sdk p{font-size:.7em}#onetrust-pc-sdk #ot-pc-hdr{margin:10px 10px 0 5px;width:calc(100% - 15px)}#onetrust-pc-sdk .vendor-search-handler{font-size:1em}#onetrust-pc-sdk #ot-back-arw{margin-left:12px}#onetrust-pc-sdk #ot-lst-cnt{margin:0;padding:0 5px 0 10px;min-width:95%}#onetrust-pc-sdk .switch+p{max-width:80%}#onetrust-pc-sdk .ot-ftr-stacked button{width:100%}#onetrust-pc-sdk #ot-fltr-cnt{max-width:320px;width:90%;border-top-right-radius:0;border-bottom-right-radius:0;margin:0;margin-left:15px;left:auto;right:40px;top:85px}#onetrust-pc-sdk .ot-fltr-opt{margin-left:25px;margin-bottom:10px}#onetrust-pc-sdk .ot-pc-refuse-all-handler{margin-bottom:0}#onetrust-pc-sdk #ot-fltr-cnt{right:40px}}@media only screen and (max-width: 476px){#onetrust-pc-sdk .ot-fltr-cntr,#onetrust-pc-sdk #ot-fltr-cnt{right:10px}#onetrust-pc-sdk #ot-anchor{right:25px}#onetrust-pc-sdk button{width:100%}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-sel-all{padding-right:9px}#onetrust-pc-sdk:not(.ot-addtl-vendors) #ot-pc-lst:not(.ot-enbl-chr) .ot-tgl-cntr{right:0}}@media only screen and (max-width: 896px)and (max-height: 425px)and (orientation: landscape){#onetrust-pc-sdk.otPcCenter{left:0;top:0;min-width:100%;height:100%;border-radius:0}#onetrust-pc-sdk .ot-pc-header{height:auto;min-height:20px}#onetrust-pc-sdk .ot-pc-header .ot-pc-logo{max-height:30px}#onetrust-pc-sdk .ot-pc-footer{max-height:60px;overflow-y:auto}#onetrust-pc-sdk #ot-pc-content,#onetrust-pc-sdk #ot-pc-lst{bottom:70px}#onetrust-pc-sdk.ot-ftr-stacked #ot-pc-content{bottom:70px}#onetrust-pc-sdk #ot-anchor{left:initial;right:50px}#onetrust-pc-sdk #ot-lst-title{margin-top:12px}#onetrust-pc-sdk #ot-lst-title *{font-size:inherit}#onetrust-pc-sdk #ot-pc-hdr input{margin-right:0;padding-right:45px}#onetrust-pc-sdk .switch+p{max-width:85%}#onetrust-pc-sdk #ot-sel-blk{position:static}#onetrust-pc-sdk #ot-pc-lst{overflow:auto}#onetrust-pc-sdk #ot-lst-cnt{max-height:none;overflow:initial}#onetrust-pc-sdk #ot-lst-cnt.no-results{height:auto}#onetrust-pc-sdk input{font-size:1em !important}#onetrust-pc-sdk p{font-size:.6em}#onetrust-pc-sdk #ot-fltr-modal{width:100%;top:0}#onetrust-pc-sdk ul li p,#onetrust-pc-sdk .category-vendors-list-handler,#onetrust-pc-sdk .category-vendors-list-handler+a,#onetrust-pc-sdk .category-host-list-handler{font-size:.6em}#onetrust-pc-sdk.ot-shw-fltr #ot-anchor{display:none !important}#onetrust-pc-sdk.ot-shw-fltr #ot-pc-lst{height:100% !important;overflow:hidden;top:0px}#onetrust-pc-sdk.ot-shw-fltr #ot-fltr-cnt{margin:0;height:100%;max-height:none;padding:10px;top:0;width:calc(100% - 20px);position:absolute;right:0;left:0;max-width:none}#onetrust-pc-sdk.ot-shw-fltr .ot-fltr-scrlcnt{max-height:calc(100% - 65px)}}
            #onetrust-consent-sdk #onetrust-pc-sdk,
                #onetrust-consent-sdk #ot-search-cntr,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-switch.ot-toggle,
                #onetrust-consent-sdk #onetrust-pc-sdk ot-grp-hdr1 .checkbox,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title:after
                ,#onetrust-consent-sdk #onetrust-pc-sdk #ot-sel-blk,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-cnt,
                        #onetrust-consent-sdk #onetrust-pc-sdk #ot-anchor {
                    background-color: #FFF;
                }
               
            #onetrust-consent-sdk #onetrust-pc-sdk h3,
                #onetrust-consent-sdk #onetrust-pc-sdk h4,
                #onetrust-consent-sdk #onetrust-pc-sdk h5,
                #onetrust-consent-sdk #onetrust-pc-sdk h6,
                #onetrust-consent-sdk #onetrust-pc-sdk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-ven-lst .ot-ven-opts p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-desc,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-li-title,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-sel-all-hdr span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-fltr-modal #modal-header,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-checkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-sel-blk p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-lst-title h3,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .back-btn-handler p,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst .ot-ven-name,
                #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-lst #ot-ven-lst .consent-category,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-label-status,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-chkbox label span,
                #onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,
                #onetrust-consent-sdk #onetrust-pc-sdk .ot-optout-signal
                {
                    color: #2E2E2E;
                }
             #onetrust-consent-sdk #onetrust-pc-sdk .privacy-notice-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-pgph-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler + a,
                    #onetrust-consent-sdk #onetrust-pc-sdk .category-host-list-handler,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-ven-legclaim-link,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-name a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-acc-hdr .ot-host-expand,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-pc-content #ot-pc-desc .ot-link-btn,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-vnd-serv .ot-vnd-item .ot-vnd-info a,
                    #onetrust-consent-sdk #onetrust-pc-sdk #ot-lst-cnt .ot-vnd-info a
                    {
                        color: #007398;
                    }
            #onetrust-consent-sdk #onetrust-pc-sdk .category-vendors-list-handler:hover { text-decoration: underline;}
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-subgrp-tgl .ot-switch.ot-toggle
             {
                background-color: #F8F8F8;
            }
             #onetrust-consent-sdk #onetrust-pc-sdk #ot-host-lst .ot-host-info,
                    #onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-txt .ot-ven-dets
                            {
                                background-color: #F8F8F8;
                            }
        #onetrust-consent-sdk #onetrust-pc-sdk
            button:not(#clear-filters-handler):not(.ot-close-icon):not(#filter-btn-handler):not(.ot-remove-objection-handler):not(.ot-obj-leg-btn-handler):not([aria-expanded]):not(.ot-link-btn),
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-active-leg-btn {
                background-color: #007398;border-color: #007398;
                color: #FFF;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-active-menu {
                border-color: #007398;
            }
            
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-remove-objection-handler{
                background-color: transparent;
                border: 1px solid transparent;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-leg-btn-container .ot-inactive-leg-btn {
                background-color: #FFFFFF;
                color: #78808E; border-color: #78808E;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-tgl input:focus + .ot-switch, .ot-switch .ot-switch-nob, .ot-switch .ot-switch-nob:before,
            #onetrust-pc-sdk .ot-checkbox input[type="checkbox"]:focus + label::before,
            #onetrust-pc-sdk .ot-chkbox input[type="checkbox"]:focus + label::before {
                outline-color: #000000;
                outline-width: 1px;
            }
            #onetrust-pc-sdk .ot-host-item > button:focus, #onetrust-pc-sdk .ot-ven-item > button:focus {
                border: 1px solid #000000;
            }
            #onetrust-consent-sdk #onetrust-pc-sdk *:focus,
            #onetrust-consent-sdk #onetrust-pc-sdk .ot-vlst-cntr > a:focus {
               outline: 1px solid #000000;
            }#onetrust-pc-sdk .ot-vlst-cntr .ot-ext-lnk,  #onetrust-pc-sdk .ot-ven-hdr .ot-ext-lnk{
                    background-image: url('https://cdn.cookielaw.org/logos/static/ot_external_link.svg');
                }
            /*! Extra code to blur out background */
.onetrust-pc-dark-filter{
background:rgba(0,0,0,.5);
z-index:2147483646;
width:100%;
height:100%;
overflow:hidden;
position:fixed;
top:0;
bottom:0;
left:0;
backdrop-filter: initial
}

/*! v6.12.0 2021-01-19 */
div#onetrust-consent-sdk #onetrust-banner-sdk{border-top:2px solid #eb6500!important;outline:1px solid transparent;box-shadow:none;padding:24px}div#onetrust-consent-sdk button{border-radius:0!important;box-shadow:none!important;box-sizing:border-box!important;font-size:20px!important;font-weight:400!important;letter-spacing:0!important;max-width:none!important;white-space:nowrap!important}div#onetrust-consent-sdk button:not(.ot-link-btn){background-color:#007398!important;border:2px solid #007398!important;color:#fff!important;height:48px!important;padding:0 1em!important;width:auto!important}div#onetrust-consent-sdk button:hover{background-color:#fff!important;border-color:#eb6500!important;color:#2e2e2e!important}div#onetrust-consent-sdk button.ot-link-btn{color:#007398!important;font-size:16px!important;text-decoration:underline}div#onetrust-consent-sdk button.ot-link-btn:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-consent-sdk a,div#onetrust-pc-sdk a{color:#007398!important;text-decoration:underline!important}div#onetrust-consent-sdk a,div#onetrust-consent-sdk button,div#onetrust-consent-sdk p:hover{opacity:1!important}div#onetrust-consent-sdk a:focus,div#onetrust-consent-sdk button:focus,div#onetrust-consent-sdk input:focus{outline:2px solid #eb6500!important;outline-offset:1px!important}div#onetrust-banner-sdk .ot-sdk-container{padding:0;width:auto}div#onetrust-banner-sdk .ot-sdk-row{align-items:flex-start;box-sizing:border-box;display:flex;flex-direction:column;justify-content:space-between;margin:auto;max-width:1152px}div#onetrust-banner-sdk .ot-sdk-row:after{display:none}div#onetrust-banner-sdk #onetrust-group-container,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-group-container,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-group-container{flex-grow:1;width:auto}div#onetrust-banner-sdk #onetrust-policy,div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy{margin:0;overflow:visible}div#onetrust-banner-sdk.ot-bnr-flift #onetrust-policy-text,div#onetrust-consent-sdk #onetrust-policy-text{font-size:16px;line-height:24px;max-width:44em;margin:0}div#onetrust-consent-sdk #onetrust-policy-text a[href]{font-weight:400;margin-left:8px}div#onetrust-banner-sdk #onetrust-button-group-parent{flex:0 0 auto;margin:32px 0 0;width:100%}div#onetrust-banner-sdk #onetrust-button-group{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:flex-end;margin:-8px}div#onetrust-banner-sdk .banner-actions-container{display:flex;flex:1 0 auto}div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group button:last-of-type,div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto;margin:8px;width:auto}div#onetrust-consent-sdk #onetrust-pc-btn-handler{background-color:#fff!important;color:inherit!important}div#onetrust-banner-sdk #onetrust-close-btn-container{display:none}@media only screen and (min-width:556px){div#onetrust-consent-sdk #onetrust-banner-sdk{padding:40px}div#onetrust-banner-sdk #onetrust-policy{margin:0 40px 0 0}div#onetrust-banner-sdk .ot-sdk-row{align-items:center;flex-direction:row}div#onetrust-banner-sdk #onetrust-button-group-parent,div#onetrust-banner-sdk.ot-bnr-flift:not(.ot-iab-2) #onetrust-button-group-parent,div#onetrust-banner-sdk:not(.ot-iab-2) #onetrust-button-group-parent{margin:0;padding:0;width:auto}div#onetrust-banner-sdk #onetrust-button-group,div#onetrust-banner-sdk.ot-buttons-fw:not(.ot-iab-2) #onetrust-button-group{align-items:stretch;flex-direction:column-reverse;margin:0}div#onetrust-consent-sdk #onetrust-accept-btn-handler,div#onetrust-consent-sdk #onetrust-pc-btn-handler{flex:1 0 auto}}@media only screen and (min-width:768px){div#onetrust-banner-sdk #onetrust-policy{margin:0 48px 0 0}div#onetrust-consent-sdk #onetrust-banner-sdk{padding:48px}}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-size:16px;line-height:24px}div#onetrust-consent-sdk #onetrust-pc-sdk p,div#onetrust-pc-sdk #ot-pc-desc,div#onetrust-pc-sdk .category-host-list-handler,div#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{font-size:16px;font-weight:400;line-height:24px}div#onetrust-consent-sdk a:hover,div#onetrust-pc-sdk a:hover{color:inherit!important;text-decoration-color:#eb6500!important}div#onetrust-pc-sdk{border-radius:0;bottom:0;height:auto;left:0;margin:auto;max-width:100%;overflow:hidden;right:0;top:0;width:512px;max-height:800px}div#onetrust-pc-sdk .ot-pc-header{display:none}div#onetrust-pc-sdk #ot-pc-content{overscroll-behavior:contain;padding:0 12px 0 24px;margin:16px 4px 0 0;top:0;right:16px;left:0;width:auto}div#onetrust-pc-sdk #ot-category-title,div#onetrust-pc-sdk #ot-pc-title{font-size:24px;font-weight:400;line-height:32px;margin:0 0 16px}div#onetrust-pc-sdk #ot-pc-desc{padding:0}div#onetrust-pc-sdk #ot-pc-desc a{display:inline}div#onetrust-pc-sdk #accept-recommended-btn-handler{display:none!important}div#onetrust-pc-sdk input[type=checkbox]:focus+.ot-acc-hdr{outline:2px solid #eb6500;outline-offset:-1px;transition:none}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item{border-width:0 0 2px}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item:first-of-type{border-width:2px 0}div#onetrust-pc-sdk .ot-accordion-layout .ot-acc-hdr{padding:8px 0;width:100%}div#onetrust-pc-sdk .ot-plus-minus{transform:translateY(2px)}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item>button{background:0 0!important;border:0!important;height:44px!important;max-width:none!important;width:calc(100% - 48px)!important}div#onetrust-consent-sdk #onetrust-pc-sdk h5{font-weight:700}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr{padding:0}div#onetrust-pc-sdk .ot-accordion-layout.ot-cat-item .ot-acc-grpdesc{padding:0;width:100%}div#onetrust-pc-sdk .ot-acc-grpcntr .ot-subgrp-cntr{border:0;padding:0}div#onetrust-pc-sdk .ot-subgrp-cntr ul.ot-subgrps li.ot-subgrp{margin:0}div#onetrust-pc-sdk .ot-always-active-group .ot-cat-header{width:calc(100% - 160px)}#onetrust-pc-sdk .ot-accordion-layout .ot-cat-header{width:calc(100% - 88px)}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active{color:inherit;font-size:12px;font-weight:400;line-height:1.5;padding-right:48px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:before{border-radius:12px;position:absolute;right:0;top:0;content:'';background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active:after{border-radius:50%;position:absolute;right:5px;top:4px;content:'';background-color:#eb6500;height:12px;width:12px}div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-always-active,div#onetrust-pc-sdk .ot-accordion-layout h4~.ot-tgl{right:2px}div#onetrust-pc-sdk .ot-switch{display:block;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob,div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob{background:#fff;border:2px solid #939393;box-sizing:border-box;height:20px;width:40px}div#onetrust-pc-sdk .ot-tgl input+.ot-switch .ot-switch-nob:before{background-color:#737373;height:8px;left:4px;top:4px;width:8px}div#onetrust-pc-sdk .ot-tgl input:checked+.ot-switch .ot-switch-nob:before{background-color:#eb6500;height:12px;left:0;top:2px;width:12px}div#onetrust-pc-sdk .ot-tgl input:focus+.ot-switch .ot-switch-nob{box-shadow:0 0;outline:2px solid #eb6500!important;outline-offset:1px;transition:none}div#onetrust-consent-sdk #onetrust-pc-sdk .ot-acc-grpcntr.ot-acc-txt{background-color:transparent;padding-left:3px}div#onetrust-pc-sdk .ot-accordion-layout .ot-hlst-cntr,div#onetrust-pc-sdk .ot-accordion-layout .ot-vlst-cntr{overflow:visible;width:100%}div#onetrust-pc-sdk .ot-pc-footer{border-top:0 solid}div#onetrust-pc-sdk .ot-btn-container{padding-top:24px;text-align:center}div#onetrust-pc-sdk .ot-pc-footer button{margin:8px 0;background-color:#fff}div#onetrust-pc-sdk .ot-pc-footer-logo{background-color:#fff}div#onetrust-pc-sdk #ot-lst-title span{font-size:24px;font-weight:400;line-height:32px}div#onetrust-pc-sdk #ot-host-lst .ot-host-desc,div#onetrust-pc-sdk #ot-host-lst .ot-host-expand,div#onetrust-pc-sdk #ot-host-lst .ot-host-name,div#onetrust-pc-sdk #ot-host-lst .ot-host-name a,div#onetrust-pc-sdk .back-btn-handler,div#onetrust-pc-sdk .ot-host-opt li>div div{font-size:16px;font-weight:400;line-height:24px}div#onetrust-pc-sdk #ot-host-lst .ot-acc-txt{width:100%}div#onetrust-pc-sdk #ot-pc-lst{top:0}div#onetrust-pc-sdk .back-btn-handler{text-decoration:none!important}div#onetrust-pc-sdk #filter-btn-handler:hover svg{filter:invert(1)}div#onetrust-pc-sdk .back-btn-handler svg{width:16px;height:16px}div#onetrust-pc-sdk .ot-host-item>button{background:0 0!important;border:0!important;height:66px!important;max-width:none!important;width:calc(100% - 5px)!important;transform:translate(2px,2px)}div#onetrust-pc-sdk .ot-host-item{border-bottom:2px solid #b9b9b9;padding:0}div#onetrust-pc-sdk .ot-host-item .ot-acc-hdr{margin:0 0 -6px;padding:8px 0}div#onetrust-pc-sdk ul li:first-child{border-top:2px solid #b9b9b9}div#onetrust-pc-sdk .ot-host-item .ot-plus-minus{margin:0 8px 0 0}div#onetrust-pc-sdk .ot-search-cntr{width:calc(100% - 48px)}div#onetrust-pc-sdk .ot-host-opt .ot-host-info{background-color:transparent}div#onetrust-pc-sdk .ot-host-opt li>div div{padding:0}div#onetrust-pc-sdk #vendor-search-handler{border-radius:0;border-color:#939393;border-style:solid;border-width:2px 0 2px 2px;font-size:20px;height:48px;margin:0}div#onetrust-pc-sdk #ot-pc-hdr{margin-left:24px}div#onetrust-pc-sdk .ot-lst-subhdr{width:calc(100% - 24px)}div#onetrust-pc-sdk .ot-lst-subhdr svg{right:0;top:8px}div#onetrust-pc-sdk .ot-fltr-cntr{box-sizing:border-box;right:0;width:48px}div#onetrust-pc-sdk #filter-btn-handler{width:48px!important;padding:8px!important}div#onetrust-consent-sdk #onetrust-pc-sdk #clear-filters-handler,div#onetrust-pc-sdk button#filter-apply-handler,div#onetrust-pc-sdk button#filter-cancel-handler{height:2em!important;padding-left:14px!important;padding-right:14px!important}div#onetrust-pc-sdk #ot-fltr-cnt{box-shadow:0 0;border:1px solid #8e8e8e;border-radius:0}div#onetrust-pc-sdk .ot-fltr-scrlcnt{max-height:calc(100% - 80px)}div#onetrust-pc-sdk #ot-fltr-modal{max-height:400px}div#onetrust-pc-sdk .ot-fltr-opt{margin-bottom:16px}div#onetrust-pc-sdk #ot-lst-cnt{margin-left:24px;width:calc(100% - 48px)}div#onetrust-pc-sdk #ot-anchor{display:none!important}
/*! Extra code to blur our background */
.onetrust-pc-dark-filter{
backdrop-filter: blur(3px)
}
.ot-sdk-cookie-policy{font-family:inherit;font-size:16px}.ot-sdk-cookie-policy.otRelFont{font-size:1rem}.ot-sdk-cookie-policy h3,.ot-sdk-cookie-policy h4,.ot-sdk-cookie-policy h6,.ot-sdk-cookie-policy p,.ot-sdk-cookie-policy li,.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy th,.ot-sdk-cookie-policy #cookie-policy-description,.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}.ot-sdk-cookie-policy h4{font-size:1.2em}.ot-sdk-cookie-policy h6{font-size:1em;margin-top:2em}.ot-sdk-cookie-policy th{min-width:75px}.ot-sdk-cookie-policy a,.ot-sdk-cookie-policy a:hover{background:#fff}.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}.ot-sdk-cookie-policy .ot-mobile-border{display:none}.ot-sdk-cookie-policy section{margin-bottom:2em}.ot-sdk-cookie-policy table{border-collapse:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy{font-family:inherit;font-size:1rem}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h3,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h4,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title{color:dimgray}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup{margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group-desc,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-table-header,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td{font-size:.9em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td span,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td a{font-size:inherit}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group{font-size:1em;margin-bottom:.6em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-title{margin-bottom:1.2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy>section{margin-bottom:1em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th{min-width:75px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a:hover{background:#fff}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead{background-color:#f6f6f4;font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-mobile-border{display:none}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy section{margin-bottom:2em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li{list-style:disc;margin-left:1.5em}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-subgroup ul li h4{display:inline-block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{border-collapse:inherit;margin:auto;border:1px solid #d7d7d7;border-radius:5px;border-spacing:initial;width:100%;overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border-bottom:1px solid #d7d7d7;border-right:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr th:last-child,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr td:last-child{border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:25%}.ot-sdk-cookie-policy[dir=rtl]{text-align:left}#ot-sdk-cookie-policy h3{font-size:1.5em}@media only screen and (max-width: 530px){.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) table,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tbody,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) th,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td,.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{display:block}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) thead tr{position:absolute;top:-9999px;left:-9999px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr{margin:0 0 1em 0}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd),.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) tr:nth-child(odd) a{background:#f6f6f4}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td{border:none;border-bottom:1px solid #eee;position:relative;padding-left:50%}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{position:absolute;height:100%;left:6px;width:40%;padding-right:10px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) .ot-mobile-border{display:inline-block;background-color:#e4e4e4;position:absolute;height:100%;top:0;left:45%;width:2px}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) td:before{content:attr(data-label);font-weight:bold}.ot-sdk-cookie-policy:not(#ot-sdk-cookie-policy-v2) li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table{overflow:hidden}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table td{border:none;border-bottom:1px solid #d7d7d7}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tbody,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{display:block}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-host,#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table .ot-cookies-type{width:auto}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy tr{margin:0 0 1em 0}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{height:100%;width:40%;padding-right:10px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td:before{content:attr(data-label);font-weight:bold}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li{word-break:break-word;word-wrap:break-word}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy thead tr{position:absolute;top:-9999px;left:-9999px;z-index:-9999}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td{border-bottom:1px solid #d7d7d7;border-right:0px}#ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table tr:last-child td:last-child{border-bottom:0px}}
                
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h5,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy h6,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy li,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy p,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy a,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy span,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy td,
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-description {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy th {
                        color: #696969;
                    }
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy .ot-sdk-cookie-policy-group {
                        color: #696969;
                    }
                    
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy #cookie-policy-title {
                            color: #696969;
                        }
                    
            
                    #ot-sdk-cookie-policy-v2.ot-sdk-cookie-policy table th {
                            background-color: #F8F8F8;
                        }
                    
            .ot-floating-button__front{background-image:url('https://cdn.cookielaw.org/logos/static/ot_persistent_cookie_icon.png')}</style><link type="text/css" rel="stylesheet" href="https://pendo-static-5661679399600128.storage.googleapis.com/guide.-323232.1721046486120.css" id="_pendo-css_"><style type="text/css" scoped="scoped" class=" pendo-style-D_T2uHq_M1r-XQq8htU6Z3GjHfE" style="white-space: pre-wrap;"></style></head>
<body data-sd-ui-layer-boundary="true" class="toolbar-stuck" style=""><div id="MathJax_Message" style="display: none;"></div>
<script type="text/javascript">
        window.__PRELOADED_STATE__ = {"abstracts":{"content":[{"$$":[{"$":{"id":"cesectitle0001"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"para0001"},"#name":"para","_":"Review on affective computing by single- and multi-modal analysis with benchmark databases."}],"$":{"id":"celistitem0001"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"para0002"},"#name":"para","_":"In the retrospect of 19 relevantly latest reviews and 350+ researches papers by 2021."}],"$":{"id":"celistitem0002"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"para0003"},"#name":"para","_":"Taxonomy of state-of-the-art methods considering ML-based or DL-based techniques."}],"$":{"id":"celistitem0003"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"para0004"},"#name":"para","_":"Comparative summary of the properties and quantitative performance of key methods."}],"$":{"id":"celistitem0004"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"para0005"},"#name":"para","_":"Discuss problems as well as some potential factors on affect recognition and analysis."}],"$":{"id":"celistitem0005"},"#name":"list-item"}],"$":{"id":"celist0001"},"#name":"list"}],"$":{"view":"all","id":"spara022"},"#name":"simple-para"}],"$":{"view":"all","id":"abss0001"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0001","class":"author-highlights"},"#name":"abstract"},{"$$":[{"$":{"id":"cesectitle0002"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"spara023"},"#name":"simple-para","_":"Affective computing conjoins the research topics of emotion recognition and sentiment analysis, and can be realized with unimodal or multimodal data, consisting primarily of physical information (e.g., text, audio, and visual) and physiological signals (e.g., EEG and ECG). Physical-based affect recognition caters to more researchers due to the availability of multiple public databases, but it is challenging to reveal one's inner emotion hidden purposefully from facial expressions, audio tones, body gestures, etc. Physiological signals can generate more precise and reliable emotional results; yet, the difficulty in acquiring these signals hinders their practical application. Besides, by fusing physical information and physiological signals, useful features of emotional states can be obtained to enhance the performance of affective computing models. While existing reviews focus on one specific aspect of affective computing, we provide a systematical survey of important components: emotion models, databases, and recent advances. Firstly, we introduce two typical emotion models followed by five kinds of commonly used databases for affective computing. Next, we survey and taxonomize state-of-the-art unimodal affect recognition and multimodal affective analysis in terms of their detailed architectures and performances. Finally, we discuss some critical aspects of affective computing and its applications and conclude this review by pointing out some of the most promising future directions, such as the establishment of benchmark database and fusion strategies. The overarching goal of this systematic review is to help academic and industrial researchers understand the recent advances as well as new developments in this fast-paced, high-impact domain."}],"$":{"view":"all","id":"abss0002"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0002","class":"author"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"label":{},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banner":{"id":"Banner"},"banners":[{"id":"Banner"},{"id":"BannerSsrn"}],"components":[{"target":"_blank","analytics":[{"ids":["accessbar:fta:single-article"],"eventName":"ctaClick"}],"label":"View&nbsp;**PDF**","ariaLabel":"View PDF. Opens in a new window.","id":"ViewPDF"},{"analytics":[{"ids":["accessbar:fta:full-issue"],"eventName":"ctaClick"}],"label":"Download full issue","id":"DownloadFullIssue"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"sd:genai-question-and-answer":{}},"article":{"analyticsMetadata":{"accountId":"50401","accountName":"IT University of Copenhagen","loginStatus":"logged in","userId":"71970787","isLoggedIn":true},"cid":"272144","content-family":"serial","copyright-line":"© 2022 Elsevier B.V. All rights reserved.","cover-date-years":["2022"],"cover-date-start":"2022-07-01","cover-date-text":"July 2022","document-subtype":"fla","document-type":"article","entitledToken":"5AE6BAE619C8BF50E40F545CC7A5B2D8EFF7CCFD52E6EC0E6A9A7756C790CE2E4E88B38134705D24","genAiToken":"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJnZW5BaUFwcHMiLCJzdWIiOiI1MDQwMSIsInBpaSI6IlMxNTY2MjUzNTIyMDAwMzY3IiwiaXNzIjoiYXJwIiwic2Vzc2lvbklkIjoiZDNlOTE2NTMxZGUwYzg0OTE2MGI3M2Y0NzkyNTBiODU5NGNhZ3hycWIiLCJleHAiOjE3Mjg5OTI1MjEsImlhdCI6MTcyODk5MDcyMSwidmVyc2lvbiI6MSwianRpIjoiNjAxYzQzYWEtZjM0Zi00ZmMxLWJhM2UtZDJiOTUwYjgyNTVjIn0.BJnzNfzRXzUG0ssdD1fT-DQ5NX1ZxAtXdIUOLz2tXKE","eid":"1-s2.0-S1566253522000367","doi":"10.1016/j.inffus.2022.03.009","first-fp":"19","hub-eid":"1-s2.0-S1566253522X0002X","issuePii":"S1566253522X0002X","item-weight":"FULL-TEXT","language":"en","last-lp":"52","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au0011","author-id":"S1566253522000367-4ebd5bda1c1bbf6eed41fd4e73518fac"},"$$":[{"#name":"given-name","_":"Wenqiang"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/funding-acquisition"},"_":"Funding acquisition"},{"#name":"cross-ref","$":{"id":"crf0021","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"crf0022","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"crf0025","refid":"cor0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"∗"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDEwJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXdxemhhbmclNDBmdWRhbi5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyd3F6aGFuZyU0MGZ1ZGFuLmVkdS5jbiUyMiU3RA=="}]}]},"normalized-first-auth-initial":"Y","normalized-first-auth-surname":"WANG","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-embargo-opening-date","_":"2024-03-30T00:00:00.000Z"}]},"pages":[{"last-page":"52","first-page":"19"}],"pii":"S1566253522000367","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2024-03-30T00:00:00.000Z"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Information Fusion","suppl":"C","timestamp":"2022-12-12T22:27:04.52721Z","title":{"content":[{"#name":"title","$":{"id":"tte0002"},"_":"A systematic review on affective computing: emotion models, databases, and recent advances"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"83","vol-iss-suppl-text":"Volumes 83–84","vol-last":"84","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":441,"freeHtmlGiven":false,"userProfile":{"departmentName":"Library","webUserId":"71970787","accountName":"IT University of Copenhagen","shibProfile":{"canRegister":false},"departmentId":"71310","hasMultipleOrganizations":false,"accountNumber":"C000050401","userName":"Gergo Gyori","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLl5c4DckcHvqhukSQOSa3Ps*","accessType":"SHIBREG","accountId":"50401","userType":"NORMAL","privilegeType":"BASIC","email":"gegy@itu.dk"},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"authenticationMethod":"SHIBBOLETH","entitled":true,"isCasaUser":false,"usageInfo":"(71970787,U|71310,D|50401,A|26,S|34,P|2,PL)(SDFE,CON|d3e916531de0c849160b73f479250b8594cagxrqb,SSO|REG_SHIBBOLETH,ACCESS_TYPE)","entitledByAccount":false},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"25 March 2022","Received":"21 July 2021","Revised":["22 December 2021"],"Accepted":"18 March 2022","Publication date":"1 July 2022","Version of Record":"30 March 2022"},"downloadFullIssue":true,"entitlementReason":"package","hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"helpUrl":"https://service.elsevier.com/ci/pta/login/redirect/home/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLl5c4DckcHvqhukSQOSa3Ps*","contactUrl":"https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLl5c4DckcHvqhukSQOSa3Ps*","userName":"Gergo Gyori","userEmail":"gegy@itu.dk","orgName":"IT University of Copenhagen","webUserId":"71970787","libraryBanner":null,"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":true,"hasMultiOrg":false,"userType":"SHIBREG","userAnonymity":"INDIVIDUAL","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","supportUserData":"xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLl5c4DckcHvqhukSQOSa3Ps*","institutionName":"your institution"},"isCorpReq":false,"isPdfFullText":false,"issn":"15662535","issn-primary-formatted":"1566-2535","issRange":"","isThirdParty":false,"pageCount":34,"pdfDownload":{"isPdfFullText":false,"urlMetadata":{"queryParams":{"md5":"8e9607abba89ef689a8b63b5cd543a4a","pid":"1-s2.0-S1566253522000367-main.pdf"},"pii":"S1566253522000367","pdfExtension":"/pdfft","path":"science/article/pii"}},"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Elsevier","id":"47"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":207,"openArchiveStatus":false,"openArchiveArticleCount":1,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S1566253522X0002X-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1566253522X0002X/cover/DOWNSAMPLED200/image/gif/ebb14ab5e46bdcc1c4c6960ed12a07b2/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"8165","pixel-height":"200","pixel-width":"150"},{"attachment-eid":"1-s2.0-S1566253522X0002X-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S1566253522X0002X/cover/DOWNSAMPLED/image/gif/48ce8e3548512a08a8506179951b16e1/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"4973","pixel-height":"150","pixel-width":"113"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S15662535.gif","title":"information-fusion","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S1566253522X0002X-cov150h.gif","logo":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S1566253522X0002X-cov150h.gif"},"volRange":"83-84","features":["aamAttachments","keywords","references","preview"],"titleString":"A systematic review on affective computing: emotion models, databases, and recent advances","ssrn":{},"renderingMode":"Article","isAbstract":false,"isContentVisible":false,"ajaxLinks":{"referenceLinks":true,"references":true,"referredToBy":true,"recommendations-entitled":true,"toc":true,"body":true,"recommendations":true,"citingArticles":true,"authorMetadata":true},"pdfEmbed":false,"displayViewFullText":false},"authors":{"content":[{"#name":"author-group","$":{"id":"aut0001"},"$$":[{"#name":"author","$":{"id":"au0001","author-id":"S1566253522000367-30e7bd23feb667ffe9e092bec55cbe53","orcid":"0000-0002-4953-2660"},"$$":[{"#name":"given-name","_":"Yan"},{"#name":"surname","_":"Wang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0001","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAxJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXlhbndhbmcxOSU0MGZ1ZGFuLmVkdS5jbiUyMiU3RCUyQyUyMl8lMjIlM0ElMjJ5YW53YW5nMTklNDBmdWRhbi5lZHUuY24lMjIlN0Q="}]},{"#name":"author","$":{"id":"au0002","author-id":"S1566253522000367-06215b52624707fb0ddc33d182f8476e","orcid":"0000-0002-0604-5563"},"$$":[{"#name":"given-name","_":"Wei"},{"#name":"surname","_":"Song"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/funding-acquisition"},"_":"Funding acquisition"},{"#name":"cross-ref","$":{"id":"crf0004","refid":"aff0003"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAyJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXdzb25nJTQwc2hvdS5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyd3NvbmclNDBzaG91LmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"id":"au0003","author-id":"S1566253522000367-61d3902d416d9dc13cfac36b99bbc7b6"},"$$":[{"#name":"given-name","_":"Wei"},{"#name":"surname","_":"Tao"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0005","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDAzJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQTE4MTEwODYwMDA4JTQwZnVkYW4uZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjE4MTEwODYwMDA4JTQwZnVkYW4uZWR1LmNuJTIyJTdE"}]},{"#name":"author","$":{"id":"au0004","author-id":"S1566253522000367-cb887d6a8ed20c1299874628bd4fe1d7","orcid":"0000-0002-2773-4421"},"$$":[{"#name":"given-name","_":"Antonio"},{"#name":"surname","_":"Liotta"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0008","refid":"aff0004"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"d"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA0JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQWFudG9uaW8ubGlvdHRhJTQwdW5pYnouaXQlMjIlN0QlMkMlMjJfJTIyJTNBJTIyYW50b25pby5saW90dGElNDB1bmliei5pdCUyMiU3RA=="}]},{"#name":"author","$":{"id":"au0005","author-id":"S1566253522000367-4d15c71eb80d1e1fa06f89ea2ceab56d"},"$$":[{"#name":"given-name","_":"Dawei"},{"#name":"surname","_":"Yang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0009","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA1JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQTE4MTEwODYwMDYxJTQwZnVkYW4uZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjE4MTEwODYwMDYxJTQwZnVkYW4uZWR1LmNuJTIyJTdE"}]},{"#name":"author","$":{"id":"au0006","author-id":"S1566253522000367-541cff4b72f31729bbaa4fc40af814c6"},"$$":[{"#name":"given-name","_":"Xinlei"},{"#name":"surname","_":"Li"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0012","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA2JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQTE4MTEwODYwMDE5JTQwZnVkYW4uZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjE4MTEwODYwMDE5JTQwZnVkYW4uZWR1LmNuJTIyJTdE"}]},{"#name":"author","$":{"id":"au0007","author-id":"S1566253522000367-3910efb951c0619e51f9cd082ee16fe7"},"$$":[{"#name":"given-name","_":"Shuyong"},{"#name":"surname","_":"Gao"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0015","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA3JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQTE4MTEwMjQwMDIyJTQwZnVkYW4uZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMjE4MTEwMjQwMDIyJTQwZnVkYW4uZWR1LmNuJTIyJTdE"}]},{"#name":"author","$":{"id":"au0008","author-id":"S1566253522000367-5b844c18650c83669efbb012ba9191ea"},"$$":[{"#name":"given-name","_":"Yixuan"},{"#name":"surname","_":"Sun"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0016","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJpbnQ5MDAxJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQTIxMjEwODYwMDE0JTQwbS5mdWRhbi5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyMjEyMTA4NjAwMTQlNDBtLmZ1ZGFuLmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"id":"au0009","author-id":"S1566253522000367-33281218090240fdda7b1c6cc8df95d2","orcid":"0000-0002-6258-6225"},"$$":[{"#name":"given-name","_":"Weifeng"},{"#name":"surname","_":"Ge"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0019","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA4JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXdmZ2UlNDBmdWRhbi5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyd2ZnZSU0MGZ1ZGFuLmVkdS5jbiUyMiU3RA=="}]},{"#name":"author","$":{"id":"au0010","author-id":"S1566253522000367-ab19122b23288bb7082afe9c668abf57"},"$$":[{"#name":"given-name","_":"Wei"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"crf0020","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDA5JTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXdlaXpoJTQwZnVkYW4uZWR1LmNuJTIyJTdEJTJDJTIyXyUyMiUzQSUyMndlaXpoJTQwZnVkYW4uZWR1LmNuJTIyJTdE"}]},{"#name":"author","$":{"id":"au0011","author-id":"S1566253522000367-4ebd5bda1c1bbf6eed41fd4e73518fac"},"$$":[{"#name":"given-name","_":"Wenqiang"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/funding-acquisition"},"_":"Funding acquisition"},{"#name":"cross-ref","$":{"id":"crf0021","refid":"aff0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"crf0022","refid":"aff0002"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"crf0025","refid":"cor0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"∗"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYWQwMDEwJTIyJTJDJTIydHlwZSUyMiUzQSUyMmVtYWlsJTIyJTJDJTIyaHJlZiUyMiUzQSUyMm1haWx0byUzQXdxemhhbmclNDBmdWRhbi5lZHUuY24lMjIlN0QlMkMlMjJfJTIyJTNBJTIyd3F6aGFuZyU0MGZ1ZGFuLmVkdS5jbiUyMiU3RA=="}]},{"#name":"affiliation","$":{"id":"aff0001","affiliation-id":"S1566253522000367-e3b328e3538df48a2484493f22863cae"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"cetextfn0001"},"_":"Academy for Engineering & Technology, Fudan University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Academy for Engineering & Technology"},{"#name":"organization","_":"Fudan University"},{"#name":"city","_":"Shanghai"}]},{"#name":"source-text","$":{"id":"staff0001"},"_":"aAcademy for Engineering & Technology, Fudan University, Shanghai 200433"}]},{"#name":"affiliation","$":{"id":"aff0002","affiliation-id":"S1566253522000367-eea62812ea02a1fbb5f33dc5a248c77a"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"cetextfn0004"},"_":"School of Computer Science, Fudan University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Shanghai Key Laboratory of Intelligent Information Processing"},{"#name":"organization","_":"Fudan University"},{"#name":"city","_":"Shanghai"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0004"},"_":"dShanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China"}]},{"#name":"affiliation","$":{"id":"aff0003","affiliation-id":"S1566253522000367-b7468aee743332a41a6d5441402dace3"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"cetextfn0005"},"_":"College of Information Technology, Shanghai Ocean University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"College of Information Technology"},{"#name":"organization","_":"Shanghai Ocean University"},{"#name":"city","_":"Shanghai"},{"#name":"postal-code","_":"201306"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0005"},"_":"eCollege of Information Technology, Shanghai Ocean University, Shanghai 201306, China"}]},{"#name":"affiliation","$":{"id":"aff0004","affiliation-id":"S1566253522000367-8350b7d04c7b2b48266bd76f0fa1a057"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"cetextfn0006"},"_":"Faculty of Computer Science, Free University of Bozen-Bolzano, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Faculty of Computer Science"},{"#name":"organization","_":"Free University of Bozen-Bolzano"},{"#name":"country","_":"Italy"}]},{"#name":"source-text","$":{"id":"staff0006"},"_":"fFaculty of Computer Science, Free University of Bozen-Bolzano, Italy"}]},{"#name":"correspondence","$":{"id":"cor0001"},"$$":[{"#name":"label","_":"∗"},{"#name":"text","$":{"id":"cetext0001"},"_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff0001":{"#name":"affiliation","$":{"id":"aff0001","affiliation-id":"S1566253522000367-e3b328e3538df48a2484493f22863cae"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","$":{"id":"cetextfn0001"},"_":"Academy for Engineering & Technology, Fudan University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Academy for Engineering & Technology"},{"#name":"organization","_":"Fudan University"},{"#name":"city","_":"Shanghai"}]},{"#name":"source-text","$":{"id":"staff0001"},"_":"aAcademy for Engineering & Technology, Fudan University, Shanghai 200433"}]},"aff0002":{"#name":"affiliation","$":{"id":"aff0002","affiliation-id":"S1566253522000367-eea62812ea02a1fbb5f33dc5a248c77a"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","$":{"id":"cetextfn0004"},"_":"School of Computer Science, Fudan University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Shanghai Key Laboratory of Intelligent Information Processing"},{"#name":"organization","_":"Fudan University"},{"#name":"city","_":"Shanghai"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0004"},"_":"dShanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China"}]},"aff0003":{"#name":"affiliation","$":{"id":"aff0003","affiliation-id":"S1566253522000367-b7468aee743332a41a6d5441402dace3"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","$":{"id":"cetextfn0005"},"_":"College of Information Technology, Shanghai Ocean University, Shanghai, China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"College of Information Technology"},{"#name":"organization","_":"Shanghai Ocean University"},{"#name":"city","_":"Shanghai"},{"#name":"postal-code","_":"201306"},{"#name":"country","_":"China"}]},{"#name":"source-text","$":{"id":"staff0005"},"_":"eCollege of Information Technology, Shanghai Ocean University, Shanghai 201306, China"}]},"aff0004":{"#name":"affiliation","$":{"id":"aff0004","affiliation-id":"S1566253522000367-8350b7d04c7b2b48266bd76f0fa1a057"},"$$":[{"#name":"label","_":"d"},{"#name":"textfn","$":{"id":"cetextfn0006"},"_":"Faculty of Computer Science, Free University of Bozen-Bolzano, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Faculty of Computer Science"},{"#name":"organization","_":"Free University of Bozen-Bolzano"},{"#name":"country","_":"Italy"}]},{"#name":"source-text","$":{"id":"staff0006"},"_":"fFaculty of Computer Science, Free University of Bozen-Bolzano, Italy"}]}},"correspondences":{"cor0001":{"#name":"correspondence","$":{"id":"cor0001"},"$$":[{"#name":"label","_":"∗"},{"#name":"text","$":{"id":"cetext0001"},"_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{},"body":{},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":false,"showAbstractLink":true},"citingArticles":{},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"keys0001","view":"all","class":"keyword"},"$$":[{"#name":"section-title","$":{"id":"cesectitle0003"},"_":"Keywords"},{"#name":"keyword","$":{"id":"key0001"},"$$":[{"#name":"text","$":{"id":"cetext0002"},"_":"Affective computing"}]},{"#name":"keyword","$":{"id":"key0002"},"$$":[{"#name":"text","$":{"id":"cetext0003"},"_":"Machine learning"}]},{"#name":"keyword","$":{"id":"key0003"},"$$":[{"#name":"text","$":{"id":"cetext0004"},"_":"Deep learning"}]},{"#name":"keyword","$":{"id":"key0004"},"$$":[{"#name":"text","$":{"id":"cetext0005"},"_":"Feature learning"}]},{"#name":"keyword","$":{"id":"key0005"},"$$":[{"#name":"text","$":{"id":"cetext0006"},"_":"Unimodal affect recognition"}]},{"#name":"keyword","$":{"id":"key0006"},"$$":[{"#name":"text","$":{"id":"cetext0007"},"_":"Multimodal affective analysis"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com","assetRoute":"https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing Ethics Policies</span></span></a> page. View our <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor anchor-secondary u-display-inline anchor-underline\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text-container\"><span class=\"anchor-text\">contact our Researcher support team.</span></span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"isOpen":true},"preview":{},"rawtext":"","recommendations":{},"references":{},"referenceLinks":{"internal":[],"internalLoaded":false,"external":[]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"showEntitledTocLinks":true},"tail":{},"transientError":{"isOpen":false},"sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true,"sdAnswersButton":false},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}};
      </script>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1728990721803?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AREG_SHIBBOLETH&c1=ae%3A50401&c12=ae%3A71970787 />
    </noscript>
<a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-content"><span class="anchor-text-container"><span class="anchor-text">Skip to main content</span></span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-primary" href="#screen-reader-main-title"><span class="anchor-text-container"><span class="anchor-text">Skip to article</span></span></a>
<div id="root"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action text-s anchor-secondary anchor-medium" href="/browse/journals-and-books" id="gh-journals-books-link" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text-container"><span class="anchor-text">Journals &amp; Books</span></span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-help text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden"><button class="button-link button-link-secondary gh-icon-btn button-link-medium button-link-icon-left" title="Help" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" width="20" class="icon icon-help gh-icon"><path d="M57 8C35.69 7.69 15.11 21.17 6.68 40.71c-8.81 19.38-4.91 43.67 9.63 59.25 13.81 15.59 36.85 21.93 56.71 15.68 21.49-6.26 37.84-26.81 38.88-49.21 1.59-21.15-10.47-42.41-29.29-52.1C74.76 10.17 65.88 7.99 57 8zm0 10c20.38-.37 39.57 14.94 43.85 34.85 4.59 18.53-4.25 39.23-20.76 48.79-17.05 10.59-40.96 7.62-54.9-6.83-14.45-13.94-17.42-37.85-6.83-54.9C26.28 26.5 41.39 17.83 57 18zm-.14 14C45.31 32.26 40 40.43 40 50v2h10v-2c0-4.22 2.22-9.66 8-9.24 5.5.4 6.32 5.14 5.78 8.14C62.68 55.06 52 58.4 52 69.4V76h10v-5.56c0-8.16 11.22-11.52 12-21.7.74-9.86-5.56-16.52-16-16.74-.39-.01-.76-.01-1.14 0zM52 82v10h10V82H52z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Help</span></span></button></div></div></div></li><li class="gh-nav-search text-s u-flex-center-ver u-gap-6 gh-nav-action"><div class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-secondary anchor-secondary u-margin-l-left gh-nav-action gh-icon-btn anchor-medium anchor-icon-left anchor-with-icon" href="/search" id="gh-search-link" title="Search" data-aa-button="search-in-header-opened-from-article" role="button"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search gh-icon"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg><span class="anchor-text-container"><span class="anchor-text">Search</span></span></a></div></li></ul></nav></div></div><div class="gh-profile-container u-hide-from-print"><div id="gh-profile-cnt" class="u-flex-center-ver gh-move-to-spine"><div class="popover" id="gh-profile-dropdown"><div id="popover-trigger-gh-profile-dropdown"><input type="hidden"><button class="button-link gh-icon-btn gh-user-icon u-margin-l-left gh-truncate text-s button-link-secondary button-link-medium button-link-icon-left" title="Gergo Gyori" aria-expanded="false" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-person"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><span class="button-link-text-container"><span class="button-link-text">Gergo Gyori</span></span></button></div></div></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id="institution-popover"><div id="popover-trigger-institution-popover"><input type="hidden"><button class="button-link gh-icon-btn gh-has-institution gh-truncate text-s button-link-secondary u-margin-l-left button-link-medium button-link-icon-left" id="gh-inst-icon-btn" aria-expanded="false" aria-label="Institutional Access" title="IT University of Copenhagen" type="button"><svg focusable="false" viewBox="0 0 106 128" height="20" class="icon icon-institution gh-inst-icon"><path d="M84 98h10v10H12V98h10V52h14v46h10V52h14v46h10V52h14v46zM12 36.86l41-20.84 41 20.84V42H12v-5.14zM104 52V30.74L53 4.8 2 30.74V52h10v36H2v30h102V88H94V52h10z"></path></svg><span class="button-link-text-container"><span class="button-link-text">IT University of Copenhagen</span></span></button></div></div></div><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-left" aria-label="Toggle mobile menu" aria-expanded="false" type="button"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="20" width="20"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class="" role="navigation"></div></div></div></header><div class="Article" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="ViewPDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" target="_blank" aria-label="View PDF. Opens in a new window." href="/science/article/pii/S1566253522000367/pdfft?md5=8e9607abba89ef689a8b63b5cd543a4a&amp;pid=1-s2.0-S1566253522000367-main.pdf" rel="nofollow"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="link-button-text-container"><span class="link-button-text"><span>View&nbsp;<strong>PDF</strong></span></span></span></a></li><li class="DownloadFullIssue"><button class="button-link accessbar-utility-component button-link-primary" aria-label="Download full issue" type="button"><span class="button-link-text-container"><span class="button-link-text"><span>Download full issue</span></span></span></button></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="search" id="article-quick-search" name="qs" class="search-input-field" aria-describedby="article-quick-search-description-message" aria-invalid="false" aria-label="Search ScienceDirect" placeholder="Search ScienceDirect" value=""></div><div class="search-input-message-container"><div class="search-input-validation-error" aria-live="polite"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button u-margin-xs-left button-primary small button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" class="icon icon-search"><path d="M19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6L73.23 80.19c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96S19.9 18.47 12.19 26.18C4.47 33.89.22 44.15.22 55.06s4.25 21.17 11.97 28.88C19.9 91.65 30.16 95.9 41.07 95.9c9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article"><input type="hidden" name="zone" value="qSearch"></form></div></div></div><div class="article-wrapper grid row"><div role="navigation" class="u-display-block-from-lg col-lg-6 u-padding-s-top sticky-table-of-contents" aria-label="Table of contents"><div class="TableOfContents" lang="en"><div class="Outline" id="toc-outline"><h2 class="u-h4">Outline</h2><ol class="u-padding-xs-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#abs0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Highlights"><span class="anchor-text-container"><span class="anchor-text">Highlights</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#abs0002" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Abstract"><span class="anchor-text-container"><span class="anchor-text">Abstract</span></span></a></li><li class="ai-components-toc-entry" id="ai-components-toc-entry"></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#keys0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Keywords"><span class="anchor-text-container"><span class="anchor-text">Keywords</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction"><span class="anchor-text-container"><span class="anchor-text">1. Introduction</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0002" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Related works"><span class="anchor-text-container"><span class="anchor-text">2. Related works</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0006" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Emotion models"><span class="anchor-text-container"><span class="anchor-text">3. Emotion models</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0009" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Databases for affective computing"><span class="anchor-text-container"><span class="anchor-text">4. Databases for affective computing</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0017" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Unimodal affect recognition"><span class="anchor-text-container"><span class="anchor-text">5. Unimodal affect recognition</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0038" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="6. Multimodal affective analysis"><span class="anchor-text-container"><span class="anchor-text">6. Multimodal affective analysis</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0045" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="7. Discussions"><span class="anchor-text-container"><span class="anchor-text">7. Discussions</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0051" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="8. Conclusion and new developments"><span class="anchor-text-container"><span class="anchor-text">8. Conclusion and new developments</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#sec0051a" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="CRediT authorship contribution statement"><span class="anchor-text-container"><span class="anchor-text">CRediT authorship contribution statement</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#coi0001" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Declaration of Competing Interest"><span class="anchor-text-container"><span class="anchor-text">Declaration of Competing Interest</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#ack0001a" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Acknowledgement"><span class="anchor-text-container"><span class="anchor-text">Acknowledgement</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary" href="#cebibl1" data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="References"><span class="anchor-text-container"><span class="anchor-text">References</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" aria-expanded="false" data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type="button"><span class="button-link-text-container"><span class="button-link-text">Show full outline</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="CitedBy" id="toc-cited-by"><h2 class="u-h4"><a class="anchor anchor-primary" href="#section-cited-by"><span class="anchor-text-container"><span class="anchor-text">Cited by (211)</span></span></a></h2><div class="PageDivider"></div></div><div class="Figures" id="toc-figures"><h2 class="u-h4">Figures (6)</h2><ol class="u-margin-s-bottom"><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0001" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 1. Taxonomy of affective computing with representative examples" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr1.sml" width="86px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0002" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 2. Two discrete emotion models for affective computing" class="u-display-block" height="155px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr2.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0003" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 4. Taxonomy of representative FER methods based on ML techniques or DL models" class="u-display-block" height="108px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr3.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0004" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 3. Dimensional emotion models" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr4.sml" width="121px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0005" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 5. The diagram of emotion recognition via physiological signals" class="u-display-block" height="96px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr5.sml" width="219px"></a></li><li><a class="anchor u-display-block anchor-primary anchor-icon-only anchor-with-icon" href="#fig0006" data-aa-button="sd:product:journal:article:type=anchor:name=figure"><img alt="Fig. 6. Taxonomy of multimodal affective analysis" class="u-display-block" height="163px" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr6.sml" width="111px"></a></li></ol><div class="PageDivider"></div></div><div class="Tables" id="toc-tables"><h2 class="u-h4">Tables (13)</h2><ol class="u-padding-s-bottom"><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0001" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Main acronyms."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 1</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0002" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Overview of the most relevant reviews related to affective computing published between 2017 and 2020 and our proposed review."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 2</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0003" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Overview of facial expression databases SBE=Seven Basic Emotions (anger, disgust, fear, happy, sad, surprise, and neutral)."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 3</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0004" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Overview of body gesture emotion databases."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 4</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0005" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Overview of physiological-based databases."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 5</span></span></a></li><li class="toc-list-entry-outline-padding"><a class="anchor u-truncate-anchor-text anchor-primary anchor-icon-left anchor-with-icon" href="#tbl0006" data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Overview of multimodal databases. Five basic sentiments=Strongly Positive, Weakly Positive, Neutral, Strongly Negative and Weakly Negative."><svg focusable="false" viewBox="0 0 98 128" height="20" class="icon icon-table"><path d="M54 68h32v32H54V68zm-42 0h32v32H12V68zm0-42h32v32H12V26zm42 0h32v32H54V26zM2 110h94V16H2v94z"></path></svg><span class="anchor-text-container"><span class="anchor-text">Table 6</span></span></a></li></ol><button class="button-link u-margin-xs-top u-margin-s-bottom button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:type=menu:name=show-tables" type="button"><span class="button-link-text-container"><span class="button-link-text">Show all tables</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right u-padding-s-top" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/information-fusion" title="Go to Information Fusion on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-brand-image" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/image/elsevier-non-solus.png" alt="Elsevier"></span></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor anchor-secondary publication-title-link" href="/journal/information-fusion" title="Go to Information Fusion on ScienceDirect"><span class="anchor-text-container"><span class="anchor-text">Information Fusion</span></span></a></h2><div class="text-xs"><a class="anchor anchor-primary" href="/journal/information-fusion/vol/83/suppl/C" title="Go to table of contents for this volume/issue"><span class="anchor-text-container"><span class="anchor-text">Volumes 83–84</span></span></a>, <!-- -->July 2022<!-- -->, Pages 19-52</div></div><div class="publication-cover u-display-block-from-sm"><a class="anchor anchor-primary" href="/journal/information-fusion/vol/83/suppl/C"><span class="anchor-text-container"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522X0002X-cov150h.gif" alt="Information Fusion"></span></span></a></div></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">A systematic review on affective computing: emotion models, databases, and recent advances</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0001" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Yan</span> <span class="text surname">Wang</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0002" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Wei</span> <span class="text surname">Song</span> </span><span class="author-ref" id="baff0003"><sup>c</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0003" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Wei</span> <span class="text surname">Tao</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0004" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Antonio</span> <span class="text surname">Liotta</span> </span><span class="author-ref" id="baff0004"><sup>d</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0005" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Dawei</span> <span class="text surname">Yang</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0006" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Xinlei</span> <span class="text surname">Li</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0007" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Shuyong</span> <span class="text surname">Gao</span> </span><span class="author-ref" id="baff0002"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0008" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Yixuan</span> <span class="text surname">Sun</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0009" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Weifeng</span> <span class="text surname">Ge</span> </span><span class="author-ref" id="baff0002"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0010" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Wei</span> <span class="text surname">Zhang</span> </span><span class="author-ref" id="baff0002"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button>, <button class="button-link button-link-secondary button-link-underline" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au0011" type="button"><span class="button-link-text-container"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Wenqiang</span> <span class="text surname">Zhang</span> </span><span class="author-ref" id="baff0001"><sup>a</sup></span> <span class="author-ref" id="baff0002"><sup>b</sup></span><svg focusable="false" viewBox="0 0 106 128" height="20" title="Correspondence author icon" class="icon icon-person react-xocs-author-icon u-fill-grey8"><path d="M11.07 120l.84-9.29C13.88 91.92 35.25 87.78 53 87.78c17.74 0 39.11 4.13 41.08 22.84l.84 9.38h10.04l-.93-10.34C101.88 89.23 83.89 78 53 78S4.11 89.22 1.95 109.73L1.04 120h10.03M53 17.71c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98S71.24 49.97 71.24 36.3c0-9.9-8.52-18.59-18.24-18.59zM53 70c-15.96 0-28-14.48-28-33.67C25 20.97 37.82 8 53 8s28 12.97 28 28.33C81 55.52 68.96 70 53 70"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="20" title="Author email or social media contact details icon" class="icon icon-envelope react-xocs-author-icon u-fill-grey8"><path d="M55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0L17.58 34h69.54L55.8 57.19zM0 32.42l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-.98 9.42-2.93L102 34.34V24H0zM92 88.9L73.94 66.16l-8.04 5.95L83.28 94H18.74l18.38-23.12-8.04-5.96L10 88.94V51.36L0 42.9V104h102V44.82l-10 8.46V88.9"></path></svg></span></span></button></div></div></div><button class="button-link u-margin-s-ver button-link-primary button-link-icon-right" id="show-more-btn" type="button" data-aa-button="icon-expand"><span class="button-link-text-container"><span class="button-link-text">Show more</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><div class="toc-button-wrap u-display-inline-block u-display-none-from-lg u-margin-s-right"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 128 128" height="20" class="icon icon-list"><path d="M23 26a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V30zM23 56a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V60zM23 86a9 9 0 0 0-9 9 9 9 0 0 0 9 9 9 9 0 0 0 9-9 9 9 0 0 0-9-9zm23 4v10h68V90z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Outline</span></span></button></div><button class="button-link AddToMendeley button-link-secondary u-margin-s-right u-display-inline-flex-from-md button-link-icon-left button-link-has-colored-icon" type="button"><svg focusable="false" viewBox="0 0 86 128" height="20" class="icon icon-plus"><path d="M48 58V20H38v38H0v10h38v38h10V68h38V58z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Add to Mendeley</span></span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link button-link-secondary u-margin-s-right button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 114 128" height="20" class="icon icon-share"><path d="M90 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zM24 76c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-60c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48L45.1 70.2c.58-1.98.9-4.04.9-6.2s-.32-4.22-.9-6.2l28.42-15.28C77.56 47.1 83.44 50 90 50c12.14 0 22-9.86 22-22S102.14 6 90 6s-22 9.86-22 22c0 1.98.28 3.9.78 5.72L40.14 49.1C36.12 44.76 30.38 42 24 42 11.86 42 2 51.86 2 64s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-.5 1.84-.78 3.76-.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Share</span></span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-secondary button-link-icon-left button-link-has-colored-icon" aria-expanded="false" aria-haspopup="true" type="button"><svg focusable="false" viewBox="0 0 104 128" height="20" class="icon icon-cited-by-66"><path d="M2 58.78V106h44V64H12v-5.22C12 40.28 29.08 32 46 32V22C20.1 22 2 37.12 2 58.78zM102 32V22c-25.9 0-44 15.12-44 36.78V106h44V64H68v-5.22C68 40.28 85.08 32 102 32z"></path></svg><span class="button-link-text-container"><span class="button-link-text">Cite</span></span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-primary" href="https://doi.org/10.1016/j.inffus.2022.03.009" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text-container"><span class="anchor-text">https://doi.org/10.1016/j.inffus.2022.03.009</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor rights-and-content anchor-primary" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1566253522000367&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text-container"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts u-font-serif" id="abstracts"><div class="abstract author-highlights" id="abs0001"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Highlights</h2><div id="abss0001"><div class="u-margin-s-bottom" id="spara022"><ul class="list"><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="para0001">Review on <a href="/topics/computer-science/affective-computing" title="Learn more about affective computing from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective computing</a> by single- and multi-modal analysis with benchmark databases.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="para0002">In the retrospect of 19 relevantly latest reviews and 350+ researches papers by 2021.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="para0003">Taxonomy of state-of-the-art methods considering ML-based or DL-based techniques.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="para0004">Comparative summary of the properties and quantitative performance of key methods.</div></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><div class="u-margin-s-bottom" id="para0005">Discuss problems as well as some potential factors on affect recognition and analysis.</div></span></li></ul></div></div></div><div class="abstract author" id="abs0002"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="abss0002"><div class="u-margin-s-bottom" id="spara023">Affective computing conjoins the research topics of emotion recognition and sentiment analysis, and can be realized with unimodal or multimodal data, consisting primarily of physical information (e.g., text, audio, and visual) and physiological signals (e.g., EEG and ECG). Physical-based affect recognition caters to more researchers due to the availability of multiple public databases, but it is challenging to reveal one's inner emotion hidden purposefully from facial expressions, audio tones, body gestures, etc. Physiological signals can generate more precise and reliable emotional results; yet, the difficulty in acquiring these signals hinders their practical application. Besides, by fusing physical information and physiological signals, useful features of emotional states can be obtained to enhance the performance of affective computing models. While existing reviews focus on one specific aspect of affective computing, we provide a systematical survey of important components: emotion models, databases, and recent advances. Firstly, we introduce two typical emotion models followed by five kinds of commonly used databases for affective computing. Next, we survey and taxonomize state-of-the-art unimodal affect recognition and multimodal affective analysis in terms of their detailed architectures and performances. Finally, we discuss some critical aspects of affective computing and its applications and conclude this review by pointing out some of the most promising future directions, such as the establishment of benchmark database and fusion strategies. The overarching goal of this systematic review is to help academic and industrial researchers understand the recent advances as well as new developments in this fast-paced, high-impact domain.</div></div></div></div><div id="reading-assistant-main-body-section"></div><ul id="issue-navigation" class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-left" href="/science/article/pii/S1566253522000276"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-left"><path d="M1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">Previous <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span></a></li><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary u-display-flex button-alternative-icon-right" href="/science/article/pii/S156625352200032X"><span class="button-alternative-text-container"><span class="button-alternative-text">Next <span class="extra-detail-1">article</span><span class="extra-detail-2"> in issue</span></span></span><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg></a></li></ul><div class="Keywords u-font-serif"><div id="keys0001" class="keywords-section"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id="key0001" class="keyword"><span id="cetext0002">Affective computing</span></div><div id="key0002" class="keyword"><span id="cetext0003">Machine learning</span></div><div id="key0003" class="keyword"><span id="cetext0004">Deep learning</span></div><div id="key0004" class="keyword"><span id="cetext0005">Feature learning</span></div><div id="key0005" class="keyword"><span id="cetext0006">Unimodal affect recognition</span></div><div id="key0006" class="keyword"><span id="cetext0007">Multimodal affective analysis</span></div></div></div><div class="Body u-font-serif" id="body"><div><section id="sec0001"><h2 id="cesectitle0004" class="u-h4 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><div class="u-margin-s-bottom" id="para0006"><span><a href="/topics/computer-science/affective-computing" title="Learn more about Affective computing from ScienceDirect's AI-generated Topic Pages" class="topic-link">Affective computing</a> is an umbrella term for human emotion, sentiment, and feelings </span><a class="anchor anchor-primary" href="#bib0001" name="bbib0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0001"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a><span>, emotion recognition, and <a href="/topics/computer-science/sentiment-analysis" title="Learn more about sentiment analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">sentiment analysis</a>. Since the concept of affective computing </span><a class="anchor anchor-primary" href="#bib0002" name="bbib0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0002"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a> was proposed by Prof. Picard in 1997, it has been guiding computers to identify and express emotions and respond intelligently to human emotions <a class="anchor anchor-primary" href="#bib0003" name="bbib0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0003"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a>. In many practical applications, it is desired to build a cognitive, intelligent system <a class="anchor anchor-primary" href="#bib0004" name="bbib0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0004"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a> that can distinguish and understand people's affect, and meanwhile make sensitive and friendly responses promptly [<a class="anchor anchor-primary" href="#bib0005" name="bbib0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0005"><span class="anchor-text-container"><span class="anchor-text">5</span></span></a>,<a class="anchor anchor-primary" href="#bib0006" name="bbib0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0006"><span class="anchor-text-container"><span class="anchor-text">6</span></span></a><span>]. For example, in an intelligent vehicle system, the real-time monitoring of the driver's <a href="/topics/computer-science/emotional-state" title="Learn more about emotional state from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional state</a> and the necessary response based on the monitored results can effectively reduce the possibility of accidents [</span><a class="anchor anchor-primary" href="#bib0007" name="bbib0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0007"><span class="anchor-text-container"><span class="anchor-text">7</span></span></a>,<a class="anchor anchor-primary" href="#bib0008" name="bbib0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0008"><span class="anchor-text-container"><span class="anchor-text">8</span></span></a>]. In social media, affective computing can avidly help to understand the opinions being expressed on different platforms <a class="anchor anchor-primary" href="#bib0009" name="bbib0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0009"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a>. Thus, many researchers [<a class="anchor anchor-primary" href="#bib0003" name="bbib0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0003"><span class="anchor-text-container"><span class="anchor-text">3</span></span></a>,<a class="anchor anchor-primary" href="#bib0010" name="bbib0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0010"><span class="anchor-text-container"><span class="anchor-text">10</span></span></a><span>] believe that affective computing is the key to promoting and advancing the development of human-centric <a href="/topics/computer-science/artificial-intelligence" title="Learn more about AI from ScienceDirect's AI-generated Topic Pages" class="topic-link">AI</a><span> and <a href="/topics/psychology/human-intelligence" title="Learn more about human intelligence from ScienceDirect's AI-generated Topic Pages" class="topic-link">human intelligence</a>.</span></span></div><div class="u-margin-s-bottom" id="para0007">Affective computing involves two distinct topics: emotion recognition and sentiment analysis <a class="anchor anchor-primary" href="#bib0011" name="bbib0011" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0011"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a>, <a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a>, <a class="anchor anchor-primary" href="#bib0013" name="bbib0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0013"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a>, <a class="anchor anchor-primary" href="#bib0014" name="bbib0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0014"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a>. To understand and compute the emotion or sentiment, psychologists proposed two typical theories to model human emotion: discrete emotion model (or categorical emotion model) <a class="anchor anchor-primary" href="#bib0015" name="bbib0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0015"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a> and dimensional emotion model <a class="anchor anchor-primary" href="#bib0016" name="bbib0016" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0016"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a><span>. The emotion recognition aims to detect the emotional state of <a href="/topics/biochemistry-genetics-and-molecular-biology/human" title="Learn more about human beings from ScienceDirect's AI-generated Topic Pages" class="topic-link">human beings</a> (i.e., discrete emotions or dimensional emotions) </span><a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a>, and mostly focuses on visual emotion recognition (VER) <a class="anchor anchor-primary" href="#bib0018" name="bbib0018" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0018"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a>, audio/speech emotion recognition (AER/SER) <a class="anchor anchor-primary" href="#bib0019" name="bbib0019" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0019"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a>, and physiological emotion recognition (PER) <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>. In contrast, sentiment analysis mostly concentrates on textual evaluations and opinion mining <a class="anchor anchor-primary" href="#bib0021" name="bbib0021" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0021"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a> on social events, marketing campaigns, and product preferences. The result of sentiment analysis is typically positive, negative, or neutral [<a class="anchor anchor-primary" href="#bib0010" name="bbib0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0010"><span class="anchor-text-container"><span class="anchor-text">10</span></span></a>,<a class="anchor anchor-primary" href="#bib0022" name="bbib0022" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0022"><span class="anchor-text-container"><span class="anchor-text">22</span></span></a><span>]. Considering that one person in a happy mood typically has a <a href="/topics/computer-science/positive-attitude" title="Learn more about positive attitude from ScienceDirect's AI-generated Topic Pages" class="topic-link">positive attitude</a> toward the surrounding environment, emotion recognition and sentiment analysis can be overlapped. For example, a framework based on the context-level inter-modal attention </span><a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a> was designed to predict the sentiment (positive or negative) and recognize expressed emotions (anger, disgust, fear, happiness, sadness, or surprise) of an utterance.</div><div class="u-margin-s-bottom" id="para0008"><span>Recent advances in affective computing have facilitated the release of public benchmark databases, mainly consisting of unimodal databases (i.e., textual, audio, visual, and physiological databases) and multimodal databases. These commonly used databases have, in turn, motivated the development of <a href="/topics/computer-science/machine-learning" title="Learn more about machine learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">machine learning</a> (ML)-based and </span><a href="/topics/computer-science/deep-learning" title="Learn more about deep learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep learning</a> (DL)-based affective computing.</div><div class="u-margin-s-bottom" id="para0009">A study <a class="anchor anchor-primary" href="#bib0024" name="bbib0024" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0024"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a><span> revealed that human emotions are expressed mainly through facial expressions (55%), voice (38%), and language (7%) in daily human communication. Throughout this review, textual, audio, and visual signals are collectively referred to as physical data. Since people tend to express their ideas and thoughts freely on <a href="/topics/computer-science/social-medium-platform" title="Learn more about social media platforms from ScienceDirect's AI-generated Topic Pages" class="topic-link">social media platforms</a> and websites, a large amount of physical affect data can be easily collected. Based on these data, many researchers pay attention to identifying subtle emotions expressed either explicitly or implicitly </span><a class="anchor anchor-primary" href="#bib0025" name="bbib0025" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0025"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a>, <a class="anchor anchor-primary" href="#bib0026" name="bbib0026" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0026"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a>, <a class="anchor anchor-primary" href="#bib0027" name="bbib0027" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0027"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a>. However, physical-based affect recognition may be ineffective since humans may involuntarily or deliberately conceal their real emotions (so-called social masking) <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a><span>. In contrast, <a href="/topics/computer-science/physiological-signal" title="Learn more about physiological signals from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological signals</a><span> (e.g., <a href="/topics/psychology/electroencephalography" title="Learn more about EEG from ScienceDirect's AI-generated Topic Pages" class="topic-link">EEG</a> and ECG) are not subject to these constraints as spontaneous physiological activities associated with emotions are hardly changed by oneself. Thus, EEG-based or ECG-based emotion recognition can generate more objective predictions in real time and provide reliable features of emotional states [</span></span><a class="anchor anchor-primary" href="#bib0028" name="bbib0028" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0028"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a>, <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a>,].</div><div class="u-margin-s-bottom" id="para0010"><span>Human affect is a complex psychological and <a href="/topics/biochemistry-genetics-and-molecular-biology/physiological-process" title="Learn more about physiological phenomenon from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological phenomenon</a> </span><a class="anchor anchor-primary" href="#bib0030" name="bbib0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0030"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a><span>. As human beings naturally communicate and express emotion or sentiment through <a href="/topics/computer-science/multimodal-information" title="Learn more about multimodal information from ScienceDirect's AI-generated Topic Pages" class="topic-link">multimodal information</a>, more researches focus on multi-physical modality fusion for affective analysis </span><a class="anchor anchor-primary" href="#bib0031" name="bbib0031" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0031"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a>. For example, in a conversation scenario, a person's emotional state can be demonstrated by the words of speech, the tones of voice, facial expressions and emotion-related body gestures <a class="anchor anchor-primary" href="#bib0032" name="bbib0032" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0032"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a>. Textual, auditory and visual information together provide more information than they do individually <a class="anchor anchor-primary" href="#bib0033" name="bbib0033" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0033"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a><span><span>, just like the brain validating events relies on several <a href="/topics/medicine-and-dentistry/sensory-stimulation" title="Learn more about sensory input from ScienceDirect's AI-generated Topic Pages" class="topic-link">sensory input</a> sources. With the rapid progress of physical-touching mechanisms or intrusive techniques such as low-cost </span><a href="/topics/neuroscience/wearable-sensor" title="Learn more about wearable sensors from ScienceDirect's AI-generated Topic Pages" class="topic-link">wearable sensors</a>, some emotion recognition methods are based on multimodal physiological signals (e.g., EEG, ECG, EMG and EDA). By integrating physiological modalities with physical modalities, physical-physiological affective computing can detect and recognize subtle sentiments and complex emotions [</span><a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">34</span></span></a>,<a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">35</span></span></a>]. It is worth mentioning that the suitable selection of the unimodal emotion data and multimodal fusion strategies <a class="anchor anchor-primary" href="#bib0036" name="bbib0036" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0036"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a> are the two key components of multimodal affective analysis systems, which often outperform the unimodal emotion recognition systems <a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a>. Therefore, this review not only provides a summary of the multimodal affective analysis, but also introduces an overview of unimodal affect recognition.</div><div class="u-margin-s-bottom" id="para0011"><span>Although there exist many survey papers about affective computing, most of them focus on physical-based affect recognition. For <a href="/topics/biochemistry-genetics-and-molecular-biology/facial-recognition" title="Learn more about facial expression recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial expression recognition</a> (FER), existing studies have provided a brief review of FER </span><a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a>, DL-based FER systems <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a>, facial micro-expressions analysis (FMEA) <a class="anchor anchor-primary" href="#bib0040" name="bbib0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0040"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a>, and 3D FER <a class="anchor anchor-primary" href="#bib0041" name="bbib0041" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0041"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a>. Besides, the work <a class="anchor anchor-primary" href="#bib0042" name="bbib0042" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0042"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a><span><span> discussed the <a href="/topics/computer-science/research-results" title="Learn more about research results from ScienceDirect's AI-generated Topic Pages" class="topic-link">research results</a> of the sentiment analysis using </span><a href="/topics/biochemistry-genetics-and-molecular-biology/transfer-of-learning" title="Learn more about transfer learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">transfer learning</a> algorithms, whereas authors </span><a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a> surveyed the DL-based methods for SER. However, there are just a few review papers related to physiological-based emotion recognition and physical-physiological fusion for affective analysis in recent years. For example, Jiang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a> discussed multimodal databases, feature extraction based on physical signals or EEG, and multimodal fusion strategies and recognition methods.</div><div class="u-margin-s-bottom" id="para0012">Several issues have not been thoroughly addressed in previous reviews: 1) Existing reviews take a specialist view and lack a broader perspective, for instance when classifying the various methods and advances, some reviews do not consider DL-based affect recognition or multimodal affective analysis; 2) Existing reviews do not provide a clear picture about the performance of state-of-the-art methods and the implications of their recognition ability. As the most important contribution of our review, we aim to cover different aspects of affective computing by introducing a series of research methods and results as well as discussions and future works.</div><div class="u-margin-s-bottom" id="para0013">To sum up, the major contributions of this paper are multi-fold:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1)</span><span><div class="u-margin-s-bottom" id="para0014">To the best of our knowledge, this is the first review that categorizes affective computing into two broad classes, i.e., unimodal affect recognition and multimodal affective analysis, and further taxonomizes them based on the data modalities.</div></span></li><li class="react-xocs-list-item"><span class="list-label">2)</span><span><div class="u-margin-s-bottom"><div id="para0015"><span>In the retrospect of 20 review papers released between 2017 and 2020, we present a <a href="/topics/medicine-and-dentistry/systematic-review" title="Learn more about systematic review from ScienceDirect's AI-generated Topic Pages" class="topic-link">systematic review</a> of more than 380 research papers published in the past 20 years in leading conferences and journals. This leads to a vast body of works, as taxonomized in </span><a class="anchor anchor-primary" href="#fig0001" name="bfig0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0001"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;1</span></span></a>, which will help the reader navigate through this complex area.</div><figure class="figure text-xs" id="fig0001"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr1.jpg" height="1016" alt="Fig 1" aria-describedby="cap0001"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr1_lrg.jpg" target="_blank" download="" title="Download high-res image (1MB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (1MB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr1.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0001"><p id="spara001"><span class="label">Fig. 1</span>. Taxonomy of affective computing with representative examples.</p></span></span></figure></div></span></li><li class="react-xocs-list-item"><span class="list-label">3)</span><span><div class="u-margin-s-bottom" id="para0016">We provide a comprehensive taxonomy of state-of-the-art (SOTA) affective computing methods from the perspective of either ML-based methods or DL-based techniques and consider how the different affective modalities are used to analyze and recognize affect.</div></span></li><li class="react-xocs-list-item"><span class="list-label">4)</span><span><div class="u-margin-s-bottom" id="para0017">Benchmark databases for affective computing are categorized by four modalities and video-physiological modalities. The key characteristics and availability of these databases are summarized. Based on publicly used databases, we provide a comparative summary of the properties and quantitative performance of some representative methods.</div></span></li><li class="react-xocs-list-item"><span class="list-label">5)</span><span><div class="u-margin-s-bottom" id="para0018">Finally, we discuss the effects of unimodal, multimodal, models as well as some potential factors on affective computing and some real-life applications of that, and further indicate future researches on emotion recognition and sentiment analysis.</div></span></li></ul></div><div class="u-margin-s-bottom"><div id="para0019">The paper is organized as followed. <a class="anchor anchor-primary" href="#sec0002" name="bsec0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0002"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;2</span></span></a> introduce the existing review works, pinpointing useful works and better identifying the contributions of this paper. <a class="anchor anchor-primary" href="#sec0006" name="bsec0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0006"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;3</span></span></a> surveys two kinds of emotion models, the discrete and the dimensional one. Then <a class="anchor anchor-primary" href="#sec0009" name="bsec0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0009"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;4</span></span></a> discusses in detail four kinds of databases, which are commonly used to train and test affective computing algorithms. We then provide an extensive review of the most recent advances in affective computing, including unimodal affect recognition in <a class="anchor anchor-primary" href="#sec0017" name="bsec0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0017"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;5</span></span></a> and multimodal affective analysis in <a class="anchor anchor-primary" href="#sec0038" name="bsec0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0038"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;6</span></span></a>. Finally, discussions are presented in <a class="anchor anchor-primary" href="#sec0045" name="bsec0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0045"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;7</span></span></a>, while conclusions and new developments are stated in <a class="anchor anchor-primary" href="#sec0051" name="bsec0051" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0051"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;8</span></span></a>. <a class="anchor anchor-primary" href="#tbl0001" name="btbl0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0001"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;1</span></span></a> lists the main acronyms used herein for the reader's reference.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0001"><span class="captions text-s"><span id="cap0007"><p id="spara007"><span class="label">Table 1</span>. Main acronyms.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0001">Acronym</th><th scope="col" class="align-left valign-top" id="en0002">Full Form</th><th scope="col" class="align-left valign-top" id="en0003">Acronym</th><th scope="col" class="align-left valign-top" id="en0004">Full Form</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0005">TSA</td><td class="align-left valign-top" id="en0006">Textual Sentiment Analysis</td><td class="align-left valign-top" id="en0007">SER</td><td class="align-left valign-top" id="en0008">Speech Emotion Recognition</td></tr><tr><td class="align-left valign-top" id="en0009">FER</td><td class="align-left valign-top" id="en0010">Facial Expression Recognition</td><td class="align-left valign-top" id="en0011">FMER</td><td class="align-left valign-top" id="en0012">Facial Micro-Expression Recognition</td></tr><tr><td class="align-left valign-top" id="en0013">4D/3D FER</td><td class="align-left valign-top" id="en0014">4D/3D Facial Expression Recognition</td><td class="align-left valign-top" id="en0015">EBGR</td><td class="align-left valign-top" id="en0016">Emotional Body Gesture Recognition</td></tr><tr><td class="align-left valign-top" id="en0017">EEG</td><td class="align-left valign-top" id="en0018">Electroencephalogram</td><td class="align-left valign-top" id="en0019">ECG</td><td class="align-left valign-top" id="en0020">Electrocardiography</td></tr><tr><td class="align-left valign-top" id="en0021">EMG</td><td class="align-left valign-top" id="en0022">Electromyography</td><td class="align-left valign-top" id="en0023">EDA</td><td class="align-left valign-top" id="en0024">Electro-Dermal Activity</td></tr><tr><td class="align-left valign-top" id="en0025">ML</td><td class="align-left valign-top" id="en0026">Machine Learning</td><td class="align-left valign-top" id="en0027">DL</td><td class="align-left valign-top" id="en0028">Deep Learning</td></tr><tr><td class="align-left valign-top" id="en0029">GMM</td><td class="align-left valign-top" id="en0030">Gaussian Mixture Model</td><td class="align-left valign-top" id="en0031">MLP</td><td class="align-left valign-top" id="en0032">Multi-Layer Perceptron</td></tr><tr><td class="align-left valign-top" id="en0033">NB</td><td class="align-left valign-top" id="en0034">Naive Bayesian</td><td class="align-left valign-top" id="en0035">LSTM</td><td class="align-left valign-top" id="en0036">Long- Short-Term Memory</td></tr><tr><td class="align-left valign-top" id="en0037">LDA</td><td class="align-left valign-top" id="en0038">Linear Discriminant Analysis</td><td class="align-left valign-top" id="en0039">DCNN</td><td class="align-left valign-top" id="en0040">Deep Convolutional Neural Network</td></tr><tr><td class="align-left valign-top" id="en0041">DT</td><td class="align-left valign-top" id="en0042">Decision Tree</td><td class="align-left valign-top" id="en0043">CNN</td><td class="align-left valign-top" id="en0044">Convolutional Neural Network</td></tr><tr><td class="align-left valign-top" id="en0045">KNN</td><td class="align-left valign-top" id="en0046">K-Nearest Neighbors</td><td class="align-left valign-top" id="en0047">RNN</td><td class="align-left valign-top" id="en0048">Recurrent Neural Network</td></tr><tr><td class="align-left valign-top" id="en0049">HMM</td><td class="align-left valign-top" id="en0050">Hidden Markov Model</td><td class="align-left valign-top" id="en0051">GRU</td><td class="align-left valign-top" id="en0052">Gated Recurrent Unit</td></tr><tr><td class="align-left valign-top" id="en0053">ANN</td><td class="align-left valign-top" id="en0054">Artificial Neural Network</td><td class="align-left valign-top" id="en0055">AE</td><td class="align-left valign-top" id="en0056">Auto-encoder</td></tr><tr><td class="align-left valign-top" id="en0057">PCA</td><td class="align-left valign-top" id="en0058">Principal Component Analysis</td><td class="align-left valign-top" id="en0059">GAN</td><td class="align-left valign-top" id="en0060">Generative Adversarial Network</td></tr><tr><td class="align-left valign-top" id="en0061">MLP</td><td class="align-left valign-top" id="en0062">Multi-layer Perceptron</td><td class="align-left valign-top" id="en0063">VGG</td><td class="align-left valign-top" id="en0064">Visual Geometry Group</td></tr><tr><td class="align-left valign-top" id="en0065">SVM</td><td class="align-left valign-top" id="en0066">Support Vector Machine</td><td class="align-left valign-top" id="en0067">DBN</td><td class="align-left valign-top" id="en0068">Deep Belief Network</td></tr><tr><td class="align-left valign-top" id="en0069">RBM</td><td class="align-left valign-top" id="en0070">Restricted Boltzmann Machine</td><td class="align-left valign-top" id="en0071">HAN</td><td class="align-left valign-top" id="en0072">Hierarchical Attention Network</td></tr><tr><td class="align-left valign-top" id="en0073">RBF</td><td class="align-left valign-top" id="en0074">Radial Basis Function</td><td class="align-left valign-top" id="en0075">ResNet</td><td class="align-left valign-top" id="en0076">Residual Networks</td></tr><tr><td class="align-left valign-top" id="en0077">FC</td><td class="align-left valign-top" id="en0078">Full-connected</td><td class="align-left valign-top" id="en0079">GAP</td><td class="align-left valign-top" id="en0080">Global Average Pooling</td></tr><tr><td class="align-left valign-top" id="en0081">MKL</td><td class="align-left valign-top" id="en0082">Multiple Kernel Learning</td><td class="align-left valign-top" id="en0083">AUs</td><td class="align-left valign-top" id="en0084">Action Units</td></tr><tr><td class="align-left valign-top" id="en0085">RF</td><td class="align-left valign-top" id="en0086">Random Forest</td><td class="align-left valign-top" id="en0087">AAM</td><td class="align-left valign-top" id="en0088">Active Appearance Model</td></tr><tr><td class="align-left valign-top" id="en0089">ICA</td><td class="align-left valign-top" id="en0090">Independent Component Analysis</td><td class="align-left valign-top" id="en0091">LFPC</td><td class="align-left valign-top" id="en0092">Logarithmic Frequency Power Coefficient</td></tr><tr><td class="align-left valign-top" id="en0093">BoW</td><td class="align-left valign-top" id="en0094">Bag-of-Words</td><td class="align-left valign-top" id="en0095">ROIs</td><td class="align-left valign-top" id="en0096">Regions of Interest</td></tr><tr><td class="align-left valign-top" id="en0097">LBP-TOP</td><td class="align-left valign-top" id="en0098">Local Binary Pattern from Three Orthogonal Planes</td><td class="align-left valign-top" id="en0099">MFCC</td><td class="align-left valign-top" id="en0100">MEL Frequency Cepstrum Coefficient</td></tr></tbody></table></div></div></div></section><section id="sec0002"><h2 id="cesectitle0005" class="u-h4 u-margin-l-top u-margin-xs-bottom">2. Related works</h2><div class="u-margin-s-bottom"><div id="para0020"><span>In this section, we introduce the most relevant reviews related to <a href="/topics/computer-science/affective-computing" title="Learn more about affective computing from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective computing</a><span> published between 2017 and 2020 in the perspective of affect modalities (i.e., <a href="/topics/medicine-and-dentistry/physical-modality" title="Learn more about physical modality from ScienceDirect's AI-generated Topic Pages" class="topic-link">physical modality</a>, physiological modality, and physical-physiological modality). </span></span><a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a><span> shows an overview of these latest reviews, compared with our survey. The comparison includes publication year (Year), emotion model, database (DB), modality, multimodal fusion, method, and <a href="/topics/computer-science/quantitative-evaluation" title="Learn more about quantitative evaluation from ScienceDirect's AI-generated Topic Pages" class="topic-link">quantitative evaluation</a> (QE). According to </span><a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a>, our review systematically covers all aspects of affective computing.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0002"><span class="captions text-s"><span id="cap0008"><p id="spara008"><span class="label">Table 2</span>. Overview of the most relevant reviews related to affective computing published between 2017 and 2020 and our proposed review.</p></span></span><div class="groups"><table><thead><tr><th scope="col" class="align-left valign-top rowsep-1" id="en0101" rowspan="2">Author</th><th scope="col" class="align-left valign-top rowsep-1" id="en0102" rowspan="2">Year</th><th scope="col" class="align-left valign-top" id="en0103" colspan="2"><a class="anchor anchor-primary" href="#tb2fn1" name="btb2fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb2fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>a</sup></span></span></a> Emotion Model</th><th scope="col" class="align-left valign-top rowsep-1" id="en0104" rowspan="2">DB</th><th scope="col" class="align-left valign-top" id="en0105" colspan="4"><a class="anchor anchor-primary" href="#tb2fn2" name="btb2fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb2fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>b</sup></span></span></a> Modality</th><th scope="col" class="align-left valign-top" id="en0106" colspan="5"><a class="anchor anchor-primary" href="#tb2fn3" name="btb2fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb2fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>c</sup></span></span></a> Multimodal Fusion</th><th scope="col" class="align-left valign-top" id="en0107" colspan="2">Method</th><th scope="col" class="align-left valign-top rowsep-1" id="en0108" rowspan="2">QE</th></tr><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0111">Dis</th><th scope="col" class="align-left valign-top" id="en0112">Dim</th><th scope="col" class="align-left valign-top" id="en0114">T</th><th scope="col" class="align-left valign-top" id="en0115">A</th><th scope="col" class="align-left valign-top" id="en0116">V</th><th scope="col" class="align-left valign-top" id="en0117">P</th><th scope="col" class="align-left valign-top" id="en0118">VA</th><th scope="col" class="align-left valign-top" id="en0119">TA</th><th scope="col" class="align-left valign-top" id="en0120">VAT</th><th scope="col" class="align-left valign-top" id="en0121">M-P</th><th scope="col" class="align-left valign-top" id="en0122">P-P</th><th scope="col" class="align-left valign-top" id="en0123">ML</th><th scope="col" class="align-left valign-top" id="en0124">DL</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0126" colspan="17"><strong><em>Reviews on physical-based Affect Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en0127">Ko <a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a></td><td class="align-left valign-top" id="en0128">2018</td><td class="align-left valign-top" id="en0129">✗</td><td class="align-left valign-top" id="en0130">✗</td><td class="align-left valign-top" id="en0131">✓</td><td class="align-left valign-top" id="en0132">✗</td><td class="align-left valign-top" id="en0133">✗</td><td class="align-left valign-top" id="en0134">✓</td><td class="align-left valign-top" id="en0135">✗</td><td class="align-left valign-top" id="en0136">✗</td><td class="align-left valign-top" id="en0137">✗</td><td class="align-left valign-top" id="en0138">✗</td><td class="align-left valign-top" id="en0139">✗</td><td class="align-left valign-top" id="en0140">✗</td><td class="align-left valign-top" id="en0141">✓</td><td class="align-left valign-top" id="en0142">✓</td><td class="align-left valign-top" id="en0143">✓</td></tr><tr><td class="align-left valign-top" id="en0144">Patel et&nbsp;al. <a class="anchor anchor-primary" href="#bib0044" name="bbib0044" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0044"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></td><td class="align-left valign-top" id="en0145">2020</td><td class="align-left valign-top" id="en0146">✗</td><td class="align-left valign-top" id="en0147">✗</td><td class="align-left valign-top" id="en0148">✓</td><td class="align-left valign-top" id="en0149">✗</td><td class="align-left valign-top" id="en0150">✗</td><td class="align-left valign-top" id="en0151">✓</td><td class="align-left valign-top" id="en0152">✗</td><td class="align-left valign-top" id="en0153">✗</td><td class="align-left valign-top" id="en0154">✗</td><td class="align-left valign-top" id="en0155">✗</td><td class="align-left valign-top" id="en0156">✗</td><td class="align-left valign-top" id="en0157">✗</td><td class="align-left valign-top" id="en0158">✓</td><td class="align-left valign-top" id="en0159">✓</td><td class="align-left valign-top" id="en0160">✓</td></tr><tr><td class="align-left valign-top" id="en0161">Li et&nbsp;al. <a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a></td><td class="align-left valign-top" id="en0162">2020</td><td class="align-left valign-top" id="en0163">✗</td><td class="align-left valign-top" id="en0164">✗</td><td class="align-left valign-top" id="en0165">✓</td><td class="align-left valign-top" id="en0166">✗</td><td class="align-left valign-top" id="en0167">✗</td><td class="align-left valign-top" id="en0168">✓</td><td class="align-left valign-top" id="en0169">✗</td><td class="align-left valign-top" id="en0170">✗</td><td class="align-left valign-top" id="en0171">✗</td><td class="align-left valign-top" id="en0172">✗</td><td class="align-left valign-top" id="en0173">✗</td><td class="align-left valign-top" id="en0174">✗</td><td class="align-left valign-top" id="en0175">✗</td><td class="align-left valign-top" id="en0176">✓</td><td class="align-left valign-top" id="en0177">✓</td></tr><tr><td class="align-left valign-top" id="en0178">Alexandre et&nbsp;al. <a class="anchor anchor-primary" href="#bib0041" name="bbib0041" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0041"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></td><td class="align-left valign-top" id="en0179">2020</td><td class="align-left valign-top" id="en0180">✗</td><td class="align-left valign-top" id="en0181">✗</td><td class="align-left valign-top" id="en0182">✓</td><td class="align-left valign-top" id="en0183">✗</td><td class="align-left valign-top" id="en0184">✗</td><td class="align-left valign-top" id="en0185">✓</td><td class="align-left valign-top" id="en0186">✗</td><td class="align-left valign-top" id="en0187">✗</td><td class="align-left valign-top" id="en0188">✗</td><td class="align-left valign-top" id="en0189">✗</td><td class="align-left valign-top" id="en0190">✗</td><td class="align-left valign-top" id="en0191">✗</td><td class="align-left valign-top" id="en0192">✓</td><td class="align-left valign-top" id="en0193">✓</td><td class="align-left valign-top" id="en0194">✓</td></tr><tr><td class="align-left valign-top" id="en0195">Merghani et&nbsp;al. <a class="anchor anchor-primary" href="#bib0040" name="bbib0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0040"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></td><td class="align-left valign-top" id="en0196">2018</td><td class="align-left valign-top" id="en0197">✗</td><td class="align-left valign-top" id="en0198">✗</td><td class="align-left valign-top" id="en0199">✓</td><td class="align-left valign-top" id="en0200">✗</td><td class="align-left valign-top" id="en0201">✗</td><td class="align-left valign-top" id="en0202">✓</td><td class="align-left valign-top" id="en0203">✗</td><td class="align-left valign-top" id="en0204">✗</td><td class="align-left valign-top" id="en0205">✗</td><td class="align-left valign-top" id="en0206">✗</td><td class="align-left valign-top" id="en0207">✗</td><td class="align-left valign-top" id="en0208">✗</td><td class="align-left valign-top" id="en0209">✓</td><td class="align-left valign-top" id="en0210">✓</td><td class="align-left valign-top" id="en0211">✓</td></tr><tr><td class="align-left valign-top" id="en0212">Noroozi et&nbsp;al. <a class="anchor anchor-primary" href="#bib0045" name="bbib0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0045"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></td><td class="align-left valign-top" id="en0213">2018</td><td class="align-left valign-top" id="en0214">✓</td><td class="align-left valign-top" id="en0215">✓</td><td class="align-left valign-top" id="en0216">✓</td><td class="align-left valign-top" id="en0217">✗</td><td class="align-left valign-top" id="en0218">✓</td><td class="align-left valign-top" id="en0219">✓</td><td class="align-left valign-top" id="en0220">✗</td><td class="align-left valign-top" id="en0221">✓</td><td class="align-left valign-top" id="en0222">✗</td><td class="align-left valign-top" id="en0223">✗</td><td class="align-left valign-top" id="en0224">✗</td><td class="align-left valign-top" id="en0225">✗</td><td class="align-left valign-top" id="en0226">✓</td><td class="align-left valign-top" id="en0227">✓</td><td class="align-left valign-top" id="en0228">✗</td></tr><tr><td class="align-left valign-top" id="en0229">Liu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0042" name="bbib0042" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0042"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></td><td class="align-left valign-top" id="en0230">2019</td><td class="align-left valign-top" id="en0231">✗</td><td class="align-left valign-top" id="en0232">✗</td><td class="align-left valign-top" id="en0233">✓</td><td class="align-left valign-top" id="en0234">✓</td><td class="align-left valign-top" id="en0235">✗</td><td class="align-left valign-top" id="en0236">✗</td><td class="align-left valign-top" id="en0237">✗</td><td class="align-left valign-top" id="en0238">✗</td><td class="align-left valign-top" id="en0239">✗</td><td class="align-left valign-top" id="en0240">✗</td><td class="align-left valign-top" id="en0241">✗</td><td class="align-left valign-top" id="en0242">✗</td><td class="align-left valign-top" id="en0243">✓</td><td class="align-left valign-top" id="en0244">✓</td><td class="align-left valign-top" id="en0245">✓</td></tr><tr><td class="align-left valign-top" id="en0246">Poria et&nbsp;al. <a class="anchor anchor-primary" href="#bib0046" name="bbib0046" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0046"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a></td><td class="align-left valign-top" id="en0247">2019</td><td class="align-left valign-top" id="en0248">✓</td><td class="align-left valign-top" id="en0249">✓</td><td class="align-left valign-top" id="en0250">✓</td><td class="align-left valign-top" id="en0251">✓</td><td class="align-left valign-top" id="en0252">✗</td><td class="align-left valign-top" id="en0253">✗</td><td class="align-left valign-top" id="en0254">✗</td><td class="align-left valign-top" id="en0255">✗</td><td class="align-left valign-top" id="en0256">✗</td><td class="align-left valign-top" id="en0257">✗</td><td class="align-left valign-top" id="en0258">✗</td><td class="align-left valign-top" id="en0259">✗</td><td class="align-left valign-top" id="en0260">✗</td><td class="align-left valign-top" id="en0261">✓</td><td class="align-left valign-top" id="en0262">✓</td></tr><tr><td class="align-left valign-top" id="en0263">Yue et&nbsp;al. <a class="anchor anchor-primary" href="#bib0047" name="bbib0047" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0047"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></td><td class="align-left valign-top" id="en0264">2019</td><td class="align-left valign-top" id="en0265">✓</td><td class="align-left valign-top" id="en0266">✓</td><td class="align-left valign-top" id="en0267">✓</td><td class="align-left valign-top" id="en0268">✓</td><td class="align-left valign-top" id="en0269">✗</td><td class="align-left valign-top" id="en0270">✗</td><td class="align-left valign-top" id="en0271">✗</td><td class="align-left valign-top" id="en0272">✗</td><td class="align-left valign-top" id="en0273">✗</td><td class="align-left valign-top" id="en0274">✗</td><td class="align-left valign-top" id="en0275">✗</td><td class="align-left valign-top" id="en0276">✗</td><td class="align-left valign-top" id="en0277">✓</td><td class="align-left valign-top" id="en0278">✓</td><td class="align-left valign-top" id="en0279">✗</td></tr><tr><td class="align-left valign-top" id="en0280">Khalil et&nbsp;al. <a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></td><td class="align-left valign-top" id="en0281">2019</td><td class="align-left valign-top" id="en0282">✗</td><td class="align-left valign-top" id="en0283">✗</td><td class="align-left valign-top" id="en0284">✓</td><td class="align-left valign-top" id="en0285">✓</td><td class="align-left valign-top" id="en0286">✗</td><td class="align-left valign-top" id="en0287">✗</td><td class="align-left valign-top" id="en0288">✗</td><td class="align-left valign-top" id="en0289">✗</td><td class="align-left valign-top" id="en0290">✗</td><td class="align-left valign-top" id="en0291">✗</td><td class="align-left valign-top" id="en0292">✗</td><td class="align-left valign-top" id="en0293">✗</td><td class="align-left valign-top" id="en0294">✗</td><td class="align-left valign-top" id="en0295">✓</td><td class="align-left valign-top" id="en0296">✓</td></tr><tr><td class="align-left valign-top" id="en0297">Wang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0048" name="bbib0048" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0048"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a></td><td class="align-left valign-top" id="en0298">2020</td><td class="align-left valign-top" id="en0299">✓</td><td class="align-left valign-top" id="en0300">✓</td><td class="align-left valign-top" id="en0301">✓</td><td class="align-left valign-top" id="en0302">✓</td><td class="align-left valign-top" id="en0303">✗</td><td class="align-left valign-top" id="en0304">✗</td><td class="align-left valign-top" id="en0305">✗</td><td class="align-left valign-top" id="en0306">✗</td><td class="align-left valign-top" id="en0307">✗</td><td class="align-left valign-top" id="en0308">✗</td><td class="align-left valign-top" id="en0309">✗</td><td class="align-left valign-top" id="en0310">✗</td><td class="align-left valign-top" id="en0311">✓</td><td class="align-left valign-top" id="en0312">✗</td><td class="align-left valign-top" id="en0313">✗</td></tr><tr><td class="align-left valign-top" id="en0314">Han et&nbsp;al. <a class="anchor anchor-primary" href="#bib0049" name="bbib0049" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0049"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></td><td class="align-left valign-top" id="en0315">2019</td><td class="align-left valign-top" id="en0316">✗</td><td class="align-left valign-top" id="en0317">✗</td><td class="align-left valign-top" id="en0318">✗</td><td class="align-left valign-top" id="en0319">✓</td><td class="align-left valign-top" id="en0320">✓</td><td class="align-left valign-top" id="en0321">✓</td><td class="align-left valign-top" id="en0322">✗</td><td class="align-left valign-top" id="en0323">✗</td><td class="align-left valign-top" id="en0324">✗</td><td class="align-left valign-top" id="en0325">✗</td><td class="align-left valign-top" id="en0326">✗</td><td class="align-left valign-top" id="en0327">✗</td><td class="align-left valign-top" id="en0328">✗</td><td class="align-left valign-top" id="en0329">✓</td><td class="align-left valign-top" id="en0330">✗</td></tr><tr><td class="align-left valign-top" id="en0331">Erik et&nbsp;al. <a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></td><td class="align-left valign-top" id="en0332">2017</td><td class="align-left valign-top" id="en0333">✓</td><td class="align-left valign-top" id="en0334">✓</td><td class="align-left valign-top" id="en0335">✓</td><td class="align-left valign-top" id="en0336">✓</td><td class="align-left valign-top" id="en0337">✓</td><td class="align-left valign-top" id="en0338">✓</td><td class="align-left valign-top" id="en0339">✗</td><td class="align-left valign-top" id="en0340">✓</td><td class="align-left valign-top" id="en0341">✓</td><td class="align-left valign-top" id="en0342">✓</td><td class="align-left valign-top" id="en0343">✗</td><td class="align-left valign-top" id="en0344">✗</td><td class="align-left valign-top" id="en0345">✓</td><td class="align-left valign-top" id="en0346">✓</td><td class="align-left valign-top" id="en0347">✓</td></tr><tr><td class="align-left valign-top" id="en0348" colspan="17"><strong><em>Reviews on physiological-based Emotion Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en0349">Bota et&nbsp;al. <a class="anchor anchor-primary" href="#bib0050" name="bbib0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0050"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></td><td class="align-left valign-top" id="en0350">2019</td><td class="align-left valign-top" id="en0351">✓</td><td class="align-left valign-top" id="en0352">✓</td><td class="align-left valign-top" id="en0353">✓</td><td class="align-left valign-top" id="en0354">✗</td><td class="align-left valign-top" id="en0355">✗</td><td class="align-left valign-top" id="en0356">✗</td><td class="align-left valign-top" id="en0357">✓</td><td class="align-left valign-top" id="en0358">✗</td><td class="align-left valign-top" id="en0359">✗</td><td class="align-left valign-top" id="en0360">✗</td><td class="align-left valign-top" id="en0361">✗</td><td class="align-left valign-top" id="en0362">✗</td><td class="align-left valign-top" id="en0363">✓</td><td class="align-left valign-top" id="en0364">✓</td><td class="align-left valign-top" id="en0365">✓</td></tr><tr><td class="align-left valign-top" id="en0366"><a class="anchor anchor-primary" href="#tb2fn4" name="btb2fn4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb2fn4"><span class="anchor-text-container"><span class="anchor-text"><sup>d</sup></span></span></a> G-M et&nbsp;al. <a class="anchor anchor-primary" href="#bib0051" name="bbib0051" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0051"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></td><td class="align-left valign-top" id="en0367">2019</td><td class="align-left valign-top" id="en0368">✓</td><td class="align-left valign-top" id="en0369">✓</td><td class="align-left valign-top" id="en0370">✗</td><td class="align-left valign-top" id="en0371">✗</td><td class="align-left valign-top" id="en0372">✗</td><td class="align-left valign-top" id="en0373">✗</td><td class="align-left valign-top" id="en0374">✓</td><td class="align-left valign-top" id="en0375">✗</td><td class="align-left valign-top" id="en0376">✗</td><td class="align-left valign-top" id="en0377">✗</td><td class="align-left valign-top" id="en0378">✗</td><td class="align-left valign-top" id="en0379">✗</td><td class="align-left valign-top" id="en0380">✓</td><td class="align-left valign-top" id="en0381">✗</td><td class="align-left valign-top" id="en0382">✓</td></tr><tr><td class="align-left valign-top" id="en0383"><a class="anchor anchor-primary" href="#tb2fn5" name="btb2fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb2fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>e</sup></span></span></a> Ala. and Fon. <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a></td><td class="align-left valign-top" id="en0384">2019</td><td class="align-left valign-top" id="en0385">✓</td><td class="align-left valign-top" id="en0386">✓</td><td class="align-left valign-top" id="en0387">✗</td><td class="align-left valign-top" id="en0388">✗</td><td class="align-left valign-top" id="en0389">✗</td><td class="align-left valign-top" id="en0390">✗</td><td class="align-left valign-top" id="en0391">✓</td><td class="align-left valign-top" id="en0392">✗</td><td class="align-left valign-top" id="en0393">✗</td><td class="align-left valign-top" id="en0394">✗</td><td class="align-left valign-top" id="en0395">✗</td><td class="align-left valign-top" id="en0396">✗</td><td class="align-left valign-top" id="en0397">✓</td><td class="align-left valign-top" id="en0398">✗</td><td class="align-left valign-top" id="en0399">✓</td></tr><tr><td class="align-left valign-top" id="en0400" colspan="17"><strong><em>Reviews on physical-physiological fusion for affective analysis</em></strong></td></tr><tr><td class="align-left valign-top" id="en0401">Rouast et&nbsp;al. <a class="anchor anchor-primary" href="#bib0013" name="bbib0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0013"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></td><td class="align-left valign-top" id="en0402">2019</td><td class="align-left valign-top" id="en0403">✓</td><td class="align-left valign-top" id="en0404">✓</td><td class="align-left valign-top" id="en0405">✓</td><td class="align-left valign-top" id="en0406">✗</td><td class="align-left valign-top" id="en0407">✓</td><td class="align-left valign-top" id="en0408">✓</td><td class="align-left valign-top" id="en0409">✓</td><td class="align-left valign-top" id="en0410">✓</td><td class="align-left valign-top" id="en0411">✗</td><td class="align-left valign-top" id="en0412">✗</td><td class="align-left valign-top" id="en0413">✓</td><td class="align-left valign-top" id="en0414">✗</td><td class="align-left valign-top" id="en0415">✓</td><td class="align-left valign-top" id="en0416">✓</td><td class="align-left valign-top" id="en0417">✓</td></tr><tr><td class="align-left valign-top" id="en0418">Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a></td><td class="align-left valign-top" id="en0419">2020</td><td class="align-left valign-top" id="en0420">✓</td><td class="align-left valign-top" id="en0421">✓</td><td class="align-left valign-top" id="en0422">✓</td><td class="align-left valign-top" id="en0423">✓</td><td class="align-left valign-top" id="en0424">✓</td><td class="align-left valign-top" id="en0425">✓</td><td class="align-left valign-top" id="en0426">✓</td><td class="align-left valign-top" id="en0427">✓</td><td class="align-left valign-top" id="en0428">✗</td><td class="align-left valign-top" id="en0429">✗</td><td class="align-left valign-top" id="en0430">✓</td><td class="align-left valign-top" id="en0431">✗</td><td class="align-left valign-top" id="en0432">✓</td><td class="align-left valign-top" id="en0433">✓</td><td class="align-left valign-top" id="en0434">✓</td></tr><tr><td class="align-left valign-top" id="en0435">Jiang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a></td><td class="align-left valign-top" id="en0436">2020</td><td class="align-left valign-top" id="en0437">✗</td><td class="align-left valign-top" id="en0438">✗</td><td class="align-left valign-top" id="en0439">✓</td><td class="align-left valign-top" id="en0440">✓</td><td class="align-left valign-top" id="en0441">✓</td><td class="align-left valign-top" id="en0442">✓</td><td class="align-left valign-top" id="en0443">✓</td><td class="align-left valign-top" id="en0444">✓</td><td class="align-left valign-top" id="en0445">✗</td><td class="align-left valign-top" id="en0446">✓</td><td class="align-left valign-top" id="en0447">✗</td><td class="align-left valign-top" id="en0448">✓</td><td class="align-left valign-top" id="en0449">✓</td><td class="align-left valign-top" id="en0450">✓</td><td class="align-left valign-top" id="en0451">✓</td></tr><tr><td class="align-left valign-top" id="en0452">Shoumy et&nbsp;al. <a class="anchor anchor-primary" href="#bib0014" name="bbib0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0014"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a></td><td class="align-left valign-top" id="en0453">2020</td><td class="align-left valign-top" id="en0454">✓</td><td class="align-left valign-top" id="en0455">✓</td><td class="align-left valign-top" id="en0456">✓</td><td class="align-left valign-top" id="en0457">✓</td><td class="align-left valign-top" id="en0458">✓</td><td class="align-left valign-top" id="en0459">✓</td><td class="align-left valign-top" id="en0460">✓</td><td class="align-left valign-top" id="en0461">✓</td><td class="align-left valign-top" id="en0462">✗</td><td class="align-left valign-top" id="en0463">✓</td><td class="align-left valign-top" id="en0464">✓</td><td class="align-left valign-top" id="en0465">✓</td><td class="align-left valign-top" id="en0466">✓</td><td class="align-left valign-top" id="en0467">✓</td><td class="align-left valign-top" id="en0468">✓</td></tr><tr><td class="align-left valign-top" id="en0469" colspan="17"><strong><em>Proposed review</em></strong></td></tr><tr><td class="align-left valign-top" id="en0470">Our proposed</td><td class="align-left valign-top" id="en0471">2021</td><td class="align-left valign-top" id="en0472">✓</td><td class="align-left valign-top" id="en0473">✓</td><td class="align-left valign-top" id="en0474">✓</td><td class="align-left valign-top" id="en0475">✓</td><td class="align-left valign-top" id="en0476">✓</td><td class="align-left valign-top" id="en0477">✓</td><td class="align-left valign-top" id="en0478">✓</td><td class="align-left valign-top" id="en0479">✓</td><td class="align-left valign-top" id="en0480">✓</td><td class="align-left valign-top" id="en0481">✓</td><td class="align-left valign-top" id="en0482">✓</td><td class="align-left valign-top" id="en0483">✓</td><td class="align-left valign-top" id="en0484">✓</td><td class="align-left valign-top" id="en0485">✓</td><td class="align-left valign-top" id="en0486">✓</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb2fn1">a</dt><dd><div class="u-margin-s-bottom" id="notep0001">Emotion Model: Dis=Discrete, Dim=Dimensional;</div></dd><dt id="tb2fn2">b</dt><dd><div class="u-margin-s-bottom" id="notep0002">Modality: V=Visual (Facial expression, Body gesture), A=Audio (Speech), T=Textual, and P= Physiological (e.g., EEG, ECG, and EMG);</div></dd><dt id="tb2fn3">c</dt><dd><div class="u-margin-s-bottom" id="notep0003">Multimodal Fusion: VA=Visual-Audio, TA=Textual-Audio, VAT=Visual-Audio-Textual, M-P=Multi-Physiological, P-P=Physical-Physiological.</div></dd><dt id="tb2fn4">d</dt><dd><div class="u-margin-s-bottom" id="notep0004">G-M=Garcia-Martinez.</div></dd><dt id="tb2fn5">e</dt><dd><div class="u-margin-s-bottom" id="notep0005">Ala. and Fon.=Alarcão and Fonseca;</div></dd></dl></div></div><section id="sec0003"><h3 id="cesectitle0006" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Reviews on physical-based affect recognition</h3><div class="u-margin-s-bottom" id="para0021">The existing researches on physical-based affect recognition mainly used visual, textual, and audio modalities <a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a><span>. For <a href="/topics/computer-science/visual-modality" title="Learn more about visual modality from ScienceDirect's AI-generated Topic Pages" class="topic-link">visual modality</a><span>, most studies surveyed <a href="/topics/biochemistry-genetics-and-molecular-biology/facial-recognition" title="Learn more about FER from ScienceDirect's AI-generated Topic Pages" class="topic-link">FER</a> [</span></span><a class="anchor anchor-primary" href="#bib0038" name="bbib0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0038"><span class="anchor-text-container"><span class="anchor-text">38</span></span></a>,<a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">39</span></span></a>,<a class="anchor anchor-primary" href="#bib0041" name="bbib0041" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0041"><span class="anchor-text-container"><span class="anchor-text">41</span></span></a>,<a class="anchor anchor-primary" href="#bib0044" name="bbib0044" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0044"><span class="anchor-text-container"><span class="anchor-text">44</span></span></a>] and FMEA <a class="anchor anchor-primary" href="#bib0040" name="bbib0040" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0040"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a><span>, and 3D <a href="/topics/medicine-and-dentistry/facial-recognition" title="Learn more about FER from ScienceDirect's AI-generated Topic Pages" class="topic-link">FER</a> </span><a class="anchor anchor-primary" href="#bib0041" name="bbib0041" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0041"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a>. Besides, Noroozi et&nbsp;al. <a class="anchor anchor-primary" href="#bib0045" name="bbib0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0045"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a><span> reviewed <a href="/topics/computer-science/representation-learning" title="Learn more about representation learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">representation learning</a><span><span> and emotion recognition from the body gestures followed by multimodal emotion recognition on the basis of speech or <a href="/topics/neuroscience/face" title="Learn more about face from ScienceDirect's AI-generated Topic Pages" class="topic-link">face</a> and body gestures. For textual modality, the works on </span><a href="/topics/computer-science/sentiment-analysis" title="Learn more about sentiment analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">sentiment analysis</a> and emotion recognition [</span></span><a class="anchor anchor-primary" href="#bib0042" name="bbib0042" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0042"><span class="anchor-text-container"><span class="anchor-text">42</span></span></a>,<a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">43</span></span></a>,<a class="anchor anchor-primary" href="#bib0046" name="bbib0046" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0046"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a>, <a class="anchor anchor-primary" href="#bib0047" name="bbib0047" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0047"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a>, <a class="anchor anchor-primary" href="#bib0048" name="bbib0048" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0048"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a>] can be completed according to the implicit emotions in the conversation. Han et&nbsp;al. <a class="anchor anchor-primary" href="#bib0049" name="bbib0049" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0049"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a><span> presented DL-based <a href="/topics/computer-science/adversarial-machine-learning" title="Learn more about adversarial training from ScienceDirect's AI-generated Topic Pages" class="topic-link">adversarial training</a><span> using three kinds of physical signals, aiming to address various challenges associated with emotional <a href="/topics/computer-science/artificial-intelligence" title="Learn more about AI from ScienceDirect's AI-generated Topic Pages" class="topic-link">AI</a> systems. In contrast, Erik et&nbsp;al. </span></span><a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a><span> considered multimodal fusion and provided a critical analysis of potential <a href="/topics/computer-science/performance-improvement" title="Learn more about performance improvements from ScienceDirect's AI-generated Topic Pages" class="topic-link">performance improvements</a> with multimodal affect recognition under different fusion categories, compared to unimodal analysis.</span></div><div class="u-margin-s-bottom" id="para0022">All reviews on physical-based affect recognition listed in <a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a> have reviewed DL-based methods, which indicates a clear development trend in the application of deep learning for this domain. However, the existing works have not fully involved the latest research advances and achievements in DL-based affective computing, which is one of the objectives of our review.</div></section><section id="sec0004"><h3 id="cesectitle0007" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Reviews on physiological-based emotion recognition</h3><div class="u-margin-s-bottom" id="para0023"><span>The emerging development of physiological emotion recognition has been made possible by the utilization of embedded devices for the acquisition of <a href="/topics/computer-science/physiological-signal" title="Learn more about physiological signals from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological signals</a>. In 2019, Bota et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0050" name="bbib0050" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0050"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a> reviewed ML-based emotion recognition using different physiological signals, along with the key theoretical concepts and backgrounds, methods, and future developments. Garcia-Martinez et&nbsp;al. <a class="anchor anchor-primary" href="#bib0051" name="bbib0051" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0051"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a> reviewed nonlinear EEG-based emotion recognition and identified some nonlinear indexes in future research. Alarcão and Fonseca <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a> also reviewed works about EEG-based emotion recognition from 2009 to 2016, but focused on subjects, feature representation, classification, and their performances.</div><div class="u-margin-s-bottom" id="para0024">All reviews on physiological-based emotion recognition listed in <a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a> have reviewed the ML-based methods in discrete and dimensional emotion space, but only one work involved a few DL-based methods. In this review, we surveyed ML-based and DL-based works which have contributed to the advance of physiological-based emotion recognition.</div></section><section id="sec0005"><h3 id="cesectitle0008" class="u-h4 u-margin-m-top u-margin-xs-bottom">2.3. Reviews on physical-physiological fusion for affective analysis</h3><div class="u-margin-s-bottom" id="para0025">There is a clear trend of using both physical and physiological signals for affective analysis. In 2019, Rouast et&nbsp;al. <a class="anchor anchor-primary" href="#bib0013" name="bbib0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0013"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a> reviewed 233 DL-based human affective recognition methods that use audio-visual and physiological signals to learn spatial, temporal, and joint feature representations. In 2020, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a><span><span><span> introduced different feature extraction, feature reduction, and ML-based classifiers in terms of the standard pipeline for multi-channel <a href="/topics/psychology/electroencephalography" title="Learn more about EEG from ScienceDirect's AI-generated Topic Pages" class="topic-link">EEG</a> emotion recognition, and discussed the recent advances of multimodal emotion recognition based on </span><a href="/topics/computer-science/machine-learning" title="Learn more about ML from ScienceDirect's AI-generated Topic Pages" class="topic-link">ML</a> or </span><a href="/topics/computer-science/deep-learning" title="Learn more about DL from ScienceDirect's AI-generated Topic Pages" class="topic-link">DL</a> techniques. Jiang et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a> summarized the current development of multimodal databases, the feature extraction based on EEG, visual, audio and text information, multimodal fusion strategies, and recognition methods according to the pipeline of the real-time emotion health surveillance system. Shoumy et&nbsp;al. <a class="anchor anchor-primary" href="#bib0014" name="bbib0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0014"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a> reviewed different frameworks and lasted techniques using textual, audio, visual and physiological signals, and extensive analysis of their performances. In the end, various applications of affective analysis were discussed followed by their trends and future works.</div><div class="u-margin-s-bottom" id="para0026">All these recent works listed in <a class="anchor anchor-primary" href="#tbl0002" name="btbl0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0002"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;2</span></span></a> have reviewed general aspects related to affective computing including emotion models, unimodal affect recognition and multimodal fusion for affective analysis as well as ML-based and DL-based models. However, the existing works have not fully elaborated their comparisons in unimodal and multimodal affective analysis, which is one of the objectives of our review.</div></section></section><section id="sec0006"><h2 id="cesectitle0009" class="u-h4 u-margin-l-top u-margin-xs-bottom">3. Emotion models</h2><div class="u-margin-s-bottom" id="para0027">The definition of the emotion or affect is essential to establish a criterion for affective computing. The basic concept of emotions was first introduced by Ekman <a class="anchor anchor-primary" href="#bib0052" name="bbib0052" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0052"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a><span> in the 1970s. Although psychologists attempt to classify emotions in different ways in the multidisciplinary fields of <a href="/topics/psychology/neuroscience" title="Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages" class="topic-link">neuroscience</a>, philosophy, and computer science </span><a class="anchor anchor-primary" href="#bib0053" name="bbib0053" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0053"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a>, there are no unanimously accepted emotion models. However, there are two types of generic emotion models in affective computing, namely discrete emotion model <a class="anchor anchor-primary" href="#bib0052" name="bbib0052" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0052"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a> and dimensional emotion model (or continuous emotion model) [<a class="anchor anchor-primary" href="#bib0016" name="bbib0016" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0016"><span class="anchor-text-container"><span class="anchor-text">16</span></span></a>,<a class="anchor anchor-primary" href="#bib0054" name="bbib0054" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0054"><span class="anchor-text-container"><span class="anchor-text">54</span></span></a>].</div><section id="sec0007"><h3 id="cesectitle0010" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.1. Discrete emotion model</h3><div class="u-margin-s-bottom"><div id="para0028">The discrete emotion model, also called as categorical emotion model, defines emotions into limited categories. Two widely used discrete emotion models are Ekman's six basic emotions <a class="anchor anchor-primary" href="#bib0052" name="bbib0052" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0052"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a> and Plutchik's emotional wheel model <a class="anchor anchor-primary" href="#bib0055" name="bbib0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0055"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a>, as shown in <a class="anchor anchor-primary" href="#fig0002" name="bfig0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0002"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;2</span></span></a> (a) and <a class="anchor anchor-primary" href="#fig0002" name="bfig0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0002"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;2</span></span></a> (b), respectively.</div><figure class="figure text-xs" id="fig0002"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr2.jpg" height="479" alt="Fig 2" aria-describedby="cap0002"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr2_lrg.jpg" target="_blank" download="" title="Download high-res image (741KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (741KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr2.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0002"><p id="spara002"><span class="label">Fig. 2</span>. Two discrete emotion models for affective computing. (a) 6 basic emotion models <a class="anchor anchor-primary" href="#bib0015" name="bbib0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0015"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a> shown in emoji types and (b) componential models (Plutchik's emotional wheel model) <a class="anchor anchor-primary" href="#bib0055" name="bbib0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0055"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a>.</p></span></span></figure></div><div class="u-margin-s-bottom" id="para0029">Ekman's basic emotion model and its variants [<a class="anchor anchor-primary" href="#bib0056" name="bbib0056" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0056"><span class="anchor-text-container"><span class="anchor-text">56</span></span></a>,<a class="anchor anchor-primary" href="#bib0057" name="bbib0057" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0057"><span class="anchor-text-container"><span class="anchor-text">57</span></span></a>] are widely accepted by the emotion recognition community [<a class="anchor anchor-primary" href="#bib0058" name="bbib0058" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0058"><span class="anchor-text-container"><span class="anchor-text">58</span></span></a>,<a class="anchor anchor-primary" href="#bib0059" name="bbib0059" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0059"><span class="anchor-text-container"><span class="anchor-text">59</span></span></a>]. Six basic emotions typically include anger, disgust, fear, happy, sad, and surprise. They were derived with the following criteria <a class="anchor anchor-primary" href="#bib0052" name="bbib0052" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0052"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a>: 1) Basic emotions must come from human instinct; 2) People can produce the same basic emotions when facing the same situation; 3) People express the same basic emotions under the same semantics; 4) These basic emotions must have the same pattern of expression for all people. The development of Ekman's basic emotion model is based on the hypothesis that human emotions are shared across races and cultures. However, different cultural backgrounds may have different interpretations of basic emotions, and different basic emotions can be mixed to produce complex or compound emotions <a class="anchor anchor-primary" href="#bib0015" name="bbib0015" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0015"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0030">In contrast, Plutchik's wheel model <a class="anchor anchor-primary" href="#bib0055" name="bbib0055" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0055"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a> involves eight basic emotions (i.e., joy, trust, fear, surprise, sadness, anticipation, anger, and disgust) and the way of how these are related to one another (<a class="anchor anchor-primary" href="#fig0002" name="bfig0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0002"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;2</span></span></a> (b)). For example, joy and sadness are opposites, and anticipation can easily develop into vigilance. This wheel model is also referred to as the componential model, where the stronger emotions occupy the centre, while the weaker emotions occupy the extremes, depending on their relative intensity levels. These discrete emotions can be generally categorized into three kinds of polarity (positive, negative, and neutral), which are often used for sentiment analysis. To describe fine-grained sentiments, ambivalent sentiment handling <a class="anchor anchor-primary" href="#bib0060" name="bbib0060" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0060"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a><span> is proposed to analyze the multi-level sentiment and improve the performance of <a href="/topics/computer-science/binary-classification" title="Learn more about binary classification from ScienceDirect's AI-generated Topic Pages" class="topic-link">binary classification</a>.</span></div></section><section id="sec0008"><h3 id="cesectitle0011" class="u-h4 u-margin-m-top u-margin-xs-bottom">3.2. Dimensional emotion model</h3><div class="u-margin-s-bottom" id="para0031">To overcome the challenges confronted by the discrete emotion models, many researchers have adopted the concept of a continuous multi-dimensional model. One of the most recognized models is the Pleasure-Arousal-Dominance (PAD) <a class="anchor anchor-primary" href="#bib0016" name="bbib0016" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0016"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a>, as shown in <a class="anchor anchor-primary" href="#fig0004" name="bfig0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0004"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a> (a).</div><div class="u-margin-s-bottom" id="para0032">Similar to Mehrabian´s three-dimensional space theory of emotion <a class="anchor anchor-primary" href="#bib0061" name="bbib0061" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0061"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a><span>, the <a href="/topics/computer-science/dominance-model" title="Learn more about PAD model from ScienceDirect's AI-generated Topic Pages" class="topic-link">PAD model</a> has three-dimensional spaces: 1) Pleasure (Valence) dimension, representing the magnitude of human joy from distress extreme to ecstasies; 2) Arousal (Activation) dimension, measuring physiological activity and psychological alertness level; 3) Dominance (Attention) dimension, expressing the feeling of influencing the surrounding environment and other people, or of being influenced by the surrounding environment and others.</span></div><div class="u-margin-s-bottom" id="para0033">Since two dimensions of Pleasure and Arousal in the PAD model could represent the vast majority of different emotions <a class="anchor anchor-primary" href="#bib0062" name="bbib0062" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0062"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a>, Russell <a class="anchor anchor-primary" href="#bib0054" name="bbib0054" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0054"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a> proposed a Valence-Arousal based circumplex model to represent complex emotions. This model defines a continuous, bi-dimensional emotion space model with the axes of Valence (the degree of pleasantness or unpleasantness) and Arousal (the degree of activation or deactivation), as shown in <a class="anchor anchor-primary" href="#fig0004" name="bfig0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0004"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;3</span></span></a> (b). The circumplex model consists of four quadrants. The first quadrant, activation arousal with positive valence, shows the feelings associated with happy emotions; And the third quadrant, with low arousal and negative valence, is associated with sad emotions. The second quadrant shows angry emotions within high arousal and negative valence; And the fourth quadrant shows calm emotion within low arousal and positive valence <a class="anchor anchor-primary" href="#bib0063" name="bbib0063" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0063"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a>.</div></section></section><section id="sec0009"><h2 id="cesectitle0012" class="u-h4 u-margin-l-top u-margin-xs-bottom">4. Databases for affective computing</h2><div class="u-margin-s-bottom" id="para0034">Databases for affective computing can be classified into textual, speech/audio, visual, physiological, and multimodal databases according to the data modalities. The properties of these databases have a far-reaching influence on the model design and <a href="/topics/computer-science/network-architecture" title="Learn more about network architecture from ScienceDirect's AI-generated Topic Pages" class="topic-link">network architecture</a> for affective computing.</div><section id="sec0010"><h3 id="cesectitle0013" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Textual databases</h3><div class="u-margin-s-bottom" id="para0035"><span>Databases for TSA consist of the text data in different <a href="/topics/computer-science/granularity" title="Learn more about granularities from ScienceDirect's AI-generated Topic Pages" class="topic-link">granularities</a> (e.g., word, sentence, and document), labelled with emotion or sentiment tags (positive, negative, neutral, emphatic, general, sad, happy, etc). The earliest textual sentiment database is </span><strong>Multi-domain sentiment (MDS)</strong> [<a class="anchor anchor-primary" href="#bib0064" name="bbib0064" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0064"><span class="anchor-text-container"><span class="anchor-text">64</span></span></a>,<a class="anchor anchor-primary" href="#bib0065" name="bbib0065" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0065"><span class="anchor-text-container"><span class="anchor-text">65</span></span></a>]<strong>,</strong><span> which contains more than 100,000 sentences of product reviews acquired from <a href="/topics/earth-and-planetary-sciences/amazon" title="Learn more about Amazon from ScienceDirect's AI-generated Topic Pages" class="topic-link">Amazon</a>.com. These sentences are labelled with both two sentiment categories (positive and negative) and five sentiment categories (strong positive, weak positive, neutral, weak negative, strong negative).</span></div><div class="u-margin-s-bottom" id="para0036">Another widely-used large database for binary sentiment classification is <strong>IMDB</strong> <a class="anchor anchor-primary" href="#bib0066" name="bbib0066" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0066"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a>. It provides 25,000 highly polar movie reviews for training and 25,000 for testing. Stanford sentiment treebank (SST) <a class="anchor anchor-primary" href="#bib0067" name="bbib0067" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0067"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a><span><span> is the <a href="/topics/psychology/lexical-semantics" title="Learn more about semantic lexical from ScienceDirect's AI-generated Topic Pages" class="topic-link">semantic lexical</a> database annotated by Stanford University. It includes fine-grained emotional labels of 215,154 phrases in a </span><a href="/topics/computer-science/parse-tree" title="Learn more about parse tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">parse tree</a> of 11,855 sentences, and it is the first corpus with fully labelled parse trees.</span></div></section><section id="sec0011"><h3 id="cesectitle0014" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Speech/Audio databases</h3><div class="u-margin-s-bottom" id="para0037">Speech databases can be divided into two types: non-spontaneous (simulated and induced) and spontaneous. In the early stage, non-spontaneous speech databases were mainly generated from professional actors’ performances. Such performance-based databases are regarded as reliable ones because they can perform well-known emotional characteristics in professional ways. <strong>Berlin Database of Emotional Speech (Emo-DB)</strong> <a class="anchor anchor-primary" href="#bib0068" name="bbib0068" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0068"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a> contains about 500 utterances spoken by 10 actors (5 men and 5 women) in a happy, angry, anxious, fearful, bored and disgusting way. However, these non-spontaneous emotions can be exaggerated a little more than the real emotions. To narrow this gap, spontaneous speech databases have been developed recently. <strong>Belfast Induced Natural Emotion (Belfast)</strong> <a class="anchor anchor-primary" href="#bib0069" name="bbib0069" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0069"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a><span> was recorded from 40 subjects (aged between 18 and 69, 20 men and 20 women) at Queen University in Northern Ireland, UK. Each subject took part in five tests, each of which contains short <a href="/topics/biochemistry-genetics-and-molecular-biology/videorecording" title="Learn more about video recordings from ScienceDirect's AI-generated Topic Pages" class="topic-link">video recordings</a> (5 to 60 seconds in length) with stereo sound, and related to one of the five emotional tendencies: anger, sadness, happiness, fear, and neutrality.</span></div></section><section id="sec0012"><h3 id="cesectitle0015" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.3. Visual databases</h3><div class="u-margin-s-bottom" id="para0038">Visual databases can also be divided into two categories: facial expression databases and body gesture emotion databases.</div><section id="sec0013"><h4 id="cesectitle0016" class="u-margin-m-top u-margin-xs-bottom">4.3.1. Facial expression databases</h4><div class="u-margin-s-bottom"><div id="para0039"><a class="anchor anchor-primary" href="#tbl0003" name="btbl0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0003"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;3</span></span></a> provides an overview of facial expression databases, including the main reference (access), year, samples, subject, and expression category. The early FER databases are derived from the emotions purposely performed by subjects in the laboratory (In-the-Lab). For example, <strong>JAFFE</strong> <a class="anchor anchor-primary" href="#bib0070" name="bbib0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0070"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a> released in 1998, includes 213 images of 7 facial expressions, posed by 10 Japanese female models. To construct the <strong>extended Cohn-Kanade (CK+)</strong> <a class="anchor anchor-primary" href="#bib0071" name="bbib0071" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0071"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a>, an extension of <strong>CK</strong> <a class="anchor anchor-primary" href="#bib0072" name="bbib0072" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0072"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a><span>, subjects were instructed to perform 7 facial expressions. The facial expression images were recorded and analyzed to provide protocols and baseline results for <a href="/topics/medicine-and-dentistry/facies" title="Learn more about facial feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial feature</a> tracking, action units (AUs), and emotion recognition. Different from </span><strong>CK+, MMI</strong> <a class="anchor anchor-primary" href="#bib0073" name="bbib0073" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0073"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a> consists of onset-apex-offset sequences. <strong>Oulu-CASIA NIR-VIS</strong> (<strong>Oulu-CASIA</strong>) <a class="anchor anchor-primary" href="#bib0074" name="bbib0074" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0074"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a><span> released in 2011, includes 2,880 image sequences captured with one of two kinds of <a href="/topics/computer-science/imaging-systems" title="Learn more about imaging systems from ScienceDirect's AI-generated Topic Pages" class="topic-link">imaging systems</a><span> under three kinds of <a href="/topics/computer-science/illumination-condition" title="Learn more about illumination conditions from ScienceDirect's AI-generated Topic Pages" class="topic-link">illumination conditions</a>.</span></span></div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0003"><span class="captions text-s"><span id="cap0009"><p id="spara009"><span class="label">Table 3</span>. Overview of facial expression databases SBE&nbsp;=&nbsp;Seven Basic Emotions (anger, disgust, fear, happy, sad, surprise, and neutral).</p><div class="u-margin-s-bottom" id="spara010"><a class="anchor anchor-primary" href="#tb3fn4" name="btb3fn4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn4"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a></div></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0487">Database</th><th scope="col" class="align-left valign-top" id="en0488">Year</th><th scope="col" class="align-left valign-top" id="en0489">Samples</th><th scope="col" class="align-left valign-top" id="en0490">Subject</th><th scope="col" class="align-left valign-top" id="en0491">Expression Category</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0492" colspan="5"><strong>In-the-Lab</strong></td></tr><tr><td class="align-left valign-top" id="en0497"><a class="anchor anchor-primary" href="#tb3fn1" name="btb3fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>JAFFE <a class="anchor anchor-primary" href="#bib0070" name="bbib0070" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0070"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a></td><td class="align-left valign-top" id="en0498">1998</td><td class="align-left valign-top" id="en0499">219 images</td><td class="align-left valign-top" id="en0500">10</td><td class="align-left valign-top" id="en0501">SBE</td></tr><tr><td class="align-left valign-top" id="en0502"><a class="anchor anchor-primary" href="#tb3fn2" name="btb3fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>CK <a class="anchor anchor-primary" href="#bib0072" name="bbib0072" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0072"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a></td><td class="align-left valign-top" id="en0503">2000</td><td class="align-left valign-top" id="en0504">1,917 images</td><td class="align-left valign-top" id="en0505">210</td><td class="align-left valign-top" id="en0506">SBE plus Contempt</td></tr><tr><td class="align-left valign-top" id="en0507"><a class="anchor anchor-primary" href="#tb3fn2" name="btb3fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>CK+ <a class="anchor anchor-primary" href="#bib0071" name="bbib0071" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0071"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a></td><td class="align-left valign-top" id="en0508">2010</td><td class="align-left valign-top" id="en0509">593 sequences</td><td class="align-left valign-top" id="en0510">123</td><td class="align-left valign-top" id="en0511">SBE plus Contempt</td></tr><tr><td class="align-left valign-top" id="en0512"><a class="anchor anchor-primary" href="#tb3fn3" name="btb3fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>MMI <a class="anchor anchor-primary" href="#bib0073" name="bbib0073" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0073"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a></td><td class="align-left valign-top" id="en0513">2010</td><td class="align-left valign-top" id="en0514">740 images, 2,900 videos</td><td class="align-left valign-top" id="en0515">25</td><td class="align-left valign-top" id="en0516">SBE</td></tr><tr><td class="align-left valign-top" id="en0517"><a class="anchor anchor-primary" href="#tb3fn5" name="btb3fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>Oulu-CASIA <a class="anchor anchor-primary" href="#bib0074" name="bbib0074" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0074"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a></td><td class="align-left valign-top" id="en0518">2011</td><td class="align-left valign-top" id="en0519">6 kinds of 480 sequences</td><td class="align-left valign-top" id="en0520">80</td><td class="align-left valign-top" id="en0521">SBE</td></tr><tr><td class="align-left valign-top" id="en0522"><a class="anchor anchor-primary" href="#tb3fn6" name="btb3fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>BU-3DFE <a class="anchor anchor-primary" href="#bib0075" name="bbib0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0075"><span class="anchor-text-container"><span class="anchor-text">[75]</span></span></a></td><td class="align-left valign-top" id="en0523">2006</td><td class="align-left valign-top" id="en0524">2,500 3D images</td><td class="align-left valign-top" id="en0525">100</td><td class="align-left valign-top" id="en0526">SBE</td></tr><tr><td class="align-left valign-top" id="en0527"><a class="anchor anchor-primary" href="#tb3fn6" name="btb3fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>BU-4DFE <a class="anchor anchor-primary" href="#bib0076" name="bbib0076" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0076"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></td><td class="align-left valign-top" id="en0528">2008</td><td class="align-left valign-top" id="en0529">606 3D sequences</td><td class="align-left valign-top" id="en0530">101</td><td class="align-left valign-top" id="en0531">SBE</td></tr><tr><td class="align-left valign-top" id="en0532"><a class="anchor anchor-primary" href="#tb3fn6" name="btb3fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>BP4D <a class="anchor anchor-primary" href="#bib0077" name="bbib0077" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0077"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a></td><td class="align-left valign-top" id="en0533">2014</td><td class="align-left valign-top" id="en0534">328 3D&nbsp;+&nbsp;2D sequences</td><td class="align-left valign-top" id="en0535">41</td><td class="align-left valign-top" id="en0536">Six basic expressions plus Embarrassed</td></tr><tr><td class="align-left valign-top" id="en0537"><a class="anchor anchor-primary" href="#tb3fn7" name="btb3fn7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn7"><span class="anchor-text-container"><span class="anchor-text"><sup>7</sup></span></span></a>4DFAB <a class="anchor anchor-primary" href="#bib0078" name="bbib0078" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0078"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a></td><td class="align-left valign-top" id="en0538">2018</td><td class="align-left valign-top" id="en0539">1.8 million+3D images</td><td class="align-left valign-top" id="en0540">180</td><td class="align-left valign-top" id="en0541">Six basic expressions</td></tr><tr><td class="align-left valign-top" id="en0542"><a class="anchor anchor-primary" href="#tb3fn8" name="btb3fn8" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn8"><span class="anchor-text-container"><span class="anchor-text"><sup>8</sup></span></span></a>SMIC <a class="anchor anchor-primary" href="#bib0079" name="bbib0079" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0079"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a></td><td class="align-left valign-top" id="en0543">2013</td><td class="align-left valign-top" id="en0544">164 sequences</td><td class="align-left valign-top" id="en0545">16</td><td class="align-left valign-top" id="en0546">Positive, Negative, Surprise</td></tr><tr><td class="align-left valign-top" id="en0547"><a class="anchor anchor-primary" href="#tb3fn9" name="btb3fn9" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn9"><span class="anchor-text-container"><span class="anchor-text"><sup>9</sup></span></span></a>CASME II <a class="anchor anchor-primary" href="#bib0080" name="bbib0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0080"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a></td><td class="align-left valign-top" id="en0548">2014</td><td class="align-left valign-top" id="en0549">255 sequences</td><td class="align-left valign-top" id="en0550">35</td><td class="align-left valign-top" id="en0551">Six basic expressions plus Others</td></tr><tr><td class="align-left valign-top" id="en0552">SAMM <a class="anchor anchor-primary" href="#bib0081" name="bbib0081" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0081"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a></td><td class="align-left valign-top" id="en0553">2018</td><td class="align-left valign-top" id="en0554">159 sequences</td><td class="align-left valign-top" id="en0555">32</td><td class="align-left valign-top" id="en0556">Six basic expressions plus Others</td></tr><tr><td class="align-left valign-top" id="en0557" colspan="5"><strong>In-the-Wild</strong></td></tr><tr><td class="align-left valign-top" id="en0562"><a class="anchor anchor-primary" href="#tb3fn10" name="btb3fn10" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn10"><span class="anchor-text-container"><span class="anchor-text"><sup>10</sup></span></span></a>FER2013 <a class="anchor anchor-primary" href="#bib0082" name="bbib0082" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0082"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a></td><td class="align-left valign-top" id="en0563">2013</td><td class="align-left valign-top" id="en0564">35,887 gray images</td><td class="align-left valign-top" id="en0565">/</td><td class="align-left valign-top" id="en0566">SBE</td></tr><tr><td class="align-left valign-top" id="en0567"><a class="anchor anchor-primary" href="#tb3fn11" name="btb3fn11" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn11"><span class="anchor-text-container"><span class="anchor-text"><sup>11</sup></span></span></a>SFEW 2.0 <a class="anchor anchor-primary" href="#bib0083" name="bbib0083" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0083"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a></td><td class="align-left valign-top" id="en0568">2015</td><td class="align-left valign-top" id="en0569">1694 images</td><td class="align-left valign-top" id="en0570"></td><td class="align-left valign-top" id="en0571">Six basic expressions</td></tr><tr><td class="align-left valign-top" id="en0572"><a class="anchor anchor-primary" href="#tb3fn12" name="btb3fn12" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn12"><span class="anchor-text-container"><span class="anchor-text"><sup>12</sup></span></span></a>EmotioNet <a class="anchor anchor-primary" href="#bib0084" name="bbib0084" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0084"><span class="anchor-text-container"><span class="anchor-text">[84]</span></span></a></td><td class="align-left valign-top" id="en0573">2016</td><td class="align-left valign-top" id="en0574">1,000,000 images</td><td class="align-left valign-top" id="en0575">/</td><td class="align-left valign-top" id="en0576">Compound expressions</td></tr><tr><td class="align-left valign-top" id="en0577"><a class="anchor anchor-primary" href="#tb3fn13" name="btb3fn13" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn13"><span class="anchor-text-container"><span class="anchor-text"><sup>13</sup></span></span></a>ExpW <a class="anchor anchor-primary" href="#bib0085" name="bbib0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0085"><span class="anchor-text-container"><span class="anchor-text">[85]</span></span></a></td><td class="align-left valign-top" id="en0578">2016</td><td class="align-left valign-top" id="en0579">91,793 images</td><td class="align-left valign-top" id="en0580">/</td><td class="align-left valign-top" id="en0581">SBE</td></tr><tr><td class="align-left valign-top" id="en0582"><a class="anchor anchor-primary" href="#tb3fn14" name="btb3fn14" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn14"><span class="anchor-text-container"><span class="anchor-text"><sup>14</sup></span></span></a>AffectNet <a class="anchor anchor-primary" href="#bib0086" name="bbib0086" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0086"><span class="anchor-text-container"><span class="anchor-text">[86]</span></span></a></td><td class="align-left valign-top" id="en0583">2017</td><td class="align-left valign-top" id="en0584">450,000 images</td><td class="align-left valign-top" id="en0585">/</td><td class="align-left valign-top" id="en0586">SBE plus Contempt Valence and Arousal</td></tr><tr><td class="align-left valign-top" id="en0587"><a class="anchor anchor-primary" href="#tb3fn15" name="btb3fn15" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn15"><span class="anchor-text-container"><span class="anchor-text"><sup>15</sup></span></span></a>RAF-DB <a class="anchor anchor-primary" href="#bib0087" name="bbib0087" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0087"><span class="anchor-text-container"><span class="anchor-text">[87]</span></span></a></td><td class="align-left valign-top" id="en0588">2017</td><td class="align-left valign-top" id="en0589">29,672 images</td><td class="align-left valign-top" id="en0590">/</td><td class="align-left valign-top" id="en0591">SBE Compound expressions</td></tr><tr><td class="align-left valign-top" id="en0592"><a class="anchor anchor-primary" href="#tb3fn16" name="btb3fn16" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb3fn16"><span class="anchor-text-container"><span class="anchor-text"><sup>16</sup></span></span></a>DFEW <a class="anchor anchor-primary" href="#bib0088" name="bbib0088" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0088"><span class="anchor-text-container"><span class="anchor-text">[88]</span></span></a></td><td class="align-left valign-top" id="en0593">2020</td><td class="align-left valign-top" id="en0594">12059 clips</td><td class="align-left valign-top" id="en0595">/</td><td class="align-left valign-top" id="en0596">SBE</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb3fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0006">kasrl.org/jaffe;</div></dd><dt id="tb3fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0007">jeffcohn.net/Resources/;</div></dd><dt id="tb3fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0008">mmifacedb.eu/;</div></dd><dt id="tb3fn4">4</dt><dd><div class="u-margin-s-bottom" id="notep0009">socsci.ru.nl:8180/RaFD2/RaFD;</div></dd><dt id="tb3fn5">5</dt><dd><div class="u-margin-s-bottom" id="notep0010">oulu.fi/cmvs/node/41316;</div></dd><dt id="tb3fn6">6</dt><dd><div class="u-margin-s-bottom" id="notep0011">cs.binghamton.edu/∼lijun/Research/3DFE/3DFE_Analysis;</div></dd><dt id="tb3fn7">7</dt><dd><div class="u-margin-s-bottom" id="notep0012">eprints.mdx.ac.uk/24259/;</div></dd><dt id="tb3fn8">8</dt><dd><div class="u-margin-s-bottom" id="notep0013">oulu.fi/cmvs/node/41319;</div></dd><dt id="tb3fn9">9</dt><dd><div class="u-margin-s-bottom" id="notep0014">fu.psych.ac.cn/CASME/casme2-en.php;</div></dd><dt id="tb3fn10">10</dt><dd><div class="u-margin-s-bottom" id="notep0015">kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge;</div></dd><dt id="tb3fn11">11</dt><dd><div class="u-margin-s-bottom" id="notep0016">cs.anu.edu.au/few/emotiw2015;</div></dd><dt id="tb3fn12">12</dt><dd><div class="u-margin-s-bottom" id="notep0017">cbcsl.ece.ohio-state.edu/dbform_emotionet;</div></dd><dt id="tb3fn13">13</dt><dd><div class="u-margin-s-bottom" id="notep0018">mmlab.ie.cuhk.edu.hk/projects/socialrelation/;</div></dd><dt id="tb3fn14">14</dt><dd><div class="u-margin-s-bottom" id="notep0019">mohammadmahoor.com/affectnet/;</div></dd><dt id="tb3fn15">15</dt><dd><div class="u-margin-s-bottom" id="notep0020">whdeng.cn/raf/model;</div></dd><dt id="tb3fn16">16</dt><dd><div class="u-margin-s-bottom" id="notep0021">dfew-database.github.io.</div></dd></dl></div></div><div class="u-margin-s-bottom" id="para0040">There are various 3D/4D databases designed for multi-view and multi-pose FER. <strong>Binghamton University 3D Facial Expression (BU-3DFE)</strong> <a class="anchor anchor-primary" href="#bib0075" name="bbib0075" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0075"><span class="anchor-text-container"><span class="anchor-text">[75]</span></span></a> contains 606 facial expression sequences captured from 100 people with one of six facial expressions. <strong>BU-4DFE</strong> <a class="anchor anchor-primary" href="#bib0076" name="bbib0076" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0076"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a> is developed based on the dynamic 3D space, which includes 606 high-resolution 3D facial expression sequences. <strong>BP4D</strong> <a class="anchor anchor-primary" href="#bib0077" name="bbib0077" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0077"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a><span> released in 2014, is a well-annotated <a href="/topics/computer-science/3d-video" title="Learn more about 3D video from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D video</a> database consisting of spontaneous facial expressions, elicited from 41 participants (23 women, 18 men), by well-validated emotion inductions. </span><strong>4DFAB</strong> <a class="anchor anchor-primary" href="#bib0078" name="bbib0078" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0078"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a><span> released in 2018, contains at least 1,800,000 dynamic high-resolution 3D <a href="/topics/neuroscience/face" title="Learn more about faces from ScienceDirect's AI-generated Topic Pages" class="topic-link">faces</a> captured from 180 subjects in four different sessions spanning.</span></div><div class="u-margin-s-bottom" id="para0041">Similar to <strong>MMI</strong> <a class="anchor anchor-primary" href="#bib0073" name="bbib0073" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0073"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a>, all micro-expression databases consist of onset-apex-offset sequences. <strong>Spontaneous Micro-expression (SMIC)</strong> <a class="anchor anchor-primary" href="#bib0079" name="bbib0079" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0079"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a> contains 164 micro-expression video clips elicited from 16 participants.&nbsp;<strong>CASME II</strong> <a class="anchor anchor-primary" href="#bib0080" name="bbib0080" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0080"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a> has videos with relatively high temporal and spatial resolution. The participants’ facial expressions have been elicited in a well-controlled laboratory environment and proper illumination. <strong>Spontaneous Micro-Facial Movement (SAMM)</strong> <a class="anchor anchor-primary" href="#bib0081" name="bbib0081" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0081"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a> is a currently-public database that contains sequences with the highest resolution, and its participants are from diverse ethnicities and the widest range of ages.</div><div class="u-margin-s-bottom" id="para0042">Acted facial expression databases are often constructed in a specific environment. Another way to build the databases is collecting facial expression images/videos from the Internet, which we refer to as In-the-Wild. <strong>FER2013</strong> <a class="anchor anchor-primary" href="#bib0082" name="bbib0082" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0082"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a> is a firstly public large-scale and unconstrained database that contains 35,887 grey images with 48&nbsp;×&nbsp;48 pixels, collected automatically through the Google image search API. <strong>Static Facial Expressions In-the-Wild (SFEW 2.0)</strong> <a class="anchor anchor-primary" href="#bib0083" name="bbib0083" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0083"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a> is divided into three sets, including Train (891 images) and Val (431 images), labelled as one of six basic expressions (anger, disgust, fear, happiness, sadness and surprise), as well as the neutral and Test (372 images) without expression labels. <strong>EmotioNet</strong> <a class="anchor anchor-primary" href="#bib0084" name="bbib0084" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0084"><span class="anchor-text-container"><span class="anchor-text">[84]</span></span></a> consists of one million images with 950,000 automatically annotated AUs and 25,000 manually annotated AUs. <strong>Expression in-the-Wild (ExpW)</strong> <a class="anchor anchor-primary" href="#bib0085" name="bbib0085" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0085"><span class="anchor-text-container"><span class="anchor-text">[85]</span></span></a><span> contains 91,793 <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a>, manually annotated as one of seven basic facial expressions. Non-face images were removed in the annotation process. </span><strong>AffectNet</strong> <a class="anchor anchor-primary" href="#bib0086" name="bbib0086" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0086"><span class="anchor-text-container"><span class="anchor-text">[86]</span></span></a> contains over 1,000,000 facial images, of which 450,000 images are manually annotated as one of eight discrete expressions (six basic expressions plus neutral and contempt), and the dimensional intensity of valence and arousal. <strong>Real-world Affective Face Database (RAF-DB)</strong> <a class="anchor anchor-primary" href="#bib0087" name="bbib0087" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0087"><span class="anchor-text-container"><span class="anchor-text">[87]</span></span></a> contains 29,672 highly diverse facial images downloaded from the Internet, with manually crowd-sourced annotations (seven basic and eleven compound emotion labels). <strong>Dynamic Facial Expression in the Wild (DFEW)</strong> <a class="anchor anchor-primary" href="#bib0088" name="bbib0088" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0088"><span class="anchor-text-container"><span class="anchor-text">[88]</span></span></a> consists of over 16,000 video clips segmented from thousands of movies with various themes. Professional crowdsourcing is applied to these clips, and 12,059 clips have been selected and labelled with one of 7 expressions (six basic expressions plus neutral).</div></section><section id="sec0014"><h4 id="cesectitle0017" class="u-margin-m-top u-margin-xs-bottom">4.3.2. Body gesture emotion databases</h4><div class="u-margin-s-bottom"><div id="para0043"><span>Although studies in emotion recognition focused mainly on facial expressions, a growing number of researchers in affective <a href="/topics/psychology/neuroscience" title="Learn more about neuroscience from ScienceDirect's AI-generated Topic Pages" class="topic-link">neuroscience</a> demonstrates the importance of the full body for unconscious emotion recognition. In general, bodily expressive cues are easier to be perceived than subtle changes in the face.&nbsp;To capture natural body movements, body gesture emotion databases contain a corpus of video sequences collected from either real life or movies. </span><a class="anchor anchor-primary" href="#tbl0004" name="btbl0004" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0004"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;4</span></span></a> provides an overview of body gesture emotion databases, as described next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0004"><span class="captions text-s"><span id="cap0010"><p id="spara011"><span class="label">Table 4</span>. Overview of body gesture emotion databases.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0597">Database</th><th scope="col" class="align-left valign-top" id="en0598">Year</th><th scope="col" class="align-left valign-top" id="en0599">Modality</th><th scope="col" class="align-left valign-top" id="en0600">Sources</th><th scope="col" class="align-left valign-top" id="en0601">Sub.</th><th scope="col" class="align-left valign-top" id="en0602">Category</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0603">EmoTV <a class="anchor anchor-primary" href="#bib0089" name="bbib0089" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0089"><span class="anchor-text-container"><span class="anchor-text">[89]</span></span></a></td><td class="align-left valign-top" id="en0604">2005</td><td class="align-left valign-top" id="en0605">Face, Body Gaze</td><td class="align-left valign-top" id="en0606">51 images</td><td class="align-left valign-top" id="en0607">/</td><td class="align-left valign-top" id="en0608">14 emotions</td></tr><tr><td class="align-left valign-top" id="en0609"><a class="anchor anchor-primary" href="#tb4fn1" name="btb4fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb4fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>FABO <a class="anchor anchor-primary" href="#bib0090" name="bbib0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0090"><span class="anchor-text-container"><span class="anchor-text">[90]</span></span></a></td><td class="align-left valign-top" id="en0610">2006</td><td class="align-left valign-top" id="en0611">Face, Body</td><td class="align-left valign-top" id="en0612">1900 videos</td><td class="align-left valign-top" id="en0613">23</td><td class="align-left valign-top" id="en0614">10 emotions</td></tr><tr><td class="align-left valign-top" id="en0615">THEATRE</td><td class="align-left valign-top" id="en0616">2009</td><td class="align-left valign-top" id="en0617">Body</td><td class="align-left valign-top" id="en0618">/</td><td class="align-left valign-top" id="en0619">/</td><td class="align-left valign-top" id="en0620">8 emotions</td></tr><tr><td class="align-left valign-top" id="en0621"><a class="anchor anchor-primary" href="#tb4fn2" name="btb4fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb4fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>GEMEP <a class="anchor anchor-primary" href="#bib0093" name="bbib0093" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0093"><span class="anchor-text-container"><span class="anchor-text">[93]</span></span></a></td><td class="align-left valign-top" id="en0622">2010</td><td class="align-left valign-top" id="en0623">Face, Body</td><td class="align-left valign-top" id="en0624">7000+</td><td class="align-left valign-top" id="en0625">10</td><td class="align-left valign-top" id="en0626">18 emotions</td></tr><tr><td class="align-left valign-top" id="en0627">EMILYA <a class="anchor anchor-primary" href="#bib0095" name="bbib0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0095"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a></td><td class="align-left valign-top" id="en0628">2014</td><td class="align-left valign-top" id="en0629">Face, Body</td><td class="align-left valign-top" id="en0630">/</td><td class="align-left valign-top" id="en0631">11</td><td class="align-left valign-top" id="en0632">8 emotions</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb4fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0022">cl.cam.ac.uk/∼hg410/fabo/;</div></dd><dt id="tb4fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0023">affective-sciences.org/gemep.</div></dd></dl></div></div><div class="u-margin-s-bottom" id="para0044"><strong>EmoTV</strong> <a class="anchor anchor-primary" href="#bib0089" name="bbib0089" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0089"><span class="anchor-text-container"><span class="anchor-text">[89]</span></span></a> contains the interview video sequences from French TV channels. It has multiple types of annotations but is not publicly available. To our best knowledge, <strong>FAce and BOdy database (FABO)</strong> <a class="anchor anchor-primary" href="#bib0090" name="bbib0090" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0090"><span class="anchor-text-container"><span class="anchor-text">[90]</span></span></a> is the first publicly available bimodal database containing both face and body gesture. The recordings for each subject take more than one-hour store rich information of various affect statements. Moreover, <strong>THEATER Corpus</strong> <a class="anchor anchor-primary" href="#bib0091" name="bbib0091" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0091"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a><span> consists of sections from two movie versions which are coded with eight <a href="/topics/psychology/emotion" title="Learn more about affective states from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective states</a> corresponding to the eight corners of PAD space </span><a class="anchor anchor-primary" href="#bib0092" name="bbib0092" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0092"><span class="anchor-text-container"><span class="anchor-text">[92]</span></span></a>. <strong>GEneva Multimodal Emotion Portrayals (GEMEP)</strong> <a class="anchor anchor-primary" href="#bib0093" name="bbib0093" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0093"><span class="anchor-text-container"><span class="anchor-text">[93]</span></span></a> is a database of body postures and gestures collected from the perspectives of both an interlocutor and an observer. The GEMEP database is one of the few databases that have frame-by-frame AU labels <a class="anchor anchor-primary" href="#bib0094" name="bbib0094" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0094"><span class="anchor-text-container"><span class="anchor-text">[94]</span></span></a>. <strong>Emotional body expression in daily actions database (EMILYA)</strong> <a class="anchor anchor-primary" href="#bib0095" name="bbib0095" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0095"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a> collected body gestures in daily motions. Participants were trained to be aware of using their bodies to express emotions through actions. <strong>EMILYA</strong> includes not only videos of facial and bodily emotional expressions, but also 3D data of the whole-body movement.</div></section></section><section id="sec0015"><h3 id="cesectitle0018" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.4. Physiological databases</h3><div class="u-margin-s-bottom"><div id="para0045">Physiological signals ((e.g., EEG, RESP, and ECG) are not affected by social masking compared to textual, audio, and visual emotion signals, and thus are more objective and reliable for emotion recognition. <a class="anchor anchor-primary" href="#tbl0005" name="btbl0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0005"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;5</span></span></a> provides an overview of physiological-based databases, as described next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0005"><span class="captions text-s"><span id="cap0011"><p id="spara012"><span class="label">Table 5</span>. Overview of physiological-based databases.</p></span></span><div class="groups"><table><thead><tr><th scope="col" class="align-left valign-top rowsep-1" id="en0633" rowspan="2">Database</th><th scope="col" class="align-left valign-top rowsep-1" id="en0634" rowspan="2">Year</th><th scope="col" class="align-left valign-top" id="en0635" colspan="7">Components</th><th scope="col" class="align-left valign-top rowsep-1" id="en0636" rowspan="2">Subjects</th><th scope="col" class="align-left valign-top rowsep-1" id="en0637" rowspan="2">Emotion Categories</th></tr><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0640">EEG</th><th scope="col" class="align-left valign-top" id="en0641">EOG</th><th scope="col" class="align-left valign-top" id="en0642">EMG</th><th scope="col" class="align-left valign-top" id="en0643">GSR</th><th scope="col" class="align-left valign-top" id="en0644">RESP</th><th scope="col" class="align-left valign-top" id="en0645">ECG</th><th scope="col" class="align-left valign-top" id="en0646">Others</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0649"><a class="anchor anchor-primary" href="#tb5fn1" name="btb5fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb5fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>DEAP <a class="anchor anchor-primary" href="#bib0096" name="bbib0096" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0096"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a></td><td class="align-left valign-top" id="en0650">2012</td><td class="align-left valign-top" id="en0651">■</td><td class="align-left valign-top" id="en0652">■</td><td class="align-left valign-top" id="en0653">■</td><td class="align-left valign-top" id="en0654">■</td><td class="align-left valign-top" id="en0655">■</td><td class="align-left valign-top" id="en0656">/</td><td class="align-left valign-top" id="en0657">Plethysmograph</td><td class="align-left valign-top" id="en0658">32</td><td class="align-left valign-top" id="en0659">Valence and Arousal</td></tr><tr><td class="align-left valign-top" id="en0660"><a class="anchor anchor-primary" href="#tb5fn2" name="btb5fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb5fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>SEED [<a class="anchor anchor-primary" href="#bib0097" name="bbib0097" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0097"><span class="anchor-text-container"><span class="anchor-text">97</span></span></a>,<a class="anchor anchor-primary" href="#bib0098" name="bbib0098" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0098"><span class="anchor-text-container"><span class="anchor-text">98</span></span></a>]</td><td class="align-left valign-top" id="en0661">2015</td><td class="align-left valign-top" id="en0662">■</td><td class="align-left valign-top" id="en0663">/</td><td class="align-left valign-top" id="en0664">/</td><td class="align-left valign-top" id="en0665">/</td><td class="align-left valign-top" id="en0666">/</td><td class="align-left valign-top" id="en0667">/</td><td class="align-left valign-top" id="en0668">/</td><td class="align-left valign-top" id="en0669">15</td><td class="align-left valign-top" id="en0670">Positive, Neutral, and Negative</td></tr><tr><td class="align-left valign-top" id="en0671">DSdRD <a class="anchor anchor-primary" href="#bib0007" name="bbib0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0007"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></td><td class="align-left valign-top" id="en0672">2005</td><td class="align-left valign-top" id="en0673">/</td><td class="align-left valign-top" id="en0674">/</td><td class="align-left valign-top" id="en0675">■</td><td class="align-left valign-top" id="en0676">■</td><td class="align-left valign-top" id="en0677">/</td><td class="align-left valign-top" id="en0678">■</td><td class="align-left valign-top" id="en0679">/</td><td class="align-left valign-top" id="en0680">24</td><td class="align-left valign-top" id="en0681">Low, medium, and high stress</td></tr><tr><td class="align-left valign-top" id="en0682"><a class="anchor anchor-primary" href="#tb5fn3" name="btb5fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb5fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>AMIGOS <a class="anchor anchor-primary" href="#bib0099" name="bbib0099" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0099"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></td><td class="align-left valign-top" id="en0683">2017</td><td class="align-left valign-top" id="en0684">■</td><td class="align-left valign-top" id="en0685">/</td><td class="align-left valign-top" id="en0686">/</td><td class="align-left valign-top" id="en0687">■</td><td class="align-left valign-top" id="en0688">/</td><td class="align-left valign-top" id="en0689">■</td><td class="align-left valign-top" id="en0690">Frontal HD video, both RGB &amp; depth full body videos</td><td class="align-left valign-top" id="en0691">40</td><td class="align-left valign-top" id="en0692">Valence and Arousal</td></tr><tr><td class="align-left valign-top" id="en0693">WESAD <a class="anchor anchor-primary" href="#bib0100" name="bbib0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0100"><span class="anchor-text-container"><span class="anchor-text">[100]</span></span></a></td><td class="align-left valign-top" id="en0694">2018</td><td class="align-left valign-top" id="en0695">/</td><td class="align-left valign-top" id="en0696">/</td><td class="align-left valign-top" id="en0697">■</td><td class="align-left valign-top" id="en0698">/</td><td class="align-left valign-top" id="en0699">■</td><td class="align-left valign-top" id="en0700">■</td><td class="align-left valign-top" id="en0701">EDA, blood volume pulse &amp; temperature, AAC</td><td class="align-left valign-top" id="en0702">15</td><td class="align-left valign-top" id="en0703">Neutral, Stress, Amusement</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb5fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0024">eecs.qmul.ac.uk/mmv/databases/deap/;</div></dd><dt id="tb5fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0025">bcmi.sjtu.edu.cn/∼seed/;</div></dd><dt id="tb5fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0026">eecs.qmul.ac.uk/mmv/databases/amigos/.</div></dd></dl></div></div><div class="u-margin-s-bottom" id="para0046"><strong>DEAP</strong> <a class="anchor anchor-primary" href="#bib0096" name="bbib0096" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0096"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a><span><span> comprises a 32-channel EEG, a 4-channel EOG, a 4-channel EMG, RESP, <a href="/topics/medicine-and-dentistry/plethysmograph" title="Learn more about plethysmograph from ScienceDirect's AI-generated Topic Pages" class="topic-link">plethysmograph</a>, </span><a href="/topics/neuroscience/electrodermal-response" title="Learn more about Galvanic Skin Response from ScienceDirect's AI-generated Topic Pages" class="topic-link">Galvanic Skin Response</a> (GSR) and body temperature, collected from 32 subjects. Immediately after watching each video, subjects were required to rate their truly-felt emotion from five dimensions: valence, arousal, dominance, liking and familiarity. </span><strong>SEED</strong> [<a class="anchor anchor-primary" href="#bib0097" name="bbib0097" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0097"><span class="anchor-text-container"><span class="anchor-text">97</span></span></a>,<a class="anchor anchor-primary" href="#bib0098" name="bbib0098" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0098"><span class="anchor-text-container"><span class="anchor-text">98</span></span></a>] contains EGG recordings from 15 subjects. In their study, participants were asked to experience three EEG recording sessions, with an interval of two weeks between two successive recording sessions. Within each session, each subject was exposed to the same sequence of fifteen movie excerpts, each one approximately four-minute-long, to induce three kinds of emotions: positive, neutral, and negative.</div><div class="u-margin-s-bottom" id="para0047">Some databases are task-driven. For example, <strong>Detecting Stress during Real-World Driving Tasks (DSdRD)</strong> <a class="anchor anchor-primary" href="#bib0007" name="bbib0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0007"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a> is used to determine the relative stress levels of drivers. It contains various signals from 24 volunteers while having a rest for at least 50 minutes after their driving tasks. These volunteers are asked to fill out questionnaires which are used to map their state into low, medium, and high-stress levels. <strong>AMIGOS</strong> <a class="anchor anchor-primary" href="#bib0099" name="bbib0099" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0099"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a><span> was designed to collect participants’ emotions in two social contexts: individual and group. AMIGOS was constructed in 2 experimental settings: 1) 40 participants watch 16 short emotional videos; 2) they watch 4 long videos, including a mix of lone and group sessions. These emotions were annotated with self-assessment of affective levels and external assessment of valence and arousal. <a href="/topics/neuroscience/wearable-sensor" title="Learn more about Wearable devices from ScienceDirect's AI-generated Topic Pages" class="topic-link">Wearable devices</a> help to bridge the gap between lab studies and real-life emotions. </span><strong>Wearable Stress and Affect Detection (WESAD)</strong> <a class="anchor anchor-primary" href="#bib0100" name="bbib0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0100"><span class="anchor-text-container"><span class="anchor-text">[100]</span></span></a><span> is built for stress detection, providing multimodal, high-quality data, including three different <a href="/topics/neuroscience/emotion" title="Learn more about affective states from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective states</a> (neutral, stress, amusement).</span></div></section><section id="sec0016"><h3 id="cesectitle0019" class="u-h4 u-margin-m-top u-margin-xs-bottom">4.5. Multimodal databases</h3><div class="u-margin-s-bottom"><div id="para0048">In our daily life, people express and/or understand emotions through multimodal signals. Multimodal databases can be mainly divided into two types: multi-physical and physical-physiological databases. <a class="anchor anchor-primary" href="#tbl0006" name="btbl0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0006"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;6</span></span></a> provides an overview of multimodal databases, as described next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0006"><span class="captions text-s"><span id="cap0012"><p id="spara013"><span class="label">Table 6</span>. Overview of multimodal databases. Five basic sentiments&nbsp;=&nbsp;Strongly Positive, Weakly Positive, Neutral, Strongly Negative and Weakly Negative.</p></span></span><div class="groups"><table><thead><tr><th scope="col" class="align-left valign-top rowsep-1" id="en0704" rowspan="2">Name</th><th scope="col" class="align-left valign-top rowsep-1" id="en0705" rowspan="2">Year</th><th scope="col" class="align-left valign-top" id="en0706" colspan="4">Components</th><th scope="col" class="align-left valign-top rowsep-1" id="en0707" rowspan="2">Subjects</th><th scope="col" class="align-left valign-top rowsep-1" id="en0708" rowspan="2">Type</th><th scope="col" class="align-left valign-top rowsep-1" id="en0709" rowspan="2">Emotion Categories</th></tr><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0712">Text</th><th scope="col" class="align-left valign-top" id="en0713">Speech</th><th scope="col" class="align-left valign-top" id="en0714">Visual</th><th scope="col" class="align-left valign-top" id="en0715">Psych.</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0719"><a class="anchor anchor-primary" href="#tb6fn1" name="btb6fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>IEMOCAP <a class="anchor anchor-primary" href="#bib0101" name="bbib0101" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0101"><span class="anchor-text-container"><span class="anchor-text">[101]</span></span></a></td><td class="align-left valign-top" id="en0720">2008</td><td class="align-left valign-top" id="en0721">■</td><td class="align-left valign-top" id="en0722">■</td><td class="align-left valign-top" id="en0723">■</td><td class="align-left valign-top" id="en0724">/</td><td class="align-left valign-top" id="en0725">10</td><td class="align-left valign-top" id="en0726">Acted</td><td class="align-left valign-top" id="en0727">Happiness, Anger, Sad, Frustration and Neutral Activation-Valence-Dominance</td></tr><tr><td class="align-left valign-top" id="en0728"><a class="anchor anchor-primary" href="#tb6fn2" name="btb6fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>CreativeIT [<a class="anchor anchor-primary" href="#bib0102" name="bbib0102" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0102"><span class="anchor-text-container"><span class="anchor-text">102</span></span></a>,<a class="anchor anchor-primary" href="#bib0103" name="bbib0103" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0103"><span class="anchor-text-container"><span class="anchor-text">103</span></span></a>]</td><td class="align-left valign-top" id="en0729">2010</td><td class="align-left valign-top" id="en0730">■</td><td class="align-left valign-top" id="en0731">■</td><td class="align-left valign-top" id="en0732">■</td><td class="align-left valign-top" id="en0733">/</td><td class="align-left valign-top" id="en0734">16</td><td class="align-left valign-top" id="en0735">Induced</td><td class="align-left valign-top" id="en0736">Activation-Valence-Dominance</td></tr><tr><td class="align-left valign-top" id="en0737"><a class="anchor anchor-primary" href="#tb6fn3" name="btb6fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>HOW <a class="anchor anchor-primary" href="#bib0104" name="bbib0104" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0104"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a></td><td class="align-left valign-top" id="en0738">2011</td><td class="align-left valign-top" id="en0739">■</td><td class="align-left valign-top" id="en0740">■</td><td class="align-left valign-top" id="en0741">■</td><td class="align-left valign-top" id="en0742">/</td><td class="align-left valign-top" id="en0743">/</td><td class="align-left valign-top" id="en0744">Natural</td><td class="align-left valign-top" id="en0745">Positive, Negative and Neutral</td></tr><tr><td class="align-left valign-top" id="en0746">ICT-MMMO <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a></td><td class="align-left valign-top" id="en0747">2013</td><td class="align-left valign-top" id="en0748">■</td><td class="align-left valign-top" id="en0749">■</td><td class="align-left valign-top" id="en0750">■</td><td class="align-left valign-top" id="en0751">/</td><td class="align-left valign-top" id="en0752">/</td><td class="align-left valign-top" id="en0753">Natural</td><td class="align-left valign-top" id="en0754">Five basic sentiments</td></tr><tr><td class="align-left valign-top" id="en0755"><a class="anchor anchor-primary" href="#tb6fn4" name="btb6fn4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn4"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a>CMU-MOSEI <a class="anchor anchor-primary" href="#bib0106" name="bbib0106" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0106"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></td><td class="align-left valign-top" id="en0756">2018</td><td class="align-left valign-top" id="en0757">■</td><td class="align-left valign-top" id="en0758">■</td><td class="align-left valign-top" id="en0759">■</td><td class="align-left valign-top" id="en0760">/</td><td class="align-left valign-top" id="en0761">/</td><td class="align-left valign-top" id="en0762">Natural</td><td class="align-left valign-top" id="en0763">Six basic emotions, Five basic sentiments</td></tr><tr><td class="align-left valign-top" id="en0764"><a class="anchor anchor-primary" href="#tb6fn5" name="btb6fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>MAHNOB-HCI <a class="anchor anchor-primary" href="#bib0107" name="bbib0107" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0107"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a></td><td class="align-left valign-top" id="en0765">2012</td><td class="align-left valign-top" id="en0766">/</td><td class="align-left valign-top" id="en0767">■</td><td class="align-left valign-top" id="en0768">■</td><td class="align-left valign-top" id="en0769">■</td><td class="align-left valign-top" id="en0770">27</td><td class="align-left valign-top" id="en0771">Induced</td><td class="align-left valign-top" id="en0772">Arousal-Valence-Dominance-Predictability Disgust, Amusement, Joy, Fear, Sadness, Neutral</td></tr><tr><td class="align-left valign-top" id="en0773"><a class="anchor anchor-primary" href="#tb6fn6" name="btb6fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>RECOLA <a class="anchor anchor-primary" href="#bib0108" name="bbib0108" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0108"><span class="anchor-text-container"><span class="anchor-text">[108]</span></span></a></td><td class="align-left valign-top" id="en0774">2013</td><td class="align-left valign-top" id="en0775">/</td><td class="align-left valign-top" id="en0776">■</td><td class="align-left valign-top" id="en0777">■</td><td class="align-left valign-top" id="en0778">■</td><td class="align-left valign-top" id="en0779">46</td><td class="align-left valign-top" id="en0780">Natural</td><td class="align-left valign-top" id="en0781">Arousal-Valence, Agreement, Dominance, Engagement, Performance and Rapport</td></tr><tr><td class="align-left valign-top" id="en0782"><a class="anchor anchor-primary" href="#tb6fn7" name="btb6fn7" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb6fn7"><span class="anchor-text-container"><span class="anchor-text"><sup>7</sup></span></span></a>DECAF <a class="anchor anchor-primary" href="#bib0109" name="bbib0109" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0109"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a></td><td class="align-left valign-top" id="en0783">2015</td><td class="align-left valign-top" id="en0784">/</td><td class="align-left valign-top" id="en0785">■</td><td class="align-left valign-top" id="en0786">■</td><td class="align-left valign-top" id="en0787">■</td><td class="align-left valign-top" id="en0788">30</td><td class="align-left valign-top" id="en0789">Induced</td><td class="align-left valign-top" id="en0790">Arousal-Valence-Dominance, Six basic expressions Amusing, Funny and Exciting</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb6fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0027">sail.usc.edu/iemocap/iemocap_release.htm;</div></dd><dt id="tb6fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0028">sail.usc.edu/CreativeIT/ImprovRelease.htm;</div></dd><dt id="tb6fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0029">ict.usc.edu/research/;</div></dd><dt id="tb6fn4">4</dt><dd><div class="u-margin-s-bottom" id="notep0030">github.com/A2Zadeh/CMU-MultimodalSDK;</div></dd><dt id="tb6fn5">5</dt><dd><div class="u-margin-s-bottom" id="notep0031">mahnob-db.eu/hci-tagging/;</div></dd><dt id="tb6fn6">6</dt><dd><div class="u-margin-s-bottom" id="notep0032">diuf.unifr.ch/diva/recola;</div></dd><dt id="tb6fn7">7</dt><dd><div class="u-margin-s-bottom" id="notep0033">mhug.disi.unitn.it/wp-content/DECAF/DECAF.</div></dd></dl></div></div><div class="u-margin-s-bottom" id="para0049"><strong>Interactive Emotional Dyadic Motion Capture (IEMOCAP)</strong> <a class="anchor anchor-primary" href="#bib0101" name="bbib0101" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0101"><span class="anchor-text-container"><span class="anchor-text">[101]</span></span></a> is constructed by the Speech Analysis and Interpretation Laboratory. During recording, 10 actors are asked to perform selected emotional scripts and improvised hypothetical scenarios designed to elicit 5 specific types of emotions. The face, head, and hands of actors are marked to provide detailed information about their facial expressions and hand movements while performing. Two famous emotion taxonomies are employed to label the utterance level: discrete categorical-based annotations and continuous attribute-based annotations. Afterwards, <strong>CreativeIT</strong> [<a class="anchor anchor-primary" href="#bib0102" name="bbib0102" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0102"><span class="anchor-text-container"><span class="anchor-text">102</span></span></a>,<a class="anchor anchor-primary" href="#bib0103" name="bbib0103" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0103"><span class="anchor-text-container"><span class="anchor-text">103</span></span></a><span><span>] contains detailed full-body motion visual-audio and text description <a href="/topics/computer-science/collected-data" title="Learn more about data collected from ScienceDirect's AI-generated Topic Pages" class="topic-link">data collected</a> from 16 actors, during their affective dyadic interactions ranging from 2-10 minutes each. Two kinds of interactions (two-sentence and paraphrases exercises) are set as improvised. According to the video frame rate, the annotator gave the values of each actor's </span><a href="/topics/computer-science/emotional-state" title="Learn more about emotional state from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotional state</a> in the three dimensions. </span><strong>Harvesting Opinions from the Web database (HOW)</strong> <a class="anchor anchor-primary" href="#bib0104" name="bbib0104" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0104"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a> contains 13 positive, 12 negative and 22 neutral videos captured from YouTube. <strong>Institute for Creative Technologies Multimodal Movie Opinion database (ICT-MMMO)</strong> <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a> contains 308 YouTube videos and 78 movie review videos from ExpoTV. It has five sentiment labels: strongly positive, weakly positive, neutral, strongly negative, and weakly negative. As far as we know, <strong>Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)</strong> <a class="anchor anchor-primary" href="#bib0106" name="bbib0106" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0106"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a><span> is the largest database for sentiment analysis and emotion recognition, consisting of 23,453 sentences and 3,228 videos collected from more than 1,000 online YouTube speakers. Each video contains a manual transcription that aligns audio and <a href="/topics/physics-and-astronomy/phoneme" title="Learn more about phoneme from ScienceDirect's AI-generated Topic Pages" class="topic-link">phoneme</a> grades.</span></div><div class="u-margin-s-bottom" id="para0050"><strong>MAHNOB-HCI</strong> <a class="anchor anchor-primary" href="#bib0107" name="bbib0107" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0107"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a><span> is a video-physiological database. Using 6 video cameras, a head-worn microphone, an eye gaze tracker, and <a href="/topics/computer-science/physiological-sensor" title="Learn more about physiological sensors from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological sensors</a>, it is constructed by monitoring and recording the emotions of 27 participants while watching 20 films. </span><strong>Remote Collaborative and Affective Interactions (RECOLA)</strong> <a class="anchor anchor-primary" href="#bib0108" name="bbib0108" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0108"><span class="anchor-text-container"><span class="anchor-text">[108]</span></span></a> consists of a multimodal corpus of spontaneous interactions from 46 participants (in French). These participants work in pairs to discuss a disaster scenario escape plan and reach an agreement via remote video conferencing. The recordings of the participants’ activities are annotated by 6 annotators with two continuous emotional dimensions: arousal and valence, as well as social behaviour labels on five dimensions. <strong>DECAF</strong> <a class="anchor anchor-primary" href="#bib0109" name="bbib0109" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0109"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a><span> is a Magnetoencephalogram-based database for decoding affective responses of 30 subjects while watching 36 movie clips and 40 one-minute music video clips. DECAF contains a detailed analysis of the correlations between participants’ self-assessments and their <a href="/topics/computer-science/physiological-response" title="Learn more about physiological responses from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological responses</a><span>, single-trial <a href="/topics/computer-science/classification-result" title="Learn more about classification results from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification results</a> for valence, arousal and dominance dimensions, performance evaluation against existing data sets, and time-continuous emotion annotations for movie clips.</span></span></div></section></section><section id="sec0017"><h2 id="cesectitle0020" class="u-h4 u-margin-l-top u-margin-xs-bottom">5. Unimodal affect recognition</h2><div class="u-margin-s-bottom" id="para0051">In this section, we systematically summarize the unimodal affect recognition methods from the perspective of affect modalities: physical modalities (e.g., textual, audio, and visual) <a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a> and physiological modalities (e.g., EEG and ECG) <a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a>.</div><section id="sec0018"><h3 id="cesectitle0021" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.1. Textual sentiment analysis</h3><div class="u-margin-s-bottom" id="para0052">With the rapid increase of online social media and e-commerce platforms, where users freely express their ideas, a huge amount of textual data are generated and collected. To identify subtle sentiment or emotions expressed explicitly or implicitly from the user-generated data, textual sentiment analysis (TSA) was introduced <a class="anchor anchor-primary" href="#bib0110" name="bbib0110" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0110"><span class="anchor-text-container"><span class="anchor-text">[110]</span></span></a>. Traditional approaches of TSA [<a class="anchor anchor-primary" href="#bib0111" name="bbib0111" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0111"><span class="anchor-text-container"><span class="anchor-text">111</span></span></a>,<a class="anchor anchor-primary" href="#bib0112" name="bbib0112" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0112"><span class="anchor-text-container"><span class="anchor-text">112</span></span></a>] often rely on the process known as “feature engineering” to find useful features that are related to sentiment. This is a tedious task. DL-based models can realize an end-to-end sentiment analysis from textual data.</div><div class="u-margin-s-bottom"><div id="para0053"><a class="anchor anchor-primary" href="#tbl0007" name="btbl0007" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0007"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;7</span></span></a><span> shows an overview of some representative methods for TSA, where the publication (or preprint) year, analysis <a href="/topics/computer-science/granularity" title="Learn more about granularity from ScienceDirect's AI-generated Topic Pages" class="topic-link">granularity</a>, feature representation, classifier, database, and performance are presented. Next, the works of TSA are described in detail.</span></div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0007"><span class="captions text-s"><span id="cap0013"><p id="spara014"><span class="label">Table 7</span>. Overview of the representative methods for TSA.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0791">Pub.</th><th scope="col" class="align-left valign-top" id="en0792">Year</th><th scope="col" class="align-left valign-top" id="en0793">Granularity</th><th scope="col" class="align-left valign-top" id="en0794">Feature Representation</th><th scope="col" class="align-left valign-top" id="en0795">Classifier</th><th scope="col" class="align-left valign-top" id="en0796">Database</th><th scope="col" class="align-left valign-top" id="en0797">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0798" colspan="7"><strong><em>ML-based SER</em></strong></td></tr><tr><td class="align-left valign-top" id="en0799"><a class="anchor anchor-primary" href="#bib0117" name="bbib0117" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0117"><span class="anchor-text-container"><span class="anchor-text">[117]</span></span></a></td><td class="align-left valign-top" id="en0800">2014</td><td class="align-left valign-top" id="en0801">Concept-level</td><td class="align-left valign-top" id="en0802">Multi-feature fusion</td><td class="align-left valign-top" id="en0803">SVM-ELM</td><td class="align-left valign-top" id="en0804">Movie Review</td><td class="align-left valign-top" id="en0805">2 classes: 86.21</td></tr><tr><td class="align-left valign-top" id="en0806"><a class="anchor anchor-primary" href="#bib0120" name="bbib0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0120"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a></td><td class="align-left valign-top" id="en0807">2018</td><td class="align-left valign-top" id="en0808">Aspect-level</td><td class="align-left valign-top" id="en0809">Multilingual features</td><td class="align-left valign-top" id="en0810">Metric-based</td><td class="align-left valign-top" id="en0811">Twitter</td><td class="align-left valign-top" id="en0812">2 classes: 66.00, 60.00</td></tr><tr><td class="align-left valign-top" id="en0813"><a class="anchor anchor-primary" href="#bib0125" name="bbib0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0125"><span class="anchor-text-container"><span class="anchor-text">[125]</span></span></a></td><td class="align-left valign-top" id="en0814">2008</td><td class="align-left valign-top" id="en0815">Aspect-level</td><td class="align-left valign-top" id="en0816">CDM</td><td class="align-left valign-top" id="en0817">NB</td><td class="align-left valign-top" id="en0818">Reuters-2157</td><td class="align-left valign-top" id="en0819">10 classes: 85.62</td></tr><tr><td class="align-left valign-top" id="en0820"><a class="anchor anchor-primary" href="#bib0130" name="bbib0130" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0130"><span class="anchor-text-container"><span class="anchor-text">[130]</span></span></a></td><td class="align-left valign-top" id="en0821">2019</td><td class="align-left valign-top" id="en0822">Aspect-level</td><td class="align-left valign-top" id="en0823">Features fusion</td><td class="align-left valign-top" id="en0824">LML</td><td class="align-left valign-top" id="en0825">Amazin; IMDB</td><td class="align-left valign-top" id="en0826">2 classes: 89.70; 10 classes: 85.20</td></tr><tr><td class="align-left valign-top" id="en0827" colspan="7"><strong><em>DL-based SER</em></strong></td></tr><tr><td class="align-left valign-top" id="en0828"><a class="anchor anchor-primary" href="#bib0133" name="bbib0133" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0133"><span class="anchor-text-container"><span class="anchor-text">[133]</span></span></a></td><td class="align-left valign-top" id="en0829">2020</td><td class="align-left valign-top" id="en0830">Document-level</td><td class="align-left valign-top" id="en0831">CNN</td><td class="align-left valign-top" id="en0832">HieNN-DWE</td><td class="align-left valign-top" id="en0833">IMDB etc.</td><td class="align-left valign-top" id="en0834">10 classes: 47.80</td></tr><tr><td class="align-left valign-top" id="en0835"><a class="anchor anchor-primary" href="#bib0137" name="bbib0137" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0137"><span class="anchor-text-container"><span class="anchor-text">[137]</span></span></a></td><td class="align-left valign-top" id="en0836">2017</td><td class="align-left valign-top" id="en0837">Document-level</td><td class="align-left valign-top" id="en0838">VDCNN</td><td class="align-left valign-top" id="en0839">ReLU</td><td class="align-left valign-top" id="en0840">Amazon etc.</td><td class="align-left valign-top" id="en0841">2 classes: 95.07</td></tr><tr><td class="align-left valign-top" id="en0842"><a class="anchor anchor-primary" href="#bib0139" name="bbib0139" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0139"><span class="anchor-text-container"><span class="anchor-text">[139]</span></span></a></td><td class="align-left valign-top" id="en0843">2018</td><td class="align-left valign-top" id="en0844">Aspect-level</td><td class="align-left valign-top" id="en0845">PF-CNN</td><td class="align-left valign-top" id="en0846">Softmax</td><td class="align-left valign-top" id="en0847">Laptops; Restaurants</td><td class="align-left valign-top" id="en0848">2 classes: 86.35; 90.15</td></tr><tr><td class="align-left valign-top" id="en0849"><a class="anchor anchor-primary" href="#bib0140" name="bbib0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0140"><span class="anchor-text-container"><span class="anchor-text">[140]</span></span></a></td><td class="align-left valign-top" id="en0850">2017</td><td class="align-left valign-top" id="en0851">Document-level</td><td class="align-left valign-top" id="en0852">cBLSTM</td><td class="align-left valign-top" id="en0853">Decision Rule</td><td class="align-left valign-top" id="en0854">IMDB</td><td class="align-left valign-top" id="en0855">2 classes: 92.83</td></tr><tr><td class="align-left valign-top" id="en0856"><a class="anchor anchor-primary" href="#bib0141" name="bbib0141" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0141"><span class="anchor-text-container"><span class="anchor-text">[141]</span></span></a></td><td class="align-left valign-top" id="en0857">2016</td><td class="align-left valign-top" id="en0858">Aspect-level</td><td class="align-left valign-top" id="en0859">DT-RNN, CRF</td><td class="align-left valign-top" id="en0860">RNCRF</td><td class="align-left valign-top" id="en0861">Laptops; Restaurants</td><td class="align-left valign-top" id="en0862">2 classes: 79.44; 84.11</td></tr><tr><td class="align-left valign-top" id="en0863"><a class="anchor anchor-primary" href="#bib0146" name="bbib0146" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0146"><span class="anchor-text-container"><span class="anchor-text">[146]</span></span></a></td><td class="align-left valign-top" id="en0864">2018</td><td class="align-left valign-top" id="en0865">Document-level</td><td class="align-left valign-top" id="en0866">HUAPA</td><td class="align-left valign-top" id="en0867">Softmax</td><td class="align-left valign-top" id="en0868">IMDB etc.</td><td class="align-left valign-top" id="en0869">10 classes: 55.00</td></tr><tr><td class="align-left valign-top" id="en0870"><a class="anchor anchor-primary" href="#bib0150" name="bbib0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0150"><span class="anchor-text-container"><span class="anchor-text">[150]</span></span></a></td><td class="align-left valign-top" id="en0871">2020</td><td class="align-left valign-top" id="en0872">Document-level</td><td class="align-left valign-top" id="en0873">Word2Vec</td><td class="align-left valign-top" id="en0874">CNN-LSTM</td><td class="align-left valign-top" id="en0875">SST etc.</td><td class="align-left valign-top" id="en0876">5 classes: 50.68</td></tr><tr><td class="align-left valign-top" id="en0877"><a class="anchor anchor-primary" href="#bib0151" name="bbib0151" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0151"><span class="anchor-text-container"><span class="anchor-text">[151]</span></span></a></td><td class="align-left valign-top" id="en0878">2018</td><td class="align-left valign-top" id="en0879">Aspect-level</td><td class="align-left valign-top" id="en0880">Word Embeddings</td><td class="align-left valign-top" id="en0881">GCAE</td><td class="align-left valign-top" id="en0882">Laptops; Restaurants</td><td class="align-left valign-top" id="en0883">4 classes: 69.14; 85.92</td></tr><tr><td class="align-left valign-top" id="en0884"><a class="anchor anchor-primary" href="#bib0157" name="bbib0157" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0157"><span class="anchor-text-container"><span class="anchor-text">[157]</span></span></a></td><td class="align-left valign-top" id="en0885">2018</td><td class="align-left valign-top" id="en0886">Sentence-level</td><td class="align-left valign-top" id="en0887">CS-GAN</td><td class="align-left valign-top" id="en0888">Softmax</td><td class="align-left valign-top" id="en0889">Amazon-5000</td><td class="align-left valign-top" id="en0890">2 classes: 86.43</td></tr><tr><td class="align-left valign-top" id="en0891"><a class="anchor anchor-primary" href="#bib0159" name="bbib0159" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0159"><span class="anchor-text-container"><span class="anchor-text">[159]</span></span></a></td><td class="align-left valign-top" id="en0892">2018</td><td class="align-left valign-top" id="en0893">Aspect-level</td><td class="align-left valign-top" id="en0894">ADAN</td><td class="align-left valign-top" id="en0895">Softmax</td><td class="align-left valign-top" id="en0896">TARGET</td><td class="align-left valign-top" id="en0897">5 classes: 42.49; 3 classes: 54.54</td></tr></tbody></table></div></div></div><section id="sec0019"><h4 id="cesectitle0022" class="u-margin-m-top u-margin-xs-bottom">5.1.1. ML-based TSA</h4><div class="u-margin-s-bottom" id="para0054"><span>TSA based on the traditional ML methods mainly relies on knowledge-based techniques or <a href="/topics/physics-and-astronomy/methods-statistical" title="Learn more about statistical methods from ScienceDirect's AI-generated Topic Pages" class="topic-link">statistical methods</a> </span><a class="anchor anchor-primary" href="#bib0113" name="bbib0113" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0113"><span class="anchor-text-container"><span class="anchor-text">[113]</span></span></a>. The former requires thesaurus modelling of large emotional vocabularies, and the latter assumes the availability of large databases, labelled with polarity or emotional labels.</div><div class="u-margin-s-bottom" id="para0055"><strong>Knowledge-based TSA.</strong> Knowledge-based TSA is often based on lexicons and linguistic rules. Different lexicons, such as WordNet, WordNet-Affect, SenticNet, MPQA, and SentiWordNet <a class="anchor anchor-primary" href="#bib0114" name="bbib0114" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0114"><span class="anchor-text-container"><span class="anchor-text">[114]</span></span></a>, often contain a bag of words and their semantic polarities. The lexicon-based approaches can classify a given word into positive or negative, but perform poorly without linguistic rules. Hence, Ding et&nbsp;al. <a class="anchor anchor-primary" href="#bib0115" name="bbib0115" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0115"><span class="anchor-text-container"><span class="anchor-text">[115]</span></span></a> adopted a lexicon-based holistic approach that combines external evidence with linguistic conventions in natural language to evaluate the semantic orientation in reviews. Melville et&nbsp;al. <a class="anchor anchor-primary" href="#bib0116" name="bbib0116" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0116"><span class="anchor-text-container"><span class="anchor-text">[116]</span></span></a> developed a framework for domain-dependent sentiment analysis using lexical association information.</div><div class="u-margin-s-bottom" id="para0056">To better understand the orientation and flow of sentiment in natural language, Poria et&nbsp;al. <a class="anchor anchor-primary" href="#bib0117" name="bbib0117" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0117"><span class="anchor-text-container"><span class="anchor-text">[117]</span></span></a> proposed a new framework combining computational intelligence, linguistics and common-sense computing <a class="anchor anchor-primary" href="#bib0118" name="bbib0118" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0118"><span class="anchor-text-container"><span class="anchor-text">[118]</span></span></a>. They found that the negation played an important role in judging the polarity of the overall sentence. Jia et&nbsp;al. <a class="anchor anchor-primary" href="#bib0119" name="bbib0119" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0119"><span class="anchor-text-container"><span class="anchor-text">[119]</span></span></a> also verified the importance of negation on emotional recognition through extensive experiments. Blekanov et&nbsp;al. <a class="anchor anchor-primary" href="#bib0120" name="bbib0120" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0120"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a> utilized specifics of the Twitter platform in their multi-lingual knowledge-based approach of sentiment analysis. Due to the limitations of knowledge itself, knowledge-based models are limited to understanding only those concepts that are typical and strictly defined.</div><div class="u-margin-s-bottom" id="para0057"><strong>Statistical-based TSA.</strong><span> Statistical-based TSA relies more on an annotated dataset to train an ML-based classifier by using prior statistics or <a href="/topics/computer-science/posterior-probability" title="Learn more about posterior probability from ScienceDirect's AI-generated Topic Pages" class="topic-link">posterior probability</a>. Compared with lexicon-based approaches [</span><a class="anchor anchor-primary" href="#bib0121" name="bbib0121" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0121"><span class="anchor-text-container"><span class="anchor-text">121</span></span></a>,<a class="anchor anchor-primary" href="#bib0122" name="bbib0122" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0122"><span class="anchor-text-container"><span class="anchor-text">122</span></span></a>], statistical-based TSA approaches are more suitable for sentiment analysis due to their ability to deal with large amounts of data. Mullen and Collier <a class="anchor anchor-primary" href="#bib0123" name="bbib0123" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0123"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a> used the semantic orientation of words to create a feature space that is classified by a designed SVM. Pak et&nbsp;al. <a class="anchor anchor-primary" href="#bib0124" name="bbib0124" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0124"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a><span> proposed a new sub-graph-based model. It represents a document as a collection of sub-graphs and inputs the features from these sub-graphs into an SVM classifier. <a href="/topics/biochemistry-genetics-and-molecular-biology/bayesian-learning" title="Learn more about Naïve Bayes from ScienceDirect's AI-generated Topic Pages" class="topic-link">Naïve Bayes</a> (NB) is another powerful and widely-used classifier, which assumes that dataset features are independent. It can be used to filter out the sentences that do not support comparative opinions </span><a class="anchor anchor-primary" href="#bib0125" name="bbib0125" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0125"><span class="anchor-text-container"><span class="anchor-text">[125]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0058"><strong>Hybrid-based TSA</strong>. By integrating knowledge with statistic models, hybrid-based TSA can take full advantage of both <a class="anchor anchor-primary" href="#bib0126" name="bbib0126" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0126"><span class="anchor-text-container"><span class="anchor-text">[126]</span></span></a>. For example, Xia et&nbsp;al. <a class="anchor anchor-primary" href="#bib0127" name="bbib0127" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0127"><span class="anchor-text-container"><span class="anchor-text">[127]</span></span></a><span> utilized SenticNet and a <a href="/topics/computer-science/bayesian-model" title="Learn more about Bayesian model from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian model</a> for contextual concept polarity disambiguation. Due to the ambiguity and little information of neutral between positive and negative, Valdivia et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0128" name="bbib0128" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0128"><span class="anchor-text-container"><span class="anchor-text">[128]</span></span></a> proposed consensus vote models and weighted aggregation to detect and filter neutrality by representing the vague boundary between two sentiment polarities. According to experiments, they concluded that detecting neutral can help improve the performance of sentiment analysis. Le et&nbsp;al. <a class="anchor anchor-primary" href="#bib0129" name="bbib0129" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0129"><span class="anchor-text-container"><span class="anchor-text">[129]</span></span></a><span> proposed a novel hybrid method in the combination of word <a href="/topics/computer-science/sentiment-score" title="Learn more about sentiment score from ScienceDirect's AI-generated Topic Pages" class="topic-link">sentiment score</a> calculation, text pre-processing, sentiment feature generation and an ML-based classifier for sentiment analysis. The hybrid method </span><a class="anchor anchor-primary" href="#bib0129" name="bbib0129" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0129"><span class="anchor-text-container"><span class="anchor-text">[129]</span></span></a><span> performed much better than popular lexicon-based methods in <a href="/topics/earth-and-planetary-sciences/amazon" title="Learn more about Amazon from ScienceDirect's AI-generated Topic Pages" class="topic-link">Amazon</a>, IMDb, and Yelp, and achieved an average accuracy of 87.13%. Li et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0130" name="bbib0130" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0130"><span class="anchor-text-container"><span class="anchor-text">[130]</span></span></a> utilized lexicons with one of the ML-based methods including NB and SVM. This hybrid method is more effective in detecting expressions that are difficult to be polarized into positive-negative categories.</div></section><section id="sec0020"><h4 id="cesectitle0023" class="u-margin-m-top u-margin-xs-bottom">5.1.2. DL-based TSA</h4><div class="u-margin-s-bottom" id="para0059"><span>DL-based techniques have a strong ability to automatically learn and discover <a href="/topics/computer-science/discriminative-feature" title="Learn more about discriminative feature from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminative feature</a><span> representations from data themselves. DL-based TSA has been proved to be successful with the success of <a href="/topics/computer-science/word-embedding" title="Learn more about word embeddings from ScienceDirect's AI-generated Topic Pages" class="topic-link">word embeddings</a> </span></span><a class="anchor anchor-primary" href="#bib0131" name="bbib0131" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0131"><span class="anchor-text-container"><span class="anchor-text">[131]</span></span></a><span> and the increase of the <a href="/topics/computer-science/training-data" title="Learn more about training data from ScienceDirect's AI-generated Topic Pages" class="topic-link">training data</a> with multi-class classification </span><a class="anchor anchor-primary" href="#bib0132" name="bbib0132" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0132"><span class="anchor-text-container"><span class="anchor-text">[132]</span></span></a><span>. Various DL-based approaches for TSA include <a href="/topics/computer-science/deep-convolutional-neural-networks" title="Learn more about deep convolutional neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep convolutional neural network</a><span> (ConvNet) learning, deep <a href="/topics/computer-science/recurrent-neural-network" title="Learn more about RNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">RNN</a> learning, deep ConvNet-RNN learning and deep adversarial learning, as detailed next.</span></span></div><div class="u-margin-s-bottom" id="para0060"><strong>Deep ConvNet learning for TSA.</strong> CNN-based methods have been applied to different levels of TSA including document-level <a class="anchor anchor-primary" href="#bib0133" name="bbib0133" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0133"><span class="anchor-text-container"><span class="anchor-text">[133]</span></span></a>, sentence-level <a class="anchor anchor-primary" href="#bib0134" name="bbib0134" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0134"><span class="anchor-text-container"><span class="anchor-text">[134]</span></span></a>, and aspect-level (or word-level) <a class="anchor anchor-primary" href="#bib0135" name="bbib0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0135"><span class="anchor-text-container"><span class="anchor-text">[135]</span></span></a><span> by using different filters to learn <a href="/topics/computer-science/local-feature" title="Learn more about local features from ScienceDirect's AI-generated Topic Pages" class="topic-link">local features</a> from the input data. Yin et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0136" name="bbib0136" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0136"><span class="anchor-text-container"><span class="anchor-text">[136]</span></span></a><span> proposed a framework of sentence-level sentiment classification based on the semantic lexical-augmented <a href="/topics/computer-science/convolutional-neural-network" title="Learn more about CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN</a> (SCNN) model, which makes full use of word information. Conneau et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0137" name="bbib0137" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0137"><span class="anchor-text-container"><span class="anchor-text">[137]</span></span></a> applied a very deep CNN (VDCNN), which learns the hierarchical representations of the document and long-range dependencies to text processing. To establish long-range dependencies in documents, Johnson and Zhang <a class="anchor anchor-primary" href="#bib0138" name="bbib0138" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0138"><span class="anchor-text-container"><span class="anchor-text">[138]</span></span></a><span> proposed a word-level deep pyramid CNN (DPCNN) model, which stacked alternately the <a href="/topics/computer-science/convolutional-layer" title="Learn more about convolutional layer from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional layer</a> and the max-pooling downsampling layer to form a pyramid to reduce computing complexity. The DPCNN with 15 weighted layers outperformed the previous best models on six benchmark databases for sentiment classification and topic categorization. For aspect-level sentiment analysis, Huang and Carley also </span><a class="anchor anchor-primary" href="#bib0139" name="bbib0139" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0139"><span class="anchor-text-container"><span class="anchor-text">[139]</span></span></a> proposed a novel aspect-specific CNN by combining parameterized filters and parametrized gates.</div><div class="u-margin-s-bottom" id="para0061"><strong>Deep RNN learning for TSA.</strong> RNN-based TSA is capable of processing long sequence data. For example, Mousa and Schuller <a class="anchor anchor-primary" href="#bib0140" name="bbib0140" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0140"><span class="anchor-text-container"><span class="anchor-text">[140]</span></span></a><span><span> designed a novel <a href="/topics/computer-science/generative-approach" title="Learn more about generative approach from ScienceDirect's AI-generated Topic Pages" class="topic-link">generative approach</a>, contextual Bi-LSTM with a </span><a href="/topics/computer-science/language-modeling" title="Learn more about language model from ScienceDirect's AI-generated Topic Pages" class="topic-link">language model</a> (cBi-LSTM LM), which changes the structure of Bi-LSTM to learn the word's contextual information based on its right and left contexts. Moreover, Wang et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0141" name="bbib0141" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0141"><span class="anchor-text-container"><span class="anchor-text">[141]</span></span></a><span> proposed a model of recursive neural <a href="/topics/computer-science/conditional-random-field" title="Learn more about conditional random field from ScienceDirect's AI-generated Topic Pages" class="topic-link">conditional random field</a><span> (RNCRF) by integrating the RNN-based dependency <a href="/topics/biochemistry-genetics-and-molecular-biology/tree" title="Learn more about tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">tree</a> of the sentence and conditional random fields.</span></span></div><div class="u-margin-s-bottom" id="para0062"><span>The <a href="/topics/computer-science/attention-machine-learning" title="Learn more about attention mechanism from ScienceDirect's AI-generated Topic Pages" class="topic-link">attention mechanism</a><span> contributes to prioritizing relevant parts of the given input sequence according to a weighted representation at a low computational cost. In the paradigm of attention-based <a href="/topics/computer-science/long-short-term-memory-network" title="Learn more about LSTM from ScienceDirect's AI-generated Topic Pages" class="topic-link">LSTM</a> for TSA, LSTM helps to construct the document representation, and then attention-based deep memory layers compute the ratings of each document. Chen et&nbsp;al. </span></span><a class="anchor anchor-primary" href="#bib0142" name="bbib0142" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0142"><span class="anchor-text-container"><span class="anchor-text">[142]</span></span></a><span> designed the recurrent attention memory (RAM) for aspect-level sentiment analysis. Specifically, a multiple-attention mechanism was employed to capture sentiment features separated by a long distance, the results of which were non-linearly combined with RNN. Inspired by the capability of human eye-movement <a href="/topics/neuroscience/behavior-neuroscience" title="Learn more about behavior from ScienceDirect's AI-generated Topic Pages" class="topic-link">behavior</a>, Mishra et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0143" name="bbib0143" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0143"><span class="anchor-text-container"><span class="anchor-text">[143]</span></span></a> introduced a hierarchical LSTM-based model trained by cognition grounded eye-tracking data, and they used the model to predict the sentiment of the overall review text. More recently, some researchers [<a class="anchor anchor-primary" href="#bib0144" name="bbib0144" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0144"><span class="anchor-text-container"><span class="anchor-text">144</span></span></a>,<a class="anchor anchor-primary" href="#bib0145" name="bbib0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0145"><span class="anchor-text-container"><span class="anchor-text">145</span></span></a>] suggested that user preferences and product characteristics should be taken into account.</div><div class="u-margin-s-bottom" id="para0063">For document-level sentiment classification, Dou <a class="anchor anchor-primary" href="#bib0145" name="bbib0145" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0145"><span class="anchor-text-container"><span class="anchor-text">[145]</span></span></a> proposed a deep memory network combining LSTM on account of the influence of users who express the sentiment and the products that are evaluated. Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0144" name="bbib0144" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0144"><span class="anchor-text-container"><span class="anchor-text">[144]</span></span></a> designed a hierarchical LSTM with an attention mechanism to generate sentence and document representations, which incorporates global user and product information to prioritize the most contributing items. Considering the irrationality of encoding user information and product information as one representation, Wu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0146" name="bbib0146" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0146"><span class="anchor-text-container"><span class="anchor-text">[146]</span></span></a> designed an attention LSTM-based model, which executed hierarchical user attention and product attention (HUAPA) to realize sentiment classification.</div><div class="u-margin-s-bottom" id="para0064">For multi-task classification (e.g., aspect category and sentiment polarity detection), J et&nbsp;al. <a class="anchor anchor-primary" href="#bib0147" name="bbib0147" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0147"><span class="anchor-text-container"><span class="anchor-text">[147]</span></span></a> proposed convolutional stacked Bi-LSTM with a multiplicative attention network concerning global-local information. In contrast, to fully exploit contextual affective knowledge in aspect-level TSA, Liang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0148" name="bbib0148" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0148"><span class="anchor-text-container"><span class="anchor-text">[148]</span></span></a><span> proposed GCN-based SenticNet to enhance graph-based dependencies of sentences. Specifically, LSTM layers were employed to learn contextual representations, and <a href="/topics/computer-science/graph-convolutional-network" title="Learn more about GCN from ScienceDirect's AI-generated Topic Pages" class="topic-link">GCN</a> layers were built to capture the relationships between contextual words in specific aspects.</span></div><div class="u-margin-s-bottom" id="para0065"><strong>Deep ConvNet-RNN learning for TSA.</strong> Although ConvNet-based or RNN-based models have been extensively employed to generate impressive results in TSA, more researchers [<a class="anchor anchor-primary" href="#bib0149" name="bbib0149" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0149"><span class="anchor-text-container"><span class="anchor-text">149</span></span></a>,<a class="anchor anchor-primary" href="#bib0150" name="bbib0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0150"><span class="anchor-text-container"><span class="anchor-text">150</span></span></a>] have tried to combine both ConvNets and RNNs to improve the performance of TSA by getting the benefits offered by each model.</div><div class="u-margin-s-bottom" id="para0066">As the CNN is proficient in extracting local features and the BiLSTM is skilled in a long sequence, Li et&nbsp;al. <a class="anchor anchor-primary" href="#bib0150" name="bbib0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0150"><span class="anchor-text-container"><span class="anchor-text">[150]</span></span></a> combined CNN and BiLSTM in a parallel manner to extract both types of features, improving the performance of sentiment analysis. To decrease the training time and complexity of LSTM and attention mechanism for predicting the sentiment polarity, Xue et&nbsp;al. <a class="anchor anchor-primary" href="#bib0151" name="bbib0151" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0151"><span class="anchor-text-container"><span class="anchor-text">[151]</span></span></a><span> designed a gated <a href="/topics/computer-science/convolutional-network" title="Learn more about convolutional network from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional network</a> with aspect embedding (GCAE) for aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). The GCAE uses two parallel CNNs, which output results combined with the gated unit and extended with the third CNN, extracting contextual information of aspect terms.</span></div><div class="u-margin-s-bottom" id="para0067">To distinguish the importance of different features, Basiri et&nbsp;al. <a class="anchor anchor-primary" href="#bib0152" name="bbib0152" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0152"><span class="anchor-text-container"><span class="anchor-text">[152]</span></span></a> proposed an attention-based CNN-RNN deep model (ABCDM), which utilized bidirectional LSTM and GRU layers to capture temporal contexts and apply the attention operations on the discriminative embeddings of outputs generated by two RNN-based networks. In addition, CNNs were employed for feature enhancement (e.g., feature dimensionality reduction and position-invariant feature extraction). All the above works focused on detecting sentiment or emotion, but it is important to predict the intensity or degree of one sentiment in the description of human intimate emotion. To address the problem, Akhtar et&nbsp;al. <a class="anchor anchor-primary" href="#bib0153" name="bbib0153" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0153"><span class="anchor-text-container"><span class="anchor-text">[153]</span></span></a><span> proposed a stacked <a href="/topics/computer-science/ensemble-method" title="Learn more about ensemble method from ScienceDirect's AI-generated Topic Pages" class="topic-link">ensemble method</a> by using an MLP to ensemble the outputs of the CNN, LSTM, GUR, and SVR.</span></div><div class="u-margin-s-bottom" id="para0068"><strong>Deep adversarial learning for TSA.</strong><span> Deep adversarial learning with the ability to regularize supervised learning algorithms was introduced to <a href="/topics/computer-science/text-classification" title="Learn more about text classification from ScienceDirect's AI-generated Topic Pages" class="topic-link">text classification</a> </span><a class="anchor anchor-primary" href="#bib0154" name="bbib0154" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0154"><span class="anchor-text-container"><span class="anchor-text">[154]</span></span></a><span>. Inspired by the domain-adversarial <a href="/topics/neuroscience/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> </span><a class="anchor anchor-primary" href="#bib0155" name="bbib0155" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0155"><span class="anchor-text-container"><span class="anchor-text">[155]</span></span></a>, Li et&nbsp;al. <a class="anchor anchor-primary" href="#bib0156" name="bbib0156" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0156"><span class="anchor-text-container"><span class="anchor-text">[156]</span></span></a><span> constructed an adversarial memory network model that contains sentiment and domain classifier modules. Both modules were trained together to reduce the sentiment <a href="/topics/computer-science/classification-error" title="Learn more about classification error from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification error</a> and allowed the domain classifier not to separate both domain samples. The attention mechanism is incorporated into the deep adversarial based model to help the selection of the pivoted words, which are useful for sentiment classification and shared between the source and target domains. Li et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0157" name="bbib0157" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0157"><span class="anchor-text-container"><span class="anchor-text">[157]</span></span></a> initiated the use of GANs <a class="anchor anchor-primary" href="#bib0158" name="bbib0158" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0158"><span class="anchor-text-container"><span class="anchor-text">[158]</span></span></a><span><span><span> in sentiment analysis, <a href="/topics/computer-science/reinforcement-learning" title="Learn more about reinforcement learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">reinforcement learning</a> and </span><a href="/topics/neuroscience/recurrent-neural-network" title="Learn more about recurrent neural networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">recurrent neural networks</a> to build a novel model, termed category sentence </span><a href="/topics/computer-science/generative-adversarial-networks" title="Learn more about generative adversarial network from ScienceDirect's AI-generated Topic Pages" class="topic-link">generative adversarial network</a> (CS-GAN). By combining GANs with RL, the CS-GAN can generate more category sentences to improve the capability of generalization during supervised training. Similarly, to tackle the sentiment classification in low-resource languages without adequate annotated data, Chen et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0159" name="bbib0159" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0159"><span class="anchor-text-container"><span class="anchor-text">[159]</span></span></a><span><span> proposed an adversarial deep averaging network (ADAN) to realize cross-lingual sentiment analysis by <a href="/topics/physics-and-astronomy/transferring" title="Learn more about transferring from ScienceDirect's AI-generated Topic Pages" class="topic-link">transferring</a> the knowledge learned from labelled data on a resource-rich source language to low-resource languages where only </span><a href="/topics/computer-science/unlabeled-data" title="Learn more about unlabeled data from ScienceDirect's AI-generated Topic Pages" class="topic-link">unlabeled data</a> existed. Especially, the ADAN was trained with labelled source text data from English and unlabeled target text data from Arabic and Chinese. More recently, Karimi et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0160" name="bbib0160" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0160"><span class="anchor-text-container"><span class="anchor-text">[160]</span></span></a><span><span> fine-tuned the general-purpose <a href="/topics/computer-science/bidirectional-encoder-representations-from-transformers" title="Learn more about BERT from ScienceDirect's AI-generated Topic Pages" class="topic-link">BERT</a> and domain-specific post-trained BERT using </span><a href="/topics/computer-science/adversarial-machine-learning" title="Learn more about adversarial training from ScienceDirect's AI-generated Topic Pages" class="topic-link">adversarial training</a>, which showed promising results in TSA.</span></div></section></section><section id="sec0021"><h3 id="cesectitle0024" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.2. Audio emotion recognition</h3><div class="u-margin-s-bottom" id="para0069">Audio emotion recognition (also called SER) detects the embedded emotions by processing and understanding speech signals <a class="anchor anchor-primary" href="#bib0161" name="bbib0161" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0161"><span class="anchor-text-container"><span class="anchor-text">[161]</span></span></a>. Various ML-based and DL-based SER systems have been carried out on the basis of these extracted features for better analysis [<a class="anchor anchor-primary" href="#bib0162" name="bbib0162" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0162"><span class="anchor-text-container"><span class="anchor-text">162</span></span></a>,<a class="anchor anchor-primary" href="#bib0163" name="bbib0163" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0163"><span class="anchor-text-container"><span class="anchor-text">163</span></span></a>]. Traditional ML-based SER concentrates on the extraction of the acoustic features and the selection of the classifiers. However DL-based SER constructs an end-to-end CNN architecture to predict the final emotion without considering feature engineering and selection <a class="anchor anchor-primary" href="#bib0164" name="bbib0164" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0164"><span class="anchor-text-container"><span class="anchor-text">[164]</span></span></a>.</div><div class="u-margin-s-bottom"><div id="para0070"><a class="anchor anchor-primary" href="#tbl0008" name="btbl0008" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0008"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;8</span></span></a> shows an overview of representative methods for SER, including the most relevant papers, their publication (or preprint) year, feature representation, classifier, database, and performance (from best or average reported results). These works are next described in detail.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0008"><span class="captions text-s"><span id="cap0014"><p id="spara015"><span class="label">Table 8</span>. Overview of the representative methods for SER.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0898">Pub.</th><th scope="col" class="align-left valign-top" id="en0899">Year</th><th scope="col" class="align-left valign-top" id="en0900">Feature Representation</th><th scope="col" class="align-left valign-top" id="en0901">Classifier</th><th scope="col" class="align-left valign-top" id="en0902">Database</th><th scope="col" class="align-left valign-top" id="en0903">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0904" colspan="6"><strong><em>ML-based SER</em></strong></td></tr><tr><td class="align-left valign-top" id="en0905"><a class="anchor anchor-primary" href="#bib0174" name="bbib0174" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0174"><span class="anchor-text-container"><span class="anchor-text">[174]</span></span></a></td><td class="align-left valign-top" id="en0906">2010</td><td class="align-left valign-top" id="en0907">Acoustic-feature</td><td class="align-left valign-top" id="en0908">SVM</td><td class="align-left valign-top" id="en0909">LDC/ Emo-DB</td><td class="align-left valign-top" id="en0910">6 classes: 43.80/79.10</td></tr><tr><td class="align-left valign-top" id="en0911"><a class="anchor anchor-primary" href="#bib0176" name="bbib0176" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0176"><span class="anchor-text-container"><span class="anchor-text">[176]</span></span></a></td><td class="align-left valign-top" id="en0912">2014</td><td class="align-left valign-top" id="en0913">Feature selection and fusion</td><td class="align-left valign-top" id="en0914">MKL</td><td class="align-left valign-top" id="en0915">Emo-DB</td><td class="align-left valign-top" id="en0916">7 classes: 83.10</td></tr><tr><td class="align-left valign-top" id="en0917"><a class="anchor anchor-primary" href="#bib0181" name="bbib0181" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0181"><span class="anchor-text-container"><span class="anchor-text">[181]</span></span></a></td><td class="align-left valign-top" id="en0918">2013</td><td class="align-left valign-top" id="en0919">MFCC</td><td class="align-left valign-top" id="en0920">SVM</td><td class="align-left valign-top" id="en0921">Emo-DB</td><td class="align-left valign-top" id="en0922">7 classes: 68.00</td></tr><tr><td class="align-left valign-top" id="en0923"><a class="anchor anchor-primary" href="#bib0186" name="bbib0186" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0186"><span class="anchor-text-container"><span class="anchor-text">[186]</span></span></a></td><td class="align-left valign-top" id="en0924">2020</td><td class="align-left valign-top" id="en0925">Acoustic features</td><td class="align-left valign-top" id="en0926">TL-FMRF</td><td class="align-left valign-top" id="en0927">CASIA/Emo-DB</td><td class="align-left valign-top" id="en0928">6 classes: 81.75/77.94</td></tr><tr><td class="align-left valign-top" id="en0929" colspan="6"><strong><em>DL-based SER</em></strong></td></tr><tr><td class="align-left valign-top" id="en0930"><a class="anchor anchor-primary" href="#bib0163" name="bbib0163" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0163"><span class="anchor-text-container"><span class="anchor-text">[163]</span></span></a></td><td class="align-left valign-top" id="en0931">2013</td><td class="align-left valign-top" id="en0932">Semi-CNN</td><td class="align-left valign-top" id="en0933">SVM</td><td class="align-left valign-top" id="en0934">SAVEE/Emo-DB<br>DES/ MES</td><td class="align-left valign-top" id="en0935">7 classes: 89.70/93.70<br>7 classes: 90.80/90.20</td></tr><tr><td class="align-left valign-top" id="en0936"><a class="anchor anchor-primary" href="#bib0193" name="bbib0193" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0193"><span class="anchor-text-container"><span class="anchor-text">[193]</span></span></a></td><td class="align-left valign-top" id="en0937">2017</td><td class="align-left valign-top" id="en0938">CNN</td><td class="align-left valign-top" id="en0939">FC</td><td class="align-left valign-top" id="en0940">Emo-DB</td><td class="align-left valign-top" id="en0941">7 classes: 84.30</td></tr><tr><td class="align-left valign-top" id="en0942"><a class="anchor anchor-primary" href="#bib0197" name="bbib0197" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0197"><span class="anchor-text-container"><span class="anchor-text">[197]</span></span></a></td><td class="align-left valign-top" id="en0943">2014</td><td class="align-left valign-top" id="en0944">Bi-LSTM-RNN</td><td class="align-left valign-top" id="en0945">RNN</td><td class="align-left valign-top" id="en0946">USC-IEMOCAP</td><td class="align-left valign-top" id="en0947">4 classes: 51.86</td></tr><tr><td class="align-left valign-top" id="en0948"><a class="anchor anchor-primary" href="#bib0198" name="bbib0198" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0198"><span class="anchor-text-container"><span class="anchor-text">[198]</span></span></a></td><td class="align-left valign-top" id="en0949">2017</td><td class="align-left valign-top" id="en0950">DNN/RNN</td><td class="align-left valign-top" id="en0951">Pooling</td><td class="align-left valign-top" id="en0952">IEMOCAP</td><td class="align-left valign-top" id="en0953">7 classes: 58.80</td></tr><tr><td class="align-left valign-top" id="en0954"><a class="anchor anchor-primary" href="#bib0199" name="bbib0199" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0199"><span class="anchor-text-container"><span class="anchor-text">[199]</span></span></a></td><td class="align-left valign-top" id="en0955">2018</td><td class="align-left valign-top" id="en0956">Attention-3D-RNN</td><td class="align-left valign-top" id="en0957">FC</td><td class="align-left valign-top" id="en0958">IEMOCAP<br>Emo-DB</td><td class="align-left valign-top" id="en0959">4 classes: 64.74<br>7 classes: 82.82</td></tr><tr><td class="align-left valign-top" id="en0960"><a class="anchor anchor-primary" href="#bib0200" name="bbib0200" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0200"><span class="anchor-text-container"><span class="anchor-text">[200]</span></span></a></td><td class="align-left valign-top" id="en0961">2016</td><td class="align-left valign-top" id="en0962">CNNs-LSTM</td><td class="align-left valign-top" id="en0963">LSTM</td><td class="align-left valign-top" id="en0964">RECOLA</td><td class="align-left valign-top" id="en0965">A/V: 74.10/ 32.50</td></tr><tr><td class="align-left valign-top" id="en0966"><a class="anchor anchor-primary" href="#bib0203" name="bbib0203" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0203"><span class="anchor-text-container"><span class="anchor-text">[203]</span></span></a></td><td class="align-left valign-top" id="en0967">2018</td><td class="align-left valign-top" id="en0968">CNN/Attention-BLSTM</td><td class="align-left valign-top" id="en0969">DNN</td><td class="align-left valign-top" id="en0970">IEMOCAP</td><td class="align-left valign-top" id="en0971">7 classes: 68.00</td></tr><tr><td class="align-left valign-top" id="en0972"><a class="anchor anchor-primary" href="#bib0204" name="bbib0204" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0204"><span class="anchor-text-container"><span class="anchor-text">[204]</span></span></a></td><td class="align-left valign-top" id="en0973">2018</td><td class="align-left valign-top" id="en0974">Adversarial AE</td><td class="align-left valign-top" id="en0975">SVM</td><td class="align-left valign-top" id="en0976">IEMOCAP</td><td class="align-left valign-top" id="en0977">7 classes: 58.38</td></tr><tr><td class="align-left valign-top" id="en0978"><a class="anchor anchor-primary" href="#bib0205" name="bbib0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0205"><span class="anchor-text-container"><span class="anchor-text">[205]</span></span></a></td><td class="align-left valign-top" id="en0979">2018</td><td class="align-left valign-top" id="en0980">Conditional adversarial</td><td class="align-left valign-top" id="en0981">CNN</td><td class="align-left valign-top" id="en0982">RECOLA</td><td class="align-left valign-top" id="en0983">A/V: 73.70/44.40</td></tr></tbody></table></div></div></div><section id="sec0022"><h4 id="cesectitle0025" class="u-margin-m-top u-margin-xs-bottom">5.2.1. ML-based SER</h4><div class="u-margin-s-bottom" id="para0071"><span>The ML-based SER systems include two key steps: the strong features <a href="/topics/computer-science/representation-learning" title="Learn more about representation learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">representation learning</a> for emotional speech and an appropriate classification for final emotion prediction </span><a class="anchor anchor-primary" href="#bib0019" name="bbib0019" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0019"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a><span>. Different kinds of acoustic features can be fused to get the mixed features for a robust SER. Although prosodic features and <a href="/topics/computer-science/spectral-feature" title="Learn more about spectral features from ScienceDirect's AI-generated Topic Pages" class="topic-link">spectral features</a> are more frequently used in SER systems </span><a class="anchor anchor-primary" href="#bib0165" name="bbib0165" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0165"><span class="anchor-text-container"><span class="anchor-text">[165]</span></span></a>, in some cases, voice-quality features and other features are sometimes more important <a class="anchor anchor-primary" href="#bib0166" name="bbib0166" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0166"><span class="anchor-text-container"><span class="anchor-text">[166]</span></span></a>. OpenSMILE <a class="anchor anchor-primary" href="#bib0167" name="bbib0167" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0167"><span class="anchor-text-container"><span class="anchor-text">[167]</span></span></a><span><span> is a popular audio feature extraction toolkit that extracts all key features of the speech. The commonly used classifiers for SER systems encompass HMM, GMM, SVM, RF and <a href="/topics/neuroscience/artificial-neural-network" title="Learn more about ANN from ScienceDirect's AI-generated Topic Pages" class="topic-link">ANN</a>. In addition to these classifiers, the improved conventional interpretable classifiers and </span><a href="/topics/computer-science/ensemble-classifier" title="Learn more about ensemble classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">ensemble classifiers</a> are also adopted for SER. In this sub-section, we divided the ML-based SER systems into acoustic-feature based SER and interpretable-classifier based SER </span><a class="anchor anchor-primary" href="#bib0168" name="bbib0168" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0168"><span class="anchor-text-container"><span class="anchor-text">[168]</span></span></a><strong>,</strong> <a class="anchor anchor-primary" href="#bib0169" name="bbib0169" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0169"><span class="anchor-text-container"><span class="anchor-text">[169]</span></span></a>. Note that different combinations of models and features result in obvious differences in the performance of SER <a class="anchor anchor-primary" href="#bib0170" name="bbib0170" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0170"><span class="anchor-text-container"><span class="anchor-text">[170]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0072"><strong>Acoustic-feature based SER.</strong> Prosodic features (e.g. intonation and rhythm) have been discovered to convey the most distinctive properties of emotional content for SER. The prosodic features consist of fundamental frequency (rhythmical and tonal characteristics) <a class="anchor anchor-primary" href="#bib0171" name="bbib0171" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0171"><span class="anchor-text-container"><span class="anchor-text">[171]</span></span></a><span>, energy (volume or the intensity), and duration (the total of time to build vowels, words and similar constructs). Voice quality is determined by the physical properties of the <a href="/topics/medicine-and-dentistry/vocal-tract" title="Learn more about vocal tract from ScienceDirect's AI-generated Topic Pages" class="topic-link">vocal tract</a> such as jitter, shimmer, and harmonics to noise ratio. Lugger and Yang </span><a class="anchor anchor-primary" href="#bib0172" name="bbib0172" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0172"><span class="anchor-text-container"><span class="anchor-text">[172]</span></span></a><span> investigated the effect of prosodic features, voice quality parameters, and different combinations of both types on emotion classification. Spectral features are often obtained by transforming the time-domain speech signal into the frequency-domain speech signal using the <a href="/topics/neuroscience/fourier-transform" title="Learn more about Fourier transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Fourier transform</a> </span><a class="anchor anchor-primary" href="#bib0173" name="bbib0173" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0173"><span class="anchor-text-container"><span class="anchor-text">[173]</span></span></a>. Bitouk et&nbsp;al. <a class="anchor anchor-primary" href="#bib0174" name="bbib0174" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0174"><span class="anchor-text-container"><span class="anchor-text">[174]</span></span></a><span> introduced a new set of fine-grained spectral features which are statistics of Mel Frequency cepstrum coefficients (MFCC) over three <a href="/topics/physics-and-astronomy/phoneme" title="Learn more about phoneme from ScienceDirect's AI-generated Topic Pages" class="topic-link">phoneme</a> type classes of interest in the utterance. Compared to prosodic features or utterance level spectral features, the fine-grained spectral features can yield results with higher accuracy. In addition, the combination of these features and prosodic features also improves accuracy. Shen et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0175" name="bbib0175" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0175"><span class="anchor-text-container"><span class="anchor-text">[175]</span></span></a><span> utilized SVM to evaluate the performance of using energy and pitch, <a href="/topics/earth-and-planetary-sciences/linear-prediction" title="Learn more about linear prediction from ScienceDirect's AI-generated Topic Pages" class="topic-link">linear prediction</a> cepstrum coefficients (LPCC), MFCC, and their combination. The experiment demonstrated the superior performance based on the combination of various acoustic features.</span></div><div class="u-margin-s-bottom" id="para0073">In order to improve the recognition performance of speaker-independent SER, Jin et&nbsp;al. <a class="anchor anchor-primary" href="#bib0176" name="bbib0176" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0176"><span class="anchor-text-container"><span class="anchor-text">[176]</span></span></a><span><span> designed feature selection with L1-normalization constraint of <a href="/topics/computer-science/multiple-kernel-learning" title="Learn more about Multiple Kernel Learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">Multiple Kernel Learning</a><span> (MKL) and <a href="/topics/computer-science/feature-fusion" title="Learn more about feature fusion from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature fusion</a> with </span></span><a href="/topics/computer-science/gaussian-kernel" title="Learn more about Gaussian kernels from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian kernels</a>. This work </span><a class="anchor anchor-primary" href="#bib0176" name="bbib0176" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0176"><span class="anchor-text-container"><span class="anchor-text">[176]</span></span></a> achieved an accuracy of 83.10% on the Berlin Database, which is 2.10% higher than the model that used the sequential floating forward selection algorithm, and a GMM <a class="anchor anchor-primary" href="#bib0177" name="bbib0177" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0177"><span class="anchor-text-container"><span class="anchor-text">[177]</span></span></a>. Wang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0178" name="bbib0178" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0178"><span class="anchor-text-container"><span class="anchor-text">[178]</span></span></a> proposed a new Fourier parameter-based model using the perceptual content of voice quality, and the first-order and second-order differences for speaker-independent SER. Experimental results revealed that the combination of Fourier parameters and MFCC could significantly increase the recognition rate of SER.</div><div class="u-margin-s-bottom" id="para0074"><strong>ML-based classifier for SER.</strong> The HMM classifier is extensively adopted for SER due to the production mechanism of the speech signals. In 2003, Nwe et&nbsp;al. <a class="anchor anchor-primary" href="#bib0179" name="bbib0179" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0179"><span class="anchor-text-container"><span class="anchor-text">[179]</span></span></a><span> designed an HMM with log frequency power coefficients (LFPC) to detect human stress and emotion. It achieved the best <a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a> of 89%, which was higher than 65.8% of human recognition. The GMM and its variants can be considered as a special continuous HMM and are appropriate for global-feature based SER. For example, Navas et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0180" name="bbib0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0180"><span class="anchor-text-container"><span class="anchor-text">[180]</span></span></a><span> proposed a GMM-based <a href="/topics/computer-science/baseline-method" title="Learn more about baseline method from ScienceDirect's AI-generated Topic Pages" class="topic-link">baseline method</a> by using prosodic, voice quality, and MFCC features.</span></div><div class="u-margin-s-bottom" id="para0075"><span><span>Different from HMM and GMM, SVM maps the emotion vector to a higher dimensional space by using a <a href="/topics/computer-science/kernel-function" title="Learn more about kernel function from ScienceDirect's AI-generated Topic Pages" class="topic-link">kernel function</a> and establishes the maximum interval </span><a href="/topics/computer-science/hyperplanes" title="Learn more about hyperplane from ScienceDirect's AI-generated Topic Pages" class="topic-link">hyperplane</a> in the high-dimensional space for optimal classification. Milton et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0181" name="bbib0181" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0181"><span class="anchor-text-container"><span class="anchor-text">[181]</span></span></a><span> designed a three-stage hierarchical SVM with linear and <a href="/topics/computer-science/radial-basis-function" title="Learn more about RBF from ScienceDirect's AI-generated Topic Pages" class="topic-link">RBF</a> kernels to classify seven emotions using MFCC features on the Berlin EmoDB. There are some works [</span><a class="anchor anchor-primary" href="#bib0182" name="bbib0182" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0182"><span class="anchor-text-container"><span class="anchor-text">182</span></span></a>,<a class="anchor anchor-primary" href="#bib0183" name="bbib0183" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0183"><span class="anchor-text-container"><span class="anchor-text">183</span></span></a><span>] using different SVM classifiers with various acoustic features and their combinations. <a href="/topics/computer-science/ensemble-learning" title="Learn more about Ensemble learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">Ensemble learning</a> has been proven to give superior performance compared to a single classifier. Yüncü et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0184" name="bbib0184" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0184"><span class="anchor-text-container"><span class="anchor-text">[184]</span></span></a> designed the SVM with Binary DT by integrating the tree architecture with SVM. Bhavan et&nbsp;al. <a class="anchor anchor-primary" href="#bib0185" name="bbib0185" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0185"><span class="anchor-text-container"><span class="anchor-text">[185]</span></span></a><span> proposed a bagged ensemble comprising of SVM with different <a href="/topics/biochemistry-genetics-and-molecular-biology/kernel-method" title="Learn more about Gaussian kernels from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian kernels</a><span> as a viable algorithm. Considering differences among various categories of <a href="/topics/biochemistry-genetics-and-molecular-biology/human" title="Learn more about human beings from ScienceDirect's AI-generated Topic Pages" class="topic-link">human beings</a>, Chen et&nbsp;al. </span></span><a class="anchor anchor-primary" href="#bib0186" name="bbib0186" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0186"><span class="anchor-text-container"><span class="anchor-text">[186]</span></span></a><span> proposed the two-layer fuzzy multiple RF by integrating the <a href="/topics/biochemistry-genetics-and-molecular-biology/decision-trees" title="Learn more about decision trees from ScienceDirect's AI-generated Topic Pages" class="topic-link">decision trees</a> with Bootstraps to recognize six emotional states.</span></div></section><section id="sec0023"><h4 id="cesectitle0026" class="u-margin-m-top u-margin-xs-bottom">5.2.2. DL-based SER</h4><div class="u-margin-s-bottom" id="para0076">DL-based SER systems can understand and detect contexts and features of emotional speech without designing a tailored feature extractor. The CNNs with auto-encoder <a class="anchor anchor-primary" href="#bib0187" name="bbib0187" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0187"><span class="anchor-text-container"><span class="anchor-text">[187]</span></span></a> are regarded as commonly used techniques for DL-based SER <a class="anchor anchor-primary" href="#bib0187" name="bbib0187" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0187"><span class="anchor-text-container"><span class="anchor-text">[187]</span></span></a>. The RNNs <a class="anchor anchor-primary" href="#bib0188" name="bbib0188" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0188"><span class="anchor-text-container"><span class="anchor-text">[188]</span></span></a> and their variants (e.g., Bi-LSTM) <a class="anchor anchor-primary" href="#bib0189" name="bbib0189" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0189"><span class="anchor-text-container"><span class="anchor-text">[189]</span></span></a> are widely introduced to capture temporal information. The hybrid deep learning for SER includes ConvNets and RNNs, as well as attention mechanisms [<a class="anchor anchor-primary" href="#bib0190" name="bbib0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0190"><span class="anchor-text-container"><span class="anchor-text">190</span></span></a>,<a class="anchor anchor-primary" href="#bib0191" name="bbib0191" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0191"><span class="anchor-text-container"><span class="anchor-text">191</span></span></a>]. For the issues of limited data amount and low quality of the databases, adversarial learning can be used for DL-based SER <a class="anchor anchor-primary" href="#bib0192" name="bbib0192" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0192"><span class="anchor-text-container"><span class="anchor-text">[192]</span></span></a> by augmenting trained data and eliminating perturbations.</div><div class="u-margin-s-bottom" id="para0077"><strong>ConvNet learning for SER</strong>. Huang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0163" name="bbib0163" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0163"><span class="anchor-text-container"><span class="anchor-text">[163]</span></span></a><span> utilized the semi-CNN to extract features of <a href="/topics/earth-and-planetary-sciences/spectrogram" title="Learn more about spectrogram from ScienceDirect's AI-generated Topic Pages" class="topic-link">spectrogram</a> images, which are fed into SVM with different parameters for SER. Badshah et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0193" name="bbib0193" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0193"><span class="anchor-text-container"><span class="anchor-text">[193]</span></span></a><span> proposed an end-to-end deep CNN with three convolutional layers and three fully connected layers to extract discriminative features from <a href="/topics/physics-and-astronomy/spectrogram" title="Learn more about spectrogram from ScienceDirect's AI-generated Topic Pages" class="topic-link">spectrogram</a> images and predict seven emotions. Zhang et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0194" name="bbib0194" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0194"><span class="anchor-text-container"><span class="anchor-text">[194]</span></span></a> designed AlexNet DCNN pre-trained on ImageNet with discriminant temporal pyramid matching (DTPM) strategy to form a global utterance-level feature representation. The deep ConvNet-based models are also used to extract emotion features from raw speech input for multi-task SER <a class="anchor anchor-primary" href="#bib0195" name="bbib0195" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0195"><span class="anchor-text-container"><span class="anchor-text">[195]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0078"><strong>RNN learning for SER.</strong><span><span> RNNs can process a sequence of speech inputs and retain its state while processing the next sequence of inputs. They learn the short-time frame-level acoustic features and aggregate appropriately these features over time into an utterance-level representation. Considering the different emotion states that may exist in a long utterance, RNN and its variants (e.g. LSTM and Bi-LSTM) can tackle the uncertainty of emotional labels. <a href="/topics/computer-science/extreme-learning-machine" title="Learn more about Extreme learning machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Extreme learning machine</a> (ELM), a single-hidden </span><a href="/topics/computer-science/layer-neural-network" title="Learn more about layer neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">layer neural network</a>, was regarded as the utterance-level classifier </span><a class="anchor anchor-primary" href="#bib0196" name="bbib0196" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0196"><span class="anchor-text-container"><span class="anchor-text">[196]</span></span></a>. Lee and Tashev <a class="anchor anchor-primary" href="#bib0188" name="bbib0188" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0188"><span class="anchor-text-container"><span class="anchor-text">[188]</span></span></a> proposed the Bi-LSTM with ELM to capture a high-level representation of temporal dynamic characteristics. Ghosh et&nbsp;al. <a class="anchor anchor-primary" href="#bib0197" name="bbib0197" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0197"><span class="anchor-text-container"><span class="anchor-text">[197]</span></span></a><span><span> pre-trained a stacked <a href="/topics/computer-science/de-noising" title="Learn more about denoising from ScienceDirect's AI-generated Topic Pages" class="topic-link">denoising</a> </span><a href="/topics/computer-science/autoencoder" title="Learn more about autoencoder from ScienceDirect's AI-generated Topic Pages" class="topic-link">autoencoder</a> to extract low-dimensional distributed feature representation and trained Bi-LSTM-RNN for final emotion classification of speech sequence.</span></div><div class="u-margin-s-bottom" id="para0079">The attention model is designed to ignore irrelevantly emotional frames and other parts of the utterance. Mirsamadi et&nbsp;al. <a class="anchor anchor-primary" href="#bib0198" name="bbib0198" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0198"><span class="anchor-text-container"><span class="anchor-text">[198]</span></span></a><span> introduced an attention model with a weighted time-pooling strategy into the RNN to more emotionally <a href="/topics/computer-science/salient-region" title="Learn more about salient regions from ScienceDirect's AI-generated Topic Pages" class="topic-link">salient regions</a>. Because there exists some irrelevantly emotional silence in the speech, the silence removal needs to be employed before BLSTMs, incorporated with the attention model used for feature extraction </span><a class="anchor anchor-primary" href="#bib0190" name="bbib0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0190"><span class="anchor-text-container"><span class="anchor-text">[190]</span></span></a>. Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0199" name="bbib0199" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0199"><span class="anchor-text-container"><span class="anchor-text">[199]</span></span></a> proposed the attention-based 3D-RNNs to learn discriminative features for SER. Experimental results show that the Attention-3D-RNNs can exceed the performance of SOTA SER on IEMOCAP and Emo-DB in terms of unweighted average recall.</div><div class="u-margin-s-bottom" id="para0080">The attention model is designed to ignore irrelevant emotional frames in the utterance. Mirsamadi et&nbsp;al. <a class="anchor anchor-primary" href="#bib0198" name="bbib0198" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0198"><span class="anchor-text-container"><span class="anchor-text">[198]</span></span></a> introduced an attention model with a weighted time-pooling strategy into the RNN to highlight more emotionally salient regions. Since there exists some irrelevant emotional silence in the speech, it is necessary to combine the attention model and silence removal for feature extraction <a class="anchor anchor-primary" href="#bib0190" name="bbib0190" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0190"><span class="anchor-text-container"><span class="anchor-text">[190]</span></span></a>. Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0199" name="bbib0199" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0199"><span class="anchor-text-container"><span class="anchor-text">[199]</span></span></a> proposed the attention-based 3D-RNNs to learn discriminative features for SER, which achieved outstanding performances on IEMOCAP and Emo-DB in terms of unweighted average recall.</div><div class="u-margin-s-bottom" id="para0081"><strong>ConvNet-RNN learning for SER.</strong> As both ConvNets and RNNs have their advantages and limitations, the combination of CNNs and RNNs enables the SER system to obtain both frequency and temporal dependency <a class="anchor anchor-primary" href="#bib0191" name="bbib0191" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0191"><span class="anchor-text-container"><span class="anchor-text">[191]</span></span></a>. For example, Trigeorgis et&nbsp;al. <a class="anchor anchor-primary" href="#bib0200" name="bbib0200" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0200"><span class="anchor-text-container"><span class="anchor-text">[200]</span></span></a> proposed an end-to-end SER system consisting of CNNs and LSTMs to automatically learn the best representation of the speech signal directly from the raw time representation. In contrast, Tzirakis et&nbsp;al. <a class="anchor anchor-primary" href="#bib0201" name="bbib0201" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0201"><span class="anchor-text-container"><span class="anchor-text">[201]</span></span></a> proposed an end-to-end model comprising of CNNs and LSTM networks, which show that a deeper network is consistently more accurate than one shallow structure, with an average improvement of 10.1% and 17.9% of Arousal and Valence (A/V) on RECOLA. Wu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0202" name="bbib0202" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0202"><span class="anchor-text-container"><span class="anchor-text">[202]</span></span></a><span> designed the capsule networks (CapsNets) based SER system by integrating with the recurrent connection. The experiments employed on IEMOCAP demonstrated that CapsNets achieved 72.73% and 59.71% of <a href="/topics/computer-science/weighted-accuracy" title="Learn more about weighted accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">weighted accuracy</a> (WA) and unweighted accuracy (UA), respectively. Zhao et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0203" name="bbib0203" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0203"><span class="anchor-text-container"><span class="anchor-text">[203]</span></span></a><span> designed a spatial CNN and an attention-based BLSTM for deep spectrum feature extraction. These features were concatenated and fed into a <a href="/topics/computer-science/deep-neural-network" title="Learn more about DNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">DNN</a> to predict the final emotion.</span></div><div class="u-margin-s-bottom" id="para0082"><strong>Adversarial learning for SER.</strong><span> In SER systems, the classifiers are often exposed to training data that have a different distribution from the test data. The difference in <a href="/topics/computer-science/data-distribution" title="Learn more about data distributions from ScienceDirect's AI-generated Topic Pages" class="topic-link">data distributions</a> between the training and testing data results in severe misclassification. Abdelwahab and Busso </span><a class="anchor anchor-primary" href="#bib0192" name="bbib0192" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0192"><span class="anchor-text-container"><span class="anchor-text">[192]</span></span></a><span> proposed the domain adversarial <a href="/topics/physics-and-astronomy/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> to train gradients coming from the domain classifier, which makes the source and target domain representations closer. Sahu et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0204" name="bbib0204" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0204"><span class="anchor-text-container"><span class="anchor-text">[204]</span></span></a><span> proposed an adversarial AE for the domain of emotion recognition while maintaining the <a href="/topics/computer-science/discriminability" title="Learn more about discriminability from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminability</a> between emotion classes. Similarly, Han et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0205" name="bbib0205" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0205"><span class="anchor-text-container"><span class="anchor-text">[205]</span></span></a> proposed a conditional adversarial training framework to predict dimensional representations of emotion while distinguishing the difference between generated predictions and the ground-truth labels.</div><div class="u-margin-s-bottom" id="para0083"><span>GAN and its variants have been demonstrated that the <a href="/topics/computer-science/synthetic-data" title="Learn more about synthetic data from ScienceDirect's AI-generated Topic Pages" class="topic-link">synthetic data</a><span> generated by <a href="/topics/computer-science/generative-model" title="Learn more about generative models from ScienceDirect's AI-generated Topic Pages" class="topic-link">generative models</a> can enhance the classification performance of SER </span></span><a class="anchor anchor-primary" href="#bib0206" name="bbib0206" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0206"><span class="anchor-text-container"><span class="anchor-text">[206]</span></span></a>. To augment the emotional speech information, Bao et&nbsp;al. <a class="anchor anchor-primary" href="#bib0207" name="bbib0207" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0207"><span class="anchor-text-container"><span class="anchor-text">[207]</span></span></a> utilized Cycle consistent adversarial networks (CycleGAN) <a class="anchor anchor-primary" href="#bib0208" name="bbib0208" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0208"><span class="anchor-text-container"><span class="anchor-text">[208]</span></span></a> to generate synthetic features representing the target emotions, by learning feature vectors extracted from unlabeled source data.</div></section></section><section id="sec0024"><h3 id="cesectitle0027" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.3. Visual emotion recognition</h3><div class="u-margin-s-bottom" id="para0084">Visual emotion recognition [<a class="anchor anchor-primary" href="#bib0209" name="bbib0209" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0209"><span class="anchor-text-container"><span class="anchor-text">209</span></span></a>,<a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">12</span></span></a><span>] can be primarily categorized into facial expression recognition (FER) and body gesture emotion recognition (also known as emotional body <a href="/topics/computer-science/gesture-recognition" title="Learn more about gesture recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">gesture recognition</a>, or EBGR). The following section looks at FER and EBGR in a great deal of detail, capturing a vast body of research (well-over 100 references).</span></div><section id="sec0025"><h4 id="cesectitle0028" class="u-margin-m-top u-margin-xs-bottom">5.3.1. Facial expression recognition</h4><div class="u-margin-s-bottom" id="para0085">FER is implemented using images or videos containing facial emotional cues [<a class="anchor anchor-primary" href="#bib0210" name="bbib0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0210"><span class="anchor-text-container"><span class="anchor-text">210</span></span></a>,<a class="anchor anchor-primary" href="#bib0211" name="bbib0211" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0211"><span class="anchor-text-container"><span class="anchor-text">211</span></span></a>]. According to whether static images or dynamic videos are to be used for facial expression representation, FER systems can be divided into static-based FER <a class="anchor anchor-primary" href="#bib0212" name="bbib0212" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0212"><span class="anchor-text-container"><span class="anchor-text">[212]</span></span></a> and dynamic-based FER <a class="anchor anchor-primary" href="#bib0213" name="bbib0213" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0213"><span class="anchor-text-container"><span class="anchor-text">[213]</span></span></a>. When it comes to the duration and intensity of facial expression [<a class="anchor anchor-primary" href="#bib0214" name="bbib0214" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0214"><span class="anchor-text-container"><span class="anchor-text">214</span></span></a>,<a class="anchor anchor-primary" href="#bib0215" name="bbib0215" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0215"><span class="anchor-text-container"><span class="anchor-text">215</span></span></a>], FER can be further divided into macro-FER and micro-FER (or FMER) <a class="anchor anchor-primary" href="#bib0216" name="bbib0216" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0216"><span class="anchor-text-container"><span class="anchor-text">[216]</span></span></a>, <a class="anchor anchor-primary" href="#bib0217" name="bbib0217" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0217"><span class="anchor-text-container"><span class="anchor-text">[217]</span></span></a>, <a class="anchor anchor-primary" href="#bib0218" name="bbib0218" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0218"><span class="anchor-text-container"><span class="anchor-text">[218]</span></span></a><span>. Based on the dimensions of the <a href="/topics/computer-science/facial-image" title="Learn more about facial images from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial images</a>, macro-FER can be further grouped into 2D FER </span><a class="anchor anchor-primary" href="#bib0219" name="bbib0219" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0219"><span class="anchor-text-container"><span class="anchor-text">[219]</span></span></a>, <a class="anchor anchor-primary" href="#bib0220" name="bbib0220" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0220"><span class="anchor-text-container"><span class="anchor-text">[220]</span></span></a>, <a class="anchor anchor-primary" href="#bib0221" name="bbib0221" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0221"><span class="anchor-text-container"><span class="anchor-text">[221]</span></span></a>, and 3D/4D FER <a class="anchor anchor-primary" href="#bib0222" name="bbib0222" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0222"><span class="anchor-text-container"><span class="anchor-text">[222]</span></span></a>, <a class="anchor anchor-primary" href="#bib0223" name="bbib0223" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0223"><span class="anchor-text-container"><span class="anchor-text">[223]</span></span></a>, <a class="anchor anchor-primary" href="#bib0224" name="bbib0224" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0224"><span class="anchor-text-container"><span class="anchor-text">[224]</span></span></a>. Note that since facial images or videos suffer from a varied range of backgrounds, illuminations, and head poses, it is essential to employ pre-processing techniques (e.g., face alignment <a class="anchor anchor-primary" href="#bib0225" name="bbib0225" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0225"><span class="anchor-text-container"><span class="anchor-text">[225]</span></span></a>, face normalization <a class="anchor anchor-primary" href="#bib0226" name="bbib0226" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0226"><span class="anchor-text-container"><span class="anchor-text">[226]</span></span></a>, and pose normalization <a class="anchor anchor-primary" href="#bib0227" name="bbib0227" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0227"><span class="anchor-text-container"><span class="anchor-text">[227]</span></span></a>) to align and normalize semantic information of face region.</div><div class="u-margin-s-bottom"><div id="para0086">In this sub-section, we distinguish FER methods (shown in <a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a>) via the point of whether the features are hand-crafted features based ML models <a class="anchor anchor-primary" href="#bib0228" name="bbib0228" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0228"><span class="anchor-text-container"><span class="anchor-text">[228]</span></span></a> or high-level features based on DL-based models <a class="anchor anchor-primary" href="#bib0229" name="bbib0229" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0229"><span class="anchor-text-container"><span class="anchor-text">[229]</span></span></a>. <a class="anchor anchor-primary" href="#tbl0009" name="btbl0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0009"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;9</span></span></a> provides an overview of representative FER methods. The first column gives the publication reference (abbreviated to Pub.), followed by the publication (or preprint) year in the second column. The feature representations and classifiers of the referenced method are given in the third and fourth columns, respectively. The last six columns show the best or average results (%) with the given databases. These works are next described in detail.</div><figure class="figure text-xs" id="fig0003"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr3.jpg" height="340" alt="Fig 4" aria-describedby="cap0003"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr3_lrg.jpg" target="_blank" download="" title="Download high-res image (337KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (337KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr3.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0003"><p id="spara003"><span class="label">Fig. 4</span>. <span>Taxonomy of representative FER methods based on <a href="/topics/computer-science/machine-learning-technique" title="Learn more about ML techniques from ScienceDirect's AI-generated Topic Pages" class="topic-link">ML techniques</a><span> or <a href="/topics/computer-science/deep-learning-model" title="Learn more about DL models from ScienceDirect's AI-generated Topic Pages" class="topic-link">DL models</a>. (a) Geometry-based FER adopted from </span></span><a class="anchor anchor-primary" href="#bib0230" name="bbib0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0230"><span class="anchor-text-container"><span class="anchor-text">[230]</span></span></a>; (b) Appearance-based FER adopted from <a class="anchor anchor-primary" href="#bib0235" name="bbib0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0235"><span class="anchor-text-container"><span class="anchor-text">[235]</span></span></a>; (c) Feature fusion for 3D FER adopted from <a class="anchor anchor-primary" href="#bib0236" name="bbib0236" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0236"><span class="anchor-text-container"><span class="anchor-text">[236]</span></span></a>; (d) Feature selection for FMER adopted from <a class="anchor anchor-primary" href="#bib0237" name="bbib0237" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0237"><span class="anchor-text-container"><span class="anchor-text">[237]</span></span></a><span>; (e) <a href="/topics/neuroscience/neural-network" title="Learn more about ConvNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ConvNet</a> learning for FMER adopted from </span><a class="anchor anchor-primary" href="#bib0218" name="bbib0218" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0218"><span class="anchor-text-container"><span class="anchor-text">[218]</span></span></a>; (f) ConvNet-RNN learning for FER adopted from <a class="anchor anchor-primary" href="#bib0238" name="bbib0238" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0238"><span class="anchor-text-container"><span class="anchor-text">[238]</span></span></a>; (g) Adversarial learning for 3D FER adopted from <a class="anchor anchor-primary" href="#bib0239" name="bbib0239" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0239"><span class="anchor-text-container"><span class="anchor-text">[239]</span></span></a>.</p></span></span></figure><figure class="figure text-xs" id="fig0004"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr4.jpg" height="915" alt="Fig 3" aria-describedby="cap0004"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr4_lrg.jpg" target="_blank" download="" title="Download high-res image (2MB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (2MB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr4.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0004"><p id="spara004"><span class="label">Fig. 3</span>. Dimensional emotion models. (a) Pleasure-Arousal-Dominance (PAD) model reproduced based on <a class="anchor anchor-primary" href="#bib0061" name="bbib0061" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0061"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a> and (b) Valence-Arousal (V-A) model reproduced based on <a class="anchor anchor-primary" href="#bib0054" name="bbib0054" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0054"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a>.</p></span></span></figure><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0009"><span class="captions text-s"><span id="cap0015"><p id="spara016"><span class="label">Table 9</span>. Overview of the representative FER methods (publicly used databases).</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en0984">Pub.</th><th scope="col" class="align-left valign-top" id="en0985">Year</th><th scope="col" class="align-left valign-top" id="en0986">Feature Representation</th><th scope="col" class="align-left valign-top" id="en0987">Classifier</th><th scope="col" class="align-left valign-top" id="en0988">CK+</th><th scope="col" class="align-left valign-top" id="en0989">Oulu-CASIA</th><th scope="col" class="align-left valign-top" id="en0990">SFEW</th><th scope="col" class="align-left valign-top" id="en0991">BU-4DFE</th><th scope="col" class="align-left valign-top" id="en0992">CASME II</th><th scope="col" class="align-left valign-top" id="en0993">SMIC</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en0994" colspan="10"><strong><em>ML-based FER</em></strong></td></tr><tr><td class="align-left valign-top" id="en0995"><a class="anchor anchor-primary" href="#bib0230" name="bbib0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0230"><span class="anchor-text-container"><span class="anchor-text">[230]</span></span></a></td><td class="align-left valign-top" id="en0996">2013</td><td class="align-left valign-top" id="en0997">FLP</td><td class="align-left valign-top" id="en0998">SVM</td><td class="align-left valign-top" id="en0999">97.35</td><td class="align-left valign-top" id="en1000">/</td><td class="align-left valign-top" id="en1001">/</td><td class="align-left valign-top" id="en1002">/</td><td class="align-left valign-top" id="en1003">/</td><td class="align-left valign-top" id="en1004">/</td></tr><tr><td class="align-left valign-top" id="en1005"><a class="anchor anchor-primary" href="#bib0234" name="bbib0234" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0234"><span class="anchor-text-container"><span class="anchor-text">[234]</span></span></a></td><td class="align-left valign-top" id="en1006">2019</td><td class="align-left valign-top" id="en1007">LPDP</td><td class="align-left valign-top" id="en1008">SVM</td><td class="align-left valign-top" id="en1009">94.50</td><td class="align-left valign-top" id="en1010">/</td><td class="align-left valign-top" id="en1011">/</td><td class="align-left valign-top" id="en1012">73.40</td><td class="align-left valign-top" id="en1013">/</td><td class="align-left valign-top" id="en1014">/</td></tr><tr><td class="align-left valign-top" id="en1015"><a class="anchor anchor-primary" href="#bib0243" name="bbib0243" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0243"><span class="anchor-text-container"><span class="anchor-text">[243]</span></span></a></td><td class="align-left valign-top" id="en1016">2012</td><td class="align-left valign-top" id="en1017">MSGF</td><td class="align-left valign-top" id="en1018">KNN</td><td class="align-left valign-top" id="en1019">91.51</td><td class="align-left valign-top" id="en1020">/</td><td class="align-left valign-top" id="en1021">/</td><td class="align-left valign-top" id="en1022">/</td><td class="align-left valign-top" id="en1023">/</td><td class="align-left valign-top" id="en1024">/</td></tr><tr><td class="align-left valign-top" id="en1025"><a class="anchor anchor-primary" href="#bib0235" name="bbib0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0235"><span class="anchor-text-container"><span class="anchor-text">[235]</span></span></a></td><td class="align-left valign-top" id="en1026">2020</td><td class="align-left valign-top" id="en1027">IFSL</td><td class="align-left valign-top" id="en1028">SVM</td><td class="align-left valign-top" id="en1029">98.70</td><td class="align-left valign-top" id="en1030">/</td><td class="align-left valign-top" id="en1031">46.50</td><td class="align-left valign-top" id="en1032">/</td><td class="align-left valign-top" id="en1033">/</td><td class="align-left valign-top" id="en1034">/</td></tr><tr><td class="align-left valign-top" id="en1035"><a class="anchor anchor-primary" href="#bib0247" name="bbib0247" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0247"><span class="anchor-text-container"><span class="anchor-text">[247]</span></span></a></td><td class="align-left valign-top" id="en1036">2018</td><td class="align-left valign-top" id="en1037">OSF+LBP-TOP</td><td class="align-left valign-top" id="en1038">SVM</td><td class="align-left valign-top" id="en1039">/</td><td class="align-left valign-top" id="en1040">/</td><td class="align-left valign-top" id="en1041">/</td><td class="align-left valign-top" id="en1042">/</td><td class="align-left valign-top" id="en1043"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>41.74/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>61.82</td><td class="align-left valign-top" id="en1044"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>50.79/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>62.74</td></tr><tr><td class="align-left valign-top" id="en1045"><a class="anchor anchor-primary" href="#bib0245" name="bbib0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0245"><span class="anchor-text-container"><span class="anchor-text">[245]</span></span></a></td><td class="align-left valign-top" id="en1046">2015</td><td class="align-left valign-top" id="en1047">LBP-MOP</td><td class="align-left valign-top" id="en1048">SVM</td><td class="align-left valign-top" id="en1049">/</td><td class="align-left valign-top" id="en1050">/</td><td class="align-left valign-top" id="en1051">/</td><td class="align-left valign-top" id="en1052">/</td><td class="align-left valign-top" id="en1053"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>45.75/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>66.80</td><td class="align-left valign-top" id="en1054"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>50.61/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>60.98</td></tr><tr><td class="align-left valign-top" id="en1055"><a class="anchor anchor-primary" href="#bib0245" name="bbib0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0245"><span class="anchor-text-container"><span class="anchor-text">[245]</span></span></a></td><td class="align-left valign-top" id="en1056">2015</td><td class="align-left valign-top" id="en1057">LBP-SIP</td><td class="align-left valign-top" id="en1058">SVM</td><td class="align-left valign-top" id="en1059">/</td><td class="align-left valign-top" id="en1060">/</td><td class="align-left valign-top" id="en1061">/</td><td class="align-left valign-top" id="en1062">/</td><td class="align-left valign-top" id="en1063"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>44.53/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>66.40</td><td class="align-left valign-top" id="en1064"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>50.00/<a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>64.02</td></tr><tr><td class="align-left valign-top" id="en1065"><a class="anchor anchor-primary" href="#bib0236" name="bbib0236" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0236"><span class="anchor-text-container"><span class="anchor-text">[236]</span></span></a></td><td class="align-left valign-top" id="en1066">2018</td><td class="align-left valign-top" id="en1067">2D+3D maps</td><td class="align-left valign-top" id="en1068">MKL</td><td class="align-left valign-top" id="en1069">/</td><td class="align-left valign-top" id="en1070">/</td><td class="align-left valign-top" id="en1071">/</td><td class="align-left valign-top" id="en1072">90.12</td><td class="align-left valign-top" id="en1073">/</td><td class="align-left valign-top" id="en1074">/</td></tr><tr><td class="align-left valign-top" id="en1075"><a class="anchor anchor-primary" href="#bib0249" name="bbib0249" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0249"><span class="anchor-text-container"><span class="anchor-text">[249]</span></span></a></td><td class="align-left valign-top" id="en1076">2019</td><td class="align-left valign-top" id="en1077">DSF</td><td class="align-left valign-top" id="en1078">HMM</td><td class="align-left valign-top" id="en1079">/</td><td class="align-left valign-top" id="en1080">/</td><td class="align-left valign-top" id="en1081">/</td><td class="align-left valign-top" id="en1082">95.13</td><td class="align-left valign-top" id="en1083">/</td><td class="align-left valign-top" id="en1084">/</td></tr><tr><td class="align-left valign-top" id="en1085"><a class="anchor anchor-primary" href="#bib0253" name="bbib0253" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0253"><span class="anchor-text-container"><span class="anchor-text">[253]</span></span></a></td><td class="align-left valign-top" id="en1086">2015</td><td class="align-left valign-top" id="en1087">DE</td><td class="align-left valign-top" id="en1088">SVM</td><td class="align-left valign-top" id="en1089">/</td><td class="align-left valign-top" id="en1090">/</td><td class="align-left valign-top" id="en1091">/</td><td class="align-left valign-top" id="en1092">85.81/<a class="anchor anchor-primary" href="#tb9fn3" name="btb9fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>84.00</td><td class="align-left valign-top" id="en1093">/</td><td class="align-left valign-top" id="en1094">/</td></tr><tr><td class="align-left valign-top" id="en1095"><a class="anchor anchor-primary" href="#bib0216" name="bbib0216" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0216"><span class="anchor-text-container"><span class="anchor-text">[216]</span></span></a></td><td class="align-left valign-top" id="en1096">2016</td><td class="align-left valign-top" id="en1097">MDMO</td><td class="align-left valign-top" id="en1098">SVM</td><td class="align-left valign-top" id="en1099">/</td><td class="align-left valign-top" id="en1100">/</td><td class="align-left valign-top" id="en1101">/</td><td class="align-left valign-top" id="en1102">/</td><td class="align-left valign-top" id="en1103"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>67.37</td><td class="align-left valign-top" id="en1104"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>80.00</td></tr><tr><td class="align-left valign-top" id="en1105"><a class="anchor anchor-primary" href="#bib0237" name="bbib0237" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0237"><span class="anchor-text-container"><span class="anchor-text">[237]</span></span></a></td><td class="align-left valign-top" id="en1106">2018</td><td class="align-left valign-top" id="en1107">HSDS</td><td class="align-left valign-top" id="en1108">KGSL</td><td class="align-left valign-top" id="en1109">/</td><td class="align-left valign-top" id="en1110">/</td><td class="align-left valign-top" id="en1111">/</td><td class="align-left valign-top" id="en1112">/</td><td class="align-left valign-top" id="en1113"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>65.18</td><td class="align-left valign-top" id="en1114"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>66.46</td></tr><tr><td class="align-left valign-top" id="en1115" colspan="10"><strong><em>DL-based FER</em></strong></td></tr><tr><td class="align-left valign-top" id="en1116"><a class="anchor anchor-primary" href="#bib0272" name="bbib0272" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0272"><span class="anchor-text-container"><span class="anchor-text">[272]</span></span></a></td><td class="align-left valign-top" id="en1117">2018</td><td class="align-left valign-top" id="en1118">FLM-CNN</td><td class="align-left valign-top" id="en1119">FC</td><td class="align-left valign-top" id="en1120">/</td><td class="align-left valign-top" id="en1121">/</td><td class="align-left valign-top" id="en1122">/</td><td class="align-left valign-top" id="en1123"><a class="anchor anchor-primary" href="#tb9fn4" name="btb9fn4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn4"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a>83.32</td><td class="align-left valign-top" id="en1124">/</td><td class="align-left valign-top" id="en1125">/</td></tr><tr><td class="align-left valign-top" id="en1126"><a class="anchor anchor-primary" href="#bib0280" name="bbib0280" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0280"><span class="anchor-text-container"><span class="anchor-text">[280]</span></span></a></td><td class="align-left valign-top" id="en1127">2020</td><td class="align-left valign-top" id="en1128">GCN-CNN</td><td class="align-left valign-top" id="en1129">FC</td><td class="align-left valign-top" id="en1130">/</td><td class="align-left valign-top" id="en1131">/</td><td class="align-left valign-top" id="en1132">/</td><td class="align-left valign-top" id="en1133">/</td><td class="align-left valign-top" id="en1134"><a class="anchor anchor-primary" href="#tb9fn2" name="btb9fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>42.70</td><td class="align-left valign-top" id="en1135">/</td></tr><tr><td class="align-left valign-top" id="en1136"><a class="anchor anchor-primary" href="#bib0263" name="bbib0263" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0263"><span class="anchor-text-container"><span class="anchor-text">[263]</span></span></a></td><td class="align-left valign-top" id="en1137">2017</td><td class="align-left valign-top" id="en1138">IACNN</td><td class="align-left valign-top" id="en1139">FC</td><td class="align-left valign-top" id="en1140">95.37</td><td class="align-left valign-top" id="en1141">/</td><td class="align-left valign-top" id="en1142">54.30</td><td class="align-left valign-top" id="en1143">/</td><td class="align-left valign-top" id="en1144">/</td><td class="align-left valign-top" id="en1145">/</td></tr><tr><td class="align-left valign-top" id="en1146"><a class="anchor anchor-primary" href="#bib0270" name="bbib0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0270"><span class="anchor-text-container"><span class="anchor-text">[270]</span></span></a></td><td class="align-left valign-top" id="en1147">2021</td><td class="align-left valign-top" id="en1148">SCAN</td><td class="align-left valign-top" id="en1149">FC</td><td class="align-left valign-top" id="en1150">97.31</td><td class="align-left valign-top" id="en1151">89.11</td><td class="align-left valign-top" id="en1152">58.93</td><td class="align-left valign-top" id="en1153">/</td><td class="align-left valign-top" id="en1154">/</td><td class="align-left valign-top" id="en1155">/</td></tr><tr><td class="align-left valign-top" id="en1156"><a class="anchor anchor-primary" href="#bib0238" name="bbib0238" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0238"><span class="anchor-text-container"><span class="anchor-text">[238]</span></span></a></td><td class="align-left valign-top" id="en1157">2017</td><td class="align-left valign-top" id="en1158">CNN-RNN</td><td class="align-left valign-top" id="en1159">Fusion</td><td class="align-left valign-top" id="en1160">98.50</td><td class="align-left valign-top" id="en1161">86.25</td><td class="align-left valign-top" id="en1162">/</td><td class="align-left valign-top" id="en1163">/</td><td class="align-left valign-top" id="en1164">/</td><td class="align-left valign-top" id="en1165">/</td></tr><tr><td class="align-left valign-top" id="en1166"><a class="anchor anchor-primary" href="#bib0286" name="bbib0286" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0286"><span class="anchor-text-container"><span class="anchor-text">[286]</span></span></a></td><td class="align-left valign-top" id="en1167">2020</td><td class="align-left valign-top" id="en1168">CNN-LSTM</td><td class="align-left valign-top" id="en1169">Pooling</td><td class="align-left valign-top" id="en1170">99.54</td><td class="align-left valign-top" id="en1171">88.33</td><td class="align-left valign-top" id="en1172">54.56</td><td class="align-left valign-top" id="en1173">/</td><td class="align-left valign-top" id="en1174">/</td><td class="align-left valign-top" id="en1175">/</td></tr><tr><td class="align-left valign-top" id="en1176"><a class="anchor anchor-primary" href="#bib0282" name="bbib0282" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0282"><span class="anchor-text-container"><span class="anchor-text">[282]</span></span></a></td><td class="align-left valign-top" id="en1177">2021</td><td class="align-left valign-top" id="en1178">CNN-LSTM</td><td class="align-left valign-top" id="en1179"><a class="anchor anchor-primary" href="#tb9fn6" name="btb9fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>Col-Cla</td><td class="align-left valign-top" id="en1180">/</td><td class="align-left valign-top" id="en1181">/</td><td class="align-left valign-top" id="en1182">/</td><td class="align-left valign-top" id="en1183">99.69</td><td class="align-left valign-top" id="en1184">/</td><td class="align-left valign-top" id="en1185">/</td></tr><tr><td class="align-left valign-top" id="en1186"><a class="anchor anchor-primary" href="#bib0284" name="bbib0284" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0284"><span class="anchor-text-container"><span class="anchor-text">[284]</span></span></a></td><td class="align-left valign-top" id="en1187">2020</td><td class="align-left valign-top" id="en1188">STRCN</td><td class="align-left valign-top" id="en1189">Pooling</td><td class="align-left valign-top" id="en1190">/</td><td class="align-left valign-top" id="en1191">/</td><td class="align-left valign-top" id="en1192">/</td><td class="align-left valign-top" id="en1193">/</td><td class="align-left valign-top" id="en1194"><a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>84.10</td><td class="align-left valign-top" id="en1195"><a class="anchor anchor-primary" href="#tb9fn1" name="btb9fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>75.80</td></tr><tr><td class="align-left valign-top" id="en1196"><a class="anchor anchor-primary" href="#bib0290" name="bbib0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0290"><span class="anchor-text-container"><span class="anchor-text">[290]</span></span></a></td><td class="align-left valign-top" id="en1197">2019</td><td class="align-left valign-top" id="en1198">DE-GAN</td><td class="align-left valign-top" id="en1199">SVM/MLP</td><td class="align-left valign-top" id="en1200">97.28</td><td class="align-left valign-top" id="en1201">89.17</td><td class="align-left valign-top" id="en1202">/</td><td class="align-left valign-top" id="en1203">/</td><td class="align-left valign-top" id="en1204">/</td><td class="align-left valign-top" id="en1205">/</td></tr><tr><td class="align-left valign-top" id="en1206"><a class="anchor anchor-primary" href="#bib0289" name="bbib0289" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0289"><span class="anchor-text-container"><span class="anchor-text">[289]</span></span></a></td><td class="align-left valign-top" id="en1207">2020</td><td class="align-left valign-top" id="en1208">SNA</td><td class="align-left valign-top" id="en1209">FC</td><td class="align-left valign-top" id="en1210">98.58</td><td class="align-left valign-top" id="en1211">87.60</td><td class="align-left valign-top" id="en1212">/</td><td class="align-left valign-top" id="en1213">/</td><td class="align-left valign-top" id="en1214">/</td><td class="align-left valign-top" id="en1215">/</td></tr><tr><td class="align-left valign-top" id="en1216"><a class="anchor anchor-primary" href="#bib0291" name="bbib0291" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0291"><span class="anchor-text-container"><span class="anchor-text">[291]</span></span></a></td><td class="align-left valign-top" id="en1217">2020</td><td class="align-left valign-top" id="en1218">ICE-GAN</td><td class="align-left valign-top" id="en1219">CED</td><td class="align-left valign-top" id="en1220">/</td><td class="align-left valign-top" id="en1221">/</td><td class="align-left valign-top" id="en1222">/</td><td class="align-left valign-top" id="en1223">/</td><td class="align-left valign-top" id="en1224"><a class="anchor anchor-primary" href="#tb9fn5" name="btb9fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>86.80</td><td class="align-left valign-top" id="en1225"><a class="anchor anchor-primary" href="#tb9fn5" name="btb9fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb9fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>79.10</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb9fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0034">Leave-one-subject-out (LOSO);</div></dd><dt id="tb9fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0035">Leave-one-video-out (LOVO);</div></dd><dt id="tb9fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0036">BU-3DFE/ Bosphorus;</div></dd><dt id="tb9fn4">4</dt><dd><div class="u-margin-s-bottom" id="notep0037">The average accuracy of BU-3DFE under both the protocols, P1 and P2;</div></dd><dt id="tb9fn5">5</dt><dd><div class="u-margin-s-bottom" id="notep0038">Unweighted Average Recall (UAR);</div></dd><dt id="tb9fn6">6</dt><dd><div class="u-margin-s-bottom" id="notep0039">Col-Cla=Collaborative Classification.</div></dd></dl></div></div><section id="sec0026"><h5 id="cesectitle0029" class="u-margin-m-top u-margin-xs-bottom">5.3.1.1. ML-based FER</h5><div class="u-margin-s-bottom" id="para0087"><span>The ML-based FER methods mainly rely on hand-crafted feature extraction and feature post-processing. Generally, hand-crafted <a href="/topics/computer-science/facial-feature" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a><span> can be categorized into geometry-based features explaining the shape of the face and its components, and appearance-based <a href="/topics/computer-science/defining-feature" title="Learn more about features defining from ScienceDirect's AI-generated Topic Pages" class="topic-link">features defining</a> facial texture. The feature post-processing can be divided into two types: feature fusion and feature selection </span></span><a class="anchor anchor-primary" href="#bib0018" name="bbib0018" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0018"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0088"><strong>Geometry-based FER.</strong> Ghimire and Lee <a class="anchor anchor-primary" href="#bib0230" name="bbib0230" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0230"><span class="anchor-text-container"><span class="anchor-text">[230]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> (a)) utilized 52 facial landmark points (FLP), which represent features of geometric positions and angles, for automatic FER in facial sequences. Sujono and Gunawan <a class="anchor anchor-primary" href="#bib0231" name="bbib0231" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0231"><span class="anchor-text-container"><span class="anchor-text">[231]</span></span></a> used the Kinect motion sensor to detect the face region based on depth information and active shape model (AAM) <a class="anchor anchor-primary" href="#bib0232" name="bbib0232" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0232"><span class="anchor-text-container"><span class="anchor-text">[232]</span></span></a><span>. The change of key features in AAM and a fuzzy logic model is utilized to recognize facial expression based on prior knowledge derived from the <a href="/topics/computer-science/facial-action-coding-system" title="Learn more about facial action coding system from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial action coding system</a> (FACS) </span><a class="anchor anchor-primary" href="#bib0233" name="bbib0233" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0233"><span class="anchor-text-container"><span class="anchor-text">[233]</span></span></a>. As the geometry of different local structures with distortions has unstable shape representations, the local prominent directional pattern descriptor (LPDP) <a class="anchor anchor-primary" href="#bib0234" name="bbib0234" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0234"><span class="anchor-text-container"><span class="anchor-text">[234]</span></span></a> was proposed by using statistical information of a pixel neighbourhood to encode more meaningful and reliable information.</div><div class="u-margin-s-bottom" id="para0089"><strong>Appearance-based FER.</strong> Appearance-based approaches often extract and analyze spatial information or spatial-temporal information from the whole or specific facial regions <a class="anchor anchor-primary" href="#bib0240" name="bbib0240" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0240"><span class="anchor-text-container"><span class="anchor-text">[240]</span></span></a>. Tian et&nbsp;al. <a class="anchor anchor-primary" href="#bib0210" name="bbib0210" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0210"><span class="anchor-text-container"><span class="anchor-text">[210]</span></span></a> designed an automatic face analysis system that captured fine-grained changes of facial expression into AUs <a class="anchor anchor-primary" href="#bib0241" name="bbib0241" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0241"><span class="anchor-text-container"><span class="anchor-text">[241]</span></span></a><span><span> of FACS by using the permanent and transient <a href="/topics/medicine-and-dentistry/facies" title="Learn more about facial features from ScienceDirect's AI-generated Topic Pages" class="topic-link">facial features</a> extracted by the Gabor wavelet, </span><a href="/topics/computer-science/scale-invariant-feature-transform" title="Learn more about SIFT from ScienceDirect's AI-generated Topic Pages" class="topic-link">SIFT</a> and local binary pattern (LBP) </span><a class="anchor anchor-primary" href="#bib0242" name="bbib0242" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0242"><span class="anchor-text-container"><span class="anchor-text">[242]</span></span></a>. Gu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0243" name="bbib0243" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0243"><span class="anchor-text-container"><span class="anchor-text">[243]</span></span></a> divided one facial image into several local regions by grids, then applied multi-scale Gabor-filter operations on local blocks, and finally encoded the mean intensity of each feature map. Yan et&nbsp;al. <a class="anchor anchor-primary" href="#bib0235" name="bbib0235" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0235"><span class="anchor-text-container"><span class="anchor-text">[235]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a><span> (b)) proposed a framework of low-resolution FER based on image filter-based subspace learning (IFSL), including deriving discriminative image filters (DIFs), their combination, and an expression-aware <a href="/topics/computer-science/transformation-matrix" title="Learn more about transformation matrix from ScienceDirect's AI-generated Topic Pages" class="topic-link">transformation matrix</a>.</span></div><div class="u-margin-s-bottom" id="para0090">The local binary pattern from three orthogonal planes (LBP-TOP) <a class="anchor anchor-primary" href="#bib0244" name="bbib0244" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0244"><span class="anchor-text-container"><span class="anchor-text">[244]</span></span></a> is an extension of LBP <a class="anchor anchor-primary" href="#bib0242" name="bbib0242" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0242"><span class="anchor-text-container"><span class="anchor-text">[242]</span></span></a> computes over three orthogonal planes at each bin of a 3D volume formed by stacking the frames. The LBP-TOP and its variants have shown a promising performance on dynamic FER. For example, Wang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0245" name="bbib0245" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0245"><span class="anchor-text-container"><span class="anchor-text">[245]</span></span></a><span> designed two kinds of feature extractors (LBP-six <a href="/topics/computer-science/intersection-point" title="Learn more about intersection points from ScienceDirect's AI-generated Topic Pages" class="topic-link">intersection points</a> (LBP-SIP) and super-compact LBP-three mean orthogonal planes (LBP-MOP)) based on the improved LBP-TOP to preserve the essential patterns while reducing the redundancy. Davison et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0246" name="bbib0246" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0246"><span class="anchor-text-container"><span class="anchor-text">[246]</span></span></a><span> employed LBP-TOP features and <a href="/topics/physics-and-astronomy/gaussian-distribution" title="Learn more about Gaussian from ScienceDirect's AI-generated Topic Pages" class="topic-link">Gaussian</a> derivatives features for FEMR. Similarly, Liong et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0247" name="bbib0247" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0247"><span class="anchor-text-container"><span class="anchor-text">[247]</span></span></a> designed a framework of FMER based on two kinds of feature extractors, consisting of optical strain flow (OSF) and block-based LBP-TOP.</div><div class="u-margin-s-bottom" id="para0091"><strong>Feature fusion for FER.</strong> It has been proved to fuse different types of geometry-based features and appearance-based features to enhance the robustness of FER <a class="anchor anchor-primary" href="#bib0236" name="bbib0236" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0236"><span class="anchor-text-container"><span class="anchor-text">[236]</span></span></a>. For example, Majumder et&nbsp;al. <a class="anchor anchor-primary" href="#bib0219" name="bbib0219" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0219"><span class="anchor-text-container"><span class="anchor-text">[219]</span></span></a> designed an automatic FER system based on the deep fusion of geometric features and LBP features using autoencoders. For the FMER task, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0248" name="bbib0248" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0248"><span class="anchor-text-container"><span class="anchor-text">[248]</span></span></a> proposed the aggregating local spatiotemporal patterns (ALSTP), which adopts cascaded fusion of local LBP-TOP and LOF extracted from 9 representative local regions of the face. For 3D/4D facial expression images, Zhen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0249" name="bbib0249" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0249"><span class="anchor-text-container"><span class="anchor-text">[249]</span></span></a> computed spatial facial deformations using a Riemannian based on dense scalar fields (DSF) and magnified them by a temporal filtering technique. For 2D/3D facial expression images, Yao et&nbsp;al. <a class="anchor anchor-primary" href="#bib0236" name="bbib0236" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0236"><span class="anchor-text-container"><span class="anchor-text">[236]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a><span><span> (c)) used the MKL fusion strategy to combine 2D <a href="/topics/computer-science/texture-feature" title="Learn more about texture features from ScienceDirect's AI-generated Topic Pages" class="topic-link">texture features</a>, 3D shape features, and their corresponding </span><a href="/topics/computer-science/fourier-transform" title="Learn more about Fourier transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Fourier transform</a> maps of different face regions.</span></div><div class="u-margin-s-bottom" id="para0092"><strong>Feature selection for FER.</strong> Although more 3D/4D features <a class="anchor anchor-primary" href="#bib0250" name="bbib0250" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0250"><span class="anchor-text-container"><span class="anchor-text">[250]</span></span></a> or dynamic features <a class="anchor anchor-primary" href="#bib0251" name="bbib0251" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0251"><span class="anchor-text-container"><span class="anchor-text">[251]</span></span></a> have different effects on FER <a class="anchor anchor-primary" href="#bib0252" name="bbib0252" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0252"><span class="anchor-text-container"><span class="anchor-text">[252]</span></span></a><span>, excessive&nbsp;features may break the <a href="/topics/computer-science/predictive-model" title="Learn more about predictive model from ScienceDirect's AI-generated Topic Pages" class="topic-link">predictive model</a>. The feature selection is to choose a relevant and useful subset of the given set of features while identifying and removing redundant attributes. To overcome the high-dimensionality problem of 3D facial features, Azazi et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0253" name="bbib0253" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0253"><span class="anchor-text-container"><span class="anchor-text">[253]</span></span></a> firstly transformed the 3D faces into 2D planes using conformal mapping, and then proposed the differential evolution (DE) to select the optimal facial feature set and SVM classifier parameters, simultaneously. Savran and Sankur <a class="anchor anchor-primary" href="#bib0254" name="bbib0254" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0254"><span class="anchor-text-container"><span class="anchor-text">[254]</span></span></a> investigated the model-free 3D FER based on the non-rigid registration by selecting the most discriminative feature points from 3D facial images. As different facial regions contributed different to micro-expressions, Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0255" name="bbib0255" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0255"><span class="anchor-text-container"><span class="anchor-text">[255]</span></span></a><span> utilized weighted 3DHOG features and weighted <a href="/topics/computer-science/fuzzy-classification" title="Learn more about fuzzy classification from ScienceDirect's AI-generated Topic Pages" class="topic-link">fuzzy classification</a><span> for FMER. Different from the <a href="/topics/computer-science/spatial-division" title="Learn more about spatial division from ScienceDirect's AI-generated Topic Pages" class="topic-link">spatial division</a> with fixed grid, a hierarchical spatial division scheme (HSDS) </span></span><a class="anchor anchor-primary" href="#bib0237" name="bbib0237" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0237"><span class="anchor-text-container"><span class="anchor-text">[237]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> (d)) was proposed to generate multiple types of gradually denser grids and designed kernelized group sparse learning (KGSL) to learn a set of importance weights.</div></section><section id="sec0027"><h5 id="cesectitle0030" class="u-margin-m-top u-margin-xs-bottom">5.3.1.2. DL-based FER</h5><div class="u-margin-s-bottom" id="para0093"><span>The <a href="/topics/computer-science/backbone-network" title="Learn more about backbone networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">backbone networks</a> of DL-based FER are mostly derived from well-known pre-trained ConvNets such as VGG </span><a class="anchor anchor-primary" href="#bib0256" name="bbib0256" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0256"><span class="anchor-text-container"><span class="anchor-text">[256]</span></span></a>, VGG-face <a class="anchor anchor-primary" href="#bib0257" name="bbib0257" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0257"><span class="anchor-text-container"><span class="anchor-text">[257]</span></span></a><span>, <a href="/topics/computer-science/residual-neural-network" title="Learn more about ResNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">ResNet</a> </span><a class="anchor anchor-primary" href="#bib0258" name="bbib0258" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0258"><span class="anchor-text-container"><span class="anchor-text">[258]</span></span></a>, and GoogLeNet <a class="anchor anchor-primary" href="#bib0259" name="bbib0259" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0259"><span class="anchor-text-container"><span class="anchor-text">[259]</span></span></a><span>. Thus, we divide DL-based FER into ConvNet learning for FER, ConvNet-RNN learning for FER, and adversarial learning for FER considering the difference of <a href="/topics/computer-science/network-architecture" title="Learn more about network architectures from ScienceDirect's AI-generated Topic Pages" class="topic-link">network architectures</a>.</span></div><div class="u-margin-s-bottom" id="para0094"><strong>ConvNet learning for FER</strong><span>. ConvNet-based FER often <a href="/topics/computer-science/design-transforms" title="Learn more about design transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">design transform</a> learning or loss function </span><a class="anchor anchor-primary" href="#bib0221" name="bbib0221" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0221"><span class="anchor-text-container"><span class="anchor-text">[221]</span></span></a> to overcome overfitting when using relatively small facial expression databases. For example, Yang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0260" name="bbib0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0260"><span class="anchor-text-container"><span class="anchor-text">[260]</span></span></a> proposed a de-expression residue learning (DeRL) to recognize facial expressions by extracting expressive information from one facial expression image. For FMER, the transform-learning based model is pre-trained on the ImageNet and several popular macro-expression databases with the original residual network <a class="anchor anchor-primary" href="#bib0261" name="bbib0261" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0261"><span class="anchor-text-container"><span class="anchor-text">[261]</span></span></a>. Su et&nbsp;al. <a class="anchor anchor-primary" href="#bib0218" name="bbib0218" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0218"><span class="anchor-text-container"><span class="anchor-text">[218]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a><span> (e)) proposed a novel knowledge transfer technique, which comprised a pre-trained deep teacher <a href="/topics/psychology/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a><span> and a shallow student <a href="/topics/computer-science/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a>. Specifically, the AU-based model is trained on the residual network, which is then distilled and transferred for FMER. Liu et&nbsp;al. </span></span><a class="anchor anchor-primary" href="#bib0262" name="bbib0262" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0262"><span class="anchor-text-container"><span class="anchor-text">[262]</span></span></a> developed an identity-disentangled FER by integrating the hard negative generation with the radial metric learning (RML). Specifically, the RML module combined inception-structured convolutional groups with an expression classification branch for the final emotion recognition by minimizing both the cross-entropy loss and RML loss. To alleviate variations introduced by personal attributes, Meng et&nbsp;al. <a class="anchor anchor-primary" href="#bib0263" name="bbib0263" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0263"><span class="anchor-text-container"><span class="anchor-text">[263]</span></span></a><span> proposed an identity-aware <a href="/topics/physics-and-astronomy/convolutional-neural-network" title="Learn more about convolutional neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">convolutional neural network</a> (IACNN) consisting of two identical sub-CNNs with shared weights. Besides, they designed two losses for identity-invariant FER: expression-sensitive loss and identity-sensitive loss.</span></div><div class="u-margin-s-bottom" id="para0095">To highlight the most helpful information of facial images, various attention mechanisms [<a class="anchor anchor-primary" href="#bib0264" name="bbib0264" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0264"><span class="anchor-text-container"><span class="anchor-text">264</span></span></a>,<a class="anchor anchor-primary" href="#bib0265" name="bbib0265" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0265"><span class="anchor-text-container"><span class="anchor-text">265</span></span></a>] are proposed to discriminate distinctive features. For example, Fernandez et&nbsp;al. <a class="anchor anchor-primary" href="#bib0266" name="bbib0266" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0266"><span class="anchor-text-container"><span class="anchor-text">[266]</span></span></a> proposed an attention network and embedded it into an encoder-decoder architecture for the facial expression representation. Similarly, Xie et&nbsp;al <a class="anchor anchor-primary" href="#bib0267" name="bbib0267" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0267"><span class="anchor-text-container"><span class="anchor-text">[267]</span></span></a> proposed an attention-based salient expressional region descriptor (SERD) to locate the most expression-related regions that are beneficial to FER. To reduce the uncertainties of FER, Wang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0268" name="bbib0268" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0268"><span class="anchor-text-container"><span class="anchor-text">[268]</span></span></a><span> proposed the self-cure network consisting of the self-attention importance weighting module, the ranking <a href="/topics/computer-science/regularization" title="Learn more about regularization from ScienceDirect's AI-generated Topic Pages" class="topic-link">regularization</a> module, and the relabeling module. Zhu et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0269" name="bbib0269" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0269"><span class="anchor-text-container"><span class="anchor-text">[269]</span></span></a><span> proposed a novel discriminative attention-based CNN, where the attention module was used to emphasize the unequal contributions of features for different expressions, and a dimensional distribution loss was designed to model the inter-expression relationship. To provide local-global attention across the channel and <a href="/topics/computer-science/spatial-location" title="Learn more about spatial location from ScienceDirect's AI-generated Topic Pages" class="topic-link">spatial location</a> for feature maps, Gera and Balasubramanian </span><a class="anchor anchor-primary" href="#bib0270" name="bbib0270" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0270"><span class="anchor-text-container"><span class="anchor-text">[270]</span></span></a> proposed the spatial-channel attention net (SCAN). Besides, the complementary context information (CCI) branch is proposed to enhance the discriminating ability by integrating with another channel-wise attention.</div><div class="u-margin-s-bottom" id="para0096">Except for the efficient network architectures with attention networks or loss functions <a class="anchor anchor-primary" href="#bib0271" name="bbib0271" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0271"><span class="anchor-text-container"><span class="anchor-text">[271]</span></span></a>, there is a fast and light manifold convolutional neural network (FLM-CNN) based on the multi-scale encoding strategy <a class="anchor anchor-primary" href="#bib0272" name="bbib0272" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0272"><span class="anchor-text-container"><span class="anchor-text">[272]</span></span></a> or the deep fusion convolutional neural network <a class="anchor anchor-primary" href="#bib0273" name="bbib0273" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0273"><span class="anchor-text-container"><span class="anchor-text">[273]</span></span></a><span>. Due to the facial expression database biases, <a href="/topics/computer-science/conditional-probability" title="Learn more about conditional probability from ScienceDirect's AI-generated Topic Pages" class="topic-link">conditional probability</a> distributions between source and target databases are often different. To implement a cross-database FER, Li and Deng </span><a class="anchor anchor-primary" href="#bib0274" name="bbib0274" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0274"><span class="anchor-text-container"><span class="anchor-text">[274]</span></span></a><span> proposed a novel deep emotion-conditional adaption network to learn domain-invariant and discriminative feature representations. Another challenge for FER is the <a href="/topics/computer-science/class-imbalance" title="Learn more about class imbalance from ScienceDirect's AI-generated Topic Pages" class="topic-link">class imbalance</a> of in-the-wild databases. Li et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0275" name="bbib0275" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0275"><span class="anchor-text-container"><span class="anchor-text">[275]</span></span></a> proposed an adaptive regular loss function named AdaReg loss, which can re-weight category importance coefficients, to learn class-imbalanced expression representations.</div><div class="u-margin-s-bottom" id="para0097"><span>The <a href="/topics/computer-science/3d-convolutional-neural-networks" title="Learn more about 3D ConvNet from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D ConvNet</a> (or C3D) </span><a class="anchor anchor-primary" href="#bib0276" name="bbib0276" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0276"><span class="anchor-text-container"><span class="anchor-text">[276]</span></span></a> has been universally used for dynamic-based FER by learning and representing the spatiotemporal features of videos or sequences. For example, the C3D was first used to learn local spatiotemporal features <a class="anchor anchor-primary" href="#bib0277" name="bbib0277" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0277"><span class="anchor-text-container"><span class="anchor-text">[277]</span></span></a> and then these features were cascaded with the multimodal deep-belief networks (DBNs) <a class="anchor anchor-primary" href="#bib0278" name="bbib0278" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0278"><span class="anchor-text-container"><span class="anchor-text">[278]</span></span></a>. Besides, the C3D can be combined with the global attention module to represent Eulerian motion feature maps generated based on the Eulerian video magnification (EVM) <a class="anchor anchor-primary" href="#bib0279" name="bbib0279" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0279"><span class="anchor-text-container"><span class="anchor-text">[279]</span></span></a>. Lo et&nbsp;al. <a class="anchor anchor-primary" href="#bib0280" name="bbib0280" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0280"><span class="anchor-text-container"><span class="anchor-text">[280]</span></span></a><span> utilized a <a href="/topics/computer-science/3-dimensional-convolutional-neural-networks" title="Learn more about 3D CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D CNN</a> to extract AU features and applied a graph convolutional network (GCN) to discover the dependency of AU nodes.</span></div><div class="u-margin-s-bottom" id="para0098"><strong>ConvNet-RNN learning for FER.</strong> For dynamic facial sequences or videos, the temporal correlations of consecutive frames should be considered as important cues. RNNs and their variants (LSTMs) can robustly derive temporal characteristics of the spatial feature representation. In contrast, spatial characteristics of the representative expression-state frames can be learned with CNNs <a class="anchor anchor-primary" href="#bib0281" name="bbib0281" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0281"><span class="anchor-text-container"><span class="anchor-text">[281]</span></span></a>. Based on the architecture of ConvNet-RNN networks, many studies have proposed cascaded fusion [<a class="anchor anchor-primary" href="#bib0224" name="bbib0224" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0224"><span class="anchor-text-container"><span class="anchor-text">224</span></span></a>,<a class="anchor anchor-primary" href="#bib0282" name="bbib0282" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0282"><span class="anchor-text-container"><span class="anchor-text">282</span></span></a>] or the ensemble strategy <a class="anchor anchor-primary" href="#bib0238" name="bbib0238" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0238"><span class="anchor-text-container"><span class="anchor-text">[238]</span></span></a> to capture both spatial and temporal information for FER.</div><div class="u-margin-s-bottom" id="para0099">The standard pipeline of ConvNet-RNN based FER using the cascaded fusion is to cascade outputs of ConvNets into RNNs to extract temporal dynamics. For example, Kim et&nbsp;al. <a class="anchor anchor-primary" href="#bib0283" name="bbib0283" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0283"><span class="anchor-text-container"><span class="anchor-text">[283]</span></span></a> proposed an end-to-end FMER framework by cascading the spatial features extracted by the CNN into the LSTM to encode the temporal characteristics. Xia et&nbsp;al. <a class="anchor anchor-primary" href="#bib0284" name="bbib0284" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0284"><span class="anchor-text-container"><span class="anchor-text">[284]</span></span></a> proposed a deep spatiotemporal recurrent convolutional network (STRCN) with a balanced loss that can capture the spatiotemporal deformations of the micro-expression sequence. For dimensional emotion recognition, Kollias and Zafeiriou <a class="anchor anchor-primary" href="#bib0285" name="bbib0285" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0285"><span class="anchor-text-container"><span class="anchor-text">[285]</span></span></a> proposed RNN subnets to explore the temporal dynamics of low-, mid- and high-level features extracted from the trained CNNs. Liu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0286" name="bbib0286" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0286"><span class="anchor-text-container"><span class="anchor-text">[286]</span></span></a> proposed a framework of dynamic FER based on the siamese action-units attention network (SAANet). Specifically, the SAANet is a pairwise sampling strategy, consisting of CNNs with global-AU attention modules, a BiLSTM module and an attentive pooling module. For 4D FER, Behzad et&nbsp;al. <a class="anchor anchor-primary" href="#bib0224" name="bbib0224" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0224"><span class="anchor-text-container"><span class="anchor-text">[224]</span></span></a> utilized CNNs to extract deep features of multi-view and augmented images, and then fed these features into the Bi-LSTM to predict 4D facial expression. On the basis of the work <a class="anchor anchor-primary" href="#bib0224" name="bbib0224" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0224"><span class="anchor-text-container"><span class="anchor-text">[224]</span></span></a>, sparsity-aware deep learning <a class="anchor anchor-primary" href="#bib0282" name="bbib0282" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0282"><span class="anchor-text-container"><span class="anchor-text">[282]</span></span></a><span> was further proposed to compute the <a href="/topics/computer-science/sparse-representation" title="Learn more about sparse representations from ScienceDirect's AI-generated Topic Pages" class="topic-link">sparse representations</a> of multi-view CNN features.</span></div><div class="u-margin-s-bottom" id="para0100">The standard pipeline of ConvNet-RNN based FER using the ensemble strategy is to fuse outputs of two streams. For example, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0238" name="bbib0238" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0238"><span class="anchor-text-container"><span class="anchor-text">[238]</span></span></a> (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> (f)) proposed a deep evolutional spatial-temporal network, which consists of a part-based hierarchical bidirectional recurrent neural network (PHRNN) and a multi-signal convolutional neural network (MSCNN), for analyzing temporal facial expression information and still appearance information, respectively.</div><div class="u-margin-s-bottom" id="para0101"><strong>Adversarial learning for FER.</strong> As GANs can generate synthetic facial expression images under different poses and views, GAN-based models are used for pose/view-invariant FER <a class="anchor anchor-primary" href="#bib0287" name="bbib0287" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0287"><span class="anchor-text-container"><span class="anchor-text">[287]</span></span></a> or identity-invariant FER <a class="anchor anchor-primary" href="#bib0288" name="bbib0288" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0288"><span class="anchor-text-container"><span class="anchor-text">[288]</span></span></a>. For pose/view-invariant FER, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0239" name="bbib0239" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0239"><span class="anchor-text-container"><span class="anchor-text">[239]</span></span></a> proposed the GAN using AE structure to generate more facial images with different expressions under arbitrary poses (<a class="anchor anchor-primary" href="#fig0003" name="bfig0003" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0003"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;4</span></span></a> (g)) and <a class="anchor anchor-primary" href="#bib0287" name="bbib0287" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0287"><span class="anchor-text-container"><span class="anchor-text">[287]</span></span></a> further took the shape geometry into consideration. Since the slight semantic perturbations of the inputs often affected prediction accuracy, Fu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0289" name="bbib0289" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0289"><span class="anchor-text-container"><span class="anchor-text">[289]</span></span></a> proposed a semantic neighbourhood aware (SNA) network, which formulated the semantic perturbation based on the asymmetric AE with additive noise. For identity-invariant FER, Ali and Hughes <a class="anchor anchor-primary" href="#bib0290" name="bbib0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0290"><span class="anchor-text-container"><span class="anchor-text">[290]</span></span></a><span> proposed a novel disentangled expression learning GAN (DE-GAN) by untangling the facial expression representation from <a href="/topics/computer-science/identity-information" title="Learn more about identity information from ScienceDirect's AI-generated Topic Pages" class="topic-link">identity information</a>. Yu et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0291" name="bbib0291" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0291"><span class="anchor-text-container"><span class="anchor-text">[291]</span></span></a><span> investigated a framework of facial micro-expression recognition and synthesis based on the identity-aware and capsule-enhanced GAN (ICE-GAN), which consisted of an AE-based generator for identity-aware expression synthesis, and a capsule-enhanced <a href="/topics/computer-science/discriminator" title="Learn more about discriminator from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminator</a> (CED) for discriminating the real/fake images and recognizing micro-expressions.</span></div></section></section><section id="sec0028"><h4 id="cesectitle0031" class="u-margin-m-top u-margin-xs-bottom">5.3.2. Body gesture emotion recognition</h4><div class="u-margin-s-bottom" id="para0102">Most studies of visual emotion recognition focus on FER due to the prominent advantages of distinguishing human emotions. However, FER will be unsuitable when the dedicated sensors fail to capture facial images or just capture low-resolution facial images in some environments. EBGR <a class="anchor anchor-primary" href="#bib0045" name="bbib0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0045"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a><span> aims to expose one's hidden emotional state from full-body visual information (e.g., body postures) and body <a href="/topics/medicine-and-dentistry/skeleton" title="Learn more about skeleton from ScienceDirect's AI-generated Topic Pages" class="topic-link">skeleton</a> movements or upper-body visual information (e.g., hand gestures, head positioning and eye movements) [</span><a class="anchor anchor-primary" href="#bib0292" name="bbib0292" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0292"><span class="anchor-text-container"><span class="anchor-text">292</span></span></a>,<a class="anchor anchor-primary" href="#bib0293" name="bbib0293" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0293"><span class="anchor-text-container"><span class="anchor-text">293</span></span></a>]. The general pipeline of EBGR includes human detection [<a class="anchor anchor-primary" href="#bib0294" name="bbib0294" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0294"><span class="anchor-text-container"><span class="anchor-text">294</span></span></a>,<a class="anchor anchor-primary" href="#bib0258" name="bbib0258" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0258"><span class="anchor-text-container"><span class="anchor-text">258</span></span></a>,<a class="anchor anchor-primary" href="#bib0295" name="bbib0295" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0295"><span class="anchor-text-container"><span class="anchor-text">295</span></span></a>] (regarded as pre-processing), feature representation, and emotion recognition. In the view of whether the process of feature extraction and emotion recognition is performed in an end-to-end manner, EBGR systems are categorized into ML-based EBGR and DL-based EBGR.</div><section id="sec0029"><h5 id="cesectitle0032" class="u-margin-m-top u-margin-xs-bottom">5.3.2.1. ML-based EBGR</h5><div class="u-margin-s-bottom" id="para0103"><span>In existing ML-based EBGR systems, the input is an abstraction of the human body gestures (or their dynamics) through an ensemble body parts or a <a href="/topics/computer-science/kinematic-model" title="Learn more about kinematic model from ScienceDirect's AI-generated Topic Pages" class="topic-link">kinematic model</a> </span><a class="anchor anchor-primary" href="#bib0296" name="bbib0296" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0296"><span class="anchor-text-container"><span class="anchor-text">[296]</span></span></a>; and the output emotion is distinguished through ML-based methods or statistical measures to map the input into the emotional feature space, where the emotional state can be recognized by an ML-based classifier <a class="anchor anchor-primary" href="#bib0297" name="bbib0297" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0297"><span class="anchor-text-container"><span class="anchor-text">[297]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0104"><strong>Statistic-based or movement-based EBGR</strong>. The ways of feature extraction can be grouped into statistic-based analysis <a class="anchor anchor-primary" href="#bib0298" name="bbib0298" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0298"><span class="anchor-text-container"><span class="anchor-text">[298]</span></span></a>, <a class="anchor anchor-primary" href="#bib0299" name="bbib0299" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0299"><span class="anchor-text-container"><span class="anchor-text">[299]</span></span></a>, <a class="anchor anchor-primary" href="#bib0300" name="bbib0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0300"><span class="anchor-text-container"><span class="anchor-text">[300]</span></span></a> and movement-based analysis <a class="anchor anchor-primary" href="#bib0301" name="bbib0301" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0301"><span class="anchor-text-container"><span class="anchor-text">[301]</span></span></a>. For example, Castellano et&nbsp;al. <a class="anchor anchor-primary" href="#bib0298" name="bbib0298" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0298"><span class="anchor-text-container"><span class="anchor-text">[298]</span></span></a> proposed an emotional behavior recognition method based on the analysis of body movement and gesture expressivity. Different statistic qualities of dynamic body gestures are used to infer emotions with designed indicators describing the dynamics of expressive motion cues. Similarly, Saha et&nbsp;al. <a class="anchor anchor-primary" href="#bib0299" name="bbib0299" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0299"><span class="anchor-text-container"><span class="anchor-text">[299]</span></span></a> and Maret et&nbsp;al. <a class="anchor anchor-primary" href="#bib0300" name="bbib0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0300"><span class="anchor-text-container"><span class="anchor-text">[300]</span></span></a> utilized low-level features for EBGR, which were calculated based on the statistical analysis of the 3-D human skeleton generated by a Kinect sensor. Besides, Senecal et&nbsp;al. <a class="anchor anchor-primary" href="#bib0301" name="bbib0301" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0301"><span class="anchor-text-container"><span class="anchor-text">[301]</span></span></a><span> proposed a framework of continuous emotional recognition based on the gestures and full-body dynamical motions using the laban movement analysis-based <a href="/topics/computer-science/feature-descriptor" title="Learn more about feature descriptors from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature descriptors</a>.</span></div><div class="u-margin-s-bottom" id="para0105"><strong>Feature fusion for EBGR</strong>. Fusing multiple body posture features can enhance the generalization capability and the robustness of EBGR <a class="anchor anchor-primary" href="#bib0296" name="bbib0296" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0296"><span class="anchor-text-container"><span class="anchor-text">[296]</span></span></a>. By analyzing postural and dynamic expressive gesture features, Glowinski et&nbsp;al. <a class="anchor anchor-primary" href="#bib0302" name="bbib0302" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0302"><span class="anchor-text-container"><span class="anchor-text">[302]</span></span></a> proposed a framework of upper-body based EBGR, using the minimal representation of emotional displays with a reduced amount of visual information related to human upper-body movements. Razzaq et&nbsp;al. <a class="anchor anchor-primary" href="#bib0303" name="bbib0303" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0303"><span class="anchor-text-container"><span class="anchor-text">[303]</span></span></a><span> utilized skeletal joint features from a Kinect v2 sensor, to build mesh distance features and mesh angular features for upper-body <a href="/topics/neuroscience/emotion-representation" title="Learn more about emotion representation from ScienceDirect's AI-generated Topic Pages" class="topic-link">emotion representation</a>. Santhoshkumar and Geetha successively proposed two EBGR methods, by using histogram of orientation gradient (HOG) and HOG-KLT features from the sequences </span><a class="anchor anchor-primary" href="#bib0304" name="bbib0304" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0304"><span class="anchor-text-container"><span class="anchor-text">[304]</span></span></a>, or by calculating and extracting four kinds of geometric body expressive features <a class="anchor anchor-primary" href="#bib0305" name="bbib0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0305"><span class="anchor-text-container"><span class="anchor-text">[305]</span></span></a>. They achieved the best recognition accuracies of 95.9% and 93.1% on GEMEP, respectively.</div><div class="u-margin-s-bottom" id="para0106"><strong>Classifier-based EBGR</strong><span>. The common classifiers for ML-based EBGR include decision tree, ensemble decision tree, <a href="/topics/computer-science/k-nearest-neighbors-algorithm" title="Learn more about KNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN</a>, SVM, etc. For example, Kapur et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0306" name="bbib0306" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0306"><span class="anchor-text-container"><span class="anchor-text">[306]</span></span></a><span> proposed gesture-based affective computing by using five different classifiers to analyze full-body skeletal movements captured by the <a href="/topics/medicine-and-dentistry/ascorbic-acid" title="Learn more about Vicon from ScienceDirect's AI-generated Topic Pages" class="topic-link">Vicon</a> system. Different from full-body based EBGR, Saha et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0299" name="bbib0299" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0299"><span class="anchor-text-container"><span class="anchor-text">[299]</span></span></a><span><span> focused on the upper-body based EBGR by employing some comparisons, using five classic ML-based classifiers in terms of the average <a href="/topics/computer-science/classification-accuracy" title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification accuracy</a> and </span><a href="/topics/computer-science/computation-time" title="Learn more about computation time from ScienceDirect's AI-generated Topic Pages" class="topic-link">computation time</a>. The experimental results showed that the ensemble decision tree achieved the highest recognition rate of 90.83%, under an acceptable execution efficiency. Maret et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0300" name="bbib0300" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0300"><span class="anchor-text-container"><span class="anchor-text">[300]</span></span></a> also utilized five commonly used classifiers, whereby the genetic algorithm was invoked to search the optimal parameters of the recognition process.</div><div class="u-margin-s-bottom" id="para0107"><strong>Non-acted EBGR</strong>. While the above works show good results on acted data, they fail to address the more difficult non-acted scenario due to the exaggerated displays of emotional body motions. Kleinsmith et&nbsp;al. <a class="anchor anchor-primary" href="#bib0307" name="bbib0307" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0307"><span class="anchor-text-container"><span class="anchor-text">[307]</span></span></a> used low-level posture descriptions and feature analysis using non-acted body gestures to implement MLP-based emotion and dimension recognition. Volkova et&nbsp;al. <a class="anchor anchor-primary" href="#bib0308" name="bbib0308" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0308"><span class="anchor-text-container"><span class="anchor-text">[308]</span></span></a> further investigated whether emotional body expressions could be recognized when they were recorded during natural scenarios. To explore the emotion recognition from daily actions, Fourati et&nbsp;al. <a class="anchor anchor-primary" href="#bib0309" name="bbib0309" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0309"><span class="anchor-text-container"><span class="anchor-text">[309]</span></span></a> recorded varieties of emotional body expressions in daily actions and constructed a new database.</div></section><section id="sec0030"><h5 id="cesectitle0033" class="u-margin-m-top u-margin-xs-bottom">5.3.2.2. DL-based EBGR</h5><div class="u-margin-s-bottom" id="para0108">Although DL-based EBGR systems do not require to design of a tailored-feature extractor, they often pre-processed the input data based on the commonly-used pose estimation models or low-level feature extractors <a class="anchor anchor-primary" href="#bib0310" name="bbib0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0310"><span class="anchor-text-container"><span class="anchor-text">[310]</span></span></a>. High-level features can be learned in spatial, temporal or spatial-temporal dimensions through the CNN-based network <a class="anchor anchor-primary" href="#bib0311" name="bbib0311" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0311"><span class="anchor-text-container"><span class="anchor-text">[311]</span></span></a>, the LSTM-based network <a class="anchor anchor-primary" href="#bib0310" name="bbib0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0310"><span class="anchor-text-container"><span class="anchor-text">[310]</span></span></a> or the CNN-LSTM based network <a class="anchor anchor-primary" href="#bib0312" name="bbib0312" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0312"><span class="anchor-text-container"><span class="anchor-text">[312]</span></span></a>. Recently, many studies have demonstrated the advantages of effectively combining different DL-based models and the attention mechanism <a class="anchor anchor-primary" href="#bib0313" name="bbib0313" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0313"><span class="anchor-text-container"><span class="anchor-text">[313]</span></span></a> to improve the performance of EBGR.</div><div class="u-margin-s-bottom" id="para0109"><strong>ConvNet-RNN learning for EBGR.</strong> Ly et&nbsp;al. <a class="anchor anchor-primary" href="#bib0312" name="bbib0312" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0312"><span class="anchor-text-container"><span class="anchor-text">[312]</span></span></a> first utilized the hashing model to detect keyframes of upper-body videos, and then applied a CNN-LSTM network to extract sequence information. The model employed on FABO achieved recognition accuracy of 72.5%. Avola et&nbsp;al. <a class="anchor anchor-primary" href="#bib0314" name="bbib0314" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0314"><span class="anchor-text-container"><span class="anchor-text">[314]</span></span></a> investigated a framework of non-acted EBGR based on 3D skeleton and DNNs, which consist of MLP and N-stacked LSTMs. Shen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0310" name="bbib0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0310"><span class="anchor-text-container"><span class="anchor-text">[310]</span></span></a><span> proposed full-body based EBGR by fusing RGB features of <a href="/topics/biochemistry-genetics-and-molecular-biology/optic-flow" title="Learn more about optical flow from ScienceDirect's AI-generated Topic Pages" class="topic-link">optical flow</a> extracted by the temporal segment network </span><a class="anchor anchor-primary" href="#bib0315" name="bbib0315" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0315"><span class="anchor-text-container"><span class="anchor-text">[315]</span></span></a>, and skeleton features extracted by spatial-temporal graph convolutional networks <a class="anchor anchor-primary" href="#bib0316" name="bbib0316" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0316"><span class="anchor-text-container"><span class="anchor-text">[316]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0110"><strong>Zero-shot based EBGR.</strong> Due to the complexity and diversity of human emotion through body gestures, it is difficult to enumerate all emotional body gestures and collect enough samples for each category <a class="anchor anchor-primary" href="#bib0317" name="bbib0317" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0317"><span class="anchor-text-container"><span class="anchor-text">[317]</span></span></a>. Therefore, the existing methods fail to determine which emotional state a new body gesture belongs to. In order to recognize unknown emotions from seen body gestures or to know emotions from unseen body gestures, Banerjee et&nbsp;al. <a class="anchor anchor-primary" href="#bib0318" name="bbib0318" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0318"><span class="anchor-text-container"><span class="anchor-text">[318]</span></span></a> and Wu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0313" name="bbib0313" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0313"><span class="anchor-text-container"><span class="anchor-text">[313]</span></span></a> introduced the generalized zero-shot learning framework, including CNN-based feature extraction, autoencoder-based representation learning, and emotion classifier.</div></section></section></section><section id="sec0031"><h3 id="cesectitle0034" class="u-h4 u-margin-m-top u-margin-xs-bottom">5.4. Physiological-based emotion recognition</h3><div class="u-margin-s-bottom" id="para0111">Facial expressions, text, voice, and body gesture from a human being can be easily collected. As the reliability of physical information largely depends on the social environment and cultural background, and personalities of testers, their emotions are easy to be forged <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>. However, the changes in physiological signals directly reflect the changes in human emotions, which can help humans recognize, interpret and simulate emotional states [<a class="anchor anchor-primary" href="#bib0030" name="bbib0030" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0030"><span class="anchor-text-container"><span class="anchor-text">30</span></span></a>,<a class="anchor anchor-primary" href="#bib0319" name="bbib0319" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0319"><span class="anchor-text-container"><span class="anchor-text">319</span></span></a>]. Therefore, it is highly objective to learn human emotions through physiological signals <a class="anchor anchor-primary" href="#bib0320" name="bbib0320" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0320"><span class="anchor-text-container"><span class="anchor-text">[320]</span></span></a>.</div><div class="u-margin-s-bottom"><div id="para0112">The diagram of physiological-based emotion recognition, as shown in <a class="anchor anchor-primary" href="#fig0005" name="bfig0005" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0005"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;5</span></span></a><span>, typically includes the following five aspects: 1) Stimulating subjects’ emotions with images, music and videos; 2) Recording physiological signals that mainly include EEG, <a href="/topics/biochemistry-genetics-and-molecular-biology/skin-conductance" title="Learn more about skin conductance from ScienceDirect's AI-generated Topic Pages" class="topic-link">skin conductance</a><span><span>, RESP, heart rate, EMG, and ECG; 3) Extracting features through physiological signals pre-processing, feature analysis, feature selection and reduction; 4) Training the <a href="/topics/computer-science/classification-models" title="Learn more about classification model from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification model</a> such as SVM, KNN, </span><a href="/topics/computer-science/linear-discriminant-analysis" title="Learn more about LDA from ScienceDirect's AI-generated Topic Pages" class="topic-link">LDA</a>, RF, NB and NN, etc.; and 5) Emotion recognition based on a discrete emotion model or a dimensional emotion model.</span></span></div><figure class="figure text-xs" id="fig0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr5.jpg" height="297" alt="Fig 5" aria-describedby="cap0005"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr5_lrg.jpg" target="_blank" download="" title="Download high-res image (607KB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (607KB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr5.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0005"><p id="spara005"><span class="label">Fig. 5</span>. The diagram of emotion recognition via physiological signals.</p></span></span></figure></div><div class="u-margin-s-bottom"><div id="para0113">Among the above physiological emotion signals, EEG or ECG can provide simple, objective, and reliable data for identifying emotions <a class="anchor anchor-primary" href="#bib0321" name="bbib0321" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0321"><span class="anchor-text-container"><span class="anchor-text">[321]</span></span></a>, and is most frequently used for sentiment analysis and emotion recognition. Afterwards, we review EEG-based and ECG-based emotion recognition in this sub-section. <a class="anchor anchor-primary" href="#tbl0010" name="btbl0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0010"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;10</span></span></a> shows an overview of representative methods for physiological-based emotion recognition, as detailed next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0010"><span class="captions text-s"><span id="cap0016"><p id="spara017"><span class="label">Table 10</span>. Overview of the representative methods for physiological-based emotion recognition.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en1226">Publication</th><th scope="col" class="align-left valign-top" id="en1227">Year</th><th scope="col" class="align-left valign-top" id="en1228">Feature Representation</th><th scope="col" class="align-left valign-top" id="en1229">Classifier</th><th scope="col" class="align-left valign-top" id="en1230">Database</th><th scope="col" class="align-left valign-top" id="en1231">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en1232" colspan="6"><em><strong>EEG-based Emotion Recognition</strong></em></td></tr><tr><td class="align-left valign-top" id="en1233"><a class="anchor anchor-primary" href="#bib0328" name="bbib0328" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0328"><span class="anchor-text-container"><span class="anchor-text">[328]</span></span></a></td><td class="align-left valign-top" id="en1234">2016</td><td class="align-left valign-top" id="en1235">mRMR-based</td><td class="align-left valign-top" id="en1236">SVM</td><td class="align-left valign-top" id="en1237">DEAP</td><td class="align-left valign-top" id="en1238">A/V: 73.06/73.14</td></tr><tr><td class="align-left valign-top" id="en1239"><a class="anchor anchor-primary" href="#bib0326" name="bbib0326" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0326"><span class="anchor-text-container"><span class="anchor-text">[326]</span></span></a></td><td class="align-left valign-top" id="en1240">2019</td><td class="align-left valign-top" id="en1241">ADMM-based</td><td class="align-left valign-top" id="en1242">Multi-class SGL</td><td class="align-left valign-top" id="en1243">MAHNOB</td><td class="align-left valign-top" id="en1244">3 classes: 74.00</td></tr><tr><td class="align-left valign-top" id="en1245"><a class="anchor anchor-primary" href="#bib0330" name="bbib0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0330"><span class="anchor-text-container"><span class="anchor-text">[330]</span></span></a></td><td class="align-left valign-top" id="en1246">2018</td><td class="align-left valign-top" id="en1247">BiDANN</td><td class="align-left valign-top" id="en1248">LSTM+Softmax</td><td class="align-left valign-top" id="en1249">SEED</td><td class="align-left valign-top" id="en1250">3 classes: 92.38</td></tr><tr><td class="align-left valign-top" id="en1251"><a class="anchor anchor-primary" href="#bib0334" name="bbib0334" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0334"><span class="anchor-text-container"><span class="anchor-text">[334]</span></span></a></td><td class="align-left valign-top" id="en1252">2020</td><td class="align-left valign-top" id="en1253">Spatial-temporal</td><td class="align-left valign-top" id="en1254">Pooling+Softmax</td><td class="align-left valign-top" id="en1255">SEED<br>DEAP</td><td class="align-left valign-top" id="en1256">3 classes: 90.63 A/V: 92.92/92.24</td></tr><tr><td class="align-left valign-top" id="en1257" colspan="6"><strong><em>ECG-based Emotion Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en1258"><a class="anchor anchor-primary" href="#bib0342" name="bbib0342" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0342"><span class="anchor-text-container"><span class="anchor-text">[342]</span></span></a></td><td class="align-left valign-top" id="en1259">2017</td><td class="align-left valign-top" id="en1260">Feature Fusion+LLN</td><td class="align-left valign-top" id="en1261">KNN</td><td class="align-left valign-top" id="en1262">Mahnob-HCI</td><td class="align-left valign-top" id="en1263">A/V: 66.10/64.10</td></tr><tr><td class="align-left valign-top" id="en1264"><a class="anchor anchor-primary" href="#bib0339" name="bbib0339" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0339"><span class="anchor-text-container"><span class="anchor-text">[339]</span></span></a></td><td class="align-left valign-top" id="en1265">2017</td><td class="align-left valign-top" id="en1266">Feature Fusion</td><td class="align-left valign-top" id="en1267">SVM</td><td class="align-left valign-top" id="en1268">BioVid EmoDB</td><td class="align-left valign-top" id="en1269">2 classes: 79.15</td></tr><tr><td class="align-left valign-top" id="en1270"><a class="anchor anchor-primary" href="#bib0343" name="bbib0343" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0343"><span class="anchor-text-container"><span class="anchor-text">[343]</span></span></a></td><td class="align-left valign-top" id="en1271">2019</td><td class="align-left valign-top" id="en1272">EmotionalGAN</td><td class="align-left valign-top" id="en1273">SVM</td><td class="align-left valign-top" id="en1274">DECAF</td><td class="align-left valign-top" id="en1275">A/V: 58.60/59.40</td></tr><tr><td class="align-left valign-top" id="en1276"><a class="anchor anchor-primary" href="#bib0344" name="bbib0344" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0344"><span class="anchor-text-container"><span class="anchor-text">[344]</span></span></a></td><td class="align-left valign-top" id="en1277">2020</td><td class="align-left valign-top" id="en1278">Self-supervised CNN</td><td class="align-left valign-top" id="en1279">FC+Sigmoid</td><td class="align-left valign-top" id="en1280">SWELL; AMIGOS</td><td class="align-left valign-top" id="en1281">A/V: 96.00/96.30; A/V: 85.80/83.70</td></tr></tbody></table></div></div></div><section id="sec0032"><h4 id="cesectitle0035" class="u-margin-m-top u-margin-xs-bottom">5.4.1. EEG-based emotion recognition</h4><div class="u-margin-s-bottom" id="para0114"><span><span>Compared with other peripheral neuro-physiological signals, EEG can directly measure the changes of <a href="/topics/computer-science/brain-activity" title="Learn more about brain activities from ScienceDirect's AI-generated Topic Pages" class="topic-link">brain activities</a>, which provides </span><a href="/topics/computer-science/internal-feature" title="Learn more about internal features from ScienceDirect's AI-generated Topic Pages" class="topic-link">internal features</a> of emotional states </span><a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>. Besides, EEG with a high temporal resolution makes it possible to monitor a real-time emotional state. Therefore, various EEG-based emotion recognition techniques [<a class="anchor anchor-primary" href="#bib0029" name="bbib0029" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0029"><span class="anchor-text-container"><span class="anchor-text">29</span></span></a>,<a class="anchor anchor-primary" href="#bib0322" name="bbib0322" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0322"><span class="anchor-text-container"><span class="anchor-text">322</span></span></a>] have been developed recently.</div><section id="sec0033"><h5 id="cesectitle0036" class="u-margin-m-top u-margin-xs-bottom">5.4.1.1. ML-based EEG emotion recognition</h5><div class="u-margin-s-bottom" id="para0115">The performance of ML-based EEG-based emotion recognition <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a> depends on how to properly design feature extraction, feature dimensionality reduction (or feature selection), and classification methods.</div><div class="u-margin-s-bottom" id="para0116"><span><span>The core objective of feature extraction is to extract important EEG features, which contain time-domain, frequency-domain, and time–frequency-domain features. The fast <a href="/topics/biochemistry-genetics-and-molecular-biology/fourier-transform" title="Learn more about Fourier transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Fourier transform</a> (FFT) analysis is often used to transform the EEG into the </span><a href="/topics/biochemistry-genetics-and-molecular-biology/power-spectra" title="Learn more about power spectrum from ScienceDirect's AI-generated Topic Pages" class="topic-link">power spectrum</a> </span><a class="anchor anchor-primary" href="#bib0323" name="bbib0323" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0323"><span class="anchor-text-container"><span class="anchor-text">[323]</span></span></a>. Feature dimensionality reduction is an important step in EEG-based emotion recognition due to the redundancy of EEG. Yoon and Chung <a class="anchor anchor-primary" href="#bib0323" name="bbib0323" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0323"><span class="anchor-text-container"><span class="anchor-text">[323]</span></span></a><span> used the FFT analysis for feature extraction and the <a href="/topics/computer-science/pearson-correlation-coefficient" title="Learn more about Pearson correlation coefficient from ScienceDirect's AI-generated Topic Pages" class="topic-link">Pearson correlation coefficient</a> (PCC) for feature selection. Yin et&nbsp;al. successively designed a novel transfer recursive feature elimination </span><a class="anchor anchor-primary" href="#bib0324" name="bbib0324" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0324"><span class="anchor-text-container"><span class="anchor-text">[324]</span></span></a> and the dynamical recursive feature elimination <a class="anchor anchor-primary" href="#bib0325" name="bbib0325" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0325"><span class="anchor-text-container"><span class="anchor-text">[325]</span></span></a> for EEG feature selection, which determined sets of robust EEG features. Puk et&nbsp;al. <a class="anchor anchor-primary" href="#bib0326" name="bbib0326" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0326"><span class="anchor-text-container"><span class="anchor-text">[326]</span></span></a><span> investigated an <a href="/topics/computer-science/alternating-direction-method-of-multipliers" title="Learn more about alternating direction method of multipliers from ScienceDirect's AI-generated Topic Pages" class="topic-link">alternating direction method of multipliers</a> ADMM-based sparse group lasso (SGL), with hierarchical splitting for recognizing the discrete states of three emotions. He et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0327" name="bbib0327" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0327"><span class="anchor-text-container"><span class="anchor-text">[327]</span></span></a><span> designed a firefly integrated <a href="/topics/computer-science/optimization-algorithm" title="Learn more about optimization algorithm from ScienceDirect's AI-generated Topic Pages" class="topic-link">optimization algorithm</a> (FIOA) to realize the optimal selection of the features subset and the classifier, without stagnating in the local optimum for the automatic emotion recognition. The FIOA evaluated on DEAP can gradually regulate the balance between ACC and feature number in the whole optimization process, which achieves an accuracy of 95.00%.</span></div><div class="u-margin-s-bottom" id="para0117">The commonly used ML-based classifier for EEG-based emotion recognition is SVM or its variations. For example, Atkinson and Campos <a class="anchor anchor-primary" href="#bib0328" name="bbib0328" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0328"><span class="anchor-text-container"><span class="anchor-text">[328]</span></span></a><span> combined the mutual information-based EEG feature selection approach and SVM to improve the recognition accuracy. Yin et&nbsp;al. successively designed the linear <a href="/topics/medicine-and-dentistry/least-square-analysis" title="Learn more about least square from ScienceDirect's AI-generated Topic Pages" class="topic-link">least square</a> SVM </span><a class="anchor anchor-primary" href="#bib0324" name="bbib0324" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0324"><span class="anchor-text-container"><span class="anchor-text">[324]</span></span></a><span> and the selected <a href="/topics/computer-science/least-squares-method" title="Learn more about least square from ScienceDirect's AI-generated Topic Pages" class="topic-link">least square</a> SVM </span><a class="anchor anchor-primary" href="#bib0325" name="bbib0325" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0325"><span class="anchor-text-container"><span class="anchor-text">[325]</span></span></a> for EEG-based emotion recognition.</div></section><section id="sec0034"><h5 id="cesectitle0037" class="u-margin-m-top u-margin-xs-bottom">5.4.1.2. DL-based EEG emotion recognition</h5><div class="u-margin-s-bottom" id="para0118">Different from the pipeline of ML-based EEG-based emotion recognition, Gao et&nbsp;al. <a class="anchor anchor-primary" href="#bib0329" name="bbib0329" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0329"><span class="anchor-text-container"><span class="anchor-text">[329]</span></span></a><span> utilized CNNs and restricted <a href="/topics/computer-science/boltzmann-machine" title="Learn more about Boltzmann machine from ScienceDirect's AI-generated Topic Pages" class="topic-link">Boltzmann machine</a> (RBM) with three layers, to simultaneously learn the features and classify EEG-based emotions. The CNN-based emotion recognition with subject-tied protocol achieves an accuracy of 68.4%. The affective computing team directed by Prof. Zheng </span><a class="anchor anchor-primary" href="#bib0330" name="bbib0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0330"><span class="anchor-text-container"><span class="anchor-text">[330]</span></span></a>, <a class="anchor anchor-primary" href="#bib0331" name="bbib0331" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0331"><span class="anchor-text-container"><span class="anchor-text">[331]</span></span></a>, <a class="anchor anchor-primary" href="#bib0332" name="bbib0332" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0332"><span class="anchor-text-container"><span class="anchor-text">[332]</span></span></a><span> from Southeast University, China, has proposed various EEG-based emotion <a href="/topics/computer-science/recognition-network" title="Learn more about recognition networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition networks</a> such as bi-hemispheres domain adversarial neural network (BiDANN) </span><a class="anchor anchor-primary" href="#bib0330" name="bbib0330" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0330"><span class="anchor-text-container"><span class="anchor-text">[330]</span></span></a>, instance-adaptive graph network <a class="anchor anchor-primary" href="#bib0331" name="bbib0331" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0331"><span class="anchor-text-container"><span class="anchor-text">[331]</span></span></a> and variational pathway reasoning <a class="anchor anchor-primary" href="#bib0332" name="bbib0332" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0332"><span class="anchor-text-container"><span class="anchor-text">[332]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0119">As the biological topology among different brain regions can capture both local and global relations among different EEG channels, Zhong et&nbsp;al. <a class="anchor anchor-primary" href="#bib0333" name="bbib0333" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0333"><span class="anchor-text-container"><span class="anchor-text">[333]</span></span></a><span><span> designed a regularized <a href="/topics/computer-science/graph-neural-network" title="Learn more about graph neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">graph neural network</a> (RGNN), which consists of both </span><a href="/topics/computer-science/regularization-operator" title="Learn more about regularization operators from ScienceDirect's AI-generated Topic Pages" class="topic-link">regularization operators</a> of node-wise domain adversarial training and emotion-aware distribution learning. Gao et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0334" name="bbib0334" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0334"><span class="anchor-text-container"><span class="anchor-text">[334]</span></span></a> proposed a channel-fused dense convolutional network for EEG-based emotion recognition. Considering the spatial information from adjacent channels and symmetric channels, Cui et&nbsp;al. <a class="anchor anchor-primary" href="#bib0335" name="bbib0335" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0335"><span class="anchor-text-container"><span class="anchor-text">[335]</span></span></a> proposed the regional-asymmetric CNN (RACNN), including temporal, regional and asymmetric feature extractors. An asymmetric differential layer is introduced into three feature extractors to capture the discriminative information, by considering the asymmetry property of emotion responses. The RACNN achieves prominent results with average accuracies of 96.88% and 96.28% on DEAP <a class="anchor anchor-primary" href="#bib0096" name="bbib0096" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0096"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a> and DREAMER <a class="anchor anchor-primary" href="#bib0321" name="bbib0321" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0321"><span class="anchor-text-container"><span class="anchor-text">[321]</span></span></a>, respectively.</div></section></section><section id="sec0035"><h4 id="cesectitle0038" class="u-margin-m-top u-margin-xs-bottom">5.4.2. ECG-based emotion recognition</h4><div class="u-margin-s-bottom" id="para0120"><span>ECG records the physiological changes of the human heart in different situations through the autonomous <a href="/topics/psychology/nervous-system" title="Learn more about nervous system from ScienceDirect's AI-generated Topic Pages" class="topic-link">nervous system</a> activity. With the change of human emotion or sentiment state, ECG will detect the corresponding waveform transformation </span><a class="anchor anchor-primary" href="#bib0336" name="bbib0336" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0336"><span class="anchor-text-container"><span class="anchor-text">[336]</span></span></a><span>, which can provide enough information in emotion recognition. Next, we introduce ECG-based emotion recognition using ML models or <a href="/topics/computer-science/deep-learning-model" title="Learn more about DL models from ScienceDirect's AI-generated Topic Pages" class="topic-link">DL models</a>.</span></div><section id="sec0036"><h5 id="cesectitle0039" class="u-margin-m-top u-margin-xs-bottom">5.4.2.1. ML-based ECG emotion recognition</h5><div class="u-margin-s-bottom" id="para0121">Following the diagram of physiological-based emotion recognition, Hsu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0336" name="bbib0336" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0336"><span class="anchor-text-container"><span class="anchor-text">[336]</span></span></a><span> first constructed a music-induced ECG emotion database, and then developed a nine-stage framework for automatic ECG-based emotion recognition, including 1) Signal preprocessing; 2) R-wave detection; 3) <a href="/topics/computer-science/windowing" title="Learn more about Windowing from ScienceDirect's AI-generated Topic Pages" class="topic-link">Windowing</a><span><span> ECG recording; 4) Noisy epoch rejection; 5) Feature extraction based on the time-, and frequency-domain and nonlinear analysis; 6) <a href="/topics/computer-science/feature-normalization" title="Learn more about Feature normalization from ScienceDirect's AI-generated Topic Pages" class="topic-link">Feature normalization</a>; 7) Feature selection using sequential forward floating selection-kernel-based class separability; 8) Feature reduction based on the generalized </span><a href="/topics/earth-and-planetary-sciences/discriminant-analysis" title="Learn more about discriminant analysis from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminant analysis</a>, and 9) Classifier construction with LS-SVM. Note that not all ML-based ECG emotion recognition methods follow the abovementioned steps, but the steps of feature extraction, feature selection and classifier are indispensable.</span></span></div><div class="u-margin-s-bottom" id="para0122">The ECG features can be directly extracted in the time domain. Bong et&nbsp;al. <a class="anchor anchor-primary" href="#bib0337" name="bbib0337" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0337"><span class="anchor-text-container"><span class="anchor-text">[337]</span></span></a> extracted three time-domain features: heart rate, mean R peak amplitude, and mean R-R intervals to detect human emotional stress detection. Another common way is to transform time-domain ECG features into those in other domains. For example, Jerritta et&nbsp;al. <a class="anchor anchor-primary" href="#bib0338" name="bbib0338" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0338"><span class="anchor-text-container"><span class="anchor-text">[338]</span></span></a><span> applied FFT, Discrete <a href="/topics/physics-and-astronomy/wavelet-analysis" title="Learn more about Wavelet Transform from ScienceDirect's AI-generated Topic Pages" class="topic-link">Wavelet Transform</a><span> (DWT), and Hilbert Huang Transform (HHT) to transform ECG signals into frequency-domain features, and then utilized PCA and <a href="/topics/computer-science/tabu-search" title="Learn more about Tabu search from ScienceDirect's AI-generated Topic Pages" class="topic-link">Tabu search</a> to select key features in low-, high- and total (low and high together) frequency range. Note that both </span></span><a class="anchor anchor-primary" href="#bib0338" name="bbib0338" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0338"><span class="anchor-text-container"><span class="anchor-text">[338]</span></span></a> and <a class="anchor anchor-primary" href="#bib0337" name="bbib0337" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0337"><span class="anchor-text-container"><span class="anchor-text">[337]</span></span></a> used the SVM to implement the final emotion classification.</div><div class="u-margin-s-bottom" id="para0123">It may be beneficial to combine different types of ECG features. Cheng et&nbsp;al. <a class="anchor anchor-primary" href="#bib0339" name="bbib0339" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0339"><span class="anchor-text-container"><span class="anchor-text">[339]</span></span></a><span> computed linear-derived features, nonlinear-derived features, time-domain features, and time-frequency domain features from ECG and its derived heart rate variability, and then fused them for SVM-based <a href="/topics/computer-science/negative-emotion" title="Learn more about negative emotion from ScienceDirect's AI-generated Topic Pages" class="topic-link">negative emotion</a> detection. The experiments implemented on BioVid Emo DB </span><a class="anchor anchor-primary" href="#bib0340" name="bbib0340" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0340"><span class="anchor-text-container"><span class="anchor-text">[340]</span></span></a> show that <a class="anchor anchor-primary" href="#bib0339" name="bbib0339" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0339"><span class="anchor-text-container"><span class="anchor-text">[339]</span></span></a> achieves an accuracy of 79.51%, with a minor time cost of 0.13ms in the classification of positive and negative emotion states.</div><div class="u-margin-s-bottom" id="para0124">Statistic ECG features are also useful. Selvaraj et&nbsp;al. <a class="anchor anchor-primary" href="#bib0341" name="bbib0341" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0341"><span class="anchor-text-container"><span class="anchor-text">[341]</span></span></a> proposed a non-linear Hurst feature extraction method by combining the rescaled range statistics and the finite variance scaling with higher-order statistics. They further investigated the performances of four conventional classifiers of NB, RT, KNN and fuzzy KNN for emotion classification. A novel Hurst feature and fuzzy KNN achieved recognition accuracy of 92.87%. Ferdinando et&nbsp;al. <a class="anchor anchor-primary" href="#bib0342" name="bbib0342" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0342"><span class="anchor-text-container"><span class="anchor-text">[342]</span></span></a><span> investigated the effect of feature dimensionality reduction in ECG-based emotion recognition. A bivariate <a href="/topics/computer-science/empirical-mode-decomposition" title="Learn more about empirical mode decomposition from ScienceDirect's AI-generated Topic Pages" class="topic-link">empirical mode decomposition</a> was employed to compute features for KNN based on the statistical distribution of dominant frequencies.</span></div></section><section id="sec0037"><h5 id="cesectitle0040" class="u-margin-m-top u-margin-xs-bottom">5.4.2.2. DL-based FER ECG emotion recognition</h5><div class="u-margin-s-bottom" id="para0125">Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0343" name="bbib0343" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0343"><span class="anchor-text-container"><span class="anchor-text">[343]</span></span></a><span> proposed a novel EmotionalGAN-based framework to enhance the <a href="/topics/computer-science/generalization-ability" title="Learn more about generalization ability from ScienceDirect's AI-generated Topic Pages" class="topic-link">generalization ability</a> of emotion recognition by incorporating the augmented ECG samples generated by EmotionalGAN. Compared with that using only original data, around 5% improvement of average accuracy shows the significance of GAN-based models on the emotion recognition task. Sarkar and Etemad </span><a class="anchor anchor-primary" href="#bib0344" name="bbib0344" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0344"><span class="anchor-text-container"><span class="anchor-text">[344]</span></span></a> introduced one self-supervised approach to training the signal transformation recognition network (STRN) to learn spatiotemporal features and abstract representations of the ECG. The weights of convolutional layers in the STRN are frozen and then train two dense layers to classify arousal and valence.</div></section></section></section></section><section id="sec0038"><h2 id="cesectitle0041" class="u-h4 u-margin-l-top u-margin-xs-bottom">6. Multimodal affective analysis</h2><div class="u-margin-s-bottom" id="para0126">We have reviewed the relevant studies of unimodal feature extraction and emotion classification, in this section we then describe how to integrate multiple unimodal signals to develop a framework of multimodal affective analysis <a class="anchor anchor-primary" href="#bib0345" name="bbib0345" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0345"><span class="anchor-text-container"><span class="anchor-text">[345]</span></span></a>, which can be regarded as the fusion of different modalities <a class="anchor anchor-primary" href="#bib0346" name="bbib0346" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0346"><span class="anchor-text-container"><span class="anchor-text">[346]</span></span></a>, aiming to achieve a more accurate result and more comprehensive understanding than unimodal affect recognition [<a class="anchor anchor-primary" href="#bib0347" name="bbib0347" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0347"><span class="anchor-text-container"><span class="anchor-text">347</span></span></a>,<a class="anchor anchor-primary" href="#bib0348" name="bbib0348" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0348"><span class="anchor-text-container"><span class="anchor-text">348</span></span></a>].</div><div class="u-margin-s-bottom"><div id="para0127">Nowadays, most reviews of multimodal affective analysis [<a class="anchor anchor-primary" href="#bib0012" name="bbib0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0012"><span class="anchor-text-container"><span class="anchor-text">12</span></span></a>,<a class="anchor anchor-primary" href="#bib0014" name="bbib0014" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0014"><span class="anchor-text-container"><span class="anchor-text">14</span></span></a>,<a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">17</span></span></a>] focus on multimodal fusion strategies and classify them into feature-level fusion (or early fusion), decision-level fusion (or late fusion), model-level fusion, and hybrid-level fusion. However, the multimodal affective analysis can be also varied with combinations of different modalities. Therefore, we categorize multimodal affective analysis into multi-physical modality fusion for affective analysis, multi-physiological modality fusion for affective analysis, and physical-physiological modality fusion for affective analysis, and further classify them based on four kinds of fusion strategies. <a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> illustrates prominent examples of using different fusion strategies:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1</span><span><div class="u-margin-s-bottom" id="para0128">Feature-level fusion combines features extracted from the multimodal inputs to form one general feature vector, which is then sent into a classifier. <a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (a), (b) and (c) show examples based on feature-level fusion for visual-audio modalities, text-audio modalities, and visual-audio-text modalities, respectively.</div></span></li><li class="react-xocs-list-item"><span class="list-label">2</span><span><div class="u-margin-s-bottom" id="para0129"><span>Decision-level fusion connects all <a href="/topics/computer-science/decision-vector" title="Learn more about decision vectors from ScienceDirect's AI-generated Topic Pages" class="topic-link">decision vectors</a> independently generated from each modality into one feature vector. </span><a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (d) shows one example based on decision-level fusion for multi-physiological modalities of EGG, ECG and EDA.</div></span></li><li class="react-xocs-list-item"><span class="list-label">3</span><span><div class="u-margin-s-bottom" id="para0130"><span>Model-level fusion discovers the correlation properties between features extracted from different modalities and uses or designs a fusion model with relaxed and smooth types such as HMM and two-stage <a href="/topics/computer-science/extreme-learning-machine" title="Learn more about ELM from ScienceDirect's AI-generated Topic Pages" class="topic-link">ELM</a> </span><a class="anchor anchor-primary" href="#bib0349" name="bbib0349" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0349"><span class="anchor-text-container"><span class="anchor-text">[349]</span></span></a>. <a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (e) and (f) are two examples based on model-level fusion for physical-physiological modalities and visual-audio-text modalities, respectively.</div></span></li><li class="react-xocs-list-item"><span class="list-label">4</span><span><div class="u-margin-s-bottom" id="para0131">Hybrid fusion combines feature-level fusion and decision-level fusion. <a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (g) shows one example based on hybrid fusion for visual-audio-text modalities.</div></span></li></ul></div><figure class="figure text-xs" id="fig0006"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr6.jpg" height="968" alt="Fig 6" aria-describedby="cap0006"><ol class="u-margin-s-bottom"><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr6_lrg.jpg" target="_blank" download="" title="Download high-res image (2MB)"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download high-res image (2MB)</span></span></span></a></li><li><a class="anchor download-link u-font-sans anchor-primary" href="https://ars.els-cdn.com/content/image/1-s2.0-S1566253522000367-gr6.jpg" target="_blank" download="" title="Download full-size image"><span class="anchor-text-container"><span class="anchor-text">Download: <span class="download-link-title">Download full-size image</span></span></span></a></li></ol></span><span class="captions text-s"><span id="cap0006"><p id="spara006"><span class="label">Fig. 6</span>. Taxonomy of multimodal affective analysis. (a) Feature-level fusion for visual-audio emotion recognition adopted from <a class="anchor anchor-primary" href="#bib0360" name="bbib0360" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0360"><span class="anchor-text-container"><span class="anchor-text">[360]</span></span></a>; (b) Feature-level fusion for text-audio emotion recognition adopted from <a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">[361]</span></span></a>; (c) Feature-level fusion for visual-audio-text emotion recognition adopted from <a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a>; (d) Decision-level fusion for multi-physiological affective analysis adopted from <a class="anchor anchor-primary" href="#bib0363" name="bbib0363" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0363"><span class="anchor-text-container"><span class="anchor-text">[363]</span></span></a>; (e) Model-level fusion for physical-physiological affective analysis adopted from <a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a>; (f) Model-level fusion for visual-audio-text emotion recognition adopted from <a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a>; (g) Hybrid-level fusion for visual-audio-text emotion recognition adopted from <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a>.</p></span></span></figure></div><section id="sec0039"><h3 id="cesectitle0042" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.1. Multi-physical modality fusion for affective analysis</h3><div class="u-margin-s-bottom"><div id="para0132">In light of common manners of modality combinations, we categorize multi-physical modalities fusion for affective analysis into visual-audio emotion recognition [<a class="anchor anchor-primary" href="#bib0350" name="bbib0350" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0350"><span class="anchor-text-container"><span class="anchor-text">350</span></span></a>,<a class="anchor anchor-primary" href="#bib0031" name="bbib0031" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0031"><span class="anchor-text-container"><span class="anchor-text">31</span></span></a>], text-audio emotion recognition [<a class="anchor anchor-primary" href="#bib0351" name="bbib0351" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0351"><span class="anchor-text-container"><span class="anchor-text">351</span></span></a>,<a class="anchor anchor-primary" href="#bib0352" name="bbib0352" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0352"><span class="anchor-text-container"><span class="anchor-text">352</span></span></a>], and visual-audio-text emotion recognition [<a class="anchor anchor-primary" href="#bib0353" name="bbib0353" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0353"><span class="anchor-text-container"><span class="anchor-text">353</span></span></a>,<a class="anchor anchor-primary" href="#bib0354" name="bbib0354" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0354"><span class="anchor-text-container"><span class="anchor-text">354</span></span></a>]. <a class="anchor anchor-primary" href="#tbl0011" name="btbl0011" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0011"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;11</span></span></a> shows an overview of representative methods for multi-physical affective analysis, as detailed next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0011"><span class="captions text-s"><span id="cap0017"><p id="spara018"><span class="label">Table 11</span>. Overview of representative methods for multi-physical affective analysis.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en1282">Publication</th><th scope="col" class="align-left valign-top" id="en1283">Year</th><th scope="col" class="align-left valign-top" id="en1284">Feature Representation</th><th scope="col" class="align-left valign-top" id="en1285">Classifier</th><th scope="col" class="align-left valign-top" id="en1286">Fusion Strategy</th><th scope="col" class="align-left valign-top" id="en1287">Database</th><th scope="col" class="align-left valign-top" id="en1288">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en1289" colspan="7"><strong><em>Visual-audio Emotion Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en1290"><a class="anchor anchor-primary" href="#bib0360" name="bbib0360" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0360"><span class="anchor-text-container"><span class="anchor-text">[360]</span></span></a></td><td class="align-left valign-top" id="en1291">2018</td><td class="align-left valign-top" id="en1292">Acoustic, Geometric and HOG-TOP</td><td class="align-left valign-top" id="en1293">Multiple kernel SVM</td><td class="align-left valign-top" id="en1294">Feature-level</td><td class="align-left valign-top" id="en1295">CK; AFEW</td><td class="align-left valign-top" id="en1296"><a class="anchor anchor-primary" href="#tb11fn1" name="btb11fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>7 classes: 95.7 <a class="anchor anchor-primary" href="#tb11fn1" name="btb11fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>7 classes: 45.20</td></tr><tr><td class="align-left valign-top" id="en1297"><a class="anchor anchor-primary" href="#bib0031" name="bbib0031" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0031"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a></td><td class="align-left valign-top" id="en1298">2017</td><td class="align-left valign-top" id="en1299">CNN, Resnet</td><td class="align-left valign-top" id="en1300">LSTM</td><td class="align-left valign-top" id="en1301">Feature-level</td><td class="align-left valign-top" id="en1302">RECOLA</td><td class="align-left valign-top" id="en1303">A/V: 78.80/73.20</td></tr><tr><td class="align-left valign-top" id="en1304"><a class="anchor anchor-primary" href="#bib0364" name="bbib0364" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0364"><span class="anchor-text-container"><span class="anchor-text">[364]</span></span></a></td><td class="align-left valign-top" id="en1305">2020</td><td class="align-left valign-top" id="en1306">2D ResNet+Attention 3D ResNet+Attention</td><td class="align-left valign-top" id="en1307">FC</td><td class="align-left valign-top" id="en1308">Feature-level</td><td class="align-left valign-top" id="en1309">VideoEmotion-8<br>Ekman-6</td><td class="align-left valign-top" id="en1310">8 classes: 54.50 6 classes: 55.30</td></tr><tr><td class="align-left valign-top" id="en1311"><a class="anchor anchor-primary" href="#bib0367" name="bbib0367" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0367"><span class="anchor-text-container"><span class="anchor-text">[367]</span></span></a></td><td class="align-left valign-top" id="en1312">2020</td><td class="align-left valign-top" id="en1313">Multitask CNN</td><td class="align-left valign-top" id="en1314">Meta-Classifier</td><td class="align-left valign-top" id="en1315">Decision-level</td><td class="align-left valign-top" id="en1316">eNTERFACCE</td><td class="align-left valign-top" id="en1317">6 classes: 81.36</td></tr><tr><td class="align-left valign-top" id="en1318"><a class="anchor anchor-primary" href="#bib0278" name="bbib0278" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0278"><span class="anchor-text-container"><span class="anchor-text">[278]</span></span></a></td><td class="align-left valign-top" id="en1319">2017</td><td class="align-left valign-top" id="en1320">C3D&nbsp;+&nbsp;DBN</td><td class="align-left valign-top" id="en1321">Score-level Fusion</td><td class="align-left valign-top" id="en1322">Model-based</td><td class="align-left valign-top" id="en1323">eNTERFACE</td><td class="align-left valign-top" id="en1324">6 classes: 89.39</td></tr><tr><td class="align-left valign-top" id="en1325"><a class="anchor anchor-primary" href="#bib0349" name="bbib0349" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0349"><span class="anchor-text-container"><span class="anchor-text">[349]</span></span></a></td><td class="align-left valign-top" id="en1326">2019</td><td class="align-left valign-top" id="en1327">2D CNN, 3D CNN</td><td class="align-left valign-top" id="en1328">ELM-based fusion, SVM</td><td class="align-left valign-top" id="en1329">Model-based</td><td class="align-left valign-top" id="en1330">Big Data eNTERFACE</td><td class="align-left valign-top" id="en1331">3 classes: 91.30<br>6 classes: 78.42</td></tr><tr><td class="align-left valign-top" id="en1332" colspan="7"><strong><em>Text-audio Emotion Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en1333"><a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">[361]</span></span></a></td><td class="align-left valign-top" id="en1334">2020</td><td class="align-left valign-top" id="en1335">A-DCNN, T-DNN Self-attention</td><td class="align-left valign-top" id="en1336">FC</td><td class="align-left valign-top" id="en1337">Feature-level</td><td class="align-left valign-top" id="en1338">IEMOCAP</td><td class="align-left valign-top" id="en1339"><a class="anchor anchor-primary" href="#tb11fn2" name="btb11fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>4 classes: 80.51<br><a class="anchor anchor-primary" href="#tb11fn3" name="btb11fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>4 classes: 79.22</td></tr><tr><td class="align-left valign-top" id="en1340"><a class="anchor anchor-primary" href="#bib0374" name="bbib0374" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0374"><span class="anchor-text-container"><span class="anchor-text">[374]</span></span></a></td><td class="align-left valign-top" id="en1341">2011</td><td class="align-left valign-top" id="en1342">Acoustic-prosodic Semantic labels</td><td class="align-left valign-top" id="en1343">Base classifiers, MDT, MaxEnt</td><td class="align-left valign-top" id="en1344">Decision-level</td><td class="align-left valign-top" id="en1345">2033 utterances</td><td class="align-left valign-top" id="en1346">4 classes: 83.55<br><a class="anchor anchor-primary" href="#tb11fn4" name="btb11fn4" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn4"><span class="anchor-text-container"><span class="anchor-text"><sup>4</sup></span></span></a>4 classes: 85.79</td></tr><tr><td class="align-left valign-top" id="en1347"><a class="anchor anchor-primary" href="#bib0376" name="bbib0376" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0376"><span class="anchor-text-container"><span class="anchor-text">[376]</span></span></a></td><td class="align-left valign-top" id="en1348">2020</td><td class="align-left valign-top" id="en1349">Acoustic features Word embeddings</td><td class="align-left valign-top" id="en1350">Pooling Scalar weight fusion</td><td class="align-left valign-top" id="en1351">Feature-level Decision-level</td><td class="align-left valign-top" id="en1352">IEMOCAP; MSP-PODCAST</td><td class="align-left valign-top" id="en1353"><a class="anchor anchor-primary" href="#tb11fn5" name="btb11fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>65.10/<a class="anchor anchor-primary" href="#tb11fn5" name="btb11fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>58.20<br><a class="anchor anchor-primary" href="#tb11fn5" name="btb11fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>63.90/<a class="anchor anchor-primary" href="#tb11fn5" name="btb11fn5" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn5"><span class="anchor-text-container"><span class="anchor-text"><sup>5</sup></span></span></a>58.00</td></tr><tr><td class="align-left valign-top" id="en1354" colspan="7"><strong><em>Visual-audio-text Emotion Recognition Emotion Recognition</em></strong></td></tr><tr><td class="align-left valign-top" id="en1355"><a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a></td><td class="align-left valign-top" id="en1356">2020</td><td class="align-left valign-top" id="en1357">Proxy and Attention<br>Multiplicative fusion</td><td class="align-left valign-top" id="en1358">FC</td><td class="align-left valign-top" id="en1359">Feature-level</td><td class="align-left valign-top" id="en1360">IEMOCAP CMU-MOSEI</td><td class="align-left valign-top" id="en1361">4 classes<a class="anchor anchor-primary" href="#tb11fn1" name="btb11fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>:82.70<br>6 classes<a class="anchor anchor-primary" href="#tb11fn1" name="btb11fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>:89.00</td></tr><tr><td class="align-left valign-top" id="en1362"><a class="anchor anchor-primary" href="#bib0382" name="bbib0382" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0382"><span class="anchor-text-container"><span class="anchor-text">[382]</span></span></a></td><td class="align-left valign-top" id="en1363">2015</td><td class="align-left valign-top" id="en1364">CNN, handcrafted, CFS, PCA</td><td class="align-left valign-top" id="en1365">MKL</td><td class="align-left valign-top" id="en1366">Feature-level Decision-level</td><td class="align-left valign-top" id="en1367">HOW</td><td class="align-left valign-top" id="en1368">3 classes: 88.60<br>3 classes: 86.27</td></tr><tr><td class="align-left valign-top" id="en1369"><a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></td><td class="align-left valign-top" id="en1370">2019</td><td class="align-left valign-top" id="en1371">Three Bi-GRU</td><td class="align-left valign-top" id="en1372">CIM-attention</td><td class="align-left valign-top" id="en1373">Model-based</td><td class="align-left valign-top" id="en1374">CMU-MOSEI</td><td class="align-left valign-top" id="en1375"><a class="anchor anchor-primary" href="#tb11fn2" name="btb11fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>Multi-label: 62.80<br><a class="anchor anchor-primary" href="#tb11fn2" name="btb11fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>2-class: 80.50</td></tr><tr><td class="align-left valign-top" id="en1376"><a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a></td><td class="align-left valign-top" id="en1377">2013</td><td class="align-left valign-top" id="en1378">Facial movement, MFCC</td><td class="align-left valign-top" id="en1379">SVM, BiLSMT</td><td class="align-left valign-top" id="en1380">Hybrid-level</td><td class="align-left valign-top" id="en1381">SEMAINE</td><td class="align-left valign-top" id="en1382"><a class="anchor anchor-primary" href="#tb11fn6" name="btb11fn6" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb11fn6"><span class="anchor-text-container"><span class="anchor-text"><sup>6</sup></span></span></a>65.20</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb11fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0041">LOSO;</div></dd><dt id="tb11fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0042"><a href="/topics/computer-science/weighted-accuracy" title="Learn more about WA from ScienceDirect's AI-generated Topic Pages" class="topic-link">WA</a>;</div></dd><dt id="tb11fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0043">UA;</div></dd><dt id="tb11fn4">4</dt><dd><div class="u-margin-s-bottom" id="notep0044">The recognition accuracy considering the individual <a href="/topics/psychology/personality-trait" title="Learn more about personality trait from ScienceDirect's AI-generated Topic Pages" class="topic-link">personality trait</a> for personalized application;</div></dd><dt id="tb11fn5">5</dt><dd><div class="u-margin-s-bottom" id="notep0045">The median values evaluated on IEMOCAP/MSP-PODCAST databases;</div></dd><dt id="tb11fn6">6</dt><dd><div class="u-margin-s-bottom" id="notep0045q">Mean WA of Arousal, Expectation, Power and Valence.</div></dd></dl></div></div><section id="sec0040"><h4 id="cesectitle0043" class="u-margin-m-top u-margin-xs-bottom">6.1.1. Visual-audio emotion recognition</h4><div class="u-margin-s-bottom" id="para0133"><span>Visual and <a href="/topics/physics-and-astronomy/audio-signal" title="Learn more about audio signals from ScienceDirect's AI-generated Topic Pages" class="topic-link">audio signals</a> are the most natural and affective cues to express emotions when people communicate in daily life </span><a class="anchor anchor-primary" href="#bib0355" name="bbib0355" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0355"><span class="anchor-text-container"><span class="anchor-text">[355]</span></span></a>. Many research works [<a class="anchor anchor-primary" href="#bib0356" name="bbib0356" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0356"><span class="anchor-text-container"><span class="anchor-text">[356]</span></span></a>, <a class="anchor anchor-primary" href="#bib0357" name="bbib0357" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0357"><span class="anchor-text-container"><span class="anchor-text">[357]</span></span></a>, <a class="anchor anchor-primary" href="#bib0358" name="bbib0358" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0358"><span class="anchor-text-container"><span class="anchor-text">[358]</span></span></a>, <a class="anchor anchor-primary" href="#bib0359" name="bbib0359" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0359"><span class="anchor-text-container"><span class="anchor-text">[359]</span></span></a>,<a class="anchor anchor-primary" href="#bib0350" name="bbib0350" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0350"><span class="anchor-text-container"><span class="anchor-text">350</span></span></a>] show that visual-audio emotion recognition outperforms visual or audio emotion recognition.</div><div class="u-margin-s-bottom" id="para0134"><strong>Feature-level fusion</strong>. Chen et&nbsp;al. <a class="anchor anchor-primary" href="#bib0360" name="bbib0360" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0360"><span class="anchor-text-container"><span class="anchor-text">[360]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a><span> (a)) proposed ML-based visual-audio emotion recognition by fusing dynamic HOG-TOP <a href="/topics/computer-science/texture-feature" title="Learn more about texture features from ScienceDirect's AI-generated Topic Pages" class="topic-link">texture features</a> and acoustic/ geometric features. These two kinds of features are then sent into a multiple kernel SVM for FER both under the wild and lab-controlled environments. Tzirakis et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0031" name="bbib0031" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0031"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a><span><span> adopted a <a href="/topics/computer-science/convolutional-neural-network" title="Learn more about CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN</a> and a </span><a href="/topics/computer-science/deep-residual-network" title="Learn more about deep residual network from ScienceDirect's AI-generated Topic Pages" class="topic-link">deep residual network</a><span> to extract audio and visual features, respectively; and then concatenated these visual-audio features to feed into a 2-layer <a href="/topics/computer-science/long-short-term-memory-network" title="Learn more about LSTM from ScienceDirect's AI-generated Topic Pages" class="topic-link">LSTM</a> to predict Arousal-Valence values.</span></span></div><div class="u-margin-s-bottom" id="para0135"><span>Various <a href="/topics/computer-science/attention-machine-learning" title="Learn more about attention mechanisms from ScienceDirect's AI-generated Topic Pages" class="topic-link">attention mechanisms</a> have been successfully applied for visual-audio emotion recognition [</span><a class="anchor anchor-primary" href="#bib0364" name="bbib0364" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0364"><span class="anchor-text-container"><span class="anchor-text">364</span></span></a>,<a class="anchor anchor-primary" href="#bib0365" name="bbib0365" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0365"><span class="anchor-text-container"><span class="anchor-text">365</span></span></a>]. For example, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0365" name="bbib0365" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0365"><span class="anchor-text-container"><span class="anchor-text">[365]</span></span></a><span> introduced an embedded attention mechanism to obtain the emotion-related regions from their respective modalities. To deeply fuse video-audio features, the factorized bilinear pooling (FBP) fusion strategy was proposed in consideration of feature differences in expressions of video frames. The FBP achieves a <a href="/topics/computer-science/recognition-accuracy" title="Learn more about recognition accuracy from ScienceDirect's AI-generated Topic Pages" class="topic-link">recognition accuracy</a> of 62.48% on AFEW of the audio-video sub-challenge in EmotiW2018. Zhao et&nbsp;al </span><a class="anchor anchor-primary" href="#bib0364" name="bbib0364" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0364"><span class="anchor-text-container"><span class="anchor-text">[364]</span></span></a><span> proposed a novel deep visual-audio attention network (VAANet) with specific attention modules and polarity-consistent cross-entropy loss. Specifically, spatial, channel-wise, and temporal attentions are integrated with a 3D <a href="/topics/physics-and-astronomy/convolutional-neural-network" title="Learn more about CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">CNN</a> </span><a class="anchor anchor-primary" href="#bib0366" name="bbib0366" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0366"><span class="anchor-text-container"><span class="anchor-text">[366]</span></span></a> for video frame segments, and spatial attention is integrated with a 2D CNN (ResNet-18) for audio MFCC segments.</div><div class="u-margin-s-bottom" id="para0136"><strong>Decision-level fusion</strong>. Hao et&nbsp;al. <a class="anchor anchor-primary" href="#bib0367" name="bbib0367" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0367"><span class="anchor-text-container"><span class="anchor-text">[367]</span></span></a> proposed an ensemble visual-audio emotion recognition framework based on multi-task and blending learning with multiple features. Specifically, SVM classifiers and CNNs for handcraft-based and DL-based visual-audio features generated four sub-models, which are then fused to predict the final emotion based on the blending ensemble algorithm. The work <a class="anchor anchor-primary" href="#bib0367" name="bbib0367" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0367"><span class="anchor-text-container"><span class="anchor-text">[367]</span></span></a> achieved the average accuracies of 81.36% (speaker-independent) and 78.42% (speaker-dependent) on eNTERFACE <a class="anchor anchor-primary" href="#bib0368" name="bbib0368" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0368"><span class="anchor-text-container"><span class="anchor-text">[368]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0137"><span>Model-level fusion. It requires an ML-based model (e.g., HMM, <a href="/topics/computer-science/kalman-filter" title="Learn more about Kalman filters from ScienceDirect's AI-generated Topic Pages" class="topic-link">Kalman filters</a> and DBN) to construct the relationships of different modalities to make decisions. Lin et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0033" name="bbib0033" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0033"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a><span> proposed a semi-coupled HMM (SC-HMM) to align the temporal relations of audio-visual signals, followed by a <a href="/topics/computer-science/bayesian-classifier" title="Learn more about Bayesian classifier from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian classifier</a> with an error weighted scheme. The SC-HMM with Bayesian achieves prominent average accuracies of 90.59% (four-class emotions) and 78.13% (four quadrants) on the multimedia human-machine communication posed database and public SEMAINE database, respectively. Glodek et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0369" name="bbib0369" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0369"><span class="anchor-text-container"><span class="anchor-text">[369]</span></span></a><span><span> designed <a href="/topics/earth-and-planetary-sciences/kalman-filter" title="Learn more about Kalman filters from ScienceDirect's AI-generated Topic Pages" class="topic-link">Kalman filters</a><span> based on a <a href="/topics/neuroscience/markov-models" title="Learn more about Markov model from ScienceDirect's AI-generated Topic Pages" class="topic-link">Markov model</a> to combine temporally ordered classifier decisions with the reject option to recognize the </span></span><a href="/topics/psychology/emotion" title="Learn more about affective states from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective states</a><span>. For audio-visual data, the unimodal feature extractors and <a href="/topics/computer-science/base-classifier" title="Learn more about base classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">base classifiers</a> are fused based on a Kalman filter and confidence measures.</span></span></div><div class="u-margin-s-bottom" id="para0138">Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a><span> utilized CNNs to extract audio-visual features and developed a deep fusion method (DBNs) for <a href="/topics/computer-science/feature-fusion" title="Learn more about feature fusion from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature fusion</a>. A linear SVM classifier achieved average accuracies of 80.36%, 54.57% and 85.97% on RML, eNTERFACE05 and BAUM-1s, respectively. Similarly, Nguyen et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0278" name="bbib0278" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0278"><span class="anchor-text-container"><span class="anchor-text">[278]</span></span></a> proposed a score-level fusion approach to compute all likelihoods of DBNs trained on spatiotemporal information of the audio-video streams. Hossain and Muhammad <a class="anchor anchor-primary" href="#bib0349" name="bbib0349" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0349"><span class="anchor-text-container"><span class="anchor-text">[349]</span></span></a><span> used a 2D CNN and a <a href="/topics/computer-science/3-dimensional-convolutional-neural-networks" title="Learn more about 3D CNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">3D CNN</a> to extract high-level representations of the pre-processed audio-video signals. A two-stage ELM based fusion model with an SVM classifier was designed to estimate different emotional states.</span></div></section><section id="sec0041"><h4 id="cesectitle0044" class="u-margin-m-top u-margin-xs-bottom">6.1.2. Text-audio emotion recognition</h4><div class="u-margin-s-bottom" id="para0139">Although SER [<a class="anchor anchor-primary" href="#bib0026" name="bbib0026" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0026"><span class="anchor-text-container"><span class="anchor-text">26</span></span></a>,<a class="anchor anchor-primary" href="#bib0163" name="bbib0163" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0163"><span class="anchor-text-container"><span class="anchor-text">163</span></span></a>,<a class="anchor anchor-primary" href="#bib0178" name="bbib0178" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0178"><span class="anchor-text-container"><span class="anchor-text">178</span></span></a>] and TSA [<a class="anchor anchor-primary" href="#bib0156" name="bbib0156" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0156"><span class="anchor-text-container"><span class="anchor-text">156</span></span></a>,<a class="anchor anchor-primary" href="#bib0370" name="bbib0370" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0370"><span class="anchor-text-container"><span class="anchor-text">370</span></span></a>] have achieved significant progress, performing the two tasks separately makes it hard to achieve compelling results <a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">[361]</span></span></a>. Text-audio emotion recognition approaches use linguistic content and speech clues to enhance the performance of the unimodal emotion recognition system [<a class="anchor anchor-primary" href="#bib0352" name="bbib0352" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0352"><span class="anchor-text-container"><span class="anchor-text">352</span></span></a>,<a class="anchor anchor-primary" href="#bib0371" name="bbib0371" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0371"><span class="anchor-text-container"><span class="anchor-text">371</span></span></a>].</div><div class="u-margin-s-bottom" id="para0140"><strong>Feature-level fusion</strong>. Yoon et&nbsp;al. <a class="anchor anchor-primary" href="#bib0372" name="bbib0372" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0372"><span class="anchor-text-container"><span class="anchor-text">[372]</span></span></a><span> proposed a deep dual recurrent <a href="/topics/physics-and-astronomy/neural-network" title="Learn more about neural network from ScienceDirect's AI-generated Topic Pages" class="topic-link">neural network</a> for encoding audio-text sequences and then concatenated their outputs to predict the final emotion. It achieves an accuracy of 71.8% (four classes) on IEMOCAP. Afterwards, Cai et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0373" name="bbib0373" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0373"><span class="anchor-text-container"><span class="anchor-text">[373]</span></span></a> designed an improved CNN and Bi-LSTM to extract spatial features and capture their temporal dynamics. Considering the phonemes effect on emotion recognition, Zhang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0371" name="bbib0371" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0371"><span class="anchor-text-container"><span class="anchor-text">[371]</span></span></a> utilized a temporal CNN to investigate the acoustic and lexical properties of phonetic information based on unimodal, multimodal single-stage fusion, and multimodal multi-stage fusion systems. To deeply exploit and fuse text-acoustic features for emotion classification, Priyasad et&nbsp;al. <a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">[361]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (b)) designed T-DNN (DCNNs and Bi-RNN followed by other DCNNs) and A-DCNN (SincNet with band-pass filters followed by a DCNN) to extract textual features and learning acoustic features, respectively. Textual-acoustic features are fused with different attention strategies (self-attention or no attention) to predict four emotions on IEMOCAP.</div><div class="u-margin-s-bottom" id="para0141"><strong>Decision-level fusion.</strong> Wu et&nbsp;al. <a class="anchor anchor-primary" href="#bib0374" name="bbib0374" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0374"><span class="anchor-text-container"><span class="anchor-text">[374]</span></span></a><span><span><span> fused acoustic-prosodic information based on a meta <a href="/topics/biochemistry-genetics-and-molecular-biology/decision-trees" title="Learn more about decision tree from ScienceDirect's AI-generated Topic Pages" class="topic-link">decision tree</a> (MDT) with multiple </span><a href="/topics/pharmacology-toxicology-and-pharmaceutical-science/base" title="Learn more about base from ScienceDirect's AI-generated Topic Pages" class="topic-link">base</a> classifiers (e.g. GMM, SVM, and MLP), and employed a </span><a href="/topics/computer-science/maximum-entropy-model" title="Learn more about maximum entropy model from ScienceDirect's AI-generated Topic Pages" class="topic-link">maximum entropy model</a><span><span> (MaxEnt) to establish the relationship between emotional states and emotion <a href="/topics/computer-science/association-rules" title="Learn more about association rules from ScienceDirect's AI-generated Topic Pages" class="topic-link">association rules</a> in </span><a href="/topics/computer-science/semantic-label" title="Learn more about semantic labels from ScienceDirect's AI-generated Topic Pages" class="topic-link">semantic labels</a><span> for <a href="/topics/computer-science/speech-emotion-recognition" title="Learn more about speech emotion recognition from ScienceDirect's AI-generated Topic Pages" class="topic-link">speech emotion recognition</a> and text emotion recognition, respectively. Using the weighted product fusion strategy, AP-based and SL-based emotion confidences are fused to predict final emotions.</span></span></span></div><div class="u-margin-s-bottom" id="para0142"><strong>Feature-level fusion versus decision-level fusion.</strong> To verify which feature-level fusion or decision-level fusion is more effective in text-audio emotion recognition, Jin et&nbsp;al. <a class="anchor anchor-primary" href="#bib0375" name="bbib0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0375"><span class="anchor-text-container"><span class="anchor-text">[375]</span></span></a> firstly generated new lexical features and different acoustic features and then utilized two fusion strategies to achieve four-class recognition accuracies of 55.4% and 69.2% on IEMOCAP, respectively. Considering the emotional dialogue composed of sound and spoken content, Pepino et&nbsp;al. <a class="anchor anchor-primary" href="#bib0376" name="bbib0376" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0376"><span class="anchor-text-container"><span class="anchor-text">[376]</span></span></a> exploited multiple dual RNNs to encode audio-text sequences. The feature-level fusion and decision-level fusion approaches in 3 different ways are explored to compare their performances.</div></section><section id="sec0042"><h4 id="cesectitle0045" class="u-margin-m-top u-margin-xs-bottom">6.1.3. Visual-audio-text emotion recognition</h4><div class="u-margin-s-bottom" id="para0143"><span>The vocal modulation, facial expression, and context-based text provide important cues to better identify the true <a href="/topics/neuroscience/emotion" title="Learn more about affective states from ScienceDirect's AI-generated Topic Pages" class="topic-link">affective states</a> of the opinion holder [</span><a class="anchor anchor-primary" href="#bib0377" name="bbib0377" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0377"><span class="anchor-text-container"><span class="anchor-text">377</span></span></a>,<a class="anchor anchor-primary" href="#bib0378" name="bbib0378" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0378"><span class="anchor-text-container"><span class="anchor-text">378</span></span></a>]. For example, when a lady is proposed and then she says “I do” in tears, none of textual-based, audio-based, or visual-based emotion recognition models can predict confident results. While visual-audio-text emotion recognition leads to a better solution.</div><div class="u-margin-s-bottom" id="para0144"><strong>Feature-level fusion.</strong> Veronica et&nbsp;al. <a class="anchor anchor-primary" href="#bib0379" name="bbib0379" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0379"><span class="anchor-text-container"><span class="anchor-text">[379]</span></span></a> used BoW, OpenEAR <a class="anchor anchor-primary" href="#bib0380" name="bbib0380" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0380"><span class="anchor-text-container"><span class="anchor-text">[380]</span></span></a>, and vision software to extract linguistic, audio and visual features, respectively. Compared with three unimodal and three bimodal models, the multimodal (text-audio-visual) model can significantly improve the model performance over the individual use of one modality or fusion of any two modalities. In addition, Poria et&nbsp;al. <a class="anchor anchor-primary" href="#bib0354" name="bbib0354" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0354"><span class="anchor-text-container"><span class="anchor-text">[354]</span></span></a><span><span> utilized a standard RNN, a deep CNN, and openSMILE to capture <a href="/topics/computer-science/temporal-dependence" title="Learn more about temporal dependence from ScienceDirect's AI-generated Topic Pages" class="topic-link">temporal dependence</a> of visual data, spatial information of textual data and low-level descriptors of audio data, respectively. The </span><a href="/topics/computer-science/multiple-kernel-learning" title="Learn more about MKL from ScienceDirect's AI-generated Topic Pages" class="topic-link">MKL</a> is further designed for feature selection of different modalities to improve the recognition results. Mittal et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a><span> (c)) proposed the multiplicative multimodal emotion recognition (M3ER): firstly, feature vectors are extracted from the raw three modalities; then, these features are transferred into modality check step to retain the effective features and discard the ineffectual ones which are used to regenerate proxy feature vectors; finally, selected features are fused to predict six emotions based on the multiplicative feature-level fusion combined with attention module. The M3ER achieved better recognition accuracies of 82.7% and 89.0% on IEMOCAP and CMU-MOSEI, respectively. Different from language-independent approaches for English or German sentiment analysis, Chinese sentiment analysis not only understands symbols with explicit meaning, but also captures <a href="/topics/earth-and-planetary-sciences/phonemics" title="Learn more about phonemic from ScienceDirect's AI-generated Topic Pages" class="topic-link">phonemic</a> orthography (tonal language) with implicit meaning. Based on this assumption, Peng et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0381" name="bbib0381" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0381"><span class="anchor-text-container"><span class="anchor-text">[381]</span></span></a><span> proposed <a href="/topics/computer-science/reinforcement-learning" title="Learn more about reinforcement learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">reinforcement learning</a> based disambiguate intonation for sentiment analysis (DISA) which consists of policy network, embedding lookup, loss computation, and feature-level fusion of three modalities. By integrating phonetic features with textual and visual representations, multimodal Chinese sentiment analysis significantly outperforms than unimodal models.</span></div><div class="u-margin-s-bottom" id="para0145"><strong>Feature-level fusion versus decision-level fusion.</strong> Poria et&nbsp;al. <a class="anchor anchor-primary" href="#bib0382" name="bbib0382" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0382"><span class="anchor-text-container"><span class="anchor-text">[382]</span></span></a> utilized a CNN to extract textual features, calculated handcrafted features from visual data, and generated audio features by the openSMILE. These three kinds of feature vectors were fused and then used to train a classifier based on the MKL. Feature-level fusion and decision-level fusion with feature selection are conducted to implement unimodal, bimodal and multimodal emotion recognition employed on HOW <a class="anchor anchor-primary" href="#bib0104" name="bbib0104" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0104"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a>. Experimental results show that textual emotion recognition achieves the best performance among three unimodal emotion recognition methods, and in feature-level fusion, visual-audio-text emotion recognition outperforms other unimodal and bimodal emotion recognition. Besides, the accuracy of feature-level fusion is significantly higher than that of decision-level fusion at the expense of computational speed.</div><div class="u-margin-s-bottom" id="para0146"><strong>Model-level fusion.</strong><span> Considering the <a href="/topics/computer-science/interdependency" title="Learn more about interdependencies from ScienceDirect's AI-generated Topic Pages" class="topic-link">interdependencies</a> and relations among the utterances of a video </span><a class="anchor anchor-primary" href="#bib0378" name="bbib0378" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0378"><span class="anchor-text-container"><span class="anchor-text">[378]</span></span></a>, Poria et&nbsp;al. <a class="anchor anchor-primary" href="#bib0383" name="bbib0383" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0383"><span class="anchor-text-container"><span class="anchor-text">[383]</span></span></a> introduced some variants of the contextual LSTM into the hierarchical architecture to extract context-dependent multimodal utterance features. The visual-audio-text features are concatenated and then fed into a contextual LSTM to predict emotions, reaching 80.3%, 68.1% and 76.1% on MOSI, MOUD and IEMOCAP, respectively. Akhtar et&nbsp;al. <a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a><span> (f)) proposed an end-to-end DL-based multi-task learning for multimodal emotion recognition and sentiment analysis. Due to the unequal importance of three modalities, a context-level inter-modal (CIM) attention module is designed to learn the joint association between textual-audio-visual features of utterances captured by three bi-directional <a href="/topics/earth-and-planetary-sciences/gated-recurrent-unit" title="Learn more about gated recurrent unit from ScienceDirect's AI-generated Topic Pages" class="topic-link">gated recurrent unit</a><span> (biGRU) networks. These features of three CIMs and three <a href="/topics/computer-science/individual-modality" title="Learn more about individual modalities from ScienceDirect's AI-generated Topic Pages" class="topic-link">individual modalities</a> are concatenated to generate high-level feature representation to predict sentiments and multi-label emotions.</span></span></div><div class="u-margin-s-bottom" id="para0147"><strong>Hybrid-level fusion.</strong> To take full advantage of feature-based fusion and decision-based fusion, and overcome the disadvantages of both, Wöllmer et&nbsp;al. <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (g)) used BoW or Bag-of-N-Gram (BoNG) features with SVM for linguistic sentiment classification, and fused audio-video features with Bi-LSTM for audio-visual emotion recognition. And then these two prediction results were fused to obtain the final emotion under a strategy of weighted fusion.</div></section></section><section id="sec0043"><h3 id="cesectitle0046" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.2. Multi-physiological modality fusion for affective analysis</h3><div class="u-margin-s-bottom"><div id="para0148"><span>With the enhancement and refinement of <a href="/topics/computer-science/wearable-technology" title="Learn more about wearable technologies from ScienceDirect's AI-generated Topic Pages" class="topic-link">wearable technologies</a>, automatic affective analysis based on multi-physiological modalities has attracted more attention </span><a class="anchor anchor-primary" href="#bib0384" name="bbib0384" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0384"><span class="anchor-text-container"><span class="anchor-text">[384]</span></span></a><span>. However, due to the complexity of emotion and significant individual differences in <a href="/topics/computer-science/physiological-response" title="Learn more about physiological responses from ScienceDirect's AI-generated Topic Pages" class="topic-link">physiological responses</a> </span><a class="anchor anchor-primary" href="#bib0348" name="bbib0348" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0348"><span class="anchor-text-container"><span class="anchor-text">[348]</span></span></a>, it is difficult to achieve satisfactory prediction performance with EEG-based or ECG-based emotion recognition. In this sub-section, we review multi-physiological modality fusion for affective analysis. <a class="anchor anchor-primary" href="#tbl0012" name="btbl0012" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0012"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;12</span></span></a> provides an overview of representative methods for multi-physiological affective analysis, as detailed next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0012"><span class="captions text-s"><span id="cap0018"><p id="spara020"><span class="label">Table 12</span>. Overview of some representative methods for multi-physiological affective analysis.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en1383">Publication</th><th scope="col" class="align-left valign-top" id="en1384">Year</th><th scope="col" class="align-left valign-top" id="en1385">Signal</th><th scope="col" class="align-left valign-top" id="en1386">Architecture</th><th scope="col" class="align-left valign-top" id="en1387">Fusion Strategy</th><th scope="col" class="align-left valign-top" id="en1388">Database</th><th scope="col" class="align-left valign-top" id="en1389">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en1390"><a class="anchor anchor-primary" href="#bib0347" name="bbib0347" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0347"><span class="anchor-text-container"><span class="anchor-text">[347]</span></span></a></td><td class="align-left valign-top" id="en1391">2014</td><td class="align-left valign-top" id="en1392">EEG, GSR, ST, BVP, RESP, EMG, EOG</td><td class="align-left valign-top" id="en1393">DWT, Multiple kernel SVM</td><td class="align-left valign-top" id="en1394">Feature-level</td><td class="align-left valign-top" id="en1395">DEAP</td><td class="align-left valign-top" id="en1396">13 classes: 85.00</td></tr><tr><td class="align-left valign-top" id="en1397"><a class="anchor anchor-primary" href="#bib0387" name="bbib0387" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0387"><span class="anchor-text-container"><span class="anchor-text">[387]</span></span></a></td><td class="align-left valign-top" id="en1398">2019</td><td class="align-left valign-top" id="en1399">EDA, BVP, zEMG</td><td class="align-left valign-top" id="en1400">DBN, FGSVM</td><td class="align-left valign-top" id="en1401">Feature-level</td><td class="align-left valign-top" id="en1402">DEAP</td><td class="align-left valign-top" id="en1403">5 classes: 89.53 A/V: 65.10/61.80</td></tr><tr><td class="align-left valign-top" id="en1404"><a class="anchor anchor-primary" href="#bib0363" name="bbib0363" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0363"><span class="anchor-text-container"><span class="anchor-text">[363]</span></span></a></td><td class="align-left valign-top" id="en1405">2019</td><td class="align-left valign-top" id="en1406">EEG, ECG, EDA</td><td class="align-left valign-top" id="en1407">AILE-VAE, SVM</td><td class="align-left valign-top" id="en1408">Decision-level</td><td class="align-left valign-top" id="en1409">AMIGOS</td><td class="align-left valign-top" id="en1410">A/V: 68.80/ 67.00</td></tr><tr><td class="align-left valign-top" id="en1411"><a class="anchor anchor-primary" href="#bib0390" name="bbib0390" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0390"><span class="anchor-text-container"><span class="anchor-text">[390]</span></span></a></td><td class="align-left valign-top" id="en1412">2020</td><td class="align-left valign-top" id="en1413">EEG, ECG, GSR</td><td class="align-left valign-top" id="en1414">Attention-based LSTM-RNNs, DNN</td><td class="align-left valign-top" id="en1415">Decision-level</td><td class="align-left valign-top" id="en1416">AMIGOS</td><td class="align-left valign-top" id="en1417">A/V: 83.3/79.40</td></tr><tr><td class="align-left valign-top" id="en1418"><a class="anchor anchor-primary" href="#bib0392" name="bbib0392" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0392"><span class="anchor-text-container"><span class="anchor-text">[392]</span></span></a></td><td class="align-left valign-top" id="en1419">2017</td><td class="align-left valign-top" id="en1420">EEG, EOG, EMG, ST, GSR, BVP, RESP</td><td class="align-left valign-top" id="en1421">Deep stacked AE, Bayesian model</td><td class="align-left valign-top" id="en1422">Model-based</td><td class="align-left valign-top" id="en1423">DEAP</td><td class="align-left valign-top" id="en1424">A/V: 77.19/76.17</td></tr></tbody></table></div></div></div><div class="u-margin-s-bottom" id="para0149"><strong>Feature-level fusion.</strong> Li et&nbsp;al. <a class="anchor anchor-primary" href="#bib0385" name="bbib0385" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0385"><span class="anchor-text-container"><span class="anchor-text">[385]</span></span></a> collected multi-physiological emotion signals induced via music, picture or video, and extracted low-level descriptors and statistical features from the signal waveforms. Then, they fused these features for emotion prediction using ML-based classifiers with a Group-based IRS (Individual Response Specificity) model. The combination of RF with Group-based IRS achieved a higher accuracy of about 90% in the imbalance of the emotion database. Similarly, Nakisa et&nbsp;al. <a class="anchor anchor-primary" href="#bib0386" name="bbib0386" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0386"><span class="anchor-text-container"><span class="anchor-text">[386]</span></span></a> pre-processed, extracted and fused time-frequency features of EEG and BVP, and then fed them into an LSTM network, whose hyperparameters were optimized by differential evolution (DE). On their self-collected dataset, its overall accuracy was 77.68% for the four-quadrant dimensional emotions.</div><div class="u-margin-s-bottom" id="para0150"><span>Using ECG, <a href="/topics/neuroscience/electrodermal-response" title="Learn more about GSR from ScienceDirect's AI-generated Topic Pages" class="topic-link">GSR</a>, ST, BVP, RESP, EMG, and EOG selected from DEAP </span><a class="anchor anchor-primary" href="#bib0096" name="bbib0096" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0096"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a>, Verma and Tiwary <a class="anchor anchor-primary" href="#bib0347" name="bbib0347" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0347"><span class="anchor-text-container"><span class="anchor-text">[347]</span></span></a><span> extracted multi-resolution features based on <a href="/topics/physics-and-astronomy/wavelet-analysis" title="Learn more about DWT from ScienceDirect's AI-generated Topic Pages" class="topic-link">DWT</a> and used them to estimate the valence-arousal-dominance emotions. Hossain et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0387" name="bbib0387" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0387"><span class="anchor-text-container"><span class="anchor-text">[387]</span></span></a><span><span><span> extracted 9 statistical features, 9 power <a href="/topics/medicine-and-dentistry/spectroscopy" title="Learn more about spectral density from ScienceDirect's AI-generated Topic Pages" class="topic-link">spectral density</a> features, and 46 DBN features of EDA, Photo </span><a href="/topics/medicine-and-dentistry/plethysmography" title="Learn more about plethysmogram from ScienceDirect's AI-generated Topic Pages" class="topic-link">plethysmogram</a>, and zEMG. These features were fused to train a fine </span><a href="/topics/physics-and-astronomy/gaussian-distribution" title="Learn more about gaussian from ScienceDirect's AI-generated Topic Pages" class="topic-link">gaussian</a> SVM to recognize 5 basic emotions. Ma et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0388" name="bbib0388" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0388"><span class="anchor-text-container"><span class="anchor-text">[388]</span></span></a><span> designed a multimodal residual LSTM network for learning the dependency of high-level temporal-feature EEG, EOG, and EMG to predict emotions. It achieves the <a href="/topics/computer-science/classification-accuracy" title="Learn more about classification accuracies from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification accuracies</a> of 92.87% and 92.30% for arousal and valence, respectively.</span></div><div class="u-margin-s-bottom" id="para0151"><strong>Decision-level fusion.</strong> Wei et&nbsp;al. <a class="anchor anchor-primary" href="#bib0389" name="bbib0389" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0389"><span class="anchor-text-container"><span class="anchor-text">[389]</span></span></a><span> selected EEG, ECG, RESP and <a href="/topics/biochemistry-genetics-and-molecular-biology/electrodermal-response" title="Learn more about GSR from ScienceDirect's AI-generated Topic Pages" class="topic-link">GSR</a> from MAHNOB-HCI </span><a class="anchor anchor-primary" href="#bib0107" name="bbib0107" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0107"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a>, and designed a linear fusing weight matrix to fuse the outputs from multiple SVM classifiers to predict 5 basic emotions. Its highest average accuracy of recognition was 84.6%. To explore the emotional physiological-based temporal features, Li et&nbsp;al. <a class="anchor anchor-primary" href="#bib0390" name="bbib0390" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0390"><span class="anchor-text-container"><span class="anchor-text">[390]</span></span></a><span> transformed EEG, ECG and <a href="/topics/computer-science/galvanic-skin-response" title="Learn more about GSR from ScienceDirect's AI-generated Topic Pages" class="topic-link">GSR</a> selected from AMIGOS </span><a class="anchor anchor-primary" href="#bib0099" name="bbib0099" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0099"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a><span><span> into <a href="/topics/earth-and-planetary-sciences/spectrogram" title="Learn more about spectrogram from ScienceDirect's AI-generated Topic Pages" class="topic-link">spectrogram</a> images to represent their time-frequency information. The attention-based Bi-LSTM-RNNs was designed to automatically learn the best temporal features, which were further fed into a </span><a href="/topics/computer-science/deep-neural-network" title="Learn more about DNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">DNN</a> to predict the probability of unimodal emotion. The final emotion state was computed based on either an equal weights scheme or a variable weights scheme. As personality-specific human beings often show different physiological reactions after being stimulated by emotional elements, Yang and Lee </span><a class="anchor anchor-primary" href="#bib0363" name="bbib0363" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0363"><span class="anchor-text-container"><span class="anchor-text">[363]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a><span> (d)) proposed an attribute-invariance loss embedded variational <a href="/topics/computer-science/autoencoder" title="Learn more about autoencoder from ScienceDirect's AI-generated Topic Pages" class="topic-link">autoencoder</a><span> to learn personality-invariant representations. With EEG, ECG and EDA selected from AMIGOS, different features were extracted and then fed into different SVM classifiers to predict unimodal <a href="/topics/computer-science/classification-result" title="Learn more about classification results from ScienceDirect's AI-generated Topic Pages" class="topic-link">classification results</a>. Dar et&nbsp;al. </span></span><a class="anchor anchor-primary" href="#bib0391" name="bbib0391" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0391"><span class="anchor-text-container"><span class="anchor-text">[391]</span></span></a><span><span> designed a 2D-CNN for EEG and combined LSTM and 1D-CNN for ECG and <a href="/topics/medicine-and-dentistry/electrodermal-response" title="Learn more about GSR from ScienceDirect's AI-generated Topic Pages" class="topic-link">GSR</a>. By using majority voting based on decisions made by </span><a href="/topics/computer-science/multiple-classifier" title="Learn more about multiple classifiers from ScienceDirect's AI-generated Topic Pages" class="topic-link">multiple classifiers</a>, the framework achieved the overall highest accuracy of 99.0% and 90.8% for AMIGOS </span><a class="anchor anchor-primary" href="#bib0099" name="bbib0099" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0099"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a> and DREAMER <a class="anchor anchor-primary" href="#bib0321" name="bbib0321" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0321"><span class="anchor-text-container"><span class="anchor-text">[321]</span></span></a>, respectively.</div><div class="u-margin-s-bottom" id="para0152"><strong>Model-level fusion.</strong> Yin et&nbsp;al. <a class="anchor anchor-primary" href="#bib0392" name="bbib0392" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0392"><span class="anchor-text-container"><span class="anchor-text">[392]</span></span></a><span> first pre-processed and extracted 425 salient physiological features of 7 signals selected from DEAP. Separate deep hidden neurons in stacked-AEs were then investigated to extract higher-level abstractions of these salient features. An ensemble of deep classifiers with an adjacent graph-based hierarchical feature fusion network was designed for recognizing emotions based on a <a href="/topics/computer-science/bayesian-model" title="Learn more about Bayesian model from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian model</a>.</span></div></section><section id="sec0044"><h3 id="cesectitle0047" class="u-h4 u-margin-m-top u-margin-xs-bottom">6.3. Physical-physiological modality fusion for affective analysis</h3><div class="u-margin-s-bottom"><div id="para0153">Since the change of human emotions is a complicated psycho-physiological activity, the research on affective analysis is related to many cues (e.g., behavioral, physical, and psychological signals) [<a class="anchor anchor-primary" href="#bib0393" name="bbib0393" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0393"><span class="anchor-text-container"><span class="anchor-text">393</span></span></a>,<a class="anchor anchor-primary" href="#bib0394" name="bbib0394" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0394"><span class="anchor-text-container"><span class="anchor-text">394</span></span></a>]. Researchers have focused on the physical-physiological modality fusion for affective analysis via mining the strong external expression of physical data and undisguisable internal changes of physiological signals <a class="anchor anchor-primary" href="#bib0395" name="bbib0395" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0395"><span class="anchor-text-container"><span class="anchor-text">[395]</span></span></a>, <a class="anchor anchor-primary" href="#bib0396" name="bbib0396" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0396"><span class="anchor-text-container"><span class="anchor-text">[396]</span></span></a>, <a class="anchor anchor-primary" href="#bib0397" name="bbib0397" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0397"><span class="anchor-text-container"><span class="anchor-text">[397]</span></span></a>. <a class="anchor anchor-primary" href="#tbl0013" name="btbl0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tbl0013"><span class="anchor-text-container"><span class="anchor-text">Table&nbsp;13</span></span></a> provides an overview of representative methods for physical-physiological affective analysis, as detailed next.</div><div class="tables rowsep-0 colsep-0 frame-topbot" id="tbl0013"><span class="captions text-s"><span id="cap0019"><p id="spara021"><span class="label">Table 13</span>. Overview of the representative methods for physical-physiological affective analysis.</p></span></span><div class="groups"><table><thead><tr class="rowsep-1"><th scope="col" class="align-left valign-top" id="en1425">Publication</th><th scope="col" class="align-left valign-top" id="en1426">Year</th><th scope="col" class="align-left valign-top" id="en1427">Signal</th><th scope="col" class="align-left valign-top" id="en1428">Architecture</th><th scope="col" class="align-left valign-top" id="en1429">Fusion Strategy</th><th scope="col" class="align-left valign-top" id="en1430">Database</th><th scope="col" class="align-left valign-top" id="en1431">Performance (%)</th></tr></thead><tbody><tr><td class="align-left valign-top" id="en1432"><a class="anchor anchor-primary" href="#bib0397" name="bbib0397" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0397"><span class="anchor-text-container"><span class="anchor-text">[397]</span></span></a></td><td class="align-left valign-top" id="en1433">2020</td><td class="align-left valign-top" id="en1434">EDA, Music</td><td class="align-left valign-top" id="en1435">RTCAN-1D</td><td class="align-left valign-top" id="en1436">Feature-level</td><td class="align-left valign-top" id="en1437">PMEmo</td><td class="align-left valign-top" id="en1438">A/V: 82.51/ 77.30</td></tr><tr><td class="align-left valign-top" id="en1439"><a class="anchor anchor-primary" href="#bib0400" name="bbib0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0400"><span class="anchor-text-container"><span class="anchor-text">[400]</span></span></a></td><td class="align-left valign-top" id="en1440">2020</td><td class="align-left valign-top" id="en1441">EEG, Video</td><td class="align-left valign-top" id="en1442">LSTM, Self-attention</td><td class="align-left valign-top" id="en1443">Model-level</td><td class="align-left valign-top" id="en1444">DEAP</td><td class="align-left valign-top" id="en1445"><a class="anchor anchor-primary" href="#tb13fn2" name="btb13fn2" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb13fn2"><span class="anchor-text-container"><span class="anchor-text"><sup>2</sup></span></span></a>AAVA: 85.90</td></tr><tr><td class="align-left valign-top" id="en1446"><a class="anchor anchor-primary" href="#bib0395" name="bbib0395" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0395"><span class="anchor-text-container"><span class="anchor-text">[395]</span></span></a></td><td class="align-left valign-top" id="en1447">2015</td><td class="align-left valign-top" id="en1448">EEG, Eye Movements</td><td class="align-left valign-top" id="en1449">Feature extraction Fuzzy integral fusion</td><td class="align-left valign-top" id="en1450">Feature-level Decision-level</td><td class="align-left valign-top" id="en1451">SEED</td><td class="align-left valign-top" id="en1452">3 classes: 83.70<br><a class="anchor anchor-primary" href="#tb13fn1" name="btb13fn1" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb13fn1"><span class="anchor-text-container"><span class="anchor-text"><sup>1</sup></span></span></a>3 classes: 87.59</td></tr><tr><td class="align-left valign-top" id="en1453"><a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></td><td class="align-left valign-top" id="en1454">2020</td><td class="align-left valign-top" id="en1455">ECG, SCL, tEMG, Video</td><td class="align-left valign-top" id="en1456">Inception-ResNet-v2, BDBN, SVM</td><td class="align-left valign-top" id="en1457">Model-level</td><td class="align-left valign-top" id="en1458">BioVid Emo DB</td><td class="align-left valign-top" id="en1459">5 classes: 80.89<br><a class="anchor anchor-primary" href="#tb13fn3" name="btb13fn3" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="tb13fn3"><span class="anchor-text-container"><span class="anchor-text"><sup>3</sup></span></span></a>AAVAL: 84.29</td></tr></tbody></table></div><dl class="footnotes"><dt id="tb13fn1">1</dt><dd><div class="u-margin-s-bottom" id="notep0046"><span class="small-caps">The fuzzy integral fusion strategy;</span></div></dd><dt id="tb13fn2">2</dt><dd><div class="u-margin-s-bottom" id="notep0047"><span class="small-caps">AAVA&nbsp;=&nbsp;Average accuracy of Valence-Arousal;</span></div></dd><dt id="tb13fn3">3</dt><dd><div class="u-margin-s-bottom" id="notep0048"><span class="small-caps">AAVAL&nbsp;=&nbsp;Average accuracy of Valence-Arousal-Liking.</span></div></dd></dl></div></div><div class="u-margin-s-bottom" id="para0154"><strong>Feature-level fusion.</strong><span> To fully leverage the advantages of the complementary property between EEG and <a href="/topics/biochemistry-genetics-and-molecular-biology/eye-movement" title="Learn more about eye movement from ScienceDirect's AI-generated Topic Pages" class="topic-link">eye movement</a>, Liu et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0398" name="bbib0398" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0398"><span class="anchor-text-container"><span class="anchor-text">[398]</span></span></a><span> proposed a bimodal deep AE based on an <a href="/topics/computer-science/boltzmann-machine" title="Learn more about RBM from ScienceDirect's AI-generated Topic Pages" class="topic-link">RBM</a> to recognize emotions on SEED and DEAP. Soleymani et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0399" name="bbib0399" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0399"><span class="anchor-text-container"><span class="anchor-text">[399]</span></span></a><span> designed a framework of video-EEG based emotion detection using an LSTM-RNN and continuous <a href="/topics/computer-science/conditional-random-field" title="Learn more about conditional random fields from ScienceDirect's AI-generated Topic Pages" class="topic-link">conditional random fields</a>. Wu et&nbsp;al. </span><a class="anchor anchor-primary" href="#bib0400" name="bbib0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0400"><span class="anchor-text-container"><span class="anchor-text">[400]</span></span></a> proposed a hierarchical LSTM with a self-attention mechanism to fuse the facial features and EEG features to calculate the final emotion. Yin et&nbsp;al. <a class="anchor anchor-primary" href="#bib0397" name="bbib0397" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0397"><span class="anchor-text-container"><span class="anchor-text">[397]</span></span></a> proposed an efficient end-to-end framework of EDA-music fused emotion recognition, denominating it as a 1-D residual temporal and channel attention network (RTCAN-1D). Specially, the RTCAN consists of shallow feature extraction, residual feature extraction, the attention module stacked by a signal channel attention module, and a residual non-local temporal attention module. It achieved outstanding performances in AMIGOS, DEAP and PMEmo <a class="anchor anchor-primary" href="#bib0401" name="bbib0401" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0401"><span class="anchor-text-container"><span class="anchor-text">[401]</span></span></a>.</div><div class="u-margin-s-bottom" id="para0155"><strong>Feature-level fusion versus decision-level fusion.</strong> Huang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0034" name="bbib0034" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0034"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a> proposed video-EEG based multimodal affective analysis by fusing external facial expression of spatiotemporal local monogenic binary pattern and discriminative spectral power of internal EEG on feature-level and decision-level aspects. According to their experiments, the multimodal affective analysis achieves a better performance than the unimodal emotion recognition; and when it comes to fusion strategies, decision-level fusion outperforms feature-level fusion in the recognition of valence and arousal on MAHNOB-HCI.</div><div class="u-margin-s-bottom" id="para0156"><strong>Model-level fusion.</strong> Wang et&nbsp;al. <a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a> (<a class="anchor anchor-primary" href="#fig0006" name="bfig0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="fig0006"><span class="anchor-text-container"><span class="anchor-text">Fig.&nbsp;6</span></span></a> (e)) designed a multimodal deep belief network for fusing and optimizing multiple psycho-physiological features, a bimodal DBN (BDBN) for representing discriminative video-based features, and another BDBN to extract high multimodal features of both video and psycho-physiological modalities. The SVM was employed for emotion recognition after the features of all modalities were integrated into a unified dimension.</div></section></section><section id="sec0045"><h2 id="cesectitle0048" class="u-h4 u-margin-l-top u-margin-xs-bottom">7. Discussions</h2><div class="u-margin-s-bottom" id="para0157">In this review, we have involved emotion models and databases commonly used for affective computing, as well as unimodal affect recognition and multimodal affective analysis. In this section, we mainly discuss the following aspects:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1)</span><span><div class="u-margin-s-bottom" id="para0158">Effects of different signals (textual, audio, visual, or physiological) on unimodal affect recognition [<a class="anchor anchor-primary" href="#bib0152" name="bbib0152" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0152"><span class="anchor-text-container"><span class="anchor-text">152</span></span></a>,<a class="anchor anchor-primary" href="#bib0194" name="bbib0194" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0194"><span class="anchor-text-container"><span class="anchor-text">194</span></span></a>,<a class="anchor anchor-primary" href="#bib0244" name="bbib0244" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0244"><span class="anchor-text-container"><span class="anchor-text">244</span></span></a>,<a class="anchor anchor-primary" href="#bib0289" name="bbib0289" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0289"><span class="anchor-text-container"><span class="anchor-text">289</span></span></a>,<a class="anchor anchor-primary" href="#bib0327" name="bbib0327" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0327"><span class="anchor-text-container"><span class="anchor-text">327</span></span></a>,<a class="anchor anchor-primary" href="#bib0331" name="bbib0331" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0331"><span class="anchor-text-container"><span class="anchor-text">331</span></span></a>];</div></span></li><li class="react-xocs-list-item"><span class="list-label">2)</span><span><div class="u-margin-s-bottom" id="para0159">Effects of modality combinations and fusion strategies on multimodal affective analysis [<a class="anchor anchor-primary" href="#bib0037" name="bbib0037" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0037"><span class="anchor-text-container"><span class="anchor-text">37</span></span></a>,<a class="anchor anchor-primary" href="#bib0371" name="bbib0371" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0371"><span class="anchor-text-container"><span class="anchor-text">371</span></span></a>,<a class="anchor anchor-primary" href="#bib0375" name="bbib0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0375"><span class="anchor-text-container"><span class="anchor-text">375</span></span></a>,<a class="anchor anchor-primary" href="#bib0383" name="bbib0383" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0383"><span class="anchor-text-container"><span class="anchor-text">383</span></span></a>,<a class="anchor anchor-primary" href="#bib0388" name="bbib0388" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0388"><span class="anchor-text-container"><span class="anchor-text">388</span></span></a>,<a class="anchor anchor-primary" href="#bib0400" name="bbib0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0400"><span class="anchor-text-container"><span class="anchor-text">400</span></span></a>];</div></span></li><li class="react-xocs-list-item"><span class="list-label">3)</span><span><div class="u-margin-s-bottom" id="para0160">Effects of ML-based techniques [<a class="anchor anchor-primary" href="#bib0127" name="bbib0127" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0127"><span class="anchor-text-container"><span class="anchor-text">127</span></span></a>,<a class="anchor anchor-primary" href="#bib0184" name="bbib0184" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0184"><span class="anchor-text-container"><span class="anchor-text">184</span></span></a>,<a class="anchor anchor-primary" href="#bib0249" name="bbib0249" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0249"><span class="anchor-text-container"><span class="anchor-text">249</span></span></a>,<a class="anchor anchor-primary" href="#bib0305" name="bbib0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0305"><span class="anchor-text-container"><span class="anchor-text">305</span></span></a>,<a class="anchor anchor-primary" href="#bib0326" name="bbib0326" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0326"><span class="anchor-text-container"><span class="anchor-text">326</span></span></a>] or DL-based methods [<a class="anchor anchor-primary" href="#bib0146" name="bbib0146" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0146"><span class="anchor-text-container"><span class="anchor-text">146</span></span></a>,<a class="anchor anchor-primary" href="#bib0203" name="bbib0203" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0203"><span class="anchor-text-container"><span class="anchor-text">203</span></span></a>,<a class="anchor anchor-primary" href="#bib0273" name="bbib0273" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0273"><span class="anchor-text-container"><span class="anchor-text">273</span></span></a>,<a class="anchor anchor-primary" href="#bib0283" name="bbib0283" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0283"><span class="anchor-text-container"><span class="anchor-text">283</span></span></a>,<a class="anchor anchor-primary" href="#bib0316" name="bbib0316" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0316"><span class="anchor-text-container"><span class="anchor-text">316</span></span></a>,<a class="anchor anchor-primary" href="#bib0335" name="bbib0335" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0335"><span class="anchor-text-container"><span class="anchor-text">335</span></span></a>] on affective computing;</div></span></li><li class="react-xocs-list-item"><span class="list-label">4)</span><span><div class="u-margin-s-bottom" id="para0161">Effects of some potential factors (e.g., released databases and performance metrics) on affective computing;</div></span></li><li class="react-xocs-list-item"><span class="list-label">5)</span><span><div class="u-margin-s-bottom" id="para0162">Applications of affective computing in real-life scenarios.</div></span></li></ul></div><section id="sec0046"><h3 id="cesectitle0049" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.1. Effects of different signals on unimodal affect recognition</h3><div class="u-margin-s-bottom" id="para0163">According to unimodal affect recognition based on the text [<a class="anchor anchor-primary" href="#bib0126" name="bbib0126" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0126"><span class="anchor-text-container"><span class="anchor-text">126</span></span></a>,<a class="anchor anchor-primary" href="#bib0402" name="bbib0402" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0402"><span class="anchor-text-container"><span class="anchor-text">402</span></span></a>], audio [<a class="anchor anchor-primary" href="#bib0178" name="bbib0178" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0178"><span class="anchor-text-container"><span class="anchor-text">178</span></span></a>,<a class="anchor anchor-primary" href="#bib0194" name="bbib0194" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0194"><span class="anchor-text-container"><span class="anchor-text">194</span></span></a>,<a class="anchor anchor-primary" href="#bib0199" name="bbib0199" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0199"><span class="anchor-text-container"><span class="anchor-text">199</span></span></a>], visual [<a class="anchor anchor-primary" href="#bib0253" name="bbib0253" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0253"><span class="anchor-text-container"><span class="anchor-text">253</span></span></a>,<a class="anchor anchor-primary" href="#bib0281" name="bbib0281" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0281"><span class="anchor-text-container"><span class="anchor-text">281</span></span></a>,<a class="anchor anchor-primary" href="#bib0403" name="bbib0403" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0403"><span class="anchor-text-container"><span class="anchor-text">403</span></span></a>], EEG <a class="anchor anchor-primary" href="#bib0404" name="bbib0404" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0404"><span class="anchor-text-container"><span class="anchor-text">[404]</span></span></a>, or ECG <a class="anchor anchor-primary" href="#bib0336" name="bbib0336" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0336"><span class="anchor-text-container"><span class="anchor-text">[336]</span></span></a><span>, we can find that the most widely used modality is the visual signal, mainly consisting of facial expressions and body gestures. The number of visual-based emotion recognition systems is comparable to the sum of that of systems based on other modalities since the visual signals are easier to capture than other signals and emotional information in visual signals is more helpful than other signals in recognizing the emotion state of human beings. Visual-based emotion recognition is more effective than audio-based emotion recognition because <a href="/topics/physics-and-astronomy/audio-signal" title="Learn more about audio signals from ScienceDirect's AI-generated Topic Pages" class="topic-link">audio signals</a> are susceptible to noise </span><a class="anchor anchor-primary" href="#bib0405" name="bbib0405" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0405"><span class="anchor-text-container"><span class="anchor-text">[405]</span></span></a>. However, a study <a class="anchor anchor-primary" href="#bib0023" name="bbib0023" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0023"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a><span> reveals that textual-based affective analysis achieves the highest accuracy in emotion recognition and sentiment analysis. Although the physiological signals collected by <a href="/topics/physics-and-astronomy/wearable-sensors" title="Learn more about wearable sensors from ScienceDirect's AI-generated Topic Pages" class="topic-link">wearable sensors</a> are more difficult to obtain than physical signals, numerous EEG-based </span><a class="anchor anchor-primary" href="#bib0334" name="bbib0334" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0334"><span class="anchor-text-container"><span class="anchor-text">[334]</span></span></a> or ECG-based <a class="anchor anchor-primary" href="#bib0336" name="bbib0336" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0336"><span class="anchor-text-container"><span class="anchor-text">[336]</span></span></a> emotion recognition methods have been investigated and proposed due to their objective and reliable outcomes.</div></section><section id="sec0047"><h3 id="cesectitle0050" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.2. Effects of modality combinations and fusion strategies on multimodal affective analysis</h3><div class="u-margin-s-bottom" id="para0164">The combination of different modalities and the fusion strategy are two key aspects of the multimodal affective analysis. Multimodal combinations are divided into multi-physical modalities, multi-physiological modalities, and physical-physiological modalities. The fusion strategies consist of feature-level fusion [<a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">361</span></span></a>,<a class="anchor anchor-primary" href="#bib0387" name="bbib0387" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0387"><span class="anchor-text-container"><span class="anchor-text">387</span></span></a>], decision-level fusion <a class="anchor anchor-primary" href="#bib0375" name="bbib0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0375"><span class="anchor-text-container"><span class="anchor-text">[375]</span></span></a>, hybrid-level fusion <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a>, and model-level fusion.</div><div class="u-margin-s-bottom" id="para0165">In the multi-physical modalities <a class="anchor anchor-primary" href="#bib0406" name="bbib0406" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0406"><span class="anchor-text-container"><span class="anchor-text">[406]</span></span></a>, there are three kinds of combinations of different modalities, consisting of visual-audio, text-audio and visual-audio-text. Integrating visual and audio information can enhance performance over unimodal affect recognition <a class="anchor anchor-primary" href="#bib0407" name="bbib0407" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0407"><span class="anchor-text-container"><span class="anchor-text">[407]</span></span></a>. There are similar results in other combinations of multi-physical modalities <a class="anchor anchor-primary" href="#bib0375" name="bbib0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0375"><span class="anchor-text-container"><span class="anchor-text">[375]</span></span></a>, in which the text modality plays the most vital role in multimodal sentiment analysis [<a class="anchor anchor-primary" href="#bib0382" name="bbib0382" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0382"><span class="anchor-text-container"><span class="anchor-text">382</span></span></a>,<a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">362</span></span></a>]. In studies of multi-physiological modality fusion for affective analysis, in addition to EEG and ECG, other types of physiological signals (e.g., ECG, EOG, BVP, GSR, and EMG) are jointly combined to interpret emotional states [<a class="anchor anchor-primary" href="#bib0388" name="bbib0388" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0388"><span class="anchor-text-container"><span class="anchor-text">388</span></span></a>,<a class="anchor anchor-primary" href="#bib0390" name="bbib0390" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0390"><span class="anchor-text-container"><span class="anchor-text">390</span></span></a><span>]. The <a href="/topics/computer-science/visual-modality" title="Learn more about visual modality from ScienceDirect's AI-generated Topic Pages" class="topic-link">visual modality</a> (facial expression, voice, gesture, posture, etc.) may also be integrated with multimodal physiological signals for visual-physiological affective analysis [</span><a class="anchor anchor-primary" href="#bib0397" name="bbib0397" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0397"><span class="anchor-text-container"><span class="anchor-text">397</span></span></a>,<a class="anchor anchor-primary" href="#bib0400" name="bbib0400" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0400"><span class="anchor-text-container"><span class="anchor-text">400</span></span></a>].</div><div class="u-margin-s-bottom" id="para0166">Two basic fusion strategies for multimodal affective analysis are feature-level fusion [<a class="anchor anchor-primary" href="#bib0361" name="bbib0361" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0361"><span class="anchor-text-container"><span class="anchor-text">361</span></span></a>,<a class="anchor anchor-primary" href="#bib0387" name="bbib0387" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0387"><span class="anchor-text-container"><span class="anchor-text">387</span></span></a>] and decision-level fusion <a class="anchor anchor-primary" href="#bib0375" name="bbib0375" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0375"><span class="anchor-text-container"><span class="anchor-text">[375]</span></span></a>. The concatenation <a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a> or factorized bilinear pooling <a class="anchor anchor-primary" href="#bib0365" name="bbib0365" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0365"><span class="anchor-text-container"><span class="anchor-text">[365]</span></span></a> of feature vectors is commonly used for feature-level fusion. The majority/average voting is often used for decision-level fusion. Linear weighted computing <a class="anchor anchor-primary" href="#bib0374" name="bbib0374" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0374"><span class="anchor-text-container"><span class="anchor-text">[374]</span></span></a> can be utilized for both feature-level and decision-level fusion, by employing sum or product operators to fuse features or classification decisions of different modalities. According to the multimodal affective analysis, we find that feature-level fusion <a class="anchor anchor-primary" href="#bib0388" name="bbib0388" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0388"><span class="anchor-text-container"><span class="anchor-text">[388]</span></span></a> is strikingly more common than decision-level fusion. The performance of an affect classifier based on feature-level fusion is significantly influenced by the time scales and metric levels of features coming from different modalities. On the other hand, in decision-level fusion, the input coming from each modality is modelled independently, and these results of unimodal affect recognition are combined in the end. Compared with feature-level fusion, decision-level fusion <a class="anchor anchor-primary" href="#bib0362" name="bbib0362" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0362"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a> is performed easier, but ignores the relevance among features of different modalities. Hybrid-level fusion <a class="anchor anchor-primary" href="#bib0105" name="bbib0105" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0105"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a> aims to make full use of the advantages of feature-based fusion and decision-based fusion strategies as well as overcome the disadvantages of either one. Unlike the above three fusion strategies, model-level fusion uses HMM <a class="anchor anchor-primary" href="#bib0033" name="bbib0033" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0033"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a><span> or <a href="/topics/computer-science/bayesian-networks" title="Learn more about Bayesian networks from ScienceDirect's AI-generated Topic Pages" class="topic-link">Bayesian networks</a> </span><a class="anchor anchor-primary" href="#bib0392" name="bbib0392" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0392"><span class="anchor-text-container"><span class="anchor-text">[392]</span></span></a> to establish the correlation between features of different modalities and one relaxed fusion mode. The selection and establishment of HMM or Bayesian have a fatal effect on the results of the model-level fusion, which is often designed for one specific task.</div></section><section id="sec0048"><h3 id="cesectitle0051" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.3. Effects of ML-based and DL-based models on affective computing</h3><div class="u-margin-s-bottom" id="para0167">The majority of the early works on affective computing have employed ML-based techniques [<a class="anchor anchor-primary" href="#bib0019" name="bbib0019" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0019"><span class="anchor-text-container"><span class="anchor-text">19</span></span></a>,<a class="anchor anchor-primary" href="#bib0010" name="bbib0010" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0010"><span class="anchor-text-container"><span class="anchor-text">10</span></span></a>,<a class="anchor anchor-primary" href="#bib0018" name="bbib0018" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0018"><span class="anchor-text-container"><span class="anchor-text">18</span></span></a>]. The ML-based pipeline [<a class="anchor anchor-primary" href="#bib0124" name="bbib0124" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0124"><span class="anchor-text-container"><span class="anchor-text">124</span></span></a>,<a class="anchor anchor-primary" href="#bib0181" name="bbib0181" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0181"><span class="anchor-text-container"><span class="anchor-text">181</span></span></a>,<a class="anchor anchor-primary" href="#bib0249" name="bbib0249" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0249"><span class="anchor-text-container"><span class="anchor-text">249</span></span></a>,<a class="anchor anchor-primary" href="#bib0309" name="bbib0309" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0309"><span class="anchor-text-container"><span class="anchor-text">309</span></span></a>,<a class="anchor anchor-primary" href="#bib0326" name="bbib0326" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0326"><span class="anchor-text-container"><span class="anchor-text">326</span></span></a><span>] consists of pre-processing of raw signals, hand-crafted feature extractor (feature selection if possible), and well-designed classifiers. Although various types of hand-crafted features have been designed for different modalities, ML-based techniques for affective analysis are hard to be reused across similar problems on account of their task-specific and domain-specific <a href="/topics/computer-science/feature-descriptor" title="Learn more about feature descriptors from ScienceDirect's AI-generated Topic Pages" class="topic-link">feature descriptors</a>. The commonly used ML-based classifiers are SVM [</span><a class="anchor anchor-primary" href="#bib0182" name="bbib0182" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0182"><span class="anchor-text-container"><span class="anchor-text">182</span></span></a>,<a class="anchor anchor-primary" href="#bib0253" name="bbib0253" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0253"><span class="anchor-text-container"><span class="anchor-text">253</span></span></a>], HMM <a class="anchor anchor-primary" href="#bib0179" name="bbib0179" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0179"><span class="anchor-text-container"><span class="anchor-text">[179]</span></span></a>, GMM <a class="anchor anchor-primary" href="#bib0180" name="bbib0180" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0180"><span class="anchor-text-container"><span class="anchor-text">[180]</span></span></a>, RF <a class="anchor anchor-primary" href="#bib0305" name="bbib0305" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0305"><span class="anchor-text-container"><span class="anchor-text">[305]</span></span></a><span>, <a href="/topics/computer-science/k-nearest-neighbors-algorithm" title="Learn more about KNN from ScienceDirect's AI-generated Topic Pages" class="topic-link">KNN</a> </span><a class="anchor anchor-primary" href="#bib0337" name="bbib0337" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0337"><span class="anchor-text-container"><span class="anchor-text">[337]</span></span></a><span> and <a href="/topics/neuroscience/artificial-neural-network" title="Learn more about ANN from ScienceDirect's AI-generated Topic Pages" class="topic-link">ANN</a> </span><a class="anchor anchor-primary" href="#bib0374" name="bbib0374" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0374"><span class="anchor-text-container"><span class="anchor-text">[374]</span></span></a>, of which the SVM classifier is the most effective one, and is indeed used in most tasks of ML-based affective computing. These ML-based classifiers are also used for final classification when the DL-based model is only designed for unimodal feature extraction <a class="anchor anchor-primary" href="#bib0290" name="bbib0290" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0290"><span class="anchor-text-container"><span class="anchor-text">[290]</span></span></a> or multimodal feature analysis [<a class="anchor anchor-primary" href="#bib0035" name="bbib0035" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0035"><span class="anchor-text-container"><span class="anchor-text">35</span></span></a>,<a class="anchor anchor-primary" href="#bib0387" name="bbib0387" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0387"><span class="anchor-text-container"><span class="anchor-text">387</span></span></a>].</div><div class="u-margin-s-bottom" id="para0168"><span>Nowadays, DL-based models have become <a href="/topics/computer-science/hotspot" title="Learn more about hot spots from ScienceDirect's AI-generated Topic Pages" class="topic-link">hot spots</a> and outperformed ML-based models in most areas of affective computing [</span><a class="anchor anchor-primary" href="#bib0013" name="bbib0013" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0013"><span class="anchor-text-container"><span class="anchor-text">13</span></span></a>,<a class="anchor anchor-primary" href="#bib0135" name="bbib0135" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0135"><span class="anchor-text-container"><span class="anchor-text">135</span></span></a>,<a class="anchor anchor-primary" href="#bib0043" name="bbib0043" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0043"><span class="anchor-text-container"><span class="anchor-text">43</span></span></a>,<a class="anchor anchor-primary" href="#bib0017" name="bbib0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0017"><span class="anchor-text-container"><span class="anchor-text">17</span></span></a>,<a class="anchor anchor-primary" href="#bib0039" name="bbib0039" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0039"><span class="anchor-text-container"><span class="anchor-text">39</span></span></a><span>] due to their strong ability of feature representation learning. For static information (e.g., facial and spectrogram images), CNNs and their variants are designed to extract important and <a href="/topics/computer-science/discriminative-feature" title="Learn more about discriminative features from ScienceDirect's AI-generated Topic Pages" class="topic-link">discriminative features</a> [</span><a class="anchor anchor-primary" href="#bib0137" name="bbib0137" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0137"><span class="anchor-text-container"><span class="anchor-text">137</span></span></a>,<a class="anchor anchor-primary" href="#bib0194" name="bbib0194" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0194"><span class="anchor-text-container"><span class="anchor-text">194</span></span></a>,<a class="anchor anchor-primary" href="#bib0289" name="bbib0289" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0289"><span class="anchor-text-container"><span class="anchor-text">289</span></span></a>]. For sequence information (e.g., physiological signals and videos), RNNs and their variants are designed for capturing temporal dynamics [<a class="anchor anchor-primary" href="#bib0143" name="bbib0143" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0143"><span class="anchor-text-container"><span class="anchor-text">143</span></span></a>,<a class="anchor anchor-primary" href="#bib0201" name="bbib0201" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0201"><span class="anchor-text-container"><span class="anchor-text">201</span></span></a>,<a class="anchor anchor-primary" href="#bib0310" name="bbib0310" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0310"><span class="anchor-text-container"><span class="anchor-text">310</span></span></a>]. The CNN-LSTM models can perform the deep spatial-temporal feature extraction. Adversarial learning is widely used to improve the robustness of models by augmenting data [<a class="anchor anchor-primary" href="#bib0206" name="bbib0206" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0206"><span class="anchor-text-container"><span class="anchor-text">206</span></span></a>,<a class="anchor anchor-primary" href="#bib0291" name="bbib0291" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0291"><span class="anchor-text-container"><span class="anchor-text">291</span></span></a>] and cross-domain learning [<a class="anchor anchor-primary" href="#bib0156" name="bbib0156" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0156"><span class="anchor-text-container"><span class="anchor-text">156</span></span></a>,<a class="anchor anchor-primary" href="#bib0192" name="bbib0192" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0192"><span class="anchor-text-container"><span class="anchor-text">192</span></span></a>]. Besides, different attention mechanisms [<a class="anchor anchor-primary" href="#bib0142" name="bbib0142" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0142"><span class="anchor-text-container"><span class="anchor-text">142</span></span></a>,<a class="anchor anchor-primary" href="#bib0199" name="bbib0199" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0199"><span class="anchor-text-container"><span class="anchor-text">199</span></span></a>,<a class="anchor anchor-primary" href="#bib0203" name="bbib0203" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0203"><span class="anchor-text-container"><span class="anchor-text">203</span></span></a>,<a class="anchor anchor-primary" href="#bib0279" name="bbib0279" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0279"><span class="anchor-text-container"><span class="anchor-text">279</span></span></a>] and autoencoders [<a class="anchor anchor-primary" href="#bib0363" name="bbib0363" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0363"><span class="anchor-text-container"><span class="anchor-text">363</span></span></a>,<a class="anchor anchor-primary" href="#bib0289" name="bbib0289" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0289"><span class="anchor-text-container"><span class="anchor-text">289</span></span></a>] are integrated with DL-based techniques to improve the overall performance. It seems that DL-based methods have an advantage in automatically learning the most discriminative features. However, DL-based approaches have not yet had a huge impact on physiological emotion recognition, if compared with ML-based models <a class="anchor anchor-primary" href="#bib0020" name="bbib0020" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0020"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a>.</div></section><section id="sec0049"><h3 id="cesectitle0052" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.4. Effects of some potential factors on affective computing</h3><div class="u-margin-s-bottom" id="para0169">Throughout this review, we have consistently found that advances in affective computing are driven by various database benchmarks. For example, there are few video-physiological emotion recognition methods due to the limitations of physical-physiological emotion databases. In contrast, the rapid development of FER is inseparable from various baseline databases, which are publicly available and can be freely downloaded. Besides, some large-scale visual databases such as BU-4DFE and BP4D can be employed to pre-train the target model to recognize facial expressions <a class="anchor anchor-primary" href="#bib0260" name="bbib0260" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0260"><span class="anchor-text-container"><span class="anchor-text">[260]</span></span></a> or micro-expressions <a class="anchor anchor-primary" href="#bib0408" name="bbib0408" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0408"><span class="anchor-text-container"><span class="anchor-text">[408]</span></span></a>. However, there are significant discrepancies in size, quality, and collection conditions <a class="anchor anchor-primary" href="#bib0274" name="bbib0274" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0274"><span class="anchor-text-container"><span class="anchor-text">[274]</span></span></a> across different databases. For example, most body gesture emotion databases contain only several hundred samples with limited gesture categories. What is worse, samples are typically collected in a laboratory environment, which is often far away from real-world conditions. Furthermore, the size and quality of databases have a more obvious effect on DL-based emotion recognition than on ML-based emotion recognition. Many studies have concluded that the reduced size of the available databases is a key factor in the quest for high-performance affective analysis [<a class="anchor anchor-primary" href="#bib0409" name="bbib0409" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0409"><span class="anchor-text-container"><span class="anchor-text">409</span></span></a>,<a class="anchor anchor-primary" href="#bib0207" name="bbib0207" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0207"><span class="anchor-text-container"><span class="anchor-text">207</span></span></a>]. To tackle this problem, the pre-trained DL-based models [<a class="anchor anchor-primary" href="#bib0410" name="bbib0410" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0410"><span class="anchor-text-container"><span class="anchor-text">410</span></span></a>,<a class="anchor anchor-primary" href="#bib0411" name="bbib0411" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0411"><span class="anchor-text-container"><span class="anchor-text">411</span></span></a>] may be transferred into task-based models specialized for affective analysis.</div><div class="u-margin-s-bottom" id="para0170">Although the representation of the natural affective states has no consensus, most affective analyses are trained and evaluated based on two types of emotion models: discrete models and dimensional models. In building the affective databases, either discrete or dimensional labels are typically chosen alternatively to fit the raw signals. For example, emotional images or sequences are typically matched with a discrete affective state (basic emotions or polarity). Affective recognition can be divided into classification (emotions, dimensional or polarity) and continuous dimensional regression (Pleasure, Arousal, Dominance Expectation, or Intensity) [<a class="anchor anchor-primary" href="#bib0412" name="bbib0412" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0412"><span class="anchor-text-container"><span class="anchor-text">412</span></span></a>,<a class="anchor anchor-primary" href="#bib0413" name="bbib0413" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0413"><span class="anchor-text-container"><span class="anchor-text">413</span></span></a><span>]. The metrics of accuracy, precision and recall are generally adopted for categorical or componential emotion classification. When the databases are imbalanced, the F-Measure (or F1-Score) seems to be the best choice out of the existing <a href="/topics/computer-science/evaluation-metric" title="Learn more about evaluation metrics from ScienceDirect's AI-generated Topic Pages" class="topic-link">evaluation metrics</a> of the emotional classification, across 10-fold cross validation and LOSO. The weighted average recall/F1-score (WAR/WF1) and the unweighted average recall/F1-score (UAR/UF1) are best suited for the classification performance of visual, audio or multimodal affective analysis </span><a class="anchor anchor-primary" href="#bib0291" name="bbib0291" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0291"><span class="anchor-text-container"><span class="anchor-text">[291]</span></span></a>. On the other hand, MSE and RMSE are commonly used for the evaluation of continuous dimensional emotion prediction <a class="anchor anchor-primary" href="#bib0414" name="bbib0414" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0414"><span class="anchor-text-container"><span class="anchor-text">[414]</span></span></a><span>. In order to describe the degree of coincidence, by integrating <a href="/topics/earth-and-planetary-sciences/correlation-coefficient" title="Learn more about PCC from ScienceDirect's AI-generated Topic Pages" class="topic-link">PCC</a> and MSR, the coefficient concordance correlation coefficient is advised to assess the baseline performance assessment [</span><a class="anchor anchor-primary" href="#bib0415" name="bbib0415" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0415"><span class="anchor-text-container"><span class="anchor-text">415</span></span></a>,<a class="anchor anchor-primary" href="#bib0416" name="bbib0416" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0416"><span class="anchor-text-container"><span class="anchor-text">416</span></span></a>].</div></section><section id="sec0050"><h3 id="cesectitle0053" class="u-h4 u-margin-m-top u-margin-xs-bottom">7.5. Applications of affective computing in real-life scenarios</h3><div class="u-margin-s-bottom" id="para0171">In recent years, more and more research teams have shifted their focus to applications of affective computing in real-life scenarios [<a class="anchor anchor-primary" href="#bib0417" name="bbib0417" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0417"><span class="anchor-text-container"><span class="anchor-text">417</span></span></a>,<a class="anchor anchor-primary" href="#bib0418" name="bbib0418" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0418"><span class="anchor-text-container"><span class="anchor-text">418</span></span></a>]. In order to detect emotions and sentiments from the textual information, the SenticNet directed by Erik Cambria of NTU applied the research outputs of affective computing [<a class="anchor anchor-primary" href="#bib0106" name="bbib0106" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0106"><span class="anchor-text-container"><span class="anchor-text">106</span></span></a>,<a class="anchor anchor-primary" href="#bib0150" name="bbib0150" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0150"><span class="anchor-text-container"><span class="anchor-text">150</span></span></a>,<a class="anchor anchor-primary" href="#bib0419" name="bbib0419" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0419"><span class="anchor-text-container"><span class="anchor-text">419</span></span></a>] and sentiment analysis [<a class="anchor anchor-primary" href="#bib0022" name="bbib0022" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0022"><span class="anchor-text-container"><span class="anchor-text">22</span></span></a>,<a class="anchor anchor-primary" href="#bib0420" name="bbib0420" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0420"><span class="anchor-text-container"><span class="anchor-text">[420]</span></span></a>, <a class="anchor anchor-primary" href="#bib0421" name="bbib0421" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0421"><span class="anchor-text-container"><span class="anchor-text">[421]</span></span></a>, <a class="anchor anchor-primary" href="#bib0422" name="bbib0422" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0422"><span class="anchor-text-container"><span class="anchor-text">[422]</span></span></a>, <a class="anchor anchor-primary" href="#bib0423" name="bbib0423" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0423"><span class="anchor-text-container"><span class="anchor-text">[423]</span></span></a>, <a class="anchor anchor-primary" href="#bib0424" name="bbib0424" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0424"><span class="anchor-text-container"><span class="anchor-text">[424]</span></span></a>] into many aspects of daily life, including HCI <a class="anchor anchor-primary" href="#bib0425" name="bbib0425" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0425"><span class="anchor-text-container"><span class="anchor-text">[425]</span></span></a>, finance <a class="anchor anchor-primary" href="#bib0426" name="bbib0426" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0426"><span class="anchor-text-container"><span class="anchor-text">[426]</span></span></a> and social media monitoring and forecasting [<a class="anchor anchor-primary" href="#bib0132" name="bbib0132" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0132"><span class="anchor-text-container"><span class="anchor-text">132</span></span></a>,<a class="anchor anchor-primary" href="#bib0427" name="bbib0427" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0427"><span class="anchor-text-container"><span class="anchor-text">427</span></span></a><span>]. TSA is often used for <a href="/topics/computer-science/recommender-systems" title="Learn more about recommender systems from ScienceDirect's AI-generated Topic Pages" class="topic-link">recommender systems</a>, by integrating diverse feedback information </span><a class="anchor anchor-primary" href="#bib0428" name="bbib0428" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0428"><span class="anchor-text-container"><span class="anchor-text">[428]</span></span></a><span> or <a href="/topics/computer-science/microblogs" title="Learn more about microblog from ScienceDirect's AI-generated Topic Pages" class="topic-link">microblog</a> texts </span><a class="anchor anchor-primary" href="#bib0429" name="bbib0429" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0429"><span class="anchor-text-container"><span class="anchor-text">[429]</span></span></a>. The applications of visual emotion recognition include course teaching <a class="anchor anchor-primary" href="#bib0430" name="bbib0430" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0430"><span class="anchor-text-container"><span class="anchor-text">[430]</span></span></a>, smarter decision aid <a class="anchor anchor-primary" href="#bib0431" name="bbib0431" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0431"><span class="anchor-text-container"><span class="anchor-text">[431]</span></span></a>, HCI <a class="anchor anchor-primary" href="#bib0432" name="bbib0432" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0432"><span class="anchor-text-container"><span class="anchor-text">[432]</span></span></a>, dynamic quality adaption to the players in games <a class="anchor anchor-primary" href="#bib0433" name="bbib0433" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0433"><span class="anchor-text-container"><span class="anchor-text">[433]</span></span></a>, <a class="anchor anchor-primary" href="#bib0434" name="bbib0434" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0434"><span class="anchor-text-container"><span class="anchor-text">[434]</span></span></a>, <a class="anchor anchor-primary" href="#bib0435" name="bbib0435" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0435"><span class="anchor-text-container"><span class="anchor-text">[435]</span></span></a>, depression recognition <a class="anchor anchor-primary" href="#bib0436" name="bbib0436" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0436"><span class="anchor-text-container"><span class="anchor-text">[436]</span></span></a><span> and for helping medical <a href="/topics/medicine-and-dentistry/child-rehabilitation" title="Learn more about rehabilitation children from ScienceDirect's AI-generated Topic Pages" class="topic-link">rehabilitation children</a><span> affected by the <a href="/topics/medicine-and-dentistry/autism-spectrum" title="Learn more about autism spectrum from ScienceDirect's AI-generated Topic Pages" class="topic-link">autism spectrum</a> condition </span></span><a class="anchor anchor-primary" href="#bib0437" name="bbib0437" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0437"><span class="anchor-text-container"><span class="anchor-text">[437]</span></span></a>. In particular, audio and physiological signals are often used for detecting clinical depression and stress [<a class="anchor anchor-primary" href="#bib0100" name="bbib0100" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0100"><span class="anchor-text-container"><span class="anchor-text">100</span></span></a>,<a class="anchor anchor-primary" href="#bib0166" name="bbib0166" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0166"><span class="anchor-text-container"><span class="anchor-text">166</span></span></a>,<a class="anchor anchor-primary" href="#bib0438" name="bbib0438" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0438"><span class="anchor-text-container"><span class="anchor-text">438</span></span></a>] due to the reliability and stability of audio/speech emotion signals and the accessibility of physiological signals from wearable devices <a class="anchor anchor-primary" href="#bib0439" name="bbib0439" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0439"><span class="anchor-text-container"><span class="anchor-text">[439]</span></span></a>. As multimodal affective analysis can enhance the robustness and performance of unimodal affect recognition, more researches have begun to transform them into various real-life applications [<a class="anchor anchor-primary" href="#bib0440" name="bbib0440" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0440"><span class="anchor-text-container"><span class="anchor-text">440</span></span></a>,<a class="anchor anchor-primary" href="#bib0441" name="bbib0441" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="bib0441"><span class="anchor-text-container"><span class="anchor-text">441</span></span></a>], making it a promising research avenue.</div></section></section><section id="sec0051"><h2 id="cesectitle0054" class="u-h4 u-margin-l-top u-margin-xs-bottom">8. Conclusion and new developments</h2><div class="u-margin-s-bottom" id="para0172">This review has comprehensively surveyed more than 400 papers, including an overview of the recent reviews on affective computing in <a class="anchor anchor-primary" href="#sec0002" name="bsec0002" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0002"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;2</span></span></a>, and built the taxonomy of affective computing with representative examples in <a class="anchor anchor-primary" href="#sec0001" name="bsec0001" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0001"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;1</span></span></a>. In <a class="anchor anchor-primary" href="#sec0006" name="bsec0006" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0006"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;3</span></span></a><span>, we categorize current emotion models based on psychological theories into discrete models and dimensional models, which determine the category of the output of the affective analysis. These recognition results via either classification or regression are evaluated by a range of corresponding metrics. More importantly, the development of affective computing requires benchmark databases for training and <a href="/topics/earth-and-planetary-sciences/computational-modeling" title="Learn more about computational models from ScienceDirect's AI-generated Topic Pages" class="topic-link">computational models</a> for either DL-based or ML-based affective understanding. In </span><a class="anchor anchor-primary" href="#sec0009" name="bsec0009" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0009"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;4</span></span></a><span>, we survey five kinds of the commonly adopted baseline databases for affective computing, which are classified into textual, audio, visual, physiological, and multimodal databases. Most methods for affective <a href="/topics/medicine-and-dentistry/cost-benefit-analysis" title="Learn more about analysis benefit from ScienceDirect's AI-generated Topic Pages" class="topic-link">analysis benefit</a> from these released databases.</span></div><div class="u-margin-s-bottom" id="para0173">In <a class="anchor anchor-primary" href="#sec0017" name="bsec0017" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0017"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;5</span></span></a> and <a class="anchor anchor-primary" href="#sec0038" name="bsec0038" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0038"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;6</span></span></a>, we introduced recent advances of affective computing, which are mainly grouped into unimodal affect recognition and multimodal affective analysis, and further divide them into ML-based techniques and DL-based models. The unimodal affect recognition systems are further divided into textual sentiment analysis, speech emotion recognition, visual emotion recognition (FER and EBGR) and physiological emotion recognition (EEG-based and ECG-based). Traditional ML-based unimodal affect recognition mostly investigates hand-crafted feature extractors or pre-defined rules and interpretable classifiers. In contrast, DL-based unimodal affect recognition further improves the ability of the feature representation and classification by designing deeper network architectures or task-specific network modules and learning objectives. Generally, in addition to employing different strategies (feature-level, decision-level, model-level, or hybrid fusion strategies), the multimodal affective analysis is divided into multi-physical approaches (visual-audio, text-audio and visual-audio-text modalities), multi-physiological approaches, and physical-physiological approaches. The performance of multimodal affective analysis is mainly affected by both modality combination and fusion strategy.</div><div class="u-margin-s-bottom" id="para0174">In <a class="anchor anchor-primary" href="#sec0045" name="bsec0045" data-sd-ui-side-panel-opener="true" data-xocs-content-type="reference" data-xocs-content-id="sec0045"><span class="anchor-text-container"><span class="anchor-text">Section&nbsp;7</span></span></a>, we discuss some important issues related to affective computing including effects of textual, audio, visual, or physiological signals on unimodal affect recognition, effects of modality combinations and fusion strategies on multimodal affective analysis, the effects of ML-based and DL-based models on affective computing, effects of some potential factors on affective computing, and applications of affective computing in the real-life scenarios.</div><div class="u-margin-s-bottom" id="para0175">Although affective computing systems using either unimodal or multimodal data have made significant breakthroughs, there are only a few robust and effective algorithms to predict emotion and recognize sentiment under diverse and challenging scenes. Hence, we would like to conclude this review with many important recommendations for future research in affective computing:<ul class="list"><li class="react-xocs-list-item"><span class="list-label">1)</span><span><div class="u-margin-s-bottom" id="para0176">It will be instrumental to develop new and more extended baseline databases, particularly multimodal affect databases, consisting of different modalities (textual, audio, visual and physiological). Conditions should include both spontaneous and non-spontaneous scenarios, with the provision of annotating data in both discrete and dimensional emotion models.</div></span></li><li class="react-xocs-list-item"><span class="list-label">2)</span><span><div class="u-margin-s-bottom" id="para0177">There are some challenging tasks of affective analysis to be solved including FER under partial occlusion or fake emotion expression, physiological emotion recognition based on various complex signals, and a <a href="/topics/computer-science/baseline-model" title="Learn more about baseline model from ScienceDirect's AI-generated Topic Pages" class="topic-link">baseline model</a> specifically for both discrete emotion recognition and dimensional emotion prediction.</div></span></li><li class="react-xocs-list-item"><span class="list-label">3)</span><span><div class="u-margin-s-bottom" id="para0178">There is significant space for improving fusion strategies, particularly with rule-based or statistic-based knowledge, to implement a mutual fusion of different modalities that can consider the role and importance of each modality in affect recognition.</div></span></li><li class="react-xocs-list-item"><span class="list-label">4)</span><span><div class="u-margin-s-bottom" id="para0179">Zero/few-shot learning or <a href="/topics/computer-science/unsupervised-learning" title="Learn more about unsupervised learning from ScienceDirect's AI-generated Topic Pages" class="topic-link">unsupervised learning</a> methods (e.g., self-supervised learning) need to be further explored, particularly thanks to their potential to enhance the robustness and stability of affective analysis under limited or biased databases.</div></span></li><li class="react-xocs-list-item"><span class="list-label">5)</span><span><div class="u-margin-s-bottom" id="para0180">A prominent application of affective analysis is robotics. Advances presented in this review make it possible to conceive robots equipped with emotional intelligence, which can appropriately imitate and promptly respond to mankind affect and the surrounding environment.</div></span></li></ul></div></section><section id="sec0051a"><h2 id="cesectitle0054a" class="u-h4 u-margin-l-top u-margin-xs-bottom">CRediT authorship contribution statement</h2><div class="u-margin-s-bottom" id="para0180a"><strong>Yan Wang:</strong> Conceptualization, Investigation, Visualization, Writing – original draft, Writing – review &amp; editing. <strong>Wei Song:</strong> Writing – review &amp; editing, Supervision, Funding acquisition. <strong>Wei Tao:</strong> Writing – original draft, Writing – review &amp; editing. <span>Antonio <a href="/topics/medicine-and-dentistry/heart-valve-bioprosthesis" title="Learn more about Liotta from ScienceDirect's AI-generated Topic Pages" class="topic-link">Liotta</a>:</span> Writing – review &amp; editing. <strong>Dawei Yang:</strong> Writing – original draft, Writing – review &amp; editing. <strong>Xinlei Li:</strong> Writing – review &amp; editing. <strong>Shuyong Gao:</strong> Writing – review &amp; editing. <strong>Yixuan Sun:</strong> Writing – review &amp; editing. <strong>Weifeng Ge:</strong> Writing – review &amp; editing. <strong>Wei Zhang:</strong> Writing – review &amp; editing. <strong>Wenqiang Zhang:</strong> Supervision, Writing – review &amp; editing, Funding acquisition.</div></section></div><section id="coi0001"><h2 id="cesectitle0056" class="u-h4 u-margin-l-top u-margin-xs-bottom">Declaration of Competing Interest</h2><div class="u-margin-s-bottom" id="para0182">We declare that we have no financial and personal relationships with other people or organizations that&nbsp;can inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled.</div></section><section id="ack0001a"><h2 id="sectt0030e" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgement</h2><div class="u-margin-s-bottom" id="para0125r">This work was supported by National Key R&amp;D Program of China (2020AAA0108301, 2019YFC1711800), <span id="gs00001">National Natural Science Foundation of China</span> (No. <a class="anchor anchor-primary" href="#gs00001"><span class="anchor-text-container"><span class="anchor-text">62072112</span></span></a>), Fudan University-CIOMP Joint Fund (FC2019-005), and partly supported by the <span id="gs00002">National Natural Science Foundation of China</span> under Grant No. <a class="anchor anchor-primary" href="#gs00002"><span class="anchor-text-container"><span class="anchor-text">61972240</span></span></a>.</div></section></div><div class="related-content-links u-display-none-from-md"><button class="button-link button-link-primary button-link-small" type="button"><span class="button-link-text-container"><span class="button-link-text">Recommended articles</span></span></button></div><div class="Tail"></div><div><section class="bibliography u-font-serif text-s" id="cebibl1"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">References</h2><section class="bibliography-sec" id="cebibsec1"><ol class="references" id="reference-links-cebibsec1"><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0001" id="ref-id-bib0001" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[1]</span></span></a></span><span class="reference" id="sbref0001"><div class="contribution"><div class="authors u-font-sans">K.S. Fleckenstein</div><div id="ref-id-sbref0001" class="title text-m">Defining affect in relation to cognition: a response to Susan McLeod</div></div><div class="host u-font-sans">J. Adv. Compos., 11 (1991), pp. 447-453</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Defining%20affect%20in%20relation%20to%20cognition%3A%20a%20response%20to%20Susan%20McLeod&amp;publication_year=1991&amp;author=K.S.%20Fleckenstein" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0001"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0002" id="ref-id-bib0002" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[2]</span></span></a></span><span class="reference" id="sbref0002"><div class="contribution"><div class="authors u-font-sans">R.W. Picard</div><div id="ref-id-sbref0002" class="title text-m">Affective Computing</div></div><div class="host u-font-sans">MIT Press, Cambridge, MA, USA (1997)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Affective%20Computing&amp;publication_year=1997&amp;author=R.W.%20Picard" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0002"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0003" id="ref-id-bib0003" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[3]</span></span></a></span><span class="reference" id="sbref0003"><div class="contribution"><div class="authors u-font-sans">R.W. Picard, E. Vyzas, J. Healey</div><div id="ref-id-sbref0003" class="title text-m">Toward machine emotional intelligence: analysis of affective physiological state</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 23 (2001), pp. 1175-1191, <a class="anchor anchor-primary" href="https://doi.org/10.1109/34.954607" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/34.954607</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035481502&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0003"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Toward%20machine%20emotional%20intelligence%3A%20analysis%20of%20affective%20physiological%20state&amp;publication_year=2001&amp;author=R.W.%20Picard&amp;author=E.%20Vyzas&amp;author=J.%20Healey" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0003"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0004" id="ref-id-bib0004" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[4]</span></span></a></span><span class="reference" id="sbref0004"><div class="contribution"><div class="authors u-font-sans">J. Park, J. Kim, Y. Oh</div><div id="ref-id-sbref0004" class="title text-m">Feature vector classification based speech emotion recognition for service robots</div></div><div class="host u-font-sans">IEEE Trans. Consum. Electron., 55 (2009), pp. 1590-1596, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCE.2009.5278031" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCE.2009.5278031</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70350300961&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0004"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20vector%20classification%20based%20speech%20emotion%20recognition%20for%20service%20robots&amp;publication_year=2009&amp;author=J.%20Park&amp;author=J.%20Kim&amp;author=Y.%20Oh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0004"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0005" id="ref-id-bib0005" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[5]</span></span></a></span><span class="reference" id="sbref0005"><div class="contribution"><div class="authors u-font-sans">M. Scheutz</div><div id="ref-id-sbref0005" class="title text-m">The Affect dilemma for artificial agents: should we develop affective artificial agents?</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 424-433, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2012.29" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2012.29</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84872234280&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0005"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Affect%20dilemma%20for%20artificial%20agents%3A%20should%20we%20develop%20affective%20artificial%20agents&amp;publication_year=2012&amp;author=M.%20Scheutz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0005"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0006" id="ref-id-bib0006" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[6]</span></span></a></span><span class="reference" id="sbref0006"><div class="contribution"><div class="authors u-font-sans">D. McColl, A. Hong, N. Hatakeyama, G. Nejat, B. Benhabib</div><div id="ref-id-sbref0006" class="title text-m">A survey of autonomous human affect detection methods for social robots engaged in natural HRI</div></div><div class="host u-font-sans">J. Intell. Robot. Syst., 82 (2016), pp. 101-133, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10846-015-0259-2" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10846-015-0259-2</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84961181969&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0006"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20autonomous%20human%20affect%20detection%20methods%20for%20social%20robots%20engaged%20in%20natural%20HRI&amp;publication_year=2016&amp;author=D.%20McColl&amp;author=A.%20Hong&amp;author=N.%20Hatakeyama&amp;author=G.%20Nejat&amp;author=B.%20Benhabib" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0006"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0007" id="ref-id-bib0007" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[7]</span></span></a></span><span class="reference" id="sbref0007"><div class="contribution"><div class="authors u-font-sans">J.A. Healey, R.W. Picard</div><div id="ref-id-sbref0007" class="title text-m">Detecting stress during real-world driving tasks using physiological sensors</div></div><div class="host u-font-sans">IEEE Trans. Intell. Transp. Syst., 6 (2005), pp. 156-166, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TITS.2005.848368" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TITS.2005.848368</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-21644474051&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0007"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Detecting%20stress%20during%20real-world%20driving%20tasks%20using%20physiological%20sensors&amp;publication_year=2005&amp;author=J.A.%20Healey&amp;author=R.W.%20Picard" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0007"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0008" id="ref-id-bib0008" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[8]</span></span></a></span><span class="reference" id="sbref0008"><div class="contribution"><div class="authors u-font-sans">L.-P. Morency, J. Whitehill, J. Movellan</div><div id="ref-id-sbref0008" class="title text-m">Generalized adaptive view-based appearance model: integrated framework for monocular head pose estimation</div></div><div class="host u-font-sans">2008 8th IEEE Int. Conf. Autom. Face Gesture Recognit., IEEE, Amsterdam, Netherlands (2008), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/AFGR.2008.4813429" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.2008.4813429</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Generalized%20adaptive%20view-based%20appearance%20model%3A%20integrated%20framework%20for%20monocular%20head%20pose%20estimation&amp;publication_year=2008&amp;author=L.-P.%20Morency&amp;author=J.%20Whitehill&amp;author=J.%20Movellan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0008"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0009" id="ref-id-bib0009" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[9]</span></span></a></span><span class="reference" id="sbref0009"><div class="contribution"><div class="authors u-font-sans">J.A. Balazs, J.D. Velásquez</div><div id="ref-id-sbref0009" class="title text-m">Opinion mining and information fusion: a survey</div></div><div class="host u-font-sans">Inf. Fusion., 27 (2016), pp. 95-110, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2015.06.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2015.06.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253515000536/pdfft?md5=268ea691bc7dd37a6cfd0fc6177ff688&amp;pid=1-s2.0-S1566253515000536-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0009"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253515000536" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84934960865&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Opinion%20mining%20and%20information%20fusion%3A%20a%20survey&amp;publication_year=2016&amp;author=J.A.%20Balazs&amp;author=J.D.%20Vel%C3%A1squez" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0009"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0010" id="ref-id-bib0010" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[10]</span></span></a></span><span class="reference" id="sbref0010"><div class="contribution"><div class="authors u-font-sans">E. Cambria</div><div id="ref-id-sbref0010" class="title text-m">Affective computing and sentiment analysis</div></div><div class="host u-font-sans">IEEE Intell. Syst., 31 (2016), pp. 102-107, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MIS.2016.31" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MIS.2016.31</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84963783209&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0010"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Affective%20computing%20and%20sentiment%20analysis&amp;publication_year=2016&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0010"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0011" id="ref-id-bib0011" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[11]</span></span></a></span><span class="reference" id="sbref0011"><div class="contribution"><div class="authors u-font-sans">M. Munezero, C.S. Montero, E. Sutinen, J. Pajunen</div><div id="ref-id-sbref0011" class="title text-m">Are they different? Affect, feeling, emotion, sentiment, and opinion detection in text</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 5 (2014), pp. 101-111, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2014.2317187" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2014.2317187</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84905698338&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0011"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Are%20they%20different%20Affect%2C%20feeling%2C%20emotion%2C%20sentiment%2C%20and%20opinion%20detection%20in%20text&amp;publication_year=2014&amp;author=M.%20Munezero&amp;author=C.S.%20Montero&amp;author=E.%20Sutinen&amp;author=J.%20Pajunen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0011"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0012" id="ref-id-bib0012" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[12]</span></span></a></span><span class="reference" id="sbref0012"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, R. Bajpai, A. Hussain</div><div id="ref-id-sbref0012" class="title text-m">A review of affective computing: from unimodal analysis to multimodal fusion</div></div><div class="host u-font-sans">Inf. Fusion., 37 (2017), pp. 98-125, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2017.02.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2017.02.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253517300738/pdfft?md5=822281861315600452fb583096333978&amp;pid=1-s2.0-S1566253517300738-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0012"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253517300738" aria-describedby="ref-id-sbref0012"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85011844403&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0012"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20review%20of%20affective%20computing%3A%20from%20unimodal%20analysis%20to%20multimodal%20fusion&amp;publication_year=2017&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=R.%20Bajpai&amp;author=A.%20Hussain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0012"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0013" id="ref-id-bib0013" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[13]</span></span></a></span><span class="reference" id="sbref0013"><div class="contribution"><div class="authors u-font-sans">P.V. Rouast, M. Adam, R. Chiong</div><div id="ref-id-sbref0013" class="title text-m">Deep learning for human affect recognition: insights and new developments</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 12 (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2018.2890471" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2018.2890471</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20human%20affect%20recognition%3A%20insights%20and%20new%20developments&amp;publication_year=2019&amp;author=P.V.%20Rouast&amp;author=M.%20Adam&amp;author=R.%20Chiong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0013"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0014" id="ref-id-bib0014" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[14]</span></span></a></span><span class="reference" id="sbref0014"><div class="contribution"><div class="authors u-font-sans">N.J. Shoumy, L.-M. Ang, K.P. Seng, D.M.M. Rahaman, T. Zia</div><div id="ref-id-sbref0014" class="title text-m">Multimodal big data affective analytics: a comprehensive survey using text, audio, visual and physiological signals</div></div><div class="host u-font-sans">J. Netw. Comput. Appl., 149 (2020), Article 102447, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.jnca.2019.102447" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.jnca.2019.102447</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1084804519303078/pdfft?md5=0ff72d624abab853d56b151dd5d15d6f&amp;pid=1-s2.0-S1084804519303078-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0014"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1084804519303078" aria-describedby="ref-id-sbref0014"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85074459721&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0014"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20big%20data%20affective%20analytics%3A%20a%20comprehensive%20survey%20using%20text%2C%20audio%2C%20visual%20and%20physiological%20signals&amp;publication_year=2020&amp;author=N.J.%20Shoumy&amp;author=L.-M.%20Ang&amp;author=K.P.%20Seng&amp;author=D.M.M.%20Rahaman&amp;author=T.%20Zia" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0014"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0015" id="ref-id-bib0015" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[15]</span></span></a></span><span class="reference" id="sbref0015"><div class="contribution"><div class="authors u-font-sans">E. Paul</div><div id="ref-id-sbref0015" class="title text-m">Basic Emotions</div></div><div class="host u-font-sans">Wiley Online Library, New York: Sussex U.K (1999)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Basic%20Emotions&amp;publication_year=1999&amp;author=E.%20Paul" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0015"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0016" id="ref-id-bib0016" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[16]</span></span></a></span><span class="reference" id="sbref0016"><div class="contribution"><div class="authors u-font-sans">A. Mehrabian</div><div id="ref-id-sbref0016" class="title text-m">Basic Dimensions For A General Psychological Theory : Implications For Personality, Social, Environmental, And Developmental Studies</div></div><div class="host u-font-sans">Oelgeschlager, Gunn &amp; Hain, Cambridge (1980)</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="http://archive.org/details/basicdimensionsf0000mehr" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://archive.org/details/basicdimensionsf0000mehr</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed September 1, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Basic%20Dimensions%20For%20A%20General%20Psychological%20Theory%20%3A%20Implications%20For%20Personality%2C%20Social%2C%20Environmental%2C%20And%20Developmental%20Studies&amp;publication_year=1980&amp;author=A.%20Mehrabian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0016"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0017" id="ref-id-bib0017" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[17]</span></span></a></span><span class="reference" id="sbref0017"><div class="contribution"><div class="authors u-font-sans">Y. Jiang, W. Li, M.S. Hossain, M. Chen, A. Alelaiwi, M. Al-Hammadi</div><div id="ref-id-sbref0017" class="title text-m">A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition</div></div><div class="host u-font-sans">Inf. Fusion., 53 (2020), pp. 209-221, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2019.06.019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2019.06.019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253519301381/pdfft?md5=c654cd02d0aa3871a6264560b50ce4bb&amp;pid=1-s2.0-S1566253519301381-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0017"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253519301381" aria-describedby="ref-id-sbref0017"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85067959783&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0017"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20snapshot%20research%20and%20implementation%20of%20multimodal%20information%20fusion%20for%20data-driven%20emotion%20recognition&amp;publication_year=2020&amp;author=Y.%20Jiang&amp;author=W.%20Li&amp;author=M.S.%20Hossain&amp;author=M.%20Chen&amp;author=A.%20Alelaiwi&amp;author=M.%20Al-Hammadi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0017"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0018" id="ref-id-bib0018" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[18]</span></span></a></span><span class="reference" id="sbref0018"><div class="contribution"><div class="authors u-font-sans">C.A. Corneanu, M.O. Simón, J.F. Cohn, S.E. Guerrero</div><div id="ref-id-sbref0018" class="title text-m">Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: history, trends, and affect-related applications</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 1548-1568, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2016.2515606" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2016.2515606</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84978370387&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0018"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Survey%20on%20RGB%2C%203D%2C%20thermal%2C%20and%20multimodal%20approaches%20for%20facial%20expression%20recognition%3A%20history%2C%20trends%2C%20and%20affect-related%20applications&amp;publication_year=2016&amp;author=C.A.%20Corneanu&amp;author=M.O.%20Sim%C3%B3n&amp;author=J.F.%20Cohn&amp;author=S.E.%20Guerrero" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0018"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0019" id="ref-id-bib0019" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[19]</span></span></a></span><span class="reference" id="sbref0019"><div class="contribution"><div class="authors u-font-sans">M. El Ayadi, M.S. Kamel, F. Karray</div><div id="ref-id-sbref0019" class="title text-m">Survey on speech emotion recognition: features, classification schemes, and databases</div></div><div class="host u-font-sans">Pattern Recognit, 44 (2011), pp. 572-587, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2010.09.020" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2010.09.020</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320310004619/pdfft?md5=b4543715de5e68e600981332ad04473c&amp;pid=1-s2.0-S0031320310004619-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0019"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320310004619" aria-describedby="ref-id-sbref0019"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Survey%20on%20speech%20emotion%20recognition%3A%20features%2C%20classification%20schemes%2C%20and%20databases&amp;publication_year=2011&amp;author=M.%20El%20Ayadi&amp;author=M.S.%20Kamel&amp;author=F.%20Karray" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0019"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0020" id="ref-id-bib0020" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[20]</span></span></a></span><span class="reference" id="sbref0020"><div class="contribution"><div class="authors u-font-sans">J. Zhang, Z. Yin, P. Chen, S. Nichele</div><div id="ref-id-sbref0020" class="title text-m">Emotion recognition using multi-modal data and machine learning techniques: a tutorial and review</div></div><div class="host u-font-sans">Inf. Fusion., 59 (2020), pp. 103-126, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.01.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.01.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253519302532/pdfft?md5=61a40038165345780b6c981890b3ff8d&amp;pid=1-s2.0-S1566253519302532-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0020"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253519302532" aria-describedby="ref-id-sbref0020"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85079047781&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0020"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20multi-modal%20data%20and%20machine%20learning%20techniques%3A%20a%20tutorial%20and%20review&amp;publication_year=2020&amp;author=J.%20Zhang&amp;author=Z.%20Yin&amp;author=P.%20Chen&amp;author=S.%20Nichele" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0020"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0021" id="ref-id-bib0021" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[21]</span></span></a></span><span class="reference" id="sbref0021"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, A. Gelbukh</div><div id="ref-id-sbref0021" class="title text-m">Aspect extraction for opinion mining with a deep convolutional neural network</div></div><div class="host u-font-sans">Knowl. Based Syst, 108 (2016), pp. 42-49, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.knosys.2016.06.009" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.knosys.2016.06.009</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0950705116301721/pdfft?md5=ed706ce5e2b57bfd033f60d1db89f1ba&amp;pid=1-s2.0-S0950705116301721-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0021"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0950705116301721" aria-describedby="ref-id-sbref0021"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84981713239&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0021"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Aspect%20extraction%20for%20opinion%20mining%20with%20a%20deep%20convolutional%20neural%20network&amp;publication_year=2016&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=A.%20Gelbukh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0021"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0022" id="ref-id-bib0022" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[22]</span></span></a></span><span class="reference" id="sbref0022"><div class="other-ref"><span>E. Cambria, R. Speer, C. Havasi, A. Hussain, SenticNet: a publicly available semantic resource for opinion mining, in: AAAI2010, 2010: pp. 14–18.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=E.%20Cambria%2C%20R.%20Speer%2C%20C.%20Havasi%2C%20A.%20Hussain%2C%20SenticNet%3A%20a%20publicly%20available%20semantic%20resource%20for%20opinion%20mining%2C%20in%3A%20AAAI2010%2C%202010%3A%20pp.%2014%E2%80%9318." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0023" id="ref-id-bib0023" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[23]</span></span></a></span><span class="reference" id="sbref0023"><div class="contribution"><div class="authors u-font-sans">M.S. Akhtar, D. Chauhan, D. Ghosal, S. Poria, A. Ekbal, P. Bhattacharyya</div><div id="ref-id-sbref0023" class="title text-m">Multi-task learning for multi-modal emotion recognition and sentiment analysis</div></div><div class="host u-font-sans">Proc. 2019 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol. Vol. 1 Long Short Pap., Association for Computational Linguistics, Minneapolis, Minnesota (2019), pp. 370-379, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/N19-1034" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/N19-1034</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85084289281&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0023"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-task%20learning%20for%20multi-modal%20emotion%20recognition%20and%20sentiment%20analysis&amp;publication_year=2019&amp;author=M.S.%20Akhtar&amp;author=D.%20Chauhan&amp;author=D.%20Ghosal&amp;author=S.%20Poria&amp;author=A.%20Ekbal&amp;author=P.%20Bhattacharyya" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0023"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0024" id="ref-id-bib0024" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[24]</span></span></a></span><span class="reference" id="sbref0024"><div class="other-ref"><span>A. Mehrabian, Communicating without words, Psychol. Today. (1968) 53–55.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Mehrabian%2C%20Communicating%20without%20words%2C%20Psychol.%20Today.%20(1968)%2053%E2%80%9355." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0025" id="ref-id-bib0025" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[25]</span></span></a></span><span class="reference" id="sbref0025"><div class="contribution"><div class="authors u-font-sans">C.O. Alm, D. Roth, R. Sproat</div><div id="ref-id-sbref0025" class="title text-m">Emotions from text: machine learning for text-based emotion prediction</div></div><div class="host u-font-sans">Proc. Conf. Hum. Lang. Technol. Empir. Methods Nat. Lang. Process. - HLT 05, Association for Computational Linguistics, Vancouver, British Columbia, Canada (2005), pp. 579-586, <a class="anchor anchor-primary" href="https://doi.org/10.3115/1220575.1220648" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3115/1220575.1220648</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80053247759&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0025"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotions%20from%20text%3A%20machine%20learning%20for%20text-based%20emotion%20prediction&amp;publication_year=2005&amp;author=C.O.%20Alm&amp;author=D.%20Roth&amp;author=R.%20Sproat" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0025"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0026" id="ref-id-bib0026" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[26]</span></span></a></span><span class="reference" id="sbref0026"><div class="contribution"><div class="authors u-font-sans">Z.-T. Liu, Q. Xie, M. Wu, W.-H. Cao, Y. Mei, J.-W. Mao</div><div id="ref-id-sbref0026" class="title text-m">Speech emotion recognition based on an improved brain emotion learning model</div></div><div class="host u-font-sans">Neurocomputing, 309 (2018), pp. 145-156, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2018.05.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2018.05.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231218305344/pdfft?md5=8261a691880fc08138cb963134da98c0&amp;pid=1-s2.0-S0925231218305344-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0026"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231218305344" aria-describedby="ref-id-sbref0026"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048259554&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0026"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20based%20on%20an%20improved%20brain%20emotion%20learning%20model&amp;publication_year=2018&amp;author=Z.-T.%20Liu&amp;author=Q.%20Xie&amp;author=M.%20Wu&amp;author=W.-H.%20Cao&amp;author=Y.%20Mei&amp;author=J.-W.%20Mao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0026"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0027" id="ref-id-bib0027" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[27]</span></span></a></span><span class="reference" id="sbref0027"><div class="contribution"><div class="authors u-font-sans">M. Sajjad, M. Nasir, F.U.M. Ullah, K. Muhammad, A.K. Sangaiah, S.W. Baik</div><div id="ref-id-sbref0027" class="title text-m">Raspberry Pi assisted facial expression recognition framework for smart security in law-enforcement services</div></div><div class="host u-font-sans">Inf. Sci., 479 (2019), pp. 416-431, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ins.2018.07.027" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ins.2018.07.027</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0020025518305425/pdfft?md5=c56ef1793f072a2eb6878bdcfd65ac28&amp;pid=1-s2.0-S0020025518305425-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0027"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0020025518305425" aria-describedby="ref-id-sbref0027"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85050885664&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0027"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Raspberry%20Pi%20assisted%20facial%20expression%20recognition%20framework%20for%20smart%20security%20in%20law-enforcement%20services&amp;publication_year=2019&amp;author=M.%20Sajjad&amp;author=M.%20Nasir&amp;author=F.U.M.%20Ullah&amp;author=K.%20Muhammad&amp;author=A.K.%20Sangaiah&amp;author=S.W.%20Baik" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0027"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0028" id="ref-id-bib0028" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[28]</span></span></a></span><span class="reference" id="sbref0028"><div class="contribution"><div class="authors u-font-sans">P. Sarkar, A. Etemad</div><div id="ref-id-sbref0028" class="title text-m">Self-supervised ECG representation learning for emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), p. 1, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.3014842" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.3014842</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85086034815&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0028"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Self-supervised%20ECG%20representation%20learning%20for%20emotion%20recognition&amp;publication_year=2020&amp;author=P.%20Sarkar&amp;author=A.%20Etemad" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0028"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0029" id="ref-id-bib0029" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[29]</span></span></a></span><span class="reference" id="sbref0029"><div class="contribution"><div class="authors u-font-sans">S.M. Alarcão, M.J. Fonseca</div><div id="ref-id-sbref0029" class="title text-m">Emotions recognition using EEG signals: a survey</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 10 (2019), pp. 374-393, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2714671" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2714671</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021838158&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0029"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotions%20recognition%20using%20EEG%20signals%3A%20a%20survey&amp;publication_year=2019&amp;author=S.M.%20Alarc%C3%A3o&amp;author=M.J.%20Fonseca" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0029"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0030" id="ref-id-bib0030" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[30]</span></span></a></span><span class="reference" id="sbref0030"><div class="contribution"><div class="authors u-font-sans">J. Kim, E. Andre</div><div id="ref-id-sbref0030" class="title text-m">Emotion recognition based on physiological changes in music listening</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 30 (2008), pp. 2067-2083, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2008.26" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2008.26</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-56649124649&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0030"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20based%20on%20physiological%20changes%20in%20music%20listening&amp;publication_year=2008&amp;author=J.%20Kim&amp;author=E.%20Andre" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0030"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0031" id="ref-id-bib0031" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[31]</span></span></a></span><span class="reference" id="sbref0031"><div class="contribution"><div class="authors u-font-sans">P. Tzirakis, G. Trigeorgis, M.A. Nicolaou, B.W. Schuller, S. Zafeiriou</div><div id="ref-id-sbref0031" class="title text-m">End-to-end multimodal emotion recognition using deep neural networks</div></div><div class="host u-font-sans">IEEE J. Sel. Top. Signal Process., 11 (2017), pp. 1301-1309, <a class="anchor anchor-primary" href="https://doi.org/10.1109/JSTSP.2017.2764438" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/JSTSP.2017.2764438</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85032261257&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0031"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=End-to-end%20multimodal%20emotion%20recognition%20using%20deep%20neural%20networks&amp;publication_year=2017&amp;author=P.%20Tzirakis&amp;author=G.%20Trigeorgis&amp;author=M.A.%20Nicolaou&amp;author=B.W.%20Schuller&amp;author=S.%20Zafeiriou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0031"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0032" id="ref-id-bib0032" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[32]</span></span></a></span><span class="reference" id="sbref0032"><div class="contribution"><div class="authors u-font-sans">T. Baltrusaitis, P. Robinson, L. Morency</div><div id="ref-id-sbref0032" class="title text-m">3D Constrained Local Model for rigid and non-rigid facial tracking</div></div><div class="host u-font-sans">2012 IEEE Conf. Comput. Vis. Pattern Recognit., IEEE, Providence, RI (2012), pp. 2610-2617, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2012.6247980" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2012.6247980</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866686647&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0032"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3D%20Constrained%20Local%20Model%20for%20rigid%20and%20non-rigid%20facial%20tracking&amp;publication_year=2012&amp;author=T.%20Baltrusaitis&amp;author=P.%20Robinson&amp;author=L.%20Morency" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0032"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0033" id="ref-id-bib0033" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[33]</span></span></a></span><span class="reference" id="sbref0033"><div class="contribution"><div class="authors u-font-sans">J.-C. Lin, C.-H. Wu, W.-L. Wei</div><div id="ref-id-sbref0033" class="title text-m">Error weighted semi-coupled hidden markov model for audio-visual emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 14 (2012), pp. 142-156, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2011.2171334" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2011.2171334</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84862961455&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0033"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Error%20weighted%20semi-coupled%20hidden%20markov%20model%20for%20audio-visual%20emotion%20recognition&amp;publication_year=2012&amp;author=J.-C.%20Lin&amp;author=C.-H.%20Wu&amp;author=W.-L.%20Wei" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0033"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0034" id="ref-id-bib0034" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[34]</span></span></a></span><span class="reference" id="sbref0034"><div class="contribution"><div class="authors u-font-sans">X. Huang, J. Kortelainen, G. Zhao, X. Li, A. Moilanen, T. Seppänen, M. Pietikäinen</div><div id="ref-id-sbref0034" class="title text-m">Multi-modal emotion analysis from facial expressions and electroencephalogram</div></div><div class="host u-font-sans">Comput. Vis. Image Underst., 147 (2016), pp. 114-124, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.cviu.2015.09.015" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.cviu.2015.09.015</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1077314215002106/pdfft?md5=28a2752b3226e70475fc0a322ea6262c&amp;pid=1-s2.0-S1077314215002106-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0034"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1077314215002106" aria-describedby="ref-id-sbref0034"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84990050332&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0034"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-modal%20emotion%20analysis%20from%20facial%20expressions%20and%20electroencephalogram&amp;publication_year=2016&amp;author=X.%20Huang&amp;author=J.%20Kortelainen&amp;author=G.%20Zhao&amp;author=X.%20Li&amp;author=A.%20Moilanen&amp;author=T.%20Sepp%C3%A4nen&amp;author=M.%20Pietik%C3%A4inen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0034"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0035" id="ref-id-bib0035" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[35]</span></span></a></span><span class="reference" id="sbref0035"><div class="contribution"><div class="authors u-font-sans">Z. Wang, X. Zhou, W. Wang, C. Liang</div><div id="ref-id-sbref0035" class="title text-m">Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video</div></div><div class="host u-font-sans">Int. J. Mach. Learn. Cybern., 11 (2020), pp. 923-934, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s13042-019-01056-8" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s13042-019-01056-8</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85078245062&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0035"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20multimodal%20deep%20learning%20in%20multiple%20psychophysiological%20signals%20and%20video&amp;publication_year=2020&amp;author=Z.%20Wang&amp;author=X.%20Zhou&amp;author=W.%20Wang&amp;author=C.%20Liang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0035"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0036" id="ref-id-bib0036" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[36]</span></span></a></span><span class="reference" id="sbref0036"><div class="contribution"><div class="authors u-font-sans">T. Meng, X. Jing, Z. Yan, W. Pedrycz</div><div id="ref-id-sbref0036" class="title text-m">A survey on machine learning for data fusion</div></div><div class="host u-font-sans">Inf. Fusion., 57 (2020), pp. 115-129, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2019.12.001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2019.12.001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253519303902/pdfft?md5=501ffbc7489b25e9fc6805269daa1ea5&amp;pid=1-s2.0-S1566253519303902-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0036"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253519303902" aria-describedby="ref-id-sbref0036"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85076856977&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0036"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20on%20machine%20learning%20for%20data%20fusion&amp;publication_year=2020&amp;author=T.%20Meng&amp;author=X.%20Jing&amp;author=Z.%20Yan&amp;author=W.%20Pedrycz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0036"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0037" id="ref-id-bib0037" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[37]</span></span></a></span><span class="reference" id="sbref0037"><div class="contribution"><div class="authors u-font-sans">S. Zhang, S. Zhang, T. Huang, W. Gao, Q. Tian</div><div id="ref-id-sbref0037" class="title text-m">Learning affective features with a hybrid deep model for audio–visual emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Circuits Syst. Video Technol., 28 (2018), pp. 3030-3043, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCSVT.2017.2719043" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCSVT.2017.2719043</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85023777983&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0037"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20affective%20features%20with%20a%20hybrid%20deep%20model%20for%20audiovisual%20emotion%20recognition&amp;publication_year=2018&amp;author=S.%20Zhang&amp;author=S.%20Zhang&amp;author=T.%20Huang&amp;author=W.%20Gao&amp;author=Q.%20Tian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0037"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0038" id="ref-id-bib0038" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[38]</span></span></a></span><span class="reference" id="sbref0038"><div class="contribution"><div class="authors u-font-sans">B. Ko</div><div id="ref-id-sbref0038" class="title text-m">A brief review of facial emotion recognition based on visual information</div></div><div class="host u-font-sans">Sensors, 18 (2018), p. 401, <a class="anchor anchor-primary" href="https://doi.org/10.3390/s18020401" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/s18020401</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85041431671&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0038"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20brief%20review%20of%20facial%20emotion%20recognition%20based%20on%20visual%20information&amp;publication_year=2018&amp;author=B.%20Ko" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0038"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0039" id="ref-id-bib0039" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[39]</span></span></a></span><span class="reference" id="sbref0039"><div class="contribution"><div class="authors u-font-sans">S. Li, W. Deng</div><div id="ref-id-sbref0039" class="title text-m">Deep facial expression recognition: a survey</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.2981446" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.2981446</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20facial%20expression%20recognition%3A%20a%20survey&amp;publication_year=2020&amp;author=S.%20Li&amp;author=W.%20Deng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0039"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0040" id="ref-id-bib0040" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[40]</span></span></a></span><span class="reference" id="sbref0040"><div class="other-ref"><span>W. Merghani, A.K. Davison, M.H. Yap, A review on facial micro-expressions analysis: datasets, features and metrics, ArXiv180502397 Cs. (2018). <a class="anchor anchor-primary" href="http://arxiv.org/abs/1805.02397" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/1805.02397</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a> (accessed November 21, 2019).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=W.%20Merghani%2C%20A.K.%20Davison%2C%20M.H.%20Yap%2C%20A%20review%20on%20facial%20micro-expressions%20analysis%3A%20datasets%2C%20features%20and%20metrics%2C%20ArXiv180502397%20Cs.%20(2018).%20http%3A%2F%2Farxiv.org%2Fabs%2F1805.02397%20(accessed%20November%2021%2C%202019)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0041" id="ref-id-bib0041" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[41]</span></span></a></span><span class="reference" id="sbref0041"><div class="contribution"><div class="authors u-font-sans">G.R. Alexandre, J.M. Soares, G.A. Pereira Thé</div><div id="ref-id-sbref0041" class="title text-m">Systematic review of 3D facial expression recognition methods</div></div><div class="host u-font-sans">Pattern Recognit, 100 (2020), Article 107108, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2019.107108" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2019.107108</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320319304091/pdfft?md5=86ee07af88734ed075cc05380be02b24&amp;pid=1-s2.0-S0031320319304091-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0041"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320319304091" aria-describedby="ref-id-sbref0041"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85075008227&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0041"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Systematic%20review%20of%203D%20facial%20expression%20recognition%20methods&amp;publication_year=2020&amp;author=G.R.%20Alexandre&amp;author=J.M.%20Soares&amp;author=G.A.%20Pereira%20Th%C3%A9" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0041"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0042" id="ref-id-bib0042" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[42]</span></span></a></span><span class="reference" id="sbref0042"><div class="contribution"><div class="authors u-font-sans">R. Liu, Y. Shi, C. Ji, M. Jia</div><div id="ref-id-sbref0042" class="title text-m">A survey of sentiment analysis based on transfer learning</div></div><div class="host u-font-sans">IEEE Access, 7 (2019), pp. 85401-85412, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2019.2925059" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2019.2925059</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068910187&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0042"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20sentiment%20analysis%20based%20on%20transfer%20learning&amp;publication_year=2019&amp;author=R.%20Liu&amp;author=Y.%20Shi&amp;author=C.%20Ji&amp;author=M.%20Jia" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0042"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0043" id="ref-id-bib0043" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[43]</span></span></a></span><span class="reference" id="sbref0043"><div class="contribution"><div class="authors u-font-sans">R.A. Khalil, E. Jones, M.I. Babar, T. Jan, M.H. Zafar, T. Alhussain</div><div id="ref-id-sbref0043" class="title text-m">Speech emotion recognition using deep learning techniques: a review</div></div><div class="host u-font-sans">IEEE Access, 7 (2019), pp. 117327-117345, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2019.2936124" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2019.2936124</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85097333678&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0043"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20deep%20learning%20techniques%3A%20a%20review&amp;publication_year=2019&amp;author=R.A.%20Khalil&amp;author=E.%20Jones&amp;author=M.I.%20Babar&amp;author=T.%20Jan&amp;author=M.H.%20Zafar&amp;author=T.%20Alhussain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0043"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0044" id="ref-id-bib0044" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[44]</span></span></a></span><span class="reference" id="sbref0044"><div class="contribution"><div class="authors u-font-sans">K. Patel, D. Mehta, C. Mistry, R. Gupta, S. Tanwar, N. Kumar, M. Alazab</div><div id="ref-id-sbref0044" class="title text-m">Facial sentiment analysis using ai techniques: state-of-the-art, taxonomies, and challenges</div></div><div class="host u-font-sans">IEEE Access., 8 (2020), pp. 90495-90519, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2020.2993803" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2020.2993803</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85085573248&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0044"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20sentiment%20analysis%20using%20ai%20techniques%3A%20state-of-the-art%2C%20taxonomies%2C%20and%20challenges&amp;publication_year=2020&amp;author=K.%20Patel&amp;author=D.%20Mehta&amp;author=C.%20Mistry&amp;author=R.%20Gupta&amp;author=S.%20Tanwar&amp;author=N.%20Kumar&amp;author=M.%20Alazab" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0044"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0045" id="ref-id-bib0045" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[45]</span></span></a></span><span class="reference" id="sbref0045"><div class="contribution"><div class="authors u-font-sans">F. Noroozi, D. Kaminska, C. Corneanu, T. Sapinski, S. Escalera, G. Anbarjafari</div><div id="ref-id-sbref0045" class="title text-m">Survey on emotional body gesture recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2018), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2018.2874986" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2018.2874986</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Survey%20on%20emotional%20body%20gesture%20recognition&amp;publication_year=2018&amp;author=F.%20Noroozi&amp;author=D.%20Kaminska&amp;author=C.%20Corneanu&amp;author=T.%20Sapinski&amp;author=S.%20Escalera&amp;author=G.%20Anbarjafari" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0045"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0046" id="ref-id-bib0046" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[46]</span></span></a></span><span class="reference" id="sbref0046"><div class="contribution"><div class="authors u-font-sans">S. Poria, N. Majumder, R. Mihalcea, E. Hovy</div><div id="ref-id-sbref0046" class="title text-m">Emotion recognition in conversation: research challenges, datasets, and recent advances</div></div><div class="host u-font-sans">IEEE Access, 7 (2019), pp. 100943-100953, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2019.2929050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2019.2929050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85084025588&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0046"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20in%20conversation%3A%20research%20challenges%2C%20datasets%2C%20and%20recent%20advances&amp;publication_year=2019&amp;author=S.%20Poria&amp;author=N.%20Majumder&amp;author=R.%20Mihalcea&amp;author=E.%20Hovy" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0046"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0047" id="ref-id-bib0047" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[47]</span></span></a></span><span class="reference" id="sbref0047"><div class="contribution"><div class="authors u-font-sans">L. Yue, W. Chen, X. Li, W. Zuo, M. Yin</div><div id="ref-id-sbref0047" class="title text-m">A survey of sentiment analysis in social media</div></div><div class="host u-font-sans">Knowl. Inf. Syst., 60 (2019), pp. 617-663, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10115-018-1236-4" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10115-018-1236-4</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85049564231&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0047"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20sentiment%20analysis%20in%20social%20media&amp;publication_year=2019&amp;author=L.%20Yue&amp;author=W.%20Chen&amp;author=X.%20Li&amp;author=W.%20Zuo&amp;author=M.%20Yin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0047"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0048" id="ref-id-bib0048" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[48]</span></span></a></span><span class="reference" id="sbref0048"><div class="contribution"><div class="authors u-font-sans">Z. Wang, S.-B. Ho, E. Cambria</div><div id="ref-id-sbref0048" class="title text-m">A review of emotion sensing: categorization models and algorithms</div></div><div class="host u-font-sans">Multimed. Tools Appl., 79 (2020), pp. 35553-35582, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11042-019-08328-z" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11042-019-08328-z</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85077555792&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0048"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20review%20of%20emotion%20sensing%3A%20categorization%20models%20and%20algorithms&amp;publication_year=2020&amp;author=Z.%20Wang&amp;author=S.-B.%20Ho&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0048"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0049" id="ref-id-bib0049" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[49]</span></span></a></span><span class="reference" id="sbref0049"><div class="contribution"><div class="authors u-font-sans">J. Han, Z. Zhang, N. Cummins, B. Schuller</div><div id="ref-id-sbref0049" class="title text-m">Adversarial Training in affective computing and sentiment analysis: recent advances and perspectives</div></div><div class="host u-font-sans">IEEE Comput. Intell. Mag., 14 (2019), pp. 68-81, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MCI.2019.2901088" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MCI.2019.2901088</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adversarial%20Training%20in%20affective%20computing%20and%20sentiment%20analysis%3A%20recent%20advances%20and%20perspectives&amp;publication_year=2019&amp;author=J.%20Han&amp;author=Z.%20Zhang&amp;author=N.%20Cummins&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0049"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0050" id="ref-id-bib0050" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[50]</span></span></a></span><span class="reference" id="sbref0050"><div class="contribution"><div class="authors u-font-sans">P.J. Bota, C. Wang, A.L.N. Fred, H. Placido Da Silva, A Review</div><div id="ref-id-sbref0050" class="title text-m">Current challenges, and future possibilities on emotion recognition using machine learning and physiological signals</div></div><div class="host u-font-sans">IEEE Access, 7 (2019), pp. 140990-141020, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2019.2944001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2019.2944001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85077742997&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0050"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Current%20challenges%2C%20and%20future%20possibilities%20on%20emotion%20recognition%20using%20machine%20learning%20and%20physiological%20signals&amp;publication_year=2019&amp;author=P.J.%20Bota&amp;author=C.%20Wang&amp;author=A.L.N.%20Fred&amp;author=H.%20Placido%20Da%20Silva&amp;author=A%20Review" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0050"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0051" id="ref-id-bib0051" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[51]</span></span></a></span><span class="reference" id="sbref0051"><div class="contribution"><div class="authors u-font-sans">B. Garcia-Martinez, A. Martinez-Rodrigo, R. Alcaraz, A. Fernandez-Caballero</div><div id="ref-id-sbref0051" class="title text-m">A review on nonlinear methods using electroencephalographic recordings for emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2018.2890636" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2018.2890636</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20review%20on%20nonlinear%20methods%20using%20electroencephalographic%20recordings%20for%20emotion%20recognition&amp;publication_year=2019&amp;author=B.%20Garcia-Martinez&amp;author=A.%20Martinez-Rodrigo&amp;author=R.%20Alcaraz&amp;author=A.%20Fernandez-Caballero" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0051"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0052" id="ref-id-bib0052" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[52]</span></span></a></span><span class="reference" id="sbref0052"><div class="contribution"><div class="authors u-font-sans">P. Ekman</div><div id="ref-id-sbref0052" class="title text-m">Universals and cultural differences in facial expressions of emotion</div></div><div class="host u-font-sans">Nebr. Symp. Motiv., 19 (1971), pp. 207-283</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Universals%20and%20cultural%20differences%20in%20facial%20expressions%20of%20emotion&amp;publication_year=1971&amp;author=P.%20Ekman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0052"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0053" id="ref-id-bib0053" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[53]</span></span></a></span><span class="reference" id="sbref0053"><div class="contribution"><div class="authors u-font-sans">J.L. Tracy, D. Randles</div><div id="ref-id-sbref0053" class="title text-m">Four models of basic emotions: a review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt</div></div><div class="host u-font-sans">Emot. Rev., 3 (2011), pp. 397-405, <a class="anchor anchor-primary" href="https://doi.org/10.1177/1754073911410747" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1177/1754073911410747</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80053331253&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0053"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Four%20models%20of%20basic%20emotions%3A%20a%20review%20of%20Ekman%20and%20Cordaro%2C%20Izard%2C%20Levenson%2C%20and%20Panksepp%20and%20Watt&amp;publication_year=2011&amp;author=J.L.%20Tracy&amp;author=D.%20Randles" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0053"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0054" id="ref-id-bib0054" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[54]</span></span></a></span><span class="reference" id="sbref0054"><div class="contribution"><div class="authors u-font-sans">J. Russell</div><div id="ref-id-sbref0054" class="title text-m">A circumplex model of affect</div></div><div class="host u-font-sans">J. Pers. Soc. Psychol., 39 (1980), pp. 1161-1178, <a class="anchor anchor-primary" href="https://doi.org/10.1037/h0077714" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1037/h0077714</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20circumplex%20model%20of%20affect&amp;publication_year=1980&amp;author=J.%20Russell" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0054"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0055" id="ref-id-bib0055" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[55]</span></span></a></span><span class="reference" id="sbref0055"><div class="contribution"><div class="authors u-font-sans">Robert plutchik</div><div id="ref-id-sbref0055" class="title text-m">Emotion and life: perspective from psychology biology and evolution</div></div><div class="host u-font-sans">Am. Physiol. Assoc. (2003)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20and%20life%3A%20perspective%20from%20psychology%20biology%20and%20evolution&amp;publication_year=2003&amp;author=Robert%20plutchik" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0055"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0056" id="ref-id-bib0056" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[56]</span></span></a></span><span class="reference" id="sbref0056"><div class="contribution"><div class="authors u-font-sans">E. Cambria, A. Livingstone, A. Hussain</div><div id="ref-id-sbref0056" class="title text-m">The Hourglass of emotions</div></div><div class="host u-font-sans">A. Esposito, A.M. Esposito, A. Vinciarelli, R. Hoffmann, V.C. Müller (Eds.), Cogn. Behav. Syst., Springer Berlin Heidelberg, Berlin, Heidelberg (2012), pp. 144-157, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-34584-5_11" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-34584-5_11</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84870364413&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0056"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Hourglass%20of%20emotions&amp;publication_year=2012&amp;author=E.%20Cambria&amp;author=A.%20Livingstone&amp;author=A.%20Hussain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0056"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0057" id="ref-id-bib0057" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[57]</span></span></a></span><span class="reference" id="sbref0057"><div class="contribution"><div class="authors u-font-sans">Y. Susanto, A.G. Livingstone, B.C. Ng, E. Cambria</div><div id="ref-id-sbref0057" class="title text-m">The Hourglass model revisited</div></div><div class="host u-font-sans">IEEE Intell. Syst., 35 (2020), pp. 96-102, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MIS.2020.2992799" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MIS.2020.2992799</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85086986772&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0057"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Hourglass%20model%20revisited&amp;publication_year=2020&amp;author=Y.%20Susanto&amp;author=A.G.%20Livingstone&amp;author=B.C.%20Ng&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0057"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0058" id="ref-id-bib0058" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[58]</span></span></a></span><span class="reference" id="sbref0058"><div class="contribution"><div class="authors u-font-sans">A.T. Lopes, E. de Aguiar, A.F. De Souza, T. Oliveira-Santos</div><div id="ref-id-sbref0058" class="title text-m">Facial expression recognition with Convolutional Neural Networks: coping with few data and the training sample order</div></div><div class="host u-font-sans">Pattern Recognit., 61 (2017), pp. 610-628, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2016.07.026" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2016.07.026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320316301753/pdfft?md5=3652c422d2874205c7eb2b753a3aa71b&amp;pid=1-s2.0-S0031320316301753-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0058"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320316301753" aria-describedby="ref-id-sbref0058"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84991821737&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0058"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20with%20Convolutional%20Neural%20Networks%3A%20coping%20with%20few%20data%20and%20the%20training%20sample%20order&amp;publication_year=2017&amp;author=A.T.%20Lopes&amp;author=E.%20de%20Aguiar&amp;author=A.F.%20De%20Souza&amp;author=T.%20Oliveira-Santos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0058"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0059" id="ref-id-bib0059" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[59]</span></span></a></span><span class="reference" id="sbref0059"><div class="contribution"><div class="authors u-font-sans">Z. Ren, A. Baird, J. Han, Z. Zhang, B. Schuller</div><div id="ref-id-sbref0059" class="title text-m">Generating and protecting against adversarial attacks for deep speech-based emotion recognition models</div></div><div class="host u-font-sans">ICASSP 2020 - 2020 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2020), pp. 7184-7188, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP40776.2020.9054087" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP40776.2020.9054087</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089210952&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0059"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Generating%20and%20protecting%20against%20adversarial%20attacks%20for%20deep%20speech-based%20emotion%20recognition%20models&amp;publication_year=2020&amp;author=Z.%20Ren&amp;author=A.%20Baird&amp;author=J.%20Han&amp;author=Z.%20Zhang&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0059"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0060" id="ref-id-bib0060" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[60]</span></span></a></span><span class="reference" id="sbref0060"><div class="contribution"><div class="authors u-font-sans">Z. Wang, S.-B. Ho, E. Cambria</div><div id="ref-id-sbref0060" class="title text-m">Multi-level fine-scaled sentiment sensing with ambivalence handling</div></div><div class="host u-font-sans">Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 28 (2020), pp. 683-697, <a class="anchor anchor-primary" href="https://doi.org/10.1142/S0218488520500294" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1142/S0218488520500294</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85093829726&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0060"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-level%20fine-scaled%20sentiment%20sensing%20with%20ambivalence%20handling&amp;publication_year=2020&amp;author=Z.%20Wang&amp;author=S.-B.%20Ho&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0060"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0061" id="ref-id-bib0061" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[61]</span></span></a></span><span class="reference" id="sbref0061"><div class="contribution"><div class="authors u-font-sans">I. Bakker, T. van der Voordt, P. Vink, J. de Boon</div><div id="ref-id-sbref0061" class="title text-m">Pleasure, arousal, dominance: mehrabian and russell revisited</div></div><div class="host u-font-sans">Curr. Psychol., 33 (2014), pp. 405-421, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12144-014-9219-4" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12144-014-9219-4</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84957434394&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0061"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Pleasure%2C%20arousal%2C%20dominance%3A%20mehrabian%20and%20russell%20revisited&amp;publication_year=2014&amp;author=I.%20Bakker&amp;author=T.%20van%20der%20Voordt&amp;author=P.%20Vink&amp;author=J.%20de%20Boon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0061"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0062" id="ref-id-bib0062" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[62]</span></span></a></span><span class="reference" id="sbref0062"><div class="contribution"><div class="authors u-font-sans">J.A. Russell, A. Mehrabian</div><div id="ref-id-sbref0062" class="title text-m">Evidence for a three-factor theory of emotions</div></div><div class="host u-font-sans">J. Res. Personal., 11 (1977), pp. 273-294, <a class="anchor anchor-primary" href="https://doi.org/10.1016/0092-6566(77)90037-X" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/0092-6566(77)90037-X</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/009265667790037X/pdf?md5=c4cc17921e58903f44330a4b02c8fa68&amp;pid=1-s2.0-009265667790037X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0062"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/009265667790037X" aria-describedby="ref-id-sbref0062"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0017712350&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0062"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Evidence%20for%20a%20three-factor%20theory%20of%20emotions&amp;publication_year=1977&amp;author=J.A.%20Russell&amp;author=A.%20Mehrabian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0062"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0063" id="ref-id-bib0063" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[63]</span></span></a></span><span class="reference" id="sbref0063"><div class="other-ref"><span>H. Dabas, C. Sethi, C. Dua, M. Dalawat, D. Sethia, Emotion classification using EEG Signals, in: 2018: pp. 380–384. 10.1145/3297156.3297177.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=H.%20Dabas%2C%20C.%20Sethi%2C%20C.%20Dua%2C%20M.%20Dalawat%2C%20D.%20Sethia%2C%20Emotion%20classification%20using%20EEG%20Signals%2C%20in%3A%202018%3A%20pp.%20380%E2%80%93384.%2010.1145%2F3297156.3297177." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0064" id="ref-id-bib0064" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[64]</span></span></a></span><span class="reference" id="sbref0064"><div class="contribution"><div class="authors u-font-sans">J. Blitzer, M. Dredze, F. Pereira</div><div id="ref-id-sbref0064" class="title text-m">Biographies, bollywood, boom-boxes and blenders: domain adaptation for sentiment classification</div></div><div class="host u-font-sans">ACL (2007), p. 8</div><div class="comment">(n.d.)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Biographies%2C%20bollywood%2C%20boom-boxes%20and%20blenders%3A%20domain%20adaptation%20for%20sentiment%20classification&amp;publication_year=2007&amp;author=J.%20Blitzer&amp;author=M.%20Dredze&amp;author=F.%20Pereira" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0064"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0065" id="ref-id-bib0065" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[65]</span></span></a></span><span class="reference" id="sbref0065"><div class="contribution"><div class="authors u-font-sans">M. Dredze, K. Crammer, F. Pereira</div><div id="ref-id-sbref0065" class="title text-m">Confidence-weighted linear classification</div></div><div class="host u-font-sans">Proc. 25th Int. Conf. Mach. Learn. - ICML 08, ACM Press, Helsinki, Finland (2008), pp. 264-271, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1390156.1390190" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1390156.1390190</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-56449101965&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0065"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Confidence-weighted%20linear%20classification&amp;publication_year=2008&amp;author=M.%20Dredze&amp;author=K.%20Crammer&amp;author=F.%20Pereira" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0065"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0066" id="ref-id-bib0066" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[66]</span></span></a></span><span class="reference" id="sbref0066"><div class="contribution"><div class="authors u-font-sans">A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, C. Potts</div><div id="ref-id-sbref0066" class="title text-m">Learning word vectors for sentiment analysis</div></div><div class="host u-font-sans">Proc. 49th Annu. Meet. Assoc. Comput. Linguist. Hum. Lang. Technol., Association for Computational Linguistics, Portland, Oregon, USA (2011), pp. 142-150</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/P11-1015" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/P11-1015</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed July 23, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859023447&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0066"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20word%20vectors%20for%20sentiment%20analysis&amp;publication_year=2011&amp;author=A.L.%20Maas&amp;author=R.E.%20Daly&amp;author=P.T.%20Pham&amp;author=D.%20Huang&amp;author=A.Y.%20Ng&amp;author=C.%20Potts" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0066"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0067" id="ref-id-bib0067" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[67]</span></span></a></span><span class="reference" id="sbref0067"><div class="other-ref"><span>R. Socher, A. Perelygin, J. Wu, J. Chuang, C.D. Manning, A. Ng, C. Potts, Recursive deep models for semantic compositionality over a sentiment treebank, (n.d.) 12.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=R.%20Socher%2C%20A.%20Perelygin%2C%20J.%20Wu%2C%20J.%20Chuang%2C%20C.D.%20Manning%2C%20A.%20Ng%2C%20C.%20Potts%2C%20Recursive%20deep%20models%20for%20semantic%20compositionality%20over%20a%20sentiment%20treebank%2C%20(n.d.)%2012." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0068" id="ref-id-bib0068" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[68]</span></span></a></span><span class="reference" id="sbref0068"><div class="other-ref"><span>F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, B. Weiss, A database of German emotional speech, (2005) 4.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=F.%20Burkhardt%2C%20A.%20Paeschke%2C%20M.%20Rolfes%2C%20W.%20Sendlmeier%2C%20B.%20Weiss%2C%20A%20database%20of%20German%20emotional%20speech%2C%20(2005)%204." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0069" id="ref-id-bib0069" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[69]</span></span></a></span><span class="reference" id="sbref0069"><div class="contribution"><div class="authors u-font-sans">I. Sneddon, M. McRorie, G. McKeown, J. Hanratty</div><div id="ref-id-sbref0069" class="title text-m">The Belfast Induced natural emotion database</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 32-41, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.26" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.26</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859889885&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0069"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20Belfast%20Induced%20natural%20emotion%20database&amp;publication_year=2012&amp;author=I.%20Sneddon&amp;author=M.%20McRorie&amp;author=G.%20McKeown&amp;author=J.%20Hanratty" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0069"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0070" id="ref-id-bib0070" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[70]</span></span></a></span><span class="reference" id="sbref0070"><div class="contribution"><div class="authors u-font-sans">M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba</div><div id="ref-id-sbref0070" class="title text-m">Coding facial expressions with Gabor wavelets</div></div><div class="host u-font-sans">Proc. Third IEEE Int. Conf. Autom. Face Gesture Recognit (1998), pp. 200-205, <a class="anchor anchor-primary" href="https://doi.org/10.1109/AFGR.1998.670949" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.1998.670949</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84857444525&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0070"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Coding%20facial%20expressions%20with%20Gabor%20wavelets&amp;publication_year=1998&amp;author=M.%20Lyons&amp;author=S.%20Akamatsu&amp;author=M.%20Kamachi&amp;author=J.%20Gyoba" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0070"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0071" id="ref-id-bib0071" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[71]</span></span></a></span><span class="reference" id="sbref0071"><div class="contribution"><div class="authors u-font-sans">P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews</div><div id="ref-id-sbref0071" class="title text-m">The extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression</div></div><div class="host u-font-sans">2010 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. - Workshop, IEEE, San Francisco, CA, USA (2010), pp. 94-101, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPRW.2010.5543262" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPRW.2010.5543262</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77956509035&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0071"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20extended%20Cohn-Kanade%20Dataset%20%3A%20A%20complete%20dataset%20for%20action%20unit%20and%20emotion-specified%20expression&amp;publication_year=2010&amp;author=P.%20Lucey&amp;author=J.F.%20Cohn&amp;author=T.%20Kanade&amp;author=J.%20Saragih&amp;author=Z.%20Ambadar&amp;author=I.%20Matthews" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0071"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0072" id="ref-id-bib0072" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[72]</span></span></a></span><span class="reference" id="sbref0072"><div class="contribution"><div class="authors u-font-sans">T. Kanade, J.F. Cohn, Yingli Tian</div><div id="ref-id-sbref0072" class="title text-m">Comprehensive database for facial expression analysis</div></div><div class="host u-font-sans">Proc. Fourth IEEE Int. Conf. Autom. Face Gesture Recognit. Cat No PR00580 (2000), pp. 46-53, <a class="anchor anchor-primary" href="https://doi.org/10.1109/AFGR.2000.840611" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.2000.840611</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84905395782&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0072"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Comprehensive%20database%20for%20facial%20expression%20analysis&amp;publication_year=2000&amp;author=T.%20Kanade&amp;author=J.F.%20Cohn&amp;author=Yingli%20Tian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0072"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0073" id="ref-id-bib0073" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[73]</span></span></a></span><span class="reference" id="sbref0073"><div class="other-ref"><span>M.F. Valstar, M. Pantic, Induced disgust, happiness and surprise: an addition to the MMI facial expression database, (2010) 6.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.F.%20Valstar%2C%20M.%20Pantic%2C%20Induced%20disgust%2C%20happiness%20and%20surprise%3A%20an%20addition%20to%20the%20MMI%20facial%20expression%20database%2C%20(2010)%206." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0074" id="ref-id-bib0074" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[74]</span></span></a></span><span class="reference" id="sbref0074"><div class="contribution"><div class="authors u-font-sans">G. Zhao, X. Huang, M. Taini, S.Z. Li, M. Pietikäinen</div><div id="ref-id-sbref0074" class="title text-m">Facial expression recognition from near-infrared videos</div></div><div class="host u-font-sans">Image Vis. Comput., 29 (2011), pp. 607-619, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.imavis.2011.07.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.imavis.2011.07.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0262885611000515/pdfft?md5=9117a88e2958ec35b47c9caa69169926&amp;pid=1-s2.0-S0262885611000515-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0074"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0262885611000515" aria-describedby="ref-id-sbref0074"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052725518&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0074"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20from%20near-infrared%20videos&amp;publication_year=2011&amp;author=G.%20Zhao&amp;author=X.%20Huang&amp;author=M.%20Taini&amp;author=S.Z.%20Li&amp;author=M.%20Pietik%C3%A4inen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0074"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0075" id="ref-id-bib0075" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[75]</span></span></a></span><span class="reference" id="sbref0075"><div class="contribution"><div class="authors u-font-sans">Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, M.J. Rosato</div><div id="ref-id-sbref0075" class="title text-m">A 3D facial expression database for facial behavior research</div></div><div class="host u-font-sans">7th Int. Conf. Autom. Face Gesture Recognit. FGR06 (2006), pp. 211-216, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FGR.2006.6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FGR.2006.6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33750814056&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0075"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%203D%20facial%20expression%20database%20for%20facial%20behavior%20research&amp;publication_year=2006&amp;author=Lijun%20Yin&amp;author=Xiaozhou%20Wei&amp;author=Yi%20Sun&amp;author=Jun%20Wang&amp;author=M.J.%20Rosato" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0075"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0076" id="ref-id-bib0076" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[76]</span></span></a></span><span class="reference" id="sbref0076"><div class="contribution"><div class="authors u-font-sans">L. Yin, X. Chen, Y. Sun, T. Worm, M. Reale</div><div id="ref-id-sbref0076" class="title text-m">A high-resolution 3D dynamic facial expression database</div></div><div class="host u-font-sans">2008 8th IEEE Int. Conf. Autom. Face Gesture Recognit (2008), pp. 1-6, <a class="anchor anchor-primary" href="https://doi.org/10.1109/AFGR.2008.4813324" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/AFGR.2008.4813324</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-67649087653&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0076"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20high-resolution%203D%20dynamic%20facial%20expression%20database&amp;publication_year=2008&amp;author=L.%20Yin&amp;author=X.%20Chen&amp;author=Y.%20Sun&amp;author=T.%20Worm&amp;author=M.%20Reale" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0076"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0077" id="ref-id-bib0077" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[77]</span></span></a></span><span class="reference" id="sbref0077"><div class="contribution"><div class="authors u-font-sans">X. Zhang, L. Yin, J.F. Cohn, S. Canavan, M. Reale, A. Horowitz, P. Liu, J.M. Girard</div><div id="ref-id-sbref0077" class="title text-m">BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database</div></div><div class="host u-font-sans">Image Vis. Comput., 32 (2014), pp. 692-706, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.imavis.2014.06.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.imavis.2014.06.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0262885614001012/pdfft?md5=37f723c04e21f6a0e870d1082b628c03&amp;pid=1-s2.0-S0262885614001012-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0077"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0262885614001012" aria-describedby="ref-id-sbref0077"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84906782817&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0077"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=BP4D-Spontaneous%3A%20a%20high-resolution%20spontaneous%203D%20dynamic%20facial%20expression%20database&amp;publication_year=2014&amp;author=X.%20Zhang&amp;author=L.%20Yin&amp;author=J.F.%20Cohn&amp;author=S.%20Canavan&amp;author=M.%20Reale&amp;author=A.%20Horowitz&amp;author=P.%20Liu&amp;author=J.M.%20Girard" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0077"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0078" id="ref-id-bib0078" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[78]</span></span></a></span><span class="reference" id="sbref0078"><div class="contribution"><div class="authors u-font-sans">S. Cheng, I. Kotsia, M. Pantic, S. Zafeiriou</div><div id="ref-id-sbref0078" class="title text-m">4DFAB: A large scale 4D database for facial expression analysis and biometric applications</div></div><div class="host u-font-sans">2018 IEEECVF Conf. Comput. Vis. Pattern Recognit., IEEE, Salt Lake City, UT, USA (2018), pp. 5117-5126, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2018.00537" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2018.00537</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85061785301&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0078"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=4DFAB%3A%20A%20large%20scale%204D%20database%20for%20facial%20expression%20analysis%20and%20biometric%20applications&amp;publication_year=2018&amp;author=S.%20Cheng&amp;author=I.%20Kotsia&amp;author=M.%20Pantic&amp;author=S.%20Zafeiriou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0078"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0079" id="ref-id-bib0079" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[79]</span></span></a></span><span class="reference" id="sbref0079"><div class="contribution"><div class="authors u-font-sans">X. Li, T. Pfister, X. Huang, G. Zhao, M. Pietikainen</div><div id="ref-id-sbref0079" class="title text-m">A spontaneous micro-expression database: inducement, collection and baseline</div></div><div class="host u-font-sans">2013 10th IEEE Int. Conf. Workshop Autom. Face Gesture Recognit. FG, IEEE, Shanghai, China (2013), pp. 1-6, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2013.6553717" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2013.6553717</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1381116913002197/pdfft?md5=b462887766948333f23e5c56daefab1c&amp;pid=1-s2.0-S1381116913002197-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0079"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1381116913002197" aria-describedby="ref-id-sbref0079"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20spontaneous%20micro-expression%20database%3A%20inducement%2C%20collection%20and%20baseline&amp;publication_year=2013&amp;author=X.%20Li&amp;author=T.%20Pfister&amp;author=X.%20Huang&amp;author=G.%20Zhao&amp;author=M.%20Pietikainen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0079"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0080" id="ref-id-bib0080" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[80]</span></span></a></span><span class="reference" id="sbref0080"><div class="contribution"><div class="authors u-font-sans">W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, X. Fu, CASME II</div><div id="ref-id-sbref0080" class="title text-m">An improved spontaneous micro-expression database and the baseline evaluation</div></div><div class="host u-font-sans">PLoS ONE, 9 (2014), <a class="anchor anchor-primary" href="https://doi.org/10.1371/journal.pone.0086041" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1371/journal.pone.0086041</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20improved%20spontaneous%20micro-expression%20database%20and%20the%20baseline%20evaluation&amp;publication_year=2014&amp;author=W.-J.%20Yan&amp;author=X.%20Li&amp;author=S.-J.%20Wang&amp;author=G.%20Zhao&amp;author=Y.-J.%20Liu&amp;author=Y.-H.%20Chen&amp;author=X.%20Fu&amp;author=CASME%20II" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0080"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0081" id="ref-id-bib0081" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[81]</span></span></a></span><span class="reference" id="sbref0081"><div class="contribution"><div class="authors u-font-sans">A.K. Davison, C. Lansley, N. Costen, K. Tan, M.H. Yap</div><div id="ref-id-sbref0081" class="title text-m">SAMM: a spontaneous micro-facial movement dataset</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 9 (2018), pp. 116-129, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2016.2573832" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2016.2573832</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85043295290&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0081"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SAMM%3A%20a%20spontaneous%20micro-facial%20movement%20dataset&amp;publication_year=2018&amp;author=A.K.%20Davison&amp;author=C.%20Lansley&amp;author=N.%20Costen&amp;author=K.%20Tan&amp;author=M.H.%20Yap" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0081"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0082" id="ref-id-bib0082" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[82]</span></span></a></span><span class="reference" id="sbref0082"><div class="contribution"><div class="authors u-font-sans">I.J. Goodfellow, D. Erhan, P.L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis, J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra, J. Xie, L. Romaszko, B. Xu, Z. Chuang, Y. Bengio</div><div id="ref-id-sbref0082" class="title text-m">Challenges in representation learning: a report on three machine learning contests</div></div><div class="host u-font-sans">M. Lee, A. Hirose, Z.-G. Hou, R.M. Kil (Eds.), Neural Inf. Process., Springer, Berlin, Heidelberg (2013), pp. 117-124, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-42051-1_16" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-42051-1_16</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84893416950&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0082"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Challenges%20in%20representation%20learning%3A%20a%20report%20on%20three%20machine%20learning%20contests&amp;publication_year=2013&amp;author=I.J.%20Goodfellow&amp;author=D.%20Erhan&amp;author=P.L.%20Carrier&amp;author=A.%20Courville&amp;author=M.%20Mirza&amp;author=B.%20Hamner&amp;author=W.%20Cukierski&amp;author=Y.%20Tang&amp;author=D.%20Thaler&amp;author=D.-H.%20Lee&amp;author=Y.%20Zhou&amp;author=C.%20Ramaiah&amp;author=F.%20Feng&amp;author=R.%20Li&amp;author=X.%20Wang&amp;author=D.%20Athanasakis&amp;author=J.%20Shawe-Taylor&amp;author=M.%20Milakov&amp;author=J.%20Park&amp;author=R.%20Ionescu&amp;author=M.%20Popescu&amp;author=C.%20Grozea&amp;author=J.%20Bergstra&amp;author=J.%20Xie&amp;author=L.%20Romaszko&amp;author=B.%20Xu&amp;author=Z.%20Chuang&amp;author=Y.%20Bengio" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0082"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0083" id="ref-id-bib0083" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[83]</span></span></a></span><span class="reference" id="sbref0083"><div class="contribution"><div class="authors u-font-sans">A. Dhall, R. Goecke, S. Lucey, T. Gedeon</div><div id="ref-id-sbref0083" class="title text-m">Static facial expression analysis in tough conditions: data, evaluation protocol and benchmark</div></div><div class="host u-font-sans">2011 IEEE Int. Conf. Comput. Vis. Workshop ICCV Workshop (2011), pp. 2106-2112, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCVW.2011.6130508" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCVW.2011.6130508</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84856645363&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0083"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Static%20facial%20expression%20analysis%20in%20tough%20conditions%3A%20data%2C%20evaluation%20protocol%20and%20benchmark&amp;publication_year=2011&amp;author=A.%20Dhall&amp;author=R.%20Goecke&amp;author=S.%20Lucey&amp;author=T.%20Gedeon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0083"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0084" id="ref-id-bib0084" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[84]</span></span></a></span><span class="reference" id="sbref0084"><div class="contribution"><div class="authors u-font-sans">C.F. Benitez-Quiroz, R. Srinivasan, A.M. Martinez</div><div id="ref-id-sbref0084" class="title text-m">EmotioNet: an accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</div></div><div class="host u-font-sans">2016 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE, Las Vegas, NV, USA (2016), pp. 5562-5570, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2016.600" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2016.600</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84986320310&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0084"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EmotioNet%3A%20an%20accurate%2C%20real-time%20algorithm%20for%20the%20automatic%20annotation%20of%20a%20million%20facial%20expressions%20in%20the%20wild&amp;publication_year=2016&amp;author=C.F.%20Benitez-Quiroz&amp;author=R.%20Srinivasan&amp;author=A.M.%20Martinez" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0084"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0085" id="ref-id-bib0085" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[85]</span></span></a></span><span class="reference" id="sbref0085"><div class="contribution"><div class="authors u-font-sans">Z. Zhang, P. Luo, C.C. Loy, X. Tang</div><div id="ref-id-sbref0085" class="title text-m">From facial expression recognition to interpersonal relation prediction</div></div><div class="host u-font-sans">Int. J. Comput. Vis., 126 (2018), pp. 550-569, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11263-017-1055-1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11263-017-1055-1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85035130905&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0085"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=From%20facial%20expression%20recognition%20to%20interpersonal%20relation%20prediction&amp;publication_year=2018&amp;author=Z.%20Zhang&amp;author=P.%20Luo&amp;author=C.C.%20Loy&amp;author=X.%20Tang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0085"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0086" id="ref-id-bib0086" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[86]</span></span></a></span><span class="reference" id="sbref0086"><div class="contribution"><div class="authors u-font-sans">A. Mollahosseini, B. Hasani, M.H. Mahoor</div><div id="ref-id-sbref0086" class="title text-m">AffectNet: a database for facial expression, valence, and arousal computing in the wild</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 10 (2019), pp. 18-31, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2740923" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2740923</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85028454548&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0086"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AffectNet%3A%20a%20database%20for%20facial%20expression%2C%20valence%2C%20and%20arousal%20computing%20in%20the%20wild&amp;publication_year=2019&amp;author=A.%20Mollahosseini&amp;author=B.%20Hasani&amp;author=M.H.%20Mahoor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0086"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0087" id="ref-id-bib0087" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[87]</span></span></a></span><span class="reference" id="sbref0087"><div class="contribution"><div class="authors u-font-sans">S. Li, W. Deng, J. Du</div><div id="ref-id-sbref0087" class="title text-m">Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</div></div><div class="host u-font-sans">2017 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE, Honolulu, HI (2017), pp. 2584-2593, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2017.277" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2017.277</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044307466&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0087"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Reliable%20crowdsourcing%20and%20deep%20locality-preserving%20learning%20for%20expression%20recognition%20in%20the%20wild&amp;publication_year=2017&amp;author=S.%20Li&amp;author=W.%20Deng&amp;author=J.%20Du" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0087"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0088" id="ref-id-bib0088" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[88]</span></span></a></span><span class="reference" id="sbref0088"><div class="contribution"><div class="authors u-font-sans">X. Jiang, Y. Zong, W. Zheng, C. Tang, W. Xia, C. Lu, J. Liu</div><div id="ref-id-sbref0088" class="title text-m">DFEW: a large-scale database for recognizing dynamic facial expressions in the wild</div></div><div class="host u-font-sans">Proc. 28th ACM Int. Conf. Multimed., ACM, Seattle WA USA (2020), pp. 2881-2889, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3394171.3413620" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3394171.3413620</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85102559805&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0088"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=DFEW%3A%20a%20large-scale%20database%20for%20recognizing%20dynamic%20facial%20expressions%20in%20the%20wild&amp;publication_year=2020&amp;author=X.%20Jiang&amp;author=Y.%20Zong&amp;author=W.%20Zheng&amp;author=C.%20Tang&amp;author=W.%20Xia&amp;author=C.%20Lu&amp;author=J.%20Liu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0088"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0089" id="ref-id-bib0089" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[89]</span></span></a></span><span class="reference" id="sbref0089"><div class="other-ref"><span>S. Abrilian, L. Devillers, S. Buisine, J.-C. Martin, EmoTV1: Annotation of real-life emotions for the specifications of multimodal a ective interfaces, in: 2005.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Abrilian%2C%20L.%20Devillers%2C%20S.%20Buisine%2C%20J.-C.%20Martin%2C%20EmoTV1%3A%20Annotation%20of%20real-life%20emotions%20for%20the%20specifications%20of%20multimodal%20a%20ective%20interfaces%2C%20in%3A%202005." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0090" id="ref-id-bib0090" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[90]</span></span></a></span><span class="reference" id="sbref0090"><div class="contribution"><div class="authors u-font-sans">H. Gunes, M. Piccardi</div><div id="ref-id-sbref0090" class="title text-m">A bimodal face and body gesture database for automatic analysis of human nonverbal affective behavior</div></div><div class="host u-font-sans">18th Int. Conf. Pattern Recognit. ICPR06 (2006), pp. 1148-1153, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICPR.2006.39" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICPR.2006.39</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34047197554&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0090"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20bimodal%20face%20and%20body%20gesture%20database%20for%20automatic%20analysis%20of%20human%20nonverbal%20affective%20behavior&amp;publication_year=2006&amp;author=H.%20Gunes&amp;author=M.%20Piccardi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0090"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0091" id="ref-id-bib0091" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[91]</span></span></a></span><span class="reference" id="sbref0091"><div class="other-ref"><span>M. Kipp, J.-C. Martin, Gesture and Emotion: Can basic gestural form features discriminate emotions?, 2009. 10.1109/ACII.2009.5349544.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=M.%20Kipp%2C%20J.-C.%20Martin%2C%20Gesture%20and%20Emotion%3A%20Can%20basic%20gestural%20form%20features%20discriminate%20emotions%3F%2C%202009.%2010.1109%2FACII.2009.5349544." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0092" id="ref-id-bib0092" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[92]</span></span></a></span><span class="reference" id="sbref0092"><div class="contribution"><div class="authors u-font-sans">A. Mehrabian</div><div id="ref-id-sbref0092" class="title text-m">Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament</div></div><div class="host u-font-sans">Curr. Psychol., 14 (1996), pp. 261-292, <a class="anchor anchor-primary" href="https://doi.org/10.1007/BF02686918" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/BF02686918</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-21344454051&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0092"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Pleasure-arousal-dominance%3A%20A%20general%20framework%20for%20describing%20and%20measuring%20individual%20differences%20in%20temperament&amp;publication_year=1996&amp;author=A.%20Mehrabian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0092"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0093" id="ref-id-bib0093" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[93]</span></span></a></span><span class="reference" id="sbref0093"><div class="contribution"><div class="authors u-font-sans">T. Bänziger, K. Scherer</div><div id="ref-id-sbref0093" class="title text-m">Introducing the Geneva Multimodal Emotion Portrayal (GEMEP) corpus</div></div><div class="host u-font-sans">Bluepr. Affect. Comput. Sourceb. (2010)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Introducing%20the%20Geneva%20Multimodal%20Emotion%20Portrayal%20%20corpus&amp;publication_year=2010&amp;author=T.%20B%C3%A4nziger&amp;author=K.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0093"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0094" id="ref-id-bib0094" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[94]</span></span></a></span><span class="reference" id="sbref0094"><div class="contribution"><div class="authors u-font-sans">M.F. Valstar, M. Mehu, B. Jiang, M. Pantic, K. Scherer</div><div id="ref-id-sbref0094" class="title text-m">Meta-analysis of the first facial expression recognition challenge</div></div><div class="host u-font-sans">IEEE Trans. Syst. Man Cybern. Part B Cybern., 42 (2012), pp. 966-979, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TSMCB.2012.2200675" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TSMCB.2012.2200675</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84864118983&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0094"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Meta-analysis%20of%20the%20first%20facial%20expression%20recognition%20challenge&amp;publication_year=2012&amp;author=M.F.%20Valstar&amp;author=M.%20Mehu&amp;author=B.%20Jiang&amp;author=M.%20Pantic&amp;author=K.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0094"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0095" id="ref-id-bib0095" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[95]</span></span></a></span><span class="reference" id="sbref0095"><div class="other-ref"><span>N. Fourati, C. Pelachaud, Emilya: emotional body expression in daily actions database, in: Reykjavik, Iceland, 2014.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=N.%20Fourati%2C%20C.%20Pelachaud%2C%20Emilya%3A%20emotional%20body%20expression%20in%20daily%20actions%20database%2C%20in%3A%20Reykjavik%2C%20Iceland%2C%202014." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0096" id="ref-id-bib0096" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[96]</span></span></a></span><span class="reference" id="sbref0096"><div class="contribution"><div class="authors u-font-sans">S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, I. Patras</div><div id="ref-id-sbref0096" class="title text-m">DEAP: a database for emotion analysis; using physiological signals</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 18-31, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.15" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.15</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859916185&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0096"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=DEAP%3A%20a%20database%20for%20emotion%20analysis%3B%20using%20physiological%20signals&amp;publication_year=2012&amp;author=S.%20Koelstra&amp;author=C.%20Muhl&amp;author=M.%20Soleymani&amp;author=J.-S.%20Lee&amp;author=A.%20Yazdani&amp;author=T.%20Ebrahimi&amp;author=T.%20Pun&amp;author=A.%20Nijholt&amp;author=I.%20Patras" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0096"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0097" id="ref-id-bib0097" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[97]</span></span></a></span><span class="reference" id="sbref0097"><div class="contribution"><div class="authors u-font-sans">R.-N. Duan, J.-Y. Zhu, B.-L. Lu</div><div id="ref-id-sbref0097" class="title text-m">Differential entropy feature for EEG-based emotion classification</div></div><div class="host u-font-sans">2013 6th Int. IEEEEMBS Conf. Neural Eng. NER (2013), pp. 81-84, <a class="anchor anchor-primary" href="https://doi.org/10.1109/NER.2013.6695876" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/NER.2013.6695876</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84897741760&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0097"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Differential%20entropy%20feature%20for%20EEG-based%20emotion%20classification&amp;publication_year=2013&amp;author=R.-N.%20Duan&amp;author=J.-Y.%20Zhu&amp;author=B.-L.%20Lu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0097"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0098" id="ref-id-bib0098" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[98]</span></span></a></span><span class="reference" id="sbref0098"><div class="contribution"><div class="authors u-font-sans">W.-L. Zheng, B.-L. Lu</div><div id="ref-id-sbref0098" class="title text-m">Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</div></div><div class="host u-font-sans">IEEE Trans. Auton. Ment. Dev., 7 (2015), pp. 162-175, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAMD.2015.2431497" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAMD.2015.2431497</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84946923238&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0098"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Investigating%20critical%20frequency%20bands%20and%20channels%20for%20eeg-based%20emotion%20recognition%20with%20deep%20neural%20networks&amp;publication_year=2015&amp;author=W.-L.%20Zheng&amp;author=B.-L.%20Lu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0098"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0099" id="ref-id-bib0099" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[99]</span></span></a></span><span class="reference" id="sbref0099"><div class="contribution"><div class="authors u-font-sans">J.A. Miranda Correa, M.K. Abadi, N. Sebe, I. Patras</div><div id="ref-id-sbref0099" class="title text-m">AMIGOS: a dataset for affect, personality and mood research on individuals and groups</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2018), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2018.2884461" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2018.2884461</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AMIGOS%3A%20a%20dataset%20for%20affect%2C%20personality%20and%20mood%20research%20on%20individuals%20and%20groups&amp;publication_year=2018&amp;author=J.A.%20Miranda%20Correa&amp;author=M.K.%20Abadi&amp;author=N.%20Sebe&amp;author=I.%20Patras" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0099"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0100" id="ref-id-bib0100" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[100]</span></span></a></span><span class="reference" id="sbref0100"><div class="contribution"><div class="authors u-font-sans">P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, K. Van Laerhoven</div><div id="ref-id-sbref0100" class="title text-m">Introducing WESAD, a multimodal dataset for wearable stress and affect detection</div></div><div class="host u-font-sans">Proc. 20th ACM Int. Conf. Multimodal Interact., Association for Computing Machinery, Boulder, CO, USA (2018), pp. 400-408, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3242969.3242985" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3242969.3242985</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85056645811&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0100"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Introducing%20WESAD%2C%20a%20multimodal%20dataset%20for%20wearable%20stress%20and%20affect%20detection&amp;publication_year=2018&amp;author=P.%20Schmidt&amp;author=A.%20Reiss&amp;author=R.%20Duerichen&amp;author=C.%20Marberger&amp;author=K.%20Van%20Laerhoven" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0100"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0101" id="ref-id-bib0101" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[101]</span></span></a></span><span class="reference" id="sbref0101"><div class="contribution"><div class="authors u-font-sans">C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J.N. Chang, S. Lee, S.S. Narayanan</div><div id="ref-id-sbref0101" class="title text-m">IEMOCAP: interactive emotional dyadic motion capture database</div></div><div class="host u-font-sans">Lang. Resour. Eval., 42 (2008), p. 335, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10579-008-9076-6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10579-008-9076-6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-59849093076&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0101"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=IEMOCAP%3A%20interactive%20emotional%20dyadic%20motion%20capture%20database&amp;publication_year=2008&amp;author=C.%20Busso&amp;author=M.%20Bulut&amp;author=C.-C.%20Lee&amp;author=A.%20Kazemzadeh&amp;author=E.%20Mower&amp;author=S.%20Kim&amp;author=J.N.%20Chang&amp;author=S.%20Lee&amp;author=S.S.%20Narayanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0101"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0102" id="ref-id-bib0102" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[102]</span></span></a></span><span class="reference" id="sbref0102"><div class="other-ref"><span>A. Metallinou, C.-C. Lee, C. Busso, S. Carnicke, S. Narayanan, The USC CreativeIT database: a multimodal database of theatrical improvisation, (n.d.) 4.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Metallinou%2C%20C.-C.%20Lee%2C%20C.%20Busso%2C%20S.%20Carnicke%2C%20S.%20Narayanan%2C%20The%20USC%20CreativeIT%20database%3A%20a%20multimodal%20database%20of%20theatrical%20improvisation%2C%20(n.d.)%204." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0103" id="ref-id-bib0103" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[103]</span></span></a></span><span class="reference" id="sbref0103"><div class="contribution"><div class="authors u-font-sans">A. Metallinou, Z. Yang, C. Lee, C. Busso, S. Carnicke, S. Narayanan</div><div id="ref-id-sbref0103" class="title text-m">The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations</div></div><div class="host u-font-sans">Lang. Resour. Eval., 50 (2016), pp. 497-521, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10579-015-9300-0" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10579-015-9300-0</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84928138103&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0103"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20USC%20CreativeIT%20database%20of%20multimodal%20dyadic%20interactions%3A%20from%20speech%20and%20full%20body%20motion%20capture%20to%20continuous%20emotional%20annotations&amp;publication_year=2016&amp;author=A.%20Metallinou&amp;author=Z.%20Yang&amp;author=C.%20Lee&amp;author=C.%20Busso&amp;author=S.%20Carnicke&amp;author=S.%20Narayanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0103"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0104" id="ref-id-bib0104" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[104]</span></span></a></span><span class="reference" id="sbref0104"><div class="contribution"><div class="authors u-font-sans">L.-P. Morency, R. Mihalcea, P. Doshi</div><div id="ref-id-sbref0104" class="title text-m">Towards multimodal sentiment analysis: harvesting opinions from the web</div></div><div class="host u-font-sans">Proc. 13th Int. Conf. Multimodal Interfaces, Association for Computing Machinery, Alicante, Spain (2011), pp. 169-176, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2070481.2070509" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2070481.2070509</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-83455176765&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0104"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20multimodal%20sentiment%20analysis%3A%20harvesting%20opinions%20from%20the%20web&amp;publication_year=2011&amp;author=L.-P.%20Morency&amp;author=R.%20Mihalcea&amp;author=P.%20Doshi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0104"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0105" id="ref-id-bib0105" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[105]</span></span></a></span><span class="reference" id="sbref0105"><div class="contribution"><div class="authors u-font-sans">M. Wollmer, F. Weninger, T. Knaup, B. Schuller, C. Sun, K. Sagae, L.-P. Morency</div><div id="ref-id-sbref0105" class="title text-m">YouTube movie reviews: sentiment analysis in an audio-visual context</div></div><div class="host u-font-sans">IEEE Intell. Syst., 28 (2013), pp. 46-53, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MIS.2013.34" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MIS.2013.34</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84884134833&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0105"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=YouTube%20movie%20reviews%3A%20sentiment%20analysis%20in%20an%20audio-visual%20context&amp;publication_year=2013&amp;author=M.%20Wollmer&amp;author=F.%20Weninger&amp;author=T.%20Knaup&amp;author=B.%20Schuller&amp;author=C.%20Sun&amp;author=K.%20Sagae&amp;author=L.-P.%20Morency" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0105"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0106" id="ref-id-bib0106" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[106]</span></span></a></span><span class="reference" id="sbref0106"><div class="contribution"><div class="authors u-font-sans">A. Bagher Zadeh, P.P. Liang, S. Poria, E. Cambria, L.-P. Morency</div><div id="ref-id-sbref0106" class="title text-m">Multimodal language analysis in the wild: CMU-MOSEI Dataset and interpretable dynamic fusion graph</div></div><div class="host u-font-sans">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Melbourne, Australia (2018), pp. 2236-2246, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P18-1208" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P18-1208</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20language%20analysis%20in%20the%20wild%3A%20CMU-MOSEI%20Dataset%20and%20interpretable%20dynamic%20fusion%20graph&amp;publication_year=2018&amp;author=A.%20Bagher%20Zadeh&amp;author=P.P.%20Liang&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=L.-P.%20Morency" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0106"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0107" id="ref-id-bib0107" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[107]</span></span></a></span><span class="reference" id="sbref0107"><div class="contribution"><div class="authors u-font-sans">M. Soleymani, J. Lichtenauer, T. Pun, M. Pantic</div><div id="ref-id-sbref0107" class="title text-m">A Multimodal database for affect recognition and implicit tagging</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 42-55, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.25" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.25</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859911947&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0107"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20Multimodal%20database%20for%20affect%20recognition%20and%20implicit%20tagging&amp;publication_year=2012&amp;author=M.%20Soleymani&amp;author=J.%20Lichtenauer&amp;author=T.%20Pun&amp;author=M.%20Pantic" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0107"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0108" id="ref-id-bib0108" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[108]</span></span></a></span><span class="reference" id="sbref0108"><div class="contribution"><div class="authors u-font-sans">F. Ringeval, A. Sonderegger, J. Sauer, D. Lalanne</div><div id="ref-id-sbref0108" class="title text-m">Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions</div></div><div class="host u-font-sans">2013 10th IEEE Int. Conf. Workshop Autom. Face Gesture Recognit. FG (2013), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2013.6553805" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2013.6553805</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Introducing%20the%20RECOLA%20multimodal%20corpus%20of%20remote%20collaborative%20and%20affective%20interactions&amp;publication_year=2013&amp;author=F.%20Ringeval&amp;author=A.%20Sonderegger&amp;author=J.%20Sauer&amp;author=D.%20Lalanne" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0108"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0109" id="ref-id-bib0109" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[109]</span></span></a></span><span class="reference" id="sbref0109"><div class="contribution"><div class="authors u-font-sans">M.K. Abadi, R. Subramanian, S.M. Kia, P. Avesani, I. Patras, N. Sebe</div><div id="ref-id-sbref0109" class="title text-m">DECAF: MEG-based multimodal database for decoding affective physiological responses</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 6 (2015), pp. 209-222, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2015.2392932" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2015.2392932</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84940987310&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0109"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=DECAF%3A%20MEG-based%20multimodal%20database%20for%20decoding%20affective%20physiological%20responses&amp;publication_year=2015&amp;author=M.K.%20Abadi&amp;author=R.%20Subramanian&amp;author=S.M.%20Kia&amp;author=P.%20Avesani&amp;author=I.%20Patras&amp;author=N.%20Sebe" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0109"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0110" id="ref-id-bib0110" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[110]</span></span></a></span><span class="reference" id="sbref0110"><div class="contribution"><div class="authors u-font-sans">F.A. Pozzi, E. Fersini, E. Messina, B. Liu</div><div id="ref-id-sbref0110" class="title text-m">Chapter 1–challenges of sentiment analysis in social networks: an overview</div></div><div class="host u-font-sans">F.A. Pozzi, E. Fersini, E. Messina, B. Liu (Eds.), Sentiment Analysis in Social Networks, Morgan Kaufmann, Boston (2017), pp. 1-11, <a class="anchor anchor-primary" href="https://doi.org/10.1016/B978-0-12-804412-4.00001-2" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/B978-0-12-804412-4.00001-2</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/B9780128044124000012/pdfft?md5=f20f10095c0423f04290d997f95c17d7&amp;pid=3-s2.0-B9780128044124000012-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0110"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/B9780128044124000012" aria-describedby="ref-id-sbref0110"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85023623657&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0110"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Chapter%201challenges%20of%20sentiment%20analysis%20in%20social%20networks%3A%20an%20overview&amp;publication_year=2017&amp;author=F.A.%20Pozzi&amp;author=E.%20Fersini&amp;author=E.%20Messina&amp;author=B.%20Liu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0110"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0111" id="ref-id-bib0111" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[111]</span></span></a></span><span class="reference" id="sbref0111"><div class="contribution"><div class="authors u-font-sans">P.J. Stone, E.B. Hunt</div><div id="ref-id-sbref0111" class="title text-m">A computer approach to content analysis: studies using the General Inquirer system</div></div><div class="host u-font-sans">Proc. May 21-23 1963 Spring Jt. Comput. Conf., Association for Computing Machinery, New York, NY, USA (1963), pp. 241-256, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1461551.1461583" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1461551.1461583</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84962914820&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0111"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20computer%20approach%20to%20content%20analysis%3A%20studies%20using%20the%20General%20Inquirer%20system&amp;publication_year=1963&amp;author=P.J.%20Stone&amp;author=E.B.%20Hunt" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0111"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0112" id="ref-id-bib0112" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[112]</span></span></a></span><span class="reference" id="sbref0112"><div class="contribution"><div class="authors u-font-sans">B. Pang, L. Lee, S. Vaithyanathan</div><div id="ref-id-sbref0112" class="title text-m">Thumbs up?: sentiment classification using machine learning techniques</div></div><div class="host u-font-sans">Proc. ACL-02 Conf. Empir. Methods Nat. Lang. Process. - EMNLP 02, Association for Computational Linguistics, Not Known (2002), pp. 79-86, <a class="anchor anchor-primary" href="https://doi.org/10.3115/1118693.1118704" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3115/1118693.1118704</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85141803251&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0112"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Thumbs%20up%3A%20sentiment%20classification%20using%20machine%20learning%20techniques&amp;publication_year=2002&amp;author=B.%20Pang&amp;author=L.%20Lee&amp;author=S.%20Vaithyanathan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0112"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0113" id="ref-id-bib0113" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[113]</span></span></a></span><span class="reference" id="sbref0113"><div class="contribution"><div class="authors u-font-sans">L. Oneto, F. Bisio, E. Cambria, D. Anguita</div><div id="ref-id-sbref0113" class="title text-m">Statistical Learning theory and ELM for big social data analysis</div></div><div class="host u-font-sans">IEEE Comput. Intell. Mag., 11 (2016), pp. 45-55, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MCI.2016.2572540" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MCI.2016.2572540</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84979680823&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0113"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Statistical%20Learning%20theory%20and%20ELM%20for%20big%20social%20data%20analysis&amp;publication_year=2016&amp;author=L.%20Oneto&amp;author=F.%20Bisio&amp;author=E.%20Cambria&amp;author=D.%20Anguita" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0113"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0114" id="ref-id-bib0114" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[114]</span></span></a></span><span class="reference" id="sbref0114"><div class="contribution"><div class="authors u-font-sans">M. Taboada, J. Brooke, M. Tofiloski, K. Voll, M. Stede</div><div id="ref-id-sbref0114" class="title text-m">Lexicon-based methods for sentiment analysis</div></div><div class="host u-font-sans">Comput. Linguist., 37 (2011), pp. 267-307, <a class="anchor anchor-primary" href="https://doi.org/10.1162/COLI_a_00049" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1162/COLI_a_00049</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79958257877&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0114"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Lexicon-based%20methods%20for%20sentiment%20analysis&amp;publication_year=2011&amp;author=M.%20Taboada&amp;author=J.%20Brooke&amp;author=M.%20Tofiloski&amp;author=K.%20Voll&amp;author=M.%20Stede" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0114"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0115" id="ref-id-bib0115" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[115]</span></span></a></span><span class="reference" id="sbref0115"><div class="contribution"><div class="authors u-font-sans">X. Ding, B. Liu, P.S. Yu</div><div id="ref-id-sbref0115" class="title text-m">A holistic lexicon-based approach to opinion mining</div></div><div class="host u-font-sans">Proc. Int. Conf. Web Search Web Data Min. - WSDM 08, ACM Press, Palo Alto, California, USA (2008), p. 231, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1341531.1341561" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1341531.1341561</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-42549170653&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0115"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20holistic%20lexicon-based%20approach%20to%20opinion%20mining&amp;publication_year=2008&amp;author=X.%20Ding&amp;author=B.%20Liu&amp;author=P.S.%20Yu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0115"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0116" id="ref-id-bib0116" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[116]</span></span></a></span><span class="reference" id="sbref0116"><div class="contribution"><div class="authors u-font-sans">P. Melville, W. Gryc, R.D. Lawrence</div><div id="ref-id-sbref0116" class="title text-m">Sentiment analysis of blogs by combining lexical knowledge with text classification</div></div><div class="host u-font-sans">Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. - KDD 09, ACM Press, Paris, France (2009), p. 1275, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1557019.1557156" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1557019.1557156</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70350645448&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0116"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20analysis%20of%20blogs%20by%20combining%20lexical%20knowledge%20with%20text%20classification&amp;publication_year=2009&amp;author=P.%20Melville&amp;author=W.%20Gryc&amp;author=R.D.%20Lawrence" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0116"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0117" id="ref-id-bib0117" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[117]</span></span></a></span><span class="reference" id="sbref0117"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, G. Winterstein, G.-B. Huang</div><div id="ref-id-sbref0117" class="title text-m">Sentic patterns: dependency-based rules for concept-level sentiment analysis</div></div><div class="host u-font-sans">Knowl. Based Syst, 69 (2014), pp. 45-63, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.knosys.2014.05.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.knosys.2014.05.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S095070511400183X/pdfft?md5=b5c265e8ad6f2c5968e54992bae05b88&amp;pid=1-s2.0-S095070511400183X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0117"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S095070511400183X" aria-describedby="ref-id-sbref0117"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84924582748&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0117"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentic%20patterns%3A%20dependency-based%20rules%20for%20concept-level%20sentiment%20analysis&amp;publication_year=2014&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=G.%20Winterstein&amp;author=G.-B.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0117"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0118" id="ref-id-bib0118" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[118]</span></span></a></span><span class="reference" id="sbref0118"><div class="contribution"><div class="authors u-font-sans">E. Cambria, A. Hussain, C. Havasi, C. Eckl</div><div id="ref-id-sbref0118" class="title text-m">Common Sense computing: from the society of mind to digital intuition and beyond</div></div><div class="host u-font-sans">J. Fierrez, J. Ortega-Garcia, A. Esposito, A. Drygajlo, M. Faundez-Zanuy (Eds.), Biometric ID Management and Multimodal Communication, Springer Berlin Heidelberg, Berlin, Heidelberg (2009), pp. 252-259, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-04391-8_33" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-04391-8_33</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77952049760&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0118"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Common%20Sense%20computing%3A%20from%20the%20society%20of%20mind%20to%20digital%20intuition%20and%20beyond&amp;publication_year=2009&amp;author=E.%20Cambria&amp;author=A.%20Hussain&amp;author=C.%20Havasi&amp;author=C.%20Eckl" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0118"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0119" id="ref-id-bib0119" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[119]</span></span></a></span><span class="reference" id="sbref0119"><div class="contribution"><div class="authors u-font-sans">L. Jia, C. Yu, W. Meng</div><div id="ref-id-sbref0119" class="title text-m">The effect of negation on sentiment analysis and retrieval effectiveness</div></div><div class="host u-font-sans">Proceeding 18th ACM Conf. Inf. Knowl. Manag. - CIKM 09, ACM Press, Hong Kong, China (2009), p. 1827, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1645953.1646241" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1645953.1646241</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-74549126616&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0119"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20effect%20of%20negation%20on%20sentiment%20analysis%20and%20retrieval%20effectiveness&amp;publication_year=2009&amp;author=L.%20Jia&amp;author=C.%20Yu&amp;author=W.%20Meng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0119"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0120" id="ref-id-bib0120" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[120]</span></span></a></span><span class="reference" id="sbref0120"><div class="contribution"><div class="authors u-font-sans">I. Blekanov, M. Kukarkin, A. Maksimov, S. Bodrunova</div><div id="ref-id-sbref0120" class="title text-m">Sentiment analysis for Ad Hoc discussions using multilingual knowledge-based approach</div></div><div class="host u-font-sans">Proc. 3rd Int. Conf. Appl. Inf. Technol. - ICAIT2018, ACM Press, Aizu-Wakamatsu, Japan (2018), pp. 117-121, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3274856.3274880" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3274856.3274880</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85058614911&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0120"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20analysis%20for%20Ad%20Hoc%20discussions%20using%20multilingual%20knowledge-based%20approach&amp;publication_year=2018&amp;author=I.%20Blekanov&amp;author=M.%20Kukarkin&amp;author=A.%20Maksimov&amp;author=S.%20Bodrunova" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0120"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0121" id="ref-id-bib0121" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[121]</span></span></a></span><span class="reference" id="sbref0121"><div class="contribution"><div class="authors u-font-sans">Ebru Aydogan, M.Ali Akcayol</div><div id="ref-id-sbref0121" class="title text-m">A comprehensive survey for sentiment analysis tasks using machine learning techniques</div></div><div class="host u-font-sans">Int. Symp. Innov. Intell. Syst. Appl. (2016), <a class="anchor anchor-primary" href="https://doi.org/10.1109/INISTA.2016.7571856" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/INISTA.2016.7571856</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20comprehensive%20survey%20for%20sentiment%20analysis%20tasks%20using%20machine%20learning%20techniques&amp;publication_year=2016&amp;author=Ebru%20Aydogan&amp;author=M.Ali%20Akcayol" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0121"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0122" id="ref-id-bib0122" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[122]</span></span></a></span><span class="reference" id="sbref0122"><div class="contribution"><div class="authors u-font-sans">M. Ahmad, S. Aftab, S. Muhammad, S. Ahmad</div><div id="ref-id-sbref0122" class="title text-m">Machine learning techniques for sentiment analysis: a review</div></div><div class="host u-font-sans">Int. J. Multidiscip. Sci. Eng., 8 (2017), pp. 2045-7057</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85035099595&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0122"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Machine%20learning%20techniques%20for%20sentiment%20analysis%3A%20a%20review&amp;publication_year=2017&amp;author=M.%20Ahmad&amp;author=S.%20Aftab&amp;author=S.%20Muhammad&amp;author=S.%20Ahmad" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0122"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0123" id="ref-id-bib0123" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[123]</span></span></a></span><span class="reference" id="sbref0123"><div class="contribution"><div class="authors u-font-sans">T. Mullen, N. Collier</div><div id="ref-id-sbref0123" class="title text-m">Sentiment analysis using support vector machines with diverse information sources</div></div><div class="host u-font-sans">Proc. 2004 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Barcelona, Spain (2004), pp. 412-418</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/W04-3253" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/W04-3253</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed October 29, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85112593657&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0123"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20analysis%20using%20support%20vector%20machines%20with%20diverse%20information%20sources&amp;publication_year=2004&amp;author=T.%20Mullen&amp;author=N.%20Collier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0123"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0124" id="ref-id-bib0124" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[124]</span></span></a></span><span class="reference" id="sbref0124"><div class="contribution"><div class="authors u-font-sans">A. Pak, P. Paroubek</div><div id="ref-id-sbref0124" class="title text-m">Text representation using dependency tree subgraphs for sentiment analysis</div></div><div class="host u-font-sans">J. Xu, G. Yu, S. Zhou, R. Unland (Eds.), Database Systems for Adanced Applications, Springer, Berlin, Heidelberg (2011), pp. 323-332, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-20244-5_31" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-20244-5_31</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84906989348&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0124"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Text%20representation%20using%20dependency%20tree%20subgraphs%20for%20sentiment%20analysis&amp;publication_year=2011&amp;author=A.%20Pak&amp;author=P.%20Paroubek" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0124"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0125" id="ref-id-bib0125" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[125]</span></span></a></span><span class="reference" id="sbref0125"><div class="contribution"><div class="authors u-font-sans">J. Chen, H. Huang, S. Tian, Y. Qu</div><div id="ref-id-sbref0125" class="title text-m">Feature selection for text classification with Naïve Bayes</div></div><div class="host u-font-sans">Expert Syst. Appl., 36 (2009), pp. 5432-5435, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2008.06.054" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2008.06.054</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417408003564/pdfft?md5=45edcd36eda595177eda49e1228b4373&amp;pid=1-s2.0-S0957417408003564-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0125"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417408003564" aria-describedby="ref-id-sbref0125"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-58349094507&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0125"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20selection%20for%20text%20classification%20with%20Na%C3%AFve%20Bayes&amp;publication_year=2009&amp;author=J.%20Chen&amp;author=H.%20Huang&amp;author=S.%20Tian&amp;author=Y.%20Qu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0125"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0126" id="ref-id-bib0126" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[126]</span></span></a></span><span class="reference" id="sbref0126"><div class="contribution"><div class="authors u-font-sans">R.S. Jagdale, V.S. Shirsat, S.N. Deshmukh</div><div id="ref-id-sbref0126" class="title text-m">Sentiment analysis on product reviews using machine learning techniques</div></div><div class="host u-font-sans">P.K. Mallick, V.E. Balas, A.K. Bhoi, A.F. Zobaa (Eds.), Cognitive Informatics and Soft Computing, Springer, Singapore (2019), pp. 639-647, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-13-0617-4_61" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-13-0617-4_61</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85052199665&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0126"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20analysis%20on%20product%20reviews%20using%20machine%20learning%20techniques&amp;publication_year=2019&amp;author=R.S.%20Jagdale&amp;author=V.S.%20Shirsat&amp;author=S.N.%20Deshmukh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0126"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0127" id="ref-id-bib0127" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[127]</span></span></a></span><span class="reference" id="sbref0127"><div class="contribution"><div class="authors u-font-sans">Y. Xia, E. Cambria, A. Hussain, H. Zhao</div><div id="ref-id-sbref0127" class="title text-m">Word polarity disambiguation using bayesian model and opinion-level features</div></div><div class="host u-font-sans">Cogn. Comput., 7 (2015), pp. 369-380, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12559-014-9298-4" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12559-014-9298-4</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84929702394&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0127"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Word%20polarity%20disambiguation%20using%20bayesian%20model%20and%20opinion-level%20features&amp;publication_year=2015&amp;author=Y.%20Xia&amp;author=E.%20Cambria&amp;author=A.%20Hussain&amp;author=H.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0127"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0128" id="ref-id-bib0128" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[128]</span></span></a></span><span class="reference" id="sbref0128"><div class="contribution"><div class="authors u-font-sans">A. Valdivia, M.V. Luzón, E. Cambria, F. Herrera</div><div id="ref-id-sbref0128" class="title text-m">Consensus vote models for detecting and filtering neutrality in sentiment analysis</div></div><div class="host u-font-sans">Inf. Fusion., 44 (2018), pp. 126-135, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2018.03.007" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2018.03.007</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253517306590/pdfft?md5=de2d74a694418351150821ca23e4864f&amp;pid=1-s2.0-S1566253517306590-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0128"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253517306590" aria-describedby="ref-id-sbref0128"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044926430&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0128"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Consensus%20vote%20models%20for%20detecting%20and%20filtering%20neutrality%20in%20sentiment%20analysis&amp;publication_year=2018&amp;author=A.%20Valdivia&amp;author=M.V.%20Luz%C3%B3n&amp;author=E.%20Cambria&amp;author=F.%20Herrera" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0128"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0129" id="ref-id-bib0129" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[129]</span></span></a></span><span class="reference" id="sbref0129"><div class="contribution"><div class="authors u-font-sans">T. Le</div><div id="ref-id-sbref0129" class="title text-m">A hybrid method for text-based sentiment analysis</div></div><div class="host u-font-sans">2019 Int. Conf. Comput. Sci. Comput. Intell. CSCI (2019), pp. 1392-1397, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CSCI49370.2019.00260" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CSCI49370.2019.00260</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20hybrid%20method%20for%20text-based%20sentiment%20analysis&amp;publication_year=2019&amp;author=T.%20Le" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0129"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0130" id="ref-id-bib0130" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[130]</span></span></a></span><span class="reference" id="sbref0130"><div class="other-ref"><span>D. Li, R. Rzepka, M. Ptaszynski, K. Araki, A novel machine learning-based sentiment analysis method for chinese social media considering chinese slang lexicon and emoticons, in: Honolulu, Hawaii, USA, 2019.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=D.%20Li%2C%20R.%20Rzepka%2C%20M.%20Ptaszynski%2C%20K.%20Araki%2C%20A%20novel%20machine%20learning-based%20sentiment%20analysis%20method%20for%20chinese%20social%20media%20considering%20chinese%20slang%20lexicon%20and%20emoticons%2C%20in%3A%20Honolulu%2C%20Hawaii%2C%20USA%2C%202019." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0131" id="ref-id-bib0131" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[131]</span></span></a></span><span class="reference" id="sbref0131"><div class="other-ref"><span>T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, S. Khudanpur, Recurrent neural network based language model, in: 2010: pp. 1045–1048.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=T.%20Mikolov%2C%20M.%20Karafi%C3%A1t%2C%20L.%20Burget%2C%20J.%20Cernock%C3%BD%2C%20S.%20Khudanpur%2C%20Recurrent%20neural%20network%20based%20language%20model%2C%20in%3A%202010%3A%20pp.%201045%E2%80%931048." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0132" id="ref-id-bib0132" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[132]</span></span></a></span><span class="reference" id="sbref0132"><div class="contribution"><div class="authors u-font-sans">A. Khatua, A. Khatua, E. Cambria</div><div id="ref-id-sbref0132" class="title text-m">Predicting political sentiments of voters from Twitter in multi-party contexts</div></div><div class="host u-font-sans">Appl. Soft Comput., 97 (2020), Article 106743, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.asoc.2020.106743" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.asoc.2020.106743</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494620306815/pdfft?md5=7e5c3ba47b87682b1ed7cdac5fb62720&amp;pid=1-s2.0-S1568494620306815-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0132"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1568494620306815" aria-describedby="ref-id-sbref0132"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85093692222&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0132"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Predicting%20political%20sentiments%20of%20voters%20from%20Twitter%20in%20multi-party%20contexts&amp;publication_year=2020&amp;author=A.%20Khatua&amp;author=A.%20Khatua&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0132"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0133" id="ref-id-bib0133" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[133]</span></span></a></span><span class="reference" id="sbref0133"><div class="contribution"><div class="authors u-font-sans">F. Liu, L. Zheng, J. Zheng</div><div id="ref-id-sbref0133" class="title text-m">HieNN-DWE: A hierarchical neural network with dynamic word embeddings for document level sentiment classification</div></div><div class="host u-font-sans">Neurocomputing, 403 (2020), pp. 21-32, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.04.084" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2020.04.084</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S092523122030669X/pdfft?md5=a846b9e13c5cc6a71f1e63c6139c3f3b&amp;pid=1-s2.0-S092523122030669X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0133"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S092523122030669X" aria-describedby="ref-id-sbref0133"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85084331835&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0133"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=HieNN-DWE%3A%20A%20hierarchical%20neural%20network%20with%20dynamic%20word%20embeddings%20for%20document%20level%20sentiment%20classification&amp;publication_year=2020&amp;author=F.%20Liu&amp;author=L.%20Zheng&amp;author=J.%20Zheng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0133"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0134" id="ref-id-bib0134" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[134]</span></span></a></span><span class="reference" id="sbref0134"><div class="contribution"><div class="authors u-font-sans">Y. Kim</div></div><div class="host u-font-sans" id="ref-id-sbref0134">Convolutional Neural Networks for Sentence Classification, in: Proc. 2014 Conf. Empir. Methods Nat. Lang. Process. EMNLP, Association for Computational Linguistics, Doha, Qatar (2014), pp. 1746-1751, <a class="anchor anchor-primary" href="https://doi.org/10.3115/v1/D14-1181" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3115/v1/D14-1181</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84961376850&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0134"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Convolutional%20Neural%20Networks%20for%20Sentence%20Classification%2C%20in%3A%20Proc.%202014%20Conf.%20Empir.%20Methods%20Nat.%20Lang.%20Process.%20EMNLP%2C%20Association%20for%20Computational%20Linguistics%2C%20Doha%2C%20Qatar&amp;publication_year=2014&amp;author=Y.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0134"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0135" id="ref-id-bib0135" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[135]</span></span></a></span><span class="reference" id="sbref0135"><div class="contribution"><div class="authors u-font-sans">H.H. Do, P. Prasad, A. Maag, A. Alsadoon</div><div id="ref-id-sbref0135" class="title text-m">Deep learning for aspect-based sentiment analysis: a comparative review</div></div><div class="host u-font-sans">Expert Syst. Appl., 118 (2019), pp. 272-299, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2018.10.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2018.10.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417418306456/pdfft?md5=37c2a862d19bdfcb66145c8ee46e24f1&amp;pid=1-s2.0-S0957417418306456-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0135"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417418306456" aria-describedby="ref-id-sbref0135"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054712386&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0135"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20aspect-based%20sentiment%20analysis%3A%20a%20comparative%20review&amp;publication_year=2019&amp;author=H.H.%20Do&amp;author=P.%20Prasad&amp;author=A.%20Maag&amp;author=A.%20Alsadoon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0135"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0136" id="ref-id-bib0136" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[136]</span></span></a></span><span class="reference" id="sbref0136"><div class="contribution"><div class="authors u-font-sans">R. Yin, P. Li, B. Wang</div><div id="ref-id-sbref0136" class="title text-m">Sentiment lexical-augmented convolutional neural networks for sentiment analysis</div></div><div class="host u-font-sans">2017 IEEE Second Int. Conf. Data Sci. Cyberspace DSC (2017), pp. 630-635, <a class="anchor anchor-primary" href="https://doi.org/10.1109/DSC.2017.82" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/DSC.2017.82</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85034211509&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0136"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20lexical-augmented%20convolutional%20neural%20networks%20for%20sentiment%20analysis&amp;publication_year=2017&amp;author=R.%20Yin&amp;author=P.%20Li&amp;author=B.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0136"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0137" id="ref-id-bib0137" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[137]</span></span></a></span><span class="reference" id="sbref0137"><div class="contribution"><div class="authors u-font-sans">A. Conneau, H. Schwenk, L. Barrault, Y. Lecun</div><div id="ref-id-sbref0137" class="title text-m">Very deep convolutional networks for text classification</div></div><div class="host u-font-sans">Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Valencia, Spain (2017), pp. 1107-1116</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/E17-1104" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/E17-1104</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed August 13, 2020)</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.18653/v1/E17-1104" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0137"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021673250&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0137"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Very%20deep%20convolutional%20networks%20for%20text%20classification&amp;publication_year=2017&amp;author=A.%20Conneau&amp;author=H.%20Schwenk&amp;author=L.%20Barrault&amp;author=Y.%20Lecun" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0137"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0138" id="ref-id-bib0138" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[138]</span></span></a></span><span class="reference" id="sbref0138"><div class="contribution"><div class="authors u-font-sans">R. Johnson, T. Zhang</div><div id="ref-id-sbref0138" class="title text-m">Deep pyramid convolutional neural networks for text categorization</div></div><div class="host u-font-sans">Proc. 55th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Vancouver, Canada (2017), pp. 562-570, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P17-1052" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P17-1052</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85040914735&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0138"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20pyramid%20convolutional%20neural%20networks%20for%20text%20categorization&amp;publication_year=2017&amp;author=R.%20Johnson&amp;author=T.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0138"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0139" id="ref-id-bib0139" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[139]</span></span></a></span><span class="reference" id="sbref0139"><div class="contribution"><div class="authors u-font-sans">B. Huang, K. Carley</div><div id="ref-id-sbref0139" class="title text-m">Parameterized convolutional neural networks for aspect level sentiment classification</div></div><div class="host u-font-sans">Proc. 2018 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Brussels, Belgium (2018), pp. 1091-1096, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D18-1136" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D18-1136</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85067201874&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0139"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Parameterized%20convolutional%20neural%20networks%20for%20aspect%20level%20sentiment%20classification&amp;publication_year=2018&amp;author=B.%20Huang&amp;author=K.%20Carley" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0139"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0140" id="ref-id-bib0140" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[140]</span></span></a></span><span class="reference" id="sbref0140"><div class="contribution"><div class="authors u-font-sans">A. Mousa, B. Schuller</div><div id="ref-id-sbref0140" class="title text-m">Contextual bidirectional long short-term memory recurrent neural network language models: a generative approach to sentiment analysis</div></div><div class="host u-font-sans">Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Valencia, Spain (2017), pp. 1023-1032</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/E17-1096" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/E17-1096</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed August 15, 2020)</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.18653/v1/E17-1096" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0140"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85021656130&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0140"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Contextual%20bidirectional%20long%20short-term%20memory%20recurrent%20neural%20network%20language%20models%3A%20a%20generative%20approach%20to%20sentiment%20analysis&amp;publication_year=2017&amp;author=A.%20Mousa&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0140"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0141" id="ref-id-bib0141" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[141]</span></span></a></span><span class="reference" id="sbref0141"><div class="contribution"><div class="authors u-font-sans">W. Wang, S.J. Pan, D. Dahlmeier, X. Xiao</div><div id="ref-id-sbref0141" class="title text-m">Recursive neural conditional random fields for aspect-based sentiment analysis</div></div><div class="host u-font-sans">Proc. 2016 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Austin, Texas (2016), pp. 616-626, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D16-1059" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D16-1059</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85072831937&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0141"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recursive%20neural%20conditional%20random%20fields%20for%20aspect-based%20sentiment%20analysis&amp;publication_year=2016&amp;author=W.%20Wang&amp;author=S.J.%20Pan&amp;author=D.%20Dahlmeier&amp;author=X.%20Xiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0141"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0142" id="ref-id-bib0142" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[142]</span></span></a></span><span class="reference" id="sbref0142"><div class="contribution"><div class="authors u-font-sans">P. Chen, Z. Sun, L. Bing, W. Yang</div><div id="ref-id-sbref0142" class="title text-m">Recurrent attention network on memory for aspect sentiment analysis</div></div><div class="host u-font-sans">Proc. 2017 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Copenhagen, Denmark (2017), pp. 452-461, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D17-1047" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D17-1047</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85073152063&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0142"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recurrent%20attention%20network%20on%20memory%20for%20aspect%20sentiment%20analysis&amp;publication_year=2017&amp;author=P.%20Chen&amp;author=Z.%20Sun&amp;author=L.%20Bing&amp;author=W.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0142"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0143" id="ref-id-bib0143" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[143]</span></span></a></span><span class="reference" id="sbref0143"><div class="other-ref"><span>A. Mishra, S. Tamilselvam, R. Dasgupta, S. Nagar, K. Dey, Cognition-cognizant sentiment analysis with multitask subjectivity summarization based on Annotators’ Gaze behavior, in: 2018: p. 8.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Mishra%2C%20S.%20Tamilselvam%2C%20R.%20Dasgupta%2C%20S.%20Nagar%2C%20K.%20Dey%2C%20Cognition-cognizant%20sentiment%20analysis%20with%20multitask%20subjectivity%20summarization%20based%20on%20Annotators%E2%80%99%20Gaze%20behavior%2C%20in%3A%202018%3A%20p.%208." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0144" id="ref-id-bib0144" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[144]</span></span></a></span><span class="reference" id="sbref0144"><div class="contribution"><div class="authors u-font-sans">H. Chen, M. Sun, C. Tu, Y. Lin, Z. Liu</div><div id="ref-id-sbref0144" class="title text-m">Neural Sentiment classification with user and product attention</div></div><div class="host u-font-sans">Proc. 2016 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Austin, Texas (2016), pp. 1650-1659, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D16-1171" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D16-1171</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85045912242&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0144"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Neural%20Sentiment%20classification%20with%20user%20and%20product%20attention&amp;publication_year=2016&amp;author=H.%20Chen&amp;author=M.%20Sun&amp;author=C.%20Tu&amp;author=Y.%20Lin&amp;author=Z.%20Liu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0144"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0145" id="ref-id-bib0145" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[145]</span></span></a></span><span class="reference" id="sbref0145"><div class="contribution"><div class="authors u-font-sans">Z.-Y. Dou</div><div id="ref-id-sbref0145" class="title text-m">Capturing user and product information for document level sentiment analysis with deep memory network</div></div><div class="host u-font-sans">Proc. 2017 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Copenhagen, Denmark (2017), pp. 521-526, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D17-1054" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D17-1054</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048779173&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0145"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Capturing%20user%20and%20product%20information%20for%20document%20level%20sentiment%20analysis%20with%20deep%20memory%20network&amp;publication_year=2017&amp;author=Z.-Y.%20Dou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0145"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0146" id="ref-id-bib0146" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[146]</span></span></a></span><span class="reference" id="sbref0146"><div class="contribution"><div class="authors u-font-sans">Z. Wu, X.-Y. Dai, C. Yin, S. Huang, J. Chen</div><div id="ref-id-sbref0146" class="title text-m">Improving review representations with user attention and product attention for sentiment classification</div></div><div class="host u-font-sans">Thirty-Second AAAI Conf. Artif. Intell. AAAI-18 (2018), pp. 5989-5996</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060439296&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0146"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Improving%20review%20representations%20with%20user%20attention%20and%20product%20attention%20for%20sentiment%20classification&amp;publication_year=2018&amp;author=Z.%20Wu&amp;author=X.-Y.%20Dai&amp;author=C.%20Yin&amp;author=S.%20Huang&amp;author=J.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0146"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0147" id="ref-id-bib0147" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[147]</span></span></a></span><span class="reference" id="sbref0147"><div class="contribution"><div class="authors u-font-sans">A. Kumar J, T.E. Trueman, E. Cambria</div><div id="ref-id-sbref0147" class="title text-m">A convolutional stacked bidirectional LSTM with a multiplicative attention mechanism for aspect category and sentiment detection</div></div><div class="host u-font-sans">Cogn. Comput. (2021), <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12559-021-09948-0" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12559-021-09948-0</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20convolutional%20stacked%20bidirectional%20LSTM%20with%20a%20multiplicative%20attention%20mechanism%20for%20aspect%20category%20and%20sentiment%20detection&amp;publication_year=2021&amp;author=A.%20Kumar%20J&amp;author=T.E.%20Trueman&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0147"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0148" id="ref-id-bib0148" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[148]</span></span></a></span><span class="reference" id="sbref0148"><div class="contribution"><div class="authors u-font-sans">B. Liang, H. Su, L. Gui, E. Cambria, R. Xu</div><div id="ref-id-sbref0148" class="title text-m">Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks</div></div><div class="host u-font-sans">Knowl. Based Syst, 235 (2022), Article 107643, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.knosys.2021.107643" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.knosys.2021.107643</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0950705121009059/pdfft?md5=fe6373ad42300bc66d6785cbe447cd2d&amp;pid=1-s2.0-S0950705121009059-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0148"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0950705121009059" aria-describedby="ref-id-sbref0148"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85118591463&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0148"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Aspect-based%20sentiment%20analysis%20via%20affective%20knowledge%20enhanced%20graph%20convolutional%20networks&amp;publication_year=2022&amp;author=B.%20Liang&amp;author=H.%20Su&amp;author=L.%20Gui&amp;author=E.%20Cambria&amp;author=R.%20Xu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0148"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0149" id="ref-id-bib0149" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[149]</span></span></a></span><span class="reference" id="sbref0149"><div class="contribution"><div class="authors u-font-sans">X. Li, L. Bing, W. Lam, B. Shi</div><div id="ref-id-sbref0149" class="title text-m">Transformation networks for target-oriented sentiment classification</div></div><div class="host u-font-sans">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Melbourne, Australia (2018), pp. 946-956, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P18-1087" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P18-1087</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85059628326&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0149"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Transformation%20networks%20for%20target-oriented%20sentiment%20classification&amp;publication_year=2018&amp;author=X.%20Li&amp;author=L.%20Bing&amp;author=W.%20Lam&amp;author=B.%20Shi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0149"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0150" id="ref-id-bib0150" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[150]</span></span></a></span><span class="reference" id="sbref0150"><div class="contribution"><div class="authors u-font-sans">W. Li, L. Zhu, Y. Shi, K. Guo, E. Cambria</div><div id="ref-id-sbref0150" class="title text-m">User reviews: sentiment analysis using lexicon integrated two-channel CNN–LSTM​ family models</div></div><div class="host u-font-sans">Appl. Soft Comput., 94 (2020), Article 106435, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.asoc.2020.106435" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.asoc.2020.106435</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494620303756/pdfft?md5=b9b03464804bb4e069819fd7e9504ebf&amp;pid=1-s2.0-S1568494620303756-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0150"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1568494620303756" aria-describedby="ref-id-sbref0150"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85086477447&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0150"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=User%20reviews%3A%20sentiment%20analysis%20using%20lexicon%20integrated%20two-channel%20CNNLSTM%20family%20models&amp;publication_year=2020&amp;author=W.%20Li&amp;author=L.%20Zhu&amp;author=Y.%20Shi&amp;author=K.%20Guo&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0150"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0151" id="ref-id-bib0151" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[151]</span></span></a></span><span class="reference" id="sbref0151"><div class="contribution"><div class="authors u-font-sans">W. Xue, T. Li</div><div id="ref-id-sbref0151" class="title text-m">Aspect based sentiment analysis with gated convolutional networks</div></div><div class="host u-font-sans">Proc. 56th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Melbourne, Australia (2018), pp. 2514-2523, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P18-1234" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P18-1234</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85059103778&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0151"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Aspect%20based%20sentiment%20analysis%20with%20gated%20convolutional%20networks&amp;publication_year=2018&amp;author=W.%20Xue&amp;author=T.%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0151"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0152" id="ref-id-bib0152" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[152]</span></span></a></span><span class="reference" id="sbref0152"><div class="contribution"><div class="authors u-font-sans">M.E. Basiri, S. Nemati, M. Abdar, E. Cambria, U.R. Acharya</div><div id="ref-id-sbref0152" class="title text-m">ABCDM: an attention-based bidirectional CNN-RNN deep model for sentiment analysis</div></div><div class="host u-font-sans">Future Gener. Comput. Syst., 115 (2021), pp. 279-294, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.future.2020.08.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.future.2020.08.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167739X20309195/pdfft?md5=0c93ad11a37a5984cd1f8f405444deee&amp;pid=1-s2.0-S0167739X20309195-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0152"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167739X20309195" aria-describedby="ref-id-sbref0152"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85091237677&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0152"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=ABCDM%3A%20an%20attention-based%20bidirectional%20CNN-RNN%20deep%20model%20for%20sentiment%20analysis&amp;publication_year=2021&amp;author=M.E.%20Basiri&amp;author=S.%20Nemati&amp;author=M.%20Abdar&amp;author=E.%20Cambria&amp;author=U.R.%20Acharya" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0152"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0153" id="ref-id-bib0153" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[153]</span></span></a></span><span class="reference" id="sbref0153"><div class="contribution"><div class="authors u-font-sans">M.S. Akhtar, A. Ekbal, E. Cambria</div><div id="ref-id-sbref0153" class="title text-m">How intense are you? Predicting intensities of emotions and sentiments using stacked ensemble</div></div><div class="host u-font-sans">IEEE Comput. Intell. Mag., 15 (2020), pp. 64-75, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MCI.2019.2954667" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MCI.2019.2954667</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85078338836&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0153"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=How%20intense%20are%20you%20Predicting%20intensities%20of%20emotions%20and%20sentiments%20using%20stacked%20ensemble&amp;publication_year=2020&amp;author=M.S.%20Akhtar&amp;author=A.%20Ekbal&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0153"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0154" id="ref-id-bib0154" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[154]</span></span></a></span><span class="reference" id="sbref0154"><div class="contribution"><div class="authors u-font-sans">T. Miyato, A.M. Dai, I. Goodfellow</div><div id="ref-id-sbref0154" class="title text-m">Adversarial training methods for semi-supervised text classification</div></div><div class="host u-font-sans">Int. Conf. Learn. Represent (2017)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adversarial%20training%20methods%20for%20semi-supervised%20text%20classification&amp;publication_year=2017&amp;author=T.%20Miyato&amp;author=A.M.%20Dai&amp;author=I.%20Goodfellow" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0154"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0155" id="ref-id-bib0155" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[155]</span></span></a></span><span class="reference" id="sbref0155"><div class="contribution"><div class="authors u-font-sans">Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, V. Lempitsky</div><div id="ref-id-sbref0155" class="title text-m">Domain-Adversarial training of neural networks</div></div><div class="host u-font-sans">J. Mach. Learn. Res., 17 (2016), pp. 1-35</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Domain-Adversarial%20training%20of%20neural%20networks&amp;publication_year=2016&amp;author=Y.%20Ganin&amp;author=E.%20Ustinova&amp;author=H.%20Ajakan&amp;author=P.%20Germain&amp;author=H.%20Larochelle&amp;author=F.%20Laviolette&amp;author=M.%20March&amp;author=V.%20Lempitsky" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0155"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0156" id="ref-id-bib0156" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[156]</span></span></a></span><span class="reference" id="sbref0156"><div class="other-ref"><span>Q. Yang, Z. Li, Y. Zhang, Y. Wei, Y. Wu, End-to-End Adversarial memory network for cross-domain sentiment classification, in: IJCAI 2017, 2017: pp. 2237–2243. <a class="anchor anchor-primary" href="https://www.ijcai.org/Proceedings/2017/311" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.ijcai.org/Proceedings/2017/311</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a> (accessed August 14, 2020).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=Q.%20Yang%2C%20Z.%20Li%2C%20Y.%20Zhang%2C%20Y.%20Wei%2C%20Y.%20Wu%2C%20End-to-End%20Adversarial%20memory%20network%20for%20cross-domain%20sentiment%20classification%2C%20in%3A%20IJCAI%202017%2C%202017%3A%20pp.%202237%E2%80%932243.%20https%3A%2F%2Fwww.ijcai.org%2FProceedings%2F2017%2F311%20(accessed%20August%2014%2C%202020)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0157" id="ref-id-bib0157" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[157]</span></span></a></span><span class="reference" id="sbref0157"><div class="contribution"><div class="authors u-font-sans">Y. Li, Q. Pan, S. Wang, T. Yang, E. Cambria</div><div id="ref-id-sbref0157" class="title text-m">A generative model for category text generation</div></div><div class="host u-font-sans">Inf. Sci., 450 (2018), pp. 301-315, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ins.2018.03.050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ins.2018.03.050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0020025518302366/pdfft?md5=321a88332f279f1d9d3bfb2d5747cf6a&amp;pid=1-s2.0-S0020025518302366-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0157"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0020025518302366" aria-describedby="ref-id-sbref0157"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044670076&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0157"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20generative%20model%20for%20category%20text%20generation&amp;publication_year=2018&amp;author=Y.%20Li&amp;author=Q.%20Pan&amp;author=S.%20Wang&amp;author=T.%20Yang&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0157"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0158" id="ref-id-bib0158" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[158]</span></span></a></span><span class="reference" id="sbref0158"><div class="contribution"><div class="authors u-font-sans">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio</div><div id="ref-id-sbref0158" class="title text-m">Generative adversarial nets</div></div><div class="host u-font-sans">Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, K.Q. Weinberger (Eds.), Advances in Neural Information Processing Systems, Curran Associates, Inc. (2014), pp. 2672-2680</div><div class="comment">27</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed August 15, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Generative%20adversarial%20nets&amp;publication_year=2014&amp;author=I.%20Goodfellow&amp;author=J.%20Pouget-Abadie&amp;author=M.%20Mirza&amp;author=B.%20Xu&amp;author=D.%20Warde-Farley&amp;author=S.%20Ozair&amp;author=A.%20Courville&amp;author=Y.%20Bengio" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0158"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0159" id="ref-id-bib0159" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[159]</span></span></a></span><span class="reference" id="sbref0159"><div class="contribution"><div class="authors u-font-sans">X. Chen, Y. Sun, B. Athiwaratkun, C. Cardie, K. Weinberger</div><div id="ref-id-sbref0159" class="title text-m">Adversarial deep averaging networks for cross-lingual sentiment classification</div></div><div class="host u-font-sans">Trans. Assoc. Comput. Linguist., 6 (2018), pp. 557-570, <a class="anchor anchor-primary" href="https://doi.org/10.1162/tacl_a_00039" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1162/tacl_a_00039</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adversarial%20deep%20averaging%20networks%20for%20cross-lingual%20sentiment%20classification&amp;publication_year=2018&amp;author=X.%20Chen&amp;author=Y.%20Sun&amp;author=B.%20Athiwaratkun&amp;author=C.%20Cardie&amp;author=K.%20Weinberger" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0159"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0160" id="ref-id-bib0160" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[160]</span></span></a></span><span class="reference" id="sbref0160"><div class="contribution"><div class="authors u-font-sans">A. Karimi, L. Rossi, A. Prati</div><div id="ref-id-sbref0160" class="title text-m">Adversarial training for aspect-based sentiment analysis with BERT</div></div><div class="host u-font-sans">2020 25th Int. Conf. Pattern Recognit. ICPR, IEEE, Milan, Italy (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICPR48806.2021.9412167" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICPR48806.2021.9412167</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adversarial%20training%20for%20aspect-based%20sentiment%20analysis%20with%20BERT&amp;publication_year=2020&amp;author=A.%20Karimi&amp;author=L.%20Rossi&amp;author=A.%20Prati" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0160"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0161" id="ref-id-bib0161" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[161]</span></span></a></span><span class="reference" id="sbref0161"><div class="contribution"><div class="authors u-font-sans">Chul Min Lee, S.S. Narayanan</div><div id="ref-id-sbref0161" class="title text-m">Toward detecting emotions in spoken dialogs</div></div><div class="host u-font-sans">IEEE Trans. Speech Audio Process., 13 (2005), pp. 293-303, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TSA.2004.838534" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TSA.2004.838534</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-14644439843&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0161"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Toward%20detecting%20emotions%20in%20spoken%20dialogs&amp;publication_year=2005&amp;author=Chul%20Min%20Lee&amp;author=S.S.%20Narayanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0161"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0162" id="ref-id-bib0162" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[162]</span></span></a></span><span class="reference" id="sbref0162"><div class="contribution"><div class="authors u-font-sans">J. Pohjalainen, F. Fabien Ringeval, Z. Zhang, B. Schuller</div><div id="ref-id-sbref0162" class="title text-m">Spectral and cepstral audio noise reduction techniques in speech emotion recognition</div></div><div class="host u-font-sans">in: Proc. 2016 ACM Multimed. Conf. - MM 16, ACM Press, Amsterdam, The Netherlands (2016), pp. 670-674, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2964284.2967306" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2964284.2967306</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84994652671&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0162"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spectral%20and%20cepstral%20audio%20noise%20reduction%20techniques%20in%20speech%20emotion%20recognition&amp;publication_year=2016&amp;author=J.%20Pohjalainen&amp;author=F.%20Fabien%20Ringeval&amp;author=Z.%20Zhang&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0162"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0163" id="ref-id-bib0163" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[163]</span></span></a></span><span class="reference" id="sbref0163"><div class="contribution"><div class="authors u-font-sans">Z. Huang, M. Dong, Q. Mao, Y. Zhan</div><div id="ref-id-sbref0163" class="title text-m">Speech emotion recognition using CNN</div></div><div class="host u-font-sans">Proc. ACM Int. Conf. Multimed. - MM 14, ACM Press, Orlando, Florida, USA (2014), pp. 801-804, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2647868.2654984" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2647868.2654984</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84913537962&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0163"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20CNN&amp;publication_year=2014&amp;author=Z.%20Huang&amp;author=M.%20Dong&amp;author=Q.%20Mao&amp;author=Y.%20Zhan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0163"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0164" id="ref-id-bib0164" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[164]</span></span></a></span><span class="reference" id="sbref0164"><div class="contribution"><div class="authors u-font-sans">H.M. Fayek, M. Lech, L. Cavedon</div><div id="ref-id-sbref0164" class="title text-m">Evaluating deep learning architectures for speech emotion recognition</div></div><div class="host u-font-sans">Neural Netw., 92 (2017), pp. 60-68, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neunet.2017.02.013" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neunet.2017.02.013</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S089360801730059X/pdfft?md5=35ee95cb731c8f4659ee6ded1b0e1912&amp;pid=1-s2.0-S089360801730059X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0164"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S089360801730059X" aria-describedby="ref-id-sbref0164"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85017190163&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0164"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Evaluating%20deep%20learning%20architectures%20for%20speech%20emotion%20recognition&amp;publication_year=2017&amp;author=H.M.%20Fayek&amp;author=M.%20Lech&amp;author=L.%20Cavedon" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0164"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0165" id="ref-id-bib0165" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[165]</span></span></a></span><span class="reference" id="sbref0165"><div class="contribution"><div class="authors u-font-sans">M.B. Akçay, K. Oğuz</div><div id="ref-id-sbref0165" class="title text-m">Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers</div></div><div class="host u-font-sans">Speech Commun., 116 (2020), pp. 56-76, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.specom.2019.12.001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.specom.2019.12.001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167639319302262/pdfft?md5=ead5d293d5bb064eb8a2623c761fbb5a&amp;pid=1-s2.0-S0167639319302262-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0165"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167639319302262" aria-describedby="ref-id-sbref0165"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85076679231&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0165"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%3A%20Emotional%20models%2C%20databases%2C%20features%2C%20preprocessing%20methods%2C%20supporting%20modalities%2C%20and%20classifiers&amp;publication_year=2020&amp;author=M.B.%20Ak%C3%A7ay&amp;author=K.%20O%C4%9Fuz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0165"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0166" id="ref-id-bib0166" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[166]</span></span></a></span><span class="reference" id="sbref0166"><div class="contribution"><div class="authors u-font-sans">L.A. Low, N.C. Maddage, M. Lech, L.B. Sheeber, N.B. Allen</div><div id="ref-id-sbref0166" class="title text-m">Detection of clinical depression in adolescents’ speech during family interactions</div></div><div class="host u-font-sans">IEEE Trans. Biomed. Eng., 58 (2011), pp. 574-586, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TBME.2010.2091640" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TBME.2010.2091640</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79952151964&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0166"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Detection%20of%20clinical%20depression%20in%20adolescents%20speech%20during%20family%20interactions&amp;publication_year=2011&amp;author=L.A.%20Low&amp;author=N.C.%20Maddage&amp;author=M.%20Lech&amp;author=L.B.%20Sheeber&amp;author=N.B.%20Allen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0166"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0167" id="ref-id-bib0167" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[167]</span></span></a></span><span class="reference" id="sbref0167"><div class="contribution"><div class="authors u-font-sans">F. Eyben, M. Wollmer, B. Schuller</div><div id="ref-id-sbref0167" class="title text-m">OpenEAR — Introducing the munich open-source emotion and affect recognition toolkit</div></div><div class="host u-font-sans">2009 3rd Int. Conf. Affect. Comput. Intell. Interact. Workshop, IEEE, Amsterdam (2009), pp. 1-6, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACII.2009.5349350" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACII.2009.5349350</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=OpenEAR%20%20Introducing%20the%20munich%20open-source%20emotion%20and%20affect%20recognition%20toolkit&amp;publication_year=2009&amp;author=F.%20Eyben&amp;author=M.%20Wollmer&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0167"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0168" id="ref-id-bib0168" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[168]</span></span></a></span><span class="reference" id="sbref0168"><div class="contribution"><div class="authors u-font-sans">S. Ntalampiras, N. Fakotakis</div><div id="ref-id-sbref0168" class="title text-m">Modeling the temporal evolution of acoustic parameters for speech emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 116-125, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.31" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.31</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84859913289&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0168"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Modeling%20the%20temporal%20evolution%20of%20acoustic%20parameters%20for%20speech%20emotion%20recognition&amp;publication_year=2012&amp;author=S.%20Ntalampiras&amp;author=N.%20Fakotakis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0168"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0169" id="ref-id-bib0169" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[169]</span></span></a></span><span class="reference" id="sbref0169"><div class="contribution"><div class="authors u-font-sans">L. Zhang, M. Song, N. Li, J. Bu, C. Chen</div><div id="ref-id-sbref0169" class="title text-m">Feature selection for fast speech emotion recognition</div></div><div class="host u-font-sans">Proc. 17th ACM Int. Conf. Multimed., Association for Computing Machinery, New York, NY, USA (2009), pp. 753-756, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1631272.1631405" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1631272.1631405</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20selection%20for%20fast%20speech%20emotion%20recognition&amp;publication_year=2009&amp;author=L.%20Zhang&amp;author=M.%20Song&amp;author=N.%20Li&amp;author=J.%20Bu&amp;author=C.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0169"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0170" id="ref-id-bib0170" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[170]</span></span></a></span><span class="reference" id="sbref0170"><div class="contribution"><div class="authors u-font-sans">D. Li, Y. Zhou, Z. Wang, D. Gao</div><div id="ref-id-sbref0170" class="title text-m">Exploiting the potentialities of features for speech emotion recognition</div></div><div class="host u-font-sans">Inf. Sci. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ins.2020.09.047" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ins.2020.09.047</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Exploiting%20the%20potentialities%20of%20features%20for%20speech%20emotion%20recognition&amp;publication_year=2020&amp;author=D.%20Li&amp;author=Y.%20Zhou&amp;author=Z.%20Wang&amp;author=D.%20Gao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0170"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0171" id="ref-id-bib0171" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[171]</span></span></a></span><span class="reference" id="sbref0171"><div class="contribution"><div class="authors u-font-sans">C. Busso, S. Lee, S. Narayanan</div><div id="ref-id-sbref0171" class="title text-m">Analysis of emotionally salient aspects of fundamental frequency for emotion detection</div></div><div class="host u-font-sans">IEEE Trans. Audio Speech Lang. Process., 17 (2009), pp. 582-596, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TASL.2008.2009578" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TASL.2008.2009578</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-65249116503&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0171"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Analysis%20of%20emotionally%20salient%20aspects%20of%20fundamental%20frequency%20for%20emotion%20detection&amp;publication_year=2009&amp;author=C.%20Busso&amp;author=S.%20Lee&amp;author=S.%20Narayanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0171"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0172" id="ref-id-bib0172" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[172]</span></span></a></span><span class="reference" id="sbref0172"><div class="contribution"><div class="authors u-font-sans">M. Lugger, B. Yang</div><div id="ref-id-sbref0172" class="title text-m">The relevance of voice quality features in speaker independent emotion recognition</div></div><div class="host u-font-sans">2007 IEEE Int. Conf. Acoust. Speech Signal Process. - ICASSP 07 (2007), pp. IV.17-IV.20, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2007.367152" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2007.367152</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20relevance%20of%20voice%20quality%20features%20in%20speaker%20independent%20emotion%20recognition&amp;publication_year=2007&amp;author=M.%20Lugger&amp;author=B.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0172"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0173" id="ref-id-bib0173" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[173]</span></span></a></span><span class="reference" id="sbref0173"><div class="contribution"><div class="authors u-font-sans">M.S. Likitha, S.R.R. Gupta, K. Hasitha, A.U. Raju</div><div id="ref-id-sbref0173" class="title text-m">Speech based human emotion recognition using MFCC</div></div><div class="host u-font-sans">2017 Int. Conf. Wirel. Commun. Signal Process. Netw. WiSPNET (2017), pp. 2257-2260, <a class="anchor anchor-primary" href="https://doi.org/10.1109/WiSPNET.2017.8300161" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/WiSPNET.2017.8300161</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85046377004&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0173"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20based%20human%20emotion%20recognition%20using%20MFCC&amp;publication_year=2017&amp;author=M.S.%20Likitha&amp;author=S.R.R.%20Gupta&amp;author=K.%20Hasitha&amp;author=A.U.%20Raju" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0173"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0174" id="ref-id-bib0174" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[174]</span></span></a></span><span class="reference" id="sbref0174"><div class="contribution"><div class="authors u-font-sans">D. Bitouk, R. Verma, A. Nenkova</div><div id="ref-id-sbref0174" class="title text-m">Class-level spectral features for emotion recognition</div></div><div class="host u-font-sans">Speech Commun., 52 (2010), pp. 613-625, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.specom.2010.02.010" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.specom.2010.02.010</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167639310000348/pdfft?md5=f339c2a8730caffa1a76dd0816eb30ab&amp;pid=1-s2.0-S0167639310000348-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0174"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167639310000348" aria-describedby="ref-id-sbref0174"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77956401353&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0174"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Class-level%20spectral%20features%20for%20emotion%20recognition&amp;publication_year=2010&amp;author=D.%20Bitouk&amp;author=R.%20Verma&amp;author=A.%20Nenkova" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0174"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0175" id="ref-id-bib0175" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[175]</span></span></a></span><span class="reference" id="sbref0175"><div class="contribution"><div class="authors u-font-sans">P. Shen, Z. Changjun, X. Chen</div><div id="ref-id-sbref0175" class="title text-m">Automatic speech emotion recognition using support vector machine</div></div><div class="host u-font-sans">Proc. 2011 Int. Conf. Electron. Mech. Eng. Inf. Technol. (2011), pp. 621-625, <a class="anchor anchor-primary" href="https://doi.org/10.1109/EMEIT.2011.6023178" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/EMEIT.2011.6023178</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80053389742&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0175"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20speech%20emotion%20recognition%20using%20support%20vector%20machine&amp;publication_year=2011&amp;author=P.%20Shen&amp;author=Z.%20Changjun&amp;author=X.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0175"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0176" id="ref-id-bib0176" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[176]</span></span></a></span><span class="reference" id="sbref0176"><div class="contribution"><div class="authors u-font-sans">Y. Jin, P. Song, W. Zheng, L. Zhao</div><div id="ref-id-sbref0176" class="title text-m">A feature selection and feature fusion combination method for speaker-independent speech emotion recognition</div></div><div class="host u-font-sans">2014 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2014), pp. 4808-4812, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2014.6854515" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2014.6854515</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84905226801&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0176"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20feature%20selection%20and%20feature%20fusion%20combination%20method%20for%20speaker-independent%20speech%20emotion%20recognition&amp;publication_year=2014&amp;author=Y.%20Jin&amp;author=P.%20Song&amp;author=W.%20Zheng&amp;author=L.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0176"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0177" id="ref-id-bib0177" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[177]</span></span></a></span><span class="reference" id="sbref0177"><div class="contribution"><div class="authors u-font-sans">H. Atassi, A. Esposito</div><div id="ref-id-sbref0177" class="title text-m">A speaker independent approach to the classification of emotional vocal expressions</div></div><div class="host u-font-sans">2008 20th IEEE Int. Conf. Tools Artif. Intell., IEEE, Dayton, OH, USA (2008), pp. 147-152, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICTAI.2008.158" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICTAI.2008.158</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-57649195265&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0177"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20speaker%20independent%20approach%20to%20the%20classification%20of%20emotional%20vocal%20expressions&amp;publication_year=2008&amp;author=H.%20Atassi&amp;author=A.%20Esposito" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0177"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0178" id="ref-id-bib0178" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[178]</span></span></a></span><span class="reference" id="sbref0178"><div class="contribution"><div class="authors u-font-sans">K. Wang, N. An, B.N. Li, Y. Zhang, L. Li</div><div id="ref-id-sbref0178" class="title text-m">Speech emotion recognition using fourier parameters</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 6 (2015), pp. 69-75, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2015.2392101" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2015.2392101</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84924081025&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0178"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20fourier%20parameters&amp;publication_year=2015&amp;author=K.%20Wang&amp;author=N.%20An&amp;author=B.N.%20Li&amp;author=Y.%20Zhang&amp;author=L.%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0178"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0179" id="ref-id-bib0179" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[179]</span></span></a></span><span class="reference" id="sbref0179"><div class="contribution"><div class="authors u-font-sans">T.L. Nwe, S.W. Foo, L.C.D. Silva</div><div id="ref-id-sbref0179" class="title text-m">Detection of stress and emotion in speech using traditional and FFT based log energy features</div></div><div class="host u-font-sans">Fourth Int. Conf. Inf. Commun. Signal Process. 2003 Fourth Pac. Rim Conf. Multimed. Proc. 2003 Jt. (2003), pp. 1619-1623, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICICS.2003.1292741" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICICS.2003.1292741</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">vol.3</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Detection%20of%20stress%20and%20emotion%20in%20speech%20using%20traditional%20and%20FFT%20based%20log%20energy%20features&amp;publication_year=2003&amp;author=T.L.%20Nwe&amp;author=S.W.%20Foo&amp;author=L.C.D.%20Silva" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0179"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0180" id="ref-id-bib0180" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[180]</span></span></a></span><span class="reference" id="sbref0180"><div class="contribution"><div class="authors u-font-sans">E. Navas, I. Hernaez, Iker Luengo</div><div id="ref-id-sbref0180" class="title text-m">An objective and subjective study of the role of semantics and prosodic features in building corpora for emotional TTS</div></div><div class="host u-font-sans">IEEE Trans. Audio Speech Lang. Process., 14 (2006), pp. 1117-1127, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TASL.2006.876121" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TASL.2006.876121</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34047248387&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0180"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20objective%20and%20subjective%20study%20of%20the%20role%20of%20semantics%20and%20prosodic%20features%20in%20building%20corpora%20for%20emotional%20TTS&amp;publication_year=2006&amp;author=E.%20Navas&amp;author=I.%20Hernaez&amp;author=Iker%20Luengo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0180"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0181" id="ref-id-bib0181" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[181]</span></span></a></span><span class="reference" id="sbref0181"><div class="contribution"><div class="authors u-font-sans">A. Milton, S. Sharmy Roy, S. Tamil Selvi</div><div id="ref-id-sbref0181" class="title text-m">SVM scheme for speech emotion recognition using MFCC feature</div></div><div class="host u-font-sans">Int. J. Comput. Appl., 69 (2013), pp. 34-39, <a class="anchor anchor-primary" href="https://doi.org/10.5120/11872-7667" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.5120/11872-7667</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SVM%20scheme%20for%20speech%20emotion%20recognition%20using%20MFCC%20feature&amp;publication_year=2013&amp;author=A.%20Milton&amp;author=S.%20Sharmy%20Roy&amp;author=S.%20Tamil%20Selvi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0181"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0182" id="ref-id-bib0182" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[182]</span></span></a></span><span class="reference" id="sbref0182"><div class="contribution"><div class="authors u-font-sans">Y. Pan, P. Shen, L. Shen</div><div id="ref-id-sbref0182" class="title text-m">Speech emotion recognition using support vector machine</div></div><div class="host u-font-sans">Int. J. Smart Home., 6 (2012), pp. 101-108</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84864372670&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0182"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20support%20vector%20machine&amp;publication_year=2012&amp;author=Y.%20Pan&amp;author=P.%20Shen&amp;author=L.%20Shen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0182"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0183" id="ref-id-bib0183" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[183]</span></span></a></span><span class="reference" id="sbref0183"><div class="contribution"><div class="authors u-font-sans">T. Seehapoch, S. Wongthanavasu</div><div id="ref-id-sbref0183" class="title text-m">Speech emotion recognition using support vector machines</div></div><div class="host u-font-sans">2013 5th Int. Conf. Knowl. Smart Technol. KST (2013), pp. 86-91, <a class="anchor anchor-primary" href="https://doi.org/10.1109/KST.2013.6512793" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/KST.2013.6512793</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85097597741&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0183"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20support%20vector%20machines&amp;publication_year=2013&amp;author=T.%20Seehapoch&amp;author=S.%20Wongthanavasu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0183"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0184" id="ref-id-bib0184" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[184]</span></span></a></span><span class="reference" id="sbref0184"><div class="contribution"><div class="authors u-font-sans">E. Yüncü, H. Hacihabiboglu, C. Bozsahin</div><div id="ref-id-sbref0184" class="title text-m">Automatic speech emotion recognition using auditory models with binary decision tree and SVM</div></div><div class="host u-font-sans">2014 22nd Int. Conf. Pattern Recognit (2014), pp. 773-778, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICPR.2014.143" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICPR.2014.143</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84919897641&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0184"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20speech%20emotion%20recognition%20using%20auditory%20models%20with%20binary%20decision%20tree%20and%20SVM&amp;publication_year=2014&amp;author=E.%20Y%C3%BCnc%C3%BC&amp;author=H.%20Hacihabiboglu&amp;author=C.%20Bozsahin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0184"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0185" id="ref-id-bib0185" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[185]</span></span></a></span><span class="reference" id="sbref0185"><div class="contribution"><div class="authors u-font-sans">A. Bhavan, P. Chauhan, R.R.Shah Hitkul</div><div id="ref-id-sbref0185" class="title text-m">Bagged support vector machines for emotion recognition from speech</div></div><div class="host u-font-sans">Knowl. Based Syst, 184 (2019), Article 104886, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.knosys.2019.104886" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.knosys.2019.104886</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0950705119303533/pdfft?md5=b1d78caa4ad61b1a45fd080c72f18b99&amp;pid=1-s2.0-S0950705119303533-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0185"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0950705119303533" aria-describedby="ref-id-sbref0185"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85070357666&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0185"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Bagged%20support%20vector%20machines%20for%20emotion%20recognition%20from%20speech&amp;publication_year=2019&amp;author=A.%20Bhavan&amp;author=P.%20Chauhan&amp;author=R.R.Shah%20Hitkul" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0185"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0186" id="ref-id-bib0186" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[186]</span></span></a></span><span class="reference" id="sbref0186"><div class="contribution"><div class="authors u-font-sans">L. Chen, W. Su, Y. Feng, M. Wu, J. She, K. Hirota</div><div id="ref-id-sbref0186" class="title text-m">Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction</div></div><div class="host u-font-sans">Inf. Sci., 509 (2020), pp. 150-163, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ins.2019.09.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ins.2019.09.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0020025519308503/pdfft?md5=d95cb2a13cd0ff6dd1cb8273c9c8a61d&amp;pid=1-s2.0-S0020025519308503-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0186"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0020025519308503" aria-describedby="ref-id-sbref0186"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85071968961&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0186"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Two-layer%20fuzzy%20multiple%20random%20forest%20for%20speech%20emotion%20recognition%20in%20human-robot%20interaction&amp;publication_year=2020&amp;author=L.%20Chen&amp;author=W.%20Su&amp;author=Y.%20Feng&amp;author=M.%20Wu&amp;author=J.%20She&amp;author=K.%20Hirota" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0186"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0187" id="ref-id-bib0187" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[187]</span></span></a></span><span class="reference" id="sbref0187"><div class="contribution"><div class="authors u-font-sans">Q. Mao, M. Dong, Z. Huang, Y. Zhan</div><div id="ref-id-sbref0187" class="title text-m">Learning salient features for speech emotion recognition using convolutional neural networks</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 16 (2014), pp. 2203-2213, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2014.2360798" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2014.2360798</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84913548678&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0187"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20salient%20features%20for%20speech%20emotion%20recognition%20using%20convolutional%20neural%20networks&amp;publication_year=2014&amp;author=Q.%20Mao&amp;author=M.%20Dong&amp;author=Z.%20Huang&amp;author=Y.%20Zhan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0187"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0188" id="ref-id-bib0188" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[188]</span></span></a></span><span class="reference" id="sbref0188"><div class="other-ref"><span>J. Lee, I. Tashev, High-level feature representation using recurrent neural network for speech emotion recognition, in: Dresden, Germany, 2015: pp. 1537–1540.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=J.%20Lee%2C%20I.%20Tashev%2C%20High-level%20feature%20representation%20using%20recurrent%20neural%20network%20for%20speech%20emotion%20recognition%2C%20in%3A%20Dresden%2C%20Germany%2C%202015%3A%20pp.%201537%E2%80%931540." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0189" id="ref-id-bib0189" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[189]</span></span></a></span><span class="reference" id="sbref0189"><div class="contribution"><div class="authors u-font-sans">F. Eyben, M. Wöllmer, A. Graves, B. Schuller, E. Douglas-Cowie, R. Cowie</div><div id="ref-id-sbref0189" class="title text-m">On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues</div></div><div class="host u-font-sans">J. Multimodal User Interfaces., 3 (2010), pp. 7-19, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12193-009-0032-6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12193-009-0032-6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-77949304464&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0189"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=On-line%20emotion%20recognition%20in%20a%203-D%20activation-valence-time%20continuum%20using%20acoustic%20and%20linguistic%20cues&amp;publication_year=2010&amp;author=F.%20Eyben&amp;author=M.%20W%C3%B6llmer&amp;author=A.%20Graves&amp;author=B.%20Schuller&amp;author=E.%20Douglas-Cowie&amp;author=R.%20Cowie" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0189"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0190" id="ref-id-bib0190" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[190]</span></span></a></span><span class="reference" id="sbref0190"><div class="contribution"><div class="authors u-font-sans">B.T. Atmaja, M. Akagi</div><div id="ref-id-sbref0190" class="title text-m">Speech emotion recognition based on speech segment using LSTM with attention model</div></div><div class="host u-font-sans">2019 IEEE Int. Conf. Signals Syst. ICSigSys (2019), pp. 40-44, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICSIGSYS.2019.8811080" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICSIGSYS.2019.8811080</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85072521981&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0190"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20based%20on%20speech%20segment%20using%20LSTM%20with%20attention%20model&amp;publication_year=2019&amp;author=B.T.%20Atmaja&amp;author=M.%20Akagi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0190"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0191" id="ref-id-bib0191" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[191]</span></span></a></span><span class="reference" id="sbref0191"><div class="contribution"><div class="authors u-font-sans">M. Neumann, N.T. Vu</div><div id="ref-id-sbref0191" class="title text-m">Improving speech emotion recognition with unsupervised representation learning on unlabeled speech</div></div><div class="host u-font-sans">ICASSP 2019 - 2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2019), pp. 7390-7394, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2019.8682541" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2019.8682541</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068955597&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0191"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Improving%20speech%20emotion%20recognition%20with%20unsupervised%20representation%20learning%20on%20unlabeled%20speech&amp;publication_year=2019&amp;author=M.%20Neumann&amp;author=N.T.%20Vu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0191"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0192" id="ref-id-bib0192" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[192]</span></span></a></span><span class="reference" id="sbref0192"><div class="contribution"><div class="authors u-font-sans">M. Abdelwahab, C. Busso</div><div id="ref-id-sbref0192" class="title text-m">Domain adversarial for acoustic emotion recognition</div></div><div class="host u-font-sans">IEEEACM Trans. Audio Speech Lang. Process., 26 (2018), pp. 2423-2435, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TASLP.2018.2867099" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TASLP.2018.2867099</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85052674588&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0192"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Domain%20adversarial%20for%20acoustic%20emotion%20recognition&amp;publication_year=2018&amp;author=M.%20Abdelwahab&amp;author=C.%20Busso" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0192"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0193" id="ref-id-bib0193" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[193]</span></span></a></span><span class="reference" id="sbref0193"><div class="contribution"><div class="authors u-font-sans">A.M. Badshah, J. Ahmad, N. Rahim, S.W. Baik</div><div id="ref-id-sbref0193" class="title text-m">Speech emotion recognition from spectrograms with deep convolutional neural network</div></div><div class="host u-font-sans">2017 Int. Conf. Platf. Technol. Serv. PlatCon (2017), pp. 1-5, <a class="anchor anchor-primary" href="https://doi.org/10.1109/PlatCon.2017.7883728" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/PlatCon.2017.7883728</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20from%20spectrograms%20with%20deep%20convolutional%20neural%20network&amp;publication_year=2017&amp;author=A.M.%20Badshah&amp;author=J.%20Ahmad&amp;author=N.%20Rahim&amp;author=S.W.%20Baik" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0193"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0194" id="ref-id-bib0194" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[194]</span></span></a></span><span class="reference" id="sbref0194"><div class="contribution"><div class="authors u-font-sans">S. Zhang, S. Zhang, T. Huang, W. Gao</div><div id="ref-id-sbref0194" class="title text-m">Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 20 (2018), pp. 1576-1590, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2017.2766843" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2017.2766843</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85032447590&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0194"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20deep%20convolutional%20neural%20network%20and%20discriminant%20temporal%20pyramid%20matching&amp;publication_year=2018&amp;author=S.%20Zhang&amp;author=S.%20Zhang&amp;author=T.%20Huang&amp;author=W.%20Gao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0194"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0195" id="ref-id-bib0195" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[195]</span></span></a></span><span class="reference" id="sbref0195"><div class="contribution"><div class="authors u-font-sans">D. Bertero, F.B. Siddique, C.-S. Wu, Y. Wan, R.H.Y. Chan, P. Fung</div><div id="ref-id-sbref0195" class="title text-m">Real-time speech emotion and sentiment recognition for interactive dialogue systems</div></div><div class="host u-font-sans">Proc. 2016 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Austin, Texas (2016), pp. 1042-1047, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D16-1110" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D16-1110</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85072837881&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0195"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Real-time%20speech%20emotion%20and%20sentiment%20recognition%20for%20interactive%20dialogue%20systems&amp;publication_year=2016&amp;author=D.%20Bertero&amp;author=F.B.%20Siddique&amp;author=C.-S.%20Wu&amp;author=Y.%20Wan&amp;author=R.H.Y.%20Chan&amp;author=P.%20Fung" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0195"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0196" id="ref-id-bib0196" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[196]</span></span></a></span><span class="reference" id="sbref0196"><div class="contribution"><div class="authors u-font-sans">G.-B. Huang, Q.-Y. Zhu, C.-K. Siew</div><div id="ref-id-sbref0196" class="title text-m">Extreme learning machine: theory and applications</div></div><div class="host u-font-sans">Neurocomputing, 70 (2006), pp. 489-501, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2005.12.126" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2005.12.126</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231206000385/pdfft?md5=1f19a6f0b45d08c0a7c9888723e421a1&amp;pid=1-s2.0-S0925231206000385-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0196"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231206000385" aria-describedby="ref-id-sbref0196"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33745903481&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0196"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Extreme%20learning%20machine%3A%20theory%20and%20applications&amp;publication_year=2006&amp;author=G.-B.%20Huang&amp;author=Q.-Y.%20Zhu&amp;author=C.-K.%20Siew" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0196"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0197" id="ref-id-bib0197" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[197]</span></span></a></span><span class="reference" id="sbref0197"><div class="other-ref"><span>S. Ghosh, E. Laksana, L.-P. Morency, S. Scherer, Representation learning for speech emotion recognition, in: San Francisco, USA, 2016: pp. 3603–3607. 10.21437/Interspeech.2016-692.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Ghosh%2C%20E.%20Laksana%2C%20L.-P.%20Morency%2C%20S.%20Scherer%2C%20Representation%20learning%20for%20speech%20emotion%20recognition%2C%20in%3A%20San%20Francisco%2C%20USA%2C%202016%3A%20pp.%203603%E2%80%933607.%2010.21437%2FInterspeech.2016-692." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0198" id="ref-id-bib0198" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[198]</span></span></a></span><span class="reference" id="sbref0198"><div class="contribution"><div class="authors u-font-sans">S. Mirsamadi, E. Barsoum, C. Zhang</div><div id="ref-id-sbref0198" class="title text-m">Automatic speech emotion recognition using recurrent neural networks with local attention</div></div><div class="host u-font-sans">2017 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2017), pp. 2227-2231, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2017.7952552" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2017.7952552</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85023747161&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0198"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20speech%20emotion%20recognition%20using%20recurrent%20neural%20networks%20with%20local%20attention&amp;publication_year=2017&amp;author=S.%20Mirsamadi&amp;author=E.%20Barsoum&amp;author=C.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0198"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0199" id="ref-id-bib0199" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[199]</span></span></a></span><span class="reference" id="sbref0199"><div class="contribution"><div class="authors u-font-sans">M. Chen, X. He, J. Yang, H. Zhang</div><div id="ref-id-sbref0199" class="title text-m">3-D Convolutional recurrent neural networks with attention model for speech emotion recognition</div></div><div class="host u-font-sans">IEEE Signal Process. Lett., 25 (2018), pp. 1440-1444, <a class="anchor anchor-primary" href="https://doi.org/10.1109/LSP.2018.2860246" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/LSP.2018.2860246</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85050584396&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0199"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=3-D%20Convolutional%20recurrent%20neural%20networks%20with%20attention%20model%20for%20speech%20emotion%20recognition&amp;publication_year=2018&amp;author=M.%20Chen&amp;author=X.%20He&amp;author=J.%20Yang&amp;author=H.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0199"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0200" id="ref-id-bib0200" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[200]</span></span></a></span><span class="reference" id="sbref0200"><div class="contribution"><div class="authors u-font-sans">G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M.A. Nicolaou, B. Schuller, S. Zafeiriou</div><div id="ref-id-sbref0200" class="title text-m">Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network</div></div><div class="host u-font-sans">2016 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2016), pp. 5200-5204, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2016.7472669" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2016.7472669</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84973293291&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0200"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adieu%20features%20End-to-end%20speech%20emotion%20recognition%20using%20a%20deep%20convolutional%20recurrent%20network&amp;publication_year=2016&amp;author=G.%20Trigeorgis&amp;author=F.%20Ringeval&amp;author=R.%20Brueckner&amp;author=E.%20Marchi&amp;author=M.A.%20Nicolaou&amp;author=B.%20Schuller&amp;author=S.%20Zafeiriou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0200"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0201" id="ref-id-bib0201" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[201]</span></span></a></span><span class="reference" id="sbref0201"><div class="contribution"><div class="authors u-font-sans">P. Tzirakis, J. Zhang, B.W. Schuller</div><div id="ref-id-sbref0201" class="title text-m">End-to-end speech emotion recognition using deep neural networks</div></div><div class="host u-font-sans">2018 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2018), pp. 5089-5093, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2018.8462677" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2018.8462677</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054257106&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0201"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=End-to-end%20speech%20emotion%20recognition%20using%20deep%20neural%20networks&amp;publication_year=2018&amp;author=P.%20Tzirakis&amp;author=J.%20Zhang&amp;author=B.W.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0201"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0202" id="ref-id-bib0202" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[202]</span></span></a></span><span class="reference" id="sbref0202"><div class="contribution"><div class="authors u-font-sans">X. Wu, S. Liu, Y. Cao, X. Li, J. Yu, D. Dai, X. Ma, S. Hu, Z. Wu, X. Liu, H. Meng</div><div id="ref-id-sbref0202" class="title text-m">Speech emotion recognition using capsule networks</div></div><div class="host u-font-sans">ICASSP 2019 - 2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2019), pp. 6695-6699, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2019.8683163" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2019.8683163</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068955456&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0202"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20using%20capsule%20networks&amp;publication_year=2019&amp;author=X.%20Wu&amp;author=S.%20Liu&amp;author=Y.%20Cao&amp;author=X.%20Li&amp;author=J.%20Yu&amp;author=D.%20Dai&amp;author=X.%20Ma&amp;author=S.%20Hu&amp;author=Z.%20Wu&amp;author=X.%20Liu&amp;author=H.%20Meng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0202"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0203" id="ref-id-bib0203" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[203]</span></span></a></span><span class="reference" id="sbref0203"><div class="contribution"><div class="authors u-font-sans">Z. Zhao, Y. Zhao, Z. Bao, H. Wang, Z. Zhang, C. Li</div><div id="ref-id-sbref0203" class="title text-m">Deep spectrum feature representations for speech emotion recognition</div></div><div class="host u-font-sans">ASMMC-MMAC18, Association for Computing Machinery, New York, NY, USA (2018), pp. 27-33, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3267935.3267948" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3267935.3267948</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85061720723&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0203"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20spectrum%20feature%20representations%20for%20speech%20emotion%20recognition&amp;publication_year=2018&amp;author=Z.%20Zhao&amp;author=Y.%20Zhao&amp;author=Z.%20Bao&amp;author=H.%20Wang&amp;author=Z.%20Zhang&amp;author=C.%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0203"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0204" id="ref-id-bib0204" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[204]</span></span></a></span><span class="reference" id="sbref0204"><div class="contribution"><div class="authors u-font-sans">S. Sahu, R. Gupta, G. Sivaraman, W. AbdAlmageed, C. Espy-Wilson</div><div id="ref-id-sbref0204" class="title text-m">Adversarial auto-encoders for speech based emotion recognition</div></div><div class="host u-font-sans">Interspeech 2017, ISCA (2017), pp. 1243-1247, <a class="anchor anchor-primary" href="https://doi.org/10.21437/Interspeech.2017-1421" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.21437/Interspeech.2017-1421</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85039166524&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0204"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adversarial%20auto-encoders%20for%20speech%20based%20emotion%20recognition&amp;publication_year=2017&amp;author=S.%20Sahu&amp;author=R.%20Gupta&amp;author=G.%20Sivaraman&amp;author=W.%20AbdAlmageed&amp;author=C.%20Espy-Wilson" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0204"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0205" id="ref-id-bib0205" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[205]</span></span></a></span><span class="reference" id="sbref0205"><div class="contribution"><div class="authors u-font-sans">J. Han, Z. Zhang, Z. Ren, F. Ringeval, B. Schuller</div><div id="ref-id-sbref0205" class="title text-m">Towards conditional adversarial training for predicting emotions from speech</div></div><div class="host u-font-sans">2018 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2018), pp. 6822-6826, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2018.8462579" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2018.8462579</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054244540&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0205"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20conditional%20adversarial%20training%20for%20predicting%20emotions%20from%20speech&amp;publication_year=2018&amp;author=J.%20Han&amp;author=Z.%20Zhang&amp;author=Z.%20Ren&amp;author=F.%20Ringeval&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0205"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0206" id="ref-id-bib0206" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[206]</span></span></a></span><span class="reference" id="sbref0206"><div class="contribution"><div class="authors u-font-sans">S. Sahu, R. Gupta, C. Espy-Wilson</div><div id="ref-id-sbref0206" class="title text-m">Modeling feature representations for affective speech using generative adversarial networks</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.2998118" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.2998118</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Modeling%20feature%20representations%20for%20affective%20speech%20using%20generative%20adversarial%20networks&amp;publication_year=2020&amp;author=S.%20Sahu&amp;author=R.%20Gupta&amp;author=C.%20Espy-Wilson" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0206"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0207" id="ref-id-bib0207" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[207]</span></span></a></span><span class="reference" id="sbref0207"><div class="contribution"><div class="authors u-font-sans">F. Bao, M. Neumann, N.T. Vu</div><div id="ref-id-sbref0207" class="title text-m">CycleGAN-based emotion style transfer as data augmentation for speech emotion recognition</div></div><div class="host u-font-sans">Interspeech 2019, ISCA (2019), pp. 2828-2832, <a class="anchor anchor-primary" href="https://doi.org/10.21437/Interspeech.2019-2293" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.21437/Interspeech.2019-2293</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85074695470&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0207"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=CycleGAN-based%20emotion%20style%20transfer%20as%20data%20augmentation%20for%20speech%20emotion%20recognition&amp;publication_year=2019&amp;author=F.%20Bao&amp;author=M.%20Neumann&amp;author=N.T.%20Vu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0207"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0208" id="ref-id-bib0208" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[208]</span></span></a></span><span class="reference" id="sbref0208"><div class="contribution"><div class="authors u-font-sans">J. Zhu, T. Park, P. Isola, A.A. Efros</div><div id="ref-id-sbref0208" class="title text-m">Unpaired image-to-image translation using cycle-consistent adversarial networks</div></div><div class="host u-font-sans">2017 IEEE Int. Conf. Comput. Vis. ICCV (2017), pp. 2242-2251, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCV.2017.244" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCV.2017.244</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85041892358&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0208"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks&amp;publication_year=2017&amp;author=J.%20Zhu&amp;author=T.%20Park&amp;author=P.%20Isola&amp;author=A.A.%20Efros" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0208"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0209" id="ref-id-bib0209" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[209]</span></span></a></span><span class="reference" id="sbref0209"><div class="contribution"><div class="authors u-font-sans">Z. Zeng, M. Pantic, G.I. Roisman, T.S. Huang</div><div id="ref-id-sbref0209" class="title text-m">A survey of affect recognition methods: audio, visual, and spontaneous expressions</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 31 (2009), pp. 39-58, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2008.52" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2008.52</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-57149144228&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0209"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20survey%20of%20affect%20recognition%20methods%3A%20audio%2C%20visual%2C%20and%20spontaneous%20expressions&amp;publication_year=2009&amp;author=Z.%20Zeng&amp;author=M.%20Pantic&amp;author=G.I.%20Roisman&amp;author=T.S.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0209"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0210" id="ref-id-bib0210" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[210]</span></span></a></span><span class="reference" id="sbref0210"><div class="contribution"><div class="authors u-font-sans">Y.-I. Tian, T. Kanade, J.F. Cohn</div><div id="ref-id-sbref0210" class="title text-m">Recognizing action units for facial expression analysis</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 23 (2001), pp. 97-115, <a class="anchor anchor-primary" href="https://doi.org/10.1109/34.908962" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/34.908962</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0035250305&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0210"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognizing%20action%20units%20for%20facial%20expression%20analysis&amp;publication_year=2001&amp;author=Y.-I.%20Tian&amp;author=T.%20Kanade&amp;author=J.F.%20Cohn" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0210"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0211" id="ref-id-bib0211" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[211]</span></span></a></span><span class="reference" id="sbref0211"><div class="contribution"><div class="authors u-font-sans">E. Sariyanidi, H. Gunes, A. Cavallaro</div><div id="ref-id-sbref0211" class="title text-m">Automatic analysis of facial affect: a survey of registration, representation, and recognition</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 37 (2015), pp. 1113-1133, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2014.2366127" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2014.2366127</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84929192741&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0211"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20analysis%20of%20facial%20affect%3A%20a%20survey%20of%20registration%2C%20representation%2C%20and%20recognition&amp;publication_year=2015&amp;author=E.%20Sariyanidi&amp;author=H.%20Gunes&amp;author=A.%20Cavallaro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0211"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0212" id="ref-id-bib0212" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[212]</span></span></a></span><span class="reference" id="sbref0212"><div class="contribution"><div class="authors u-font-sans">P. Liu, S. Han, Z. Meng, Y. Tong</div><div id="ref-id-sbref0212" class="title text-m">Facial expression recognition via a boosted deep belief network</div></div><div class="host u-font-sans">2014 IEEE Conf. Comput. Vis. Pattern Recognit (2014), pp. 1805-1812, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2014.233" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2014.233</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84911384987&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0212"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20via%20a%20boosted%20deep%20belief%20network&amp;publication_year=2014&amp;author=P.%20Liu&amp;author=S.%20Han&amp;author=Z.%20Meng&amp;author=Y.%20Tong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0212"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0213" id="ref-id-bib0213" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[213]</span></span></a></span><span class="reference" id="sbref0213"><div class="contribution"><div class="authors u-font-sans">H. Jung, S. Lee, J. Yim, S. Park, J. Kim</div><div id="ref-id-sbref0213" class="title text-m">Joint fine-tuning in deep neural networks for facial expression recognition</div></div><div class="host u-font-sans">2015 IEEE Int. Conf. Comput. Vis. ICCV, IEEE, Santiago, Chile (2015), pp. 2983-2991, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCV.2015.341" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCV.2015.341</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84973917824&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0213"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Joint%20fine-tuning%20in%20deep%20neural%20networks%20for%20facial%20expression%20recognition&amp;publication_year=2015&amp;author=H.%20Jung&amp;author=S.%20Lee&amp;author=J.%20Yim&amp;author=S.%20Park&amp;author=J.%20Kim" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0213"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0214" id="ref-id-bib0214" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[214]</span></span></a></span><span class="reference" id="sbref0214"><div class="contribution"><div class="authors u-font-sans">B. Xia, W. Wang, S. Wang, E. Chen</div><div id="ref-id-sbref0214" class="title text-m">Learning from macro-expression: a micro-expression recognition framework</div></div><div class="host u-font-sans">Proc. 28th ACM Int. Conf. Multimed., ACM, Seattle WA USA (2020), pp. 2936-2944, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3394171.3413774" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3394171.3413774</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85100549213&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0214"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20from%20macro-expression%3A%20a%20micro-expression%20recognition%20framework&amp;publication_year=2020&amp;author=B.%20Xia&amp;author=W.%20Wang&amp;author=S.%20Wang&amp;author=E.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0214"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0215" id="ref-id-bib0215" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[215]</span></span></a></span><span class="reference" id="sbref0215"><div class="contribution"><div class="authors u-font-sans">X. Ben, Y. Ren, J. Zhang, S.-J. Wang, K. Kpalma, W. Meng, Y.-J. Liu</div><div id="ref-id-sbref0215" class="title text-m">Video-based facial micro-expression analysis: a survey of datasets, features and algorithms</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell. (2021), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2021.3067464" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2021.3067464</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Video-based%20facial%20micro-expression%20analysis%3A%20a%20survey%20of%20datasets%2C%20features%20and%20algorithms&amp;publication_year=2021&amp;author=X.%20Ben&amp;author=Y.%20Ren&amp;author=J.%20Zhang&amp;author=S.-J.%20Wang&amp;author=K.%20Kpalma&amp;author=W.%20Meng&amp;author=Y.-J.%20Liu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0215"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0216" id="ref-id-bib0216" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[216]</span></span></a></span><span class="reference" id="sbref0216"><div class="contribution"><div class="authors u-font-sans">Y.-J. Liu, J.-K. Zhang, W.-J. Yan, S.-J. Wang, G. Zhao, X. Fu</div><div id="ref-id-sbref0216" class="title text-m">A main directional mean optical flow feature for spontaneous micro-expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 7 (2016), pp. 299-310, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2015.2485205" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2015.2485205</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85027450216&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0216"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20main%20directional%20mean%20optical%20flow%20feature%20for%20spontaneous%20micro-expression%20recognition&amp;publication_year=2016&amp;author=Y.-J.%20Liu&amp;author=J.-K.%20Zhang&amp;author=W.-J.%20Yan&amp;author=S.-J.%20Wang&amp;author=G.%20Zhao&amp;author=X.%20Fu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0216"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0217" id="ref-id-bib0217" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[217]</span></span></a></span><span class="reference" id="sbref0217"><div class="contribution"><div class="authors u-font-sans">H. Zheng, J. Zhu, Z. Yang, Z. Jin</div><div id="ref-id-sbref0217" class="title text-m">Effective micro-expression recognition using relaxed K-SVD algorithm</div></div><div class="host u-font-sans">Int. J. Mach. Learn. Cybern., 8 (2017), pp. 2043-2049, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s13042-017-0684-6" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s13042-017-0684-6</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85032388759&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0217"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Effective%20micro-expression%20recognition%20using%20relaxed%20K-SVD%20algorithm&amp;publication_year=2017&amp;author=H.%20Zheng&amp;author=J.%20Zhu&amp;author=Z.%20Yang&amp;author=Z.%20Jin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0217"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0218" id="ref-id-bib0218" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[218]</span></span></a></span><span class="reference" id="sbref0218"><div class="contribution"><div class="authors u-font-sans">B. Sun, S. Cao, D. Li, J. He, L. Yu</div><div id="ref-id-sbref0218" class="title text-m">Dynamic Micro-expression recognition using knowledge distillation</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.2986962" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.2986962</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Dynamic%20Micro-expression%20recognition%20using%20knowledge%20distillation&amp;publication_year=2020&amp;author=B.%20Sun&amp;author=S.%20Cao&amp;author=D.%20Li&amp;author=J.%20He&amp;author=L.%20Yu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0218"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0219" id="ref-id-bib0219" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[219]</span></span></a></span><span class="reference" id="sbref0219"><div class="contribution"><div class="authors u-font-sans">A. Majumder, L. Behera, V.K. Subramanian</div><div id="ref-id-sbref0219" class="title text-m">Automatic facial expression recognition system using deep network-based data fusion</div></div><div class="host u-font-sans">IEEE Trans. Cybern., 48 (2018), pp. 103-114, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCYB.2016.2625419" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCYB.2016.2625419</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84996757699&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0219"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20recognition%20system%20using%20deep%20network-based%20data%20fusion&amp;publication_year=2018&amp;author=A.%20Majumder&amp;author=L.%20Behera&amp;author=V.K.%20Subramanian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0219"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0220" id="ref-id-bib0220" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[220]</span></span></a></span><span class="reference" id="sbref0220"><div class="contribution"><div class="authors u-font-sans">A. Barman, P. Dutta</div><div id="ref-id-sbref0220" class="title text-m">Facial expression recognition using distance and texture signature relevant features</div></div><div class="host u-font-sans">Appl. Soft Comput., 77 (2019), pp. 88-105, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.asoc.2019.01.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.asoc.2019.01.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494619300158/pdfft?md5=8c87774c4fbc501769c6042ab9fb3c2b&amp;pid=1-s2.0-S1568494619300158-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0220"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1568494619300158" aria-describedby="ref-id-sbref0220"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060340764&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0220"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20distance%20and%20texture%20signature%20relevant%20features&amp;publication_year=2019&amp;author=A.%20Barman&amp;author=P.%20Dutta" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0220"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0221" id="ref-id-bib0221" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[221]</span></span></a></span><span class="reference" id="sbref0221"><div class="contribution"><div class="authors u-font-sans">G. Wen, T. Chang, H. Li, L. Jiang</div><div id="ref-id-sbref0221" class="title text-m">Dynamic objectives learning for facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Multimed. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2020.2966858" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2020.2966858</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Dynamic%20objectives%20learning%20for%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=G.%20Wen&amp;author=T.%20Chang&amp;author=H.%20Li&amp;author=L.%20Jiang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0221"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0222" id="ref-id-bib0222" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[222]</span></span></a></span><span class="reference" id="sbref0222"><div class="contribution"><div class="authors u-font-sans">K. Yurtkan, H. Demirel</div><div id="ref-id-sbref0222" class="title text-m">Feature selection for improved 3D facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 38 (2014), pp. 26-33, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2013.10.026" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2013.10.026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865513004182/pdfft?md5=60c3b90168cd1c6951344299a60076ed&amp;pid=1-s2.0-S0167865513004182-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0222"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865513004182" aria-describedby="ref-id-sbref0222"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84888865385&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0222"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20selection%20for%20improved%203D%20facial%20expression%20recognition&amp;publication_year=2014&amp;author=K.%20Yurtkan&amp;author=H.%20Demirel" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0222"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0223" id="ref-id-bib0223" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[223]</span></span></a></span><span class="reference" id="sbref0223"><div class="contribution"><div class="authors u-font-sans">Q. Zhen, D. Huang, Y. Wang, L. Chen</div><div id="ref-id-sbref0223" class="title text-m">Muscular movement model-based automatic 3D/4D facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 18 (2016), pp. 1438-1450, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2016.2557063" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2016.2557063</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84976531968&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0223"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Muscular%20movement%20model-based%20automatic%203D4D%20facial%20expression%20recognition&amp;publication_year=2016&amp;author=Q.%20Zhen&amp;author=D.%20Huang&amp;author=Y.%20Wang&amp;author=L.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0223"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0224" id="ref-id-bib0224" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[224]</span></span></a></span><span class="reference" id="sbref0224"><div class="contribution"><div class="authors u-font-sans">M. Behzad, N. Vo, X. Li, G. Zhao</div><div id="ref-id-sbref0224" class="title text-m">Automatic 4D facial expression recognition via collaborative cross-domain dynamic image network</div></div><div class="host u-font-sans">Proc. Br. Mach. Vis. Conf. BMVC, BMVA Press (2019), pp. 149.1-149.12</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%204D%20facial%20expression%20recognition%20via%20collaborative%20cross-domain%20dynamic%20image%20network&amp;publication_year=2019&amp;author=M.%20Behzad&amp;author=N.%20Vo&amp;author=X.%20Li&amp;author=G.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0224"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0225" id="ref-id-bib0225" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[225]</span></span></a></span><span class="reference" id="sbref0225"><div class="contribution"><div class="authors u-font-sans">Z. Yu, C. Zhang</div><div id="ref-id-sbref0225" class="title text-m">Image based static facial expression recognition with multiple deep network learning</div></div><div class="host u-font-sans">Proc. 2015 ACM Int. Conf. Multimodal Interact., Association for Computing Machinery, New York, NY, USA (2015), pp. 435-442, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2818346.2830595" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2818346.2830595</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84959297432&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0225"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Image%20based%20static%20facial%20expression%20recognition%20with%20multiple%20deep%20network%20learning&amp;publication_year=2015&amp;author=Z.%20Yu&amp;author=C.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0225"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0226" id="ref-id-bib0226" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[226]</span></span></a></span><span class="reference" id="sbref0226"><div class="contribution"><div class="authors u-font-sans">D.K. Jain, P. Shamsolmoali, P. Sehdev</div><div id="ref-id-sbref0226" class="title text-m">Extended deep neural network for facial emotion recognition</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 120 (2019), pp. 69-74, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2019.01.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2019.01.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S016786551930008X/pdfft?md5=4fe7f1fdf6a320ef2e0efcef551ef975&amp;pid=1-s2.0-S016786551930008X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0226"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S016786551930008X" aria-describedby="ref-id-sbref0226"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060285318&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0226"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Extended%20deep%20neural%20network%20for%20facial%20emotion%20recognition&amp;publication_year=2019&amp;author=D.K.%20Jain&amp;author=P.%20Shamsolmoali&amp;author=P.%20Sehdev" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0226"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0227" id="ref-id-bib0227" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[227]</span></span></a></span><span class="reference" id="sbref0227"><div class="contribution"><div class="authors u-font-sans">A. Yao, D. Cai, P. Hu, S. Wang, L. Sha, Y. Chen</div><div id="ref-id-sbref0227" class="title text-m">HoloNet: towards robust emotion recognition in the wild</div></div><div class="host u-font-sans">Proc. 18th ACM Int. Conf. Multimodal Interact., Association for Computing Machinery, New York, NY, USA (2016), pp. 472-478, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2993148.2997639" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2993148.2997639</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85016550069&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0227"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=HoloNet%3A%20towards%20robust%20emotion%20recognition%20in%20the%20wild&amp;publication_year=2016&amp;author=A.%20Yao&amp;author=D.%20Cai&amp;author=P.%20Hu&amp;author=S.%20Wang&amp;author=L.%20Sha&amp;author=Y.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0227"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0228" id="ref-id-bib0228" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[228]</span></span></a></span><span class="reference" id="sbref0228"><div class="contribution"><div class="authors u-font-sans">Z. Zhang</div><div id="ref-id-sbref0228" class="title text-m">Feature-based facial expression recognition: sensitivity analysis and experiments with a multilayer perceptron</div></div><div class="host u-font-sans">Int. J. Pattern Recognit. Artif. Intell., 13 (1999), pp. 893-911, <a class="anchor anchor-primary" href="https://doi.org/10.1142/S0218001499000495" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1142/S0218001499000495</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0033311282&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0228"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature-based%20facial%20expression%20recognition%3A%20sensitivity%20analysis%20and%20experiments%20with%20a%20multilayer%20perceptron&amp;publication_year=1999&amp;author=Z.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0228"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0229" id="ref-id-bib0229" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[229]</span></span></a></span><span class="reference" id="sbref0229"><div class="contribution"><div class="authors u-font-sans">N. Sun, Q. Li, R. Huan, J. Liu, G. Han</div><div id="ref-id-sbref0229" class="title text-m">Deep spatial-temporal feature fusion for facial expression recognition in static images</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 119 (2019), pp. 49-61, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2017.10.022" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2017.10.022</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865517303902/pdfft?md5=7d06ffd4ab371ac33b9e8cd3a5e7d5b3&amp;pid=1-s2.0-S0167865517303902-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0229"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865517303902" aria-describedby="ref-id-sbref0229"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85032183409&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0229"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20spatial-temporal%20feature%20fusion%20for%20facial%20expression%20recognition%20in%20static%20images&amp;publication_year=2019&amp;author=N.%20Sun&amp;author=Q.%20Li&amp;author=R.%20Huan&amp;author=J.%20Liu&amp;author=G.%20Han" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0229"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0230" id="ref-id-bib0230" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[230]</span></span></a></span><span class="reference" id="sbref0230"><div class="contribution"><div class="authors u-font-sans">D. Ghimire, J. Lee</div><div id="ref-id-sbref0230" class="title text-m">Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines</div></div><div class="host u-font-sans">Sensors, 13 (2013), pp. 7714-7734, <a class="anchor anchor-primary" href="https://doi.org/10.3390/s130607714" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/s130607714</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84879149160&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0230"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Geometric%20feature-based%20facial%20expression%20recognition%20in%20image%20sequences%20using%20multi-class%20adaboost%20and%20support%20vector%20machines&amp;publication_year=2013&amp;author=D.%20Ghimire&amp;author=J.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0230"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0231" id="ref-id-bib0231" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[231]</span></span></a></span><span class="reference" id="sbref0231"><div class="contribution"><div class="authors u-font-sans">A.A.S.Gunawan Sujono</div><div id="ref-id-sbref0231" class="title text-m">Face expression detection on kinect using active appearance model and fuzzy logic</div></div><div class="host u-font-sans">Procedia Comput. Sci., 59 (2015), pp. 268-274, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.procs.2015.07.558" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.procs.2015.07.558</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84948400557&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0231"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Face%20expression%20detection%20on%20kinect%20using%20active%20appearance%20model%20and%20fuzzy%20logic&amp;publication_year=2015&amp;author=A.A.S.Gunawan%20Sujono" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0231"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0232" id="ref-id-bib0232" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[232]</span></span></a></span><span class="reference" id="sbref0232"><div class="contribution"><div class="authors u-font-sans">T.F. Cootes, G.J. Edwards, C.J. Taylor</div><div id="ref-id-sbref0232" class="title text-m">Active appearance models</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 23 (2001), pp. 681-685, <a class="anchor anchor-primary" href="https://doi.org/10.1109/34.927467" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/34.927467</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Active%20appearance%20models&amp;publication_year=2001&amp;author=T.F.%20Cootes&amp;author=G.J.%20Edwards&amp;author=C.J.%20Taylor" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0232"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0233" id="ref-id-bib0233" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[233]</span></span></a></span><span class="reference" id="sbref0233"><div class="other-ref"><span>P. Ekman, J.C. Hager, W.V. Friesen, Facial action coding system: the manual on CD ROM, Salt Lake City, 2002.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=P.%20Ekman%2C%20J.C.%20Hager%2C%20W.V.%20Friesen%2C%20Facial%20action%20coding%20system%3A%20the%20manual%20on%20CD%20ROM%2C%20Salt%20Lake%20City%2C%202002." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0234" id="ref-id-bib0234" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[234]</span></span></a></span><span class="reference" id="sbref0234"><div class="contribution"><div class="authors u-font-sans">F. Makhmudkhujaev, M. Abdullah-Al-Wadud, M.T.B. Iqbal, B. Ryu, O. Chae</div><div id="ref-id-sbref0234" class="title text-m">Facial expression recognition with local prominent directional pattern</div></div><div class="host u-font-sans">Signal Process. Image Commun, 74 (2019), pp. 1-12, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.image.2019.01.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.image.2019.01.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0923596518306556/pdfft?md5=23b9947fd05942cc1b0046c026f5e50d&amp;pid=1-s2.0-S0923596518306556-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0234"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0923596518306556" aria-describedby="ref-id-sbref0234"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85060216913&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0234"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20with%20local%20prominent%20directional%20pattern&amp;publication_year=2019&amp;author=F.%20Makhmudkhujaev&amp;author=M.%20Abdullah-Al-Wadud&amp;author=M.T.B.%20Iqbal&amp;author=B.%20Ryu&amp;author=O.%20Chae" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0234"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0235" id="ref-id-bib0235" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[235]</span></span></a></span><span class="reference" id="sbref0235"><div class="contribution"><div class="authors u-font-sans">Y. Yan, Z. Zhang, S. Chen, H. Wang</div><div id="ref-id-sbref0235" class="title text-m">Low-resolution facial expression recognition: a filter learning perspective</div></div><div class="host u-font-sans">Signal Process., 169 (2020), Article 107370, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.sigpro.2019.107370" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.sigpro.2019.107370</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0165168419304232/pdfft?md5=a5ffff0c6566c987c6d2f800ca5372f5&amp;pid=1-s2.0-S0165168419304232-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0235"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0165168419304232" aria-describedby="ref-id-sbref0235"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85075197969&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0235"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Low-resolution%20facial%20expression%20recognition%3A%20a%20filter%20learning%20perspective&amp;publication_year=2020&amp;author=Y.%20Yan&amp;author=Z.%20Zhang&amp;author=S.%20Chen&amp;author=H.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0235"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0236" id="ref-id-bib0236" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[236]</span></span></a></span><span class="reference" id="sbref0236"><div class="contribution"><div class="authors u-font-sans">Y. Yao, D. Huang, X. Yang, Y. Wang, L. Chen</div><div id="ref-id-sbref0236" class="title text-m">Texture and geometry scattering representation-based facial expression recognition in 2D+3D videos</div></div><div class="host u-font-sans">ACM Trans. Multimed. Comput. Commun. Appl., 14 (2018), pp. 18.1-18.23, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3131345" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3131345</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Texture%20and%20geometry%20scattering%20representation-based%20facial%20expression%20recognition%20in%202D3D%20videos&amp;publication_year=2018&amp;author=Y.%20Yao&amp;author=D.%20Huang&amp;author=X.%20Yang&amp;author=Y.%20Wang&amp;author=L.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0236"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0237" id="ref-id-bib0237" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[237]</span></span></a></span><span class="reference" id="sbref0237"><div class="contribution"><div class="authors u-font-sans">Y. Zong, X. Huang, W. Zheng, Z. Cui, G. Zhao</div><div id="ref-id-sbref0237" class="title text-m">Learning from hierarchical spatiotemporal descriptors for micro-expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 20 (2018), pp. 3160-3172, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2018.2820321" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2018.2820321</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85044749364&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0237"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20from%20hierarchical%20spatiotemporal%20descriptors%20for%20micro-expression%20recognition&amp;publication_year=2018&amp;author=Y.%20Zong&amp;author=X.%20Huang&amp;author=W.%20Zheng&amp;author=Z.%20Cui&amp;author=G.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0237"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0238" id="ref-id-bib0238" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[238]</span></span></a></span><span class="reference" id="sbref0238"><div class="contribution"><div class="authors u-font-sans">K. Zhang, Y. Huang, Y. Du, L. Wang</div><div id="ref-id-sbref0238" class="title text-m">Facial expression recognition based on deep evolutional spatial-temporal networks</div></div><div class="host u-font-sans">IEEE Trans. Image Process, 26 (2017), pp. 4193-4203, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2017.2689999" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2017.2689999</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85027517303&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0238"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%20deep%20evolutional%20spatial-temporal%20networks&amp;publication_year=2017&amp;author=K.%20Zhang&amp;author=Y.%20Huang&amp;author=Y.%20Du&amp;author=L.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0238"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0239" id="ref-id-bib0239" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[239]</span></span></a></span><span class="reference" id="sbref0239"><div class="contribution"><div class="authors u-font-sans">F. Zhang, T. Zhang, Q. Mao, C. Xu</div><div id="ref-id-sbref0239" class="title text-m">Joint pose and expression modeling for facial expression recognition</div></div><div class="host u-font-sans">2018 IEEECVF Conf. Comput. Vis. Pattern Recognit., IEEE, Salt Lake City, UT, USA (2018), pp. 3359-3368, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2018.00354" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2018.00354</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85058228451&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0239"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Joint%20pose%20and%20expression%20modeling%20for%20facial%20expression%20recognition&amp;publication_year=2018&amp;author=F.%20Zhang&amp;author=T.%20Zhang&amp;author=Q.%20Mao&amp;author=C.%20Xu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0239"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0240" id="ref-id-bib0240" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[240]</span></span></a></span><span class="reference" id="sbref0240"><div class="contribution"><div class="authors u-font-sans">B. Fasel, J. Luettin</div><div id="ref-id-sbref0240" class="title text-m">Automatic facial expression analysis: a survey</div></div><div class="host u-font-sans">Pattern Recognit., 36 (2003), pp. 259-275, <a class="anchor anchor-primary" href="https://doi.org/10.1016/S0031-3203(02)00052-3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/S0031-3203(02)00052-3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320302000523/pdfft?md5=f9f8bd0d53f5a57b0fcd5c094826f59b&amp;pid=1-s2.0-S0031320302000523-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0240"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320302000523" aria-describedby="ref-id-sbref0240"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-0037209464&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0240"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20facial%20expression%20analysis%3A%20a%20survey&amp;publication_year=2003&amp;author=B.%20Fasel&amp;author=J.%20Luettin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0240"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0241" id="ref-id-bib0241" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[241]</span></span></a></span><span class="reference" id="sbref0241"><div class="contribution"><div class="authors u-font-sans">J. Hamm, C.G. Kohler, R.C. Gur, R. Verma</div><div id="ref-id-sbref0241" class="title text-m">Automated facial action coding system for dynamic analysis of facial expressions in neuropsychiatric disorders</div></div><div class="host u-font-sans">J. Neurosci. Methods., 200 (2011), pp. 237-256, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.jneumeth.2011.06.023" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.jneumeth.2011.06.023</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S016502701100358X/pdfft?md5=e2d78fca55060dcf443d34e46f0e857a&amp;pid=1-s2.0-S016502701100358X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0241"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S016502701100358X" aria-describedby="ref-id-sbref0241"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80051583841&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0241"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automated%20facial%20action%20coding%20system%20for%20dynamic%20analysis%20of%20facial%20expressions%20in%20neuropsychiatric%20disorders&amp;publication_year=2011&amp;author=J.%20Hamm&amp;author=C.G.%20Kohler&amp;author=R.C.%20Gur&amp;author=R.%20Verma" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0241"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0242" id="ref-id-bib0242" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[242]</span></span></a></span><span class="reference" id="sbref0242"><div class="contribution"><div class="authors u-font-sans">C. Shan, S. Gong, P.W. McOwan</div><div id="ref-id-sbref0242" class="title text-m">Facial expression recognition based on local binary patterns: a comprehensive study</div></div><div class="host u-font-sans">Image Vis. Comput., 27 (2009), pp. 803-816, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.imavis.2008.08.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.imavis.2008.08.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0262885608001844/pdfft?md5=f193c98d97c488b5b841cc25dd4c0b50&amp;pid=1-s2.0-S0262885608001844-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0242"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0262885608001844" aria-describedby="ref-id-sbref0242"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-63449136395&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0242"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20based%20on%20local%20binary%20patterns%3A%20a%20comprehensive%20study&amp;publication_year=2009&amp;author=C.%20Shan&amp;author=S.%20Gong&amp;author=P.W.%20McOwan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0242"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0243" id="ref-id-bib0243" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[243]</span></span></a></span><span class="reference" id="sbref0243"><div class="contribution"><div class="authors u-font-sans">W. Gu, C. Xiang, Y.V. Venkatesh, D. Huang, H. Lin</div><div id="ref-id-sbref0243" class="title text-m">Facial expression recognition using radial encoding of local Gabor features and classifier synthesis</div></div><div class="host u-font-sans">Pattern Recognit., 45 (2012), pp. 80-91, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2011.05.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2011.05.006</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320311002056/pdfft?md5=8042c6b6eae2ceb1ef5da9e097293204&amp;pid=1-s2.0-S0031320311002056-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0243"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320311002056" aria-describedby="ref-id-sbref0243"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80052781929&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0243"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20using%20radial%20encoding%20of%20local%20Gabor%20features%20and%20classifier%20synthesis&amp;publication_year=2012&amp;author=W.%20Gu&amp;author=C.%20Xiang&amp;author=Y.V.%20Venkatesh&amp;author=D.%20Huang&amp;author=H.%20Lin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0243"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0244" id="ref-id-bib0244" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[244]</span></span></a></span><span class="reference" id="sbref0244"><div class="contribution"><div class="authors u-font-sans">G. Zhao, M. Pietikainen</div><div id="ref-id-sbref0244" class="title text-m">Dynamic texture recognition using local binary patterns with an application to facial expressions</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 29 (2007), pp. 915-928, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2007.1110" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2007.1110</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34247557079&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0244"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Dynamic%20texture%20recognition%20using%20local%20binary%20patterns%20with%20an%20application%20to%20facial%20expressions&amp;publication_year=2007&amp;author=G.%20Zhao&amp;author=M.%20Pietikainen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0244"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0245" id="ref-id-bib0245" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[245]</span></span></a></span><span class="reference" id="sbref0245"><div class="contribution"><div class="authors u-font-sans">Y. Wang, J. See, R.C.-W. Phan, Y.-H. Oh</div><div id="ref-id-sbref0245" class="title text-m">Efficient spatio-temporal local binary patterns for spontaneous facial micro-expression recognition</div></div><div class="host u-font-sans">PLOS One (2015), <a class="anchor anchor-primary" href="https://doi.org/10.1371/journal.pone.0124674" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1371/journal.pone.0124674</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Efficient%20spatio-temporal%20local%20binary%20patterns%20for%20spontaneous%20facial%20micro-expression%20recognition&amp;publication_year=2015&amp;author=Y.%20Wang&amp;author=J.%20See&amp;author=R.C.-W.%20Phan&amp;author=Y.-H.%20Oh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0245"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0246" id="ref-id-bib0246" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[246]</span></span></a></span><span class="reference" id="sbref0246"><div class="contribution"><div class="authors u-font-sans">A.K. Davison, M.H. Yap, N. Costen, K. Tan, C. Lansley, D. Leightley</div><div id="ref-id-sbref0246" class="title text-m">Micro-facial movements: an investigation on spatio-temporal descriptors</div></div><div class="host u-font-sans">L. Agapito, M.M. Bronstein, C. Rother (Eds.), Computer Visison–ECCV 2014 Workshop, Springer International Publishing, Cham (2015), pp. 111-123, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-16181-5_" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-16181-5_</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84928793977&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0246"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Micro-facial%20movements%3A%20an%20investigation%20on%20spatio-temporal%20descriptors&amp;publication_year=2015&amp;author=A.K.%20Davison&amp;author=M.H.%20Yap&amp;author=N.%20Costen&amp;author=K.%20Tan&amp;author=C.%20Lansley&amp;author=D.%20Leightley" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0246"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0247" id="ref-id-bib0247" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[247]</span></span></a></span><span class="reference" id="sbref0247"><div class="contribution"><div class="authors u-font-sans">S.-T. Liong, J. See, R.C.-W. Phan, K. Wong, S.-W. Tan</div><div id="ref-id-sbref0247" class="title text-m">Hybrid facial regions extraction for micro-expression recognition system</div></div><div class="host u-font-sans">J. Signal Process. Syst., 90 (2018), pp. 601-617, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s11265-017-1276-0" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s11265-017-1276-0</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85027705973&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0247"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Hybrid%20facial%20regions%20extraction%20for%20micro-expression%20recognition%20system&amp;publication_year=2018&amp;author=S.-T.%20Liong&amp;author=J.%20See&amp;author=R.C.-W.%20Phan&amp;author=K.%20Wong&amp;author=S.-W.%20Tan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0247"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0248" id="ref-id-bib0248" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[248]</span></span></a></span><span class="reference" id="sbref0248"><div class="contribution"><div class="authors u-font-sans">S. Zhang, B. Feng, Z. Chen, X. Huang</div><div id="ref-id-sbref0248" class="title text-m">Micro-Expression Recognition by Aggregating Local Spatio-Temporal Patterns</div></div><div class="host u-font-sans">Springer International Publishing (2017), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-51811-4_52" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-51811-4_52</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Micro-Expression%20Recognition%20by%20Aggregating%20Local%20Spatio-Temporal%20Patterns&amp;publication_year=2017&amp;author=S.%20Zhang&amp;author=B.%20Feng&amp;author=Z.%20Chen&amp;author=X.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0248"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0249" id="ref-id-bib0249" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[249]</span></span></a></span><span class="reference" id="sbref0249"><div class="contribution"><div class="authors u-font-sans">Q. Zhen, D. Huang, H. Drira, B.B. Amor, Y. Wang, M. Daoudi</div><div id="ref-id-sbref0249" class="title text-m">Magnifying subtle facial motions for effective 4D expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 10 (2019), pp. 524-536, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2747553" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2747553</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85029187097&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0249"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Magnifying%20subtle%20facial%20motions%20for%20effective%204D%20expression%20recognition&amp;publication_year=2019&amp;author=Q.%20Zhen&amp;author=D.%20Huang&amp;author=H.%20Drira&amp;author=B.B.%20Amor&amp;author=Y.%20Wang&amp;author=M.%20Daoudi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0249"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0250" id="ref-id-bib0250" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[250]</span></span></a></span><span class="reference" id="sbref0250"><div class="contribution"><div class="authors u-font-sans">A. Moeini, K. Faez, H. Sadeghi, H. Moeini</div><div id="ref-id-sbref0250" class="title text-m">2D facial expression recognition via 3D reconstruction and feature fusion</div></div><div class="host u-font-sans">J. Vis. Commun. Image Represent., 35 (2016), pp. 1-14, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.jvcir.2015.11.006" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.jvcir.2015.11.006</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1047320315002229/pdfft?md5=6e3761244b7f50f9c4723a2f7aa881b3&amp;pid=1-s2.0-S1047320315002229-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0250"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1047320315002229" aria-describedby="ref-id-sbref0250"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84950162274&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0250"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=2D%20facial%20expression%20recognition%20via%203D%20reconstruction%20and%20feature%20fusion&amp;publication_year=2016&amp;author=A.%20Moeini&amp;author=K.%20Faez&amp;author=H.%20Sadeghi&amp;author=H.%20Moeini" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0250"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0251" id="ref-id-bib0251" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[251]</span></span></a></span><span class="reference" id="sbref0251"><div class="contribution"><div class="authors u-font-sans">A.C. Le Ngo, S.-T. Liong, J. See, R.C.-W. Phan</div><div id="ref-id-sbref0251" class="title text-m">Are subtle expressions too sparse to recognize?</div></div><div class="host u-font-sans">2015 IEEE Int. Conf. Digit. Signal Process. DSP (2015), pp. 1246-1250, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICDSP.2015.7252080" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICDSP.2015.7252080</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84961344954&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0251"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Are%20subtle%20expressions%20too%20sparse%20to%20recognize&amp;publication_year=2015&amp;author=A.C.%20Le%20Ngo&amp;author=S.-T.%20Liong&amp;author=J.%20See&amp;author=R.C.-W.%20Phan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0251"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0252" id="ref-id-bib0252" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[252]</span></span></a></span><span class="reference" id="sbref0252"><div class="contribution"><div class="authors u-font-sans">W. Zheng</div><div id="ref-id-sbref0252" class="title text-m">Multi-View Facial Expression Recognition Based on group sparse reduced-rank regression</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 5 (2014), pp. 71-85, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2014.2304712" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2014.2304712</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84903643888&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0252"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-View%20Facial%20Expression%20Recognition%20Based%20on%20group%20sparse%20reduced-rank%20regression&amp;publication_year=2014&amp;author=W.%20Zheng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0252"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0253" id="ref-id-bib0253" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[253]</span></span></a></span><span class="reference" id="sbref0253"><div class="contribution"><div class="authors u-font-sans">A. Azazi, S. Lebai Lutfi, I. Venkat, F. Fernández-Martínez</div><div id="ref-id-sbref0253" class="title text-m">Towards a robust affect recognition: Automatic facial expression recognition in 3D faces</div></div><div class="host u-font-sans">Expert Syst. Appl., 42 (2015), pp. 3056-3066, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2014.10.042" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2014.10.042</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417414006666/pdfft?md5=75b8d2bc75b688b5c525cb45716d73d3&amp;pid=1-s2.0-S0957417414006666-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0253"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417414006666" aria-describedby="ref-id-sbref0253"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84919798976&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0253"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20a%20robust%20affect%20recognition%3A%20Automatic%20facial%20expression%20recognition%20in%203D%20faces&amp;publication_year=2015&amp;author=A.%20Azazi&amp;author=S.%20Lebai%20Lutfi&amp;author=I.%20Venkat&amp;author=F.%20Fern%C3%A1ndez-Mart%C3%ADnez" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0253"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0254" id="ref-id-bib0254" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[254]</span></span></a></span><span class="reference" id="sbref0254"><div class="contribution"><div class="authors u-font-sans">A. Savran, B. Sankur</div><div id="ref-id-sbref0254" class="title text-m">Non-rigid registration based model-free 3D facial expression recognition</div></div><div class="host u-font-sans">Comput. Vis. Image Underst., 162 (2017), pp. 146-165, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.cviu.2017.07.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.cviu.2017.07.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1077314217301352/pdfft?md5=36ee15a8202bce584e1f43f2245aba06&amp;pid=1-s2.0-S1077314217301352-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0254"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1077314217301352" aria-describedby="ref-id-sbref0254"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85028067153&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0254"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Non-rigid%20registration%20based%20model-free%203D%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=A.%20Savran&amp;author=B.%20Sankur" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0254"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0255" id="ref-id-bib0255" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[255]</span></span></a></span><span class="reference" id="sbref0255"><div class="contribution"><div class="authors u-font-sans">M. Chen, H.T. Ma, J. Li, H. Wang</div><div id="ref-id-sbref0255" class="title text-m">Emotion recognition using fixed length micro-expressions sequence and weighting method</div></div><div class="host u-font-sans">2016 IEEE Int. Conf. Real-Time Comput. Robot. RCAR (2016), pp. 427-430, <a class="anchor anchor-primary" href="https://doi.org/10.1109/RCAR.2016.7784067" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/RCAR.2016.7784067</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85010075925&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0255"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20fixed%20length%20micro-expressions%20sequence%20and%20weighting%20method&amp;publication_year=2016&amp;author=M.%20Chen&amp;author=H.T.%20Ma&amp;author=J.%20Li&amp;author=H.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0255"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0256" id="ref-id-bib0256" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[256]</span></span></a></span><span class="reference" id="sbref0256"><div class="contribution"><div class="authors u-font-sans">K. Simonyan, A. Zisserman</div><div id="ref-id-sbref0256" class="title text-m">Very deep convolutional networks for large-scale image recognition</div></div><div class="host u-font-sans">Int. Conf. Learn. Represent (2015)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Very%20deep%20convolutional%20networks%20for%20large-scale%20image%20recognition&amp;publication_year=2015&amp;author=K.%20Simonyan&amp;author=A.%20Zisserman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0256"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0257" id="ref-id-bib0257" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[257]</span></span></a></span><span class="reference" id="sbref0257"><div class="contribution"><div class="authors u-font-sans">O.M. Parkhi, A. Vedaldi, A. Zisserman</div><div id="ref-id-sbref0257" class="title text-m">Deep face recognition</div></div><div class="host u-font-sans">Procedings Br. Mach. Vis. Conf. 2015, British Machine Vision Association, Swansea (2015), pp. 41.1-41.12, <a class="anchor anchor-primary" href="https://doi.org/10.5244/C.29.41" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.5244/C.29.41</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20face%20recognition&amp;publication_year=2015&amp;author=O.M.%20Parkhi&amp;author=A.%20Vedaldi&amp;author=A.%20Zisserman" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0257"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0258" id="ref-id-bib0258" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[258]</span></span></a></span><span class="reference" id="sbref0258"><div class="contribution"><div class="authors u-font-sans">K. He, X. Zhang, S. Ren, J. Sun</div><div id="ref-id-sbref0258" class="title text-m">Deep residual learning for image recognition</div></div><div class="host u-font-sans">2016 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR (2016), pp. 770-778, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2016.90" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2016.90</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20residual%20learning%20for%20image%20recognition&amp;publication_year=2016&amp;author=K.%20He&amp;author=X.%20Zhang&amp;author=S.%20Ren&amp;author=J.%20Sun" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0258"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0259" id="ref-id-bib0259" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[259]</span></span></a></span><span class="reference" id="sbref0259"><div class="contribution"><div class="authors u-font-sans">C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich</div><div id="ref-id-sbref0259" class="title text-m">Going deeper with convolutions</div></div><div class="host u-font-sans">2015 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR (2015), pp. 1-9, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2015.7298594" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2015.7298594</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Going%20deeper%20with%20convolutions&amp;publication_year=2015&amp;author=C.%20Szegedy&amp;author=Wei%20Liu&amp;author=Yangqing%20Jia&amp;author=P.%20Sermanet&amp;author=S.%20Reed&amp;author=D.%20Anguelov&amp;author=D.%20Erhan&amp;author=V.%20Vanhoucke&amp;author=A.%20Rabinovich" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0259"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0260" id="ref-id-bib0260" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[260]</span></span></a></span><span class="reference" id="sbref0260"><div class="contribution"><div class="authors u-font-sans">H. Yang, U. Ciftci, L. Yin</div><div id="ref-id-sbref0260" class="title text-m">Facial expression recognition by de-expression residue learning</div></div><div class="host u-font-sans">2018 IEEECVF Conf. Comput. Vis. Pattern Recognit., IEEE, Salt Lake City, UT (2018), pp. 2168-2177, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2018.00231" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2018.00231</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054499387&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0260"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20by%20de-expression%20residue%20learning&amp;publication_year=2018&amp;author=H.%20Yang&amp;author=U.%20Ciftci&amp;author=L.%20Yin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0260"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0261" id="ref-id-bib0261" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[261]</span></span></a></span><span class="reference" id="sbref0261"><div class="contribution"><div class="authors u-font-sans">C. Wang, M. Peng, T. Bi, T. Chen</div><div id="ref-id-sbref0261" class="title text-m">Micro-attention for micro-expression recognition</div></div><div class="host u-font-sans">Neurocomputing, 410 (2020), pp. 354-362, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.06.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2020.06.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231220309711/pdfft?md5=218c2e9d25200ded95c28f0418d45ded&amp;pid=1-s2.0-S0925231220309711-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0261"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231220309711" aria-describedby="ref-id-sbref0261"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85086991968&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0261"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Micro-attention%20for%20micro-expression%20recognition&amp;publication_year=2020&amp;author=C.%20Wang&amp;author=M.%20Peng&amp;author=T.%20Bi&amp;author=T.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0261"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0262" id="ref-id-bib0262" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[262]</span></span></a></span><span class="reference" id="sbref0262"><div class="contribution"><div class="authors u-font-sans">X. Liu, B.V.K. Vijaya Kumar, P. Jia, J. You</div><div id="ref-id-sbref0262" class="title text-m">Hard negative generation for identity-disentangled facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit., 88 (2019), pp. 1-12, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2018.11.001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2018.11.001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320318303819/pdfft?md5=17bb1b79fcfea76101c51db1a7764400&amp;pid=1-s2.0-S0031320318303819-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0262"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320318303819" aria-describedby="ref-id-sbref0262"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Hard%20negative%20generation%20for%20identity-disentangled%20facial%20expression%20recognition&amp;publication_year=2019&amp;author=X.%20Liu&amp;author=B.V.K.%20Vijaya%20Kumar&amp;author=P.%20Jia&amp;author=J.%20You" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0262"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0263" id="ref-id-bib0263" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[263]</span></span></a></span><span class="reference" id="sbref0263"><div class="contribution"><div class="authors u-font-sans">Z. Meng, P. Liu, J. Cai, S. Han, Y. Tong</div><div id="ref-id-sbref0263" class="title text-m">Identity-aware convolutional neural network for facial expression recognition</div></div><div class="host u-font-sans">2017 12th IEEE Int. Conf. Autom. Face Gesture Recognit. FG 2017 (2017), pp. 558-565, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2017.140" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2017.140</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85026295599&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0263"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Identity-aware%20convolutional%20neural%20network%20for%20facial%20expression%20recognition&amp;publication_year=2017&amp;author=Z.%20Meng&amp;author=P.%20Liu&amp;author=J.%20Cai&amp;author=S.%20Han&amp;author=Y.%20Tong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0263"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0264" id="ref-id-bib0264" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[264]</span></span></a></span><span class="reference" id="sbref0264"><div class="contribution"><div class="authors u-font-sans">Z. Wang, F. Zeng, S. Liu, B. Zeng</div><div id="ref-id-sbref0264" class="title text-m">OAENet: oriented attention ensemble for accurate facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit, 112 (2021), Article 107694, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2020.107694" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2020.107694</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320320304970/pdfft?md5=8cb1cce568deff7dc16cc1da24ccbc88&amp;pid=1-s2.0-S0031320320304970-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0264"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320320304970" aria-describedby="ref-id-sbref0264"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85096601394&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0264"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=OAENet%3A%20oriented%20attention%20ensemble%20for%20accurate%20facial%20expression%20recognition&amp;publication_year=2021&amp;author=Z.%20Wang&amp;author=F.%20Zeng&amp;author=S.%20Liu&amp;author=B.%20Zeng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0264"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0265" id="ref-id-bib0265" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[265]</span></span></a></span><span class="reference" id="sbref0265"><div class="contribution"><div class="authors u-font-sans">H. Li, N. Wang, Y. Yu, X. Yang, X. Gao</div><div id="ref-id-sbref0265" class="title text-m">LBAN-IL: A novel method of high discriminative representation for facial expression recognition</div></div><div class="host u-font-sans">Neurocomputing, 432 (2021), pp. 159-169, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.12.076" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2020.12.076</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231220319767/pdfft?md5=ece0709ca16e8a5aa2cf48b5330e8db7&amp;pid=1-s2.0-S0925231220319767-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0265"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231220319767" aria-describedby="ref-id-sbref0265"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85099244747&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0265"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=LBAN-IL%3A%20A%20novel%20method%20of%20high%20discriminative%20representation%20for%20facial%20expression%20recognition&amp;publication_year=2021&amp;author=H.%20Li&amp;author=N.%20Wang&amp;author=Y.%20Yu&amp;author=X.%20Yang&amp;author=X.%20Gao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0265"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0266" id="ref-id-bib0266" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[266]</span></span></a></span><span class="reference" id="sbref0266"><div class="contribution"><div class="authors u-font-sans">P.D.M. Fernandez, F.A.G. Pena, T.I. Ren, A. Cunha</div><div id="ref-id-sbref0266" class="title text-m">FERAtt: facial expression recognition with attention net</div></div><div class="host u-font-sans">2019 IEEECVF Conf. Comput. Vis. Pattern Recognit. Workshop CVPRW, IEEE, Long Beach, CA, USA (2019), pp. 837-846, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPRW.2019.00112" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPRW.2019.00112</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85081126662&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0266"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=FERAtt%3A%20facial%20expression%20recognition%20with%20attention%20net&amp;publication_year=2019&amp;author=P.D.M.%20Fernandez&amp;author=F.A.G.%20Pena&amp;author=T.I.%20Ren&amp;author=A.%20Cunha" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0266"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0267" id="ref-id-bib0267" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[267]</span></span></a></span><span class="reference" id="sbref0267"><div class="contribution"><div class="authors u-font-sans">S. Xie, H. Hu, Y. Wu</div><div id="ref-id-sbref0267" class="title text-m">Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit., 92 (2019), pp. 177-191, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2019.03.019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2019.03.019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320319301268/pdfft?md5=08c0a5960ed9eb448d57b616913411eb&amp;pid=1-s2.0-S0031320319301268-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0267"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320319301268" aria-describedby="ref-id-sbref0267"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85063739944&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0267"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20multi-path%20convolutional%20neural%20network%20joint%20with%20salient%20region%20attention%20for%20facial%20expression%20recognition&amp;publication_year=2019&amp;author=S.%20Xie&amp;author=H.%20Hu&amp;author=Y.%20Wu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0267"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0268" id="ref-id-bib0268" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[268]</span></span></a></span><span class="reference" id="sbref0268"><div class="contribution"><div class="authors u-font-sans">K. Wang, X. Peng, J. Yang, S. Lu, Y. Qiao</div><div id="ref-id-sbref0268" class="title text-m">Suppressing uncertainties for large-scale facial expression recognition</div></div><div class="host u-font-sans">2020 IEEECVF Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE, Seattle, WA, USA (2020), pp. 6896-6905, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR42600.2020.00693" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR42600.2020.00693</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85092925028&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0268"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Suppressing%20uncertainties%20for%20large-scale%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=K.%20Wang&amp;author=X.%20Peng&amp;author=J.%20Yang&amp;author=S.%20Lu&amp;author=Y.%20Qiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0268"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0269" id="ref-id-bib0269" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[269]</span></span></a></span><span class="reference" id="sbref0269"><div class="contribution"><div class="authors u-font-sans">K. Zhu, Z. Du, W. Li, D. Huang, Y. Wang, L. Chen</div><div id="ref-id-sbref0269" class="title text-m">Discriminative attention-based convolutional neural network for 3D facial expression recognition</div></div><div class="host u-font-sans">2019 14th IEEE Int. Conf. Autom. Face Gesture Recognit. FG 2019 (2019), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2019.8756524" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2019.8756524</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Discriminative%20attention-based%20convolutional%20neural%20network%20for%203D%20facial%20expression%20recognition&amp;publication_year=2019&amp;author=K.%20Zhu&amp;author=Z.%20Du&amp;author=W.%20Li&amp;author=D.%20Huang&amp;author=Y.%20Wang&amp;author=L.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0269"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0270" id="ref-id-bib0270" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[270]</span></span></a></span><span class="reference" id="sbref0270"><div class="contribution"><div class="authors u-font-sans">D. Gera, S. Balasubramanian</div><div id="ref-id-sbref0270" class="title text-m">Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition</div></div><div class="host u-font-sans">Pattern Recognit. Lett., 145 (2021), pp. 58-66, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patrec.2021.01.029" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patrec.2021.01.029</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865521000489/pdfft?md5=4744a0def560a5dd8ab3d07f57599987&amp;pid=1-s2.0-S0167865521000489-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0270"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167865521000489" aria-describedby="ref-id-sbref0270"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85100973974&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0270"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Landmark%20guidance%20independent%20spatio-channel%20attention%20and%20complementary%20context%20information%20based%20facial%20expression%20recognition&amp;publication_year=2021&amp;author=D.%20Gera&amp;author=S.%20Balasubramanian" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0270"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0271" id="ref-id-bib0271" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[271]</span></span></a></span><span class="reference" id="sbref0271"><div class="contribution"><div class="authors u-font-sans">L. Chen, M. Zhou, W. Su, M. Wu, J. She, K. Hirota</div><div id="ref-id-sbref0271" class="title text-m">Softmax regression based deep sparse autoencoder network for facial emotion recognition in human-robot interaction</div></div><div class="host u-font-sans">Inf. Sci., 428 (2018), pp. 49-61, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ins.2017.10.044" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ins.2017.10.044</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0020025517310496/pdfft?md5=198bf5820e323703a5d5335e8c2b7919&amp;pid=1-s2.0-S0020025517310496-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0271"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0020025517310496" aria-describedby="ref-id-sbref0271"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Softmax%20regression%20based%20deep%20sparse%20autoencoder%20network%20for%20facial%20emotion%20recognition%20in%20human-robot%20interaction&amp;publication_year=2018&amp;author=L.%20Chen&amp;author=M.%20Zhou&amp;author=W.%20Su&amp;author=M.%20Wu&amp;author=J.%20She&amp;author=K.%20Hirota" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0271"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0272" id="ref-id-bib0272" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[272]</span></span></a></span><span class="reference" id="sbref0272"><div class="contribution"><div class="authors u-font-sans">Z. Chen, D. Huang, Y. Wang, L. Chen</div><div id="ref-id-sbref0272" class="title text-m">Fast and light manifold CNN based 3D facial expression recognition across pose variations</div></div><div class="host u-font-sans">Proc. 26th ACM Int. Conf. Multimed., Association for Computing Machinery, New York, NY, USA (2018), pp. 229-238, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3240508.3240568" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3240508.3240568</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fast%20and%20light%20manifold%20CNN%20based%203D%20facial%20expression%20recognition%20across%20pose%20variations&amp;publication_year=2018&amp;author=Z.%20Chen&amp;author=D.%20Huang&amp;author=Y.%20Wang&amp;author=L.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0272"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0273" id="ref-id-bib0273" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[273]</span></span></a></span><span class="reference" id="sbref0273"><div class="contribution"><div class="authors u-font-sans">Huibin Li, Jian Sun, Zongben Xu</div><div id="ref-id-sbref0273" class="title text-m">Multimodal 2D+3D facial expression recognition with deep fusion convolutional neural network</div></div><div class="host u-font-sans">IEEE Trans. Multimed. (2017), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2017.2713408" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2017.2713408</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%202D3D%20facial%20expression%20recognition%20with%20deep%20fusion%20convolutional%20neural%20network&amp;publication_year=2017&amp;author=Huibin%20Li&amp;author=Jian%20Sun&amp;author=Zongben%20Xu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0273"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0274" id="ref-id-bib0274" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[274]</span></span></a></span><span class="reference" id="sbref0274"><div class="contribution"><div class="authors u-font-sans">S. Li, W. Deng</div><div id="ref-id-sbref0274" class="title text-m">A deeper look at facial expression dataset bias</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.2973158" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.2973158</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20deeper%20look%20at%20facial%20expression%20dataset%20bias&amp;publication_year=2020&amp;author=S.%20Li&amp;author=W.%20Deng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0274"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0275" id="ref-id-bib0275" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[275]</span></span></a></span><span class="reference" id="sbref0275"><div class="contribution"><div class="authors u-font-sans">H. Li, N. Wang, X. Ding, X. Yang, X. Gao</div><div id="ref-id-sbref0275" class="title text-m">Adaptively Learning facial expression representation via C-F labels and distillation</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 30 (2021), pp. 2016-2028, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2021.3049955" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2021.3049955</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85099572650&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0275"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adaptively%20Learning%20facial%20expression%20representation%20via%20C-F%20labels%20and%20distillation&amp;publication_year=2021&amp;author=H.%20Li&amp;author=N.%20Wang&amp;author=X.%20Ding&amp;author=X.%20Yang&amp;author=X.%20Gao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0275"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0276" id="ref-id-bib0276" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[276]</span></span></a></span><span class="reference" id="sbref0276"><div class="contribution"><div class="authors u-font-sans">D. Tran, L. Bourdev, R. Fergus, L. Torresani, M. Paluri</div><div id="ref-id-sbref0276" class="title text-m">Learning spatiotemporal features with 3D convolutional networks</div></div><div class="host u-font-sans">2015 IEEE Int. Conf. Comput. Vis. ICCV (2015), pp. 4489-4497, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCV.2015.510" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCV.2015.510</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84973865953&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0276"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20spatiotemporal%20features%20with%203D%20convolutional%20networks&amp;publication_year=2015&amp;author=D.%20Tran&amp;author=L.%20Bourdev&amp;author=R.%20Fergus&amp;author=L.%20Torresani&amp;author=M.%20Paluri" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0276"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0277" id="ref-id-bib0277" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[277]</span></span></a></span><span class="reference" id="sbref0277"><div class="contribution"><div class="authors u-font-sans">D.A.A. CHANTI, A. Caplier</div><div id="ref-id-sbref0277" class="title text-m">Deep learning for spatio-temporal modeling of dynamic spontaneous emotions</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2018), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2018.2873600" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2018.2873600</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20for%20spatio-temporal%20modeling%20of%20dynamic%20spontaneous%20emotions&amp;publication_year=2018&amp;author=D.A.A.%20CHANTI&amp;author=A.%20Caplier" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0277"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0278" id="ref-id-bib0278" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[278]</span></span></a></span><span class="reference" id="sbref0278"><div class="contribution"><div class="authors u-font-sans">D. Nguyen, K. Nguyen, S. Sridharan, A. Ghasemi, D. Dean, C. Fookes</div><div id="ref-id-sbref0278" class="title text-m">Deep spatio-temporal features for multimodal emotion recognition</div></div><div class="host u-font-sans">2017 IEEE Winter Conf. Appl. Comput. Vis. WACV (2017), pp. 1215-1223, <a class="anchor anchor-primary" href="https://doi.org/10.1109/WACV.2017.140" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/WACV.2017.140</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85020205903&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0278"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20spatio-temporal%20features%20for%20multimodal%20emotion%20recognition&amp;publication_year=2017&amp;author=D.%20Nguyen&amp;author=K.%20Nguyen&amp;author=S.%20Sridharan&amp;author=A.%20Ghasemi&amp;author=D.%20Dean&amp;author=C.%20Fookes" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0278"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0279" id="ref-id-bib0279" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[279]</span></span></a></span><span class="reference" id="sbref0279"><div class="contribution"><div class="authors u-font-sans">Y. Wang, H. Ma, X. Xing, Z. Pan</div><div id="ref-id-sbref0279" class="title text-m">Eulerian motion based 3DCNN architecture for facial micro-expression recognition</div></div><div class="host u-font-sans">Y.M. Ro, W.-H. Cheng, J. Kim, W.-T. Chu, P. Cui, J.-W. Choi, M.-C. Hu, W. De Neve (Eds.), Multimed. Model, Springer International Publishing, Cham (2020), pp. 266-277, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-030-37731-1_22" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-030-37731-1_22</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Eulerian%20motion%20based%203DCNN%20architecture%20for%20facial%20micro-expression%20recognition&amp;publication_year=2020&amp;author=Y.%20Wang&amp;author=H.%20Ma&amp;author=X.%20Xing&amp;author=Z.%20Pan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0279"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0280" id="ref-id-bib0280" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[280]</span></span></a></span><span class="reference" id="sbref0280"><div class="contribution"><div class="authors u-font-sans">L. Lo, H.-X. Xie, H.-H. Shuai, W.-H. Cheng</div><div id="ref-id-sbref0280" class="title text-m">MER-GCN: micro-expression recognition based on relation modeling with graph convolutional networks</div></div><div class="host u-font-sans">2020 IEEE Conf. Multimed. Inf. Process. Retr. MIPR (2020), pp. 79-84, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MIPR49039.2020.00023" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MIPR49039.2020.00023</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85092170102&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0280"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=MER-GCN%3A%20micro-expression%20recognition%20based%20on%20relation%20modeling%20with%20graph%20convolutional%20networks&amp;publication_year=2020&amp;author=L.%20Lo&amp;author=H.-X.%20Xie&amp;author=H.-H.%20Shuai&amp;author=W.-H.%20Cheng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0280"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0281" id="ref-id-bib0281" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[281]</span></span></a></span><span class="reference" id="sbref0281"><div class="contribution"><div class="authors u-font-sans">D.H. Kim, W.J. Baddar, J. Jang, Y.M. Ro</div><div id="ref-id-sbref0281" class="title text-m">Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 10 (2019), pp. 223-236, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2695999" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2695999</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85066636077&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0281"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-objective%20based%20spatio-temporal%20feature%20representation%20learning%20robust%20to%20expression%20intensity%20variations%20for%20facial%20expression%20recognition&amp;publication_year=2019&amp;author=D.H.%20Kim&amp;author=W.J.%20Baddar&amp;author=J.%20Jang&amp;author=Y.M.%20Ro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0281"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0282" id="ref-id-bib0282" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[282]</span></span></a></span><span class="reference" id="sbref0282"><div class="contribution"><div class="authors u-font-sans">M. Behzad, N. Vo, X. Li, G. Zhao</div><div id="ref-id-sbref0282" class="title text-m">Towards reading beyond faces for sparsity-aware 3D/4D affect recognition</div></div><div class="host u-font-sans">Neurocomputing, 458 (2021), pp. 297-307, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2021.06.023" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2021.06.023</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231221009279/pdfft?md5=112adcac1ba987a8d0b8a69740de5771&amp;pid=1-s2.0-S0925231221009279-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0282"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231221009279" aria-describedby="ref-id-sbref0282"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85108611061&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0282"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20reading%20beyond%20faces%20for%20sparsity-aware%203D4D%20affect%20recognition&amp;publication_year=2021&amp;author=M.%20Behzad&amp;author=N.%20Vo&amp;author=X.%20Li&amp;author=G.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0282"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0283" id="ref-id-bib0283" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[283]</span></span></a></span><span class="reference" id="sbref0283"><div class="contribution"><div class="authors u-font-sans">D.H. Kim, W.J. Baddar, Y.M. Ro</div><div id="ref-id-sbref0283" class="title text-m">Micro-expression recognition with expression-state constrained spatio-temporal feature representations</div></div><div class="host u-font-sans">Proc. 24th ACM Int. Conf. Multimed., Association for Computing Machinery, New York, NY, USA (2016), pp. 382-386, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2964284.2967247" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2964284.2967247</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84994589917&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0283"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Micro-expression%20recognition%20with%20expression-state%20constrained%20spatio-temporal%20feature%20representations&amp;publication_year=2016&amp;author=D.H.%20Kim&amp;author=W.J.%20Baddar&amp;author=Y.M.%20Ro" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0283"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0284" id="ref-id-bib0284" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[284]</span></span></a></span><span class="reference" id="sbref0284"><div class="contribution"><div class="authors u-font-sans">Z. Xia, X. Hong, X. Gao, X. Feng, G. Zhao</div><div id="ref-id-sbref0284" class="title text-m">Spatiotemporal recurrent convolutional networks for recognizing spontaneous micro-expressions</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 22 (2020), pp. 626-640, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2019.2931351" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2019.2931351</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85081046951&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0284"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatiotemporal%20recurrent%20convolutional%20networks%20for%20recognizing%20spontaneous%20micro-expressions&amp;publication_year=2020&amp;author=Z.%20Xia&amp;author=X.%20Hong&amp;author=X.%20Gao&amp;author=X.%20Feng&amp;author=G.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0284"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0285" id="ref-id-bib0285" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[285]</span></span></a></span><span class="reference" id="sbref0285"><div class="contribution"><div class="authors u-font-sans">D. Kollias, S.P. Zafeiriou</div><div id="ref-id-sbref0285" class="title text-m">Exploiting multi-CNN features in CNN-RNN based dimensional emotion recognition on the OMG in-the-wild dataset</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.3014171" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.3014171</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Exploiting%20multi-CNN%20features%20in%20CNN-RNN%20based%20dimensional%20emotion%20recognition%20on%20the%20OMG%20in-the-wild%20dataset&amp;publication_year=2020&amp;author=D.%20Kollias&amp;author=S.P.%20Zafeiriou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0285"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0286" id="ref-id-bib0286" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[286]</span></span></a></span><span class="reference" id="sbref0286"><div class="contribution"><div class="authors u-font-sans">D. Liu, X. Ouyang, S. Xu, P. Zhou, K. He, S. Wen</div><div id="ref-id-sbref0286" class="title text-m">SAANet: Siamese action-units attention network for improving dynamic facial expression recognition</div></div><div class="host u-font-sans">Neurocomputing, 413 (2020), pp. 145-157, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.06.062" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2020.06.062</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S092523122031050X/pdfft?md5=70646d2b6569dcd491ca2f7ae4523a6a&amp;pid=1-s2.0-S092523122031050X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0286"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S092523122031050X" aria-describedby="ref-id-sbref0286"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85088093498&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0286"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SAANet%3A%20Siamese%20action-units%20attention%20network%20for%20improving%20dynamic%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=D.%20Liu&amp;author=X.%20Ouyang&amp;author=S.%20Xu&amp;author=P.%20Zhou&amp;author=K.%20He&amp;author=S.%20Wen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0286"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0287" id="ref-id-bib0287" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[287]</span></span></a></span><span class="reference" id="sbref0287"><div class="contribution"><div class="authors u-font-sans">F. Zhang, T. Zhang, Q. Mao, C. Xu</div><div id="ref-id-sbref0287" class="title text-m">Geometry guided pose-invariant facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 29 (2020), pp. 4445-4460, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2020.2972114" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2020.2972114</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85081045969&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0287"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Geometry%20guided%20pose-invariant%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=F.%20Zhang&amp;author=T.%20Zhang&amp;author=Q.%20Mao&amp;author=C.%20Xu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0287"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0288" id="ref-id-bib0288" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[288]</span></span></a></span><span class="reference" id="sbref0288"><div class="contribution"><div class="authors u-font-sans">H. Yang, Z. Zhang, L. Yin</div><div id="ref-id-sbref0288" class="title text-m">Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks</div></div><div class="host u-font-sans">2018 13th IEEE Int. Conf. Autom. Face Gesture Recognit. FG 2018 (2018), pp. 294-301, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2018.00050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2018.00050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85049396051&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0288"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Identity-adaptive%20facial%20expression%20recognition%20through%20expression%20regeneration%20using%20conditional%20generative%20adversarial%20networks&amp;publication_year=2018&amp;author=H.%20Yang&amp;author=Z.%20Zhang&amp;author=L.%20Yin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0288"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0289" id="ref-id-bib0289" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[289]</span></span></a></span><span class="reference" id="sbref0289"><div class="contribution"><div class="authors u-font-sans">Y. Fu, X. Wu, X. Li, Z. Pan, D. Luo</div><div id="ref-id-sbref0289" class="title text-m">Semantic neighborhood-aware deep facial expression recognition</div></div><div class="host u-font-sans">IEEE Trans. Image Process., 29 (2020), pp. 6535-6548, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TIP.2020.2991510" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TIP.2020.2991510</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85087542679&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0289"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Semantic%20neighborhood-aware%20deep%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=Y.%20Fu&amp;author=X.%20Wu&amp;author=X.%20Li&amp;author=Z.%20Pan&amp;author=D.%20Luo" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0289"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0290" id="ref-id-bib0290" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[290]</span></span></a></span><span class="reference" id="sbref0290"><div class="other-ref"><span>K. Ali, C.E. Hughes, Facial expression recognition using disentangled adversarial learning, ArXiv190913135 Cs. (2019). <a class="anchor anchor-primary" href="http://arxiv.org/abs/1909.13135" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/1909.13135</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a> (accessed September 23, 2020).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=K.%20Ali%2C%20C.E.%20Hughes%2C%20Facial%20expression%20recognition%20using%20disentangled%20adversarial%20learning%2C%20ArXiv190913135%20Cs.%20(2019).%20http%3A%2F%2Farxiv.org%2Fabs%2F1909.13135%20(accessed%20September%2023%2C%202020)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0291" id="ref-id-bib0291" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[291]</span></span></a></span><span class="reference" id="sbref0291"><div class="contribution"><div class="authors u-font-sans">J. Yu, C. Zhang, Y. Song, W. Cai</div><div id="ref-id-sbref0291" class="title text-m">ICE-GAN: identity-aware and capsule-enhanced GAN with graph-based reasoning for micro-expression recognition and synthesis</div></div><div class="host u-font-sans">2021 Int. Jt. Conf. Neural Netw. IJCNN (2021), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/IJCNN52387.2021.9533988" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/IJCNN52387.2021.9533988</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85098855538&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0291"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=ICE-GAN%3A%20identity-aware%20and%20capsule-enhanced%20GAN%20with%20graph-based%20reasoning%20for%20micro-expression%20recognition%20and%20synthesis&amp;publication_year=2021&amp;author=J.%20Yu&amp;author=C.%20Zhang&amp;author=Y.%20Song&amp;author=W.%20Cai" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0291"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0292" id="ref-id-bib0292" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[292]</span></span></a></span><span class="reference" id="sbref0292"><div class="contribution"><div class="authors u-font-sans">S. Piana, A. Staglianò, F. Odone, A. Camurri</div><div id="ref-id-sbref0292" class="title text-m">Adaptive body gesture representation for automatic emotion recognition</div></div><div class="host u-font-sans">ACM Trans. Interact. Intell. Syst., 6 (2016), pp. 1-31, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2818740" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2818740</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adaptive%20body%20gesture%20representation%20for%20automatic%20emotion%20recognition&amp;publication_year=2016&amp;author=S.%20Piana&amp;author=A.%20Staglian%C3%B2&amp;author=F.%20Odone&amp;author=A.%20Camurri" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0292"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0293" id="ref-id-bib0293" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[293]</span></span></a></span><span class="reference" id="sbref0293"><div class="contribution"><div class="authors u-font-sans">D. Stoeva, M. Gelautz</div><div id="ref-id-sbref0293" class="title text-m">Body language in affective human-robot interaction</div></div><div class="host u-font-sans">Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction, ACM, Cambridge United Kingdom (2020), pp. 606-608, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3371382.3377432" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3371382.3377432</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85083213455&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0293"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Body%20language%20in%20affective%20human-robot%20interaction&amp;publication_year=2020&amp;author=D.%20Stoeva&amp;author=M.%20Gelautz" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0293"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0294" id="ref-id-bib0294" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[294]</span></span></a></span><span class="reference" id="sbref0294"><div class="contribution"><div class="authors u-font-sans">Y. Yang, D. Ramanan</div><div id="ref-id-sbref0294" class="title text-m">Articulated human detection with flexible mixtures of parts</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 35 (2013), pp. 2878-2890, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2012.261" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2012.261</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84887598018&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0294"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Articulated%20human%20detection%20with%20flexible%20mixtures%20of%20parts&amp;publication_year=2013&amp;author=Y.%20Yang&amp;author=D.%20Ramanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0294"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0295" id="ref-id-bib0295" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[295]</span></span></a></span><span class="reference" id="sbref0295"><div class="contribution"><div class="authors u-font-sans">S. Ren, K. He, R. Girshick, J. Sun, R-CNN Faster</div><div id="ref-id-sbref0295" class="title text-m">Towards real-time object detection with region proposal networks</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 39 (2017), pp. 1137-1149, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2016.2577031" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2016.2577031</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks&amp;publication_year=2017&amp;author=S.%20Ren&amp;author=K.%20He&amp;author=R.%20Girshick&amp;author=J.%20Sun&amp;author=R-CNN%20Faster" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0295"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0296" id="ref-id-bib0296" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[296]</span></span></a></span><span class="reference" id="sbref0296"><div class="contribution"><div class="authors u-font-sans">W. Weiyi, E. Valentin, S. Hichem</div><div id="ref-id-sbref0296" class="title text-m">Adaptive real-time emotion recognition from body movements</div></div><div class="host u-font-sans">ACM Trans. Interact. Intell. Syst. (2015)</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://dl.acm.org/doi/abs/10.1145/2738221" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://dl.acm.org/doi/abs/10.1145/2738221</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed November 2, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Adaptive%20real-time%20emotion%20recognition%20from%20body%20movements&amp;publication_year=2015&amp;author=W.%20Weiyi&amp;author=E.%20Valentin&amp;author=S.%20Hichem" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0296"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0297" id="ref-id-bib0297" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[297]</span></span></a></span><span class="reference" id="sbref0297"><div class="contribution"><div class="authors u-font-sans">A. Kleinsmith, N. Bianchi-Berthouze</div><div id="ref-id-sbref0297" class="title text-m">Recognizing affective dimensions from body posture</div></div><div class="host u-font-sans">A.C.R. Paiva, R. Prada, R.W. Picard (Eds.), Affective Computing and Intelligent Interaction, Springer, Berlin, Heidelberg (2007), pp. 48-58, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-540-74889-2_5" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-540-74889-2_5</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-38049068674&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0297"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognizing%20affective%20dimensions%20from%20body%20posture&amp;publication_year=2007&amp;author=A.%20Kleinsmith&amp;author=N.%20Bianchi-Berthouze" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0297"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0298" id="ref-id-bib0298" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[298]</span></span></a></span><span class="reference" id="sbref0298"><div class="contribution"><div class="authors u-font-sans">G. Castellano, S.D. Villalba</div><div id="ref-id-sbref0298" class="title text-m">Recognising human emotions from body movement and gesture dynamics</div></div><div class="host u-font-sans">Affective Computing and Intelligent Interaction, Springer, Berlin, Heidelberg (2007), pp. 71-82, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-540-74889-2_7" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-540-74889-2_7</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-38049058694&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0298"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognising%20human%20emotions%20from%20body%20movement%20and%20gesture%20dynamics&amp;publication_year=2007&amp;author=G.%20Castellano&amp;author=S.D.%20Villalba" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0298"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0299" id="ref-id-bib0299" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[299]</span></span></a></span><span class="reference" id="sbref0299"><div class="contribution"><div class="authors u-font-sans">S. Saha, S. Datta, A. Konar, R. Janarthanan</div><div id="ref-id-sbref0299" class="title text-m">A study on emotion recognition from body gestures using Kinect sensor</div></div><div class="host u-font-sans">2014 Int. Conf. Commun. Signal Process. (2014), pp. 056-060, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICCSP.2014.6949798" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICCSP.2014.6949798</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20study%20on%20emotion%20recognition%20from%20body%20gestures%20using%20Kinect%20sensor&amp;publication_year=2014&amp;author=S.%20Saha&amp;author=S.%20Datta&amp;author=A.%20Konar&amp;author=R.%20Janarthanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0299"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0300" id="ref-id-bib0300" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[300]</span></span></a></span><span class="reference" id="sbref0300"><div class="contribution"><div class="authors u-font-sans">Y. Maret, D. Oberson, M. Gavrilova</div><div id="ref-id-sbref0300" class="title text-m">11111</div></div><div class="host u-font-sans">L. Rutkowski, R. Scherer, M. Korytkowski, W. Pedrycz, R. Tadeusiewicz, J.M. Zurada (Eds.), Artificial Intelligence and Soft Computing, Springer International Publishing, Cham (2018), pp. 474-485, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-91253-0_44" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-91253-0_44</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048049315&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0300"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=11111&amp;publication_year=2018&amp;author=Y.%20Maret&amp;author=D.%20Oberson&amp;author=M.%20Gavrilova" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0300"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0301" id="ref-id-bib0301" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[301]</span></span></a></span><span class="reference" id="sbref0301"><div class="contribution"><div class="authors u-font-sans">S. Senecal, L. Cuel, A. Aristidou, N. Magnenat-Thalmann</div><div id="ref-id-sbref0301" class="title text-m">Continuous body emotion recognition system during theater performances</div></div><div class="host u-font-sans">Comput. Animat. Virtual Worlds., 27 (2016), pp. 311-320, <a class="anchor anchor-primary" href="https://doi.org/10.1002/cav.1714" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1002/cav.1714</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84969695688&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0301"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Continuous%20body%20emotion%20recognition%20system%20during%20theater%20performances&amp;publication_year=2016&amp;author=S.%20Senecal&amp;author=L.%20Cuel&amp;author=A.%20Aristidou&amp;author=N.%20Magnenat-Thalmann" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0301"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0302" id="ref-id-bib0302" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[302]</span></span></a></span><span class="reference" id="sbref0302"><div class="contribution"><div class="authors u-font-sans">D. Glowinski, N. Dael, A. Camurri, G. Volpe, M. Mortillaro, K. Scherer</div><div id="ref-id-sbref0302" class="title text-m">Toward a minimal representation of affective gestures</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 2 (2011), pp. 106-118, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.7" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.7</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84857910982&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0302"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Toward%20a%20minimal%20representation%20of%20affective%20gestures&amp;publication_year=2011&amp;author=D.%20Glowinski&amp;author=N.%20Dael&amp;author=A.%20Camurri&amp;author=G.%20Volpe&amp;author=M.%20Mortillaro&amp;author=K.%20Scherer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0302"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0303" id="ref-id-bib0303" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[303]</span></span></a></span><span class="reference" id="sbref0303"><div class="contribution"><div class="authors u-font-sans">M.A. Razzaq, J. Bang, S.S. Kang, S. Lee</div><div id="ref-id-sbref0303" class="title text-m">UnSkEm: unobtrusive skeletal-based emotion recognition for user experience</div></div><div class="host u-font-sans">2020 Int. Conf. Inf. Netw. ICOIN (2020), pp. 92-96, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICOIN48656.2020.9016601" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICOIN48656.2020.9016601</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85082140430&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0303"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=UnSkEm%3A%20unobtrusive%20skeletal-based%20emotion%20recognition%20for%20user%20experience&amp;publication_year=2020&amp;author=M.A.%20Razzaq&amp;author=J.%20Bang&amp;author=S.S.%20Kang&amp;author=S.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0303"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0304" id="ref-id-bib0304" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[304]</span></span></a></span><span class="reference" id="sbref0304"><div class="contribution"><div class="authors u-font-sans">R. Santhoshkumar, M. Kalaiselvi Geetha</div><div id="ref-id-sbref0304" class="title text-m">Vision-based human emotion recognition using HOG-KLT feature</div></div><div class="host u-font-sans">P.K. Singh, W. Pawłowski, S. Tanwar, N. Kumar, J.J.P.C. Rodrigues, M.S. Obaidat (Eds.), Proceedings of First International Conference on Computing, Communications, and Cyber-Security (IC4S 2019), Springer, Singapore (2020), pp. 261-272, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-15-3369-3_20" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-15-3369-3_20</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85085321547&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0304"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Vision-based%20human%20emotion%20recognition%20using%20HOG-KLT%20feature&amp;publication_year=2020&amp;author=R.%20Santhoshkumar&amp;author=M.%20Kalaiselvi%20Geetha" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0304"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0305" id="ref-id-bib0305" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[305]</span></span></a></span><span class="reference" id="sbref0305"><div class="contribution"><div class="authors u-font-sans">R. Santhoshkumar, M.Kalaiselvi Geetha</div><div id="ref-id-sbref0305" class="title text-m">Human emotion recognition using body expressive feature</div></div><div class="host u-font-sans">A. Chaudhary, C. Choudhary, M.K. Gupta, C. Lal, T. Badal (Eds.), Microservices in Big Data Analytics, Springer, Singapore (2020), pp. 141-149, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-981-15-0128-9_13" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-981-15-0128-9_13</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Human%20emotion%20recognition%20using%20body%20expressive%20feature&amp;publication_year=2020&amp;author=R.%20Santhoshkumar&amp;author=M.Kalaiselvi%20Geetha" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0305"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0306" id="ref-id-bib0306" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[306]</span></span></a></span><span class="reference" id="sbref0306"><div class="contribution"><div class="authors u-font-sans">A. Kapur, A. Kapur, N. Virji-Babul, G. Tzanetakis, P.F. Driessen</div><div id="ref-id-sbref0306" class="title text-m">Gesture-based affective computing on motion capture data</div></div><div class="host u-font-sans">J. Tao, T. Tan, R.W. Picard (Eds.), Affective Computing and Intelligent Interaction, Springer, Berlin, Heidelberg (2005), pp. 1-7, <a class="anchor anchor-primary" href="https://doi.org/10.1007/11573548_1" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/11573548_1</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-33646817105&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0306"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Gesture-based%20affective%20computing%20on%20motion%20capture%20data&amp;publication_year=2005&amp;author=A.%20Kapur&amp;author=A.%20Kapur&amp;author=N.%20Virji-Babul&amp;author=G.%20Tzanetakis&amp;author=P.F.%20Driessen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0306"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0307" id="ref-id-bib0307" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[307]</span></span></a></span><span class="reference" id="sbref0307"><div class="contribution"><div class="authors u-font-sans">A. Kleinsmith, N. Bianchi-Berthouze, A. Steed</div><div id="ref-id-sbref0307" class="title text-m">Automatic recognition of non-acted affective postures</div></div><div class="host u-font-sans">IEEE Trans. Syst. Man Cybern. Part B Cybern., 41 (2011), pp. 1027-1038, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TSMCB.2010.2103557" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TSMCB.2010.2103557</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79960699221&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0307"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20recognition%20of%20non-acted%20affective%20postures&amp;publication_year=2011&amp;author=A.%20Kleinsmith&amp;author=N.%20Bianchi-Berthouze&amp;author=A.%20Steed" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0307"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0308" id="ref-id-bib0308" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[308]</span></span></a></span><span class="reference" id="sbref0308"><div class="contribution"><div class="authors u-font-sans">E.P. Volkova, B.J. Mohler, T.J. Dodds, J. Tesch, H.H. Bülthoff</div><div id="ref-id-sbref0308" class="title text-m">Emotion categorization of body expressions in narrative scenarios</div></div><div class="host u-font-sans">Front. Psychol., 5 (2014), <a class="anchor anchor-primary" href="https://doi.org/10.3389/fpsyg.2014.00623" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3389/fpsyg.2014.00623</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20categorization%20of%20body%20expressions%20in%20narrative%20scenarios&amp;publication_year=2014&amp;author=E.P.%20Volkova&amp;author=B.J.%20Mohler&amp;author=T.J.%20Dodds&amp;author=J.%20Tesch&amp;author=H.H.%20B%C3%BClthoff" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0308"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0309" id="ref-id-bib0309" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[309]</span></span></a></span><span class="reference" id="sbref0309"><div class="contribution"><div class="authors u-font-sans">N. Fourati, C. Pelachaud</div><div id="ref-id-sbref0309" class="title text-m">Multi-level classification of emotional body expression</div></div><div class="host u-font-sans">2015 11th IEEE Int. Conf. Workshop Autom. Face Gesture Recognit. FG (2015), pp. 1-8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/FG.2015.7163145" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/FG.2015.7163145</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-level%20classification%20of%20emotional%20body%20expression&amp;publication_year=2015&amp;author=N.%20Fourati&amp;author=C.%20Pelachaud" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0309"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0310" id="ref-id-bib0310" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[310]</span></span></a></span><span class="reference" id="sbref0310"><div class="contribution"><div class="authors u-font-sans">Z. Shen, J. Cheng, X. Hu, Q. Dong</div><div id="ref-id-sbref0310" class="title text-m">Emotion recognition based on multi-view body gestures</div></div><div class="host u-font-sans">2019 IEEE Int. Conf. Image Process. ICIP (2019), pp. 3317-3321, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICIP.2019.8803460" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICIP.2019.8803460</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85076813486&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0310"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20based%20on%20multi-view%20body%20gestures&amp;publication_year=2019&amp;author=Z.%20Shen&amp;author=J.%20Cheng&amp;author=X.%20Hu&amp;author=Q.%20Dong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0310"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0311" id="ref-id-bib0311" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[311]</span></span></a></span><span class="reference" id="sbref0311"><div class="contribution"><div class="authors u-font-sans">R. Santhoshkumar, M.K. Geetha</div><div id="ref-id-sbref0311" class="title text-m">Deep learning approach for emotion recognition from human body movements with feedforward deep convolution neural networks</div></div><div class="host u-font-sans">Procedia Comput. Sci., 152 (2019), pp. 158-165, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.procs.2019.05.038" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.procs.2019.05.038</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1877050919306908/pdf?md5=ab1fa07aacc2e051fb461dcb421f0c5b&amp;pid=1-s2.0-S1877050919306908-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0311"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1877050919306908" aria-describedby="ref-id-sbref0311"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068399966&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0311"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20approach%20for%20emotion%20recognition%20from%20human%20body%20movements%20with%20feedforward%20deep%20convolution%20neural%20networks&amp;publication_year=2019&amp;author=R.%20Santhoshkumar&amp;author=M.K.%20Geetha" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0311"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0312" id="ref-id-bib0312" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[312]</span></span></a></span><span class="reference" id="sbref0312"><div class="contribution"><div class="authors u-font-sans">S.T. Ly, G.-S. Lee, S.-H. Kim, H.-J. Yang</div><div id="ref-id-sbref0312" class="title text-m">Emotion recognition via body gesture: deep learning model coupled with keyframe selection</div></div><div class="host u-font-sans">Proc. 2018 Int. Conf. Mach. Learn. Mach. Intell., Association for Computing Machinery, New York, NY, USA (2018), pp. 27-31, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3278312.3278313" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3278312.3278313</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85059886069&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0312"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20via%20body%20gesture%3A%20deep%20learning%20model%20coupled%20with%20keyframe%20selection&amp;publication_year=2018&amp;author=S.T.%20Ly&amp;author=G.-S.%20Lee&amp;author=S.-H.%20Kim&amp;author=H.-J.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0312"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0313" id="ref-id-bib0313" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[313]</span></span></a></span><span class="reference" id="sbref0313"><div class="contribution"><div class="authors u-font-sans">J. Wu, Y. Zhang, S. Sun, Q. Li, X. Zhao</div><div id="ref-id-sbref0313" class="title text-m">Generalized zero-shot emotion recognition from body gestures</div></div><div class="host u-font-sans">Appl. Intell. (2021), <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10489-021-02927-w" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10489-021-02927-w</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Generalized%20zero-shot%20emotion%20recognition%20from%20body%20gestures&amp;publication_year=2021&amp;author=J.%20Wu&amp;author=Y.%20Zhang&amp;author=S.%20Sun&amp;author=Q.%20Li&amp;author=X.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0313"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0314" id="ref-id-bib0314" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[314]</span></span></a></span><span class="reference" id="sbref0314"><div class="contribution"><div class="authors u-font-sans">D. Avola, L. Cinque, A. Fagioli, G.L. Foresti, C. Massaroni</div><div id="ref-id-sbref0314" class="title text-m">Deep temporal analysis for non-acted body affect recognition</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), p. 1, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.3003816" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.3003816</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85091111598&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0314"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20temporal%20analysis%20for%20non-acted%20body%20affect%20recognition&amp;publication_year=2020&amp;author=D.%20Avola&amp;author=L.%20Cinque&amp;author=A.%20Fagioli&amp;author=G.L.%20Foresti&amp;author=C.%20Massaroni" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0314"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0315" id="ref-id-bib0315" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[315]</span></span></a></span><span class="reference" id="sbref0315"><div class="contribution"><div class="authors u-font-sans">L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, L.V. Gool</div><div id="ref-id-sbref0315" class="title text-m">Temporal segment networks for action recognition in videos</div></div><div class="host u-font-sans">IEEE Trans. Pattern Anal. Mach. Intell., 41 (2019), pp. 2740-2755, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TPAMI.2018.2868668" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TPAMI.2018.2868668</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85052804139&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0315"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Temporal%20segment%20networks%20for%20action%20recognition%20in%20videos&amp;publication_year=2019&amp;author=L.%20Wang&amp;author=Y.%20Xiong&amp;author=Z.%20Wang&amp;author=Y.%20Qiao&amp;author=D.%20Lin&amp;author=X.%20Tang&amp;author=L.V.%20Gool" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0315"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0316" id="ref-id-bib0316" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[316]</span></span></a></span><span class="reference" id="sbref0316"><div class="contribution"><div class="authors u-font-sans">S. Yan, Y. Xiong, D. Lin</div><div id="ref-id-sbref0316" class="title text-m">Spatial temporal graph convolutional networks for skeleton-based action recognition</div></div><div class="host u-font-sans">Proc. Thirty-Second AAAI Conf. Artif. Intell., AAAI Press (2018)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spatial%20temporal%20graph%20convolutional%20networks%20for%20skeleton-based%20action%20recognition&amp;publication_year=2018&amp;author=S.%20Yan&amp;author=Y.%20Xiong&amp;author=D.%20Lin" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0316"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0317" id="ref-id-bib0317" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[317]</span></span></a></span><span class="reference" id="sbref0317"><div class="contribution"><div class="authors u-font-sans">C.H. Lampert, H. Nickisch, S. Harmeling</div><div id="ref-id-sbref0317" class="title text-m">Learning to detect unseen object classes by between-class attribute transfer</div></div><div class="host u-font-sans">2009 IEEE Conf. Comput. Vis. Pattern Recognit (2009), pp. 951-958, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2009.5206594" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2009.5206594</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-70450172710&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0317"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer&amp;publication_year=2009&amp;author=C.H.%20Lampert&amp;author=H.%20Nickisch&amp;author=S.%20Harmeling" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0317"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0318" id="ref-id-bib0318" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[318]</span></span></a></span><span class="reference" id="sbref0318"><div class="other-ref"><span>A. Banerjee, U. Bhattacharya, A. Bera, Learning unseen emotions from gestures via semantically-conditioned zero-shot perception with adversarial autoencoders, ArXiv200908906 Cs. (2020). <a class="anchor anchor-primary" href="http://arxiv.org/abs/2009.08906" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/2009.08906</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a> (accessed October 28, 2020).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=A.%20Banerjee%2C%20U.%20Bhattacharya%2C%20A.%20Bera%2C%20Learning%20unseen%20emotions%20from%20gestures%20via%20semantically-conditioned%20zero-shot%20perception%20with%20adversarial%20autoencoders%2C%20ArXiv200908906%20Cs.%20(2020).%20http%3A%2F%2Farxiv.org%2Fabs%2F2009.08906%20(accessed%20October%2028%2C%202020)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0319" id="ref-id-bib0319" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[319]</span></span></a></span><span class="reference" id="sbref0319"><div class="contribution"><div class="authors u-font-sans">M. Egger, M. Ley, S. Hanke</div><div id="ref-id-sbref0319" class="title text-m">Emotion recognition from physiological signal analysis: a review</div></div><div class="host u-font-sans">Electron. Notes Theor. Comput. Sci., 343 (2019), pp. 35-55, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.entcs.2019.04.009" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.entcs.2019.04.009</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S157106611930009X/pdf?md5=85290eb14fd158853bb41bb60597f9c4&amp;pid=1-s2.0-S157106611930009X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0319"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S157106611930009X" aria-describedby="ref-id-sbref0319"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85073162689&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0319"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20from%20physiological%20signal%20analysis%3A%20a%20review&amp;publication_year=2019&amp;author=M.%20Egger&amp;author=M.%20Ley&amp;author=S.%20Hanke" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0319"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0320" id="ref-id-bib0320" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[320]</span></span></a></span><span class="reference" id="sbref0320"><div class="contribution"><div class="authors u-font-sans">K. Shirahama, M. Grzegorzek</div><div id="ref-id-sbref0320" class="title text-m">Emotion recognition based on physiological sensor data using codebook approach</div></div><div class="host u-font-sans">E. Piętka, P. Badura, J. Kawa, W. Wieclawek (Eds.), Inf. Technol. Med., Springer International Publishing, Cham (2016), pp. 27-39, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-39904-1_3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-39904-1_3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84976520649&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0320"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20based%20on%20physiological%20sensor%20data%20using%20codebook%20approach&amp;publication_year=2016&amp;author=K.%20Shirahama&amp;author=M.%20Grzegorzek" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0320"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0321" id="ref-id-bib0321" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[321]</span></span></a></span><span class="reference" id="sbref0321"><div class="contribution"><div class="authors u-font-sans">S. Katsigiannis, N. Ramzan</div><div id="ref-id-sbref0321" class="title text-m">DREAMER: a database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices</div></div><div class="host u-font-sans">IEEE J. Biomed. Health Inform., 22 (2018), pp. 98-107, <a class="anchor anchor-primary" href="https://doi.org/10.1109/JBHI.2017.2688239" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/JBHI.2017.2688239</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85040313807&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0321"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=DREAMER%3A%20a%20database%20for%20emotion%20recognition%20through%20EEG%20and%20ECG%20signals%20from%20wireless%20low-cost%20off-the-shelf%20devices&amp;publication_year=2018&amp;author=S.%20Katsigiannis&amp;author=N.%20Ramzan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0321"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0322" id="ref-id-bib0322" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[322]</span></span></a></span><span class="reference" id="sbref0322"><div class="contribution"><div class="authors u-font-sans">C. Li, Z. Zhang, R. Song, J. Cheng, Y. Liu, X. Chen</div><div id="ref-id-sbref0322" class="title text-m">EEG-based emotion recognition via neural architecture search</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2021), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2021.3130387" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2021.3130387</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EEG-based%20emotion%20recognition%20via%20neural%20architecture%20search&amp;publication_year=2021&amp;author=C.%20Li&amp;author=Z.%20Zhang&amp;author=R.%20Song&amp;author=J.%20Cheng&amp;author=Y.%20Liu&amp;author=X.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0322"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0323" id="ref-id-bib0323" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[323]</span></span></a></span><span class="reference" id="sbref0323"><div class="contribution"><div class="authors u-font-sans">H.J. Yoon, S.Y. Chung</div><div id="ref-id-sbref0323" class="title text-m">EEG-based emotion estimation using Bayesian weighted-log-posterior function and perceptron convergence algorithm</div></div><div class="host u-font-sans">Comput. Biol. Med., 43 (2013), pp. 2230-2237, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.compbiomed.2013.10.017" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.compbiomed.2013.10.017</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0010482513003041/pdfft?md5=fe9a2506586a79a8620dc0d7fcc7bec9&amp;pid=1-s2.0-S0010482513003041-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0323"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0010482513003041" aria-describedby="ref-id-sbref0323"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84887506958&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0323"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EEG-based%20emotion%20estimation%20using%20Bayesian%20weighted-log-posterior%20function%20and%20perceptron%20convergence%20algorithm&amp;publication_year=2013&amp;author=H.J.%20Yoon&amp;author=S.Y.%20Chung" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0323"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0324" id="ref-id-bib0324" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[324]</span></span></a></span><span class="reference" id="sbref0324"><div class="contribution"><div class="authors u-font-sans">Z. Yin, Y. Wang, L. Liu, W. Zhang, J. Zhang</div><div id="ref-id-sbref0324" class="title text-m">Cross-subject EEG feature selection for emotion recognition using transfer recursive feature elimination</div></div><div class="host u-font-sans">Front. Neurorobotics. (2017), p. 11, <a class="anchor anchor-primary" href="https://doi.org/10.3389/fnbot.2017.00019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3389/fnbot.2017.00019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85018177827&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0324"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Cross-subject%20EEG%20feature%20selection%20for%20emotion%20recognition%20using%20transfer%20recursive%20feature%20elimination&amp;publication_year=2017&amp;author=Z.%20Yin&amp;author=Y.%20Wang&amp;author=L.%20Liu&amp;author=W.%20Zhang&amp;author=J.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0324"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0325" id="ref-id-bib0325" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[325]</span></span></a></span><span class="reference" id="sbref0325"><div class="contribution"><div class="authors u-font-sans">Z. Yin, L. Liu, L. Liu, J. Zhang, Y. Wang</div><div id="ref-id-sbref0325" class="title text-m">Dynamical recursive feature elimination technique for neurophysiological signal-based emotion recognition</div></div><div class="host u-font-sans">Cogn. Technol. Work., 19 (2017), pp. 667-685, <a class="anchor anchor-primary" href="https://doi.org/10.1007/s10111-017-0450-2" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s10111-017-0450-2</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85033671546&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0325"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Dynamical%20recursive%20feature%20elimination%20technique%20for%20neurophysiological%20signal-based%20emotion%20recognition&amp;publication_year=2017&amp;author=Z.%20Yin&amp;author=L.%20Liu&amp;author=L.%20Liu&amp;author=J.%20Zhang&amp;author=Y.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0325"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0326" id="ref-id-bib0326" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[326]</span></span></a></span><span class="reference" id="sbref0326"><div class="contribution"><div class="authors u-font-sans">K.M. Puk, S. Wang, J. Rosenberger, K.C. Gandy, H.N. Harris, Y.B. Peng, A. Nordberg, P. Lehmann, J. Tommerdahl, J.-C. Chiao</div><div id="ref-id-sbref0326" class="title text-m">Emotion recognition and analysis using ADMM-based sparse group lasso</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2019.2943551" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2019.2943551</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20and%20analysis%20using%20ADMM-based%20sparse%20group%20lasso&amp;publication_year=2019&amp;author=K.M.%20Puk&amp;author=S.%20Wang&amp;author=J.%20Rosenberger&amp;author=K.C.%20Gandy&amp;author=H.N.%20Harris&amp;author=Y.B.%20Peng&amp;author=A.%20Nordberg&amp;author=P.%20Lehmann&amp;author=J.%20Tommerdahl&amp;author=J.-C.%20Chiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0326"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0327" id="ref-id-bib0327" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[327]</span></span></a></span><span class="reference" id="sbref0327"><div class="contribution"><div class="authors u-font-sans">H. He, Y. Tan, J. Ying, W. Zhang</div><div id="ref-id-sbref0327" class="title text-m">Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm</div></div><div class="host u-font-sans">Appl. Soft Comput., 94 (2020), Article 106426, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.asoc.2020.106426" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.asoc.2020.106426</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494620303665/pdfft?md5=5a6edb0a4cc87d05497554e2508410c9&amp;pid=1-s2.0-S1568494620303665-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0327"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1568494620303665" aria-describedby="ref-id-sbref0327"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85085937163&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0327"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Strengthen%20EEG-based%20emotion%20recognition%20using%20firefly%20integrated%20optimization%20algorithm&amp;publication_year=2020&amp;author=H.%20He&amp;author=Y.%20Tan&amp;author=J.%20Ying&amp;author=W.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0327"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0328" id="ref-id-bib0328" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[328]</span></span></a></span><span class="reference" id="sbref0328"><div class="contribution"><div class="authors u-font-sans">J. Atkinson, D. Campos</div><div id="ref-id-sbref0328" class="title text-m">Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers</div></div><div class="host u-font-sans">Expert Syst. Appl., 47 (2016), pp. 35-41, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2015.10.049" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2015.10.049</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417415007538/pdfft?md5=d17951f07c03be3b53d99814a3f24e26&amp;pid=1-s2.0-S0957417415007538-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0328"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417415007538" aria-describedby="ref-id-sbref0328"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84949024191&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0328"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Improving%20BCI-based%20emotion%20recognition%20by%20combining%20EEG%20feature%20selection%20and%20kernel%20classifiers&amp;publication_year=2016&amp;author=J.%20Atkinson&amp;author=D.%20Campos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0328"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0329" id="ref-id-bib0329" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[329]</span></span></a></span><span class="reference" id="sbref0329"><div class="contribution"><div class="authors u-font-sans">Y. Gao, H.J. Lee, R.M. Mehmood</div><div id="ref-id-sbref0329" class="title text-m">Deep learninig of EEG signals for emotion recognition</div></div><div class="host u-font-sans">2015 IEEE Int. Conf. Multimed. Expo Workshop ICMEW (2015), pp. 1-5, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICMEW.2015.7169796" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICMEW.2015.7169796</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84925845227&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0329"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learninig%20of%20EEG%20signals%20for%20emotion%20recognition&amp;publication_year=2015&amp;author=Y.%20Gao&amp;author=H.J.%20Lee&amp;author=R.M.%20Mehmood" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0329"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0330" id="ref-id-bib0330" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[330]</span></span></a></span><span class="reference" id="sbref0330"><div class="contribution"><div class="authors u-font-sans">Y. Li, W. Zheng, Z. Cui, T. Zhang, Y. Zong</div><div id="ref-id-sbref0330" class="title text-m">A novel neural network model based on cerebral hemispheric asymmetry for EEG emotion recognition</div></div><div class="host u-font-sans">Proc. Twenty-Seventh Int. Jt. Conf. Artif. Intell., International Joint Conferences on Artificial Intelligence Organization, Stockholm, Sweden (2018), pp. 1561-1567, <a class="anchor anchor-primary" href="https://doi.org/10.24963/ijcai.2018/216" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.24963/ijcai.2018/216</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85055686148&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0330"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20novel%20neural%20network%20model%20based%20on%20cerebral%20hemispheric%20asymmetry%20for%20EEG%20emotion%20recognition&amp;publication_year=2018&amp;author=Y.%20Li&amp;author=W.%20Zheng&amp;author=Z.%20Cui&amp;author=T.%20Zhang&amp;author=Y.%20Zong" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0330"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0331" id="ref-id-bib0331" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[331]</span></span></a></span><span class="reference" id="sbref0331"><div class="contribution"><div class="authors u-font-sans">T. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui</div><div id="ref-id-sbref0331" class="title text-m">Instance-adaptive graph for EEG emotion recognition</div></div><div class="host u-font-sans">Proc. AAAI Conf. Artif. Intell., 34 (2020), pp. 2701-2708, <a class="anchor anchor-primary" href="https://doi.org/10.1609/aaai.v34i03.5656" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1609/aaai.v34i03.5656</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85099733083&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0331"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Instance-adaptive%20graph%20for%20EEG%20emotion%20recognition&amp;publication_year=2020&amp;author=T.%20Song&amp;author=S.%20Liu&amp;author=W.%20Zheng&amp;author=Y.%20Zong&amp;author=Z.%20Cui" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0331"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0332" id="ref-id-bib0332" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[332]</span></span></a></span><span class="reference" id="sbref0332"><div class="contribution"><div class="authors u-font-sans">T. Zhang, Z. Cui, C. Xu, W. Zheng, J. Yang</div><div id="ref-id-sbref0332" class="title text-m">Variational pathway reasoning for EEG emotion recognition</div></div><div class="host u-font-sans">Proc. AAAI Conf. Artif. Intell., 34 (2020), pp. 2709-2716, <a class="anchor anchor-primary" href="https://doi.org/10.1609/aaai.v34i03.5657" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1609/aaai.v34i03.5657</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85094709473&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0332"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Variational%20pathway%20reasoning%20for%20EEG%20emotion%20recognition&amp;publication_year=2020&amp;author=T.%20Zhang&amp;author=Z.%20Cui&amp;author=C.%20Xu&amp;author=W.%20Zheng&amp;author=J.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0332"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0333" id="ref-id-bib0333" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[333]</span></span></a></span><span class="reference" id="sbref0333"><div class="contribution"><div class="authors u-font-sans">P. Zhong, D. Wang, C. Miao</div><div id="ref-id-sbref0333" class="title text-m">EEG-based emotion recognition using regularized graph neural networks</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2020.2994159" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2020.2994159</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EEG-based%20emotion%20recognition%20using%20regularized%20graph%20neural%20networks&amp;publication_year=2020&amp;author=P.%20Zhong&amp;author=D.%20Wang&amp;author=C.%20Miao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0333"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0334" id="ref-id-bib0334" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[334]</span></span></a></span><span class="reference" id="sbref0334"><div class="contribution"><div class="authors u-font-sans">Z. Gao, X. Wang, Y. Yang, Y. Li, K. Ma, G. Chen</div><div id="ref-id-sbref0334" class="title text-m">A Channel-fused dense convolutional network for EEG-based emotion recognition</div></div><div class="host u-font-sans">IEEE Trans. Cogn. Dev. Syst. (2020), <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCDS.2020.2976112" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCDS.2020.2976112</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">1–1</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20Channel-fused%20dense%20convolutional%20network%20for%20EEG-based%20emotion%20recognition&amp;publication_year=2020&amp;author=Z.%20Gao&amp;author=X.%20Wang&amp;author=Y.%20Yang&amp;author=Y.%20Li&amp;author=K.%20Ma&amp;author=G.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0334"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0335" id="ref-id-bib0335" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[335]</span></span></a></span><span class="reference" id="sbref0335"><div class="contribution"><div class="authors u-font-sans">H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang, X. Chen</div><div id="ref-id-sbref0335" class="title text-m">EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network</div></div><div class="host u-font-sans">Knowl. Based Syst, 205 (2020), Article 106243, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.knosys.2020.106243" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.knosys.2020.106243</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0950705120304433/pdfft?md5=35d1823e8552344c02f102e364d1a498&amp;pid=1-s2.0-S0950705120304433-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0335"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0950705120304433" aria-describedby="ref-id-sbref0335"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85088012317&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0335"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EEG-based%20emotion%20recognition%20using%20an%20end-to-end%20regional-asymmetric%20convolutional%20neural%20network&amp;publication_year=2020&amp;author=H.%20Cui&amp;author=A.%20Liu&amp;author=X.%20Zhang&amp;author=X.%20Chen&amp;author=K.%20Wang&amp;author=X.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0335"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0336" id="ref-id-bib0336" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[336]</span></span></a></span><span class="reference" id="sbref0336"><div class="contribution"><div class="authors u-font-sans">Y.-L. Hsu, J.-S. Wang, W.-C. Chiang, C.-H. Hung</div><div id="ref-id-sbref0336" class="title text-m">Automatic ECG-based emotion recognition in music listening</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 11 (2020), pp. 85-99, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2017.2781732" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2017.2781732</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85039791780&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0336"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Automatic%20ECG-based%20emotion%20recognition%20in%20music%20listening&amp;publication_year=2020&amp;author=Y.-L.%20Hsu&amp;author=J.-S.%20Wang&amp;author=W.-C.%20Chiang&amp;author=C.-H.%20Hung" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0336"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0337" id="ref-id-bib0337" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[337]</span></span></a></span><span class="reference" id="sbref0337"><div class="contribution"><div class="authors u-font-sans">S.Z. Bong, M. Murugappan, S. Yaacob</div><div id="ref-id-sbref0337" class="title text-m">Analysis of Electrocardiogram (ECG) signals for human emotional stress classification</div></div><div class="host u-font-sans">S.G. Ponnambalam, J. Parkkinen, K.C. Ramanathan (Eds.), Trends in Intelligent Robotics, Automation, and Manufacturing, Springer, Berlin, Heidelberg (2012), pp. 198-205, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-35197-6_22" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-35197-6_22</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84870817554&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0337"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Analysis%20of%20Electrocardiogram%20%20signals%20for%20human%20emotional%20stress%20classification&amp;publication_year=2012&amp;author=S.Z.%20Bong&amp;author=M.%20Murugappan&amp;author=S.%20Yaacob" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0337"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0338" id="ref-id-bib0338" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[338]</span></span></a></span><span class="reference" id="sbref0338"><div class="contribution"><div class="authors u-font-sans">S. Jerritta, M. Murugappan, K. Wan, S. Yaacob</div><div id="ref-id-sbref0338" class="title text-m">Emotion recognition from electrocardiogram signals using Hilbert Huang transform</div></div><div class="host u-font-sans">2012 IEEE Conf. Sustain. Util. Dev. Eng. Technol. Stud. (2012), pp. 82-86, <a class="anchor anchor-primary" href="https://doi.org/10.1109/STUDENT.2012.6408370" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/STUDENT.2012.6408370</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84873966240&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0338"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20from%20electrocardiogram%20signals%20using%20Hilbert%20Huang%20transform&amp;publication_year=2012&amp;author=S.%20Jerritta&amp;author=M.%20Murugappan&amp;author=K.%20Wan&amp;author=S.%20Yaacob" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0338"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0339" id="ref-id-bib0339" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[339]</span></span></a></span><span class="reference" id="sbref0339"><div class="contribution"><div class="authors u-font-sans">Z. Cheng, L. Shu, J. Xie, C.L.P. Chen</div><div id="ref-id-sbref0339" class="title text-m">A novel ECG-based real-time detection method of negative emotions in wearable applications</div></div><div class="host u-font-sans">2017 Int. Conf. Secur. Pattern Anal. Cybern. SPAC (2017), pp. 296-301, <a class="anchor anchor-primary" href="https://doi.org/10.1109/SPAC.2017.8304293" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/SPAC.2017.8304293</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85049166971&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0339"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20novel%20ECG-based%20real-time%20detection%20method%20of%20negative%20emotions%20in%20wearable%20applications&amp;publication_year=2017&amp;author=Z.%20Cheng&amp;author=L.%20Shu&amp;author=J.%20Xie&amp;author=C.L.P.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0339"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0340" id="ref-id-bib0340" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[340]</span></span></a></span><span class="reference" id="sbref0340"><div class="contribution"><div class="authors u-font-sans">L. Zhang, S. Walter, X. Ma, P. Werner, A. Al-Hamadi, H.C. Traue, S. Gruss</div><div id="ref-id-sbref0340" class="title text-m">BioVid Emo DB”: a multimodal database for emotion analyses validated by subjective ratings</div></div><div class="host u-font-sans">2016 IEEE Symp. Ser. Comput. Intell. SSCI (2016), pp. 1-6, <a class="anchor anchor-primary" href="https://doi.org/10.1109/SSCI.2016.7849931" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/SSCI.2016.7849931</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0042207X16302226/pdfft?md5=0bd94df04e2d2698eb54c1c50ba6be33&amp;pid=1-s2.0-S0042207X16302226-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0340"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0042207X16302226" aria-describedby="ref-id-sbref0340"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=BioVid%20Emo%20DB%3A%20a%20multimodal%20database%20for%20emotion%20analyses%20validated%20by%20subjective%20ratings&amp;publication_year=2016&amp;author=L.%20Zhang&amp;author=S.%20Walter&amp;author=X.%20Ma&amp;author=P.%20Werner&amp;author=A.%20Al-Hamadi&amp;author=H.C.%20Traue&amp;author=S.%20Gruss" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0340"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0341" id="ref-id-bib0341" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[341]</span></span></a></span><span class="reference" id="sbref0341"><div class="contribution"><div class="authors u-font-sans">J. Selvaraj, M. Murugappan, K. Wan, S. Yaacob</div><div id="ref-id-sbref0341" class="title text-m">Classification of emotional states from electrocardiogram signals: a non-linear approach based on hurst</div></div><div class="host u-font-sans">Biomed. Eng. OnLine., 12 (2013), p. 44, <a class="anchor anchor-primary" href="https://doi.org/10.1186/1475-925X-12-44" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1186/1475-925X-12-44</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84879017985&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0341"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Classification%20of%20emotional%20states%20from%20electrocardiogram%20signals%3A%20a%20non-linear%20approach%20based%20on%20hurst&amp;publication_year=2013&amp;author=J.%20Selvaraj&amp;author=M.%20Murugappan&amp;author=K.%20Wan&amp;author=S.%20Yaacob" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0341"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0342" id="ref-id-bib0342" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[342]</span></span></a></span><span class="reference" id="sbref0342"><div class="contribution"><div class="authors u-font-sans">H. Ferdinando, T. Seppänen, E. Alasaarela</div><div id="ref-id-sbref0342" class="title text-m">Enhancing emotion recognition from ECG signals using supervised dimensionality reduction</div></div><div class="host u-font-sans">Proc. 6th Int. Conf. Pattern Recognit. Appl. Methods, SCITEPRESS - Science and Technology Publications, Porto, Portugal (2017), pp. 112-118, <a class="anchor anchor-primary" href="https://doi.org/10.5220/0006147801120118" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.5220/0006147801120118</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Enhancing%20emotion%20recognition%20from%20ECG%20signals%20using%20supervised%20dimensionality%20reduction&amp;publication_year=2017&amp;author=H.%20Ferdinando&amp;author=T.%20Sepp%C3%A4nen&amp;author=E.%20Alasaarela" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0342"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0343" id="ref-id-bib0343" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[343]</span></span></a></span><span class="reference" id="sbref0343"><div class="contribution"><div class="authors u-font-sans">G. Chen, Y. Zhu, Z. Hong, Z. Yang</div><div id="ref-id-sbref0343" class="title text-m">EmotionalGAN: generating ECG to enhance emotion state classification</div></div><div class="host u-font-sans">Proc. 2019 Int. Conf. Artif. Intell. Comput. Sci., Association for Computing Machinery, New York, NY, USA (2019), pp. 309-313, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3349341.3349422" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3349341.3349422</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85073066579&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0343"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EmotionalGAN%3A%20generating%20ECG%20to%20enhance%20emotion%20state%20classification&amp;publication_year=2019&amp;author=G.%20Chen&amp;author=Y.%20Zhu&amp;author=Z.%20Hong&amp;author=Z.%20Yang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0343"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0344" id="ref-id-bib0344" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[344]</span></span></a></span><span class="reference" id="sbref0344"><div class="contribution"><div class="authors u-font-sans">P. Sarkar, A. Etemad</div><div id="ref-id-sbref0344" class="title text-m">Self-supervised learning for ECG-based emotion recognition</div></div><div class="host u-font-sans">ICASSP 2020 - 2020 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2020), pp. 3217-3221, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP40776.2020.9053985" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP40776.2020.9053985</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089234210&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0344"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Self-supervised%20learning%20for%20ECG-based%20emotion%20recognition&amp;publication_year=2020&amp;author=P.%20Sarkar&amp;author=A.%20Etemad" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0344"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0345" id="ref-id-bib0345" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[345]</span></span></a></span><span class="reference" id="sbref0345"><div class="contribution"><div class="authors u-font-sans">G. Caridakis, G. Castellano, L. Kessous, A. Raouzaiou, L. Malatesta, S. Asteriadis, K. Karpouzis</div><div id="ref-id-sbref0345" class="title text-m">Multimodal emotion recognition from expressive faces, body gestures and speech</div></div><div class="host u-font-sans">C. Boukis, A. Pnevmatikakis, L. Polymenakos (Eds.), Artif. Intell. Innov. 2007 Theory Appl., Springer US, Boston, MA (2007), pp. 375-388, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-0-387-74161-1_41" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-0-387-74161-1_41</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-36349013764&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0345"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20emotion%20recognition%20from%20expressive%20faces%2C%20body%20gestures%20and%20speech&amp;publication_year=2007&amp;author=G.%20Caridakis&amp;author=G.%20Castellano&amp;author=L.%20Kessous&amp;author=A.%20Raouzaiou&amp;author=L.%20Malatesta&amp;author=S.%20Asteriadis&amp;author=K.%20Karpouzis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0345"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0346" id="ref-id-bib0346" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[346]</span></span></a></span><span class="reference" id="sbref0346"><div class="contribution"><div class="authors u-font-sans">C. Sarkar, S. Bhatia, A. Agarwal, J. Li</div><div id="ref-id-sbref0346" class="title text-m">Feature analysis for computational personality recognition using youtube personality data set</div></div><div class="host u-font-sans">Proc. 2014 ACM Multi Media Workshop Comput. Personal. Recognit. - WCPR 14, ACM Press, Orlando, Florida, USA (2014), pp. 11-14, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2659522.2659528" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2659522.2659528</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84916619842&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0346"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20analysis%20for%20computational%20personality%20recognition%20using%20youtube%20personality%20data%20set&amp;publication_year=2014&amp;author=C.%20Sarkar&amp;author=S.%20Bhatia&amp;author=A.%20Agarwal&amp;author=J.%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0346"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0347" id="ref-id-bib0347" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[347]</span></span></a></span><span class="reference" id="sbref0347"><div class="contribution"><div class="authors u-font-sans">G.K. Verma, U.S. Tiwary</div><div id="ref-id-sbref0347" class="title text-m">Multimodal fusion framework: a multiresolution approach for emotion classification and recognition from physiological signals</div></div><div class="host u-font-sans">NeuroImage, 102 (2014), pp. 162-172, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neuroimage.2013.11.007" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neuroimage.2013.11.007</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1053811913010999/pdfft?md5=8ca9b9c6d7885836d5d788352ef9efab&amp;pid=1-s2.0-S1053811913010999-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0347"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1053811913010999" aria-describedby="ref-id-sbref0347"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84908272142&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0347"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20fusion%20framework%3A%20a%20multiresolution%20approach%20for%20emotion%20classification%20and%20recognition%20from%20physiological%20signals&amp;publication_year=2014&amp;author=G.K.%20Verma&amp;author=U.S.%20Tiwary" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0347"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0348" id="ref-id-bib0348" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[348]</span></span></a></span><span class="reference" id="sbref0348"><div class="contribution"><div class="authors u-font-sans">X. Zhang, J. Liu, J. Shen, S. Li, K. Hou, B. Hu, J. Gao, T. Zhang, B. Hu</div><div id="ref-id-sbref0348" class="title text-m">Emotion recognition from multimodal physiological signals using a regularized deep fusion of Kernel machine</div></div><div class="host u-font-sans">IEEE Trans. Cybern. (2020), pp. 1-14, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCYB.2020.2987575" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCYB.2020.2987575</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20from%20multimodal%20physiological%20signals%20using%20a%20regularized%20deep%20fusion%20of%20Kernel%20machine&amp;publication_year=2020&amp;author=X.%20Zhang&amp;author=J.%20Liu&amp;author=J.%20Shen&amp;author=S.%20Li&amp;author=K.%20Hou&amp;author=B.%20Hu&amp;author=J.%20Gao&amp;author=T.%20Zhang&amp;author=B.%20Hu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0348"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0349" id="ref-id-bib0349" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[349]</span></span></a></span><span class="reference" id="sbref0349"><div class="contribution"><div class="authors u-font-sans">M.S. Hossain, G. Muhammad</div><div id="ref-id-sbref0349" class="title text-m">Emotion recognition using deep learning approach from audio–visual emotional big data</div></div><div class="host u-font-sans">Inf. Fusion., 49 (2019), pp. 69-78, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2018.09.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2018.09.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253517307066/pdfft?md5=49a959d4c849e8b069ed8c2100817e2c&amp;pid=1-s2.0-S1566253517307066-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0349"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253517307066" aria-describedby="ref-id-sbref0349"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054161545&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0349"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20deep%20learning%20approach%20from%20audiovisual%20emotional%20big%20data&amp;publication_year=2019&amp;author=M.S.%20Hossain&amp;author=G.%20Muhammad" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0349"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0350" id="ref-id-bib0350" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[350]</span></span></a></span><span class="reference" id="sbref0350"><div class="contribution"><div class="authors u-font-sans">N. Sebe, I. Cohen, T. Gevers, T.S. Huang</div><div id="ref-id-sbref0350" class="title text-m">Emotion recognition based on joint visual and audio cues</div></div><div class="host u-font-sans">18th Int. Conf. Pattern Recognit. ICPR06, IEEE, Hong Kong, China (2006), pp. 1136-1139, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICPR.2006.489" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICPR.2006.489</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34047198092&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0350"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20based%20on%20joint%20visual%20and%20audio%20cues&amp;publication_year=2006&amp;author=N.%20Sebe&amp;author=I.%20Cohen&amp;author=T.%20Gevers&amp;author=T.S.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0350"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0351" id="ref-id-bib0351" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[351]</span></span></a></span><span class="reference" id="sbref0351"><div class="contribution"><div class="authors u-font-sans">B. Schuller, G. Rigoll, M. Lang</div><div id="ref-id-sbref0351" class="title text-m">Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</div></div><div class="host u-font-sans">2004 IEEE Int. Conf. Acoust. Speech Signal Process. (2004), pp. 1-577, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2004.1326051" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2004.1326051</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20combining%20acoustic%20features%20and%20linguistic%20information%20in%20a%20hybrid%20support%20vector%20machine-belief%20network%20architecture&amp;publication_year=2004&amp;author=B.%20Schuller&amp;author=G.%20Rigoll&amp;author=M.%20Lang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0351"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0352" id="ref-id-bib0352" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[352]</span></span></a></span><span class="reference" id="sbref0352"><div class="contribution"><div class="authors u-font-sans">J. Sebastian, P. Pierucci</div><div id="ref-id-sbref0352" class="title text-m">Fusion techniques for utterance-level emotion recognition combining speech and transcripts</div></div><div class="host u-font-sans">Interspeech 2019, ISCA (2019), pp. 51-55, <a class="anchor anchor-primary" href="https://doi.org/10.21437/Interspeech.2019-3201" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.21437/Interspeech.2019-3201</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089969810&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0352"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fusion%20techniques%20for%20utterance-level%20emotion%20recognition%20combining%20speech%20and%20transcripts&amp;publication_year=2019&amp;author=J.%20Sebastian&amp;author=P.%20Pierucci" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0352"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0353" id="ref-id-bib0353" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[353]</span></span></a></span><span class="reference" id="sbref0353"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, A. Hussain, G.-B. Huang</div><div id="ref-id-sbref0353" class="title text-m">Towards an intelligent framework for multimodal affective data analysis</div></div><div class="host u-font-sans">Neural Netw, 63 (2015), pp. 104-116, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neunet.2014.10.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neunet.2014.10.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0893608014002342/pdfft?md5=a9791ee01cedbfea01ca34a32d8e85b9&amp;pid=1-s2.0-S0893608014002342-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0353"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0893608014002342" aria-describedby="ref-id-sbref0353"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84917739875&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0353"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Towards%20an%20intelligent%20framework%20for%20multimodal%20affective%20data%20analysis&amp;publication_year=2015&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=A.%20Hussain&amp;author=G.-B.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0353"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0354" id="ref-id-bib0354" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[354]</span></span></a></span><span class="reference" id="sbref0354"><div class="contribution"><div class="authors u-font-sans">S. Poria, I. Chaturvedi, E. Cambria, A. Hussain</div><div id="ref-id-sbref0354" class="title text-m">Convolutional MKL based multimodal emotion recognition and sentiment analysis</div></div><div class="host u-font-sans">2016 IEEE 16th Int. Conf. Data Min. ICDM (2016), pp. 439-448, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICDM.2016.0055" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICDM.2016.0055</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85014552954&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0354"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Convolutional%20MKL%20based%20multimodal%20emotion%20recognition%20and%20sentiment%20analysis&amp;publication_year=2016&amp;author=S.%20Poria&amp;author=I.%20Chaturvedi&amp;author=E.%20Cambria&amp;author=A.%20Hussain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0354"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0355" id="ref-id-bib0355" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[355]</span></span></a></span><span class="reference" id="sbref0355"><div class="contribution"><div class="authors u-font-sans">S.E. Eskimez, R.K. Maddox, C. Xu, Z. Duan</div><div id="ref-id-sbref0355" class="title text-m">Noise-resilient training method for face landmark generation from speech</div></div><div class="host u-font-sans">IEEEACM Trans. Audio Speech Lang. Process., 28 (2020), pp. 27-38, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TASLP.2019.2947741" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TASLP.2019.2947741</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85077209770&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0355"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Noise-resilient%20training%20method%20for%20face%20landmark%20generation%20from%20speech&amp;publication_year=2020&amp;author=S.E.%20Eskimez&amp;author=R.K.%20Maddox&amp;author=C.%20Xu&amp;author=Z.%20Duan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0355"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0356" id="ref-id-bib0356" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[356]</span></span></a></span><span class="reference" id="sbref0356"><div class="contribution"><div class="authors u-font-sans">Mingli Song, Jiajun Bu, Chun Chen, Nan Li</div><div id="ref-id-sbref0356" class="title text-m">Audio-visual based emotion recognition-a new approach</div></div><div class="host u-font-sans">Proc. 2004 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2004 CVPR 2004, IEEE, Washington, DC, USA (2004), pp. 1020-1025, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2004.1315276" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2004.1315276</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Audio-visual%20based%20emotion%20recognition-a%20new%20approach&amp;publication_year=2004&amp;author=Mingli%20Song&amp;author=Jiajun%20Bu&amp;author=Chun%20Chen&amp;author=Nan%20Li" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0356"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0357" id="ref-id-bib0357" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[357]</span></span></a></span><span class="reference" id="sbref0357"><div class="contribution"><div class="authors u-font-sans">K. Nickel, T. Gehrig, R. Stiefelhagen, J. McDonough</div><div id="ref-id-sbref0357" class="title text-m">A joint particle filter for audio-visual speaker tracking</div></div><div class="host u-font-sans">Proc. 7th Int. Conf. Multimodal Interfaces - ICMI 05, ACM Press, Torento, Italy (2005), p. 61, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1088463.1088477" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1088463.1088477</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-32344434992&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0357"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20joint%20particle%20filter%20for%20audio-visual%20speaker%20tracking&amp;publication_year=2005&amp;author=K.%20Nickel&amp;author=T.%20Gehrig&amp;author=R.%20Stiefelhagen&amp;author=J.%20McDonough" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0357"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0358" id="ref-id-bib0358" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[358]</span></span></a></span><span class="reference" id="sbref0358"><div class="contribution"><div class="authors u-font-sans">Z. Zeng, Y. Hu, M. Liu, Y. Fu, T.S. Huang</div><div id="ref-id-sbref0358" class="title text-m">Training combination strategy of multi-stream fused hidden Markov model for audio-visual affect recognition</div></div><div class="host u-font-sans">Proc. 14th Annu. ACM Int. Conf. Multimed. - Multimed. 06, ACM Press, Santa Barbara, CA, USA (2006), p. 65, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1180639.1180661" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1180639.1180661</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34547223416&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0358"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Training%20combination%20strategy%20of%20multi-stream%20fused%20hidden%20Markov%20model%20for%20audio-visual%20affect%20recognition&amp;publication_year=2006&amp;author=Z.%20Zeng&amp;author=Y.%20Hu&amp;author=M.%20Liu&amp;author=Y.%20Fu&amp;author=T.S.%20Huang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0358"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0359" id="ref-id-bib0359" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[359]</span></span></a></span><span class="reference" id="sbref0359"><div class="contribution"><div class="authors u-font-sans">G. Caridakis, L. Malatesta, L. Kessous, N. Amir, A. Raouzaiou, K. Karpouzis</div><div id="ref-id-sbref0359" class="title text-m">Modeling naturalistic affective states via facial and vocal expressions recognition</div></div><div class="host u-font-sans">Proc. 8th Int. Conf. Multimodal Interfaces - ICMI 06, ACM Press, Banff, Alberta, Canada (2006), p. 146, <a class="anchor anchor-primary" href="https://doi.org/10.1145/1180995.1181029" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/1180995.1181029</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-34547153664&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0359"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Modeling%20naturalistic%20affective%20states%20via%20facial%20and%20vocal%20expressions%20recognition&amp;publication_year=2006&amp;author=G.%20Caridakis&amp;author=L.%20Malatesta&amp;author=L.%20Kessous&amp;author=N.%20Amir&amp;author=A.%20Raouzaiou&amp;author=K.%20Karpouzis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0359"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0360" id="ref-id-bib0360" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[360]</span></span></a></span><span class="reference" id="sbref0360"><div class="contribution"><div class="authors u-font-sans">J. Chen, Z. Chen, Z. Chi, H. Fu</div><div id="ref-id-sbref0360" class="title text-m">Facial expression recognition in video with multiple feature fusion</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 9 (2018), pp. 38-50, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2016.2593719" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2016.2593719</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20in%20video%20with%20multiple%20feature%20fusion&amp;publication_year=2018&amp;author=J.%20Chen&amp;author=Z.%20Chen&amp;author=Z.%20Chi&amp;author=H.%20Fu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0360"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0361" id="ref-id-bib0361" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[361]</span></span></a></span><span class="reference" id="sbref0361"><div class="contribution"><div class="authors u-font-sans">D. Priyasad, T. Fernando, S. Denman, S. Sridharan, C. Fookes</div><div id="ref-id-sbref0361" class="title text-m">Attention driven fusion for multi-modal emotion recognition</div></div><div class="host u-font-sans">ICASSP 2020 - 2020 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2020), pp. 3227-3231, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP40776.2020.9054441" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP40776.2020.9054441</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089218624&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0361"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Attention%20driven%20fusion%20for%20multi-modal%20emotion%20recognition&amp;publication_year=2020&amp;author=D.%20Priyasad&amp;author=T.%20Fernando&amp;author=S.%20Denman&amp;author=S.%20Sridharan&amp;author=C.%20Fookes" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0361"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0362" id="ref-id-bib0362" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[362]</span></span></a></span><span class="reference" id="sbref0362"><div class="contribution"><div class="authors u-font-sans">T. Mittal, U. Bhattacharya, R. Chandra, A. Bera, D. Manocha</div><div id="ref-id-sbref0362" class="title text-m">M3ER: multiplicative multimodal emotion recognition using facial, textual, and speech cues</div></div><div class="host u-font-sans">Proc. AAAI Conf. Artif. Intell., 34 (2020), pp. 1359-1367, <a class="anchor anchor-primary" href="https://doi.org/10.1609/aaai.v34i02.5492" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1609/aaai.v34i02.5492</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85099596866&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0362"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=M3ER%3A%20multiplicative%20multimodal%20emotion%20recognition%20using%20facial%2C%20textual%2C%20and%20speech%20cues&amp;publication_year=2020&amp;author=T.%20Mittal&amp;author=U.%20Bhattacharya&amp;author=R.%20Chandra&amp;author=A.%20Bera&amp;author=D.%20Manocha" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0362"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0363" id="ref-id-bib0363" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[363]</span></span></a></span><span class="reference" id="sbref0363"><div class="contribution"><div class="authors u-font-sans">H.-C. Yang, C.-C. Lee</div><div id="ref-id-sbref0363" class="title text-m">An attribute-invariant variational learning for emotion recognition using physiology</div></div><div class="host u-font-sans">ICASSP 2019 - 2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2019), pp. 1184-1188, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2019.8683290" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2019.8683290</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068976620&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0363"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20attribute-invariant%20variational%20learning%20for%20emotion%20recognition%20using%20physiology&amp;publication_year=2019&amp;author=H.-C.%20Yang&amp;author=C.-C.%20Lee" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0363"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0364" id="ref-id-bib0364" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[364]</span></span></a></span><span class="reference" id="sbref0364"><div class="contribution"><div class="authors u-font-sans">S. Zhao, Y. Ma, Y. Gu, J. Yang, T. Xing, P. Xu, R. Hu, H. Chai, K. Keutzer</div><div id="ref-id-sbref0364" class="title text-m">An end-to-end visual-audio attention network for emotion recognition in user-generated videos</div></div><div class="host u-font-sans">Proc. AAAI Conf. Artif. Intell., 34 (2020), pp. 303-311, <a class="anchor anchor-primary" href="https://doi.org/10.1609/aaai.v34i01.5364" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1609/aaai.v34i01.5364</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85091503975&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0364"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20end-to-end%20visual-audio%20attention%20network%20for%20emotion%20recognition%20in%20user-generated%20videos&amp;publication_year=2020&amp;author=S.%20Zhao&amp;author=Y.%20Ma&amp;author=Y.%20Gu&amp;author=J.%20Yang&amp;author=T.%20Xing&amp;author=P.%20Xu&amp;author=R.%20Hu&amp;author=H.%20Chai&amp;author=K.%20Keutzer" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0364"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0365" id="ref-id-bib0365" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[365]</span></span></a></span><span class="reference" id="sbref0365"><div class="contribution"><div class="authors u-font-sans">Y. Zhang, Z.-R. Wang, J. Du</div><div id="ref-id-sbref0365" class="title text-m">Deep fusion: an attention guided factorized bilinear pooling for audio-video emotion recognition</div></div><div class="host u-font-sans">Int. Jt. Conf. Neural Netw (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1109/IJCNN.2019.8851942" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/IJCNN.2019.8851942</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20fusion%3A%20an%20attention%20guided%20factorized%20bilinear%20pooling%20for%20audio-video%20emotion%20recognition&amp;publication_year=2019&amp;author=Y.%20Zhang&amp;author=Z.-R.%20Wang&amp;author=J.%20Du" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0365"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0366" id="ref-id-bib0366" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[366]</span></span></a></span><span class="reference" id="sbref0366"><div class="contribution"><div class="authors u-font-sans">K. Hara, H. Kataoka, Y. Satoh</div><div id="ref-id-sbref0366" class="title text-m">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?</div></div><div class="host u-font-sans">2018 IEEECVF Conf. Comput. Vis. Pattern Recognit., IEEE, Salt Lake City, UT, USA (2018), pp. 6546-6555, <a class="anchor anchor-primary" href="https://doi.org/10.1109/CVPR.2018.00685" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/CVPR.2018.00685</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85058240692&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0366"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Can%20spatiotemporal%203D%20CNNs%20retrace%20the%20history%20of%202D%20CNNs%20and%20ImageNet&amp;publication_year=2018&amp;author=K.%20Hara&amp;author=H.%20Kataoka&amp;author=Y.%20Satoh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0366"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0367" id="ref-id-bib0367" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[367]</span></span></a></span><span class="reference" id="sbref0367"><div class="contribution"><div class="authors u-font-sans">M. Hao, W.-H. Cao, Z.-T. Liu, M. Wu, P. Xiao</div><div id="ref-id-sbref0367" class="title text-m">Visual-audio emotion recognition based on multi-task and ensemble learning with multiple features</div></div><div class="host u-font-sans">Neurocomputing, 391 (2020), pp. 42-51, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2020.01.048" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2020.01.048</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231220300990/pdfft?md5=f508ff0a178c28990765c2523bc963f1&amp;pid=1-s2.0-S0925231220300990-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0367"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231220300990" aria-describedby="ref-id-sbref0367"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85078922012&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0367"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Visual-audio%20emotion%20recognition%20based%20on%20multi-task%20and%20ensemble%20learning%20with%20multiple%20features&amp;publication_year=2020&amp;author=M.%20Hao&amp;author=W.-H.%20Cao&amp;author=Z.-T.%20Liu&amp;author=M.%20Wu&amp;author=P.%20Xiao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0367"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0368" id="ref-id-bib0368" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[368]</span></span></a></span><span class="reference" id="sbref0368"><div class="contribution"><div class="authors u-font-sans">O. Martin, I. Kotsia, B. Macq, I. Pitas</div><div id="ref-id-sbref0368" class="title text-m">The eNTERFACE’05 audio-visual emotion database</div></div><div class="host u-font-sans">Proc. 22nd Int. Conf. Data Eng. Workshop, IEEE Computer Society, USA (2006), p. 8, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICDEW.2006.145" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICDEW.2006.145</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20eNTERFACE05%20audio-visual%20emotion%20database&amp;publication_year=2006&amp;author=O.%20Martin&amp;author=I.%20Kotsia&amp;author=B.%20Macq&amp;author=I.%20Pitas" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0368"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0369" id="ref-id-bib0369" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[369]</span></span></a></span><span class="reference" id="sbref0369"><div class="contribution"><div class="authors u-font-sans">M. Glodek, S. Reuter, M. Schels, K. Dietmayer, F. Schwenker</div><div id="ref-id-sbref0369" class="title text-m">Kalman filter based classifier fusion for affective state recognition</div></div><div class="host u-font-sans">Z.-H. Zhou, F. Roli, J. Kittler (Eds.), Mult. Classif. Syst., Springer Berlin Heidelberg, Berlin, Heidelberg (2013), pp. 85-94, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-38067-9_8" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-38067-9_8</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84892929459&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0369"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Kalman%20filter%20based%20classifier%20fusion%20for%20affective%20state%20recognition&amp;publication_year=2013&amp;author=M.%20Glodek&amp;author=S.%20Reuter&amp;author=M.%20Schels&amp;author=K.%20Dietmayer&amp;author=F.%20Schwenker" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0369"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0370" id="ref-id-bib0370" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[370]</span></span></a></span><span class="reference" id="sbref0370"><div class="contribution"><div class="authors u-font-sans">W. Xue, W. Zhou, T. Li, Q. Wang</div><div id="ref-id-sbref0370" class="title text-m">MTNA: a neural multi-task model for aspect category classification and aspect term extraction on restaurant reviews</div></div><div class="host u-font-sans">Proc. Eighth Int. Jt. Conf. Nat. Lang. Process. Vol. 2 Short Pap., Asian Federation of Natural Language Processing, Taipei, Taiwan (2017), pp. 151-156</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/I17-2026" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/I17-2026</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed August 17, 2020)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=MTNA%3A%20a%20neural%20multi-task%20model%20for%20aspect%20category%20classification%20and%20aspect%20term%20extraction%20on%20restaurant%20reviews&amp;publication_year=2017&amp;author=W.%20Xue&amp;author=W.%20Zhou&amp;author=T.%20Li&amp;author=Q.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0370"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0371" id="ref-id-bib0371" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[371]</span></span></a></span><span class="reference" id="sbref0371"><div class="contribution"><div class="authors u-font-sans">B. Zhang, S. Khorram, E.M. Provost</div><div id="ref-id-sbref0371" class="title text-m">Exploiting acoustic and lexical properties of phonemes to recognize valence from speech</div></div><div class="host u-font-sans">ICASSP 2019 - 2019 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP, IEEE, Brighton, United Kingdom (2019), pp. 5871-5875, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2019.8683190" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2019.8683190</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85068958952&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0371"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Exploiting%20acoustic%20and%20lexical%20properties%20of%20phonemes%20to%20recognize%20valence%20from%20speech&amp;publication_year=2019&amp;author=B.%20Zhang&amp;author=S.%20Khorram&amp;author=E.M.%20Provost" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0371"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0372" id="ref-id-bib0372" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[372]</span></span></a></span><span class="reference" id="sbref0372"><div class="contribution"><div class="authors u-font-sans">S. Yoon, S. Byun, K. Jung</div><div id="ref-id-sbref0372" class="title text-m">Multimodal speech emotion recognition using audio and text</div></div><div class="host u-font-sans">2018 IEEE Spok. Lang. Technol. Workshop SLT (2018), pp. 112-118, <a class="anchor anchor-primary" href="https://doi.org/10.1109/SLT.2018.8639583" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/SLT.2018.8639583</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20speech%20emotion%20recognition%20using%20audio%20and%20text&amp;publication_year=2018&amp;author=S.%20Yoon&amp;author=S.%20Byun&amp;author=K.%20Jung" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0372"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0373" id="ref-id-bib0373" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[373]</span></span></a></span><span class="reference" id="sbref0373"><div class="contribution"><div class="authors u-font-sans">L. Cai, Y. Hu, J. Dong, S. Zhou</div><div id="ref-id-sbref0373" class="title text-m">Audio-textual emotion recognition based on improved neural networks</div></div><div class="host u-font-sans">Math. Probl. Eng., 2019 (2019), pp. 1-9, <a class="anchor anchor-primary" href="https://doi.org/10.1155/2019/2593036" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1155/2019/2593036</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85077395973&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0373"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Audio-textual%20emotion%20recognition%20based%20on%20improved%20neural%20networks&amp;publication_year=2019&amp;author=L.%20Cai&amp;author=Y.%20Hu&amp;author=J.%20Dong&amp;author=S.%20Zhou" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0373"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0374" id="ref-id-bib0374" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[374]</span></span></a></span><span class="reference" id="sbref0374"><div class="contribution"><div class="authors u-font-sans">C.-H. Wu, W.-B. Liang</div><div id="ref-id-sbref0374" class="title text-m">Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 2 (2011), pp. 10-21, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2010.16" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2010.16</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84857911091&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0374"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20of%20affective%20speech%20based%20on%20multiple%20classifiers%20using%20acoustic-prosodic%20information%20and%20semantic%20labels&amp;publication_year=2011&amp;author=C.-H.%20Wu&amp;author=W.-B.%20Liang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0374"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0375" id="ref-id-bib0375" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[375]</span></span></a></span><span class="reference" id="sbref0375"><div class="contribution"><div class="authors u-font-sans">Q. Jin, C. Li, S. Chen, H. Wu</div><div id="ref-id-sbref0375" class="title text-m">Speech emotion recognition with acoustic and lexical features</div></div><div class="host u-font-sans">2015 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP (2015), pp. 4749-4753, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP.2015.7178872" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP.2015.7178872</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84946032638&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0375"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Speech%20emotion%20recognition%20with%20acoustic%20and%20lexical%20features&amp;publication_year=2015&amp;author=Q.%20Jin&amp;author=C.%20Li&amp;author=S.%20Chen&amp;author=H.%20Wu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0375"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0376" id="ref-id-bib0376" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[376]</span></span></a></span><span class="reference" id="sbref0376"><div class="contribution"><div class="authors u-font-sans">L. Pepino, P. Riera, L. Ferrer, A. Gravano</div><div id="ref-id-sbref0376" class="title text-m">Fusion approaches for emotion recognition from speech using acoustic and text-based features</div></div><div class="host u-font-sans">ICASSP 2020 - 2020 IEEE Int. Conf. Acoust. Speech Signal Process. ICASSP, IEEE, Barcelona, Spain (2020), pp. 6484-6488, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICASSP40776.2020.9054709" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICASSP40776.2020.9054709</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85091310773&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0376"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Fusion%20approaches%20for%20emotion%20recognition%20from%20speech%20using%20acoustic%20and%20text-based%20features&amp;publication_year=2020&amp;author=L.%20Pepino&amp;author=P.%20Riera&amp;author=L.%20Ferrer&amp;author=A.%20Gravano" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0376"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0377" id="ref-id-bib0377" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[377]</span></span></a></span><span class="reference" id="sbref0377"><div class="contribution"><div class="authors u-font-sans">A. Metallinou, M. Wollmer, A. Katsamanis, F. Eyben, B. Schuller, S. Narayanan</div><div id="ref-id-sbref0377" class="title text-m">Context-sensitive learning for enhanced audiovisual emotion classification</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 3 (2012), pp. 184-198, <a class="anchor anchor-primary" href="https://doi.org/10.1109/T-AFFC.2011.40" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/T-AFFC.2011.40</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84863942898&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0377"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Context-sensitive%20learning%20for%20enhanced%20audiovisual%20emotion%20classification&amp;publication_year=2012&amp;author=A.%20Metallinou&amp;author=M.%20Wollmer&amp;author=A.%20Katsamanis&amp;author=F.%20Eyben&amp;author=B.%20Schuller&amp;author=S.%20Narayanan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0377"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0378" id="ref-id-bib0378" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[378]</span></span></a></span><span class="reference" id="sbref0378"><div class="contribution"><div class="authors u-font-sans">E. Cambria, D. Hazarika, S. Poria, A. Hussain, R.B.V. Subramanyam</div><div id="ref-id-sbref0378" class="title text-m">Benchmarking multimodal sentiment analysis</div></div><div class="host u-font-sans">A. Gelbukh (Ed.), Computational Linguistics and Intelligent Text Processing, Springer International Publishing (2018), pp. 166-179, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-77116-8_13" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-77116-8_13</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85055689226&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0378"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Benchmarking%20multimodal%20sentiment%20analysis&amp;publication_year=2018&amp;author=E.%20Cambria&amp;author=D.%20Hazarika&amp;author=S.%20Poria&amp;author=A.%20Hussain&amp;author=R.B.V.%20Subramanyam" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0378"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0379" id="ref-id-bib0379" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[379]</span></span></a></span><span class="reference" id="sbref0379"><div class="contribution"><div class="authors u-font-sans">V. Perez Rosas, R. Mihalcea, L.-P. Morency</div><div id="ref-id-sbref0379" class="title text-m">Multimodal sentiment analysis of spanish online videos</div></div><div class="host u-font-sans">IEEE Intell. Syst., 28 (2013), pp. 38-45, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MIS.2013.9" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MIS.2013.9</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84884196983&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0379"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20sentiment%20analysis%20of%20spanish%20online%20videos&amp;publication_year=2013&amp;author=V.%20Perez%20Rosas&amp;author=R.%20Mihalcea&amp;author=L.-P.%20Morency" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0379"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0380" id="ref-id-bib0380" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[380]</span></span></a></span><span class="reference" id="sbref0380"><div class="contribution"><div class="authors u-font-sans">J. Arguello, C. Rosé</div><div id="ref-id-sbref0380" class="title text-m">Topic-segmentation of dialogue</div></div><div class="host u-font-sans">Proc. Anal. Conversat. Text Speech, Association for Computational Linguistics, New York City, New York (2006), pp. 42-49</div><div class="host u-font-sans"><a class="anchor anchor-primary" href="https://www.aclweb.org/anthology/W06-3407" target="_blank"><span class="anchor-text-container"><span class="anchor-text">https://www.aclweb.org/anthology/W06-3407</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="comment">(accessed August 23, 2020)</div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://doi.org/10.3115/1564535.1564542" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0380"><span class="anchor-text-container"><span class="anchor-text">Crossref</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85187557516&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0380"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Topic-segmentation%20of%20dialogue&amp;publication_year=2006&amp;author=J.%20Arguello&amp;author=C.%20Ros%C3%A9" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0380"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0381" id="ref-id-bib0381" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[381]</span></span></a></span><span class="reference" id="sbref0381"><div class="contribution"><div class="authors u-font-sans">H. Peng, Y. Ma, S. Poria, Y. Li, E. Cambria</div><div id="ref-id-sbref0381" class="title text-m">Phonetic-enriched text representation for Chinese sentiment analysis with reinforcement learning</div></div><div class="host u-font-sans">Inf. Fusion., 70 (2021), pp. 88-99, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2021.01.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2021.01.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253521000117/pdfft?md5=a64912622e22e16e981d466e46440e12&amp;pid=1-s2.0-S1566253521000117-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0381"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253521000117" aria-describedby="ref-id-sbref0381"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85099510729&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0381"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Phonetic-enriched%20text%20representation%20for%20Chinese%20sentiment%20analysis%20with%20reinforcement%20learning&amp;publication_year=2021&amp;author=H.%20Peng&amp;author=Y.%20Ma&amp;author=S.%20Poria&amp;author=Y.%20Li&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0381"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0382" id="ref-id-bib0382" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[382]</span></span></a></span><span class="reference" id="sbref0382"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, A. Gelbukh</div><div id="ref-id-sbref0382" class="title text-m">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</div></div><div class="host u-font-sans">Proc. 2015 Conf. Empir. Methods Nat. Lang. Process., Association for Computational Linguistics, Lisbon, Portugal (2015), pp. 2539-2544, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/D15-1303" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/D15-1303</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84943617823&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0382"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20convolutional%20neural%20network%20textual%20features%20and%20multiple%20kernel%20learning%20for%20utterance-level%20multimodal%20sentiment%20analysis&amp;publication_year=2015&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=A.%20Gelbukh" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0382"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0383" id="ref-id-bib0383" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[383]</span></span></a></span><span class="reference" id="sbref0383"><div class="contribution"><div class="authors u-font-sans">S. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, L.-P. Morency</div><div id="ref-id-sbref0383" class="title text-m">Context-dependent sentiment analysis in user-generated videos</div></div><div class="host u-font-sans">Proc. 55th Annu. Meet. Assoc. Comput. Linguist. Vol. 1 Long Pap., Association for Computational Linguistics, Vancouver, Canada (2017), pp. 873-883, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P17-1081" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P17-1081</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85039788178&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0383"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Context-dependent%20sentiment%20analysis%20in%20user-generated%20videos&amp;publication_year=2017&amp;author=S.%20Poria&amp;author=E.%20Cambria&amp;author=D.%20Hazarika&amp;author=N.%20Majumder&amp;author=A.%20Zadeh&amp;author=L.-P.%20Morency" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0383"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0384" id="ref-id-bib0384" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[384]</span></span></a></span><span class="reference" id="sbref0384"><div class="contribution"><div class="authors u-font-sans">P. Schmidt, A. Reiss, R. Dürichen, K. Van Laerhoven</div><div id="ref-id-sbref0384" class="title text-m">Wearable-based affect recognition—a review</div></div><div class="host u-font-sans">Sensors, 19 (2019), p. 4079, <a class="anchor anchor-primary" href="https://doi.org/10.3390/s19194079" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/s19194079</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85072569038&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0384"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Wearable-based%20affect%20recognitiona%20review&amp;publication_year=2019&amp;author=P.%20Schmidt&amp;author=A.%20Reiss&amp;author=R.%20D%C3%BCrichen&amp;author=K.%20Van%20Laerhoven" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0384"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0385" id="ref-id-bib0385" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[385]</span></span></a></span><span class="reference" id="sbref0385"><div class="contribution"><div class="authors u-font-sans">C. Li, C. Xu, Z. Feng</div><div id="ref-id-sbref0385" class="title text-m">Analysis of physiological for emotion recognition with the IRS model</div></div><div class="host u-font-sans">Neurocomputing, 178 (2016), pp. 103-111, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.neucom.2015.07.112" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.neucom.2015.07.112</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0925231215016045/pdfft?md5=934e1826fde5eeb06609708ec0e52a14&amp;pid=1-s2.0-S0925231215016045-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0385"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0925231215016045" aria-describedby="ref-id-sbref0385"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84957428476&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0385"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Analysis%20of%20physiological%20for%20emotion%20recognition%20with%20the%20IRS%20model&amp;publication_year=2016&amp;author=C.%20Li&amp;author=C.%20Xu&amp;author=Z.%20Feng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0385"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0386" id="ref-id-bib0386" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[386]</span></span></a></span><span class="reference" id="sbref0386"><div class="contribution"><div class="authors u-font-sans">B. Nakisa, M.N. Rastgoo, A. Rakotonirainy, F. Maire, V. Chandran</div><div id="ref-id-sbref0386" class="title text-m">Long short term memory hyperparameter optimization for a neural network based emotion recognition framework</div></div><div class="host u-font-sans">IEEE Access, 6 (2018), pp. 49325-49338, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2018.2868361" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2018.2868361</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85052840246&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0386"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Long%20short%20term%20memory%20hyperparameter%20optimization%20for%20a%20neural%20network%20based%20emotion%20recognition%20framework&amp;publication_year=2018&amp;author=B.%20Nakisa&amp;author=M.N.%20Rastgoo&amp;author=A.%20Rakotonirainy&amp;author=F.%20Maire&amp;author=V.%20Chandran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0386"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0387" id="ref-id-bib0387" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[387]</span></span></a></span><span class="reference" id="sbref0387"><div class="contribution"><div class="authors u-font-sans">M.M. Hassan, Md.G.R. Alam, Md.Z. Uddin, S. Huda, A. Almogren, G. Fortino</div><div id="ref-id-sbref0387" class="title text-m">Human emotion recognition using deep belief network architecture</div></div><div class="host u-font-sans">Inf. Fusion., 51 (2019), pp. 10-18, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2018.10.009" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2018.10.009</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253518301301/pdfft?md5=7945518d8671644de02bab6a5aa6913a&amp;pid=1-s2.0-S1566253518301301-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0387"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253518301301" aria-describedby="ref-id-sbref0387"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85056201045&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0387"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Human%20emotion%20recognition%20using%20deep%20belief%20network%20architecture&amp;publication_year=2019&amp;author=M.M.%20Hassan&amp;author=Md.G.R.%20Alam&amp;author=Md.Z.%20Uddin&amp;author=S.%20Huda&amp;author=A.%20Almogren&amp;author=G.%20Fortino" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0387"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0388" id="ref-id-bib0388" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[388]</span></span></a></span><span class="reference" id="sbref0388"><div class="contribution"><div class="authors u-font-sans">J. Ma, H. Tang, W.-L. Zheng, B.-L. Lu</div><div id="ref-id-sbref0388" class="title text-m">Emotion recognition using multimodal residual LSTM network</div></div><div class="host u-font-sans">Proc. 27th ACM Int. Conf. Multimed., ACM, Nice France (2019), pp. 176-183, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3343031.3350871" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3343031.3350871</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85074842066&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0388"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20multimodal%20residual%20LSTM%20network&amp;publication_year=2019&amp;author=J.%20Ma&amp;author=H.%20Tang&amp;author=W.-L.%20Zheng&amp;author=B.-L.%20Lu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0388"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0389" id="ref-id-bib0389" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[389]</span></span></a></span><span class="reference" id="sbref0389"><div class="contribution"><div class="authors u-font-sans">W. Wei, Q. Jia, Y. Feng, G. Chen</div><div id="ref-id-sbref0389" class="title text-m">Emotion recognition based on weighted fusion strategy of multichannel physiological signals</div></div><div class="host u-font-sans">Comput. Intell. Neurosci., 2018 (2018), Article e5296523, <a class="anchor anchor-primary" href="https://doi.org/10.1155/2018/5296523" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1155/2018/5296523</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20based%20on%20weighted%20fusion%20strategy%20of%20multichannel%20physiological%20signals&amp;publication_year=2018&amp;author=W.%20Wei&amp;author=Q.%20Jia&amp;author=Y.%20Feng&amp;author=G.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0389"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0390" id="ref-id-bib0390" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[390]</span></span></a></span><span class="reference" id="sbref0390"><div class="contribution"><div class="authors u-font-sans">C. Li, Z. Bao, L. Li, Z. Zhao</div><div id="ref-id-sbref0390" class="title text-m">Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition</div></div><div class="host u-font-sans">Inf. Process. Manag., 57 (2020), Article 102185, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.ipm.2019.102185" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.ipm.2019.102185</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0306457319307204/pdfft?md5=7e46312d7672d08efdad2ae8f0630cac&amp;pid=1-s2.0-S0306457319307204-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0390"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0306457319307204" aria-describedby="ref-id-sbref0390"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85077270857&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0390"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Exploring%20temporal%20representations%20by%20leveraging%20attention-based%20bidirectional%20LSTM-RNNs%20for%20multi-modal%20emotion%20recognition&amp;publication_year=2020&amp;author=C.%20Li&amp;author=Z.%20Bao&amp;author=L.%20Li&amp;author=Z.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0390"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0391" id="ref-id-bib0391" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[391]</span></span></a></span><span class="reference" id="sbref0391"><div class="contribution"><div class="authors u-font-sans">M.N. Dar, M.U. Akram, S.G. Khawaja, A.N. Pujari</div><div id="ref-id-sbref0391" class="title text-m">CNN and LSTM-based emotion charting using physiological signals</div></div><div class="host u-font-sans">Sensors, 20 (2020), p. 4551, <a class="anchor anchor-primary" href="https://doi.org/10.3390/s20164551" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.3390/s20164551</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=CNN%20and%20LSTM-based%20emotion%20charting%20using%20physiological%20signals&amp;publication_year=2020&amp;author=M.N.%20Dar&amp;author=M.U.%20Akram&amp;author=S.G.%20Khawaja&amp;author=A.N.%20Pujari" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0391"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0392" id="ref-id-bib0392" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[392]</span></span></a></span><span class="reference" id="sbref0392"><div class="contribution"><div class="authors u-font-sans">Z. Yin, M. Zhao, Y. Wang, J. Yang, J. Zhang</div><div id="ref-id-sbref0392" class="title text-m">Recognition of emotions using multimodal physiological signals and an ensemble deep learning model</div></div><div class="host u-font-sans">Comput. Methods Programs Biomed., 140 (2017), pp. 93-110, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.cmpb.2016.12.005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.cmpb.2016.12.005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0169260716305090/pdfft?md5=5e0a6a8f5a66d451204f9edae56b9a83&amp;pid=1-s2.0-S0169260716305090-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0392"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0169260716305090" aria-describedby="ref-id-sbref0392"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85006761745&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0392"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Recognition%20of%20emotions%20using%20multimodal%20physiological%20signals%20and%20an%20ensemble%20deep%20learning%20model&amp;publication_year=2017&amp;author=Z.%20Yin&amp;author=M.%20Zhao&amp;author=Y.%20Wang&amp;author=J.%20Yang&amp;author=J.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0392"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0393" id="ref-id-bib0393" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[393]</span></span></a></span><span class="reference" id="sbref0393"><div class="contribution"><div class="authors u-font-sans">L. He, D. Jiang, L. Yang, E. Pei, P. Wu, H. Sahli</div><div id="ref-id-sbref0393" class="title text-m">Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks</div></div><div class="host u-font-sans">Proc. 5th Int. Workshop Audiov. Emot. Chall. - AVEC 15, ACM Press, Brisbane, Australia (2015), pp. 73-80, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2808196.2811641" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2808196.2811641</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84960882502&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0393"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20affective%20dimension%20prediction%20using%20deep%20bidirectional%20long%20short-term%20memory%20recurrent%20neural%20networks&amp;publication_year=2015&amp;author=L.%20He&amp;author=D.%20Jiang&amp;author=L.%20Yang&amp;author=E.%20Pei&amp;author=P.%20Wu&amp;author=H.%20Sahli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0393"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0394" id="ref-id-bib0394" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[394]</span></span></a></span><span class="reference" id="sbref0394"><div class="contribution"><div class="authors u-font-sans">H. Ranganathan, S. Chakraborty, S. Panchanathan</div><div id="ref-id-sbref0394" class="title text-m">Multimodal emotion recognition using deep learning architectures</div></div><div class="host u-font-sans">2016 IEEE Winter Conf. Appl. Comput. Vis. WACV (2016), pp. 1-9, <a class="anchor anchor-primary" href="https://doi.org/10.1109/WACV.2016.7477679" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/WACV.2016.7477679</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20emotion%20recognition%20using%20deep%20learning%20architectures&amp;publication_year=2016&amp;author=H.%20Ranganathan&amp;author=S.%20Chakraborty&amp;author=S.%20Panchanathan" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0394"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0395" id="ref-id-bib0395" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[395]</span></span></a></span><span class="reference" id="sbref0395"><div class="contribution"><div class="authors u-font-sans">Y. Lu, W.-L. Zheng, B. Li, B.-L. Lu</div><div id="ref-id-sbref0395" class="title text-m">Combining eye movements and EEG to enhance emotion recognition</div></div><div class="host u-font-sans">Proc. Twenty-Fourth Int. Jt. Conf. Artif. Intell. IJCAI (2015), p. 7</div><div class="comment">(n.d.)</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Combining%20eye%20movements%20and%20EEG%20to%20enhance%20emotion%20recognition&amp;publication_year=2015&amp;author=Y.%20Lu&amp;author=W.-L.%20Zheng&amp;author=B.%20Li&amp;author=B.-L.%20Lu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0395"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0396" id="ref-id-bib0396" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[396]</span></span></a></span><span class="reference" id="sbref0396"><div class="contribution"><div class="authors u-font-sans">B. Xing, H. Zhang, K. Zhang, L. Zhang, X. Wu, X. Shi, S. Yu, S. Zhang</div><div id="ref-id-sbref0396" class="title text-m">Exploiting EEG signals and audiovisual feature fusion for video emotion recognition</div></div><div class="host u-font-sans">IEEE Access, 7 (2019), pp. 59844-59861, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2019.2914872" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2019.2914872</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85065957097&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0396"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Exploiting%20EEG%20signals%20and%20audiovisual%20feature%20fusion%20for%20video%20emotion%20recognition&amp;publication_year=2019&amp;author=B.%20Xing&amp;author=H.%20Zhang&amp;author=K.%20Zhang&amp;author=L.%20Zhang&amp;author=X.%20Wu&amp;author=X.%20Shi&amp;author=S.%20Yu&amp;author=S.%20Zhang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0396"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0397" id="ref-id-bib0397" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[397]</span></span></a></span><span class="reference" id="sbref0397"><div class="other-ref"><span>G. Yin, S. Sun, D. Yu, D. Li, K. Zhang, A efficient multimodal framework for large scale emotion recognition by fusing music and electrodermal activity signals, ArXiv200809743 Cs. (2021). <a class="anchor anchor-primary" href="http://arxiv.org/abs/2008.09743" target="_blank"><span class="anchor-text-container"><span class="anchor-text">http://arxiv.org/abs/2008.09743</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a> (accessed December 13, 2021).</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=G.%20Yin%2C%20S.%20Sun%2C%20D.%20Yu%2C%20D.%20Li%2C%20K.%20Zhang%2C%20A%20efficient%20multimodal%20framework%20for%20large%20scale%20emotion%20recognition%20by%20fusing%20music%20and%20electrodermal%20activity%20signals%2C%20ArXiv200809743%20Cs.%20(2021).%20http%3A%2F%2Farxiv.org%2Fabs%2F2008.09743%20(accessed%20December%2013%2C%202021)." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0398" id="ref-id-bib0398" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[398]</span></span></a></span><span class="reference" id="sbref0398"><div class="contribution"><div class="authors u-font-sans">W. Liu, W.-L. Zheng, B.-L. Lu</div><div id="ref-id-sbref0398" class="title text-m">Emotion recognition using multimodal deep learning</div></div><div class="host u-font-sans">Proc. 23rd Int. Conf. Neural Inf. Process. - Vol. 9948, Springer-Verlag, Berlin, Heidelberg (2016), pp. 521-529, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-46672-9_58" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-46672-9_58</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84992665958&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0398"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Emotion%20recognition%20using%20multimodal%20deep%20learning&amp;publication_year=2016&amp;author=W.%20Liu&amp;author=W.-L.%20Zheng&amp;author=B.-L.%20Lu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0398"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0399" id="ref-id-bib0399" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[399]</span></span></a></span><span class="reference" id="sbref0399"><div class="contribution"><div class="authors u-font-sans">M. Soleymani, S. Asghari-Esfeden, Y. Fu, M. Pantic</div><div id="ref-id-sbref0399" class="title text-m">Analysis of EEG signals and facial expressions for continuous emotion detection</div></div><div class="host u-font-sans">IEEE Trans. Affect. Comput., 7 (2016), pp. 17-28, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TAFFC.2015.2436926" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TAFFC.2015.2436926</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84963876689&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0399"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Analysis%20of%20EEG%20signals%20and%20facial%20expressions%20for%20continuous%20emotion%20detection&amp;publication_year=2016&amp;author=M.%20Soleymani&amp;author=S.%20Asghari-Esfeden&amp;author=Y.%20Fu&amp;author=M.%20Pantic" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0399"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0400" id="ref-id-bib0400" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[400]</span></span></a></span><span class="reference" id="sbref0400"><div class="contribution"><div class="authors u-font-sans">D. Wu, J. Zhang, Q. Zhao</div><div id="ref-id-sbref0400" class="title text-m">Multimodal fused emotion recognition about expression-EEG interaction and collaboration using deep learning</div></div><div class="host u-font-sans">IEEE Access, 8 (2020), pp. 133180-133189, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ACCESS.2020.3010311" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ACCESS.2020.3010311</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089302989&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0400"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20fused%20emotion%20recognition%20about%20expression-EEG%20interaction%20and%20collaboration%20using%20deep%20learning&amp;publication_year=2020&amp;author=D.%20Wu&amp;author=J.%20Zhang&amp;author=Q.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0400"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0401" id="ref-id-bib0401" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[401]</span></span></a></span><span class="reference" id="sbref0401"><div class="contribution"><div class="authors u-font-sans">K. Zhang, H. Zhang, S. Li, C. Yang, L. Sun</div><div id="ref-id-sbref0401" class="title text-m">The PMEmo dataset for music emotion recognition</div></div><div class="host u-font-sans">Proc. 2018 ACM Int. Conf. Multimed. Retr., Association for Computing Machinery, New York, NY, USA (2018), pp. 135-142, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3206025.3206037" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3206025.3206037</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=The%20PMEmo%20dataset%20for%20music%20emotion%20recognition&amp;publication_year=2018&amp;author=K.%20Zhang&amp;author=H.%20Zhang&amp;author=S.%20Li&amp;author=C.%20Yang&amp;author=L.%20Sun" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0401"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0402" id="ref-id-bib0402" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[402]</span></span></a></span><span class="reference" id="sbref0402"><div class="contribution"><div class="authors u-font-sans">K. Shuang, Q. Yang, J. Loo, R. Li, M. Gu</div><div id="ref-id-sbref0402" class="title text-m">Feature distillation network for aspect-based sentiment analysis</div></div><div class="host u-font-sans">Inf. Fusion., 61 (2020), pp. 13-23, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.03.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.03.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253519303951/pdfft?md5=15d4b44188983d32f08dbdcde45ea2fd&amp;pid=1-s2.0-S1566253519303951-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0402"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253519303951" aria-describedby="ref-id-sbref0402"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85082007693&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0402"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature%20distillation%20network%20for%20aspect-based%20sentiment%20analysis&amp;publication_year=2020&amp;author=K.%20Shuang&amp;author=Q.%20Yang&amp;author=J.%20Loo&amp;author=R.%20Li&amp;author=M.%20Gu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0402"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0403" id="ref-id-bib0403" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[403]</span></span></a></span><span class="reference" id="sbref0403"><div class="contribution"><div class="authors u-font-sans">H. Fang, N. Mac Parthaláin, A.J. Aubrey, G.K.L. Tam, R. Borgo, P.L. Rosin, P.W. Grant, D. Marshall, M. Chen</div><div id="ref-id-sbref0403" class="title text-m">Facial expression recognition in dynamic sequences: An integrated approach</div></div><div class="host u-font-sans">Pattern Recognit., 47 (2014), pp. 1271-1281, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.patcog.2013.09.023" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.patcog.2013.09.023</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0031320313003956/pdfft?md5=f9f22d71953b37b6f00c9286d43b2967&amp;pid=1-s2.0-S0031320313003956-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0403"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0031320313003956" aria-describedby="ref-id-sbref0403"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84888348216&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0403"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20recognition%20in%20dynamic%20sequences%3A%20An%20integrated%20approach&amp;publication_year=2014&amp;author=H.%20Fang&amp;author=N.%20Mac%20Parthal%C3%A1in&amp;author=A.J.%20Aubrey&amp;author=G.K.L.%20Tam&amp;author=R.%20Borgo&amp;author=P.L.%20Rosin&amp;author=P.W.%20Grant&amp;author=D.%20Marshall&amp;author=M.%20Chen" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0403"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0404" id="ref-id-bib0404" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[404]</span></span></a></span><span class="reference" id="sbref0404"><div class="contribution"><div class="authors u-font-sans">J. Chen, B. Hu, P. Moore, X. Zhang, X. Ma</div><div id="ref-id-sbref0404" class="title text-m">Electroencephalogram-based emotion assessment system using ontology and data mining techniques</div></div><div class="host u-font-sans">Appl. Soft Comput., 30 (2015), pp. 663-674, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.asoc.2015.01.007" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.asoc.2015.01.007</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1568494615000083/pdfft?md5=e94b3034eba3df9c484aba11b38d7c6a&amp;pid=1-s2.0-S1568494615000083-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0404"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1568494615000083" aria-describedby="ref-id-sbref0404"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84923798530&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0404"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Electroencephalogram-based%20emotion%20assessment%20system%20using%20ontology%20and%20data%20mining%20techniques&amp;publication_year=2015&amp;author=J.%20Chen&amp;author=B.%20Hu&amp;author=P.%20Moore&amp;author=X.%20Zhang&amp;author=X.%20Ma" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0404"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0405" id="ref-id-bib0405" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[405]</span></span></a></span><span class="reference" id="sbref0405"><div class="contribution"><div class="authors u-font-sans">S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, R. Mihalcea</div><div id="ref-id-sbref0405" class="title text-m">MELD: a multimodal multi-party dataset for emotion recognition in conversations</div></div><div class="host u-font-sans">Proc. 57th Annu. Meet. Assoc. Comput. Linguist., Association for Computational Linguistics, Florence, Italy (2019), pp. 527-536, <a class="anchor anchor-primary" href="https://doi.org/10.18653/v1/P19-1050" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.18653/v1/P19-1050</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=MELD%3A%20a%20multimodal%20multi-party%20dataset%20for%20emotion%20recognition%20in%20conversations&amp;publication_year=2019&amp;author=S.%20Poria&amp;author=D.%20Hazarika&amp;author=N.%20Majumder&amp;author=G.%20Naik&amp;author=E.%20Cambria&amp;author=R.%20Mihalcea" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0405"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0406" id="ref-id-bib0406" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[406]</span></span></a></span><span class="reference" id="sbref0406"><div class="contribution"><div class="authors u-font-sans">S.A. Abdu, A.H. Yousef, A. Salem</div><div id="ref-id-sbref0406" class="title text-m">Multimodal video sentiment analysis using deep learning approaches, a survey</div></div><div class="host u-font-sans">Inf. Fusion., 76 (2021), pp. 204-226, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2021.06.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2021.06.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253521001299/pdfft?md5=52efe7c5b41548c61f87888c19b49ade&amp;pid=1-s2.0-S1566253521001299-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0406"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253521001299" aria-describedby="ref-id-sbref0406"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85108096174&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0406"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multimodal%20video%20sentiment%20analysis%20using%20deep%20learning%20approaches%2C%20a%20survey&amp;publication_year=2021&amp;author=S.A.%20Abdu&amp;author=A.H.%20Yousef&amp;author=A.%20Salem" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0406"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0407" id="ref-id-bib0407" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[407]</span></span></a></span><span class="reference" id="sbref0407"><div class="contribution"><div class="authors u-font-sans">S. Petridis, M. Pantic</div><div id="ref-id-sbref0407" class="title text-m">Audiovisual discrimination between speech and laughter: why and when visual information might help</div></div><div class="host u-font-sans">IEEE Trans. Multimed., 13 (2011), pp. 216-234, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TMM.2010.2101586" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TMM.2010.2101586</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-79952983096&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0407"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Audiovisual%20discrimination%20between%20speech%20and%20laughter%3A%20why%20and%20when%20visual%20information%20might%20help&amp;publication_year=2011&amp;author=S.%20Petridis&amp;author=M.%20Pantic" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0407"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0408" id="ref-id-bib0408" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[408]</span></span></a></span><span class="reference" id="sbref0408"><div class="contribution"><div class="authors u-font-sans">Devangini Patel, X. Hong, G. Zhao</div><div id="ref-id-sbref0408" class="title text-m">Selective deep features for micro-expression recognition</div></div><div class="host u-font-sans">2016 23rd Int. Conf. Pattern Recognit. ICPR (2016), pp. 2258-2263, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICPR.2016.7899972" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICPR.2016.7899972</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85019109097&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0408"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Selective%20deep%20features%20for%20micro-expression%20recognition&amp;publication_year=2016&amp;author=Devangini%20Patel&amp;author=X.%20Hong&amp;author=G.%20Zhao" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0408"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0409" id="ref-id-bib0409" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[409]</span></span></a></span><span class="reference" id="sbref0409"><div class="contribution"><div class="authors u-font-sans">A.C.L. Ngo, C.W. Phan, J. See</div><div id="ref-id-sbref0409" class="title text-m">Spontaneous subtle expression recognition: imbalanced databases and solutions</div></div><div class="host u-font-sans">Asian Conf. Comput. Vis (2014), <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-16817-3_3" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-16817-3_3</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Spontaneous%20subtle%20expression%20recognition%3A%20imbalanced%20databases%20and%20solutions&amp;publication_year=2014&amp;author=A.C.L.%20Ngo&amp;author=C.W.%20Phan&amp;author=J.%20See" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0409"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0410" id="ref-id-bib0410" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[410]</span></span></a></span><span class="reference" id="sbref0410"><div class="contribution"><div class="authors u-font-sans">X. Jia, X. Ben, H. Yuan, K. Kpalma, W. Meng</div><div id="ref-id-sbref0410" class="title text-m">Macro-to-micro transformation model for micro-expression recognition</div></div><div class="host u-font-sans">J. Comput. Sci., 25 (2018), pp. 289-297, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.jocs.2017.03.016" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.jocs.2017.03.016</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1877750317303198/pdfft?md5=4ef1e3ae8ec457ea347a516d0ed661f3&amp;pid=1-s2.0-S1877750317303198-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0410"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1877750317303198" aria-describedby="ref-id-sbref0410"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85016400510&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0410"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Macro-to-micro%20transformation%20model%20for%20micro-expression%20recognition&amp;publication_year=2018&amp;author=X.%20Jia&amp;author=X.%20Ben&amp;author=H.%20Yuan&amp;author=K.%20Kpalma&amp;author=W.%20Meng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0410"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0411" id="ref-id-bib0411" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[411]</span></span></a></span><span class="reference" id="sbref0411"><div class="contribution"><div class="authors u-font-sans">H. Zhang, W. Su, J. Yu, Z. Wang</div><div id="ref-id-sbref0411" class="title text-m">Weakly supervised local-global relation network for facial expression recognition</div></div><div class="host u-font-sans">Proc. Twenty-Ninth Int. Jt. Conf. Artif. Intell., International Joint Conferences on Artificial Intelligence Organization, Yokohama, Japan (2020), pp. 1040-1046, <a class="anchor anchor-primary" href="https://doi.org/10.24963/ijcai.2020/145" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.24963/ijcai.2020/145</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85095494691&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0411"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Weakly%20supervised%20local-global%20relation%20network%20for%20facial%20expression%20recognition&amp;publication_year=2020&amp;author=H.%20Zhang&amp;author=W.%20Su&amp;author=J.%20Yu&amp;author=Z.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0411"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0412" id="ref-id-bib0412" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[412]</span></span></a></span><span class="reference" id="sbref0412"><div class="contribution"><div class="authors u-font-sans">B. Schuller, M. Valstar, F. Eyben, G. McKeown, R. Cowie, M. Pantic</div><div id="ref-id-sbref0412" class="title text-m">AVEC 2011–The first international audio/visual emotion challenge</div></div><div class="host u-font-sans">S. D'Mello, A. Graesser, B. Schuller, J.-C. Martin (Eds.), Affective Computing and Intelligent Interaction, Springer, Berlin, Heidelberg (2011), pp. 415-424, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-642-24571-8_53" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-642-24571-8_53</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-80054831955&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0412"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AVEC%202011The%20first%20international%20audiovisual%20emotion%20challenge&amp;publication_year=2011&amp;author=B.%20Schuller&amp;author=M.%20Valstar&amp;author=F.%20Eyben&amp;author=G.%20McKeown&amp;author=R.%20Cowie&amp;author=M.%20Pantic" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0412"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0413" id="ref-id-bib0413" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[413]</span></span></a></span><span class="reference" id="sbref0413"><div class="contribution"><div class="authors u-font-sans">F. Ringeval, A. Michaud, E. Ciftçi, H. Güleç, A.A. Salah, M. Pantic, B. Schuller, M. Valstar, R. Cowie, H. Kaya, M. Schmitt, S. Amiriparian, N. Cummins, D. Lalanne</div><div id="ref-id-sbref0413" class="title text-m">AVEC 2018 workshop and challenge: bipolar disorder and cross-cultural affect recognition</div></div><div class="host u-font-sans">Proc. 2018 Audiov. Emot. Chall. Workshop - AVEC18, ACM Press, Seoul, Republic of Korea (2018), pp. 3-13, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3266302.3266316" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3266302.3266316</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AVEC%202018%20workshop%20and%20challenge%3A%20bipolar%20disorder%20and%20cross-cultural%20affect%20recognition&amp;publication_year=2018&amp;author=F.%20Ringeval&amp;author=A.%20Michaud&amp;author=E.%20Cift%C3%A7i&amp;author=H.%20G%C3%BCle%C3%A7&amp;author=A.A.%20Salah&amp;author=M.%20Pantic&amp;author=B.%20Schuller&amp;author=M.%20Valstar&amp;author=R.%20Cowie&amp;author=H.%20Kaya&amp;author=M.%20Schmitt&amp;author=S.%20Amiriparian&amp;author=N.%20Cummins&amp;author=D.%20Lalanne" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0413"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0414" id="ref-id-bib0414" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[414]</span></span></a></span><span class="reference" id="sbref0414"><div class="contribution"><div class="authors u-font-sans">Ercheng Pei, Xiaohan Xia, Le Yang, Dongmei Jiang, H. Sahli</div><div id="ref-id-sbref0414" class="title text-m">Deep neural network and switching Kalman filter based continuous affect recognition</div></div><div class="host u-font-sans">2016 IEEE Int. Conf. Multimed. Expo Workshop ICMEW (2016), pp. 1-6, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICMEW.2016.7574729" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICMEW.2016.7574729</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20neural%20network%20and%20switching%20Kalman%20filter%20based%20continuous%20affect%20recognition&amp;publication_year=2016&amp;author=Ercheng%20Pei&amp;author=Xiaohan%20Xia&amp;author=Le%20Yang&amp;author=Dongmei%20Jiang&amp;author=H.%20Sahli" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0414"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0415" id="ref-id-bib0415" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[415]</span></span></a></span><span class="reference" id="sbref0415"><div class="contribution"><div class="authors u-font-sans">Y. Song, L.-P. Morency, R. Davis</div><div id="ref-id-sbref0415" class="title text-m">Learning a sparse codebook of facial and body microexpressions for emotion recognition</div></div><div class="host u-font-sans">Proc. 15th ACM Int. Conf. Multimodal Interact., Association for Computing Machinery, New York, NY, USA (2013), pp. 237-244, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2522848.2522851" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2522848.2522851</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0039914013004451/pdfft?md5=d2d540a3dd10a813cf29cf93a8a77f0d&amp;pid=1-s2.0-S0039914013004451-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0415"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0039914013004451" aria-describedby="ref-id-sbref0415"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Learning%20a%20sparse%20codebook%20of%20facial%20and%20body%20microexpressions%20for%20emotion%20recognition&amp;publication_year=2013&amp;author=Y.%20Song&amp;author=L.-P.%20Morency&amp;author=R.%20Davis" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0415"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0416" id="ref-id-bib0416" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[416]</span></span></a></span><span class="reference" id="sbref0416"><div class="contribution"><div class="authors u-font-sans">M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M.Torres Torres, S. Scherer, G. Stratou, R. Cowie, M. Pantic</div><div id="ref-id-sbref0416" class="title text-m">AVEC 2016: depression, mood, and emotion recognition workshop and challenge</div></div><div class="host u-font-sans">Proc. 6th Int. Workshop Audiov. Emot. Chall., Association for Computing Machinery, New York, NY, USA (2016), pp. 3-10, <a class="anchor anchor-primary" href="https://doi.org/10.1145/2988257.2988258" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/2988257.2988258</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84995553765&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0416"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=AVEC%202016%3A%20depression%2C%20mood%2C%20and%20emotion%20recognition%20workshop%20and%20challenge&amp;publication_year=2016&amp;author=M.%20Valstar&amp;author=J.%20Gratch&amp;author=B.%20Schuller&amp;author=F.%20Ringeval&amp;author=D.%20Lalanne&amp;author=M.Torres%20Torres&amp;author=S.%20Scherer&amp;author=G.%20Stratou&amp;author=R.%20Cowie&amp;author=M.%20Pantic" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0416"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0417" id="ref-id-bib0417" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[417]</span></span></a></span><span class="reference" id="sbref0417"><div class="contribution"><div class="authors u-font-sans">Y. Zhang, D. Song, X. Li, P. Zhang, P. Wang, L. Rong, G. Yu, B. Wang</div><div id="ref-id-sbref0417" class="title text-m">A Quantum-Like multimodal network framework for modeling interaction dynamics in multiparty conversational sentiment analysis</div></div><div class="host u-font-sans">Inf. Fusion., 62 (2020), pp. 14-31, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.04.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.04.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253520302554/pdfft?md5=4515051bc9b527fe7fd67a3dcf9c339e&amp;pid=1-s2.0-S1566253520302554-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0417"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253520302554" aria-describedby="ref-id-sbref0417"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20Quantum-Like%20multimodal%20network%20framework%20for%20modeling%20interaction%20dynamics%20in%20multiparty%20conversational%20sentiment%20analysis&amp;publication_year=2020&amp;author=Y.%20Zhang&amp;author=D.%20Song&amp;author=X.%20Li&amp;author=P.%20Zhang&amp;author=P.%20Wang&amp;author=L.%20Rong&amp;author=G.%20Yu&amp;author=B.%20Wang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0417"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0418" id="ref-id-bib0418" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[418]</span></span></a></span><span class="reference" id="sbref0418"><div class="contribution"><div class="authors u-font-sans">P. Tzirakis, J. Chen, S. Zafeiriou, B. Schuller</div><div id="ref-id-sbref0418" class="title text-m">End-to-end multimodal affect recognition in real-world environments</div></div><div class="host u-font-sans">Inf. Fusion., 68 (2021), pp. 46-53, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.10.011" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.10.011</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253520303808/pdfft?md5=4d5d302e308d7bf2f65ca0892b834f5c&amp;pid=1-s2.0-S1566253520303808-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0418"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253520303808" aria-describedby="ref-id-sbref0418"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85095916539&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0418"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=End-to-end%20multimodal%20affect%20recognition%20in%20real-world%20environments&amp;publication_year=2021&amp;author=P.%20Tzirakis&amp;author=J.%20Chen&amp;author=S.%20Zafeiriou&amp;author=B.%20Schuller" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0418"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0419" id="ref-id-bib0419" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[419]</span></span></a></span><span class="reference" id="sbref0419"><div class="contribution"><div class="authors u-font-sans">D. Camacho, M.V. Luzón, E. Cambria</div><div id="ref-id-sbref0419" class="title text-m">New trends and applications in social media analytics</div></div><div class="host u-font-sans">Future Gener. Comput. Syst., 114 (2021), pp. 318-321, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.future.2020.08.007" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.future.2020.08.007</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167739X20324924/pdfft?md5=0826acb5a7ddd660a608e5ed1de24625&amp;pid=1-s2.0-S0167739X20324924-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0419"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0167739X20324924" aria-describedby="ref-id-sbref0419"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85089506782&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0419"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=New%20trends%20and%20applications%20in%20social%20media%20analytics&amp;publication_year=2021&amp;author=D.%20Camacho&amp;author=M.V.%20Luz%C3%B3n&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0419"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0420" id="ref-id-bib0420" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[420]</span></span></a></span><span class="reference" id="sbref0420"><div class="contribution"><div class="authors u-font-sans">E. Cambria, C. Havasi, A. Hussain</div><div id="ref-id-sbref0420" class="title text-m">SenticNet 2: A semantic and affective resource for opinion mining and sentiment analysis</div></div><div class="host u-font-sans">Fla. Artif. Intell. Res. Soc. Conf. (2012), pp. 202-207</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84864992296&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0420"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SenticNet%202%3A%20A%20semantic%20and%20affective%20resource%20for%20opinion%20mining%20and%20sentiment%20analysis&amp;publication_year=2012&amp;author=E.%20Cambria&amp;author=C.%20Havasi&amp;author=A.%20Hussain" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0420"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0421" id="ref-id-bib0421" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[421]</span></span></a></span><span class="reference" id="sbref0421"><div class="contribution"><div class="authors u-font-sans">E. Cambria, D. Olsher, D. Rajagopal</div><div id="ref-id-sbref0421" class="title text-m">SenticNet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis</div></div><div class="host u-font-sans">Proc. Twenty-Eighth AAAI Conf. Artif. Intell., AAAI Press, Québec City, Québec, Canada (2014), pp. 1515-1521</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SenticNet%203%3A%20a%20common%20and%20common-sense%20knowledge%20base%20for%20cognition-driven%20sentiment%20analysis&amp;publication_year=2014&amp;author=E.%20Cambria&amp;author=D.%20Olsher&amp;author=D.%20Rajagopal" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0421"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0422" id="ref-id-bib0422" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[422]</span></span></a></span><span class="reference" id="sbref0422"><div class="other-ref"><span>E. Cambria, S. Poria, R. Bajpai, B. Schuller, SenticNet 4: A Semantic resource for sentiment analysis based on conceptual primitives, in: COLING2016, Osaka, Japan, 2016: pp. 2666–2677.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=E.%20Cambria%2C%20S.%20Poria%2C%20R.%20Bajpai%2C%20B.%20Schuller%2C%20SenticNet%204%3A%20A%20Semantic%20resource%20for%20sentiment%20analysis%20based%20on%20conceptual%20primitives%2C%20in%3A%20COLING2016%2C%20Osaka%2C%20Japan%2C%202016%3A%20pp.%202666%E2%80%932677." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0423" id="ref-id-bib0423" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[423]</span></span></a></span><span class="reference" id="sbref0423"><div class="contribution"><div class="authors u-font-sans">E. Cambria, S. Poria, D. Hazarika, K. Kwok</div><div id="ref-id-sbref0423" class="title text-m">SenticNet 5: discovering conceptual primitives for sentiment analysis by means of context embeddings</div></div><div class="host u-font-sans">Proc. Twenty-Eighth AAAI Conf. Artif. Intell. (2018), pp. 1795-1802</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85039765367&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0423"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SenticNet%205%3A%20discovering%20conceptual%20primitives%20for%20sentiment%20analysis%20by%20means%20of%20context%20embeddings&amp;publication_year=2018&amp;author=E.%20Cambria&amp;author=S.%20Poria&amp;author=D.%20Hazarika&amp;author=K.%20Kwok" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0423"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0424" id="ref-id-bib0424" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[424]</span></span></a></span><span class="reference" id="sbref0424"><div class="contribution"><div class="authors u-font-sans">E. Cambria, Y. Li, F.Z. Xing, S. Poria, K. Kwok</div><div id="ref-id-sbref0424" class="title text-m">SenticNet 6: ensemble application of symbolic and subsymbolic AI for sentiment analysis</div></div><div class="host u-font-sans">Proc. 29th ACM Int. Conf. Inf. Knowl. Manag., ACM, Virtual Event Ireland (2020), pp. 105-114, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3340531.3412003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3340531.3412003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85095720560&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0424"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=SenticNet%206%3A%20ensemble%20application%20of%20symbolic%20and%20subsymbolic%20AI%20for%20sentiment%20analysis&amp;publication_year=2020&amp;author=E.%20Cambria&amp;author=Y.%20Li&amp;author=F.Z.%20Xing&amp;author=S.%20Poria&amp;author=K.%20Kwok" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0424"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0425" id="ref-id-bib0425" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[425]</span></span></a></span><span class="reference" id="sbref0425"><div class="contribution"><div class="authors u-font-sans">Y. Ma, H. Peng, E. Cambria</div><div id="ref-id-sbref0425" class="title text-m">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</div></div><div class="host u-font-sans">Proc. AAAI Conf. Artif. Intell. (2018), pp. 5876-5883</div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Targeted%20aspect-based%20sentiment%20analysis%20via%20embedding%20commonsense%20knowledge%20into%20an%20attentive%20LSTM&amp;publication_year=2018&amp;author=Y.%20Ma&amp;author=H.%20Peng&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0425"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0426" id="ref-id-bib0426" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[426]</span></span></a></span><span class="reference" id="sbref0426"><div class="contribution"><div class="authors u-font-sans">F.Z. Xing, E. Cambria, R.E. Welsch</div><div id="ref-id-sbref0426" class="title text-m">Intelligent asset allocation via market sentiment views</div></div><div class="host u-font-sans">IEEE Comput. Intell. Mag., 13 (2018), pp. 25-34, <a class="anchor anchor-primary" href="https://doi.org/10.1109/MCI.2018.2866727" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/MCI.2018.2866727</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85055283489&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0426"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Intelligent%20asset%20allocation%20via%20market%20sentiment%20views&amp;publication_year=2018&amp;author=F.Z.%20Xing&amp;author=E.%20Cambria&amp;author=R.E.%20Welsch" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0426"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0427" id="ref-id-bib0427" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[427]</span></span></a></span><span class="reference" id="sbref0427"><div class="contribution"><div class="authors u-font-sans">A. Picasso, S. Merello, Y. Ma, L. Oneto, E. Cambria</div><div id="ref-id-sbref0427" class="title text-m">Technical analysis and sentiment embeddings for market trend prediction</div></div><div class="host u-font-sans">Expert Syst. Appl., 135 (2019), pp. 60-70, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.eswa.2019.06.014" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.eswa.2019.06.014</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0957417419304142/pdfft?md5=257fea04820bafa8fa14bea1e442b79d&amp;pid=1-s2.0-S0957417419304142-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0427"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S0957417419304142" aria-describedby="ref-id-sbref0427"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85067037094&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0427"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Technical%20analysis%20and%20sentiment%20embeddings%20for%20market%20trend%20prediction&amp;publication_year=2019&amp;author=A.%20Picasso&amp;author=S.%20Merello&amp;author=Y.%20Ma&amp;author=L.%20Oneto&amp;author=E.%20Cambria" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0427"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0428" id="ref-id-bib0428" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[428]</span></span></a></span><span class="reference" id="sbref0428"><div class="contribution"><div class="authors u-font-sans">Y. Qian, Y. Zhang, X. Ma, H. Yu, L. Peng</div><div id="ref-id-sbref0428" class="title text-m">EARS: Emotion-aware recommender system based on hybrid information fusion</div></div><div class="host u-font-sans">Inf. Fusion., 46 (2019), pp. 141-146, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2018.06.004" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2018.06.004</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253518301234/pdfft?md5=01ba97dcbb2aa27c69b747a135dcc094&amp;pid=1-s2.0-S1566253518301234-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0428"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253518301234" aria-describedby="ref-id-sbref0428"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85048977367&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0428"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=EARS%3A%20Emotion-aware%20recommender%20system%20based%20on%20hybrid%20information%20fusion&amp;publication_year=2019&amp;author=Y.%20Qian&amp;author=Y.%20Zhang&amp;author=X.%20Ma&amp;author=H.%20Yu&amp;author=L.%20Peng" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0428"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0429" id="ref-id-bib0429" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[429]</span></span></a></span><span class="reference" id="sbref0429"><div class="contribution"><div class="authors u-font-sans">D. Xu, Z. Tian, R. Lai, X. Kong, Z. Tan, W. Shi</div><div id="ref-id-sbref0429" class="title text-m">Deep learning based emotion analysis of microblog texts</div></div><div class="host u-font-sans">Inf. Fusion., 64 (2020), pp. 1-11, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.06.002" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.06.002</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S156625352030302X/pdfft?md5=6fe40c2b84b18c4b0fc19e1740412512&amp;pid=1-s2.0-S156625352030302X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0429"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S156625352030302X" aria-describedby="ref-id-sbref0429"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85086725184&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0429"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20based%20emotion%20analysis%20of%20microblog%20texts&amp;publication_year=2020&amp;author=D.%20Xu&amp;author=Z.%20Tian&amp;author=R.%20Lai&amp;author=X.%20Kong&amp;author=Z.%20Tan&amp;author=W.%20Shi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0429"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0430" id="ref-id-bib0430" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[430]</span></span></a></span><span class="reference" id="sbref0430"><div class="contribution"><div class="authors u-font-sans">D. Yang, A. Alsadoon, P.W.C. Prasad, A.K. Singh, A. Elchouemi</div><div id="ref-id-sbref0430" class="title text-m">An emotion recognition model based on facial recognition in virtual learning environment</div></div><div class="host u-font-sans">Procedia Comput. Sci., 125 (2018), pp. 2-10, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.procs.2017.12.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.procs.2017.12.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S037843201730948X/pdfft?md5=6bb1995d2bbb1106648230f49184153a&amp;pid=1-s2.0-S037843201730948X-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0430"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S037843201730948X" aria-describedby="ref-id-sbref0430"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=An%20emotion%20recognition%20model%20based%20on%20facial%20recognition%20in%20virtual%20learning%20environment&amp;publication_year=2018&amp;author=D.%20Yang&amp;author=A.%20Alsadoon&amp;author=P.W.C.%20Prasad&amp;author=A.K.%20Singh&amp;author=A.%20Elchouemi" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0430"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0431" id="ref-id-bib0431" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[431]</span></span></a></span><span class="reference" id="sbref0431"><div class="contribution"><div class="authors u-font-sans">C. Zuheros, E. Martínez-Cámara, E. Herrera-Viedma, F. Herrera</div><div id="ref-id-sbref0431" class="title text-m">Sentiment analysis based multi-person multi-criteria decision making methodology using natural language processing and deep learning for smarter decision aid. Case study of restaurant choice using TripAdvisor reviews</div></div><div class="host u-font-sans">Inf. Fusion., 68 (2021), pp. 22-36, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.10.019" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.10.019</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253520304000/pdfft?md5=f9c235fb1d5184af4e2fdbcbc1cd09ab&amp;pid=1-s2.0-S1566253520304000-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0431"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253520304000" aria-describedby="ref-id-sbref0431"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85095781765&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0431"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Sentiment%20analysis%20based%20multi-person%20multi-criteria%20decision%20making%20methodology%20using%20natural%20language%20processing%20and%20deep%20learning%20for%20smarter%20decision%20aid.%20Case%20study%20of%20restaurant%20choice%20using%20TripAdvisor%20reviews&amp;publication_year=2021&amp;author=C.%20Zuheros&amp;author=E.%20Mart%C3%ADnez-C%C3%A1mara&amp;author=E.%20Herrera-Viedma&amp;author=F.%20Herrera" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0431"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0432" id="ref-id-bib0432" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[432]</span></span></a></span><span class="reference" id="sbref0432"><div class="contribution"><div class="authors u-font-sans">L. Zhang, B. Verma, D. Tjondronegoro, V. Chandran</div><div id="ref-id-sbref0432" class="title text-m">Facial expression analysis under partial occlusion: a survey</div></div><div class="host u-font-sans">ACM Comput. Surv., 51 (2018), pp. 1-49, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3158369" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3158369</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Facial%20expression%20analysis%20under%20partial%20occlusion%3A%20a%20survey&amp;publication_year=2018&amp;author=L.%20Zhang&amp;author=B.%20Verma&amp;author=D.%20Tjondronegoro&amp;author=V.%20Chandran" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0432"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0433" id="ref-id-bib0433" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[433]</span></span></a></span><span class="reference" id="sbref0433"><div class="contribution"><div class="authors u-font-sans">N. Savva, A. Scarinzi, N. Bianchi-Berthouze</div><div id="ref-id-sbref0433" class="title text-m">Continuous recognition of player's affective body expression as dynamic quality of aesthetic experience</div></div><div class="host u-font-sans">IEEE Trans. Comput. Intell. AI Games., 4 (2012), pp. 199-212, <a class="anchor anchor-primary" href="https://doi.org/10.1109/TCIAIG.2012.2202663" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/TCIAIG.2012.2202663</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84866512354&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0433"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Continuous%20recognition%20of%20players%20affective%20body%20expression%20as%20dynamic%20quality%20of%20aesthetic%20experience&amp;publication_year=2012&amp;author=N.%20Savva&amp;author=A.%20Scarinzi&amp;author=N.%20Bianchi-Berthouze" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0433"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0434" id="ref-id-bib0434" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[434]</span></span></a></span><span class="reference" id="sbref0434"><div class="contribution"><div class="authors u-font-sans">K. Kaza, A. Psaltis, K. Stefanidis, K.C. Apostolakis, S. Thermos, K. Dimitropoulos, P. Daras</div><div id="ref-id-sbref0434" class="title text-m">Body motion analysis for emotion recognition in serious games</div></div><div class="host u-font-sans">M. Antona, C. Stephanidis (Eds.), Universal Access in Human-Computer Interaction. Interaction Techniques and Environments, Springer International Publishing, Cham (2016), pp. 33-42, <a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-319-40244-4_4" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/978-3-319-40244-4_4</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-84978907394&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0434"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Body%20motion%20analysis%20for%20emotion%20recognition%20in%20serious%20games&amp;publication_year=2016&amp;author=K.%20Kaza&amp;author=A.%20Psaltis&amp;author=K.%20Stefanidis&amp;author=K.C.%20Apostolakis&amp;author=S.%20Thermos&amp;author=K.%20Dimitropoulos&amp;author=P.%20Daras" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0434"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0435" id="ref-id-bib0435" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[435]</span></span></a></span><span class="reference" id="sbref0435"><div class="contribution"><div class="authors u-font-sans">W. Dong, L. Yang, R. Gravina, G. Fortino</div><div id="ref-id-sbref0435" class="title text-m">ANFIS fusion algorithm for eye movement recognition via soft multi-functional electronic skin</div></div><div class="host u-font-sans">Inf. Fusion., 71 (2021), pp. 99-108, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2021.02.003" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2021.02.003</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253521000191/pdfft?md5=04f56c88b56219d1ba62e2b6c8a32a71&amp;pid=1-s2.0-S1566253521000191-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0435"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253521000191" aria-describedby="ref-id-sbref0435"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85101314985&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0435"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=ANFIS%20fusion%20algorithm%20for%20eye%20movement%20recognition%20via%20soft%20multi-functional%20electronic%20skin&amp;publication_year=2021&amp;author=W.%20Dong&amp;author=L.%20Yang&amp;author=R.%20Gravina&amp;author=G.%20Fortino" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0435"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0436" id="ref-id-bib0436" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[436]</span></span></a></span><span class="reference" id="sbref0436"><div class="contribution"><div class="authors u-font-sans">H. Cai, Z. Qu, Z. Li, Y. Zhang, X. Hu, B. Hu</div><div id="ref-id-sbref0436" class="title text-m">Feature-level fusion approaches based on multimodal EEG data for depression recognition</div></div><div class="host u-font-sans">Inf. Fusion., 59 (2020), pp. 127-138, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2020.01.008" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2020.01.008</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253519302143/pdfft?md5=3ec4f1da1fff5ab186c56ccd06081dbb&amp;pid=1-s2.0-S1566253519302143-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0436"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253519302143" aria-describedby="ref-id-sbref0436"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85079327800&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0436"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Feature-level%20fusion%20approaches%20based%20on%20multimodal%20EEG%20data%20for%20depression%20recognition&amp;publication_year=2020&amp;author=H.%20Cai&amp;author=Z.%20Qu&amp;author=Z.%20Li&amp;author=Y.%20Zhang&amp;author=X.%20Hu&amp;author=B.%20Hu" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0436"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0437" id="ref-id-bib0437" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[437]</span></span></a></span><span class="reference" id="sbref0437"><div class="other-ref"><span>S. Piana, A. Staglianò, A. Camurri, A set of full-body movement features for emotion recognition to help children affected by autism spectrum condition, in: 2013: p. 7.</span></div><div class="ReferenceLinks u-font-sans"><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar?q=S.%20Piana%2C%20A.%20Staglian%C3%B2%2C%20A.%20Camurri%2C%20A%20set%20of%20full-body%20movement%20features%20for%20emotion%20recognition%20to%20help%20children%20affected%20by%20autism%20spectrum%20condition%2C%20in%3A%202013%3A%20p.%207." target="_blank" rel="noopener noreferrer"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0438" id="ref-id-bib0438" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[438]</span></span></a></span><span class="reference" id="sbref0438"><div class="contribution"><div class="authors u-font-sans">L.O. Sawada, L.Y. Mano, J.R. Torres Neto, J. Ueyama</div><div id="ref-id-sbref0438" class="title text-m">A module-based framework to emotion recognition by speech: a case study in clinical simulation</div></div><div class="host u-font-sans">J. Ambient Intell. Humaniz. Comput. (2019), <a class="anchor anchor-primary" href="https://doi.org/10.1007/s12652-019-01280-8" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1007/s12652-019-01280-8</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=A%20module-based%20framework%20to%20emotion%20recognition%20by%20speech%3A%20a%20case%20study%20in%20clinical%20simulation&amp;publication_year=2019&amp;author=L.O.%20Sawada&amp;author=L.Y.%20Mano&amp;author=J.R.%20Torres%20Neto&amp;author=J.%20Ueyama" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0438"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0439" id="ref-id-bib0439" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[439]</span></span></a></span><span class="reference" id="sbref0439"><div class="contribution"><div class="authors u-font-sans">E. Kanjo, E.M.G. Younis, C.S. Ang</div><div id="ref-id-sbref0439" class="title text-m">Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection</div></div><div class="host u-font-sans">Inf. Fusion., 49 (2019), pp. 46-56, <a class="anchor anchor-primary" href="https://doi.org/10.1016/j.inffus.2018.09.001" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1016/j.inffus.2018.09.001</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><a class="anchor pdf link anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1566253518300460/pdfft?md5=b4ca3aa8b38227f2d786639de4aa48ad&amp;pid=1-s2.0-S1566253518300460-main.pdf" target="_blank" rel="nofollow" aria-describedby="ref-id-sbref0439"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a><a class="anchor link anchor-primary" href="/science/article/pii/S1566253518300460" aria-describedby="ref-id-sbref0439"><span class="anchor-text-container"><span class="anchor-text">View article</span></span></a><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85054071151&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0439"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Deep%20learning%20analysis%20of%20mobile%20physiological%2C%20environmental%20and%20location%20sensor%20data%20for%20emotion%20detection&amp;publication_year=2019&amp;author=E.%20Kanjo&amp;author=E.M.G.%20Younis&amp;author=C.S.%20Ang" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0439"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0440" id="ref-id-bib0440" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[440]</span></span></a></span><span class="reference" id="sbref0440"><div class="contribution"><div class="authors u-font-sans">D.H. Kim, M.K. Lee, D.Y. Choi, B.C. Song</div><div id="ref-id-sbref0440" class="title text-m">Multi-modal emotion recognition using semi-supervised learning and multiple neural networks in the wild</div></div><div class="host u-font-sans">Proc. 19th ACM Int. Conf. Multimodal Interact., Association for Computing Machinery, New York, NY, USA (2017), pp. 529-535, <a class="anchor anchor-primary" href="https://doi.org/10.1145/3136755.3143005" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1145/3136755.3143005</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85046638757&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0440"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi-modal%20emotion%20recognition%20using%20semi-supervised%20learning%20and%20multiple%20neural%20networks%20in%20the%20wild&amp;publication_year=2017&amp;author=D.H.%20Kim&amp;author=M.K.%20Lee&amp;author=D.Y.%20Choi&amp;author=B.C.%20Song" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0440"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li><li><span class="label u-font-sans"><a class="anchor anchor-primary" href="#bbib0441" id="ref-id-bib0441" data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name"><span class="anchor-text-container"><span class="anchor-text">[441]</span></span></a></span><span class="reference" id="sbref0441"><div class="contribution"><div class="authors u-font-sans">A. Tsiami, P. Koutras, N. Efthymiou, P.P. Filntisis, G. Potamianos, P. Maragos</div><div id="ref-id-sbref0441" class="title text-m">Multi3: multi-sensory perception system for multi-modal child interaction with multiple robots</div></div><div class="host u-font-sans">2018 IEEE Int. Conf. Robot. Autom. ICRA (2018), pp. 4585-4592, <a class="anchor anchor-primary" href="https://doi.org/10.1109/ICRA.2018.8461210" target="_blank"><span class="anchor-text-container"><span class="anchor-text">10.1109/ICRA.2018.8461210</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div><div class="ReferenceLinks u-font-sans"><span class="link lazy-third-party-pdf-link"><span></span></span><a class="anchor link anchor-primary" href="https://www.scopus.com/inward/record.url?eid=2-s2.0-85063145395&amp;partnerID=10&amp;rel=R3.0.0" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0441"><span class="anchor-text-container"><span class="anchor-text">View in Scopus</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a><a class="anchor link anchor-primary" href="https://scholar.google.com/scholar_lookup?title=Multi3%3A%20multi-sensory%20perception%20system%20for%20multi-modal%20child%20interaction%20with%20multiple%20robots&amp;publication_year=2018&amp;author=A.%20Tsiami&amp;author=P.%20Koutras&amp;author=N.%20Efthymiou&amp;author=P.P.%20Filntisis&amp;author=G.%20Potamianos&amp;author=P.%20Maragos" target="_blank" rel="noopener noreferrer" aria-describedby="ref-id-sbref0441"><span class="anchor-text-container"><span class="anchor-text">Google Scholar</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></div></span></li></ol></section></section></div><div id="section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (211)</h2></header><div aria-describedby="citing-articles-header"><div class="citing-articles u-margin-l-bottom"><ul><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-0-title"><a class="anchor anchor-primary" href="/science/article/pii/S156625352300163X"><span class="anchor-text-container"><span class="anchor-text">Emotion recognition from unimodal to multimodal analysis: A review</span></span></a></h3><div>2023, Information Fusion</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-0-title" aria-controls="citing-articles-article-0" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="d1e1392">The omnipresence of numerous information sources in our daily life brings up new alternatives for emotion recognition in several domains including e-health, e-learning, robotics, and e-commerce. Due to the variety of data, the research area of multimodal machine learning poses special problems for computer scientists; how did the field of emotion recognition progress in each modality and what are the most common strategies for recognizing emotions? What part does deep learning play in this? What is multimodality? How did it progress? What are the methods of information fusion? What are the most used datasets in each modality and in multimodal recognition? We can understand and compare the various methods by answering these questions.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-1-title"><a class="anchor anchor-primary" href="/science/article/pii/S0261517722002205"><span class="anchor-text-container"><span class="anchor-text">Restaurant survival prediction using customer-generated content: An aspect-based sentiment analysis of online reviews</span></span></a></h3><div>2023, Tourism Management</div><div class="CitedSection u-margin-s-top"><div class="u-margin-s-left"><div class="cite-header u-text-italic u-font-sans">Citation Excerpt :</div><p class="u-font-serif text-xs">Sentic GCN is a novel method for building graph neural networks by incorporating SenticNet affective knowledge to improve sentence dependency graphs. To acquire contextual representations, LSTM layers were used in Sentic GCN, and GCN layers were developed to identify connections between contextual words in specific aspects (Wang et al., 2022). This approach considers the dependencies of contextual terms and aspect terms as well as the affective information between opinion terms and aspect terms.</p></div></div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-1-title" aria-controls="citing-articles-article-1" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="abspara0010">Business failure prediction or survival analysis can assist corporate organizations in better understanding their performance and improving decision making. Based on aspect-based sentiment analysis (ABSA), this study investigates the effect of customer-generated content (i.e., online reviews) in predicting restaurant survival using datasets for restaurants in two world famous tourism destinations in the United States. ABSA divides the overall review sentiment of each online review into five categories, namely location, tastiness, price, service, and atmosphere. By employing the machine learning–based conditional survival forest model, empirical results show that compared with overall review sentiment, aspect-based sentiment for various factors can improve the prediction performance of restaurant survival. Based on feature importance analysis, this study also highlights the effects of different types of aspect sentiment on restaurant survival prediction to identify which features of online reviews are optimal indicators of restaurant survival.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-2-title"><a class="anchor anchor-primary" href="/science/article/pii/S0925231223000103"><span class="anchor-text-container"><span class="anchor-text">An ongoing review of speech emotion recognition</span></span></a></h3><div>2023, Neurocomputing</div></div><div class="buttons"><button class="button-link button-link-primary button-link-icon-right" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby="citing-articles-article-2-title" aria-controls="citing-articles-article-2" aria-expanded="false" type="button"><span class="button-link-text-container"><span class="button-link-text">Show abstract</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"><div class="u-margin-ver-m"><div class="u-margin-s-bottom" id="sp010">User emotional status recognition is becoming a key feature in advanced Human Computer Interfaces (HCI). A key source of emotional information is the spoken expression, which may be part of the interaction between the human and the machine. Speech emotion recognition (SER) is a very active area of research that involves the application of current machine learning and neural networks tools. This ongoing review covers recent and classical approaches to SER reported in the literature.</div></div></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-3-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/JIOT.2023.3265848" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Virtual-Reality Interpromotion Technology for Metaverse: A Survey</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, IEEE Internet of Things Journal</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-4-title"><a class="anchor anchor-primary" href="https://doi.org/10.1109/MCOM.001.2200272" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Design and Implementation of a Flexible Neuromorphic Computing System for Affective Communication via Memristive Circuits</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2023, IEEE Communications Magazine</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li><li class="ListArticleItem u-margin-l-bottom"><div class="sub-heading u-margin-xs-bottom"><h3 class="u-font-serif" id="citing-articles-article-5-title"><a class="anchor anchor-primary" href="https://doi.org/10.1007/978-3-031-19772-7_31" target="_blank"><span class="anchor-text-container"><span class="anchor-text">Shape Matters: Deformable Patch Attack</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></h3><div>2022, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</div></div><div class="buttons"></div><div class="u-display-none" aria-hidden="true"><div class="abstract u-margin-xs-top u-margin-m-bottom u-font-serif" id="reference-abstract"></div></div></li></ul><a class="button-alternative button-alternative-secondary large-alternative button-alternative-icon-left" href="http://www.scopus.com/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-85127142402&amp;md5=721619608be988b0ad93101b927fe16f" target="_blank" id="citing-articles-view-all-btn"><svg focusable="false" viewBox="0 0 54 128" height="20" class="icon icon-navigate-right"><path d="M1 99l38-38L1 23l7-7 45 45-45 45z"></path></svg><span class="button-alternative-text-container"><span class="button-alternative-text">View all citing articles on Scopus</span></span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></a></div></div></section></div><a class="anchor abstract-link anchor-primary" href="/science/article/abs/pii/S1566253522000367"><span class="anchor-text-container"><span class="anchor-text">View Abstract</span></span></a><div class="Copyright"><span class="copyright-line">© 2022 Elsevier B.V. All rights reserved.</span></div></article><div class="u-display-block-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0163638323000371" title="The interplay between cognition and emotion during infancy and toddlerhood: Special issue editorial"><span class="anchor-text-container"><span class="anchor-text"><span>The interplay between cognition and emotion during infancy and toddlerhood: Special issue editorial</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Infant Behavior and Development, Volume 72, 2023, Article 101845</div></div><div class="authors"><span>Christy D.</span> <span>Wolfe</span>, <span>Martha Ann</span> <span>Bell</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0163638323000371/pdfft?md5=901c2fb3ea84205a2b021d2a9c967fd2&amp;pid=1-s2.0-S0163638323000371-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article0-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0306457319307204" title="Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multi-modal emotion recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Information Processing &amp; Management, Volume 57, Issue 3, 2020, Article 102185</div></div><div class="authors"><span>Chao</span> <span>Li</span>, …, <span>Ziping</span> <span>Zhao</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0306457319307204/pdfft?md5=7e46312d7672d08efdad2ae8f0630cac&amp;pid=1-s2.0-S0306457319307204-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article1-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1877050922013345" title="A note on the affective computing systems and machines: a classification and appraisal"><span class="anchor-text-container"><span class="anchor-text"><span>A note on the affective computing systems and machines: a classification and appraisal</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Procedia Computer Science, Volume 207, 2022, pp. 3798-3807</div></div><div class="authors"><span>Paweł</span> <span>Weichbroth</span>, <span>Wiktor</span> <span>Sroka</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1877050922013345/pdf?md5=5d7adc162886dc09f5365593c41fee15&amp;pid=1-s2.0-S1877050922013345-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article2-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S0167865521000878" title="Leveraging recent advances in deep learning for audio-Visual emotion recognition"><span class="anchor-text-container"><span class="anchor-text"><span>Leveraging recent advances in deep learning for audio-Visual emotion recognition</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Pattern Recognition Letters, Volume 146, 2021, pp. 1-7</div></div><div class="authors"><span>Liam</span> <span>Schoneveld</span>, …, <span>Hazem</span> <span>Abdelkawy</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S0167865521000878/pdfft?md5=783c34353c7d1ed1afc6e0cbc3d5c6ba&amp;pid=1-s2.0-S0167865521000878-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article3-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S1877050916324607" title="Affective Computing as Complex Systems Science"><span class="anchor-text-container"><span class="anchor-text"><span>Affective Computing as Complex Systems Science</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Procedia Computer Science, Volume 95, 2016, pp. 18-23</div></div><div class="authors"><span>William</span> <span>Lee</span>, <span>Michael D.</span> <span>Norman</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S1877050916324607/pdf?md5=a9876dc455037755bc3723624d8e6f05&amp;pid=1-s2.0-S1877050916324607-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article4-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-primary" href="/science/article/pii/S157106611930009X" title="Emotion Recognition from Physiological Signal Analysis: A Review"><span class="anchor-text-container"><span class="anchor-text"><span>Emotion Recognition from Physiological Signal Analysis: A Review</span></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Electronic Notes in Theoretical Computer Science, Volume 343, 2019, pp. 35-55</div></div><div class="authors"><span>Maria</span> <span>Egger</span>, …, <span>Sten</span> <span>Hanke</span></div></div><div class="buttons"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="/science/article/pii/S157106611930009X/pdf?md5=85290eb14fd158853bb41bb60597f9c4&amp;pid=1-s2.0-S157106611930009X-main.pdf" target="_blank" rel="nofollow" aria-describedby="recommended-articles-article5-title"><svg focusable="false" viewBox="0 0 35 32" height="20" class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span class="anchor-text-container"><span class="anchor-text">View PDF</span></span></a></div></li></ul></div><button class="button-link more-recommendations-button u-margin-s-bottom button-link-primary button-link-icon-right" type="button"><span class="button-link-text-container"><span class="button-link-text">Show 3 more articles</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link button-link-secondary related-content-panel-toggle is-up button-link-icon-right button-link-has-colored-icon" aria-expanded="true" type="button"><span class="button-link-text-container"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span></span><svg focusable="false" viewBox="0 0 92 128" height="20" class="icon icon-navigate-down"><path d="M1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class="" aria-hidden="false" aria-describedby="metrics-header"><div class="plum-sciencedirect-theme"><div class="PlumX-Summary"><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-inverse-logo.png" class="plx-logo"></div><div class="pps-cols"><div class="pps-col plx-citation"><div class="plx-citation"><div class="pps-title">Citations</div><ul><li class="plx-citation"><span class="pps-label">Citation Indexes: </span><span class="pps-count">205</span></li><li class="plx-citation"><span class="pps-label">Policy Citations: </span><span class="pps-count">2</span></li></ul></div></div><div class="pps-col plx-capture"><div class="plx-capture"><div class="pps-title">Captures</div><ul><li class="plx-capture"><span class="pps-label">Readers: </span><span class="pps-count">352</span></li></ul></div></div><div class="pps-col plx-mention"><div class="plx-mention"><div class="pps-title">Mentions</div><ul><li class="plx-mention"><span class="pps-label">References: </span><span class="pps-count">1</span></li></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src="//cdn.plu.mx/3ba727faf225e19d2c759f6ebffc511d/plumx-logo.png" class="plx-logo"></div><a target="_blank" href="https://plu.mx/plum/a/?doi=10.1016/j.inffus.2022.03.009&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class="pps-seemore" title="PlumX Metrics Detail Page">View details<svg fill="currentColor" tabindex="-1" focusable="false" width="16" height="16" viewBox="0 0 16 16" class="svg-arrow"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy"></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS1566253522000367" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Remote access</span></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://service.elsevier.com/ci/pta/login/redirect/contact/supporthub/sciencedirect/p_li/xyhJEXWoHAMsz8VQOu3h2LHHIekBL3hT38urfpOCxeAty3TF61ibE7vsQPaZh4C6srqm9MNOxus2v65ykDkikA4MoWWax9xsxmy7qS6Jhq1MDk8fC~ZGE1crT~IU0fB88KgH29YS658ka3vhWTqW3QsF38M7JrWC3EPvnlMgTGg39gl1xRqh4DNloSU9hGNUBWmK_Teu~5LuRZ6Al86jUzFxO7CiLa3WIvaVAP5f3g900Gfh5TZfq33GuKqA9hfimBRbW0fFzY3a21RwOu4PLl5c4DckcHvqhukSQOSa3Ps*" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li><li><a class="anchor u-display-flex u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-primary anchor-small" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text-container"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 8 8" height="20" aria-label="Opens in new window" class="icon icon-arrow-up-right-tiny arrow-external-link"><path d="M1.12949 2.1072V1H7V6.85795H5.89111V2.90281L0.784057 8L0 7.21635L5.11902 2.1072H1.12949Z"></path></svg></span></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary button-link-small" id="ot-sdk-btn" type="button">Cookie Settings</button></p><p id="els-footer-copyright">All content on this site: Copyright © <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-primary anchor-icon-left anchor-with-icon" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page"></a></div></footer></div></div></div></div>
<div id="floating-ui-node" class="floating-ui-node" data-sd-ui-floating-ui="true"></div>
<script async="" src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="text/javascript"></script>
<script type="text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S1566253522000367","type":"sd:article:JL:scope-full","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"1566-2535","volumeNumber":"83","suppl":"C","provider":"elsevier","entitlementType":"package"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"full-direct","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1728990721798,"loadTime":""},"visitor":{"accessType":"ae:REG_SHIBBOLETH","accountId":"ae:50401","accountName":"ae:IT University of Copenhagen","loginStatus":"logged in","userId":"ae:71970787","ipAddress":"77.165.246.201","appSessionId":"6465a12a-1939-4136-ac19-1d08b328e0a8"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
<script nomodule="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react/18.3.1/react.production.min.js" type="text/javascript"></script>
<script src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/shared-assets/108/js/react-dom/18.3.1/react-dom.production.min.js" type="text/javascript"></script>
<script async="" src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/fe5f1a9a67d6a2bd1341815211e4a5a7e50a5117/arp.js" type="text/javascript"></script>
<script type="text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:REG_SHIBBOLETH","countryCode":"NL"},"account":{"id":"ae:50401","name":"ae:IT University of Copenhagen"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
<span id="pendo-answer-rating"></span>
<script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="text/javascript"></script>
<script async="" src="https://www.googletagservices.com/tag/js/gpt.js" type="text/javascript"></script>
<script async="" src="https://scholar.google.com/scholar_js/casa.js" type="text/javascript"></script>
<script data-cfasync="false">
      (function initOneTrust()  {
        const monitor = {
  init: () => {},
  loaded: () => {},
};
        function enableGroup(group) {
  document.querySelectorAll(`script[type*="ot-${group}"]`).forEach(script => {
    script.type = 'text/javascript';
    document.head.appendChild(script);
  });
}
        function runOneTrustCookies(doClear, monitor) {
  const oneTrustConsentSdkId = 'onetrust-consent-sdk';
  const emptyNodeSelectors = 'h3.ot-host-name, h4.ot-host-desc, button.ot-host-box';
  const ariaLabelledByButtonNodes = 'div.ot-accordion-layout > button';
  const ariaAttribute = 'aria-labelledby';
  function adjustOneTrustDOM() {
    const oneTrustRoot = document.getElementById('onetrust-consent-sdk');

    /* remove empty nodes */
    [...(oneTrustRoot?.querySelectorAll(emptyNodeSelectors) ?? [])].filter(e => e.textContent === '').forEach(e => e.remove());

    /* remove invalid aria-labelledby values */
    oneTrustRoot?.querySelectorAll(ariaLabelledByButtonNodes).forEach(e => {
      const presentIdValue = e.getAttribute(ariaAttribute)?.split(' ').filter(label => document.getElementById(label)).join(' ');
      if (presentIdValue) {
        e.setAttribute(ariaAttribute, presentIdValue);
      }
    });
  }
  function observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent) {
    const cb = (mutationList, observer) => {
      const oneTrustRoot = mutationList.filter(mutationRecord => mutationRecord.type === 'childList' && mutationRecord.addedNodes.length).map(mutationRecord => [...mutationRecord.addedNodes]).flat().find(e => e.id === oneTrustConsentSdkId);
      if (oneTrustRoot && typeof OneTrust !== 'undefined') {
        monitor.loaded(true);
        OneTrust.OnConsentChanged(() => {
          const perfAllowed = decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1]?.match('2:([0|1])')[1] === '1';
          if (perfAllowed) {
            enableGroup('performance');
          }
        });
        if (!isConsentPresent && (shouldSetOTDefaults || OneTrust.GetDomainData().ConsentModel.Name === 'implied consent')) {
          OneTrust.AllowAll();
        }
        document.dispatchEvent(new CustomEvent('@sdtech/onetrust/loaded', {}));
        observer.disconnect();
        adjustOneTrustDOM();
      }
    };
    const observer = new MutationObserver(cb);
    observer.observe(document.querySelector('body'), {
      childList: true
    });
  }
  if (doClear) {
    document.cookie = 'OptanonAlertBoxClosed=; expires=Thu, 01 Jan 1970 00:00:00 UTC; samesite=lax; path=/';
  }
  const isConsentPresent = !!decodeURIComponent(document.cookie.match('(^| )OptanonConsent=([^;]+)')?.[2])?.match('groups=([0-9:0|1,?]+)&?')?.[1];
  const shouldSetOTDefaults = 'false' === 'false' && !document.cookie?.match('OptanonAlertBoxClosed=');
  if (shouldSetOTDefaults) {
    const date = new Date();
    date.setFullYear(date.getFullYear() + 1);
    document.cookie = `OptanonAlertBoxClosed=${new Date().toISOString()}; expires=${date.toUTCString()}; samesite=lax; path=/; domain=sciencedirect.com`;
  }
  observeOneTrustLoaded(shouldSetOTDefaults, isConsentPresent, monitor);
  window.addOTScript = () => {
    const otSDK = document.createElement('script');
    otSDK.setAttribute('data-cfasync', 'false');
    otSDK.setAttribute('src', 'https://cdn.cookielaw.org/scripttemplates/otSDKStub.js');
    otSDK.setAttribute('data-document-language', 'true');
    otSDK.setAttribute('data-domain-script', '865ea198-88cc-4e41-8952-1df75d554d02');
    window.addOTScript = () => {};
    document.head.appendChild(otSDK);
    monitor.init();
  };
  window.addEventListener('load', () => window.addOTScript());
}
        if (document.location.host.match(/.sciencedirect.com$/)) {
          runOneTrustCookies(true, monitor);
        }
        else {
          window.addEventListener('load', (event) => {
            enableGroup('performance');
          });
        }
      }());
    </script>

<script>
var pageDataTracker = {
    eventCookieName: 'eventTrack',
    debugCookie: 'els-aa-debugmode',
    debugCounter: 1,
    warnings: [],
    measures: {},
    timeoffset: 0

    ,trackPageLoad: function(data) {
        if (window.pageData && ((pageData.page && pageData.page.noTracking == 'true') || window.pageData_isLoaded)) {
            return false;
        }

        this.updatePageData(data);

        this.initWarnings();
        if(!(window.pageData && pageData.page && pageData.page.name)) {
            console.error('pageDataTracker.trackPageLoad() called without pageData.page.name being defined!');
            return;
        }

        this.processIdPlusData(window.pageData);

        if(window.pageData && pageData.page && !pageData.page.loadTime) {
          pageData.page.loadTime = performance ? Math.round((performance.now())).toString() : '';
        }

        if(window.pageData && pageData.page) {
            var localTime = new Date().getTime();
            if(pageData.page.loadTimestamp) {
                // calculate timeoffset
                var serverTime = parseInt(pageData.page.loadTimestamp);
                if(!isNaN(serverTime)) {
                    this.timeoffset = pageData.page.loadTimestamp - localTime;
                }
            } else {
                pageData.page.loadTimestamp = localTime;
            }
        }

        this.validateData(window.pageData);

        try {
            var cookieTest = 'aa-cookie-test';
            this.setCookie(cookieTest, cookieTest);
            if(this.getCookie(cookieTest) != cookieTest) {
                this.warnings.push('dtm5');
            }
            this.deleteCookie(cookieTest);
        } catch(e){
            this.warnings.push('dtm5');
        }

        this.registerCallbacks();
        this.setAnalyticsData();

        // handle any cookied event data
        this.getEvents();

        window.pageData_isLoaded = true;

        this.debugMessage('Init - trackPageLoad()', window.pageData);

        _satellite.track('eventDispatcher', JSON.stringify({
          eventName: 'newPage',
          eventData: {eventName: 'newPage'},
          pageData: window.pageData
        }));
    }

    ,trackEvent: function(event, data, callback) {
        if (window.pageData && pageData.page && pageData.page.noTracking == 'true') {
            return false;
        }
        
        if(!window.pageData_isLoaded) {
            if(this.isDebugEnabled()) {
                console.log('[AA] pageDataTracker.trackEvent() called without calling trackPageLoad() first.');
            }
            return false;
        }

        if (event) {
            this.initWarnings();
            if(event === 'newPage') {
                // auto fillings
                if(data && data.page && !data.page.loadTimestamp) {
                    data.page.loadTimestamp = ''+(new Date().getTime() + this.timeoffset);
                }
                this.processIdPlusData(data);
            }

            window.eventData = data ? data : {};
            window.eventData.eventName = event;
            if(!_satellite.getVar('blacklisted')) {
                this.handleEventData(event, data);
    
                if(event === 'newPage') {
                    this.validateData(window.pageData);
                }
                this.debugMessage('Event: ' + event, data);
    
                _satellite.track('eventDispatcher', JSON.stringify({
                  eventName: event,
                  eventData: window.eventData,
                  pageData: window.pageData
                }));
            } else {
                this.debugMessage('!! Blocked Event: ' + event, data);
            }
        }

        if (typeof(callback) == 'function') {
            callback.call();
        }
    }

    ,processIdPlusData: function(data) {
        if(data && data.visitor && data.visitor.idPlusData) {
            var idPlusFields = ['userId', 'accessType', 'accountId', 'accountName'];
            for(var i=0; i < idPlusFields.length; i++) {
                if(typeof data.visitor.idPlusData[idPlusFields[i]] !== 'undefined') {
                    data.visitor[idPlusFields[i]] = data.visitor.idPlusData[idPlusFields[i]];
                }
            }
            data.visitor.idPlusData = undefined;
        }
    }

    ,validateData: function(data) {
        if(!data) {
            this.warnings.push('dv0');
            return;
        }

        // top 5
        if(!(data.visitor && data.visitor.accessType)) {
            this.warnings.push('dv1');
        }
        if(data.visitor && (data.visitor.accountId || data.visitor.accountName)) {
            if(!data.visitor.accountName) {
                this.warnings.push('dv2');
            }
            if(!data.visitor.accountId) {
                this.warnings.push('dv3');
            }
        }
        if(!(data.page && data.page.productName)) {
            this.warnings.push('dv4');
        }
        if(!(data.page && data.page.businessUnit)) {
            this.warnings.push('dv5');
        }
        if(!(data.page && data.page.name)) {
            this.warnings.push('dv6');
        }

        // rp mandatory
        if(data.page && data.page.businessUnit && (data.page.businessUnit.toLowerCase().indexOf('els:rp:') !== -1 || data.page.businessUnit.toLowerCase().indexOf('els:rap:') !== -1)) {
            if(!(data.page && data.page.loadTimestamp)) {
                this.warnings.push('dv7');
            }
            if(!(data.page && data.page.loadTime)) {
                this.warnings.push('dv8');
            }
            if(!(data.visitor && data.visitor.ipAddress)) {
                this.warnings.push('dv9');
            }
            if(!(data.page && data.page.type)) {
                this.warnings.push('dv10');
            }
            if(!(data.page && data.page.language)) {
                this.warnings.push('dv11');
            }
        }

        // other
        if(data.page && data.page.environment) {
            var env = data.page.environment.toLowerCase();
            if(!(env === 'dev' || env === 'cert' || env === 'prod')) {
                this.warnings.push('dv12');
            }
        }
        if(data.content && data.content.constructor !== Array) {
            this.warnings.push('dv13');
        }

        if(data.visitor && data.visitor.accountId && data.visitor.accountId.indexOf(':') == -1) {
            this.warnings.push('dv14');
            data.visitor.accountId = "data violation"
        }
    }

    ,initWarnings: function() {
        this.warnings = [];
        try {
            var hdn = document.head.childNodes;
            var libf = false;
            for(var i=0; i<hdn.length; i++) {
                if(hdn[i].src && (hdn[i].src.indexOf('satelliteLib') !== -1 || hdn[i].src.indexOf('launch') !== -1)) {
                    libf = true;
                    break;
                }
            }
            if(!libf) {
                this.warnings.push('dtm1');
            }
        } catch(e) {}

        try {
            for (let element of document.querySelectorAll('*')) {
                // Check if the element has a src attribute and if it meets the criteria
                if (element.src && element.src.includes('assets.adobedtm.com') && element.src.includes('launch')) {
                    if (element.tagName.toLowerCase() !== 'script') {
                        this.warnings.push('dtm5');
                    }
                    if (!element.hasAttribute('async')) {
                        this.warnings.push('dtm4');
                    }
                }
            }
        } catch (e) { }
    }

    ,getMessages: function() {
        return ['v1'].concat(this.warnings).join('|');
    }
    ,addMessage: function(message) {
        this.warnings.push(message);
    }

    ,getPerformance: function() {
        var copy = {};
        for (var attr in this.measures) {
            if(this.measures.hasOwnProperty(attr)) {
                copy[attr] = this.measures[attr];
            }
        }

        this.measures = {};
        return copy;
    }

    ,dtmCodeDesc: {
        dtm1: 'satellite-lib must be placed in the <head> section',
        dtm2: 'trackPageLoad() must be placed and called before the closing </body> tag',
        dtm3: 'trackEvent() must be called at a stage where Document.readyState=complete (e.g. on the load event or a user event)',
        dtm4: 'Embed codes need to be loaded in async mode',
        dtm5: 'Embed codes not in type script',
        dv1: 'visitor.accessType not set but mandatory',
        dv2: 'visitor.accountName not set but mandatory',
        dv3: 'visitor.accountId not set but mandatory',
        dv4: 'page.productName not set but mandatory',
        dv5: 'page.businessUnit not set but mandatory',
        dv6: 'page.name not set but mandatory',
        dv7: 'page.loadTimestamp not set but mandatory',
        dv8: 'page.loadTime not set but mandatory',
        dv9: 'visitor.ipAddress not set but mandatory',
        dv10: 'page.type not set but mandatory',
        dv11: 'page.language not set but mandatory',
        dv12: 'page.environment must be set to \'prod\', \'cert\' or \'dev\'',
        dv13: 'content must be of type array of objects',
        dv14: 'account number must contain at least one \':\', e.g. \'ae:12345\''
    }

    ,debugMessage: function(event, data) {
        if(this.isDebugEnabled()) {
            console.log('[AA] --------- [' + (this.debugCounter++) + '] Web Analytics Data ---------');
            console.log('[AA] ' + event);
            console.groupCollapsed("[AA] AA Data: ");
            if(window.eventData) {
                console.log("[AA] eventData:\n" + JSON.stringify(window.eventData, true, 2));
            }
            if(window.pageData) {
                console.log("[AA] pageData:\n" + JSON.stringify(window.pageData, true, 2));
            }
            console.groupEnd();
            if(this.warnings.length > 0) {
                console.groupCollapsed("[AA] Warnings ("+this.warnings.length+"): ");
                for(var i=0; i<this.warnings.length; i++) {
                    var error = this.dtmCodeDesc[this.warnings[i]] ? this.dtmCodeDesc[this.warnings[i]] : 'Error Code: ' + this.warnings[i];
                    console.log('[AA] ' + error);
                }
                console.log('[AA] More can be found here: https://confluence.cbsels.com/display/AA/AA+Error+Catalog');
                console.groupEnd();
            }
            console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        }
    }

    ,getTrackingCode: function() {
      var campaign = _satellite.getVar('Campaign - ID');
      if(!campaign) {
        campaign = window.sessionStorage ? sessionStorage.getItem('dgcid') : '';
      }
      return campaign;
    }

    ,isDebugEnabled: function() {
        if(typeof this.debug === 'undefined') {
            this.debug = (document.cookie.indexOf(this.debugCookie) !== -1) || (window.pageData && pageData.page && pageData.page.environment && pageData.page.environment.toLowerCase() === 'dev');
            //this.debug = (document.cookie.indexOf(this.debugCookie) !== -1);
        }
        return this.debug;
    }

    ,enableDebug: function(expire) {
        if (typeof expire === 'undefined') {
            expire = 86400;
        }
        console.log('You just enabled debug mode for Adobe Analytics tracking. This mode will persist for 24h.');
        console.log("This mode can be disabled by calling 'pageDataTracker.disableDebug()'");
        this.setCookie(this.debugCookie, 'true', expire, document.location.hostname);
        this.debug = true;
    }

    ,disableDebug: function() {
        console.log('Debug mode is now disabled.');
        this.deleteCookie(this.debugCookie);
        this.debug = false;
    }

    ,setAnalyticsData: function() {
        if(!(window.pageData && pageData.page && pageData.page.productName && pageData.page.name)) {
            return;
        }
        pageData.page.analyticsPagename = pageData.page.productName + ':' + pageData.page.name;

        var pageEls = pageData.page.name.indexOf(':') > -1 ? pageData.page.name.split(':') : [pageData.page.name];
        pageData.page.sectionName = pageData.page.productName + ':' + pageEls[0];
    }

    ,getEvents: function() {
        pageData.savedEvents = {};
        pageData.eventList = [];

        var val = this.getCookie(this.eventCookieName);
        if (val) {
            pageData.savedEvents = val;
        }

        this.deleteCookie(this.eventCookieName);
    }

    ,updatePageData(data) {
        window.pageData = window.pageData || {};
        if (data && typeof(data) === 'object') {
            for (var x in data) {
                if(data.hasOwnProperty(x) && data[x] instanceof Array) {
                    pageData[x] = data[x];
                } else if(data.hasOwnProperty(x) && typeof(data[x]) === 'object') {
                    if(!pageData[x]) {
                        pageData[x] = {};
                    }
                    for (var y in data[x]) {
                        if(data[x].hasOwnProperty(y)) {
                            pageData[x][y] = data[x][y];
                        }
                    }
                }
            }
        }
    }

    ,handleEventData: function(event, data) {
        var val;
        switch(event) {
            case 'newPage':
                this.updatePageData(data);
                this.setAnalyticsData();
            case 'saveSearch':
            case 'searchResultsUpdated':
                if (data) {
                    // overwrite page-load object
                    if (data.search && typeof(data.search) == 'object') {
                        window.eventData.search.resultsPosition = '';
                        pageData.search = pageData.search || {};
                        var fields = ['advancedCriteria', 'criteria', 'currentPage', 'dataFormCriteria', 'facets', 'resultsByType', 'resultsPerPage', 'sortType', 'totalResults', 'type', 'database',
                        'suggestedClickPosition','suggestedLetterCount','suggestedResultCount', 'autoSuggestCategory', 'autoSuggestDetails','typedTerm','selectedTerm', 'channel',
                        'facetOperation', 'details'];
                        for (var i=0; i<fields.length; i++) {
                            if (data.search[fields[i]]) {
                                pageData.search[fields[i]] = data.search[fields[i]];
                            }
                        }
                    }
                }
                this.setAnalyticsData();
                break;
            case 'navigationClick':
                if (data && data.link) {
                    window.eventData.navigationLink = {
                        name: ((data.link.location || 'no location') + ':' + (data.link.name || 'no name'))
                    };
                }
                break;
            case 'autoSuggestClick':
                if (data && data.search) {
                    val = {
                        autoSuggestSearchData: (
                            'letterct:' + (data.search.suggestedLetterCount || 'none') +
                            '|resultct:' + (data.search.suggestedResultCount || 'none') +
                            '|clickpos:' + (data.search.suggestedClickPosition || 'none')
                        ).toLowerCase(),
                        autoSuggestSearchTerm: (data.search.typedTerm || ''),
                        autoSuggestTypedTerm: (data.search.typedTerm || ''),
                        autoSuggestSelectedTerm: (data.search.selectedTerm || ''),
                        autoSuggestCategory: (data.search.autoSuggestCategory || ''),
                        autoSuggestDetails: (data.search.autoSuggestDetails || '')
                    };
                }
                break;
            case 'linkOut':
                if (data && data.content && data.content.length > 0) {
                    window.eventData.linkOut = data.content[0].linkOut;
                    window.eventData.referringProduct = _satellite.getVar('Page - Product Name') + ':' + data.content[0].id;
                }
                break;
            case 'socialShare':
                if (data && data.social) {
                    window.eventData.sharePlatform = data.social.sharePlatform || '';
                }
                break;
            case 'contentInteraction':
                if (data && data.action) {
                    window.eventData.action.name = pageData.page.productName + ':' + data.action.name;
                }
                break;
            case 'searchWithinContent':
                if (data && data.search) {
                    window.pageData.search = window.pageData.search || {};
                    pageData.search.withinContentCriteria = data.search.withinContentCriteria;
                }
                break;
            case 'contentShare':
                if (data && data.content) {
                    window.eventData.sharePlatform = data.content[0].sharePlatform;
                }
                break;
            case 'contentLinkClick':
                if (data && data.link) {
                    window.eventData.action = { name: pageData.page.productName + ':' + (data.link.type || 'no link type') + ':' + (data.link.name || 'no link name') };
                }
                break;
            case 'contentWindowLoad':
            case 'contentTabClick':
                if (data && data.content) {
                    window.eventData.tabName = data.content[0].tabName || '';
                    window.eventData.windowName = data.content[0].windowName || '';
                }
                break;
            case 'userProfileUpdate':
                if (data && data.user) {
                    if (Object.prototype.toString.call(data.user) === "[object Array]") {
                        window.eventData.user = data.user[0];
                    }
                }
                break;
            case 'videoStart':
                if (data.video) {
                    data.video.length = parseFloat(data.video.length || '0');
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.open(data.video.id, data.video.length, s.Media.playerName);
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoPlay':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.play(data.video.id, data.video.position);
                }
                break;
            case 'videoStop':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                }
                break;
            case 'videoComplete':
                if (data.video) {
                    data.video.position = parseFloat(data.video.position || '0');
                    s.Media.stop(data.video.id, data.video.position);
                    s.Media.close(data.video.id);
                }
                break;
            case 'addWebsiteExtension':
                if(data && data.page) {
                    val = {
                        wx: data.page.websiteExtension
                    }
                }
                break;
        }

        if (val) {
            this.setCookie(this.eventCookieName, val);
        }
    }

    ,registerCallbacks: function() {
        var self = this;
        if(window.usabilla_live) {
            window.usabilla_live('setEventCallback', function(category, action, label, value) {
                if(action == 'Campaign:Open') {
                    self.trackEvent('ctaImpression', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                } else if(action == 'Campaign:Success') {
                    self.trackEvent('ctaClick', {
                        cta: {
                            ids: ['usabillaid:' + label]
                        }
                    });
                }
            });
        }
    }

    ,getConsortiumAccountId: function() {
        var id = '';
        if (window.pageData && pageData.visitor && (pageData.visitor.consortiumId || pageData.visitor.accountId)) {
            id = (pageData.visitor.consortiumId || 'no consortium ID') + '|' + (pageData.visitor.accountId || 'no account ID');
        }

        return id;
    }

    ,getSearchClickPosition: function() {
        if (window.eventData && eventData.search && eventData.search.resultsPosition) {
            var pos = parseInt(eventData.search.resultsPosition), clickPos;
            if (!isNaN(pos)) {
                var page = pageData.search.currentPage ? parseInt(pageData.search.currentPage) : '', perPage = pageData.search.resultsPerPage ? parseInt(pageData.search.resultsPerPage) : '';
                if (!isNaN(page) && !isNaN(perPage)) {
                    clickPos = pos + ((page - 1) * perPage);
                }
            }
            return clickPos ? clickPos.toString() : eventData.search.resultsPosition;
        }
        return '';
    }

    ,getSearchFacets: function() {
        var facetList = '';
        if (window.pageData && pageData.search && pageData.search.facets) {
            if (typeof(pageData.search.facets) == 'object') {
                for (var i=0; i<pageData.search.facets.length; i++) {
                    var f = pageData.search.facets[i];
                    facetList += (facetList ? '|' : '') + f.name + '=' + f.values.join('^');
                }
            }
        }
        return facetList;
    }

    ,getSearchResultsByType: function() {
        var resultTypes = '';
        if (window.pageData && pageData.search && pageData.search.resultsByType) {
            for (var i=0; i<pageData.search.resultsByType.length; i++) {
                var r = pageData.search.resultsByType[i];
                resultTypes += (resultTypes ? '|' : '') + r.name + (r.results || r.values ? '=' + (r.results || r.values) : '');
            }
        }
        return resultTypes;
    }

    ,getJournalInfo: function() {
        var info = '';
        if (window.pageData && pageData.journal && (pageData.journal.name || pageData.journal.specialty || pageData.journal.section || pageData.journal.issn || pageData.journal.issueNumber || pageData.journal.volumeNumber || pageData.journal.family || pageData.journal.publisher)) {
            var journal = pageData.journal;
            info = (journal.name || 'no name') 
            + '|' + (journal.specialty || 'no specialty') 
            + '|' + (journal.section || 'no section') 
            + '|' + (journal.issn || 'no issn') 
            + '|' + (journal.issueNumber || 'no issue #') 
            + '|' + (journal.volumeNumber || 'no volume #')
            + '|' + (journal.family || 'no family')
            + '|' + (journal.publisher || 'no publisher');

        }
        return info;
    }

    ,getBibliographicInfo: function(doc) {
        if (!doc || !(doc.publisher || doc.indexTerms || doc.publicationType || doc.publicationRights || doc.volumeNumber || doc.issueNumber || doc.subjectAreas || doc.isbn)) {
            return '';
        }

        var terms = doc.indexTerms ? doc.indexTerms.split('+') : '';
        if (terms) {
            terms = terms.slice(0, 5).join('+');
            terms = terms.length > 100 ? terms.substring(0, 100) : terms;
        }

        var areas = doc.subjectAreas ? doc.subjectAreas.split('>') : '';
        if (areas) {
            areas = areas.slice(0, 5).join('>');
            areas = areas.length > 100 ? areas.substring(0, 100) : areas;
        }

        var biblio	= (doc.publisher || 'none')
            + '^' + (doc.publicationType || 'none')
            + '^' + (doc.publicationRights || 'none')
            + '^' + (terms || 'none')
            + '^' + (doc.volumeNumber || 'none')
            + '^' + (doc.issueNumber || 'none')
            + '^' + (areas || 'none')
            + '^' + (doc.isbn || 'none');

        return this.stripProductDelimiters(biblio).toLowerCase();
    }

    ,getContentItem: function() {
        var docs = window.eventData && eventData.content ? eventData.content : pageData.content;
        if (docs && docs.length > 0) {
            return docs[0];
        }
    }

    ,getFormattedDate: function(ts) {
        if (!ts) {
            return '';
        }

        var d = new Date(parseInt(ts) * 1000);

        // now do formatting
        var year = d.getFullYear()
            ,month = ((d.getMonth() + 1) < 10 ? '0' : '') + (d.getMonth() + 1)
            ,date = (d.getDate() < 10 ? '0' : '') + d.getDate()
            ,hours = d.getHours() > 12 ? d.getHours() - 12 : d.getHours()
            ,mins = (d.getMinutes() < 10 ? '0' : '') + d.getMinutes()
            ,ampm = d.getHours() > 12 ? 'pm' : 'am';

        hours = (hours < 10 ? '0' : '') + hours;
        return year + '-' + month + '-' + date;
    }

    ,getVisitorId: function() {
        var orgId = '4D6368F454EC41940A4C98A6@AdobeOrg';
        if(Visitor && Visitor.getInstance(orgId)) {
            return Visitor.getInstance(orgId).getMarketingCloudVisitorID();
        } else {
            return ''
        }
    }

    ,setProductsVariable: function() {
        var prodList = window.eventData && eventData.content ? eventData.content : pageData.content
            ,prods = [];
        if (prodList) {
            for (var i=0; i<prodList.length; i++) {
                if (prodList[i].id || prodList[i].type || prodList[i].publishDate || prodList[i].onlineDate) {
                    if (!prodList[i].id) {
                        prodList[i].id = 'no id';
                    }
                    var prodName = (pageData.page.productName || 'xx').toLowerCase();
                    if (prodList[i].id.indexOf(prodName + ':') != 0) {
                        prodList[i].id = prodName + ':' + prodList[i].id;
                    }
                    prodList[i].id = this.stripProductDelimiters(prodList[i].id);
                    var merch = [];
                    if (prodList[i].format) {
                        merch.push('evar17=' + this.stripProductDelimiters(prodList[i].format.toLowerCase()));
                    }
                    if (prodList[i].type) {
                        var type = prodList[i].type;
                        if (prodList[i].accessType) {
                            type += ':' + prodList[i].accessType;
                        }
                        merch.push('evar20=' + this.stripProductDelimiters(type.toLowerCase()));

                        if(type.indexOf(':manuscript') > 0) {
                            /*
                            var regex = /[a-z]+:manuscript:id:([a-z]+-[a-z]-[0-9]+-[0-9]+)/gmi;
                            var m = regex.exec(prodList[i].id);
                            if(m) {
                                merch.push('evar200=' + m[1]);
                            }
                            merch.push('evar200=' + prodList[i].id);
                            */
                            a = prodList[i].id.lastIndexOf(':');
                            if(a>0) {
                                merch.push('evar200=' + prodList[i].id.substring(a+1).toUpperCase());
                            }
                        } else if(type.indexOf(':submission') > 0) {
                            merch.push('evar200=' + prodList[i].id);
                        }
                    }
                    if(!prodList[i].title) {
                        prodList[i].title = prodList[i].name;
                    }
                    if (prodList[i].title) {
                        merch.push('evar75=' + this.stripProductDelimiters(prodList[i].title.toLowerCase()));
                    }
                    if (prodList[i].breadcrumb) {
                        merch.push('evar63=' + this.stripProductDelimiters(prodList[i].breadcrumb).toLowerCase());
                    }
                    var nowTs = new Date().getTime()/1000;
                    if (prodList[i].onlineDate && !isNaN(prodList[i].onlineDate)) {
                        if(prodList[i].onlineDate > 32503680000) {
                            prodList[i].onlineDate = prodList[i].onlineDate/1000;
                        }
                        merch.push('evar122=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate)));
                        var onlineAge = Math.floor((nowTs - prodList[i].onlineDate) / 86400);
                        onlineAge = (onlineAge === 0) ? 'zero' : onlineAge;
                        merch.push('evar128=' + onlineAge);
                    }
                    if (prodList[i].publishDate && !isNaN(prodList[i].publishDate)) {
                        if(prodList[i].publishDate > 32503680000) {
                            prodList[i].publishDate = prodList[i].publishDate/1000;
                        }
                        merch.push('evar123=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                        var publishAge = Math.floor((nowTs - prodList[i].publishDate) / 86400);
                        publishAge = (publishAge === 0) ? 'zero' : publishAge;
                        merch.push('evar127=' + publishAge);
                    }
                    if (prodList[i].onlineDate && prodList[i].publishDate) {
                        merch.push('evar38=' + this.stripProductDelimiters(pageDataTracker.getFormattedDate(prodList[i].onlineDate) + '^' + pageDataTracker.getFormattedDate(prodList[i].publishDate)));
                    }
                    if (prodList[i].mapId) {
                        merch.push('evar70=' + this.stripProductDelimiters(prodList[i].mapId));
                    }
					if (prodList[i].relevancyScore) {
						merch.push('evar71=' + this.stripProductDelimiters(prodList[i].relevancyScore));
					}
                    if (prodList[i].status) {
                        merch.push('evar73=' + this.stripProductDelimiters(prodList[i].status));
                    }
                    if (prodList[i].previousStatus) {
                        merch.push('evar111=' + this.stripProductDelimiters(prodList[i].previousStatus));
                    }
                    if (prodList[i].entitlementType) {
                        merch.push('evar80=' + this.stripProductDelimiters(prodList[i].entitlementType));
                    }
                    if (prodList[i].recordType) {
                        merch.push('evar93=' + this.stripProductDelimiters(prodList[i].recordType));
                    }
                    if (prodList[i].exportType) {
                        merch.push('evar99=' + this.stripProductDelimiters(prodList[i].exportType));
                    }
                    if (prodList[i].importType) {
                        merch.push('evar142=' + this.stripProductDelimiters(prodList[i].importType));
                    }
                    if (prodList[i].section) {
                        merch.push('evar100=' + this.stripProductDelimiters(prodList[i].section));
                    }
                    if (prodList[i].detail) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].detail.toLowerCase()));
                    } else if(prodList[i].details) {
                        merch.push('evar104=' + this.stripProductDelimiters(prodList[i].details.toLowerCase()));
                    }
                    if (prodList[i].position) {
                        merch.push('evar116=' + this.stripProductDelimiters(prodList[i].position));
                    }
                    if (prodList[i].publicationTitle) {
                        merch.push('evar129=' + this.stripProductDelimiters(prodList[i].publicationTitle));
                    }
                    if (prodList[i].specialIssueTitle) {
                        merch.push('evar130=' + this.stripProductDelimiters(prodList[i].specialIssueTitle));
                    }
                    if (prodList[i].specialIssueNumber) {
                        merch.push('evar131=' + this.stripProductDelimiters(prodList[i].specialIssueNumber));
                    }
                    if (prodList[i].referenceModuleTitle) {
                        merch.push('evar139=' + this.stripProductDelimiters(prodList[i].referenceModuleTitle));
                    }
                    if (prodList[i].referenceModuleISBN) {
                        merch.push('evar140=' + this.stripProductDelimiters(prodList[i].referenceModuleISBN));
                    }
                    if (prodList[i].volumeTitle) {
                        merch.push('evar132=' + this.stripProductDelimiters(prodList[i].volumeTitle));
                    }
                    if (prodList[i].publicationSection) {
                        merch.push('evar133=' + this.stripProductDelimiters(prodList[i].publicationSection));
                    }
                    if (prodList[i].publicationSpecialty) {
                        merch.push('evar134=' + this.stripProductDelimiters(prodList[i].publicationSpecialty));
                    }
                    if (prodList[i].issn) {
                        merch.push('evar135=' + this.stripProductDelimiters(prodList[i].issn));
                    }
                    if (prodList[i].id2) {
                        merch.push('evar159=' + this.stripProductDelimiters(prodList[i].id2));
                    }
                    if (prodList[i].id3) {
                        merch.push('evar160=' + this.stripProductDelimiters(prodList[i].id3));
                    }
                    if (prodList[i].provider) {
                        merch.push('evar164=' + this.stripProductDelimiters(prodList[i].provider));
                    }
                    if (prodList[i].citationStyle) {
                        merch.push('evar170=' + this.stripProductDelimiters(prodList[i].citationStyle));
                    }

                    var biblio = this.getBibliographicInfo(prodList[i]);
                    if (biblio) {
                        merch.push('evar28=' + biblio);
                    }

                    if (prodList[i].turnawayId) {
                        pageData.eventList.push('product turnaway');
                    }

                    var price = prodList[i].price || '', qty = prodList[i].quantity || '', evts = [];
                    if (price && qty) {
                        qty = parseInt(qty || '1');
                        price = parseFloat(price || '0');
                        price = (price * qty).toFixed(2);

                        if (window.eventData && eventData.eventName && eventData.eventName == 'cartAdd') {
                            evts.push('event20=' + price);
                        }
                    }

                    var type = window.pageData && pageData.page && pageData.page.type ? pageData.page.type : '', evt = window.eventData && eventData.eventName ? eventData.eventName : '';
                    if (type.match(/^CP\-/gi) !== null && (!evt || evt == 'newPage' || evt == 'contentView')) {
                        evts.push('event181=1');
                    }
                    if (evt == 'contentDownload' || type.match(/^CP\-DL/gi) !== null) {
                        evts.push('event182=1');
                    }
                    if (evt == 'contentDownloadRequest') {
                        evts.push('event319=1');
                    }
                    if (evt == 'contentExport') {
                        evts.push('event184=1');
                    }
                    if (this.eventFires('recommendationViews')) {
                        evts.push('event264=1');
                    }

                    if(prodList[i].datapoints) {
                        evts.push('event239=' + prodList[i].datapoints);
                    }
                    if(prodList[i].documents) {
                        evts.push('event240=' + prodList[i].documents);
                    }
                    if(prodList[i].size) {
                        evts.push('event335=' + prodList[i].size);
                        evts.push('event336=1')
                    }

                    prods.push([
                        ''					// empty category
                        ,prodList[i].id		// id
                        ,qty				// qty
                        ,price				// price
                        ,evts.join('|')		// events
                        ,merch.join('|')	// merchandising eVars
                    ].join(';'));
                }
            }
        }

        return prods.join(',');
    }
    ,eventFires: function(eventName) {
      var evt = window.eventData && eventData.eventName ? eventData.eventName : '';
      if(evt == eventName) {
        return true;
      }
      // initial pageload and new pages
      if((!window.eventData || evt == 'newPage') && window.pageData && window.pageData.trackEvents) {
        var tEvents = window.pageData.trackEvents;
        for(var i=0; i<tEvents.length; i++) {
          if(tEvents[i] == eventName) {
            return true;
          }
        }
      }
      return false;
    }

    ,md5: function(s){function L(k,d){return(k<<d)|(k>>>(32-d))}function K(G,k){var I,d,F,H,x;F=(G&2147483648);H=(k&2147483648);I=(G&1073741824);d=(k&1073741824);x=(G&1073741823)+(k&1073741823);if(I&d){return(x^2147483648^F^H)}if(I|d){if(x&1073741824){return(x^3221225472^F^H)}else{return(x^1073741824^F^H)}}else{return(x^F^H)}}function r(d,F,k){return(d&F)|((~d)&k)}function q(d,F,k){return(d&k)|(F&(~k))}function p(d,F,k){return(d^F^k)}function n(d,F,k){return(F^(d|(~k)))}function u(G,F,aa,Z,k,H,I){G=K(G,K(K(r(F,aa,Z),k),I));return K(L(G,H),F)}function f(G,F,aa,Z,k,H,I){G=K(G,K(K(q(F,aa,Z),k),I));return K(L(G,H),F)}function D(G,F,aa,Z,k,H,I){G=K(G,K(K(p(F,aa,Z),k),I));return K(L(G,H),F)}function t(G,F,aa,Z,k,H,I){G=K(G,K(K(n(F,aa,Z),k),I));return K(L(G,H),F)}function e(G){var Z;var F=G.length;var x=F+8;var k=(x-(x%64))/64;var I=(k+1)*16;var aa=Array(I-1);var d=0;var H=0;while(H<F){Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=(aa[Z]| (G.charCodeAt(H)<<d));H++}Z=(H-(H%4))/4;d=(H%4)*8;aa[Z]=aa[Z]|(128<<d);aa[I-2]=F<<3;aa[I-1]=F>>>29;return aa}function B(x){var k="",F="",G,d;for(d=0;d<=3;d++){G=(x>>>(d*8))&255;F="0"+G.toString(16);k=k+F.substr(F.length-2,2)}return k}function J(k){k=k.replace(/rn/g,"n");var d="";for(var F=0;F<k.length;F++){var x=k.charCodeAt(F);if(x<128){d+=String.fromCharCode(x)}else{if((x>127)&&(x<2048)){d+=String.fromCharCode((x>>6)|192);d+=String.fromCharCode((x&63)|128)}else{d+=String.fromCharCode((x>>12)|224);d+=String.fromCharCode(((x>>6)&63)|128);d+=String.fromCharCode((x&63)|128)}}}return d}var C=Array();var P,h,E,v,g,Y,X,W,V;var S=7,Q=12,N=17,M=22;var A=5,z=9,y=14,w=20;var o=4,m=11,l=16,j=23;var U=6,T=10,R=15,O=21;s=J(s);C=e(s);Y=1732584193;X=4023233417;W=2562383102;V=271733878;for(P=0;P<C.length;P+=16){h=Y;E=X;v=W;g=V;Y=u(Y,X,W,V,C[P+0],S,3614090360);V=u(V,Y,X,W,C[P+1],Q,3905402710);W=u(W,V,Y,X,C[P+2],N,606105819);X=u(X,W,V,Y,C[P+3],M,3250441966);Y=u(Y,X,W,V,C[P+4],S,4118548399);V=u(V,Y,X,W,C[P+5],Q,1200080426);W=u(W,V,Y,X,C[P+6],N,2821735955);X=u(X,W,V,Y,C[P+7],M,4249261313);Y=u(Y,X,W,V,C[P+8],S,1770035416);V=u(V,Y,X,W,C[P+9],Q,2336552879);W=u(W,V,Y,X,C[P+10],N,4294925233);X=u(X,W,V,Y,C[P+11],M,2304563134);Y=u(Y,X,W,V,C[P+12],S,1804603682);V=u(V,Y,X,W,C[P+13],Q,4254626195);W=u(W,V,Y,X,C[P+14],N,2792965006);X=u(X,W,V,Y,C[P+15],M,1236535329);Y=f(Y,X,W,V,C[P+1],A,4129170786);V=f(V,Y,X,W,C[P+6],z,3225465664);W=f(W,V,Y,X,C[P+11],y,643717713);X=f(X,W,V,Y,C[P+0],w,3921069994);Y=f(Y,X,W,V,C[P+5],A,3593408605);V=f(V,Y,X,W,C[P+10],z,38016083);W=f(W,V,Y,X,C[P+15],y,3634488961);X=f(X,W,V,Y,C[P+4],w,3889429448);Y=f(Y,X,W,V,C[P+9],A,568446438);V=f(V,Y,X,W,C[P+14],z,3275163606);W=f(W,V,Y,X,C[P+3],y,4107603335);X=f(X,W,V,Y,C[P+8],w,1163531501);Y=f(Y,X,W,V,C[P+13],A,2850285829);V=f(V,Y,X,W,C[P+2],z,4243563512);W=f(W,V,Y,X,C[P+7],y,1735328473);X=f(X,W,V,Y,C[P+12],w,2368359562);Y=D(Y,X,W,V,C[P+5],o,4294588738);V=D(V,Y,X,W,C[P+8],m,2272392833);W=D(W,V,Y,X,C[P+11],l,1839030562);X=D(X,W,V,Y,C[P+14],j,4259657740);Y=D(Y,X,W,V,C[P+1],o,2763975236);V=D(V,Y,X,W,C[P+4],m,1272893353);W=D(W,V,Y,X,C[P+7],l,4139469664);X=D(X,W,V,Y,C[P+10],j,3200236656);Y=D(Y,X,W,V,C[P+13],o,681279174);V=D(V,Y,X,W,C[P+0],m,3936430074);W=D(W,V,Y,X,C[P+3],l,3572445317);X=D(X,W,V,Y,C[P+6],j,76029189);Y=D(Y,X,W,V,C[P+9],o,3654602809);V=D(V,Y,X,W,C[P+12],m,3873151461);W=D(W,V,Y,X,C[P+15],l,530742520);X=D(X,W,V,Y,C[P+2],j,3299628645);Y=t(Y,X,W,V,C[P+0],U,4096336452);V=t(V,Y,X,W,C[P+7],T,1126891415);W=t(W,V,Y,X,C[P+14],R,2878612391);X=t(X,W,V,Y,C[P+5],O,4237533241);Y=t(Y,X,W,V,C[P+12],U,1700485571);V=t(V,Y,X,W,C[P+3],T,2399980690);W=t(W,V,Y,X,C[P+10],R,4293915773);X=t(X,W,V,Y,C[P+1],O,2240044497);Y=t(Y,X,W,V,C[P+8],U,1873313359);V=t(V,Y,X,W,C[P+15],T,4264355552);W=t(W,V,Y,X,C[P+6],R,2734768916);X=t(X,W,V,Y,C[P+13],O,1309151649);Y=t(Y,X,W,V,C[P+4],U,4149444226);V=t(V,Y,X,W,C[P+11],T,3174756917);W=t(W,V,Y,X,C[P+2],R,718787259);X=t(X,W,V,Y,C[P+9],O,3951481745);Y=K(Y,h);X=K(X,E);W=K(W,v);V=K(V,g)}var i=B(Y)+B(X)+B(W)+B(V);return i.toLowerCase()}
    ,stripProductDelimiters: function(val) {
        if (val) {
            return val.replace(/\;|\||\,/gi, '-');
        }
    }

    ,setCookie: function(name, value, seconds, domain) {
        domain = document.location.hostname;
        var expires = '';
        var expiresNow = '';
        var date = new Date();
        date.setTime(date.getTime() + (-1 * 1000));
        expiresNow = "; expires=" + date.toGMTString();

        if (typeof(seconds) != 'undefined') {
            date.setTime(date.getTime() + (seconds * 1000));
            expires = '; expires=' + date.toGMTString();
        }

        var type = typeof(value);
        type = type.toLowerCase();
        if (type != 'undefined' && type != 'string') {
            value = JSON.stringify(value);
        }

        // fix scoping issues
        // keep writing the old cookie, but make it expire
        document.cookie = name + '=' + value + expiresNow + '; path=/';

        // now just set the right one
        document.cookie = name + '=' + value + expires + '; path=/; domain=' + domain;
    }

    ,getCookie: function(name) {
        name = name + '=';
        var carray = document.cookie.split(';'), value;

        for (var i=0; i<carray.length; i++) {
            var c = carray[i];
            while (c.charAt(0) == ' ') {
                c = c.substring(1, c.length);
            }
            if (c.indexOf(name) == 0) {
                value = c.substring(name.length, c.length);
                try {
                    value = JSON.parse(value);
                } catch(ex) {}

                return value;
            }
        }

        return null;
    }

    ,deleteCookie: function(name) {
        this.setCookie(name, '', -1);
        this.setCookie(name, '', -1, document.location.hostname);
    }

    ,mapAdobeVars: function(s) {
        var vars = {
            pageName		: 'Page - Analytics Pagename'
            ,channel		: 'Page - Section Name'
            ,campaign		: 'Campaign - ID'
            ,currencyCode	: 'Page - Currency Code'
            ,purchaseID		: 'Order - ID'
            ,prop1			: 'Visitor - Account ID'
            ,prop2			: 'Page - Product Name'
            ,prop4			: 'Page - Type'
            ,prop6			: 'Search - Type'
            ,prop7			: 'Search - Facet List'
            ,prop8			: 'Search - Feature Used'
            ,prop12			: 'Visitor - User ID'
            ,prop13			: 'Search - Sort Type'
            ,prop14			: 'Page - Load Time'
            ,prop15         : 'Support - Topic Name'
            ,prop16			: 'Page - Business Unit'
            ,prop21			: 'Search - Criteria'
            ,prop24			: 'Page - Language'
            ,prop25			: 'Page - Product Feature'
            ,prop28         : 'Support - Search Criteria'
            ,prop30			: 'Visitor - IP Address'
            ,prop33         : 'Page - Product Application Version'
            ,prop34         : 'Page - Website Extensions'
            ,prop60			: 'Search - Data Form Criteria'
            ,prop63			: 'Page - Extended Page Name'
            ,prop65         : 'Page - Online State'
            ,prop67         : 'Research Networks'
            ,prop40: 'Page - UX Properties'

            ,eVar3			: 'Search - Total Results'
            ,eVar7			: 'Visitor - Account Name'
            ,eVar15			: 'Event - Search Results Click Position'
            ,eVar19			: 'Search - Advanced Criteria'
            ,eVar21			: 'Promo - Clicked ID'
            ,eVar22			: 'Page - Test ID'
            ,eVar27			: 'Event - AutoSuggest Search Data'
            ,eVar157		: 'Event - AutoSuggest Search Typed Term'
            ,eVar156		: 'Event - AutoSuggest Search Selected Term'
            ,eVar162		: 'Event - AutoSuggest Search Category'
            ,eVar163		: 'Event - AutoSuggest Search Details'
            ,eVar33			: 'Visitor - Access Type'
            ,eVar34			: 'Order - Promo Code'
            ,eVar39			: 'Order - Payment Method'
            ,eVar41			: 'Visitor - Industry'
            ,eVar42			: 'Visitor - SIS ID'
            ,eVar43			: 'Page - Error Type'
            ,eVar44			: 'Event - Updated User Fields'
            ,eVar48			: 'Email - Recipient ID'
            ,eVar51			: 'Email - Message ID'
            ,eVar52			: 'Visitor - Department ID'
            ,eVar53			: 'Visitor - Department Name'
            ,eVar60			: 'Search - Within Content Criteria'
            ,eVar61			: 'Search - Within Results Criteria'
            ,eVar62			: 'Search - Result Types'
            ,eVar74			: 'Page - Journal Info'
            ,eVar59			: 'Page - Journal Publisher'
            ,eVar76			: 'Email - Broadlog ID'
            ,eVar78			: 'Visitor - Details'
            ,eVar80         : 'Visitor - Usage Path Info'
            ,eVar102		: 'Form - Name'
            ,eVar103        : 'Event - Conversion Driver'
            ,eVar105        : 'Search - Current Page'
            ,eVar106        : 'Visitor - App Session ID'
            ,eVar107        : 'Page - Secondary Product Name'
            ,eVar117        : 'Search - Database'
            ,eVar126        : 'Page - Environment'
            ,eVar141        : 'Search - Criteria Original'
            ,eVar143        : 'Page - Tabs'
            ,eVar161        : 'Search - Channel'
            ,eVar169        : 'Search - Facet Operation'
            ,eVar173        : 'Search - Details'
            ,eVar174        : 'Campaign - Spredfast ID'
            ,eVar175        : 'Visitor - TMX Device ID'
            ,eVar176        : 'Visitor - TMX Request ID'
            ,eVar148        : 'Visitor - Platform Name'
            ,eVar149        : 'Visitor - Platform ID'
            ,eVar152        : 'Visitor - Product ID'
            ,eVar153        : 'Visitor - Superaccount ID'
            ,eVar154        : 'Visitor - Superaccount Name'
            ,eVar177        : 'Page - Context Domain'
            ,eVar189    : 'Page - Experimentation User Id'
            ,eVar190    : 'Page - Identity User'
            ,eVar199    : 'Page - ID+ Parameters'

            ,list2			: 'Page - Widget Names'
            ,list3			: 'Promo - IDs'
        };

        for (var i in vars) {
            s[i] = s[i] ? s[i] : _satellite.getVar(vars[i]);
        }
    }
};

// async support fallback
(function(w) {
	var eventBuffer = [];
	if(w.appData) {
		if(Array.isArray(w.appData)) {
			eventBuffer = w.appData;
		} else {
			console.error('Elsevier DataLayer "window.appData" must be specified as array');
			return;
		}
    }

	w.appData = [];

	var oldPush = w.appData.push;

	var appDataPush = function() {
        oldPush.apply(w.appData, arguments);
        for(var i=0; i<arguments.length; i++) {
            var data = arguments[i];
            if(data.event) {
                if(data.event == 'pageLoad') {
                    w.pageDataTracker.trackPageLoad(data);
                } else {
                    w.pageDataTracker.trackEvent(data.event, data);
                }
            }
        }
	};

	w.appData.push = appDataPush;
	for(var i=0; i<eventBuffer.length; i++) {
	    var data = eventBuffer[i];
	    w.appData.push(data);
	}
})(window);

</script><script>_satellite["_runScript1"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><script src="https://cdn.plu.mx/widget-summary.js" async=""></script><script>_satellite["_runScript2"](function(event, target, Promise) {
_satellite.logger.log("eventDispatcher: clearing tracking state");try{s.events="",s.linkTrackVars="",s.linkTrackEvents=""}catch(e){_satellite.logger.log("eventDispatcher: s object - could not reset state.")}try{dispatcherData=JSON.parse(event.detail),window.ddqueue=window.ddqueue||[],window.ddqueue.push(dispatcherData),window.eventData=dispatcherData.eventData,window.pageData=dispatcherData.pageData,_satellite.track(dispatcherData.eventName)}catch(e){_satellite.logger.log("eventDispatcher: exception"),_satellite.logger.log(e)}
});</script><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div class="js-react-modal"></div><div id="onetrust-consent-sdk"><div class="onetrust-pc-dark-filter ot-hide ot-fade-in"></div><div id="onetrust-pc-sdk" class="otPcCenter ot-hide ot-fade-in" lang="en" aria-label="Preference center" role="region"><div role="alertdialog" aria-modal="true" aria-describedby="ot-pc-desc" style="height: 100%;" aria-label="Cookie Preference Center"><!-- Close Button --><div class="ot-pc-header"><!-- Logo Tag --><div class="ot-pc-logo" role="img" aria-label="Company Logo"><img alt="Company Logo" src="https://cdn.cookielaw.org/logos/static/ot_company_logo.png"></div></div><!-- Close Button --><div id="ot-pc-content" class="ot-pc-scrollbar"><div class="ot-optout-signal ot-hide"><div class="ot-optout-icon"><svg xmlns="http://www.w3.org/2000/svg"><path class="ot-floating-button__svg-fill" d="M14.588 0l.445.328c1.807 1.303 3.961 2.533 6.461 3.688 2.015.93 4.576 1.746 7.682 2.446 0 14.178-4.73 24.133-14.19 29.864l-.398.236C4.863 30.87 0 20.837 0 6.462c3.107-.7 5.668-1.516 7.682-2.446 2.709-1.251 5.01-2.59 6.906-4.016zm5.87 13.88a.75.75 0 00-.974.159l-5.475 6.625-3.005-2.997-.077-.067a.75.75 0 00-.983 1.13l4.172 4.16 6.525-7.895.06-.083a.75.75 0 00-.16-.973z" fill="#FFF" fill-rule="evenodd"></path></svg></div><span></span></div><h2 id="ot-pc-title">Cookie Preference Center</h2><div id="ot-pc-desc">We use cookies which are necessary to make our site work. We may also use additional cookies to analyse, improve and personalise our content and your digital experience. For more information, see our <a href="https://www.elsevier.com/legal/cookienotice/_nocache" target="_blank">Cookie Policy</a> and the list of <a href="https://support.google.com/admanager/answer/9012903" target="_blank">Google Ad-Tech Vendors</a>.
<br>
<br>
You may choose not to allow some types of cookies. However, blocking some types may impact your experience of our site and the services we are able to offer. See the different category headings below to find out more or change your settings.
<br>
</div><button id="accept-recommended-btn-handler">Allow all</button><section class="ot-sdk-row ot-cat-grp"><h3 id="ot-category-title"> Manage Consent Preferences</h3><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="1"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-1" aria-labelledby="ot-header-id-1 ot-status-id-1"></button><!-- Accordion header --><div class="ot-acc-hdr ot-always-active-group"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-1">Strictly Necessary Cookies</h4><div id="ot-status-id-1" class="ot-always-active">Always active</div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-1">These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.
<br><br></p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="1">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="3"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-3" aria-labelledby="ot-header-id-3"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-3">Functional Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-3" id="ot-group-id-3" role="switch" class="category-switch-handler" data-optanongroupid="3" checked="" aria-labelledby="ot-header-id-3"> <label class="ot-switch" for="ot-group-id-3"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Functional Cookies"></span> <span class="ot-label-txt">Functional Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-3">These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="3">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="2"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-2" aria-labelledby="ot-header-id-2"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-2">Performance Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-2" id="ot-group-id-2" role="switch" class="category-switch-handler" data-optanongroupid="2" checked="" aria-labelledby="ot-header-id-2"> <label class="ot-switch" for="ot-group-id-2"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Performance Cookies"></span> <span class="ot-label-txt">Performance Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-2">These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="2">Cookie Details List‎</button></div></div></div><div class="ot-accordion-layout ot-cat-item ot-vs-config" data-optanongroupid="4"><button aria-expanded="false" ot-accordion="true" aria-controls="ot-desc-id-4" aria-labelledby="ot-header-id-4"></button><!-- Accordion header --><div class="ot-acc-hdr"><div class="ot-plus-minus"><span></span><span></span></div><h4 class="ot-cat-header" id="ot-header-id-4">Targeting Cookies</h4><div class="ot-tgl"><input type="checkbox" name="ot-group-id-4" id="ot-group-id-4" role="switch" class="category-switch-handler" data-optanongroupid="4" checked="" aria-labelledby="ot-header-id-4"> <label class="ot-switch" for="ot-group-id-4"><span class="ot-switch-nob" aria-checked="true" role="switch" aria-label="Targeting Cookies"></span> <span class="ot-label-txt">Targeting Cookies</span></label> </div></div><!-- accordion detail --><div class="ot-acc-grpcntr ot-acc-txt"><p class="ot-acc-grpdesc ot-category-desc" id="ot-desc-id-4">These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. If you do not allow these cookies, you will experience less targeted advertising.</p><div class="ot-hlst-cntr"><button class="ot-link-btn category-host-list-handler" aria-label="Cookie Details List" data-parent-id="4">Cookie Details List‎</button></div></div></div><!-- Groups sections starts --><!-- Group section ends --><!-- Accordion Group section starts --><!-- Accordion Group section ends --></section></div><section id="ot-pc-lst" class="ot-hide ot-hosts-ui ot-pc-scrollbar"><div id="ot-pc-hdr"><div id="ot-lst-title"><button class="ot-link-btn back-btn-handler" aria-label="Back"><svg id="ot-back-arw" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 444.531 444.531" xml:space="preserve"><title>Back Button</title><g><path fill="#656565" d="M213.13,222.409L351.88,83.653c7.05-7.043,10.567-15.657,10.567-25.841c0-10.183-3.518-18.793-10.567-25.835
                    l-21.409-21.416C323.432,3.521,314.817,0,304.637,0s-18.791,3.521-25.841,10.561L92.649,196.425
                    c-7.044,7.043-10.566,15.656-10.566,25.841s3.521,18.791,10.566,25.837l186.146,185.864c7.05,7.043,15.66,10.564,25.841,10.564
                    s18.795-3.521,25.834-10.564l21.409-21.412c7.05-7.039,10.567-15.604,10.567-25.697c0-10.085-3.518-18.746-10.567-25.978
                    L213.13,222.409z"></path></g></svg></button><h3>Cookie List</h3></div><div class="ot-lst-subhdr"><div class="ot-search-cntr"><p role="status" class="ot-scrn-rdr"></p><input id="vendor-search-handler" type="text" name="vendor-search-handler" placeholder="Search…" aria-label="Cookie list search"> <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 -30 110 110" aria-hidden="true"><title>Search Icon</title><path fill="#2e3644" d="M55.146,51.887L41.588,37.786c3.486-4.144,5.396-9.358,5.396-14.786c0-12.682-10.318-23-23-23s-23,10.318-23,23
            s10.318,23,23,23c4.761,0,9.298-1.436,13.177-4.162l13.661,14.208c0.571,0.593,1.339,0.92,2.162,0.92
            c0.779,0,1.518-0.297,2.079-0.837C56.255,54.982,56.293,53.08,55.146,51.887z M23.984,6c9.374,0,17,7.626,17,17s-7.626,17-17,17
            s-17-7.626-17-17S14.61,6,23.984,6z"></path></svg></div><div class="ot-fltr-cntr"><button id="filter-btn-handler" aria-label="Filter" aria-haspopup="true"><svg role="presentation" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 402.577 402.577" xml:space="preserve"><title>Filter Icon</title><g><path fill="#fff" d="M400.858,11.427c-3.241-7.421-8.85-11.132-16.854-11.136H18.564c-7.993,0-13.61,3.715-16.846,11.136
      c-3.234,7.801-1.903,14.467,3.999,19.985l140.757,140.753v138.755c0,4.955,1.809,9.232,5.424,12.854l73.085,73.083
      c3.429,3.614,7.71,5.428,12.851,5.428c2.282,0,4.66-0.479,7.135-1.43c7.426-3.238,11.14-8.851,11.14-16.845V172.166L396.861,31.413
      C402.765,25.895,404.093,19.231,400.858,11.427z"></path></g></svg></button></div><div id="ot-anchor"></div><section id="ot-fltr-modal"><div id="ot-fltr-cnt"><button id="clear-filters-handler">Clear</button><div class="ot-fltr-scrlcnt ot-pc-scrollbar"><div class="ot-fltr-opts"><div class="ot-fltr-opt"><div class="ot-chkbox"><input id="chkbox-id" type="checkbox" class="category-filter-handler"> <label for="chkbox-id"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div><div class="ot-fltr-btns"><button id="filter-apply-handler">Apply</button> <button id="filter-cancel-handler">Cancel</button></div></div></div></section></div></div><section id="ot-lst-cnt" class="ot-host-cnt ot-pc-scrollbar"><div id="ot-sel-blk"><div class="ot-sel-all"><div class="ot-sel-all-hdr"><span class="ot-consent-hdr">Consent</span> <span class="ot-li-hdr">Leg.Interest</span></div><div class="ot-sel-all-chkbox"><div class="ot-chkbox" id="ot-selall-hostcntr"><input id="select-all-hosts-groups-handler" type="checkbox"> <label for="select-all-hosts-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-vencntr"><input id="select-all-vendor-groups-handler" type="checkbox"> <label for="select-all-vendor-groups-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div><div class="ot-chkbox" id="ot-selall-licntr"><input id="select-all-vendor-leg-handler" type="checkbox"> <label for="select-all-vendor-leg-handler"><span class="ot-label-txt">checkbox label</span></label> <span class="ot-label-status">label</span></div></div></div></div><div class="ot-sdk-row"><div class="ot-sdk-column"><ul id="ot-host-lst"></ul></div></div></section></section><div class="ot-pc-footer ot-pc-scrollbar"><div class="ot-btn-container"> <button class="save-preference-btn-handler onetrust-close-btn-handler">Confirm my choices</button></div><!-- Footer logo --><div class="ot-pc-footer-logo"><a href="https://www.onetrust.com/products/cookie-consent/" target="_blank" rel="noopener noreferrer" aria-label="Powered by OneTrust Opens in a new Tab"><img alt="Powered by Onetrust" src="https://cdn.cookielaw.org/logos/static/powered_by_logo.svg" title="Powered by OneTrust Opens in a new Tab"></a></div></div><!-- Cookie subgroup container --><!-- Vendor list link --><!-- Cookie lost link --><!-- Toggle HTML element --><!-- Checkbox HTML --><!-- plus minus--><!-- Arrow SVG element --><!-- Accordion basic element --><span class="ot-scrn-rdr" aria-atomic="true" aria-live="polite"></span><!-- Vendor Service container and item template --></div><iframe class="ot-text-resize" sandbox="allow-same-origin" title="onetrust-text-resize" style="position: absolute; top: -50000px; width: 100em;" aria-hidden="true"></iframe></div></div><button aria-label="Feedback" type="button" id="_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E" data-layout="badgeBlank" class="_pendo-badge _pendo-badge_" style="z-index: 19000; margin: 0px; height: 32px; width: 128px; font-size: 0px; background: rgba(255, 255, 255, 0); padding: 0px; line-height: 1; min-width: auto; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; border: 0px; float: none; vertical-align: baseline; cursor: pointer; position: absolute; top: 152432px; left: 912px;"><img id="pendo-image-badge-19b66351" src="https://pendo-static-5661679399600128.storage.googleapis.com/D_T2uHq_M1r-XQq8htU6Z3GjHfE/guide-media-75af8ddc-3c43-49fd-8836-cfc7e2c3ea60" alt="Feedback" data-_pendo-image-1="" class="_pendo-image _pendo-badge-image" style="display: block; height: 32px; width: 128px; box-shadow: rgb(136, 136, 136) 0px 0px 0px 0px; float: none; vertical-align: baseline;"></button></body></html>